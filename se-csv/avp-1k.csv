title,viewcount,title,body,body,acceptedanswerid
"How can I crop a video with ffmpeg?","137952","","<p>I have an MP4 file of a screen recording that I need to crop down. How can I accomplish this without using expensive tools like Adobe Premier or Final Cut? I prefer <code>ffmpeg</code> because I have used it before.</p>
","<p>Use the <a href=""https://ffmpeg.org/ffmpeg-filters.html#crop""><code>crop</code> filter</a>:</p>

<pre><code>ffmpeg -i in.mp4 -filter:v ""crop=out_w:out_h:x:y"" out.mp4
</code></pre>

<p>Where the options are as follows:</p>

<ul>
<li><code>out_w</code> is the width of the output rectangle</li>
<li><code>out_h</code> is the height of the output rectangle</li>
<li><code>x</code> and <code>y</code> specify the top left corner of the output rectangle</li>
</ul>

<h2>Original image</h2>

<p><img src=""https://i.stack.imgur.com/LAW6C.png"" alt=""original image""><br>
<sub>Original 320x240 image</sub></p>

<h2>Example 1</h2>

<p><img src=""https://i.stack.imgur.com/vqW5O.png"" alt=""80x60""></p>

<p>To crop a 80&times;60 section, starting from position (200, 100):</p>

<pre><code>ffmpeg -i in.mp4 -filter:v ""crop=80:60:200:100"" -c:a copy out.mp4
</code></pre>

<ul>
<li>The audio is <a href=""http://ffmpeg.org/ffmpeg.html#Stream-copy"">stream copied</a> in this example, so re-encoding is avoided.</li>
</ul>

<h2>Example 2</h2>

<p><img src=""https://i.stack.imgur.com/IwvRw.png"" alt=""bottom right quarter""></p>

<p>To crop the bottom right quarter:</p>

<pre><code>ffmpeg -i in.mp4 -filter:v ""crop=in_w/2:in_h/2:in_w/2:in_h/2"" -c:a copy out.mp4
</code></pre>

<p>This is the same as:</p>

<pre><code>ffmpeg -i in.mp4 -filter:v ""crop=320/2:240/2:320/2:240/2"" -c:a copy out.mp4
</code></pre>

<p>Which is the same as:</p>

<pre><code>ffmpeg -i in.mp4 -filter:v ""crop=240:120:240:120"" -c:a copy out.mp4
</code></pre>

<ul>
<li>You can refer to the input image size with <code>in_w</code> and <code>in_h</code> as shown in this first example. The output width and height can also be used with <code>out_w</code> and <code>out_h</code>.</li>
</ul>

<h2>Example 3</h2>

<p><img src=""https://i.stack.imgur.com/l1ILH.png"" alt=""20 pixels from the top, and 20 from the bottom""></p>

<p>Crop 20 pixels from the top, and 20 from the bottom:</p>

<pre><code> ffmpeg -i in.mp4 -filter:v ""crop=in_w:in_h-40"" -c:a copy out.mp4
</code></pre>

<ul>
<li>The filter will automatically center the crop if <code>x</code> and <code>y</code> are omitted such as in this example.</li>
</ul>

<h2>Previewing</h2>

<p>You can take a crop (heh heh) and preview it live with <code>ffplay</code>:</p>

<pre><code>ffplay -i input -vf ""crop=in_w:in_h-40""
</code></pre>

<p>This way you can experiment and adjust your cropping without the need to encode, view, repeat.</p>

<h2>Notes</h2>

<ul>
<li><p><a href=""http://ffmpeg.org/ffmpeg-filters.html#crop""><code>crop</code> filter documentation</a></p></li>
<li><p>Default encoder for MP4 is <code>libx264</code> (H.264 video) or <code>mpeg4</code> (MPEG-4 Part 2 video) depending on your <code>ffmpeg</code> build. See <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"">FFmpeg Wiki: H.264 Video Encoding Guide</a> for more info.</p></li>
<li><p>Instead of cropping and re-encoding, consider cropping upon playback. This is possible with any player worth using.</p></li>
<li><p>Ancient <code>ffmpeg</code> builds used <code>-croptop</code>, <code>-cropbottom</code>, <code>-cropleft</code>, <code>-cropright</code> options instead of the <code>crop</code> filter. If this is the case for you then <a href=""http://ffmpeg.org/download.html"">get a modern <code>ffmpeg</code></a>. Development is very active and there is no reason to use an antique.</p></li>
</ul>
","4571"
"How to Create a High Quality, Small File Size .mp4 from .mov","101193","","<p>Recently, I helped a friend produce a short zombie comedy. </p>

<p>He has now asked me to add it to the film's website as a digital download. I'm not having issues with the digital download. My issue is how to create a high-quality file (preferably .mp4) with a small file size.</p>

<p>I currently have a copy of the film in .mov (created in Final Cut). The .mov file is 3.2GB. The film is 29:52 in running time.</p>

<p>From other experiences, I know a 30 minute high quality .mp4 can be 185mb - 250mb. The problem, I can't seem to find any legit tutorials to create a high quality .mp4 file with this type of compression. The closest I have been able to get is 385mb (I believe that was with VLC, but I last converted the file a few months ago, so I'm not positive)</p>

<p>I am on a Windows 7 machine.</p>

<p>I have a wide variety of software at my disposal and would likely be able to purchase software, if needed.</p>
","<p>A really good program you can use is <a href=""http://handbrake.fr/"">Handbrake</a>. It's a very popular program among so called ""pirates"" because it's really good at getting a high quality even at tiny file sizes. And this is with good reason, because the main purpose of Handbrake is, indeed, to make the movies small with high quality. Personally I use it when I distribute my short movies on the internet, and it works like a charm. But enough talking, let's get to the settings!</p>

<p>Let's start with the beginning and go through the important settings.
On the right you have a presets panel. Choose Regular -> Normal or Regular -> High Profile.</p>

<p>Now, we start with the Picture settings.
The anamorphic setting is about the aspect ratio. So you've probably heard about the fact that in video, pixels aren't always square. So if you have widescreen footage, if you compress that to be much thinner, like an OLD SD TV, you could tell the video player to make each pixel wider than it actually is. So, you started have a 500 wide picture, you compressed it to 300, then you tell the player to make those 300 as wide as 500 again.</p>

<p>This is just a basic idea about what it is. You can just leave it at Loose with Modulus at 16. But if you want to learn more and squeeze the absolute maximum out of the file, read about it <a href=""https://trac.handbrake.fr/wiki/AnamorphicGuide"">here</a>.</p>

<p>Leave the cropping at Automatic, unless you want to crop.</p>

<p>Then we go over to the Video Filters. You can turn all of these off. They are made to correct bad footage, which you've probably already done in your video composing software..</p>

<p>Now, Video. These are the crucial settings. Use H.264. This is the format that gives you the highest quality compared to file size today. Quality. This is THE setting if you only want to set ONE. Set it to Avg Bitrate. This is the best one because if you have a lot of blacks in the picture (at night), you can optimize for that and get even smaller file sizes, but at other points in the movie you'll need more space, and so average is the best here. Notice that the size is in kbps which means kilo-bits-per-second. One kilo is 1000 (not 1024, that's kibibit.). One bit is 1/8 of a byte. So if you want a 10 second movie that's 1 MB, you want to enter 1 MB = (1024 * 1kB * 8bits) / 10s = 820 kbps. </p>

<p>Remember that the audio will also add up in addition to this. You just have to test some values until you get an acceptable quality that's small enough for you. If you enable 2-pass encoding, it will evaluate the movie twice, which means smaller file sizes compared to the quality, but it also means that you need more CPU (computer power) to watch it. So turn it on if you want to watch it on computers, leave it off if you want to watch it on mobile phones/tablets.</p>

<p>As for the audio, you just set it to what the output of your original file is. The sample rate of most movies is 48 kHz, but if you recorded in 44.1 kHz, set it to 44.1 kHz... As for the bit rate, 160 is acceptable, but if you have high quality mics and music, set it to 192. You can go higher than this, but as you want small file sizes, there is no need. You could also go lower depending on what size you want to end up with. But do a Preview and listen to the result until you are happy with it.</p>

<p>Then, the last thing you need to do is to tick off ""Large file size"". You need to have this on if the file is going to be larger than 4GB, but it will break compability with a lot of devices if you leave it on. Then you turn on ""Web Optimized"". It will make your movie better suited for progressive download so you can shuffle in the movie without downloading it all. But if you are planning for your users to download the whole movie as .mp4 and watch it on the computer, leave it off.</p>

<p>Now, set a file name and click Start. You can also Preview a small clip when you're testing the bitrate setting. Make sure that the extension is .mp4 and not .m4v. This isn't important, but you asked for .mp4, not .m4v. It's just the extension that is the difference anyway..</p>

<p>Hope you get your video squeezed down in size and yet keep the quality as high as possible! Good luck :)</p>
","7345"
"Repeat/loop Input Video with ffmpeg?","60233","","<p>I just want to loop a mp4 video with ffmpeg and keep the current settings and codec.</p>

<p>For example if input.mp4 is 0:10 long, and I would want to loop it 4 times so output.mp4 is 0:40 long, how could I change the following command line to do that?</p>

<pre><code>ffmpeg -i input.mp4 -c copy output.mp4
</code></pre>

<p>I tried...</p>

<pre><code>ffmpeg -loop 4 -i input.mp4 -c copy output.mp4
</code></pre>

<p>...but I get the error ""Option loop not found.""</p>
","<h1>Concat demuxer</h1>

<p>This allows you to loop an input without needing to re-encode.</p>

<ol>
<li><p>Make a text file. Contents of an example text file to repeat 4 times.</p>

<pre><code>file 'input.mp4'
file 'input.mp4'
file 'input.mp4'
file 'input.mp4'
</code></pre></li>
<li><p>Then run <code>ffmpeg</code>:</p>

<pre><code>ffmpeg -f concat -i list.txt -c copy output.mp4
</code></pre></li>
</ol>

<h3>For Linux users</h3>

<p>This example is the same as above but you don't have to manually make <code>list.txt</code>:</p>

<pre><code>for i in {1..4}; do printf ""file '%s'\n"" input.mp4 &gt;&gt; list.txt; done
ffmpeg -f concat -i list.txt -c copy output.mp4
</code></pre>

<p>With most commonly-used modern shells, you can even avoid the creation of the <code>list.txt</code> file entirely. For example, with bash:</p>

<pre><code>ffmpeg -f concat -i &lt;(for i in {1..4}; do printf ""file '%s'\n"" input.mp4; done) -c copy output.mp4
</code></pre>

<p>Also see:</p>

<ul>
<li><a href=""https://ffmpeg.org/ffmpeg-formats.html#concat"" rel=""nofollow noreferrer"">FFmpeg concat demuxer documentation</a></li>
<li><a href=""https://trac.ffmpeg.org/wiki/Concatenate"" rel=""nofollow noreferrer"">FFmpeg Wiki: Concatenate</a></li>
</ul>

<hr>

<h1>loop filter</h1>

<p>Example using the <a href=""https://ffmpeg.org/ffmpeg-filters.html#loop"" rel=""nofollow noreferrer"">loop filter</a> to loop 4 times, each loop is 75 frames, each loop skips the first 25 frames of the input:</p>

<pre><code>ffmpeg -i input -filter_complex loop=3:75:25 output
</code></pre>

<ul>
<li>Filtering requires re-encoding.</li>
<li>This filter <a href=""https://video.stackexchange.com/a/18386/1760"">places all frames into memory</a>.</li>
<li>Using <code>loop=3</code> will loop 4 times.</li>
<li>To loop infinitely use <code>-1</code>.</li>
<li>You must list the number of frames to loop (shown as 75 in the example above). Max value is 32767.</li>
<li>Also see <code>ffmpeg -h filter=loop</code>.</li>
</ul>

<hr>

<h1>movie filter</h1>

<p>The <a href=""https://ffmpeg.org/ffmpeg-filters.html#movie"" rel=""nofollow noreferrer"">movie filter</a> has a loop option. See <a href=""https://video.stackexchange.com/a/16933/1760"">answer</a> below by Monah Tuk. Filtering requires re-encoding. For audio use the amovie filter.</p>

<hr>

<h1><code>-stream_loop</code> option</h1>

<p>See <a href=""https://video.stackexchange.com/a/17389/1760"">answer</a> below by Thingy and <a href=""https://video.stackexchange.com/a/18379/1760"">answer</a> by Mulvya. Note that this option is buggy (see bug reports <a href=""https://trac.ffmpeg.org/ticket/6121"" rel=""nofollow noreferrer"">#6121</a> and <a href=""https://trac.ffmpeg.org/ticket/6121"" rel=""nofollow noreferrer"">#5719</a>) and may not work for you. As mentioned by mulvya in ticket #6121 one workaround is to first re-mux to MKV or TS then use <code>-stream_loop</code> in the next command.</p>

<hr>

<h1><code>-loop</code> option</h1>

<p>The <code>-loop</code> option is specific to the <a href=""https://ffmpeg.org/ffmpeg-formats.html#image2-1"" rel=""nofollow noreferrer"">image file demuxer</a> and <a href=""https://ffmpeg.org/ffmpeg-formats.html#gif-2"" rel=""nofollow noreferrer"">gif muxer</a>, so it can't be used for typical video files, but it can be used to infinitely loop a series of input images. This example will loop over and over but the <code>-t 30</code> will limit the output duration to 30 seconds:</p>

<pre><code>ffmpeg -loop 1 -i %03d.png -t 30 output.mkv
</code></pre>

<p>Or to loop a GIF output:</p>

<pre><code>ffmpeg -i input -loop 3 output.gif
</code></pre>
","12906"
"How do I 'downgrade' an Adobe Premiere Pro project file to open in older versions?","52849","","<p>Is there any way at all (perhaps with a plugin, paid or otherwise) to save an Adobe Premiere Pro project file so that it's compatible with an older version of the software? The scenario I have is that I use Adobe Premiere Pro CS5.5 but I need to send project files for editing to someone who only has CS3.</p>
","<p>I dont know of any software for Premiere that can do this (there are plugins for After Effects but cant find any for Premiere - thats not to say they arent out there). </p>

<p>I think your best option would be to <strong><a href=""http://help.adobe.com/en_US/premierepro/cs/using/WScbecf3f614eb38be7a1f773012504357269-8000.html"">export your sequence as an XML</a></strong>. You will loose any CS5.5 features there are not in CS3, but this would occur regardless of which method you use to get the project into CS3.</p>

<blockquote>
  <ol>
  <li>Choose File > Export > Final Cut Pro XML.</li>
  <li>In the Save Converted Project As dialog box, browse to a location for the XML file, and type a file name. </li>
  <li>Click Save.</li>
  </ol>
</blockquote>

<hr>
","3813"
"How do I change frame size, preserving width (using ffmpeg)?","49798","","<p>I'm new to this StackExchange site, and to using the FFmpeg encoder. I have a video that is 720x480 (3:2) that I would like to change to 720x406 (16:9) to match the other videos I have for a site. I have tried various command combinations, including:</p>

<pre><code>ffmpeg -i myfile.mv4 -s 720x406 outfile.mp4
ffmpeg -i myfile.m4v -c copy -aspect 16:9 outfile.mp4
</code></pre>

<p>What I end up with is a video that scaled down to the specified height, but didn't preserve the width.  It ends up at 608x406 or thereabouts. I've been playing with various GUI versions of FFmpeg today, and they seem to do the same thing.</p>

<p>What would be the proper options/parameters to pass in (assuming I'll have more control via the command line) to achieve the desired frame size?</p>
","<p>using the scale filter will do it, but there is a bit more to it.</p>

<pre><code>ffmpeg -i input.mov -vf scale=720x406 output.mov 
</code></pre>

<p>will create a movie with the required pixel dimensions, but if you look at the output you'll find that it adds information into the metadata so that it will play back at the same aspect ratio as the original, by using non-square pixels. So if you want to stretch the movie anamorphically to a new aspect ratio you need to manually set the pixel aspect ratio, called the SAR for ""Sample Aspect Ratio"", thus for square pixels use:</p>

<pre><code>ffmpeg -i input.mov -vf scale=720x406,setsar=1:1 output.mov
</code></pre>

<p>Alternatively you can set the display aspect ratio to whatever you want, thus:</p>

<pre><code>ffmpeg -i input.mov -vf scale=720x406,setdar=16:9 output.mov
</code></pre>

<p>What I'm doing with the -vf command and the x=y,z=a expressions that follow is creating a chain of <a href=""http://ffmpeg.org/ffmpeg-filters.html"">filters</a>. Filters can be quite complex, but in the most simple usage they take the form <code>effect=parameter,nexteffect=anotherparameter</code>, and they get processed in the order you write them.</p>

<p>You may want to put additional commands for the codec and so on, eg <code>-c:v libx264</code> to use the x264 mp4 encoder, and something like <code>-crf 20</code> to set the constant rate factor to 20 (usually a pretty good compromise between size and quality). So, a reasonably complete command would look like:</p>

<pre><code>ffmpeg -i input.mov -vf scale=720x406,setdar=16:9 -c:v libx264 -preset slow -profile:v main -crf 20 output.mov
</code></pre>

<p>Main profile is good for device compatibility, the <code>slow</code> preset for the libx264 encoder is a pretty good balance of speed and quality, so this is a good general web-encoding workhorse. You can make it faster by using <code>fast</code> or <code>veryfast</code> or slower with <code>veryslow</code> and <code>placebo</code> will make it ever so slightly better than <code>veryslow</code> at the expense of a lot more processing time (hence the name).</p>

<p>BTW <strong>Don't use -c copy</strong>, that means just copy the video and audio streams without doing anything to them <strong>at all</strong>, so nothing you do in terms of scale, codec, bitrate etc will have any effect.</p>
","9967"
"How can I crop a section of the video screen in Premiere Pro?","47782","","<p>Hi I've taken a 1080p screen recording on my desktop but only want a specific crop of the actual recorded screen in the final video. How can I do this in Premiere Pro CS4?</p>
","<p>You want to use an effect that's called a 4 point garbage matte.  It allows you to set 4 points which create a box and everything outside it is removed.
Here is a link to a tutorial on how to use the effect 4 point garbage matte.
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/Y2FI_d1WVDc?start=0""></iframe>
            </div></div></p>
","4229"
"Adobe Premiere Pro CC - Crop a video and export at the exact cropped size","44433","","<p>I have a video that has 4 up-down-left-right black borders of different sizes.  </p>

<p>I want to get rid of these black borders so the exported file has the exact size of the cropped area.</p>

<p>To do this, I used the ""crop"" effect (I would have preferred a tool that allows me to set the crop dimensions in pixels, but I guess that's not supported). But then I had a problem when exporting the video:</p>

<p>I'd like to be able to keep a hand on the parameters of the exported video, and ensure that the video will have the exact resolution of the cropped area. But I could not accomplish this.</p>

<p>Can someone tell me how to do this?</p>

<p><strong>EDIT:</strong><br>
My source video is 1002 x 720 (so it's in landscape mode)
It contains a 316 x 562 portion I want to extract  (so, portrait mode)<br>
The minimum I want is to be able to obtain a 316 x 562 video containing just the cropped section. The best I could obtain is a 632 x 1124 (or any other proportional scale) video on output.</p>
","<p>I would probably approach it by first measuring the size of the area that you want to crop to.  This can be done by exporting a frame of the image and cropping in another program (like Photoshop).  </p>

<p>I would then create a sequence based on the other settings of the video, but alter the resolution of the sequence to be that of the portion of the video you want.  </p>

<p>After that, you should be able to bring in the video to the sequence.  By default, it will be come in at 100% of it's size.  It may not be perfectly centered in your frame though, so use the Motion settings under the effects control to position the actual video portion in the center of the frame.</p>

<p>You should then simply be able to export and the outside potions will be cropped off without any scaling.</p>

<p>This may seem a bit complicated, but it is the simplest way to do it since the default behavior is to fit the sequence to the size of the original clip and cropping doesn't do anything to change the sequence size.  That's why we have to start with the correct sequence size and when we have that, no crop effect is needed as only the content we want fits in the sequence.</p>
","10472"
"How to encode a video at 30 FPS from images taken at 7 FPS","37499","","<p>I have no knowledge whatsoever about video encoding, I have an application written in OpenGL and I am saving the frames it generated into <code>png</code> images. The frames from my app are at 7 FPS and I am trying to create a video from those.</p>

<p>To that purpose I use ffmpeg like this</p>

<p><code>ffmpeg -framerate 30 -i img%03d.png -c:v libx264 -r 30 -pix_fmt yuv420p out.mp4</code></p>

<p>which was taken <a href=""https://trac.ffmpeg.org/wiki/Create%20a%20video%20slideshow%20from%20images"">from this site</a>, however I was wondering if it is possible to have a video that shows the animation at 30 FPS?</p>
","<p>I'm using the below code to do the same thing.</p>

<pre><code>ffmpeg -framerate 30 -i img%03d.png -c:v libx264 -pix_fmt yuv420p -crf 23 output.mp4
</code></pre>

<p>This reads the input at 30 fps (the <code>-framerate 30</code> option) and encodes it using the <code>libx264</code> codec with <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264#crf"">constant rate factor</a> of 23 (the <code>-crf 23</code> option).</p>
","13069"
"Add an image overlay in front of video using ffmpeg","37116","","<p>I have many videos and I need to put an image in front of them for about 5 seconds, but adding it manually and rendering it each time would take really long, so I am asking if it is possible to do so via ffmpeg and if you could, please, help me out with it since I have no experience with ffmpeg. I've found some commands already but none of them worked.
This one for example.</p>

<pre><code>ffmpeg -itsoffset 5 -i in.mp4 -r 25 -loop 1 -i intro.png -filter_complex ""[1:v] fade=out:125:25:alpha=1 [intro]; [0:v][intro] overlay [v]"" -map ""[v]"" -map 0:a -acodec copy out.mp4
</code></pre>
","<p>You can do a simple image overlay using the following syntax:</p>

<pre><code>ffmpeg -i input.mp4 -i image.png \
-filter_complex ""[0:v][1:v] overlay=25:25:enable='between(t,0,20)'"" \
-pix_fmt yuv420p -c:a copy \
output.mp4
</code></pre>

<p><code>overlay=25:25</code> means we want to position the image 25px to the right and 25px down, originating from the top left corner (0:0).</p>

<p><code>enable='between(t,0,20)'</code> means we want the image to show between second 0 and 20.</p>

<p><code>[0:v][1:v]</code> means that we want the first video file we import with <code>-i</code>, in our case input.mp4 or how ffmpeg sees it video input file number 0, to be under video input file 1, in our case image.png. <code>:v</code> just means we want video 0 and video 1. <code>[0:a]</code> would mean we want the first imported audio track. Which would also come from input.mp4 but would point to the audio track instead of the video track in the mp4 file.</p>

<p>If you want a certain image quality/settings and not the settings ffmpeg chooses, add the image and or audio encoding options you want to use. The default video encoder will be x264. Check the <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"" rel=""nofollow noreferrer"">H.264 encoding guide</a> for possible settings.</p>

<p>The <code>-acodec copy</code> / <code>-c:a copy</code> that you have in your command f.e. would simply re-use the audio from the source file. Though you can't do that with the video of course (in this case), that has to be transcoded because we are creating a new video source.</p>

<p>If you want to transcode audio, remove the <code>-c:a copy</code> part. You may have to explicitly specify an encoder, e.g. <code>-c:a aac -strict experimental</code>. See the <a href=""https://trac.ffmpeg.org/wiki/Encode/AAC"" rel=""nofollow noreferrer"">AAC encoding guide</a> for more info.</p>
","12111"
"Can I use the videos marked as ""youtube standard license"" for a non profit video?","36209","","<p>I'm creating a video for my institution.</p>

<p>It's a scientific institution, so I suppose that the video can be considered no-profit (we don't make it to sell our products but to show our scientific project).</p>

<p>I already searched around for explanations about the youtube standard license, but it seems there is not much clear information on that: only vague posts ( <a href=""http://productforums.google.com/forum/#!topic/youtube/N7LaB524fkA"" rel=""noreferrer"">google groups</a> or <a href=""http://answers.yahoo.com/question/index?qid=20110725170611AANFmJo"" rel=""noreferrer"">yahoo answers</a> ) in which there is not a clear reference to the term of use (which I'm not able to read as I'm not a lawyer), while there is much documentation on the <a href=""http://creativecommons.org/licenses/by/2.0/"" rel=""noreferrer"">CC BY</a> license.</p>

<p>So my questions are:</p>

<ol>
<li><p>can I use the videos marked as ""<em>youtube standard license</em>"" for this video, of course citing the source?</p></li>
<li><p>In case I can't, do you know any other place where I can take video I can use for this non commercial purpose of makning a scientific video to show the project?</p></li>
</ol>
","<p>I know you probably already found this link, but according to <a href=""http://answers.yahoo.com/question/index?qid=20110725170611AANFmJo"">Yahoo</a>, you will want to contact the Uploader before you try to use it. Thats what I would try. I don't think I would want my youtube videos just used by anyone who wanted to. Seems <a href=""http://productforums.google.com/forum/#!topic/youtube/N7LaB524fkA"">this</a> person ran into that problem.</p>

<p>If a video is only using the <a href=""http://www.youtube.com/yt/copyright/creative-commons.html"">Creative Commons License</a> you may use it, but otherwise contact the Youtube user.</p>

<p>Good luck.</p>
","7010"
"Why processor is ""better"" for encoding than GPU?","32086","","<p>I was reading this <a href=""https://ericolon.wordpress.com/2013/01/06/the-secrets-of-yify-and-high-quality-and-small-file-sizes-are-not-so-secret-after-all-encoding-high-quality-low-bitrate-videos-in-handbrake-for-any-device/"" rel=""noreferrer"">article</a> and I saw that a CPU is better for video compression than a GPU.</p>

<p>The article only says that happens because the processor can handle more complex algorithms than the GPU, but I want a more technical explanation, I did some search's on internet but I didn't find anything.</p>

<p>So, anyone know to explain or link a site to I had a more deep explanation of this?</p>
","<p>The article you linked is not very good.  </p>

<blockquote>
  <p>Normally, single pass bitrate encodings convert your bitrate into a RF
  value with a maximum bitrate limit and takes it from there.</p>
</blockquote>

<p>x264's one-pass ABR ratecontrol is not implemented as CRF + limit.  He's right that 2pass is by far the best way to hit a target bitrate, though.</p>

<p>And he apparently doesn't realize that he could start x264 with threads=3 or something, to leave some CPU time free for other tasks.  Or set x264's priority to verylow, so it only gets CPU time that no other task wants.</p>

<p>He also mixes up threads=1 with using CUDA, or something.  No wonder you have questions, because that article has a TERRIBLE explanation.  The whole article basically boils down to: use <code>x264 --preset veryslow --tune film --crf 26 in.m2ts --out out.mkv</code>, or maybe use some light filtering with an input AviSynth script.  He actually recommends ""placebo"".  That's hilarious.  I've never seen a pirated file encoded with placebo.  (you can tell from <code>me=esa</code> or <code>me=tesa</code>, instead of <code>me=umh</code> for all the good quality presets, right up to <code>veryslow</code>.</p>

<p>He also doesn't mention using 10bit color depth.  Slower to encode and decode, but even after downconverting back to 8bit, you get better 8-bit SSIM.  Having more precision for motion vectors apparently helps.  Also, not having to round off to exactly a whole 8 bit value helps.  You can think of 8-bit per component as a speed-hack; quantizing in the frequency-domain and then compressing that with CABAC means that higher bit-depth coefficients don't have to take more space.</p>

<p>(BTW, h.265 gets less benefit from 10-bit encodes for 8-bit video because it already has more precision for motion vectors.  If there is a benefit to using 10-bit x265 for 8-bit video inputs, it's smaller than with x264.  So it's less likely that the speed penalty will be worth it.)</p>

<h2>To answer your actual question:</h2>

<p>edit: doom9 is up again now, so I'll tidy up the link.  Go to it for proper quoting of who said what.</p>

<p><a href=""http://forum.doom9.org/showthread.php?p=1135399#post1135399"" rel=""noreferrer"">http://forum.doom9.org/showthread.php?p=1135399#post1135399</a></p>

<p>google only caches the stupid print version which doesn't properly show the quoting.  I'm not quite sure which parts of these messages are quotes, and which are attributed to the person themselves.</p>

<blockquote>
  <blockquote>
    <p>Highly irregular branching patterns (skip modes) and bit manipulation (quantization/entropy coding) don't suit present GPUs. IMO the only really good application at the moment are full search ME algorithms, in the end though accelerated full search is still slow even if it's faster than on the CPU.<br>
    -- MfA</p>
  </blockquote>
  
  <p>Actually, basically everything can be reasonably done on the GPU except CABAC (which could be done, it just couldn't be parallelized).</p>
  
  <p>x264 CUDA will implement a fullpel and subpel ME algorithm initially; later on we could do something like RDO with a bit-cost approximation instead of CABAC. </p>
  
  <blockquote>
    <p>Because it has to do everything at single precision floating point<br>
    -- MfA</p>
  </blockquote>
  
  <p>Wrong, CUDA supports integer math.</p>
  
  <p>-- Dark Shikari</p>
</blockquote>

<p>Dark Shikari is the x264 maintainer, and developer of most of the features since 2007 or so.</p>

<p>AFAIK, this CUDA project didn't pan out.  There is support for using OpenCL to offload some work from the lookahead thread (quick I/P/B decision, not a high quality final encode of the frame).</p>

<hr>

<p><strong>My understanding</strong> is that the search space for video encoding is SO big that smart heuristics for early-termination of search paths on CPUs beat the brute-force GPUs bring to the table, at least for high quality encoding.  It's only compared to <code>-preset ultrafast</code> where you might reasonably choose HW encoding over x264, esp. if you have a slow CPU (like laptop with dual core and no hyperthreading).  On a fast CPU (i7 quad core with hyperthreading), x264 <code>superfast</code> is probably going to be as fast, and look better (at the same bitrate).</p>

<p>If you're making an encode where rate-distortion (quality per file size) matters at all, you should use x264 <code>-preset medium</code> or slower.  If you're archiving something, spending a bit more CPU time now will save bytes for as long as you're keeping that file around.</p>

<p>side note, if you ever see messages from deadrats on a video forum, it's not going to be helpful.  He's been wrong about most stuff he's talking about in every thread I've ever seen.  His posts turned up in a couple threads I googled about x264 GPU encoding.  Apparently he doesn't understand why it isn't easy, and has posted several times to tell the x264 developers why they're dumb...</p>
","14657"
"How to force Adobe Media Encoder to use 100% CPU and memory?","31859","","<p>On my machine, Adobe Media Encoder is very slow. It takes 12 hours to render 4 minutes of 1080p video for YouTube (the video is based on Adobe After Effects project with an original MOV file and four effects: curves, remove grain, unsharp mask and hue/saturation.</p>

<p><img src=""https://i.stack.imgur.com/Y0CVN.png"" alt=""enter image description here""></p>

<p>I noticed that it doesn't use all the memory available, nor does it try to use as much CPU as possible. Here's the actual usage during the encoding process:</p>

<p><img src=""https://i.stack.imgur.com/Zac98.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/lEpr2.png"" alt=""enter image description here""></p>

<p>Is there a way to speed up the encoding process by convincing the application to use all CPU and all memory available?</p>

<p><s>As a side note, is it expected to be <em>that</em> slow? I can't possibly imagine movie making companies using it to encode a movie which usually is slightly longer than 4 minutes and has slightly more than four basic effects, even with a whole data center.</s> Removing ""remove grain"" effect speeds up the encoding from 12 hours to 3:30.</p>
","<p>There might not be a way.  Based on your description of the problem, it sounds like the processing is the slow part.  While the video encoder itself is able to do multi-threaded processing, the image processing you are doing may not be able to.  By default, Premiere has always done as much parallel processing as possible for me and I frequently see it hit 99% CPU usage on my hyper-threaded quad core desktop.</p>

<p>The problem is most likely that some of your effects require the result of the previous frame to begin processing on the next frame.  If this is the case, there isn't an easy way to break it down across multiple threads and that would make it impossible to hit 100% CPU usage on a multi-core computer.</p>

<p>Similarly, RAM wouldn't be expected to cap on encoding as it is a stream operation.  There are some things that may need to be tracked over time, but for the most part, it is a stream operation with data going out as fast as it comes in, so there isn't much accumulation of data.</p>

<p>There are two main things that can improve the performance.  The first is GPU processing.  General purpose CPUs aren't really ideal for many audio/video processing tasks.  They are designed for doing a wide variety of general purpose operations, but aren't super efficient at basic operations.  GPUs, on the other hand, are designed for doing simple operations very quickly.  They can often reduce video processing tasks quite a bit.</p>

<p>A further improvement can be found in purpose built hardware.  There are professional cards designed specifically for processing video and provide real time processing and encoding of video.  These are often included in high end professional video editing workstations, but the price can be quite high.  It isn't unheard of for a professional video editing workstation to be able to get up in the $15,000 to $20,000 range or even higher, just for hardware.  They can go for a lot less if you don't mind waiting for renders (I work on a $2,350 system for my video work) but they can also get very high.</p>
","12860"
"Encode settings for HTML5 background video","31682","","<p>I want to create a website with a HTML5 video background. </p>

<p>I have a short 1080p mp4 clip that I want to use (20s). I intend to run the video in a letterbox format (5:2 ish), full screen behind the page content (for an example see Paypal's website: <a href=""https://www.paypal.com/uk/webapps/mpp/home"">https://www.paypal.com/uk/webapps/mpp/home</a>)</p>

<p>What would the best export settings in Premiere CC for this purpose?</p>
","<p>You should export multiple versions to accommodate for different browsers. With HTML5, you can offer different versions of the video and the browser will automatically pick the first one that's supported. For example, take a look at the source code of the paypal website you refered to:</p>

<pre><code>&lt;video autoplay=""autoplay"" muted=""muted"" poster=""none""&gt;
    &lt;source src=""https://www.paypalobjects.com/webstatic/mktg/wright/videos/send-money.mp4"" type=""video/mp4""&gt;
    &lt;source src=""https://www.paypalobjects.com/webstatic/mktg/wright/videos/send-money.webm"" type=""video/webm""&gt;
&lt;/video&gt;
</code></pre>

<p>As you can see, they offer both a mp4 (H264 codec) and a webm version of the video. Those would be good options for you as well. </p>

<p>Furthermore, you should try to keep the videos as small as possible, so that site visitors with slow internet will also be able to watch it. For that purpose, you should export the video with 720p instead of 1080p (nobody will see the difference, especially if the video is in the background) and aim for a bitrate below 1 Mbit/s. For example, the paypal video uses a bitrate of about 700 Kbit/s. If you don't need the audio, you could export without audio or with a low audio bitrate, i.e. 96 Kbit/s.</p>

<p>You can also save the video from the website to your computer and check the codec, bitrate and other parameters e.g. with <a href=""http://mediaarea.net/en/MediaInfo"" rel=""noreferrer"">MediaInfo</a> or similar programs.</p>
","14730"
"How to record HDMI video output?","31415","","<p>How to record HDMI video, which is being sent from a notebook to a projector without interrupting it?</p>

<p>The HDMI signal shows desktop activity and/or video game gameplay on different devices that run Windows, Mac OS X or Linux.</p>

<ol>
<li>Is it possible to use an HD camcorder with HDMI in and out (example device: Sony HDR PJ620)?</li>
<li>Are there devices with built-in storage or that can write to SATA, USB or SD storage?</li>
<li>Are there devices which can be connected to a second laptop which then records the video?</li>
</ol>

<p><em>Hardware/Software solutions that run on/in the computer which outputs the HDMI signal are not relevant. The video-outputting computers can not be manipulated.</em></p>
","<p>I would use three pieces of hardware for this task.</p>

<ol>
<li>A recording laptop.</li>
<li>A device that allows you to input the HDMI to the recording laptop, aka, a frame grabber.</li>
<li>An HDMI spliter.</li>
</ol>

<h2>A recording laptop</h2>

<p>Any old laptop will do, really. It doesn't need super specs, like dedicated graphics, but you certainly want at least a mid-range machine. Like an Intel i5 or better, 8 Gig's RAM, fast HDD (SSD would be great, but not necessary). Software wise, anything that can see the generic devices will work. Pro software like Vegas Pro or Adobe Primere if you have it works great. There's also Wirecast and vMix if you have webcast requirements.</p>

<h2>A video frame grabber</h2>

<p>I've had pretty good success with the <a href=""http://www.epiphan.com/products/dvi2usb-3-0/"" rel=""nofollow noreferrer"">DVI2USB3.0 Frame Grabber by Epiphan</a>. Use that with a <a href=""https://i.stack.imgur.com/zvv63.jpg"" rel=""nofollow noreferrer"">DVI to HDMI converter</a> and it will work just fine. It also allows audio capture. I especially like its very small size, solid construction, and that it's USB powered. It will work on USB 2.0, but you will get limited bandwidth in the form of limited framerates per the source resolution (I often get about 45 fps at 1024x768 on USB 2.0). It's a bit on the pricey side, but Epiphan has very good customer service. I know other options are available, but I have no experience with them. About 5 years ago I decided against Black Magic products because of the mixed reviews on Amazon, but that may have gotten better. Other items that are like $10,000 are not the kind of equipment you are looking for. I've never actually seen one of those being used for more than what a $1,000 in hardware can do. Basically, if you don't know why it costs so much, you don't need it.</p>

<h2>HDMI video splitter</h2>

<p>There are plenty available on the market (<a href=""https://www.google.com/search?q=hdmi%20video%20splitter&amp;espv=2&amp;biw=1280&amp;bih=923&amp;tbm=isch&amp;tbo=u&amp;source=univ&amp;sa=X&amp;ved=0ahUKEwikqYOxwLXMAhUQ12MKHW46C_8QsAQIWQ"" rel=""nofollow noreferrer"">Google image search</a>). They're pretty simple. They have a single HDMI input and at least two HDMI outputs, which are both exact duplicates of the input signal. Make sure you get one that is powered because the ones that aren't reduce signal strength and it often shows, especially on projectors (at least, this is true for the VGA splitters). If you intend to audio record over HDMI make sure the splitter you get can handle that.</p>

<h2>Setup</h2>

<ol>
<li>Take your source HDMI and run it to the video splitter.</li>
<li>Run an HDMI from an output on the splitter to the projector.</li>
<li>Run an HDMI from an output on the splitter to the Epiphan frame grabber via a DVI to HDMI converter.</li>
<li>Plug the Epiphan USB into the recording laptop.</li>
</ol>

<h1>Alternative, VGA-based setup</h1>

<p>I would actually recommend this setup if this is in a professional conference setting and there is a professional audio/visual crew working it. I also recommend this setup if any of the HDMI runs are farther than 30 feet. HDMI just can't push very far and VGA can easily do 100 feet.</p>

<p>If this is a professional setup and you're just hooking in to what they already have, then the projector is not yours and you don't know what to expect. Chances are very high that projector will be gathering it's feed via VGA, not HDMI (you really should call the AV company to find out though).</p>

<p>So instead of using HDMI from the source laptop, use the VGA out (if it has one) or convert it to VGA with an <a href=""https://i.stack.imgur.com/5OxEt.jpg"" rel=""nofollow noreferrer"">HDMI to VGA dongle</a>. The setup is exactly the same as above except everything HDMI is VGA now, including the <a href=""https://i.stack.imgur.com/zsT1F.jpg"" rel=""nofollow noreferrer"">VGA video splitter</a> and the <a href=""https://i.stack.imgur.com/b0GTT.jpg"" rel=""nofollow noreferrer"">DVI to VGA adapter</a>. For audio, just plug into the recording laptop's mic input port. As a warning, you might need some kind of audio cleanup when gathering audio this way. I regularly use the <a href=""https://i.stack.imgur.com/Yf2OL.jpg"" rel=""nofollow noreferrer"">Whirlwind line isolator</a> and the <a href=""https://i.stack.imgur.com/cRJy3.jpg"" rel=""nofollow noreferrer"">Dsan laptop sound port</a>.</p>

<h1>Source</h1>

<p>I work for a company that does this all the time. If you need professional help with content production <a href=""https://video.stackexchange.com/users/3643/fredsbend?tab=profile"">you can find information in my profile</a>.</p>
","18281"
"No audio during footage preview","30223","","<p>I'm using AfterEffects CS6 on Windows 7. During footage preview/playback, I never get any audio. I have the <code>Mute Audio</code> option in the <code>Preview</code> panel turned off (audio is not muted). I checked the <code>Edit &gt; Preferences &gt; Audio Hardware</code> settings and found that 1) the only device listed in the <code>Default Device</code> list is ""After Effects WDM Sound"", and 2) clicking the <code>Settings</code> button and checking all of my audio devices (with and without the <code>Device 32-bit Playback</code> option selected) does nothing.</p>

<p>Am I missing something here? I have to preview my clips in Windows Media Player and then go back to AfterEffects to do the trimming - it's annoying!</p>

<p><img src=""https://i.stack.imgur.com/upDRi.png"" alt=""AfterEffects audio settings.""></p>
","<p>After Effects is not designed as an editing platform.  Audio preview only works when you do a RAM Preview and pre-render all the frames for the content you are previewing.  You never get Audio with the real-time preview.</p>

<p>The intent behind RAM Preview is that it lets you check the final quality of the render prior to writing it to disk, but it isn't really designed to be used constantly while doing effects work in most cases.</p>
","10294"
"How do I write up a shot list?","30193","","<p>I am writing a script for a short movie. How do I write a shooting script? Is it needed? What is the difference between a script and a shooting script?</p>
","<p>Shot lists are fairly easy to write. You can draw up your own grid or download a <a href=""http://www.anchorboltstudios.com/2010/02/how-to-make-a-shotlist"" rel=""noreferrer"">template</a> online. The simplest way is to have four columns: shot number, shot type, action, notes. However you can customise a shot list to include whatever you think is necessary to illustrate your vision. </p>

<p>Here is a template I use for simple shoots:</p>

<p><img src=""https://i.stack.imgur.com/blirK.png"" alt=""example_shot_list"">
Notice that they are written in an order that would be logical to shoot on the day. They are not listed in order of how they will play out in the story.</p>

<p>Hope that helps :)</p>
","2045"
"Looking for an 80's VHS effect!","29970","","<p>I'm looking for a way to turn an HD video clip into 80's style VHS low-quality.
Here is an example of the look I'm desperately looking for:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/asSgIrIcMZc?start=0""></iframe>
            </div></div></p>

<p>Thank you!!!</p>
","<p>Every decent video editor has built-in filters (like Color Correction, Film Effects, TV simulator, Color Curves, etc.) that can be used to achieve the desired 'look'. There are plug-ins and tools that contain many ready-to-use presets, such as:</p>

<ul>
<li><a href=""http://www.redgiantsoftware.com/products/all/magic-bullet-looks/"" rel=""nofollow"">Magic Bullet Looks</a></li>
<li><a href=""http://vegasaur.com/film-looks"" rel=""nofollow"">Film Looks SVP</a></li>
<li><a href=""http://www.adobe.com/products/speedgrade.html"" rel=""nofollow"">Adobe SpeedGrade</a></li>
<li><a href=""http://www.blackmagicdesign.com/products/davinciresolve/"" rel=""nofollow"">DaVinci Resolve</a>
etc.</li>
</ul>
","5729"
"Does canon EOS 1000D support video recording?","29860","","<p>I want to buy an entry level DSLR and EOS 1000D seems like a good buy but I have some doubts regarding it. Does it support video recording?</p>

<p>Do I need to install some software for the video recording to work? If yes, does that record the videos with sound? If no, how can I record video with sound on the Canon EOS 1000D?</p>
","<p>It cannot officially record video, but there is an <a href=""http://sourceforge.net/projects/eos-movrec"" rel=""nofollow"">open source project</a> to allow recording direct to a computer.</p>

<p><a href=""http://en.wikipedia.org/wiki/Canon_EOS_1000D"" rel=""nofollow"">Source</a></p>

<p>Given that it doesn't support video natively then there won't be a built in microphone to record sound. If you went with the open source software you'd have to record the sound separately and sync it afterwards - which is not a straightforward process.</p>
","2985"
"What are the key PC specs for best editing HD videos?","29746","","<p>My wife bought a GoPro Hero3 for some simple home video: kids, holidays, etc.</p>

<p>The problem is that some videos are not playing very well on our old PCs.  We want to buy a new one for video editing. As far as I understood, such a computer should have as strong of a CPU as possible and plenty of RAM. It seems that the video card also has some role, but it's not as important as CPU/RAM in general. Is this assumption correct?</p>
","<p>Dealing with HD video has two main components.  The first is the ability to decode the highly compressed video (which is computationally intensive) and then is the ability to deal with the large uncompressed data (which is bandwidth intensive).  </p>

<p>The fastest encoders/decoders use the GPU on the graphics card to improve performance as a GPU is far more capable at attacking the problems related to rendering and compression than a traditional CPU is.  So if you have one of these, then the GPU is by far the most important piece.  If you don't however, then the CPU becomes by far the most important piece as encoding/decoding on a CPU is very, very intensive.</p>

<p>As far as dealing with the uncompressed data, the relatively small compressed files will be decompressed into memory and then played out of memory, so it is important to have high speed RAM, but not necessarily the need for a lot of it for play back.  For editing on the other hand, quantity becomes a bigger factor since the more video that can be stored in memory, the smoother your editing experience will be.</p>

<p>Finally, you have the hard drives.  It is very helpful if these are quick for loading videos (particularly for editing) and also for working as scratch disk (where editing stores temporary files that don't fit in memory), however it is probably the least critical part.  This can change though if you start working with higher quality, less compressed video where the file sizes start getting larger.</p>

<p>So, to summarize, GPU is highest priority if your software can use it, otherwise CPU and then speed of memory, quantity of memory and speed of hard drive in that order.  CPU is still second most important if you do have GPU processing support.  If you don't, then GPU is of almost no importance at all.</p>
","10400"
"How much GB or TB is 1 hour of raw uncompressed Ultra HD film?","29001","","<p>I was looking on wikipedia but couldn't find any definitive answer. </p>

<p>I think it's because it depends on how many bits is the color pallete and frames per second.</p>

<p>Please, could somebody help me make a rough estimate <strong>how much GB or TB</strong> would be a standard <strong>24fps? or 30fps?</strong> (I am not sure which is the standard frame rate for UHD movies) <strong>Ultra HD movie (3840x2160)</strong> with the length of <strong>1 hour</strong>. </p>

<p>I want to know the uncompressed raw size of the movie file after exporting from Adobe Premiere</p>

<p>I am not sure what is the standard 8 or 12 bit or some other? So, I will be thankful for your advice.</p>
","<p>There are some storage calculators: <a href=""http://www.aja.com/en/products/software/"" rel=""nofollow"">AJA DataCalc</a>, <a href=""http://www.digitalrebellion.com/webapps/video_calc.html"" rel=""nofollow"">Video Space Calculator</a>. Nothing complex, but keep also in mind that actual required storage size will be more than raw disk size due formatting, RAID levels etc. </p>
","5829"
"How can I use FFmpeg to lower the quality of H264 video but keep it in H264 format?","28627","","<p>I'm a complete noob to video editing, so forgive me if I ask for something ridiculous.</p>

<p>I'm getting video clips from a set top recorder box.  The clips are AVI files:</p>

<pre><code>Resolution: 1920 x 1080
Frames Per Second: 60
Video Format: ITU H.264
Audio Format: MPEG 1 Audio, Layer 3 (MP3)
</code></pre>

<p>The resolution and FPS are well beyond what I require, so I'm trying to run a command to scale things down:</p>

<pre><code>ffmpeg -i highres.avi -r 30 -s 960x540 lowerres.avi
</code></pre>

<p>This is successful in giving me the desired resolution, framerate and lower file size, but it also degraded the quality way more than I expected.</p>

<p>I looked at the new file, and found that for some reason FFmpeg converted the format used:</p>

<pre><code>Video Format: FFmpeg MPEG-4
Audio Format: MPEG 1 Audio, Layer 2
</code></pre>

<p>It's unclear to me why it wouldn't use the same output formats as input formats.  I tried to use this command to force it to use H.264:</p>

<pre><code>ffmpeg -i highres.avi -r 30 -s 960x540 -f h264 lowerresforceh264.avi
</code></pre>

<p>Unfortunately, it failed with the following output:</p>

<pre><code>ffmpeg version 0.7.3-4:0.7.3-0ubuntu0.11.10.1, Copyright (c) 2000-2011 the Libav developers
  built on Jan  4 2012 16:21:50 with gcc 4.6.1
  configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --enable-shared --disable-static
  WARNING: library configuration mismatch
  avutil      configuration: --extra-version='4:0.7.3ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-libopenjpeg --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdirac --enable-libmp3lame --enable-librtmp --enable-libx264 --enable-libxvid --enable-libvo-aacenc --enable-version3 --enable-libvo-amrwbenc --enable-version3 --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  avcodec     configuration: --extra-version='4:0.7.3ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-libopenjpeg --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdirac --enable-libmp3lame --enable-librtmp --enable-libx264 --enable-libxvid --enable-libvo-aacenc --enable-version3 --enable-libvo-amrwbenc --enable-version3 --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  avformat    configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  avdevice    configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  avfilter    configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  swscale     configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  postproc    configuration: --extra-version='4:0.7.3-0ubuntu0.11.10.1' --arch=i386 --prefix=/usr --enable-vdpau --enable-bzlib --enable-libgsm --enable-libschroedinger --enable-libspeex --enable-libtheora --enable-libvorbis --enable-pthreads --enable-zlib --enable-libvpx --enable-runtime-cpudetect --enable-vaapi --enable-gpl --enable-postproc --enable-swscale --enable-x11grab --enable-libdc1394 --shlibdir=/usr/lib/i686/cmov --cpu=i686 --enable-shared --disable-static --disable-ffmpeg --disable-ffplay
  libavutil    51.  7. 0 / 51.  7. 0
  libavcodec   53.  6. 0 / 53.  6. 0
  libavformat  53.  3. 0 / 53.  3. 0
  libavdevice  53.  0. 0 / 53.  0. 0
  libavfilter   2.  4. 0 /  2.  4. 0
  libswscale    2.  0. 0 /  2.  0. 0
  libpostproc  52.  0. 0 / 52.  0. 0
Input #0, avi, from 'highres.avi':
  Metadata:
    comment         : Quality_GOOD
    encoder         : Lavf52.64.2
    encoded_by      : AverMedia_c281_1.7.2
  Duration: 00:01:59.28, start: 0.000000, bitrate: 10263 kb/s
    Stream #0.0: Video: h264 (High), yuv420p, 1920x1080, 59.94 fps, 59.94 tbr, 59.94 tbn, 119.88 tbc
    Stream #0.1: Audio: mp3, 48000 Hz, stereo, s16, 128 kb/s
[buffer @ 0x8f438a0] w:1920 h:1080 pixfmt:yuv420p
[scale @ 0x8f431e0] w:1920 h:1080 fmt:yuv420p -&gt; w:960 h:540 fmt:yuv420p flags:0x4
[libx264 @ 0x8f0fb40] broken ffmpeg default settings detected
[libx264 @ 0x8f0fb40] use an encoding preset (e.g. -vpre medium)
[libx264 @ 0x8f0fb40] preset usage: -vpre &lt;speed&gt; -vpre &lt;profile&gt;
[libx264 @ 0x8f0fb40] speed presets are listed in x264 --help
[libx264 @ 0x8f0fb40] profile is optional; x264 defaults to high
Output #0, h264, to 'lowerresforceh264.avi':
    Stream #0.0: Video: libx264, yuv420p, 960x540, q=2-31, 200 kb/s, 90k tbn, 30 tbc
Stream mapping:
  Stream #0.0 -&gt; #0.0
Error while opening encoder for output stream #0.0 - maybe incorrect parameters such as bit_rate, rate, width or height
</code></pre>

<p>I'm not sure why I can't do this.  Running ""ffmpeg -formats"" listed h264 as one it could use for both input and output.</p>

<p>If anyone could point me in the right direction, I'd appreciate it greatly.  Thanks.</p>

<p>PS: I'm using FFmpeg version 0.7.3-4:0.7.3-0 on Ubuntu 11.10.</p>

<p>EDIT</p>

<p>I've found a solution looking at the use of presets.  After looking for information about presets, I found the following tutorial discussing them:</p>

<p><a href=""http://juliensimon.blogspot.com/2009/01/howto-ffmpeg-x264-presets.html"">http://juliensimon.blogspot.com/2009/01/howto-ffmpeg-x264-presets.html</a></p>

<p>I just needed to specify the codec and the proper preset setting like so:</p>

<pre><code>ffmpeg -i highres.avi -r 30 -s 960x540 -vcodec libx264 -vpre medium lowerresforceh264vpremedium.avi
</code></pre>

<p>I still have some tweaking to do, but this is basically what I wanted.  Thanks.</p>
","<p>I'm trying to help. First, I don't understand why you're saying FFMPEG H.264 is not the input format? </p>

<p>I don't know the FFMPEG command line well enough (is there anyone who does? ;)  ). There's got to be a switch for multipass encoding. I know there are some built-in presets you could probably use and do almost certainly what you're aiming for. These are normally called x264 presets or x264 tune. Can you find anything like that?</p>
","3877"
"How do I record a computer monitor without banding/flickering with a Canon t2i/550D?","27507","","<p>As you've probably guessed, when recording video with a computer monitor in frame I am getting banding. I'm aware that this has to do with refresh rates of the monitor vs the scan rate of the camera or something like that. I've read that most cameras have a feature to adjust for this - synchro scan, clear scan or variable scan is what I've seen it referred to as. However, I can't find anything on what or how to do this with my Canon t2i.</p>

<p>I've seen a number of videos where people do this with a t2i, so I'm sure it's possible, I'm just not sure how to set the camera up properly.</p>

<p><a href=""http://www.youtube.com/watch?v=tR61M54rBNk"" rel=""nofollow"">Here is a video using a t2i to record a computer screen without banding.</a></p>

<p>I've noticed that I can adjust f/stop and shutter speed to get varying results when put in to Tv mode. I'm having trouble getting to a setting that has minimal/no banding and isn't either under or over exposed.</p>
","<p>While reviewing the Instruction Manual for the Canon 2Ti EOS 550 on page 127 you will find information on how to manually set the shutter time. Here is the pdf download for the manual:
<a href=""http://gdlp01.c-wss.com/gds/9/0300003169/01/eosrt2i-eos550d-im-en.pdf"" rel=""nofollow"">http://gdlp01.c-wss.com/gds/9/0300003169/01/eosrt2i-eos550d-im-en.pdf</a></p>

<p>There is wide range of 1/4000 to 1/30 of a second to work with depending on your frame rate.</p>

<p>For 50 fps or 60 fps the range is 1/4000 to 1/60.</p>

<p>For 24, 25, &amp; 30 fps the range is 1/4000 to 1/30.</p>

<p>I suggest that you try different shutter times at your preferred frame rate. I know this is not definitive, as I don't have this camera, nor have I tested this. However, the flicker you are referring to is as you suggest a synchronization issue; you are trying to have the camera capture at a rate that synchronizes with the display. Another thing you can try is change the display frame rate, if you have this option on your monitor.</p>

<p>Hope this helps,</p>
","3095"
"What exactly does ""Ethernet-capable HDMI cable"" mean?","25176","","<p>What exactly does ""Ethernet-capable HDMI cable"" mean?</p>

<p>Does this mean I can stream content wirelessly to my projector?</p>
","<p>No it does not mean you can do anything wirelessly - it is a cable - and it isn't likely to help you stream anything to it either, unless you have a requirement for HDMI devices to share a network connection.</p>

<p>Version 1.4 of the HDMI standard allows for 100Mb ethernet to be transmitted along the cable between to HDMI 1.4 capable devices.</p>

<p>Check out the <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/HDMI"">HDMI wikipedia</a> page for some basic info</p>
","2456"
"How much recording time at 1080p30 on a 32GB card on a GoPro Hero (1)?","24959","","<p>Is there a generic data recording rate for the different capture options on GoPro Heroes? </p>

<p>I'm trying to determine what size of SD card I should buy for my GoPro and I'd like to get an approximation. Is there a grid somewhere that details this?</p>
","<p>Have you try to go to the product web site and check?
<a href=""http://gopro.com/support/articles/recording-time-in-each-video-setting"" rel=""nofollow"">http://gopro.com/support/articles/recording-time-in-each-video-setting</a></p>

<p>I know it is about Hero2, but i think the values will not change much. Be aware fully charged battery will power the camera for no more than 3 hours</p>
","13302"
"encoding 4:2:2 in 10-bit with libx264","24935","","<p>I believe that libx264 is now capable of doing 10-bit 4:2:2 encodings, but I can't seem to get it to work. I'm using ffmpeg (info below), and I've also tried the x264 encoder directly. I've tried </p>

<pre><code>ffmpeg.exe -i input.mov -c:v libx264 -profile:v high422 -crf 20 -pix_fmt yuv422p output.mp4
</code></pre>

<p>and that produces nice 4:2:2 output, but only at 8 bit depth,</p>

<pre><code>[libx264 @ 00000000055a9de0] profile High 4:2:2, level 4.0, 4:2:2 8-bit
</code></pre>

<p>and I've tried</p>

<pre><code>ffmpeg.exe -i input.mov -c:v libx264 -profile:v high10 -crf 20 -pix_fmt yuv422p output.mp4
</code></pre>

<p>and that gives me the error:</p>

<pre><code>x264 [error]: high10 profile doesn't support 4:2:2
[libx264 @ 00000000051ead60] Error setting profile high10.
[libx264 @ 00000000051ead60] Possible profiles: baseline main high high10 high422 high444
</code></pre>

<p>In the x264 --fullhelp documentation I find:</p>

<pre><code>  --profile &lt;string&gt;      Force the limits of an H.264 profile
                              Overrides all settings.
                              [...]
                              - high10:
                                No lossless.
                                Support for bit depth 8-10.
                              - high422:
                                No lossless.
                                Support for bit depth 8-10.
                                Support for 4:2:0/4:2:2 chroma subsampling.
                              - high444:
                                Support for bit depth 8-10.
                                Support for 4:2:0/4:2:2/4:4:4 chroma subsampling.
</code></pre>

<p>So it can do 4:2:2 at 10 bit depth, and even 4:4:4 at 10 bits apparently, but there's no indication of how to set the output bit depth. There is an option <code>--input-depth &lt;integer&gt; Specify input bit depth for raw input</code> but nothing for output bit depth.</p>
","<p>x264 must be compiled with <code>--bit-depth=10</code>. Note that the resulting build will not be able to output 8-bit depth. Of course you can have separate builds, but whatever build <code>ffmpeg</code> is linked to will determine what bit depth it can output.</p>
","13166"
"Why is Audio and Video out of Sync during recording?","24685","","<p><em>I make a 30 second recording using a webcam on a laptop. When I play the .avi file afterwards, the audio and video are out of step. The sound starts arriving about a second after it should do and gets increasingly delayed.</em></p>

<p><strong>Why is this?</strong></p>

<p>Whilst I am making the recording, I can see the video that is being captured being displayed on the laptop monitor. I can actually get lip synch if I  s-p-e-a-k  l-i-k-e  t-h-i-s. </p>

<p>I clap my hands at the start of the video. When I watch the video afterwards, the sounId is delayed by about a second.</p>

<p>The audio track continues for about ten or so seconds after the video component has finished. (A still image remains in the video player while the voice continues.)</p>

<p>I can fix this problem after the recording (laboriously), but I want to have the recording work properly in the first place.</p>

<p>The webcam is a BisonCam NB Pro integrated into the laptop. It is a 2 Megapixel camera that can record at 15 fps. The CPU is an Intel dual core T4200. The machine has 4GB of RAM. I am saving the recording to a file on a ramdisk, so I don't think the hard drive is the problem. I am using BisonCap to record, the software that came with the computer. I have set recording for 15 fps.</p>

<p>Please help!</p>
","<p>You should check your recorded video with <a href=""http://mediainfo.sourceforge.net/en"" rel=""nofollow"">MediaInfo</a>. Regarding your problem, it shows,</p>

<ul>
<li>if the durations of the video and the audio is the same or not,</li>
<li>if the interleave of the audio (the starting offset) is set correctly</li>
<li>in which sampling rate your audio was recorded.</li>
</ul>

<p>Then, I would import your video file into <a href=""http://audacity.sourceforge.net/"" rel=""nofollow"">Audacity</a> (which imports only the audio, of course). </p>

<ul>
<li>On the left side of each track you find the sampling rate, too. Is this the same sampling rate as being reported by MediaInfo?</li>
<li>Does Audacity play the audio too slow, too?</li>
</ul>

<p>My impression from the symptoms that you describe is that the audio is recorded at a certain sampling rate, but reported as a lower sampling rate, or not at all.</p>

<p>Try to modify your sampling rate, either in your recordings settings, or in the Windows system. On Win7, it's under <em>Control Panel->Hardware &amp; Sound->Manage Audio Devices->Recording</em>. There, select your microphone, then <em>Properties->Advanced</em>. There you can change the sampling rate.</p>

<p>If changing the sampling rate doesn't help, then you can only try another webcam software, or record the audio independently.</p>
","7072"
"How to use ffprobe to obtain certain information about mp4/h.264 files","23986","","<p>I am but a lowly PHP developer that has been tasked with retrieving some basic information about 100's of thousands of video files.  I've had some luck using ffmpeg extracting the <code>video bitrate</code>, <code>video width</code>, <code>video height</code>, <code>duration</code>, and <code>aspect ratio</code> from them, but with an error ratio of about 5%, this still leaves me with an enormous number of files that I don't know how to deal with.  The videos DO play,
but the massive amount of data that ffmpeg returns in has me baffled as to how parse the returned XML to find what I need with all the conditions that I seem to be encountering.  (The files are a collection from the past 15 years or so, some newly encoded, others converted years ago)</p>

<p>At any rate, I found the following use of <code>ffprobe</code> to get duration, and was hoping that someone here who understands the complexities of using ffmpeg &amp; ffprobe would be kind enough in assist me with finding the other values I need in a more straightforward way than the crazy code I'm finding myself creating to climb through the full results of ffmpeg.</p>

<p><code>ffprobe -i ""video.mp4"" -show_entries format=duration -v quiet -of csv=""p=0""</code></p>

<p>Thanks so very much in advance.</p>

<p><strong>CLARIFICATION:</strong></p>

<p>I should add that with the below cmd, I can ""see"" what I need, but I don't understand how to extract what I need, e.g. retrieving the VIDEO bitrate, and suppressing/ignoring the AUDIO bitrate, or determining which of 2 different durations to use (Audio/Video durations?)</p>

<p><code>C:\&gt;ffprobe -v error -show_entries stream=width,height,bit_rate,duration -of default=noprint_wrappers=1 input.mp4</code></p>

<pre><code>duration=1712.000000
bit_rate=64000
width=320
height=240
duration=1711.946113
bit_rate=359827
duration=1712.000000
bit_rate=N/A
duration=1712.000000
bit_rate=N/A
</code></pre>
","<h1><code>-select_streams</code></h1>

<p>If you only want the information from the first video stream, then add this to your command:</p>

<pre><code>-select_streams v:0
</code></pre>

<h1>duration</h1>

<p>Note that duration may be missing or incorrect; especially with damaged files. You can always compare it with the output of:</p>

<pre><code>ffmpeg -i input -f null -
</code></pre>

<p>Then look at <code>time=</code> in the second to last line. This will decode the whole input, so it may take some time depending on the input duration and decoding complexity.</p>

<h1>XML</h1>

<p>You mentioned XML output, but your commands are providing default or CSV. <code>ffprobe</code> does support XML output, and others, if you prefer that. See <a href=""https://ffmpeg.org/ffprobe.html#Writers"">FFprobe Documentation: Writers</a>.</p>
","16359"
"Easiest way to stitch GoPro videos that have been split due to length?","23975","","<p>I am recording 15-minutes videos with my GoPro Hero 3 Black edition. The camera starts a new file every 10 minutes of recording by default. I do not think I can change this in a setting.</p>

<p>When I do the white-balance and other edits in the GoPro Cineform Studio, I cannot find a function to stitch these videos back together into one continuous clip. </p>

<p>I was somehow expecting that if GoPro automatically splits these videos that there would be a function to rejoin them, too. Am I missing something? Is there such a function in this software or do I have to get another software to do this? </p>

<p>If so, what software should I get, assuming that I do not want to do anything else than that?</p>
","<p>The latest version of Cineform can join clips together as I found out recently, so the problem solved itself.</p>
","9508"
"How can I extend the ten minute video limit on a Canon PowerShot ELPH 300 HS?","22333","","<p><a href=""https://photo.stackexchange.com/questions/14563/why-is-there-a-limit-restriction-to-the-1080p-film-video-recording-time-duration"">This question</a> addresses the fact that DSLRs have limits on video recording time and provides some speculation as to why the limit exists. It mentions that Canon has a 30 minute limit.</p>

<p>I am recording HD video on a Canon PowerShot ELPH 300 HS point-and-shoot, and am coming up against a 10 minute limit. <a href=""http://www.digitalcamerareview.com/default.asp?newsID=4680&amp;p=4"" rel=""nofollow noreferrer"">This site</a> says that there is a 60 minute limit on standard definition and 10 minute limit on high definition. With 720p a 10 minute video is about 1.5 Gb. I wanted to mention the 10 minute limit explicitly because it seems that many people talk about a 30 minute limit and so this seemed like a somewhat unique case.</p>

<p>My question is whether there is a way to extend the limit from 10 to something like 30 minutes? 10 minutes is just short for my purposes of recording my own judo tournament matches.</p>

<p>I guess it would have to be some kind of patch to the firmware. Does such a thing exist?</p>
","<p>I believe the limit is due to the <em>file size</em>, not the actual recording length.  <a href=""http://forums.dpreview.com/forums/read.asp?forum=1031&amp;message=35289169"" rel=""nofollow"">This forum thread</a> seems to confirm this for the T2i/500D, and <a href=""http://en.wikipedia.org/wiki/Canon_EOS_5D_Mark_II"" rel=""nofollow"">Wikipedia</a> agrees for the 5D mkII.  Also according to these sources, the limit for both cameras is 4gb.</p>

<p>For the PowerShot cameras, the file size limit is likely much lower. For the S2, <a href=""http://forums.dpreview.com/forums/read.asp?forum=1010&amp;message=13834915"" rel=""nofollow"">it appears</a> to be 1gb.</p>

<p>What all this means is that the time limit isn't a time limit, per se... but a file size limit.  So if you are able/willing to reduce the file <em>quality</em>, you may be able to extend the time limit.</p>

<p>If you can shoot in a lower quality video mode (say 480i instead of 720p), that will likely improve your recording time.  If you can reduce the frame rate, that may improve your time limit, as well.  Of course all of these things will make the video noticibly lower quality, as well.</p>

<p>If you can't get different equipment, you may be best off strategically stopping and starting the video during pauses in the action.</p>
","2675"
"How to use mouse drag gesture to position & size a still image in Premiere Pro CS6","21204","","<p>I would like to be able to position a still image using the mouse/pointer directly on the image, to be able to drag the whole image around or resize it by dragging one of its corners or sides.</p>

<p>I'm sure I did this earlier but don't know which part of Adobe Premiere Pro CS6 does it. </p>

<p>Sure, there's the Effect Controls but this seems to only provide for indirectly resizing and positioning the image - i.e. I mouse drag on the numbers themselves and see the effect.</p>
","<p>Simply double click the Sequence Preview and you get handles to resize the footage and you can drag the footage around in the frame.</p>

<p><img src=""https://i.stack.imgur.com/CBTl1.jpg"" alt=""enter image description here""></p>
","12070"
"Seeking free and simple way to brighten a dark video","19361","","<p>The title says it all. My video is too dark. I Learned how to avoid it next time (after the horse had bolted).</p>

<p>Is there anything I can do to lighten it up? Preferably freeware and not to complex. THe video is .AVI format</p>

<p>[update] Windows 7 pro (and I am wondering whether brightness or contrast is the way to go (ans if some magic s/w has an ""auto"" feature))</p>

<p>Thanks a 1,000,000 in advance</p>
","<p>I've used <a href=""http://www.virtualdub.org/"" rel=""nofollow"">VirtualDub</a> (one of the open source tools) with some success for just this.</p>

<p>It's not trivial to use, and I'm afraid I don't remember the exact settings, etc., but there are plenty of tutorials/explanations around the web.</p>
","4688"
"What is the cheapest camera I can plug external microphone to?","19345","","<p>I am working on some homemade lectures so I need good quality of sound, but unfortunately I do not have any budget so far(amateur stuff). </p>

<p>I am looking for a camera that can shoot in Full HD and can have an external microphone attached. I do not need anything further than that, though.</p>

<p>I was considering <code>SONY HDR-CX240</code> until I found out it cannot have external mic.
(What if it is a USB mic? It does have a USB slot.) So, should I quit this choice and buy a more expensive camera? Which one whould you recommend?</p>

<p>Is it possible that I record with the camera and external mic attached to my laptop at the same time?</p>
","<p>Do you own a high-end smartphone? You can plug-in pretty much any microphone and have very decent audio quality. Some Android phones even support usb mics.
So you probably need a cheap adapter for standard stereo or mono mics to work with the 3 channel type headphone+mic jacks in modern phones. Something like this: <a href=""http://rads.stackoverflow.com/amzn/click/B004SP0WAQ"" rel=""nofollow noreferrer"">https://www.amazon.com/StarTech-com-headsets-separate-headphone-microphone/dp/B004SP0WAQ/</a></p>

<p>It then only depends on the video quality of your phone. If that is satisfactory just get some sort of tripod adapter or other type of stand for phones and you're good to go. Definitely the cheapest solution if you already own a fitting phone.</p>

<p>If your phone is not an option I recommend you go with a webcam and USB microphone attached to your PC or Laptop. Most recording softwares have the option to choose a different audio source than your video recording device.
You can get a decent quality webcam for 20-30 and a decent quality USB mic that can capture room audio for 15-20.</p>
","12288"
"How do you zoom in (animated) on a portion of your canvas?","18777","","<p>I'm using After Effects to create a Motion graphic and I would like to make the animation zoom in on a specific point, do something and then zoom out again. </p>

<p>To clarify what I exactly try to achieve: I'm animating a browser and I would like to zoom in on the URL bar, type an URL and then zoom out and work on just like before zooming. </p>

<p>One option is to scale all the visible layers, so it looks zoomed, but that's a strange way to achieve the zooming effect, I think.</p>

<p>What's your advice?</p>
","<p>Here's what I would do...</p>

<ul>
<li>Create a camera</li>
<li>Create a null object</li>
<li>Set the parent of the null object to be the camera</li>
<li>Make the null object a '3D' object</li>
<li>Use the null object's properties to zoom and move the camera around, e.g. use the 'z' position to zoom</li>
</ul>

<p>You could do this with the camera by itself, but the null object is sometimes easier to deal with. Ease in and out of the keyframes for a smooth animation. I hope this helps!</p>
","15350"
"Why do my Objects show up black in Cinema-4D?","18545","","<p>I am new to 3D software and have just started with Cinema 4d. This is a very noobish question but when I create any object in a project it always shows up pitch black.</p>

<p>The tutorial videos I have seen create objects where the faces can be see in a shaded grey. What am I doing wrong?</p>

<p>Image relating to the problem:
<a href=""http://i48.tinypic.com/2e1d5bk.jpg"" rel=""nofollow"">http://i48.tinypic.com/2e1d5bk.jpg</a></p>

<p>Thanks,
Hiren</p>
","<p>This can either be an issue with OpenGL and you may want to disable it in the program settings (under ""File) to test if thats the issue. Or you may have disabled the standard infinite light that is present in every new scene until you add your first light. Add an infinite light from the lights menu and see if that gives your objects some shading.
Also try what @Bart Arondson said and look if you have the ""Use Color"" option enabled and set to black. If yes set it to ""off"".</p>
","7183"
"Does Adobe Premiere Pro CS6 have a fullscreen mode?","18379","","<p>Essentially what I'm looking for is a way to make Adobe Premiere Pro CS6 go fullscreen. I do not mean the fullscreen preview of the video or fullscreen on a specific panel (like the timeline).</p>

<p>I want to get rid of the Windows taskbar and the window border. The same way that you would go fullscreen with a browser by pressing F11.</p>

<p>I'm surprised I haven't found any shortcut or any discussion online. I'm working on a 15"" laptop monitor and would like to maximize my workspace. I've tried to press F11, searched for it in the menus and the keyboard shortcut list but haven't found anything.</p>

<p>As clarified in the comments on AJ's answer below, I am looking for something like pushing F in Photoshop or Lightroom, where the workspace is otherwise the same.  For example, this is how it looks in Lightroom.
<img src=""https://i.stack.imgur.com/qgc8V.jpg"" alt=""enter image description here""></p>

<p>Any help would be greatly appreciated!</p>
","<p>I'm fairly certain there is no such mode.  It doesn't have the same kind of feature that lightroom does as the full screen video preview is really the closest in terms of being related functionality.  </p>

<p>The full screen mode is helpful in Lightroom because it allows focusing on the photo with only basic controls.  Premiere is much more complex than LR and a significant portion of screen real-estate is taken up by those controls.  If you aren't purely focusing on the video, then the amount of lost and distracting space really isn't a big issue.</p>

<p>Note that if you really need every last scrap of screen real estate though, you can adjust the task bar so that you can make it only a small line on the bottom of the screen though.  You really are not practically losing any meaningful amount of screen real-estate.</p>

<p>The only functionality is the functionality that maximizes a particular panel or full screens the preview, but you already mentioned those are not what you are looking for.  That's the closest you are going to get though.</p>
","10496"
"Should you shoot flat when shooting with a DSLR?","18183","","<p>I've read a bit about how it can be useful to use a ""flat"" profile for shooting video on a DSLR which produces only compressed video. For example as described in <a href=""http://prolost.com/flat/"" rel=""nofollow noreferrer"">this article</a>. The reasoning is that most lossy compressors (like H.264) will do more compression in the darks, removing that data and making it impossible to adjust in post. (For example, if you brighten it, you'll get bright gray squares instead of whatever data was there before compression.)</p>

<p>The author of the above seems reasonable. He worked at LucasFilm and has directed many music videos and feature film VFX shots. It doesn't seem like he's suggesting some fad, but is talking from experience.</p>

<p>In the answer to [<a href=""https://video.stackexchange.com/questions/10542/post-production-workflow-vfx-before-color-correction](Post-production"">Post-production workflow: VFX before color correction?</a> workflow: VFX before color correction), this was included in the accepted answer:</p>

<blockquote>
  <p>As far as low contrast and low saturation, I would disagree rather strongly with shooting for low contrast/low saturation. This is how you get noisy, low quality color. Your camera has a limited number of colors it can describe, if you ignore a large portion of that color space, then you can't describe as many colors and the resolution of your color after boosting the saturation and contrast will be very greatly reduced.</p>
</blockquote>

<p>These sound contradictory to me, but maybe I'm misunderstanding. Is the advice to shoot flat good advice for DSLRs with lossy compression or not?</p>
","<p>Neither viewpoint is wrong, they rather have different advantages.  It is a question of dynamic range vs compression noise.  My answer on the previous question may have been a little too strongly from my particular camp of thought (I was not aware of the compression related dark color issues at the time as I shoot high bit rate video primarily, which makes it much less of an issue.)</p>

<p>If you are shooting non-raw and using a flat color space, you have fewer values to represent the range of colors in your shot.  You can get more detail on the brightest and darkest spots since they are exposed as mid-range colors, but you lose the depth of color since you have to artificially expand it later in post.  I'm not sure what I was thinking with the noisy bit, probably thinking about the loss of color detail from reducing your color space based on the surrounding explanation, but noise was a poor choice of term for that.  (I have updated accordingly.)</p>

<p>The flip side of that is to get it mostly right in camera.  You have to be careful about your black point and make sure that you actually have sufficient detail in the shadows, but utilizing the full color space allows for far more colors to be described, even if it offers less ability to adjust after the fact.</p>

<p>It is all a question of recovery.  When you deal with RAW, you have detail going all the way down to the darkest and all the way up to the brightest point from which you can work to pull out any details that the camera captured.  When you are dealing with an encoded video though, you are limited to whatever is displayed.  Using flat allows more recovery since you capture beyond the black and white point of the final video (and does reduce sensor noise in the darks since they are actually exposed as mid-tones) but it means much less color detail to work with.</p>

<p>I prefer making full use of my color space and controlling shooting, others prefer to have extra control at the cost of color depth.  Neither is a wrong choice.</p>
","12201"
"How to get FFMPEG to join non-sequential image files? (skip by 3s)","17917","","<p>I have a bunch of images, that follow this pattern.</p>

<pre><code>0001.png
0004.png
0007.png
0010.png
0013.png
0016.png
0019.png
...
</code></pre>

<p>They're in the correct order, but FFMPEG ignores anything after the first file, because it can't find a <code>0002.png</code>.  How can I get it to join these files?</p>

<p>I tried</p>

<pre><code>ffmpeg -i %04d.png out.avi
</code></pre>
","<p>with ffmpeg 0.11.1 it's as easy as:</p>

<pre><code>ffmpeg -f image2 -i %*.png out.avi
</code></pre>

<p>From the <code>man</code> page, in an example under ""Video and Audio file format conversion"":</p>

<blockquote>
  <p>When importing an image sequence, <code>-i</code> also supports expanding shell-like wildcard patterns (globbing) internally. To lower the chance of interfering with your actual file names and the shell's glob expansion, you are required to activate glob meta characters by prefixing them with a single <code>%</code> character, like in <code>foo-%*.jpeg</code>, <code>foo-%?%?%?.jpeg</code> or <code>foo-00%[234%]%*.jpeg</code>.</p>
</blockquote>

<p>Update per comments: on recent versions you should now use <code>-i '*.png'</code> syntax.</p>
","7585"
"How to downsample 4k to 1080p using ffmpeg while maintaining the quality?","17808","","<p>I have some 4K 3840x2160 footage in MP4 format that I need to bring down to 1080p. I tried running </p>

<pre><code>ffmpeg -i orig.mp4 -vf scale=1920:1080 smaller.mp4  
</code></pre>

<p>but the result is very poor quality, with the entire image being composed of square ""tiles"" as if I was magnifying 4:1.</p>

<p>Here is the output of running this command:</p>

<pre><code>Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'origs/P1000003.MP4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp42avc1
    creation_time   : 2015-02-19 17:10:38
  Duration: 00:05:14.48, start: 0.000000, bitrate: 95903 kb/s
    Stream #0.0(und): Video: h264 (High), yuvj420p, 3840x2160 [PAR 1:1 DAR 16:9], 95792 kb/s, 25 fps, 25 tbr, 90k tbn, 50 tbc
    Metadata:
      creation_time   : 2015-02-19 17:10:38
    Stream #0.1(und): Audio: aac, 48000 Hz, stereo, s16, 125 kb/s
    Metadata:
      creation_time   : 2015-02-19 17:10:38
Incompatible pixel format 'yuvj420p' for codec 'mpeg4', auto-selecting format 'yuv420p'
[buffer @ 0x22a3420] w:3840 h:2160 pixfmt:yuvj420p
[scale @ 0x22a3ce0] w:3840 h:2160 fmt:yuvj420p -&gt; w:1920 h:1080 fmt:yuv420p flags:0x4
Output #0, mp4, to '1-short.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp42avc1
    creation_time   : 2015-02-19 17:10:38
    encoder         : Lavf53.21.1
    Stream #0.0(und): Video: mpeg4, yuv420p, 1920x1080 [PAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 25 tbn, 25 tbc
    Metadata:
      creation_time   : 2015-02-19 17:10:38
    Stream #0.1(und): Audio: libvo_aacenc, 48000 Hz, stereo, s16, 200 kb/s
    Metadata:
      creation_time   : 2015-02-19 17:10:38
Stream mapping:
  Stream #0.0 -&gt; #0.0
  Stream #0.1 -&gt; #0.1
Press ctrl-c to stop encoding
frame=  125 fps=  6 q=31.0 Lsize=     968kB time=5.00 bitrate=1586.7kbits/s    
video:842kB audio:123kB global headers:0kB muxing overhead 0.421047%
</code></pre>

<p>I know from experience that ffmpeg is an excellent tool, so I must be screwing up the options/parameters somehow...</p>

<p>How can I do this? </p>
","<p><strike>The default settings for ffmpeg are very low quality, and since you don't specify any codec or quality parameters it's just using the defaults (I don't know why the devs don't fix that because it generates a lot of questions on forums everywhere).</strike> </p>

<p><strong><em>Edit</strong>: the defaults are now quite sane. With a recent build of ffmpeg you don't need to specify anything more than input and output files to achieve good useable results. You can, of course tweak to your heart's desire.</em></p>

<p>Try adding <code>-c:v libx264 -crf 20 -preset slow</code> to the command. </p>

<ul>
<li><code>-c:v libx264</code> tells it to use the libx264 encoder, </li>
<li><code>crf 20</code> uses the Constant Rate Factor quantiser (which paradoxially means variable bit rate, but constant quality) with a value of 20 (pretty good quality; lower is better quality / larger files, higher is crappier / smaller),</li>
<li>the <code>slow</code> preset is a shortcut for a bunch of encoder settings that means it puts a bit more effort into it than the default (medium). </li>
</ul>

<p>You can tweak these settings, see the <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"" rel=""nofollow noreferrer"">h.264 encoding guide</a> for instructions on what knobs to twiddle.</p>

<p>And if you're using the audio as-is, add <code>c:a copy</code>. That will do a straight copy of the audio stream without re-encoding.</p>
","14913"
"How does Apple Motion compare to Adobe After Effects?","17780","","<p>I am looking at Apple Motion and Adobe After Effects. How to they compare to one another? What are the pros and cons of each?</p>
","<p>After Effects is much more powerful than Motion when you get into advanced stuff like scripting your animations.  After Effects is much more tolerant to large projects, and you can ""precomp"" different sequences (which is difficult to explain without you actually seeing it; basically allows you to put one timeline inside of another or reuse a specific portion of your project over and over, if that makes any sense).  Apple Motion only allows one timeline per project, you cannot nest sequences inside one another without rendering out a QuickTime movie.</p>

<p>Apple Motion does however boast that it gives you ""realtime previews"" which plays your animation instantly, whereas After Effects typically requires you to render a RAM preview in order to see your animation in realtime.  Of course this depends on the speed of your computer and the complexity of your project, so this is really only handy for simple projects.</p>

<p>When you want to start doing really custom effects like particle systems and other physics based or true 3D compositing, your options are very limited in Motion.  After Effects has a whole range of plugins available online that will do almost anything you can think of.  The <a href=""http://www.trapcode.com/"">Trapcode</a> plugins are very well-built and very popular.  There are very few plugins for Motion compared to After Effects.</p>

<p>I've used both for years, and Apple Motion seems to crash more often than After Effects, especially as your projects get more complex (just personal experience, maybe this has improved as of version 5).</p>

<p>I actually started animating for the first time in Apple Motion, and I agree that the learning curve isn't quite as steep as After Effects (although that was Motion 2.0 and 3.0, and now they're at version 5.0).  Once I mastered Motion and dove into After Effects, I haven't looked back.  I recently fired up Motion for the first time in a few years and I was struggling to get it to do what I wanted to do, my brain has just been trained to <strong>think</strong> like After Effects.  Even though at first I was more productive in Motion than After Effects (because After Effects is more complex), I have completely reversed and now I feel like I'm a speed machine in After Effects and Motion is now useless to me.</p>

<p><strong>The bottom line:</strong> if you're serious about getting a professional tool to do professional work, save up to get After Effects.  If you're just a hobbyist that wants to dabble in animation, get Apple Motion (now that it's on the Mac App Store the price has come down considerably).</p>

<p>Hope this answers most of your questions!  If you have any other specific ones, let me know and I'll be happy to talk.</p>
","2343"
"Non-Monotonous DTS on concat (ffmpeg)","17754","","<p>After running this command <code>ffmpeg -f concat -i mylist.txt -c copy output.mp4</code> - I'm getting corrupted <code>output.mp4</code> file and this message:</p>

<pre><code>ffmpeg -f concat -i mylist.txt -c copy output.mp4
ffmpeg version 2.6.2 Copyright (c) 2000-2015 the FFmpeg developers
  built with Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/2.6.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-libx264 --enable-libmp3lame --enable-libvo-aacenc --enable-libxvid --enable-vda
  libavutil      54. 20.100 / 54. 20.100
  libavcodec     56. 26.100 / 56. 26.100
  libavformat    56. 25.101 / 56. 25.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 11.102 /  5. 11.102
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, concat, from 'mylist.txt':
  Duration: N/A, start: 0.000000, bitrate: 829 kb/s
    Stream #0:0: Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1440x900, 701 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc
    Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s
Output #0, mp4, to 'output.mp4':
  Metadata:
    encoder         : Lavf56.25.101
    Stream #0:0: Video: h264 ([33][0][0][0] / 0x0021), yuv420p, 1440x900, q=2-31, 701 kb/s, 30 fps, 30 tbr, 15360 tbn, 15360 tbc
    Stream #0:1: Audio: aac ([64][0][0][0] / 0x0040), 44100 Hz, stereo, 128 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (copy)
  Stream #0:1 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
[mp4 @ 0x7f897a01bc00] Non-monotonous DTS in output stream 0:0; previous: 598061, current: 467644; changing to 598062. This may result in incorrect timestamps in the output file.
[mp4 @ 0x7f897a01bc00] Non-monotonous DTS in output stream 0:0; previous: 598062, current: 468044; changing to 598063. This may result in incorrect timestamps in the output file.
[mp4 @ 0x7f897a01bc00] Non-monotonous DTS in output stream 0:0; previous: 598063, current: 468444; changing to 598064. This may result in incorrect timestamps in the output file.
...
[mp4 @ 0x7f897a01bc00] Non-monotonous DTS in output stream 0:0; previous: 598362, current: 588044; changing to 598363. This may result in incorrect timestamps in the output file.
frame= 1472 fps=0.0 q=-1.0 Lsize=    5825kB time=00:00:49.04 bitrate= 973.0kbits/s
video:4903kB audio:877kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.776358%
</code></pre>

<p>Content of <code>mylist.txt</code></p>

<pre><code>file 'cut.mp4'
file 'cut2.mp4'
</code></pre>

<p>cut.mp4 output from ffmpeg:</p>

<pre><code>ffmpeg -i cut.mp4
ffmpeg version 2.6.2 Copyright (c) 2000-2015 the FFmpeg developers
  built with Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/2.6.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-libx264 --enable-libmp3lame --enable-libvo-aacenc --enable-libxvid --enable-vda
  libavutil      54. 20.100 / 54. 20.100
  libavcodec     56. 26.100 / 56. 26.100
  libavformat    56. 25.101 / 56. 25.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 11.102 /  5. 11.102
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'cut.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf56.25.101
  Duration: 00:00:39.04, start: 0.036281, bitrate: 837 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1440x900, 701 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>

<p>cut2.mp4 output from ffmpeg:</p>

<pre><code>ffmpeg -i cut2.mp4
ffmpeg version 2.6.2 Copyright (c) 2000-2015 the FFmpeg developers
  built with Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/2.6.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-libx264 --enable-libmp3lame --enable-libvo-aacenc --enable-libxvid --enable-vda
  libavutil      54. 20.100 / 54. 20.100
  libavcodec     56. 26.100 / 56. 26.100
  libavformat    56. 25.101 / 56. 25.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 11.102 /  5. 11.102
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'cut2.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf56.25.101
  Duration: 00:00:10.07, start: 0.000000, bitrate: 1498 kb/s
    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 1440x900, 1271 kb/s, 30 fps, 30 tbr, 12k tbn, 60 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 218 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>

<p><code>cut.mp4</code> I got by this command <code>ffmpeg -ss 00:00:11 -i myfile.mp4 -to 00:00:39 -vf 'drawbox= : x=0 : y=0 : color=invert' cut.mp4</code></p>

<p><code>cut2.mp4</code> I got by this command <code>ffmpeg -ss 00:00:00 -i myfile.mp4 -to 00:00:10 -c copy cut2.mp4</code></p>

<p>I searched a lot - didn't find any solution, maybe, someone can help me out  with this one.</p>

<p><code>output.mp4</code> is playable, but looks weird.</p>

<p><img src=""https://i.stack.imgur.com/qLCHA.jpg"" alt=""enter image description here""></p>
","<h1>Using the latest <code>ffmpeg</code></h1>

<p>General users should always use <code>ffmpeg</code> from the current git master branch (the latest code available):</p>

<ul>
<li>When encountering an issue the first thing to do is check to see if you are using a build from git master.</li>
<li>It is considered stable.</li>
<li>It will have more bug fixes and features.</li>
<li><a href=""http://git.videolan.org/?p=ffmpeg.git;a=shortlog"">FFmpeg development is very active</a>.</li>
<li>If you want to get support from official help resources you must use git master.</li>
<li>Releases are for distributors.</li>
</ul>

<p>See the <a href=""https://ffmpeg.org/download.html"">FFmpeg Download</a> page for various options, or refer to <a href=""https://trac.ffmpeg.org/wiki/CompilationGuide"">FFmpeg Wiki: Compile Guides</a>.</p>

<h2>Other stuff</h2>

<ul>
<li><p>In your command with <code>drawbox</code> you can <a href=""https://ffmpeg.org/ffmpeg.html#Stream-copy"">steam copy</a> the audio with <code>-c:a copy</code> instead of re-encoding it.</p></li>
<li><p>When using <code>-ss</code>, <code>-to</code> only works as expected if <code>-ss</code> is used as an output option.</p></li>
</ul>
","15478"
"Professional text animation software","17690","","<p>In addition to Adobe Flash, what other tools people usually use to make text animations.<br>
By text animation I mean, like animations that has a narration and word or letters from the narration dance on the screen, change in size and color, bump to each other, fall down or fly away.<br>
I am looking for something that professionals would use, some like what is used to produce this video: <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/yfRVCaA5o18?start=0""></iframe>
            </div></div> </p>
","<p>All of the pro video editors have a means of animating titles to some degree, some are better than others. I am only experienced with Sony Vegas Pro 10, but I have seen my colleagues use Adobe Premiere with Adobe After Effects to create some mind blowing 'dancing' titles. Vegas has plugins available to boost their basic package but the basic package can do most if not all the effects you have in your example while Adobe's Premiere and After Effects might be able to take this to another level as in more 3d attributes.</p>

<p>I recommend a pro video editor over power point as you have tons of flexibility (avi, mov, mp4, m2t, etc) vs. only a ppt file. However, there are many products available to convert ppt files into video such as this free version I just found:</p>

<p><a href=""http://www.effectmatrix.com/PowerPoint-Video-Converter/Free-PowerPoint-Video-Converter.htm"" rel=""nofollow"">http://www.effectmatrix.com/PowerPoint-Video-Converter/Free-PowerPoint-Video-Converter.htm</a></p>

<p>Caution: I have never tried this product so I am uncertain on quality.</p>

<p>I should mention that animating titles is not the only thing going on in your example, there is some degree of track motion control and pan/crop as well not to mention compositing.</p>

<p>If you are seriously looking for a pro product, I would seek advice from professionals that already use these products in conjunction with trying out the trail versions if available. I do know that Sony Vegas Pro 11 is available for free download for a 30 day trail. Most of these products will cost between $500 and $2500 and the learning curve can be heavy.</p>

<p>Sony Vegas trial version:
<a href=""http://www.sonycreativesoftware.com/download/trials/vegaspro?keycode=64506"" rel=""nofollow"">http://www.sonycreativesoftware.com/download/trials/vegaspro?keycode=64506</a></p>

<p>This is a serious product that costs you both time and money.</p>

<p>I would recommend posting on the appropriate forum on creative cow to seek more inside advice.</p>

<p><a href=""http://creativecow.com/"" rel=""nofollow"">http://creativecow.com/</a></p>

<p>Hope this helps</p>
","3136"
"How to detect and extract words from audio/video stream online in a webpage itself?","17160","","<p>I have a video (or audio, I think it is not important) stream/file which does not contain subtitles. But it has clean english speech. Is it possible to detect and extract words from it. I don't need 100% accuracy.</p>

<p>Are there some services or applications for this case? Maybe some services gives API for this? Any ideas are welcome.</p>
","<p>If you work on a regular basis with the ""owner"" of the voice I can recommend Nuance's <a href=""http://www.nuance.com/dragon/index.htm"" rel=""nofollow"">Dragon</a> it offers very accurate speech to text with very few errors but it needs some ""calibration"" to a specific voice.</p>

<p>So if you can get your actor/narrator to read the calibration text you will have an easy time making transcripts and subtitles (though subs will require manual timing afterwards).
Nuance also offers a <a href=""http://www.nuance.com/for-developers/dragon/index.htm"" rel=""nofollow"">developer SDK/API</a> if you need that.</p>

<p>However it will not help you in the case of interviews with random people on the street.</p>

<p>An alternative that doesn't need calibration and is also free to use is the Google Voice to Text service. There is a nice un-offical ""API"" for that on <a href=""https://github.com/gillesdemey/google-speech-v2"" rel=""nofollow"">Github</a>.</p>

<p>It will give you usable results in most cases and its fairly easy to use if you know a bit about programming and can use a command line.
Though be aware that I do not know the legal status of that, I'm not sure if that service is allowed to be used for commercial use. You might want to read into the <a href=""http://www.google.com/intl/en/policies/terms/"" rel=""nofollow"">Google Terms of Service</a> for that, they unified that about a year ago so it should apply for the Voice to Text service as well. Also a downside, it only accepts 15 seconds snippets but if you want to automate this anyway you can just split your audio file up with FFmpeg and upload them at the same time.</p>

<p>There is also another commercial alternative called <a href=""https://www.ispeech.org/developers"" rel=""nofollow"">iSpeech</a> which works on a pay per use model.</p>

<p>Also there is a built-in speech recognition in Adobe Premiere which should be the easiest way to make subitles as it integrates into the complete video workflow and allows almost automatic adjustment of the timing. A nice how-to can be found <a href=""http://www.premiumbeat.com/blog/speech-analysis-premiere-pro/"" rel=""nofollow"">here</a>.</p>
","10854"
"ffmpeg drawtext filter - create transparent background with text","16594","","<p>I am currently working with the <code>drawtext</code> filter. So far I have only been successful with <code>drawtext</code> option configurations to use the right font and place the text horizontally centered. How could i get <code>drawtext</code> to display text on a video as shown below?</p>

<p>ffmpeg</p>

<pre><code>ffmpeg -i ""/media/test/test.mp4"" -vf drawtext=""fontfile=/usr/share/fonts/truetype/open-sans/OpenSans-Regular.ttf:text='Title of this Video':x=(w-tw)/2:y=(h-th)/2""  /media/test_edited.mp4""
</code></pre>
","<p>Use <a href=""https://ffmpeg.org/ffmpeg-filters.html#drawbox"" rel=""noreferrer""><code>drawbox</code></a> for the box, and <a href=""https://ffmpeg.org/ffmpeg-filters.html#drawtext"" rel=""noreferrer""><code>drawtext</code></a> for the text.</p>

<p><img src=""https://i.stack.imgur.com/Bhs0u.jpg"" alt=""Red beetle""></p>

<pre><code>ffmpeg -i input.mp4 -vf \
""format=yuv444p, \
 drawbox=y=ih/PHI:color=black@0.4:width=iw:height=48:t=max, \
 drawtext=fontfile=OpenSans-Regular.ttf:text='Title of this Video':fontcolor=white:fontsize=24:x=(w-tw)/2:y=(h/PHI)+th, \
 format=yuv420p"" \
-c:v libx264 -c:a copy -movflags +faststart output.mp4
</code></pre>

<ul>
<li><p><code>drawtext</code> has a <code>box</code> option, but as far as I can tell it can't be an arbitrary width and is relative to the text size, so that is why <code>drawbox</code> is used instead.</p></li>
<li><p>The placement of your box in your example made me think of the ""golden ratio"" (<code>PHI</code>), so I used that in my example.</p></li>
<li><p>The <a href=""https://ffmpeg.org/ffmpeg-filters.html#format"" rel=""noreferrer""><code>format</code></a> filter is used to improve the color of the <code>drawbox</code> area; otherwise, depending on your input and output formats, the area could look de-saturated or monochrome. The <code>format</code> filter is then used again to ensure that the pixel format of the output file is compatible with all players.</p></li>
<li><p>The audio is being <a href=""https://ffmpeg.org/ffmpeg.html#Stream-copy"" rel=""noreferrer"">stream copied</a> in this example because you may not want to needlessly re-encode it.</p></li>
</ul>
","15554"
"Fix bad files and streams with ffmpeg so VLC and other players would not crash","16177","","<p>It happened me many times that I could end up in corrupted video files:</p>

<ul>
<li>video downloaded from official sources but with torrent and for some reason not all the pieces are there in .mkv, .ts or .mp4</li>
<li>records in .ts format where at a certain point the data is missing</li>
<li>copied videos from an old hard drive with damaged sectors</li>
</ul>

<p>When I play this videos with VLC or other players they plays till a certain point and then instantly exits. Sometimes I can try to skip the bad part but I could meet another bad sector and have an insta-quit again.</p>

<p>Question is:</p>

<p>Can I fix this problems (I suppose stream and keyframes related) with ffmpeg and maybe in a -c copy fashion so I haven't to recompress everything?</p>

<p>What I want to achieve is a full lenght video where the missing parts are skipped, I prefer a glitch rather than having my videos crash.</p>
","<p>If parts of the file reside on physically bad sectors, or for whatever reason, the OS cannot serve the whole file to FFmpeg, then naturally FFmpeg can't do anything about that. You should get a utility which can ignore those portions and write the salvageable parts to a new file, like <a href=""http://www.gnu.org/software/ddrescue/ddrescue.html"" rel=""noreferrer"">ddrescue</a>.</p>

<p>Now, if the file protocol is not the cause of errors, you can try</p>

<pre><code>ffmpeg -err_detect ignore_err -i video.mkv -c copy video_fixed.mkv
</code></pre>
","18226"
"How do I scale an object in only one direction?","15965","","<p>I am animating a volume meter, and have done the ""tricky bit"" of getting the volume value.</p>

<p>Now I want to apply the volume value to a mask , so that as the music moves, the mask is exposed in time to the beat.</p>

<p>I have tried two ways, the first by scaling the mask. However, I can't get it to scale in one direction only (i.e. it scales around a center point.) Even if I move the pan anchor it still doesn't pin the base of the shape.</p>

<p>The other way I tried was using a linear wipe. However, this was no good either, as the whole wipe effect is completed at 38% (i.e. if I move the slider the object vanishes totally at 38%, not 100% as I would expect.)</p>

<p>Is there a better way to do this?</p>

<p>Seems simple, but has me stumped.</p>
","<ul>
<li>Set the <em>Anchor Point</em> to the <strong>bottom</strong> of the object by pressing <kbd>y</kbd> or clicking on <img src=""https://i.stack.imgur.com/65jz5.jpg"" alt=""enter image description here""></li>
<li>Disable <strong>proportional scaling</strong> <img src=""https://i.stack.imgur.com/lB9Gz.jpg"" alt=""enter image description here""></li>
<li>Optional you can add a <strong>slider control</strong> to to get a better control of <strong>direction scale values</strong></li>
</ul>

<p><img src=""https://i.stack.imgur.com/xRPvl.jpg"" alt=""enter image description here""></p>

<p>The shape can also used as <strong>track matte</strong>, just set the layer below to <code>Alpha</code>. </p>

<p><strong>Note:</strong> As alternative you could use a trim path on a shape layer as track matte, see this answer: </p>

<ul>
<li><a href=""https://graphicdesign.stackexchange.com/questions/49643/how-to-use-after-effects-trim-path-with-an-illustrator-path/49975#49975"">https://graphicdesign.stackexchange.com/questions/49643/how-to-use-after-effects-trim-path-with-an-illustrator-path/49975#49975</a></li>
</ul>
","15348"
"How to deinterlacing with ffmpeg?","15890","","<p>I have the following VOB file:</p>

<p> <code>ffmpeg -i input.vob</code> output:</p>

<pre><code>ffmpeg version N-77455-g4707497 Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 4.8 (Ubuntu 4.8.4-2ubuntu1~14.04)
  configuration: --extra-libs=-ldl --prefix=/opt/ffmpeg --mandir=/usr/share/man --enable-avresample --disable-debug --enable-nonfree --enable-gpl --enable-version3 --enable-libopencore-amrnb --enable-libopencore-amrwb --disable-decoder=amrnb --disable-decoder=amrwb --enable-libpulse --enable-libdcadec --enable-libfreetype --enable-libx264 --enable-libx265 --enable-libfdk-aac --enable-libvorbis --enable-libmp3lame --enable-libopus --enable-libvpx --enable-libspeex --enable-libass --enable-avisynth --enable-libsoxr --enable-libxvid --enable-libvo-aacenc --enable-libvidstab
  libavutil      55. 11.100 / 55. 11.100
  libavcodec     57. 20.100 / 57. 20.100
  libavformat    57. 20.100 / 57. 20.100
  libavdevice    57.  0.100 / 57.  0.100
  libavfilter     6. 21.101 /  6. 21.101
  libavresample   3.  0.  0 /  3.  0.  0
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, mpeg, from 'input.vob':
  Duration: 00:31:36.96, start: 0.335967, bitrate: 6517 kb/s
    Stream #0:0[0x1e0]: Video: mpeg2video (Main), yuv420p(tv, bt470bg), 720x576 [SAR 64:45 DAR 16:9], max. 9800 kb/s, 25 fps, 25 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x80]: Audio: ac3, 48000 Hz, stereo, fltp, 256 kb/s
</code></pre>

<p> <code>mediainfo input.vob</code> output:</p>

<pre><code>General
Complete name                            : input.vob
Format                                   : MPEG-PS
File size                                : 1.44 GiB
Duration                                 : 31mn 36s
Overall bit rate mode                    : Variable
Overall bit rate                         : 6 518 Kbps

Video
ID                                       : 224 (0xE0)
Format                                   : MPEG Video
Format version                           : Version 2
Format profile                           : Main@Main
Format settings, BVOP                    : Yes
Format settings, Matrix                  : Default
Format settings, GOP                     : M=3, N=12
Format settings, picture structure       : Frame
Duration                                 : 31mn 36s
Bit rate mode                            : Variable
Bit rate                                 : 6 132 Kbps
Maximum bit rate                         : 9 800 Kbps
Width                                    : 720 pixels
Height                                   : 576 pixels
Display aspect ratio                     : 16:9
Frame rate                               : 25.000 fps
Standard                                 : PAL
Color space                              : YUV
Chroma subsampling                       : 4:2:0
Bit depth                                : 8 bits
Scan type                                : Interlaced
Scan order                               : Top Field First
Compression mode                         : Lossy
Bits/(Pixel*Frame)                       : 0.591
Time code of first frame                 : 00:00:00:00
Time code source                         : Group of pictures header
Stream size                              : 1.35 GiB (94%)
Color primaries                          : BT.601 PAL
Transfer characteristics                 : BT.470 System B, BT.470 System G
Matrix coefficients                      : BT.601

Audio
ID                                       : 189 (0xBD)-128 (0x80)
Format                                   : AC-3
Format/Info                              : Audio Coding 3
Mode extension                           : CM (complete main)
Format settings, Endianness              : Big
Muxing mode                              : DVD-Video
Duration                                 : 31mn 36s
Bit rate mode                            : Constant
Bit rate                                 : 256 Kbps
Channel(s)                               : 2 channels
Channel positions                        : Front: L R
Sampling rate                            : 48.0 KHz
Compression mode                         : Lossy
Stream size                              : 57.9 MiB (4%)
</code></pre>

<p>I have converted it to mp4 with the following command:</p>

<pre><code>ffmpeg -i input.vob -c:v libx264 -preset slow -crf 19 -c:a aac -b:a 256k -threads 0 output.mp4
</code></pre>

<p>In the resulting video I see this effect:</p>

<p><a href=""https://i.stack.imgur.com/mh7yr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mh7yr.png"" alt=""img1""></a> <a href=""https://i.stack.imgur.com/WdyOG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WdyOG.png"" alt=""img2""></a> <a href=""https://i.stack.imgur.com/4vOt3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4vOt3.png"" alt=""img3""></a></p>

<p>How can I solve?</p>
","<p>Try</p>

<pre><code>ffmpeg -i input.vob -vf yadif -c:v libx264 -preset slow -crf 19 -c:a aac -b:a 256k output.mp4
</code></pre>

<p>See <a href=""https://ffmpeg.org/ffmpeg-filters.html#yadif"" rel=""noreferrer"">FFmpeg Filter Documentation: yadif</a> for more info.</p>
","17397"
"Render srt subtitles to video?","15547","","<p>I have subtitles in .srt file format, and I need to render them on top of video file (prefereably mp4). I have tried handbrake and mediacoder so far, with no luck. What should I do?</p>
","<p>With a recent version of ffmpeg (requires the subtitles to be in a separate file, due to current libavfilter limitations):</p>

<pre><code>ffmpeg -i input.mp4 -filter:v subtitles=subtitle.srt -c:a copy -c:v libx264 -crf 22 -preset veryfast output.mp4
</code></pre>

<p>If your SRT subtitles are muxed into an MKV container, you can extract them like this:</p>

<pre><code>ffmpeg -i input.mkv -map 0:s:0 subtitles.srt
</code></pre>
","6867"
"High quality ogv files - possible?","15426","","<p>I'm converting mov (or flv or mp4) files into ogv. Unfortunately, the programs I've been using (Miro, FFMPEG's ogv plugin) have terrible encoding that appears extremely fuzzy.  Does anyone have recommendations for less compressed files in this format? Is it possible? Software recommendations?
Thanks.</p>
","<p>You can encode to Theora video and Vorbis audio with <code>ffmpeg</code> if it has been compiled with <code>--enable-libtheora</code> and <code>--enable-libvorbis</code>. Depending on your <code>ffmpeg</code> version the default settings may not provide good enough quality. Therefore you must add some additional parameters to enable a constant quality type of mode for the video.</p>

<h2>Theora video</h2>

<p>Adjust video quality with the <code>-qscale:v</code> (or the alias <code>-q:v</code>) option. The valid range to set video quality with libtheora is -1 to 10. A higher value is a higher quality.</p>

<p><strong>Note:</strong> More modern alternatives such as VP8 can provide better quality at lower bitrates. See the <a href=""https://trac.ffmpeg.org/wiki/Encode/VP8"">FFmpeg: VP8 in WebM Encoding Guide</a> for more info.</p>

<h2>Vorbis audio</h2>

<p>Adjust audio quality with the <code>-qscale:a</code> (or the alias <code>-q:a</code>) option for VBR audio. The valid range to set audio quality with libvorbis is 0 to 10. A higher value is higher quality. <a href=""http://wiki.hydrogenaudio.org/index.php?title=Recommended_Ogg_Vorbis#Recommended_Encoder_Settings"">See the Recommended Vorbis Encoder Settings</a> to get an idea of what values to use. The default of <code>-q:a 3</code> will be used if you omit <code>-q:a</code> or <code>-b:a</code>.</p>

<p>Note that <code>ffmpeg</code> supports two Vorbis encoders: the external library libtheora (<code>-c:a libvorbis</code>) and the native Vorbis encoder (<code>-c:a vorbis -strict experimental</code>). The native encoder is experimental, does not compare well to libvorbis, and usage is not recommended.</p>

<h2>Example</h2>

<p>The following command will create a good quality output for both video and audio:</p>

<pre><code>ffmpeg -i input -c:v libtheora -c:a libvorbis -q:v 6 -q:a 5 output.ogg
</code></pre>

<h3>Getting <code>ffmpeg</code></h3>

<p>Make sure to always use a recent <code>ffmpeg</code> build and refer to the current documentation specific to your build since options can change. See the <a href=""http://ffmpeg.org/download.html"">FFmpeg download</a> page for various ways to acquire <code>ffmpeg</code> or follow one of the <a href=""https://trac.ffmpeg.org/wiki/CompilationGuide"">FFmpeg compile guides</a>.</p>

<h3>Also see</h3>

<ul>
<li><a href=""http://ffmpeg.org/ffmpeg-codecs.html#libtheora"">FFmpeg documentation on libtheora</a></li>
<li><a href=""http://ffmpeg.org/ffmpeg-codecs.html#libvorbis"">FFmpeg documentation on libvorbis</a></li>
</ul>
","4230"
"How does graphics card's memory affect video rendering performance?","14919","","<p>So, I'm asking that, since I've got a GTX650Ti 1 GB graphics, and it takes 5 minutes to render a simple 1 minute video with minimum effects. Is it normal? I've tried it both with Sony Vegas and Adobe Premiere
Will the 2 GB version of GTX650Ti increase performance?</p>
","<p><strong>1)</strong> Everytime the GPU has to process something, it will load the necessary data (in that case your video frames) into the VRAM. Simple as that, the GPU cannot work with your system memory.
Though the GPU gets it's data from the system memory, so your system memory will act as a buffer and you don't need to load that much data at once into the VRAM.</p>

<p>Though programs which offer a real-time workflow will want to load as much data into the VRAM as possible. That's the reason why you can allocate a specific amount of VRAM in Adobe After Effects (not 100% sure if you can do the same in Premiere).
So every time you apply an effect in Premiere that is GPU accelerated, the GPU will have to request data into the VRAM to process the frames.</p>

<p>New versions of OpenCL and CUDA are improving the way of how the GPU will get its data and reducing the overhead caused by the CPU which will reduce the amount of VRAM needed at a time but VRAM will always be a very important factor with the current PC architecture.
Current GPUs use GDDR5 which is A LOT faster than DDR3 system memory. This is needed as the memory speed is a very important performance factor in GPU computing. Overclocking your VRAM can give you quite a performance boost in memory intensive 3D rendering applications (though probably not in Premiere).
Current <a href=""http://en.wikipedia.org/wiki/GDDR5"" rel=""nofollow"">GDDR5</a> based GPUs like the <a href=""http://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units#GeForce_700_Series"" rel=""nofollow"">GTX 780 Ti</a> are capable of more than 300 GB/s (336 GB/s in this case). <a href=""http://en.wikipedia.org/wiki/Ddr3"" rel=""nofollow"">DDR3</a>-2133 is substantially slower with a peak bandwidth of 16,66 GB/s per module. Also 2133 is rather uncommon in OEM PCs, DDR3-1600 and 1300 is more common.</p>

<p><strong>2)</strong> So you will benefit from a large amount of VRAM everytime you work with very high resolution images or many videos at once aswell as with high-frame rate footage. Premieres rendering engine is utilizing the GPU for both the preview aswell as the final rendering if you chose so.</p>

<p>In practice I'd say it's nonsense to have more than 2GB of VRAM for Premiere unless you work with other applications at the same time that utilize the GPU. Processing bitmaps doesn't require all that much data at once unless you work with crazy resolutions like 8k or 16k, the processing capability of your GPU will be the bottleneck before your VRAM.</p>

<p>Thats my opinion based on what I know about the Premiere video engine and GP-GPU. Premiere might do some other stuff in the background that requires more VRAM. So if you have a very specific use case I'd rather make a benchmark or search for one before making a purchase decision. If you just want to make a general guess this information should help you quite well to make a decision.</p>
","10835"
"Why does the color red always appear pixelated on TV and videos on the PC?","14829","","<p>I hope this isn't off-topic.  While technically about video production, it's not about a problem I am having.</p>

<p>Have you ever noticed on TV how the color red is always noticeably pixelated?  It's the same why while watching video on the computer, as well, be it a Blu-ray, DVD, a video playing directly from disk, or a video being streamed from the internet.  No other other that I know of looks pixelated like the color red.  I've noticed this since as far back as I can remember starting with DVD.  I haven't watched any VHS tapes for many, many years, so I can't say whether or not this pixelation occurs with tapes, but it'd make sense to think that it wouldn't since those are analog.</p>

<p>BTW, I've searched for this online and found a lot of people asking the same questions, but I have yet to see an actual answer.</p>

<p>Here's an example of red pixelation that I just happened to come across on YouTube, although the same thing occurs even on TV broadcasts.  Although you can still see it at actual size, zooming in lets you see how pixelated the color red is in comparison to the rest of the colors which really aren't pixelated at all.  I strongly doubt that this is merely a visual anomaly.  Instead, I believe it has to do with how the color red is processed during encoding.</p>

<p><img src=""https://i.stack.imgur.com/CYXz9.png"" alt=""enter image description here""></p>
","<p>It's not an illusion - it's called chroma subsampling.</p>

<p>Most video codecs do not represent colour in full resolution. This allows for more efficient ""lossy"" compression because it takes advantage of the fact that the human eye is more sensitive to brightness (""luma"") than colour (""chroma""). Most lossy codecs lower the chroma resolution to half or one quarter of the overall resolution, so you may only get one pixel's worth of colour for every four pixels of brightness. This dramatically decreases the amount of data needed, with only a small loss of apparent quality.</p>

<p>It's slightly more complicated though: the brightness is actually made up of the sum of the three colour components Red, Green and Blue. And they're not encoded as RGB, that would be require more bandwidth, they're encoded as YUV. Y corresponds roughly to the green component, and the U and V are Y minus the red component and Y minus the blue component (a gross approximation, actually - if you want the whole formula look <a href=""https://en.wikipedia.org/wiki/Yuv#Converting_between_Y.27UV_and_RGB"" rel=""noreferrer"">here</a>).</p>

<p>In most codecs the U and V components are sampled at a lower resolution than the Y. this is expressed in the three-way ratio you often see if you hang around video forums too much, e.g. 4:2:2 or 4:2:0. For a two row rectangle of pixels the numbers represent:</p>

<blockquote>
  <p>""width of sample region (Y samples)"":""UV samples in the first
  row"":""additional UV samples in the second row""</p>
</blockquote>

<p>A common example of this notation is in the codec name ""proRes422"" the 422 part of the name comes from 4:2:2 meaning for every a 4x2 rectangle there will be 4 Y samples in each row 2 UV samples in the first row (half the horizontal resolution) and 2 UV samples in the second row. So proRes422 has half the chroma resolution of the luma.</p>

<p>On the internet and on telly you're most likely seeing everything in a 4:2:0 codec. In every 4x2 rectangle of picture there are only two UV samples (the 0 means that there are no additional samples on the second row). So the colour part of the image is composed of chunks 2x2 pixels in size, in other words, one quarter the resolution.</p>

<p>This means the red channel on its own has one quarter the resolution of the overall picture. </p>

<p><strong>TL;DR</strong> the red looks pixelated - because it actually <em>is</em>.</p>
","5628"
"How is ""definition"" (SD, HD, UHD) actually defined?","14386","","<p>OK, so at first it might sound like a stupid question but I'm trying to get my head around all these definitions...</p>

<p>It seems that they don't have a specific resolution, but they do at the same time. It's as they have their own resolution but also are part of a group of resolutions.</p>

<p><a href=""http://en.wikipedia.org/wiki/Display_resolution#Televisions"">Wiki says:</a></p>

<ul>
<li><p><strong>HD</strong></p>

<ul>
<li>720i</li>
<li>1080i</li>
<li>1080p</li>
</ul></li>
<li><p><strong>UHD</strong></p>

<ul>
<li>2160p</li>
<li>4320p</li>
<li>8640p</li>
</ul></li>
</ul>

<p>But generally we say that 1080i is HD and 1080p is FULLHD.</p>

<p>UHD has resolution 3840 x 2160 and 4K is 4096 x 2160 but 4K is UHD. Same vertical resolution (2160) but different aspect ratio, making the horizontal slightly longer by 256 pixels. And it also includes 8K...</p>

<p>Is UHD a resolution, a group of resolutions or both? If it's both, why?</p>
","<p>The problem is that definitions such as HD, UHD and 4k etc. are partly hype. Marketers for AV equipment like to be rather elastic with the truth, hence the rather rubbery definitions. They're not standards such as <a href=""https://en.wikipedia.org/wiki/Broadcast_television_systems#ATSC"" rel=""nofollow noreferrer"">ATSC</a> or DVB, or <a href=""https://en.wikipedia.org/wiki/Graphics_display_resolution"" rel=""nofollow noreferrer"">graphic display resolutions</a> such as VGA, WXGA etc., they're really just buzzwords.</p>

<p>There are other complications - like the different broadcast standards for SD  so that SD PAL is 576 visible lines and SD NTSC is 480 lines  or <a href=""https://en.wikipedia.org/wiki/Pixel_aspect_ratio"" rel=""nofollow noreferrer"">non-square pixels</a> in digital video meaning that even a 16:9 HD 1080p image can be either 1440x1080 or 1920x1080*. So it's pretty understandable that you're confused.</p>

<p>For what it's worth, the general consensus is that:</p>

<ul>
<li><strong>SD</strong> refers to 576i / 480i (with the unusual <a href=""https://en.wikipedia.org/wiki/Enhanced-definition_television"" rel=""nofollow noreferrer""><strong>EDTV</strong></a> variant being 576p and 480p). Display resolution is 768  576 PAL or 720  534 NTSC, often the actual pixel resolution is reduced, as in the case of DV: 720  576 PAL or 720  480 NTSC, with non-square pixels</li>
<li><strong>HD</strong> refers to 720p and 1080i / 1080p. The display resolution is 1280  720 or 1920  1080, but sometimes the horizontal pixel resolution is reduced, e.g. HDV which is 1440  1080 with non-square pixels. You could also say that resolutions higher than this are also High Definition. So in one sense it's anything that isn't standard definition. Note that there is no 720i.</li>
<li><strong>Full HD</strong> or <strong>FHD</strong> means the 1080 flavour of HD (either interlaced or progressive)</li>
<li><strong>2K</strong> describes the Digital Cinema Initiative resolution of 2048  1080, this resolution is based on the historical resolution of scanned super 35mm film. As with 4K it is a little slippery, and can sometimes be used to describe FHD. As the wikipedia article states it describes content <em>""having horizontal resolution <strong>on the order of</strong> 2,000 pixels""</em>. So in other words: near enough is good enough.</li>
<li><strong>QHD</strong> or <strong>Quad HD</strong> - 2560  1440 is a whacky one, it's twice the resolution of 720p. The ""quad"" refers to there being four times the pixels of a 720p frame. Not to be confused with QFHD - see below.</li>
<li><strong>UHD</strong> refers to 3840  2160, and like HD also sometimes to higher resolutions like <strong>4K</strong> and <strong>UHD 8K</strong></li>
<li><strong>QFHD</strong> is 3840 x 2160 ""Quad Full HD"", the quad referring to the fact that it has four times the pixels of a FHD frame, which sounds better than saying it has twice the horizontal and vertical resolution. QFHD = UHD</li>
<li><strong>UHD 4K</strong> - 3840  2160, is double the HD 1080 frame size, and is also sometimes called <strong>4K</strong>, even though the horizontal resolution is less than 4000 pixels. UHD 4K = UHD</li>
<li><strong>UHD 8K</strong> or <strong>FUHD</strong> is 7680  4320. I'm guessing that as the FUHD sets hit the shops they'll be called ""8K"" by the salespeople.</li>
<li><strong>4K</strong> usually refers to Digital Cinema Initiatives 4K which is 4096  2160 but the term is sometimes used to refer to <strong>UHD 4K</strong></li>
<li>Similarly <strong>8K</strong> should properly refer to 8192  4320 but is also applied to <strong>FUHD</strong></li>
</ul>

<p>Note that there are cameras such as the Red cameras that shoot in-between resolutions such as 3K, 5K and 6K. But that's just for production, the finished media will usually be cropped / scaled to a more standard resolution for display or broadcast.</p>

<p><strong>TL;DR</strong> If you're trying to define anything in technical terms and want to be unambiguous just give the pixel dimensions; terms like HD, UHD etc are really <a href=""https://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt"" rel=""nofollow noreferrer"">FUD</a>. They're more for marketing than anything else.</p>

<p>Edit: Here's a handy chart:</p>

<p><img src=""https://i.stack.imgur.com/BBTL1.png"" alt=""here&#39;s a handy chart""></p>

<p>pdf version available <a href=""http://blob.pureandapplied.com.au/i-can-haz-all-the-resolutions/"" rel=""nofollow noreferrer"">here</a></p>
","14776"
"poor quality export of still images from premiere","13816","","<p>This is the actual <strong>source image</strong> to a video track in Premiere (a PNG):
<img src=""https://i.stack.imgur.com/NiZYL.jpg"" alt=""decentqualityinput""></p>

<p>With no resizing whatsoever, and .flv output, this is a <strong>resultant frame</strong> from the exported video:
<img src=""https://i.stack.imgur.com/wg7j4.png"" alt=""poorqualityoutput""></p>

<h3>How could I improve the output quality, especially the text?</h3>

<p>What settings should I use</p>

<ul>
<li>I've tried to max out pretty much everything in the FLV export settings.</li>
<li>And now I moved to MP4 output with H.264 encoding, but still get poor quality stills.</li>
<li>Here are the MP4 Export settings, what should I change?</li>
</ul>

<p><img src=""https://i.stack.imgur.com/Bcutp.png"" alt=""MP4-poor""></p>

<h3>Solution</h3>

<p>Inspired from the answers, turns out that have to set the source to Progressive when creating a New Sequence (instead of Fields: Upper)</p>

<ul>
<li>this is why I did not get better results whatever my configuration once I had created a sequence with Fields set to Upper</li>
<li>and this is why the following settings for my next project, yielded good stills

<ul>
<li>although also using higher resolution, so not sure high that plays into the test results</li>
<li>H.264 / MP4 Export settings otherwise the same </li>
</ul></li>
</ul>

<p><img src=""https://i.stack.imgur.com/H0USB.png"" alt=""ProgressiveFields""></p>
","<p>That looks more like a 'field' from the video, rather than a frame -- jagged diagonals are the tell.</p>

<p>If your video is interlaced, you may only get half the vertical resolution in a still, unless you specifically set it to output a full frame. Check the export settings.</p>
","7744"
"How to see if a video file is progressive or interlaced?","13641","","<p>I can check the resolution of a video by <code>Right Clicking</code> -> <code>Properties</code> -> <code>Details</code> -> <code>Video</code>.</p>

<p>Is there a way to see if the video is progressive or interlaced?</p>
","<p>You should be able to tell just by looking at it.  When you watch for motion and see a comb-like horizontal pattern, the video is interlaced.  You could also try pausing the video at several points and looking for this pattern, but not every frame will look interlaced.  Pause the video at points where there is quick motion, and step forward one frame at a time.  Make sure the video is displayed at %100 zoom. If you find a frame which exhibits this pattern, the whole movie is interlaced.  Once you know what to look for, you'll recognize it instantly.  <img src=""https://i.stack.imgur.com/DnI7S.jpg"" alt=""enter image description here""></p>
","12982"
"Encoding videos for MPEG-DASH","13587","","<p>I read <a href=""http://blog.streamroot.io/encode-multi-bitrate-videos-mpeg-dash-mse-based-media-players/"" rel=""nofollow"">this article</a> on encoding for MPEG-DASH, which has helped me a little and then follow up <a href=""http://blog.streamroot.io/encode-multi-bitrate-videos-mpeg-dash-mse-based-media-players-22/"" rel=""nofollow"">article</a>.</p>

<p>My end goal is to create a batch file that can read in a directory of MP4 files, and then output the configured video bitrates and MPD file needed for MPEG dash consumption by a client.</p>

<p>Previously I was testing with IIS Smooth Streaming, but it seems Microsoft is abandoning that and has been behind the progress of MPEG-DASH.  Their expression encoder 4 encodes videos very nice for Smooth Streaming, but they stopped selling the pro version that supports h.264, of which MPEG-DASH clients can play.  The free version does the VC-1 Advanced which is not supported by MPEG-DASH.</p>

<p>How to encode for MPEG-DASH?  I need it to be targeted to Windows as I have a public server that is plenty fast (Xeon) to encode and will be the delivery method to players as well.</p>
","<p>I would get as far away from EE as possible. Using the x264 tool, and mp4box, you can convert and segment out the files which are ready to be streamed to any dash compatible players. Especially since you mentioned using batch scripts, this is a great solution I think.</p>

<p>This is a good guide: <a href=""http://www.dash-player.com/blog/2014/11/mpeg-dash-content-generation-using-mp4box-and-x264/"" rel=""nofollow"">http://www.dash-player.com/blog/2014/11/mpeg-dash-content-generation-using-mp4box-and-x264/</a></p>
","17414"
"How to Set the defaults for Render Queue in After Effects","13495","","<p>I'm using After Effects CS3 (<em>yeah, I know it's old</em>) and I'm rendering my composition out. When I <strong>Add to Render Queue</strong> I have to change the settings like <strong>Render Settings &amp; Output Module</strong> every time. </p>

<p>Is there a way to set these once and have AE use them every time you <strong>Add to Render Queue</strong>?</p>
","<p>I'm pretty sure it hasn't changed since CS3 (in fact It's been the same for as long as I can remember): in <code>Edit &gt; Templates &gt; Render Settings &gt;</code>. Another way of getting to this dialogue is from the <code>Render Settings</code> drop-down in the render queue. At the bottom of the list of templates in the drop-down is <code>Make Template</code> </p>

<p>Choosing that brings up the dialogue box pictured below. Up the top of the dialogue box you'll find <code>Movie Default</code>; set that to whatever template you want to be the default for renders. If you haven't got a template that has the settings you need then you can make one here - hit the <code>New</code> button, set all your settings and give it a name.  I find duplicating an existing one and modifying it a useful way to go.</p>

<p>Same goes for the output modules, which you get to from <code>Edit &gt; Templates &gt; Output Modules</code>, or from the <code>Output Modules</code> drop-down in the render queue.</p>

<p><img src=""https://i.stack.imgur.com/plOFl.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/KB2Pg.png"" alt=""enter image description here""></p>
","10156"
"How to concatenate clips from the same video with ffmpeg","13430","","<p>I have a long video with different scenes in it.
I want to extract and concatenate 2 scenes from the video (that do not start on an I-frame) using ffmpeg 2.1.4.</p>

<p>For argument, say I want 5 seconds from 01:00 and 02:00.</p>

<p>I can do this:</p>

<pre class=""lang-bash prettyprint-override""><code>ffmpeg -ss 01:00 -i in.mkv -ss 02:00 -i in.mkv -filter_complex ""
  [0:v]select='lt(t,5)'[v0];
  [0:a]aselect='lt(t,5)'[a0];
  [v0][a0][1:v][1:a]concat=n=2:v=1:a=1
"" -c:a libvorbis -t 10 out.mkv
</code></pre>

<p>That gives me the movie I want, but select actually forces ffmpeg to decode the entire rest of the movie.  So, how do I tell it to truncate the first movie instead of decoding it?</p>

<p>I would prefer to do this with an ffmpeg complex filter, if possible.  I know that I can just use <code>-t</code> to recode separate movies first, but the extra encoding step is very slow in my case and also loses a fair amount of quality for this video.</p>

<p>I can't seem to find a filter that truncates the length of a steam.  Are there filter nodes that correspond to the <code>-t</code> or <code>-ss</code> parameters?</p>
","<h1>concat filter</h1>

<p>This method is best if you need to perform additional filtering:</p>

<p>Use the <a href=""http://ffmpeg.org/ffmpeg-filters.html#trim""><code>trim</code></a>, <a href=""http://ffmpeg.org/ffmpeg-filters.html#atrim""><code>atrim</code></a>, <a href=""http://ffmpeg.org/ffmpeg-filters.html#setpts_002c-asetpts""><code>setpts</code></a>, <a href=""http://ffmpeg.org/ffmpeg-filters.html#setpts_002c-asetpts""><code>asetpts</code></a>, and <a href=""http://ffmpeg.org/ffmpeg-filters.html#concat""><code>concat</code></a> filters:</p>

<pre class=""lang-bash prettyprint-override""><code>ffmpeg -i input -filter_complex \
""[0:v]trim=60:65,setpts=PTS-STARTPTS[v0]; \
 [0:a]atrim=60:65,asetpts=PTS-STARTPTS[a0]; \
 [0:v]trim=120:125,setpts=PTS-STARTPTS[v1];
 [0:a]atrim=120:125,asetpts=PTS-STARTPTS[a1]; \
 [v0][a0][v1][a1]concat=n=2:v=1:a=1[out]"" \
-map ""[out]"" output.mkv
</code></pre>

<ul>
<li><p><code>setpts</code> and <code>asetpts</code> will prevent a jerky output due to presentation timestamp issues.</p></li>
<li><p>Make sure to use a recent version. See the <a href=""http://ffmpeg.org/download.html"">FFmpeg Download</a> page for links to builds for Windows, OS X, and Linux.</p></li>
</ul>

<hr>

<h1>concat demuxer</h1>

<p>Another method is to to create the segments individually and <a href=""http://ffmpeg.org/ffmpeg.html#Stream-copy"">stream copy</a> them instead of re-encoding (to save time and quality), and join them with the <a href=""http://ffmpeg.org/ffmpeg-formats.html#concat-1"">concat demuxer</a>.</p>

<pre class=""lang-bash prettyprint-override""><code>$ ffmpeg -ss 60 -i input -t 5 -codec copy clip1.mkv
$ ffmpeg -ss 120 -i input -t 5 -codec copy clip2.mkv
$ echo ""file 'clip1.mkv'"" &gt; concat.txt
$ echo ""file 'clip2.mkv'"" &gt;&gt; concat.txt
$ ffmpeg -f concat -i concat.txt -codec copy output.mkv
</code></pre>
","10401"
"H.264 Max Resolution","13099","","<p>I was trying to find the technical reasons for why H.264 is limited for 4K.
Obviously that in terms of bit-rate it becomes less efficient but the encoding itself is limited and for some reason, I couldn't find any public available references.</p>

<p>Someone?</p>

<p>Thanks</p>
","<p>One of the bits of information associated with a H264 stream is its <a href=""https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Levels"">level</a>. The level informs the decoder the computational resources needed for a successful decode. Turns out that the highest level 5.2 supports upto 9437184 luma samples per frame, which is the number of luma samples needed for a frame of size 4096x2304. So, there may be encoders that can (be forced to) encode higher resolutions but there's no valid level which can be set for the stream, and probably no decoder that can be guaranteed to produce reliable playback.</p>
","16539"
"What is the correct setting in Adobe Premiere CS6 for Nikon Video?","12938","","<p>I have a Nikon D600 and when I set up a new project in Adobe Premiere CS6 there are many options available for Sequence Presets. I suppose this could be a subjective question, but what is the best preset for Nikon DSLR video? ARRI? Digital SLR, etc.?</p>

<p>Thank you.</p>
","<p>The easiest way to set up a sequence with the right settings for your footage is to first import the video footage into your Premiere Pro project. Then right click on the clip in the media bin, and select ""new sequence from clip."" That will create a new sequence with settings that match your footage.</p>
","7399"
"How can I find duplicate videos from a large set of videos of varying formats?","12508","","<p>After losing a backup device with a bunch of my original videos I've had to download a large number of them (~300) back from Youtube. The new downloads are a mix of MP4 and FLV, and the originals are a mix of almost any video format due to lots of experimenting. </p>

<p>To make matters worse there's no clear cutoff for when my local copies disappeared; I've found a point in my youtube videos chonologically where I have some of the videos and I don't have others. I've been trying to manually weed out videos where I already have a copy, but with this many videos I'm sure I've missed some.</p>

<p>How can I effectively go through several hundred videos of different formats and weed out duplicates? The names will be similar but not identical. The formats may be different, and the file size may be different due to that (and or encoding differences). The file lengths should be the same, and visually the videos should be similar enough, but I'm not sure there's any tool to sort that out visually. </p>

<p>Am I doomed to go through the list based on running time and manually look for duplicates or is there a better way?</p>
","<p>I'd recommend a hybrid approach using both computers and people.</p>

<ol>
<li>Bucket the videos by their length (round to the nearest second)</li>
<li>For each bucket, use ffmpeg to <a href=""http://blog.prashanthellina.com/2008/03/29/creating-video-thumbnails-using-ffmpeg/"">generate thumbnails</a> at a predictable and uniform point in the videos (ex: a frame from 10 seconds into the video)</li>
<li>Look at the generated thumbnails in a grid (most OS's provide a nice thumbnail view) and scan for duplicates to remove.</li>
</ol>

<p>You shouldn't have to do any programming to perform these steps, though creating the thumbnails on the command line with ffmpeg may take some finesse.  Good luck!</p>
","5315"
"4k vs. 1080p on a 1080p monitor","11880","","<p>Is it smart to watch a 4k video on a 1080p monitor versus watching it in 1080p (especially on youtube)? Is there any difference for the human eye?</p>
","<p>No, there is no difference to the human eye (or exceptionally minimal).  Your monitor can't display higher quality than it is capable of displaying.  The only advantage you would have is if you were to zoom in on part of the image, you would have more detail when you zoomed in.</p>

<p>Additionally, as user1118321 pointed out, since 4k is not an even multiple of 1080p, you would have some amount of artifacts from the downsampling that your computer would have to do to fit the 4k signal to a 1080p display.  It might be worth it if you have a higher than 1080p display but less than 4k since the loss from downscaling will likely be offset by the extra detail your monitor can display, but for a 1080p display, it is hands down best to use a 1080p stream unless the 4k uses enough of a higher bandwidth to have less artifacts in the stream even after downscaling (which is highly unlikely on any well configured stream).</p>
","12684"
"Converting a Adobe Premiere CS6 project to a CS5 one","11726","","<p>I run the old CS 5 version of Adobe Premiere but my friend just got the newer CS 6 version.</p>

<p>Now she's handing over a project on to me. So I have to turn her project files into files I can continue working in.</p>

<p>The problem is that Premiere's own built-in conversion process did not quite do it. I ended up with a faulty project where the project window is actually blank, even though the files are clearly there (they show up in the timeline). Also, it is still possible to play back the timeline - it's just that I can't actually do much editing with the project window being as it is.</p>

<p>This leads me to my question: Is there a better and simpler way to do the conversion from a CS 6 file to a CS 5 one yet? Are there any good workarounds I have not thought of (that aren't ""render every single track individually and go from there"") or anything else I can do?</p>
","<p>It seems that I have solved my problem.</p>

<p>All I had to do was to import the CS6 project file into a brand new CS5 project. That did it for me.</p>
","5527"
"How do I set up and use ffmpeg in Windows?","11690","","<p>I understand that ffmpeg is a powerful tool for video file conversions. I see online in many places people suggesting it for solutions or getting help in how to use it. The problem is I don't even know how to set it up or use it at all. These posts I'm talking about usually take the form ""Use ffmpeg [complicated and unreadable line of code]."" I'm not really a programmer, but I can do few things, so I think I can figure it out, if I just had a bit of direction.</p>

<p>How do I set up and use ffmpeg in Windows?</p>
","<p>ffmpeg is indeed a powerful video encoder/decoder. It operates in the command line, as opposed to using a GUI. Command line is that black window you find by clicking [windows+r] and typing <code>cmd</code> then hitting enter. This is also called ""command prompt"". Once setup you enter ffmpeg commands in one of these windows to use it.</p>

<p>Here are the basic steps to ""install"" and use it:</p>

<h1>Installation</h1>

<ol>
<li>Go to <a href=""https://ffmpeg.zeranoe.com/builds/"" rel=""nofollow noreferrer"">the ffmpeg download site</a> and download the zip file that best fits you computer's specs. Choose ""static"" linking and the ""nightly git"" version for the most current usability.</li>
<li>Create a folder on your computer to uppack the zip file. This folder will be your ""installation"" folder. I chose <code>C:\Program Files\ffmpeg\</code>. This is a good idea because you will treat this like a regular program. Unpack the zip file into this folder.</li>
<li>The folder should now contain a number of other folders, including one titled <code>bin</code> where <code>ffmpeg.exe</code> is saved. We're not done yet. Double clicking that file does nothing. Remember, this is a command line program. It runs in <code>cmd</code>.</li>
<li>Before you can use <code>ffmpeg.exe</code> in <code>cmd</code> you have to tell your computer where it can find it. You need to add a new system path. First, right click This PC (Windows 10) or Computer (Windows 7) then click <code>Properties &gt; Advanced System Settings &gt; Advanced tab &gt; Environment Variables</code>.  </li>
<li>In the Environment Variables window, click the ""Path"" row under the ""Variable"" column, then click Edit
<a href=""https://i.stack.imgur.com/Ig6nr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ig6nr.jpg"" alt=""Steps to add a System path to Windows""></a></li>
<li>The ""Edit environment variable"" window looks different for Windows 10 and 7. In Windows 10 click New then paste the path to the folder that you created earlier where <code>ffmpeg.exe</code> is saved. For this example, that is <code>C:\Program Files\ffmpeg\bin\</code>
<a href=""https://i.stack.imgur.com/D1FO8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1FO8.jpg"" alt=""Add new system path Windows 10]""></a>
In Windows 7 all the variables are listed in a single string, separated by a semicolon. Simply go the the end of the string, type a semicolon (<code>;</code>), then paste in the path.
<a href=""https://i.stack.imgur.com/Ox5J8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ox5J8.jpg"" alt=""Add new system path Windows 7""></a></li>
<li>Click Ok on all the windows we just opened up.</li>
</ol>

<p>ffmpeg is now ""installed"". The Command Prompt will now recognize ffmpeg commands and will attempt to run them.</p>

<h1>Updating ffmpeg</h1>

<p>To update ffmpeg, just revisit the download page in step 1 above and download the zip file. Unpack the files and copy them over the old files in the folder you created in step 2.</p>

<h1>Using ffmpeg</h1>

<p>Using ffmpeg requires that you open a command prompt window, then type ffmpeg specific commands. Here is a typical ffmpeg command:</p>

<pre><code> ffmpeg -i video.mp4 -vn -ar 44100 -ac 1 -b:a 32k -f mp3 audio.mp3
</code></pre>

<p>This command has four parts:</p>

<ol>
<li><code>ffmpeg</code> - This command tells cmd that we want to run ffmpeg commands. cmd will first look for <code>ffmpeg.exe</code> in one of the folders from step 6 in the Installation section. If it is found, it will attempt to run the command.</li>
<li><code>-i video.mp4</code> - This is an input file. We are going to be doing work on this file.</li>
<li><code>-vn -ar 44100 -ac 1 -b:a 32k -f mp3</code> - These are the ""arguments"". These characters are like mini commands that specify exactly what we want to do. In this case, it is saying create an mp3 file from the input source.

<ul>
<li><code>-vn</code> - Leave out the video stream</li>
<li><code>-ar 44100</code> - Specifies audio resolution in hertz.</li>
<li><code>-ac 1</code> - Audio channels, only 1. This is effectively ""make mono"".</li>
<li><code>-b:a 32k</code> - Audio bitrate, set to 32 kbps.</li>
<li><code>-f mp3</code> - Force to MP3 conversion. Without this command, ffmpeg attempts to interpret what you want based on the extension you use in the output file name.</li>
</ul></li>
<li><code>audio.mp3</code>- This is the output file.</li>
</ol>

<p>As you can probably guess, this short command makes an MP3 audio file from an MP4 file.</p>

<p>To run this command, assuming you have an MP4 file to try this on, follow these steps:</p>

<ol>
<li>Hit the Windows key + r.</li>
<li>Type <code>cmd</code> then enter.</li>
<li>Change the path to where the file is that you want to work on. Type <code>cd [path]</code>. It should look something like <code>cd C:\Users\name\Desktop\</code>.</li>
<li>Now type the ffmpeg command with the name of your input file. The command will run with some feedback. When it's done, cmd will be available for more commands.</li>
</ol>

<p>This is the basic way to use ffmpeg. The commands can get far more complicated, but that's only because the program has so much power. Using <a href=""https://ffmpeg.org/ffmpeg.html"" rel=""nofollow noreferrer"">the ffmpeg documentation</a>, you can learn all the commands and create some very powerful scripts. After that, you can save these scripts into a .bat file so that you just have to double click a file instead of type out the whole command each time. For example, <a href=""https://video.stackexchange.com/a/20494/3643"">this answer contains a script that will create MP3's from all the MP4's in a folder</a>. Then we would be combining the power of ffmpeg with the power of cmd, and that's a nice place to be when you have to do professional quality video/audio encoding on mountains of files.</p>
","20496"
"Can I use FFMPEG in a commercial product","11689","","<p>Not sure if I am correct posting my question here and I will of course delete this question if appropriate.</p>

<p>I have a c# desktop app using FFMPEG
I have a web service using FFMPEG.</p>

<p>I sell both products.</p>

<p>I want to know can i distribute my desktop app with ffmpeg without paying a fee to fmpeg people and without releasing my source code.</p>

<p>I also want to know whether I can use FFMPEG on my server for users to convert images to a video file to be emailed them.  This service is also a payable product.</p>

<p>I have read the links to the FFMPEG licensing info and I read many questions here on these boards.</p>

<p>The closet I have got to an answer is that I can use it on my web server becuase I am not distributing it to client PCS and I can use FFMPEG on a my desktop app if I only use certain features of FFMPEG - whatever they are/</p>

<p>However, this is not definitive enough for me and I cannot afford a lawyer and you would think this question/dilemma would have been resolved somewhere by someone considering how popular FFMPEG is.</p>

<p>Thanks</p>
","<h1>Yes, you can use FFmpeg in a commercial product</h1>

<p>FFmpeg is licensed under the GNU Lesser General Public License (<strong>LGPL</strong>) version 2.1 or later.</p>

<p>Some features, such as support for some external libraries (libx264 and libx265 for example) and various filters, are covered by the GNU General Public License (<strong>GPL</strong>) version 2 or later (see commercial x264 license exception below). If those parts get used the GPL applies to all of FFmpeg. See <code>LICENSE.md</code> in the source to see a list of GPL parts of FFmpeg and which external libraries require GPL.</p>

<p>So, yes, <strong>you can definitely use FFmpeg in commercial products</strong>, and both licenses allow commercial usage, distribution, and modification. FFmpeg is free but is not available under any other licensing terms.</p>

<p>Which license you use is up to you, and depends on what your needs are and how your FFmpeg build is configured, but <strong>you must comply with whatever license you end up using</strong>. FFmpeg does not care if you use it for a commercial purpose or not: only that you properly follow the license.</p>

<h1>What do I need to do to comply?</h1>

<p>If you're just using FFmpeg for private or in-house use, or you are not actually distributing anything from FFmpeg at all then you don't need to do anything. Otherwise, for a very verbose list see the <a href=""https://ffmpeg.org/legal.html"" rel=""nofollow noreferrer"">FFmpeg License Compliance Checklist</a>. To summarize you need to:</p>

<h3>GPL 2.0+</h3>

<ul>
<li>Make available the exact FFmpeg source code that you used.</li>
<li>Provide a copy of the appropriate FFmpeg license with your distributed software (see <code>COPYING.GPLv2</code> or <code>COPYING.GPLv3</code>).</li>
<li>State changes. If you modify the FFmpeg source code you must document what was changed.</li>
<li>Use same license. If you modify the FFmpeg source code it must remain under the same license.</li>
</ul>

<h3>LGPL 2.1+</h3>

<ul>
<li>Make available the exact FFmpeg source code that you used.</li>
<li>Provide a copy of the appropriate FFmpeg license with your distributed software (see <code>COPYING.LGPLv2.1</code> or <code>COPYING.LGPLv3</code>).</li>
<li>State changes. If you modify the FFmpeg source code you must document what was changed.</li>
<li>Use same license. If you modify the FFmpeg source code it must remain under the same license. This does not apply if you are just using linked FFmpeg libraries.</li>
</ul>

<h1>What license is my FFmpeg using?</h1>

<p>The <code>ffmpeg -L</code> command will output a paragraph indicating your license. Example for LGPL v2.1+:</p>

<blockquote>
  <p>ffmpeg is free software; you can redistribute it and/or modify it
  under the terms of the GNU Lesser General Public License as published
  by the Free Software Foundation; either version 2.1 of the License, or
  (at your option) any later version.</p>
</blockquote>

<h1>What about the x264 commercial license?</h1>

<p>The x264 commercial license is LGPL compatible, but FFmpeg does not have an option that disables the GPL requirement for commerical licensed x264 so you have to do that manually. Therefore, if you purchase a commercial license from <a href=""https://licensing.x264.org/"" rel=""nofollow noreferrer"">x264 licensing</a> then you are permitted to:</p>

<ul>
<li>Compile x264 with <code>--disable-gpl</code>.</li>
<li>Modify the <code>configure</code> file in the FFmpeg source code to move libx264 from <code>EXTERNAL_LIBRARY_GPL_LIST</code> to <code>EXTERNAL_LIBRARY_LIST</code>.</li>
<li>Compile ffmpeg <strong>without</strong> <code>--enable-gpl</code> and link to your x264 that has been compiled <strong>with</strong> <code>--disable-gpl</code>.</li>
</ul>

<p>The LGPL still applies in this case, so don't forget to make available the exact FFmpeg source code you used and state what changes you made.</p>

<h2>Do I need a commercial license from x264?</h2>

<p>You do if your application is not GPL-compatible and is being distributed with linked x264. See <a href=""https://mailman.videolan.org/pipermail/x264-devel/2010-July/007508.html"" rel=""nofollow noreferrer"">[x264-devel] Announcing commercial licensing for x264</a> for additional details.</p>

<h1>Also see</h1>

<ul>
<li><a href=""http://ffmpeg.org/legal.html"" rel=""nofollow noreferrer"">FFmpeg License and Legal Considerations</a></li>
<li><a href=""http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html"" rel=""nofollow noreferrer"">GNU Lesser General Public License, version 2.1</a></li>
<li><a href=""http://www.gnu.org/licenses/old-licenses/gpl-2.0.html"" rel=""nofollow noreferrer"">GNU General Public License, version 2</a></li>
<li><a href=""http://www.gnu.org/licenses/gpl-faq.html"" rel=""nofollow noreferrer"">Frequently Asked Questions about the GNU Licenses</a></li>
</ul>
","14804"
"Convert a 60 fps video to a 24 (film) video - is it possible?","11672","","<p>This may not be as simple a problem as it may seem at first, so read carefully.</p>

<p>When I set my (Canon 60D, but this doesn't really matter at all) camera to record at 24 fps (I think it's actually 23.976 fps, but again, this shouldn't really matter), the film fps, it looks so ""perfect"", like any Hollywood movie. It is a bit stuttery, kind of ""low fps"" on the first sight, but it's exactly how it should look - a perfect film - and this is exactly what I want.</p>

<p>Now, the problem is, when I record at 60 fps to have a few more frames in case I need a slowmotion, the movie (when played back at 60 fps) looks too ""fluid, fast"", like a low-budget documentary movie or something like that.</p>

<p><strong>The question</strong>: Can I record all my scenes at 60 fps and <em>then</em> somehow ""convert"" the 60 fps footage so that it looks like it was recorded at 24 fps ?</p>

<p>The reason: I need to record all the scenes in 60 fps, because I don't know which of them I will need in slowmotion later. I <em>cannot</em> record in 24 fps, but I <em>do want</em> the final cut to look as if it was recorded in 24.</p>
","<p>You can, there's a lot of frame blending algorithms that will do it for you. That said, none of them is really great. All your 60fps footage will have a different look than your 24fps footage. You flat out cannot evenly go from 60fps to 24fps, there's just too little overlap in where the frames actually show up on a timeline. For frame conversions of that magnitude (24p->60i for TV, for example), the big boys have traditionally used telecine/telesync setups where they would literally film an image projected from a movie reel with a television camera.  </p>

<p>Consider shooting 50fps (if that's an option). It's much easier to go from 50 to 24 (drop every other frame, then drop one frame a second) than it is to do 60->24.</p>
","1774"
"Cross Dissolve not available in between clips [Premiere CS6]","11571","","<p>I'm trying to add a cross dissolve effect in between two clips. Premiere doesn't allow me to do it between the first and the second clip.  </p>

<p>Does anyone know the reason?</p>
","<p>Do your clips have overlap?  If you put the clips end to end, running them right up to the end of the file and beginning of the file, then there is no video for Premiere to apply a dissolve to.  You need to have sufficient video left in the clip to complete the transition.</p>
","8624"
"Live stream from GoPro to YouTube or another video service","11287","","<p>Here is what I would like to do.</p>

<p>Get the cheapest GoPro camera(s) that will live stream events to YouTube without any other equipment. I would like to record the event to YouTube as it is happening.</p>

<p>The bandwidth is not great but I do not care about the quality of the livestream. I'm more concerned about highest quality video recording.</p>

<p>Here are my questions:
1) Which GoPro camera is appropriate for this job?
2) Is it possible to select some stream settings that will sacrifice the quality of the live stream for better quality recording?</p>

<p>Thanks.. </p>
","<p>No GoPro is appropriate for the job by itself.  Any of them (or even a basic webcam) could work as a camera, your problem is that you need something that can encode and send a stream by itself.  That is a specialized device and not something a GoPro can do.  You would want a either a device built for doing a direct stream (such as an IP camera) or you would want to have a camera with a clean HDMI (or other) output that can be fed in to a stand alone streaming device.</p>

<p>For the second half, recording quality and stream quality are completely and 100% independent of each other.  They are both two different captures of the same visual information, but the sensor is going to pick up maximum quality regardless of if you are recording in some low bandwidth format or not and the steam is going to run off the output which should be the high quality format regardless of what quality you are streaming at.</p>
","12735"
"What CRF or settings I should choose for h265 in order to achieve a similiar quality of h264?","11164","","<p>I'm doing some experiments with HEVC x265.</p>

<p>I have a raw footage and the quality of details of h264 with same CRF setting looks better than h265.</p>

<p>Shouldn't be the opposite?</p>

<p>Maybe my setup isn't the best: I'm using ffmpeg for transcoding and vlc for review the videos, then i copy the screen content and compare the screens on a program like photoshop.</p>

<p>FFmpeg commands I using are the following:</p>

<pre><code>ffmpeg -i input.mp4 -c:v libx264 -crf 30 -c:a copy output_h264.mkv
</code></pre>

<p>and</p>

<pre><code>ffmpeg -i input.mp4 -c:v libx265 -crf 30 -c:a copy output_h265.mkv
</code></pre>

<p>I used 30 as CRF for testing purposes because the artifacts are more visible :)</p>

<p>Could the loss of quality be caused by VLC and its experimental support in deconding h265? Maybe something more visible at lower bitrates?</p>
","<p>The CRF scales for x264 and x265 do not correspond. x265 CRF 28 is supposed to be e<a href=""https://trac.ffmpeg.org/wiki/Encode/H.265"" rel=""nofollow"">quivalent</a> to x264 CRF 23. But x265 is not yet as mature in its development as x264, so take that CRF equivalence with a pinch of salt. </p>

<p>That said, you can try to establish your own calibration between the current versions of the encoding libraries in your ffmpeg by running the following command, which executes two popular video quality metrics:</p>

<pre><code>ffmpeg -i encoded-video.mp4 -i reference-video.mp4 -lavfi ""ssim;[0:v][1:v]psnr"" -f null -
</code></pre>

<p>The final lines of the console output will contain:</p>

<pre><code>[Parsed_ssim_0 @ 000000000039ad80] SSIM Y:0.984483 U:0.980458 V:0.980921 All:0.983219 (17.751712)
[Parsed_psnr_1 @ 0000000000398320] PSNR y:42.63 u:43.19 v:44.09 average:42.90 min:42.07 max:46.16
</code></pre>

<p>So, run the command once with the x264 output and once with x265 and compare with different x265 outputs, till you get similar measures. Of course, <a href=""https://en.wikipedia.org/wiki/Video_quality#Examples"" rel=""nofollow"">these metrics</a> aren't perfect but you can use them as a rough guide to establish equivalence.</p>
","16668"
"After Effects Not Rendering Transparency (Quicktime Movie)","11074","","<p>I have a comp that I made that is a write-on effect.  I have selected the alpha channel toggle so in my view, I see the checkered background. </p>

<p>When I queue to render, I select <em>lossless + alpha (I've also tried custom RGB + alpha)</em> with <strong>quicktime</strong> selected and it renders a <strong>white background instead of transparent one</strong>. I had selected the white background, but transparency was toggled when I did the export, and so was the lossless + alpha. </p>

<p>Please help. </p>
","<p>Did you try re-importing the rendered video into After Effects? You should definitely have transperency with these settings.</p>

<p>If you only tried to play this back as a webm video in chrome and safari you won't see transparency, neither browser is supporting webm with transparency right now. Only Googles Cannary project offers this future at the moment.</p>

<p>As Joonas mentioned in the comments, the <a href=""https://github.com/m90/jquery-seeThru"" rel=""nofollow"">jquery-seeThru</a> project is offering a workaround for that.</p>
","12244"
"My converted videos look bad... what am I doing wrong?","11025","","<p>I am a software developer. I understand the video concepts, but I am no expert.</p>

<p>I have a Sony Action Cam AS-15. It records ate 1080/60fps/25Kbitrate. The 60FPS videos just dont; play smooth maxemised on my pc, xbox, ps etc.</p>

<p>I am looking for the best way to drop some bitrate, but keep decent video quality.</p>

<p>I have tried numerous converters/settings, but I just cant get a decent quality video going... I can, for instance, in no way, get videos to look as good as the uploaded videos for the Action Cam, on the Sony action cam youtube channel, for instance. Not even to mention nearing in any way the gopro uploaded video quality.</p>

<p><strong>Sony Youtube Video:</strong> 
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/A3H-xgWfAg8?start=0""></iframe>
            </div></div></p>

<p><strong>My Youtube Videos:</strong></p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/hWuMZU8EOXg?start=0""></iframe>
            </div></div></p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/C60gBZk6TvQ?start=0""></iframe>
            </div></div></p>

<p><strong>This Last upload was:</strong> 13 minutes, 1GB, 720p, 30fps, 9259kbps, MP4 (Windows Movie Maker), Gopro Black, Sony Action Cam, Fuji xp-100 cameras</p>

<p>The little AS-15 is not as good as the gopro, but the ""professional"" post videos uploaded to Sony's channel, look damn good. I am not talking about saturation, effects etc. I am talking purely about (pixelation/artefacts)</p>

<p>The original 1080/60 videos look pretty good, but I cant keep them looking good in a more usable format... I would like to keep 60fps, but just lower the bitrate.</p>

<p>I have tried: </p>

<ul>
<li>Any Video Converter Software, convert to 30fps/24kbitrate</li>
<li>GoPro Studio (Best quality)</li>
<li>Keep high original bitrate, converted to 720p</li>
<li>.h264</li>
<li>.mp4</li>
<li>I don't really want to buy Vegas Pro, or Adobe premier, as this is a small hobby.</li>
</ul>

<p>I see decent quality movie (feature films) encodes, running at 1080p, with only 2K bit-rate...? Which makes for a decent file size of 1.5gb/90 minutes. My encodes are 1GB for 15 minutes, and look just terrible.</p>

<p>What am I missing?</p>

<p>P.S. I know from research, and observation, that the Sony AS-15 totally messes up with video compression. There are quite a lot of artefacts on very detailed scenes... Can this be the problem, that the source just isn't good enough to convert / work with?</p>

<p>If this is the case, are there any software to keep the original file bit-rate and FPS, but just scale down to 720p, in order to keep the initial quality as it was recorded, but make it a bit more usable..</p>
","<p>Taking a closer look at the footage now that I'm at home, it's mostly the level of activity and motion in the shots.  Even in the Sony video, the quality absolutely falls apart any time that there is high motion and unstable video.</p>

<p>The way video compression works, it depends on predictable and smooth motion to achieve good compression results, especially for on the fly recording like a camera has to do since it can't look forward in time and see how things are going to move any further than it can buffer, which is generally not going to be very far (probably sub-second).</p>

<p>There are two places you could get quality loss.  The first is in the recording itself.  If you aren't recording at sufficient data rates, then if there is too much motion, fine detail is going to get lost.  Uncompressed video is absolutely massive and even most video cameras record with moderate to heavy compression already applied.  If your source footage lacks detail (and isn't out of focus) then you need to work on stabilizing the camera to ensure it can better compress the video.  There are a number of different ways you can shock mount a camera to help provide some stabilization, though they also generally add weight.</p>

<p>If the source footage is ok, then the problem lies in the final compression for web/end-user consumption.  The fundamental problem is still the same, but we now have the option of using digital video stabilization to help make movement less random.  We also can use VBR 2-pass compression since we can now look ahead and see how thing move in the future as well as the past, which allows for far more accurate compression and more efficient use of bandwidth.</p>

<p>Additionally, some of the footage could use color grading to enhance the contrast.  Ideally footage should make full range of luminosity (brightness).  Your blacks should be dark black, but without obscuring detail in the shadows.  Your whites should be bright, but without fine detail bleeding in to pure white.  You can start with a white point and black point, but really, using something like curves is the best way to tweak the video to make the most use of the available range of luminosity.  This can also be quickly, but sloppily accomplished by increasing the contrast, but results often look far more artificial that way.  Color grading is best learned by doing it and it can vary throughout a clip.  It takes time and practice to get good at it.</p>
","10736"
"After effects: masking not working in select compositions","10953","","<p>Whenever I try to mask a clip or multiple clips in my main composition, the mask has no effect, neither does inverting the mask or any of the other mask modes provided. The same thing happens when I pre-compose a clip from my main composition. Masks do still work on adjustment layers though.</p>

<p>However if i take the same clip from my clips in the project section and create a new composition with it, masking seems to work fine.</p>

<p>I don't know if I have disabled a setting in my main composition that would have caused this (I can't find any difference between my main and any new compositions), and it seems that nobody else has had this problem or that the answer is simplistic.</p>

<p>Let me know if you need anymore information. Any help will be greatly appreciated.</p>

<p>Regards</p>
","<p>Clips were originally twixtored, which prevented my masks from showing as twixtor has its own masks and splines.</p>

<p>Resolved this by first twixtoring my clips, pre-compsing the clip and then masking the composition</p>
","9182"
"FCPX => DaVinci Resolve => FCPX color grading round trip","10806","","<p>I'm trying to get my (non-graded) video from Final Cut Pro X through DaVinci Resolve 10.1 Lite, and back into FCPX for the last bits (audio, logos etc). The first part wasn't that hard, just export the FCPX project as XML, and load that from DaVinci Resolve. The color grading itself was a bit harder but still very doable.</p>

<p>The hardest part is getting the graded clips from DaVinci Resolve back into FCPX. I went to the <strong>Deliver</strong> page, set the <strong>Easy setup</strong> to <em>Final Cut Pro XML Round-Trip</em>, selected all the clips and selected a destination. After adding the job and running the job, I looked in the directory that was just created and saw a bunch of videos there and a <em>fcpxml</em> file. But importing that <em>fcpxml</em> file into FCPX just loads the clips from their original location...</p>

<p>So, how is this round trip supposed to work between Final Cut Pro X and DaVinci Resolve Lite?</p>

<p>I've spend the better part of today trying to figure that out, but all I can find are YouTube clips explaining how to do it with Premiere Pro or other video editors. And when I do manage to find something about Final Cut Pro X, it says I should just go to <strong>File</strong>=><strong>Export AAF, XML</strong>. Unfortunately that option is greyed out for me. And all the other docs are either refering to Resolve 8 or 9. Even the official documentation still refers to a <strong>Conform</strong> page which is long gone in Resolve 10, and replaced by the <strong>Color</strong> page. Too bad I can't find a way to export from there either :(</p>
","<p>Xudonax,</p>

<p>The preset Final Cut Pro XML Round-Trip renders the graded files in ProRes 422, but it uses an old XML file, not supported in FCPX. You need to go to the Edit (important!) section of DaVinci after rendering the project. Then you will be need to make File  Export AAF, XML... command and check the fcpxml version 1.3 type of file. It will be comfortable to save this five in the same folder used for rendered files in Deliver section.</p>

<p>After that you can do simple import of this FCPXML file in Final Cut Pro X, it will create new Event and Project in your current library with all your graded files.</p>

<p>Be sure to make render before XML-export. If you will export FCPXML ver. 1.3 without delivering, you will make the FCPX-project with original (non-graded) files.
Good luck!</p>
","11909"
"What is the maximum amount of memory Adobe Premiere Pro can efficiently utilize?","10789","","<p>From my experience with programming I know that just throwing more memory against even a well paralized program will not speed it up after a certain limit due to various practical limitations (tasks can only be split in so many parts, waiting==processing or countless of other scenarios). Is there a limit at which which Adobe Premiere Pro doesn't speed up significantly anymore? Does it make sense to build a PC with 128GB or 512GB of memory? (My colleague is considering building a PC with 128GB and I am just a bit skeptical of this being... worth it, though I might be absolutely wrong as this really is not my area of expertise (and just for the record, I know that it's a 64-bit application and that it can address all available memory up to windows' limit, what I am asking is about the practical world)).</p>
","<p>That entirely depends on what kind of footage he work with and how long his average cuts are. If we are talking hour long movies a bigger amount of RAM can be worth it.
Though 128GB would be completely useless if he only uses Premiere it just doesn't use that much RAM.
We built a high-end workstation once for huge 3D renderings that has 64GB RAM, the only times we utilize that fully is when working with gigantic 3D scenes in Cinema 4D or 3Ds Max. After Effects barley goes beyond 20GB unless you need an hour long RAM previews for some reason and Premiere is usually around 5-7GB even when cutting 60min plus movies.
So if hes only using Premiere, 128GB would be a complete waste of money. I would not recommend anything more than 32GB on a video editing only workstation.</p>

<p>When using Premiere a high-end GPU is a lot more worth performance wise, Premiere can render in realtime with a powerfull GPU making having a ton of system RAM a bit useless.</p>

<p>I can recommend using gaming cards instead of workstation cards, the only difference is support and drivers, the gaming cards are much more powerful and a lot cheaper. I have never experienced any stability differences in Adobe software with a Workstation card compared to a gaming card. A good mixture of gaming GPU and workstation GPU is the GeForce GTX Titan, its meant for CUDA development and other CUDA usage.</p>

<p>Alternatively in the latest Premiere version there is also full OpenCL support, so a high-end AMD card would also be a good choice.
F.e. The R9 290X or if he wants a workstation GPU for some reason the FirePro W9100 is currently the high-end model, though a waste of money in my opinion. You do not need 16GB Vram with Premiere. I would probably go for the W9000 or W8000.</p>

<p><strong>Edit:</strong> As AJ Henderson mentioned, footage gets almost never fully cached in RAM so a fast SSD or maybe an SSD array will be much more beneficial than more RAM aswell.
Going for fast disk I/O and a good GPU is a good combination nowadays. Having a high-end multi core CPU and lots of RAM is getting less and less important in the graphics world.</p>
","12561"
"How to fix Sony Vegas 12.0 blank screen render?","10697","","<p>I've got video recorded from my phone and I use Vegas to edit them, when I play the video back in VLC player the video orientation is ok, but when I import into Sony Vegas and it asks if I want to set project preferences to match the media, I have</p>

<ol>
<li><p>allowed the Vegas to match the project settings and the orientation changes to portrait. </p>

<ul>
<li>when this happens I change the project settings to make the project settings to landscape and then use track motion to straigten the video. But when I render, its always a blank screen with the audio playing in the back ground. </li>
</ul></li>
<li><p>not allow Vegas to match the project settings and the video shows portrait with the black strips on the side. </p>

<ul>
<li>I then use track motion to straighten the video but the render produces the same result. Blank screen with audio. </li>
</ul></li>
</ol>

<p>I do not know why this is happening, how would I set up the import and project preferences so that the movie renders properly?</p>

<p>Note: All videos are mp4 format. And the media offline checkbox doesn't make a difference, checked or unchecked.</p>
","<ol>
<li>Import the file into the project media box,</li>
<li>I right click the video and go to orientation</li>
<li>change it to 0 (original)</li>
</ol>

<p>Should render without any problems. </p>
","16897"
"Which sequence settings in Adobe Premiere for a YouTube video","10577","","<p>I have a series of photos 3840x2880 which I want to crop to make a 1080 timelapse video to be uploaded to YouTube.</p>

<p>Which sequence setting in Adobe Premiere CS6 I should use for YouTube videos?</p>
","<p>There are presets for Youtube specifically already in Adobe Media Encoder.  I would suggest using one of them.  You can also find the technical recommendations from Youtube on how to encode the videos <a href=""https://support.google.com/youtube/answer/1722171?hl=en"" rel=""nofollow"">here</a>, however I believe the presets already conform to these recommendations.</p>
","9038"
"AE - Imported mp4 seems to be missing audio","10576","","<p>I'm new to After Effects, but my experience with video editing so far has been that audio and video are treated separately. When I add an mp4 to my composition, I get the video element, but I can't figure out where the audio is at. When I hit play on the preview toobar, the audio does not play. I checked to make sure ""mute audio"" wasn't enabled.</p>

<p>I'm sure it's something simple, I just need to be pointed in the right direction.</p>
","<p>If you just play the comp it won't play audio. You need to do a RAM preview. Hit <kbd>0</kbd> (zero) on the numeric keypad or <kbd>Ctrl</kbd>+<kbd>0</kbd> if you're on a laptop. </p>

<p><kbd>.</kbd> (period) on the numeric keypad (or for laptop <kbd>ctrl</kbd>+<kbd>.</kbd>) plays just the audio from wherever the playhead is.</p>

<p>If your footage does have audio, you will see the waveform in the preview thumbnail in the project window, and the item description will tell you the audio format, as below (in this case 44.1kHz, 16 bit unsigned pcm, stereo audio)</p>

<p><img src=""https://i.stack.imgur.com/kVdoR.png"" alt=""AE project window showing clip with audio""></p>

<p>If the clip has audio and you bring it into a compositon you should see a little speaker icon on the layer with the audio, next to the eye icon, on the left.</p>

<p><img src=""https://i.stack.imgur.com/tYbck.png"" alt=""speaker icon highlighted""></p>
","3228"
"Creating a split screen video of a multitrack recording","10517","","<p>It's fairly common to see videos in which, to accompany a piece of music created using overdubs, we see a split screen showing the artist performing each overdub.</p>

<p>What functions, in which software, may be used to create videos like this?</p>

<p>Is it a matter of mixing the audio separately, then manually synchronising the video, or is there software that automates the process?</p>

<p>Answers for any software are welcome, but budget answers (iMovie, Movie Maker) are welcome, as well as any software designed for this specialised purpose.</p>
","<p>In general you should be able to do that with any video editing software.</p>

<ul>
<li><p>Professionals use timecode-synced equipment for this job. Low budget productions do this with simple audio marks at the beginning and the end of a take. You can do this simply by clapping with your hands. But using a slate gives you a more significant mark (peak).</p></li>
<li><p>The main audiotrack is a separate mix. This is the reference. The videos will be synced track-by-track to this reference. Usually this will be done manually. But <strong>Final Cut Pro X</strong> can also do this automatically with great success. </p></li>
<li><p>The splitscreen itself is nothing special. Just adjust size, crop and position of a track or clip as you like. </p></li>
</ul>
","4156"
"scale an image inside a mask","10440","","<p>Pretty new to after effects. </p>

<p>I was wondering if anyone could tell me how to scale an image contained inside a mask.</p>

<p>Similar to photoshop. In Photoshop you create a mask and if you want to scale the image you simply unlink the mask and scale your image.</p>

<p>Can this be done in after effects?</p>

<p>This is what I have going on.</p>

<ul>
<li>Video playing in background</li>
<li>Created a new image layer in the bottom right position of the video to show a more in-depth example of what the person in the video is talking about.</li>
<li>Want to add a scale effect to the image layer contained in the mask</li>
</ul>

<p>When I try and scale the image it scales the mask. I only want the image to scale inside the mask.</p>

<p>Any help would be appreciated.</p>
","<p>After some serious Google time I found a solution.</p>

<p>Here is the <a href=""http://forums.creativecow.net/thread/2/931763"" rel=""nofollow"">link</a>, but just in case is gets removed this is what they said to do.</p>

<p>You want to use a track matte.</p>

<p>Create a mask on a solid layer that lies on top of the layer you want to cut out. Then click on the TrkMatte dropdown menu on your footage layer, select Alpha Matte.</p>

<p>Here's the layer order for you:</p>

<ol>
<li>Masked solid</li>
<li>Your footage / image - Apply Track matte and apply Alpha Matte</li>
<li>Background (if used)</li>
</ol>

<p>Now you can move your footage layer around and the mask won't move. </p>

<p>This worked like a charm.</p>

<p>If anyone has any better solutions, please let me know.</p>
","12531"
"Is there any plugin for After Effects to create a 'kinetic typography' video?","10402","","<p>I was looking at some kinetic typography videos - like the one below - and wondering whether there is a plugin for Adobe After Effects CS4 that can help with this. If not, then is there any other easy way to do this?</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/rIDdx7NPJgo?start=0""></iframe>
            </div></div></p>
","<p>As of 2011, there aren't plugins that assemble a typography video for you, because this kind of work is really a sub-topic of motion graphics.  There are tools that can help you with camera animation and rigging in AE though.</p>

<p>My favorite is Video Copilot's <a href=""http://www.videocopilot.net/tutorials/sure_target_2/"">Sure Target 2</a>, which lets you think about the animation in terms of what you're looking <strong>at</strong>, instead of thinking about it in terms of camera perspective.  This makes the work feel similar to using something like Cinema4D's target camera.</p>

<p>Another noteworthy tutorial: <a href=""http://ae.tutsplus.com/tutorials/motion-graphics/design-rhythmic-motion-typography-in-after-effects/"">http://ae.tutsplus.com/tutorials/motion-graphics/design-rhythmic-motion-typography-in-after-effects/</a></p>

<blockquote>
  <p>If not, then is there any other easy way to do this?</p>
</blockquote>

<p>Not really.  Watch <a href=""http://www.marcofolio.net/video/15_stunning_motion_typography_videos.html"">lots of videos</a>, take notes on ideas you like, steal the best, and <a href=""http://the99percent.com/articles/6977/Rainer-Maria-Rilke-Trust-In-What-Is-Difficult"">trust in what is difficult</a>.</p>
","1831"
"How to add motion blur in timelapse videos?","10372","","<p>I remember watching a video about timelapse photography. There was a moment when the narrator explained that there is a specific trick to make the videos ""smoother"":</p>

<ul>
<li><p>In the timelapse videos which don't use this trick, if people are moving, they suddenly appear at a precise frame, then disappear,</p></li>
<li><p>In the videos which use the trick, we have a feeling that they are smoothly moving, like if there was a motion blur.</p></li>
</ul>

<p>What is this technique? What is it called? Is it done when taking photos or during post-processing?</p>
","<p>After Effects has a <strong>frame blending</strong> mode called ""<strong>Pixel Motion</strong>,"" which will attempt to match features in two temporally adjacent frames and smoothly blend between them. <a href=""http://www.youtube.com/watch?v=Ohi-X7JKwVA"" rel=""nofollow""><strong>This video</strong></a> shows it in action, where it is used to generate filler frames for video footage that was slowed down 10x. As you can see, the results are mixed.</p>

<p>There is no magic bullet for filling in missing data. As the classic computer programming expression says, ""Garbage in, garbage out."" If you only need to blend between small changes in the scene between frames, this blending mode might help. Otherwise, your best bet is probably to just do a very fast plain fade between each image, which can be achieved with the ""Frame Mix"" blending mode.</p>
","2332"
"Is it possible to convert output of TV Channel (set top box 720p) to 1080p?","10191","","<p>I have Toshiba 29PB200ZE (just purchased). Now the problem is it displays pixels with very blur effect. Reason is that the Cable operators in India (using Standard Channel Broadcasting) broadcasting 720p resolution output. As I have 1080p HDTV it does not displays output properly. Means that tiering the pixels.</p>

<p>Is there any converter available that converts 720p output from Channel (set top box) to 1080p so that I can view output properly.</p>

<p>If I go for HD Channels then issue will not be arised. But as HD channels have very high price range so could not afford them.</p>

<p>Edit: I am using 3 pin (red, yellow and white) cable which connects Set Top Box (digital signals) and my HDTV Toshiba 29PB200ZE. Is performance can be improved by installing other higher density cable like Coaxial?</p>
","<p>What you are talking about is upscaling and any current HDTV will do upscaling automatically.  Upscaling doesn't work miracles though, it will only make it so that the lower quality signal can be watched on a higher quality display.  It just multiplies the pixels so that a 720 by 480 (.9 pixel compressed) signal for example doesn't end up only taking up 1/4 of your screen.  It doesn't make the video any higher quality (it in fact will be slightly lower quality than if you were to watch it at the native resolution).</p>

<p>Your problem however is not lack of upscaling.  Your problem is using the wrong signal.  You are currently connecting your cable box to your TV through what is known as a composite cable.  It's an old type of analog connection that includes all the video information on the single yellow pin and sends left and right audio on the white and red pins respectively.  It is only capable of reproducing a standard definition signal so you are not actually using HD signals.</p>

<p>To benefit from 720 or 1080 signals, you will have to either use analog Component video (the red/green/blue connectors) or use an HDMI cable.  It's worth noting that as long as your TV and cable box both have component inputs/outputs, you can actually re-purpose the existing Composite cable to carry a component signal, but there will be no audio (you need a separate white/red pair for the audio, which is the same format as with the composite signal).</p>

<p>Once you fix the cable and hookup issue, you should see better image quality.  Personally, I'd recommend using HDMI if it is available on both your cable box and TV as that is the easiest to setup and will give a nice digital signal.</p>
","7639"
"Adobe Premiere Pro CS4 - Looping a clip","10166","","<p>I'm trying to get a clip to loop on my video for its entirety. (The clip is the water mark.)</p>

<p>I can't really copy paste it everywhere, it would take hours (the clip is 10 seconds, on an hour long vid.)</p>

<p>So is there some way to do it? To loop a video so it will play for the entirety of the video?</p>
","<p>Is the watermark a video or a still image? If it's a still you can usually just increase its duration. </p>

<p>Otherwise, copying and pasting is the best way to go about it. Copy and paste it 10 times, then select the 10 clips, copy and paste that 10 times, then select the 100 clips, copy and paste that etc. until you get the desired duration. </p>
","2842"
"How to force After Effects to interpret footage at a specific framerate?","10139","","<p>I just started using after effects CS4, and created my first video.
But if i look at my final (rendered) video, its 2x the speed it was originally.
I recorded a 22 second clip, and traced etc.
Looked awesome, but after rendering, its only 11 seconds, so 2x speed.
Here's the video
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/0TFStZlongU?start=0""></iframe>
            </div></div></p>

<p>The Cam is a Logitech C920 Pro HD Webcam</p>

<p><strong>EDIT</strong></p>

<p>I found whats wrong, but don't know how to fix it. The recorded videos are 15 fps, but AE imports them at 30, how do i change that? Immediatly when i import, its speed up</p>

<p>How can i make it so it goes normal speed?</p>
","<p>To answer your edited question:</p>

<p>If your footage is 15fps and After Effects thinks it's 30fps, you need to tell After Effects to re-interpret the footage.<br>
You do this as follows:</p>

<ol>
<li>Open the Project panel (by default on the left of your screen)</li>
<li>Right click on your footage and go to <code>Interpret Footage -&gt; Main...</code> as shown below.
<img src=""https://i.stack.imgur.com/JehOz.png"" alt=""enter image description here""></li>
<li>In the screen that opens up input the desired framerate (15fps in your case) in the textbox next to <code>Conform to framerate</code>. Press OK.
<img src=""https://i.stack.imgur.com/z7Yy0.png"" alt=""enter image description here""></li>
</ol>

<p>Now After Effects should know that the footage is 15fps.</p>
","6991"
"What is the best way to clean the outside of audio/video cables","10108","","<p>What is the best way to clean a cable that has gotten sticky. I know the best thing to do is to not get them sticky in the first place, but after time they get dirty/sticky all by themselves.</p>

<p>Lets assume that there are no breaks or cuts in the outside of the cable.</p>

<p><strong>How would one best clean the outside of a cable of stickiness without damaging it more.</strong></p>
","<p>For delicate cables use dish washing soap and warm water with a clean rag. Do not tug or squeeze on the cable too hard and be extra gentle with anything that has twisted pair.</p>

<p>For not so delicate cables rubbing alcohol on a clean rag then applied should do the trick.</p>
","3306"
"How could I make a map zoom in effect?","10081","","<p>I'm the only one that knows how to use a computer in the family and so I've been asked to produce a video for a wedding. One of the main requirements for the video is that a map location must be zoomed on. Something similar to this:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/O8xxT1Mub5M?start=0""></iframe>
            </div></div></p>

<p>I guessed Google Earth would be the best tool for this, however it wasn't as simple as I first thought as the app takes a while to download map data and sometimes the camera location jumps when zooming in. I tried using the built in video tool within the mac Google Earth app to achieve the effect, but then discovered that you have to pay to export the video. Unfortunately the price is too steep for this one project...</p>

<p>How can I overcome this problem? Or are there any other methods I could try to achieve this effect? Would screen recording Google Earth be the right thing to do (potential copyright issues)? </p>

<p>Thanks in advance.</p>
","<p>You want images of varying degrees of zoom that you can align and then scale between.  You start from far out and as you zoom in, you fade in the image that is slightly closer (and aligned to the other image).  Once the closer image takes up the full screen, you remove the first image from the background and place the next level of zoom in the correct spot and start fading it in.</p>

<p>Repeat this process for each level of zoom you can get until you reach your target level of zoom.</p>

<p>It can be helpful to align and animate the images without any transparency and then add the transparency after.  This helps to ensure the proper look before you blend the images together.</p>

<p>Note that this requires a video editor capable of keyframe animation.  If you don't have one, then your options are far more limited and there may not be a good way to do it.</p>
","10027"
"FFMpeg Encoding and Core Usage","10030","","<p>We have developed an application to transcode source .mov files into .ogg, .mp4 and .webm output. It is currently running on AWS EC2 instance g2.8xlarge. It is working (yay!).</p>

<p>My question: Even though I am passing in <code>-threads 0</code> to the ffmpeg command (actually setting <code>ffmpeg.threads</code> configuration in <a href=""https://github.com/PHP-FFMpeg/PHP-FFMpeg"" rel=""nofollow noreferrer"">php-ffmpeg</a>),  the running process is sometimes only being executed on a single core. Why is this happening?  See below output from <code>htop</code> command:</p>

<p><img src=""https://i.stack.imgur.com/hrJm8.png"" alt=""htop output""></p>

<p>As you can see, Core #21 is maxed out.  In a few seconds, it's going to switch to another one, rather than max out all of them like I would like and greatly speed up my encoding process.  The situation is transient; during some runs all of the processors <em>are</em> maxed out, but during others they are not and we only get use of the one processor.  A colleague mentioned that perhaps the codec we're using for some of the formats don't support multi-threaded execution during encoding, though I can't verify that is the behavior I'm observing yet.</p>

<p>Is this the case?  If so, what codecs for the formats above will allow us to transcode into these target formats while taking advantage of all of our available hardware?  The default codecs set for php-ffmpeg are below;</p>

<pre><code>         Video        Audio
Ogg    libtheora    libvorbis
WebM   libvpx       libvorbis
X264   libx264      libfaac
</code></pre>

<p><strong>Update</strong></p>

<p>Looking at the running processes, below is what winds up being the ffmpeg command that is run for an MP4 (currently saturating all 32 cores):</p>

<pre><code>ffmpeg -y -i my-video.mov -async 1 -metadata:s:v:0 start_time=0 -async 1 -metadata:s:v:0 -async 1 -metadata:s:v:0 start_time=0 -async 1 -metadata:s:v:0 start_time=0  -s 1920x1080 -s 1920x1080 -s 1280x720 -threads 0 -vcodec libx264 -acodec libfaac -b:v 1200k -refs 6 -coder 1 -sc_threshold 40 -flags +loop -me_range 16 -subq 7 -i_qfactor 0.71 -qcomp 0.6 -qdiff 4 -trellis 1 -b:a 128k -pass 2 -passlogfile /tmp-ffmpeg-passes55ad0d0233f711zdrg/pass-44ad0d02340a8 my-video-1200.mp4
</code></pre>

<p>I'm not actually building this command directly, <code>php-ffmpeg</code> is, though I do believe I have at least a modest amount of control over what goes in to it (for instance, I have no idea why there's multiple <code>-metadata:s:v:0</code> entries at the beginning)</p>
","<p>BTW, this question might be better on stackoverflow, or maybe unix.stackexchange, or maybe serverfault.  This site is I think less focused on questions that don't involve decisions based on creative merit or at least perceptual video / audio quality.  However, I'm all about the tech details, so I'll answer.</p>

<p>FFmpeg uses multi-threading by default, so you prob. don't need <code>-threads 0</code>.  If your encode is bottlenecked on a single-threaded filter or decoder, you'll see full load on one core, and light load on many other cores.</p>

<p>One thing you can do is check mediainfo of your output video.  x264 leaves its settings in an ASCII string in the h.264 header.  So either <code>strings -n20</code> or <code>mediainfo</code> to get:</p>

<pre><code>...
Chroma subsampling                       : 4:2:0
Bit depth                                : 8 bits
Scan type                                : Progressive
Bits/(Pixel*Frame)                       : 0.051
Stream size                              : 455 MiB (89%)
Writing library                          : x264 core 146 r2538+1 d48ec67
Encoding settings                        : cabac=1 / ref=6 / deblock=1:0:0 / analyse=0x3:0x133 / me=umh / subme=10 / psy=1 / psy_rd=0.70:0.10 / mixed_ref=1 / me_range=24 / chroma_me=1 / trellis=2 / 8x8dct=1 / cqm=0 / deadzone=21,11 / fast_pskip=1 / chroma_qp_offset=-3 / threads=4 / lookahead_threads=1 / sliced_threads=0 / nr=50 / decimate=1 / interlaced=0 / bluray_compat=0 / constrained_intra=0 / bframes=5 / b_pyramid=2 / b_adapt=2 / b_bias=0 / direct=3 / weightb=1 / open_gop=0 / weightp=2 / keyint=250 / keyint_min=25 / scenecut=40 / intra_refresh=0 / rc_lookahead=60 / rc=crf / mbtree=1 / crf=22.5 / qcomp=0.60 / qpmin=0 / qpmax=69 / qpstep=4 / ip_ratio=1.40 / aq=3:0.60
Color primaries                          : BT.709
Transfer characteristics                 : BT.709
Matrix coefficients                      : BT.709
</code></pre>

<p>Note the ""threads=4"" in there.  I think I manually set that on my quad core i5 2500k, instead of letting x264 use the default CPUs * 1.5, since I had a CPU-intensive filters (hqdn3d and lanczos-downscale) running.</p>

<p>Anyway, libx264 with a preset like <code>slower</code> should have <em>no</em> trouble keeping a lot of cores busy.  There are some parts of encoding that are inherently serial (e.g. the CABAC encode of the final bitstream), so a high-bitrate video that doesn't spend a lot of CPU-time refining references (high <code>subme</code>) to multiple frames (high <code>ref</code>) might show a load pattern like yours (one thread using 100% CPU, others not).</p>

<p>I'm not 100% sure about faster presets being less parallel, but I know CABAC is serial.</p>

<p>To get massively parallel, libx264 could use a boatload of RAM to keep frames around, and keep doing lookahead for 2 or more GOPs, and encode those independently.  It doesn't have an option to operate that way, though.</p>

<p>One way to make use of MANY cores is to run multiple separate encodes in parallel, instead of just a series of single encode using all cores.  This only works if you have multiple input files that you want encoded separately.  You're trading off threading overhead vs. more memory capacity and bandwidth (with an impact on caching, unless this is on a multi-socket system with separate L3 and DRAM for each cluster of CPUs, <em>and</em> you have the processes pinned to cores so one encode is using the cores in one socket, and the other the other).</p>
","16004"
"MP4 / h.264 patent issues?","9833","","<p>I know this is not a legal forum but I'm assuming this is a daily issue for most people on this site since h.264 video (AVC / MPEG4 part10) is so ubiquitous (and there <em>is</em> a <code>legal</code> tag here).</p>

<p>I've read about patent issues with h.264 video, but am a bit confused about it. Is there any problem with creating a video (using ffmpeg or any other tool) for commercial use? Or is the problem only for anybody writing software that creates this video type?</p>
","<p><strong>IANAL</strong>, but as far as I understand it, if you're charging viewers for h.264 / MPEG-4 AVC content you do need to pay license fees. Even though x264 / ffmpeg are Free with a big F, they are just software libraries for encoding video streams into the H.264/MPEG-4 AVC format, which <em>is</em> covered by the MPEG patent. But the threshold for when fees are applicable is fairly high.</p>

<p>According to <a href=""https://en.wikipedia.org/wiki/MPEG_LA"" rel=""noreferrer"">MPEG-LA</a> - the license holders for H.264 / MPEG-4 AVC, the <a href=""http://www.mpegla.com/main/programs/avc/Documents/avcweb.pdf"" rel=""noreferrer"">license fees applicable</a> are:</p>

<blockquote>
  <p><strong>Where End User pays for AVC Video:</strong></p>
  
  <p><strong>Subscription (not limited by title)</strong></p>
  
  <ul>
  <li>100,000 or fewer subscribers/yr = no royalty;</li>
  <li>>100,000 to 250,000 subscribers/yr = $25,000;</li>
  <li>>250,000 to 500,000 subscribers/yr = $50,000;</li>
  <li>>500,000 to 1M subscribers/yr = $75,000;</li>
  <li>>1M subscribers/yr = $100,000</li>
  </ul>
  
  <p><strong>Title by Title</strong></p>
  
  <ul>
  <li>12 minutes or less = no royalty;</li>
  <li>>12 minutes in length = lower of (a) 2% or (b) $0.02 per title</li>
  </ul>
  
  <p><strong>Enterprise cap:</strong> 
   - $3.5M/yr 2006 - 07,
   - $4.25M/yr 2008 - 09,
   - $5M/yr 2010,
   - $6.5 million per year 2011 - 2015</p>
</blockquote>

<p>They also stated that they: </p>

<blockquote>
  <p>will continue not to charge royalties for Internet Video that is free to end users (known as Internet Broadcast AVC Video) during the entire life of this License.</p>
</blockquote>

<p>So if you're putting it on the internet free you don't need to pay them license fees, ever.</p>

<p>There's other fee schedules for software developers (though Cisco have released free source code and binary implementations of h.264 encoding software and have said they will <a href=""http://blogs.cisco.com/collaboration/open-source-h-264-removes-barriers-webrtc"" rel=""noreferrer"">not pass on</a> any licensing charges for it).</p>
","14699"
"How to render on another computer using Adobe Media Encoder CC","9817","","<p>I use a macbook air (1,7Ghz i7, 8gb RAM) to edit my Premiere projects. Rendering blows CPU to the max. I also have a MacMini fileserver (2,6Ghz i5, 8gb RAM) which could be used for rendering jobs.</p>

<p>Is it possible to allocate rendering jobs from Premiere Pro CC on my Macbook Air to the MacMini?</p>
","<p>No, not from one computer to another, you'd have to copy the project files onto a server, NAS device, or external HDD and then load it on the MacMini. You cannot from your Air directly say hey PP on the MacMini let's do this unfortunately.</p>

<p>Now, one option would be network rendering. The thing about it is, both computers are required to have exactly the same project files which means it has to be hosted on a server or some sort of NAS device (costly, and won't benefit your setup that much), some frames get rendered on one, while frames get rendered on the other (both computers will be actively involved, so most likely you couldn't work on other projects).</p>

<p>PP does not support network rendering, so the file PP CC file would have to be imported to After Effects.</p>
","16143"
"How to limit file size with ffmpeg?","9779","","<p>I want to encode a video, that has a length of 60 seconds, to a <strong>target or maximum size of 10 MB</strong>.</p>

<p>There are two approaches I know of. One is explained in the FFMPEG-Wiki, and the other one I found in the documentation. Unfortunately, I haven't found an explanation on when to use what method.</p>

<p>Is any of those methods recommended? If not, what are the upsides/downsides of each method?</p>

<p>1) Calculate and set Bitrate to match the length of the video as explained in the <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264#Two-PassExample"" rel=""noreferrer"">ffmpeg-wiki</a></p>

<blockquote>
  <p>(10 MB * 8192 [converts MB to kilobits]) / 60 seconds = ~1365 kbits/s total bitrate
  1365k - 128k (desired audio bitrate) = 1237k video bitrate</p>
</blockquote>

<pre><code>ffmpeg -y -i input -c:v copy -preset medium -b:v 1237k -pass 1 -c:a copy -b:a 128k -f mp4 /dev/null &amp;&amp; \
ffmpeg -i input -c:v libx264 -preset medium -b:v 1237k -pass 2 -c:a libfdk_aac -b:a 128k output.mp4
</code></pre>

<p>2) Use the <code>-fs</code> <a href=""https://ffmpeg.org/ffmpeg.html#Main-options"" rel=""noreferrer"">parameter</a> and let ffmpeg figure it out.</p>

<pre><code>ffmpeg -i input -c:v copy -c:a copy -preset medium -crf 23 -fs 10485760 output.mp4
</code></pre>
","<p>The <code>fs</code> parameter will stop the encode once it hits its value. So, if the output hits the <code>10MB</code> mark while encoding the 15th second, then that's the duration of your output file.</p>

<p>If you want to make sure that the entire file is encoded but it doesn't cross the set target size, then use the bitrate method. To accommodate the muxing overhead and other data within the target size, set your video bitrate slightly lower by, say, 1-2%</p>
","17268"
"FFMPEG convert video with multiple audio streams?","9716","","<p>Ok so I have a video made from dxtory. The video is using dxtory video codec and unknown audio codec. When I pop the file into avimux, I can clearly see two audio tracks/sources. I have successfully converted the video without loss of quality with the following.</p>

<pre><code>ffmpeg -i ""The Bunker.avi"" -vcodec h264 TheBunkerMix.mp4
</code></pre>

<p>But, the resultant file only had one track/source. Basically I recorded game footage and the original file had both the in game sounds as well as the mic recording commentary on top. But, when I did the conversion my audio commentary track/source got lost in conversion. What other options can I include to get my audio tracks?</p>

<p><strong>EDIT 2:</strong></p>

<p>I've gotten close now, but still not right. I have used the following but again its seems to ignore the first -map option. If I flipped which map comes second, that is the stream that is audible in the output. How do I mux them together so they both play?</p>

<pre><code>ffmpeg -i ""The Bunker.avi"" -map 0:0 -map 0:1 -map 0:2 -vcodec h264 -acodec:0 mp
 -acodec:1 mp3 TheBunkerNwMix.mp4
</code></pre>

<p>This is the stream mappings so I have no idea why it isn't mapping correctly?</p>

<pre><code>Stream mapping:
  Stream #0:0 -&gt; #0:0 (dxtory -&gt; libx264)
  Stream #0:1 -&gt; #0:1 (pcm_s16le -&gt; libmp3lame)
  Stream #0:2 -&gt; #0:2 (pcm_s16le -&gt; libmp3lame)
</code></pre>
","<p>If you copied your command there then your issue is probably <code>-acodec:0 mp</code>. That should be <code>-acodec:0 mp3</code>.</p>

<p>You can just use <code>-c:a mp3</code> to apply the encoding options to all audio tracks at once, no need to apply them for each individually.</p>

<p>Other than that your mapping is correct and shouldn't be the issue.</p>

<p>Edit: To conclude the discussion in the comments regarding the actual reason behind the question: When it comes to editing it is no problem to just transcode the video stream if the source codec makes problems and import the audio from the source file by either muxing the audio into a new container with the transcoded video stream or just importing the transcoded video and audio streams separately (by demuxing the source) into your editing program and skip on the container format.</p>
","12058"
"Repeat a clip down entire timeline","9564","","<p>Is there a better way to take a short video loop (in my case a video watermark) and duplicate it again and again down the full length of my sequence than my current workflow?</p>

<h1>Current Workflow</h1>

<ol>
<li>Place the clip in the sequence</li>
<li>Hold <kbd>Alt</kbd> down and drag a copy down the timeline</li>
<li>Select the new copy and snap it back against the first one</li>
<li>Select both clips</li>
<li>Holding <kbd>Alt</kbd>, drag a new copy of them down the line</li>
<li>Select all clips and repeat step 5.</li>
<li>Repeat 5 &amp; 6 until the clip is looped down the full length of the sequence.</li>
</ol>

<p>Any insight into a faster solution would be MUCH appreciated!</p>
","<p>This is a fast way to copy the clips in succession.</p>

<ol>
<li>Copy the video clip <kbd>CMD</kbd>/<kbd>CTRL</kbd> + <kbd>C</kbd></li>
<li>Target ONLY the track that the loop will be on</li>
<li>Then swiftly insert it multiple times by just holding down <kbd>CMD</kbd>/<kbd>CTRL</kbd> and pressing <kbd>V</kbd> multiple times in a row</li>
</ol>
","12341"
"Which sensor is better for video, cropped or full frame?","9419","","<p>I was watching a you tube video on cropped sensor and they the author mentioned that for video it does not really matter whether you use cropped sensor or full frame? </p>

<p>Does the sensor size matter for videography? What could be the physics behind this?</p>

<p>Thanks,
NN</p>
","<p>The video you were watching is simply wrong.  The size of the sensor makes every bit as much difference in video as it does in still photography.  Your depth of field and your light sensitivity are still largely determined by your sensor size.</p>

<p>The only thing that really doesn't apply for video is that diffraction limiting isn't a concern since video is so much lower resolution.  (1080p video is only around a 2MP image.)  This one thing makes crop bodies slightly closer to full frames for video, but still does nothing towards the other advantages that full frame has.</p>
","10846"
"How do I temporarily disable Davinci Resolve nodes?","9410","","<p>I'm new to Resolve (free version). As a programmer I kind of assumed that unconnected nodes, being unconnected, wouldn't interact with the process in any way. Apparently I was wrong.</p>

<p>What I'm trying to do here is find out an easy way to try out a completely different approach to grading the clip without losing the previous settings. </p>

<p><strong>In the screenshot:</strong>
3 isolated nodes containing the previous grading and 1 new node for trying out alternative grading. Even though the new node has very visible adjustments, I only see the original clip in preview.</p>

<p>I also tried removing all the previous nodes. Only then the new node begins to work, and as soon as I add another node that is of course initially unconnected, the preview returns to the clip's default state.</p>

<p><img src=""https://i.stack.imgur.com/LsM8C.png"" alt=""""></p>
","<p>-D will disable the currently selected node.  Resolve really doesn't like to have loose nodes floating around, so either disable them, or delete them.  But since you're talking about creating a new, experimental node structure, what you really want to do is create a new local version.  Right click on the clip thumbnail and select Local versions-> create new version.  This basically creates a copy of your node structure that you can return to whenever you like.  After issuing the command, you'll see that the name of the new version appears under the thumbnail to let you know which one you're working on.  To revert to the original (or select from any number of alternate versions), right click the thumbnail again and choose <em>versionName</em>->Load.</p>

<p><img src=""https://i.stack.imgur.com/74DcU.png"" alt=""clip thumbnail contextual menu""></p>
","10785"
"Adobe premiere is lagging in preview window","9342","","<p>Video in source preview is lagging but it plays fine in video players. What may cause this? Why premiere is not using standard codecs for source preview?</p>

<p>Specs:
Video is: h264 2704x1536 @29.97fps (GoPro footage)
System is: Core i7 2.3, 8GB RAM, GeForce GT 650M (MacBook Pro, bootcamp)
Adobe Premiere CS 6</p>
","<p>Premiere's preview isn't simply a viewer and can't make as much use of caching as it needs to be able to scrub quickly in any direction.  Interframe compression also trips it up for the same reason.  I believe it also processes the entire amount of data.  The video size you are using is pretty large and bandwidth intensive, so unless you are working from very fast disks (solid state) (or a fast CPU if it is heavily compressed) and have fast RAM to deal with the rate of uncompressed data, it is easy for the playback to fall behind.</p>

<p>The related questions talk a bit more about methods of getting around this (such as letting Premiere finish generating conformed versions of the video or reducing the resolution of playback so that not all pixels have to be rendered).</p>
","10362"
"Audio Not Playing on Imported Video Files","9136","","<p>I a currently editing a video and imported a .avi movie into my After Effects project. I went to check the RAM preview, but there was no sound. I checked the audio preview and there still was no audio. I checked the waveform and it showed a straight lines, showing that there was no audio being produced from the video.</p>

<p>I checked the video to see if there was sound playing and I determined there was. I also have another .avi file in the project and it plays audio with no problem. </p>

<p>Please help! Thanks in advance!</p>

<p><strong>Update:</strong> It works for some reason now... but now my characters are talking really fast and the video is going too fast! Help!!!</p>

<p>Fixed it! Thanks for the help!</p>
","<p>Check for a couple things, as there's more than one thing to cause this. First, make sure sure in your preview preferences that audio is enabled. Also, check that the duration isn't set to zero (default should be 30s, set it appropriately). Then RAM Preview with <kbd>0</kbd> on the number pad. You can scrub through audio only in the timeline by <kbd>ctrl</kbd>+<kbd>alt</kbd>+dragging (Windows) or <kbd>cmd</kbd>+<kbd>option</kbd>+dragging (Mac) to double-check existence of audio. You can also preview audio only by choosing Composition > Preview > Audio Preview.</p>

<p>More detail can be found at these two links:</p>

<p><a href=""http://help.adobe.com/en_US/aftereffects/cs/using/WS3878526689cb91655866c1103906c6dea-7ec9a.html#WSBFF42075-A759-4fd1-BAAE-5BA313E0923Ca"" rel=""nofollow"">Adobe help document on previewing audio and video</a>.</p>

<p><a href=""http://forums.adobe.com/thread/847056"" rel=""nofollow"">Adobe forum post about this issue</a>.</p>
","3955"
"How to mask an animated layer in After Effects?","9020","","<p><strong>After effects</strong> noob here. I'm working on an animation where a ball falls through a hole. I created a <code>Shape layer</code> to use as the mask, but I can't get it to mask the ball layer.</p>

<p>I've also tried adding a mask directly to the ball layer, but it moves with the rest of the animation. Is there a way to turn the mask on and off without affecting the rest of the animation?</p>

<p><img src=""https://i.stack.imgur.com/J717t.gif"" alt=""Ball not being masked""> </p>
","<p>I found this post helpful: <a href=""http://forums.adobe.com/thread/704283"" rel=""nofollow"">Preventing a Mask From Animating</a> </p>

<blockquote>
  <ol>
  <li>Create a comp size solid above your text layer.  </li>
  <li>Apply the mask to the solid.  </li>
  <li>Set the <strong>Track Matte</strong> setting of your text layer to <code>Alpha Matte</code>.</li>
  <li>The text layer will now adopt the alpha channel of the masked solid.</li>
  </ol>
  
  <p>You can freely animate the text and mask independently.</p>
</blockquote>

<p>I did not know the name of what I was trying to do. I needed to convert the mask layer into a <code>Track Matte</code>. </p>
","9660"
"De-Interlace footage in Avid Media Composer using standard effects","9011","","<p>What's the best way to De-Interlace footage in Avid Media Composer using fairly standard effects?  Assume the project media format and footage are interlaced.</p>
","<p>The <strong>Timewarp</strong> motion effect (adjusted with the Motion Effect Editor) can be used to deinterlace with the following settings:</p>

<ul>
<li>Type: <strong>Both Fields</strong></li>
<li>Source: <strong>Interlaced</strong></li>
<li>Check <strong>Adaptive Deinterlace Source</strong></li>
<li>Output: <strong>Progressive</strong></li>
</ul>

<p><img src=""https://i.stack.imgur.com/lMexb.png"" alt=""enter image description here""></p>

<p>Source: <a href=""http://community.avid.com/forums/p/99392/572313.aspx"" rel=""nofollow noreferrer"">http://community.avid.com/forums/p/99392/572313.aspx</a></p>
","2193"
"Correcting exposure flickering in time-lapse footage ""in post""","8966","","<p>I have made some time-lapse footage of a flower blooming. It was taken inside, at night, with constant lighting. The pictures are great, but as footage, the exposure between frames is slightly inconsistent (probably beacuse one of the lights was fluorescent), resulting in a ""filmic"" flickering effect. The effect is actually quite cool (in an early 20th century kind of way), but I want to see what it would look like corrected.</p>

<p>I had assumed that it would be pretty quick and easy to correct this sort of thing in After Effects. I've tried using Auto Contrast, Auto Levels, and Auto Color (with and without temporal smoothing enabled), but they don't seem to correct the flicker at all.</p>

<p>What I want is to equalize the total amount of ""light energy"" (sum of pixels values) per frame, using an exposure adjustment. Anyone know how I would go about doing this (in After Effects or other software)?</p>
","<p><a href=""http://www.granitebaysoftware.com/products/productgbd.aspx"" rel=""nofollow"">GBDeflicker</a> plugin for after effects does the trick</p>
","4362"
"How to convert an MTS file without loosing quality?","8780","","<p>I have shot some videos on a Panasonic v720 camcorder. The videos are huge. I used the internal feature of the camcorder to convert to MP4 and the quality was reduced very much.</p>

<p>What is the best practice to compress MTS videos?  What is the recommended output format (avi, mp4)? </p>

<p>My goal is just to consume less disk space and upload to Youtube (and have 1080p there) with the least quality loss.</p>
","<p>What is your definition of huge and what is your definition of high quality?  Size is directly related to compression and compression is directly inversely related to quality for the most part.  </p>

<p>Some amount of compression can be had for free using lossless compression or near free with more efficient pattern finding for lossy compression, but for the most part, there is a direct relationships between size and quality.</p>

<p>In fact, you generally have to actually increase the size to maintain the quality when re-encoding a lossy file as information that was already lost by the first can't be recovered, but other information may be lost due to the encoding process running again, so the combined error requires a higher data rate to avoid losing more.</p>

<p>MTS files are already generally based on H.264 (MPEG4) and are generally relatively small.  It should be possible to extract the MPEG4 from the container and repackage without any loss, but to reduce size would pretty much require a reduction in quality.</p>
","9231"
"After Effects: how to create a color via expressions","8770","","<p>Using After Effects expressions, I can create a color reference by pick-whipping a color control and the result is:</p>

<pre><code>var rgb = rgbToHsl(effect(""Color Control"")(""Color""));
</code></pre>

<p>But, I want to avoid using a color control, and simply instantiate a color through code:</p>

<pre><code>var rgb = rgbToHsl([159,27,41,1]);
</code></pre>

<p>...but this doesn't seem to work.  Is it possible?</p>
","<p>From what I'm reading, it looks like it may just be [R,G,B] where each value is a floating point number between 0 and 1.  I'm looking in the <a href=""http://blogs.adobe.com/aftereffects/files/2012/06/After-Effects-CS6-Scripting-Guide.pdf?file=2012/06/After-Effects-CS6-Scripting-Guide.pdf"" rel=""nofollow"">Adobe After Effects Scripting Guide</a>.  Every method I can find that accepts a color value simply uses an array like that.</p>

<p>Update: While certain scripting methods take a 3 parameter RGB value, other expressions require RGBA or HSLA.  In these cases, there will be a forth parameter for Alpha (ie, transparency) where 1 is fully opaque (a good default value) and 0 is fully transparent.  If you get an error with one format, I suggest trying the other and if you get unexpected values, check if the input expects HSL or RGB.</p>
","9329"
"Best way to stabilize footage?","8696","","<p>I'm working with footages that are shot handheld. The shakes are not too aggressive but they're still noticeable.</p>

<p>For the longest time ever I've been using Final Cut Pro's stabilization function to stabilize these types of footages, but the outcomes are always less than ideal. </p>

<p>What's a good way to deal with this?</p>
","<p>Because software stabilization methods cannot compensate for the shifting parallax of a shaking lens, they will always produce less than ideal results.  In my experience, FCPX's stabilization is just as good as other software packages (I have experience with Shake and After Effects as well), and they all basically work the same way.</p>

<p>This is probably not the answer you want to hear, but the best way to stabilize footage is to shoot stable footage in the first place.  Software stabilization is really just analogous to fine grit sandpaper; It's a good final step to add polish, but you'd never use it from the start.</p>

<p>Physically steadying a camera can be expensive, but that's why audiences have learned to appreciate stable shots -- they associate stability with quality.  But of course you know this, or you wouldn't be asking.</p>

<p>But there are good, reasonably priced ways to physically steady a camera.  Where I work, we have dollys, sliders, shoulder mounts, tripods, brushless motor gimbals, camera cranes, clamps, arms, and pretty much you name it.  But in my experience, the most versatile, portable, practical, and cheap piece of kit that we routinely use is a monopod.  It doesn't allow for all of the fancy movements of the other stuff, but nice ones will at least let you do a fluid pan.</p>

<p>Sorry this doesn't help with footage you've already shot, but I hope it helps with future projects.</p>
","10539"
"FFmpeg overlaying and blending videos","8605","","<p>I am trying to use FFmpeg to overlay one video on top of the other using an additive blend. One video is the actual video I want to transcode, the other is a ~10 second long video I want to put into one corner, sort of an animated watermark. The watermark video is some white animated stuff with a black background.</p>

<p>So far, I have tried something along these lines:</p>

<pre><code>$ /d/ffmpeg/ffmpeg.exe -i actualvideo.mkv -i myoverlay.mp4 \
-filter_complex ""[1:0] setsar=sar=1 [1sared]; [0:0][1sared] blend=all_mode='addition':repeatlast=1"" \
test.mkv
</code></pre>

<p>The <code>setsar</code> seemed necessary as otherwise it would complain:</p>

<pre><code>[Parsed_blend_0 @ 00000000043e0e40] First input link top parameters (size 1280x720, SAR 1:1) do not match the corresponding second input link bottom parameters (1280x720, SAR 0:1)
</code></pre>

<p>With the <code>setsar</code> there's no complaint, but the output video looks.. mostly pink. Both videos look fine when viewed on their own before I feed them into FFmpeg.</p>

<p>Any ideas? Am I doing something wrong?</p>
","<p>I found a helpful post on stackoverflow that addresses this issue: <a href=""https://stackoverflow.com/a/21400416/377875"">https://stackoverflow.com/a/21400416/377875</a></p>

<p>Apparently, it's a problem with color space. Something like this works:</p>

<pre><code>ffmpeg -i ""$1"" -i ""$2"" \
-filter_complex ""[1:0] setsar=sar=1,format=rgba [1sared]; [0:0]format=rgba [0rgbd]; [0rgbd][1sared]blend=all_mode='addition':repeatlast=1:all_opacity=1,format=yuva422p10le"" \
-c:v libx264 -preset slow -tune film -crf 19 \
-c:a aac -strict -2 -ac 2 -b:a 256k \
-pix_fmt yuv420p ""$3""
</code></pre>

<p>(substitute <code>$1</code> with your actual video, <code>$2</code> with your overlay, and <code>$3</code> with the output file name)</p>
","10644"
"How to encode apple ProRes on windows or linux?","8537","","<p>Is it also possible to encode video with Apple ProRes on windows and linux?</p>
","<p>Ffmpeg <a href=""https://ffmpeg.org/ffmpeg-codecs.html#ProRes"" rel=""nofollow noreferrer"">can encode video using ProRes</a>, and runs cross-platform</p>

<pre><code>ffmpeg -i input.avi -c:v prores -profile:v 3 -c:a pcm_s16le output.mov
</code></pre>

<p>will do the trick.</p>

<p>The <code>-profile</code> switch takes an integer from 0 to 3 to match the prores profiles </p>

<ul>
<li>0=proxy,</li>
<li>1=lt, </li>
<li>2=standard, </li>
<li>3=hq</li>
</ul>

<p>The Prores-ks codec allows you to encode prores 4444, (but there <strike> are </strike> were <a href=""https://trac.ffmpeg.org/ticket/5995?replyto=1#comment"" rel=""nofollow noreferrer"">problems</a> with encoding alpha channels in 4444 - <em>edit: Fixed as of August '17</em>)  see @vjgalaxy's answer for details on how to do it.</p>
","14715"
"Is it possible to export a Video with transparent Background? (No Image Sequences!)","8471","","<p>Is it possible to export a video file from After Effects with transparency?</p>

<p>I don't want to export to an image sequence.</p>
","<p>File > Add to Render Queue > Output mode > Format 'Quicktime' / Channels 'RGB + Alpha'</p>

<p>See if that works for you!</p>
","17184"
"Picture in picture for my face while recording video games","8467","","<p>What (or how) can I insert a box with a visual of my face while I am recording the game I'm playing? I currently use Hypercam but there is no option for this game/reality split screen.</p>
","<p><a href=""http://www.xsplit.com/"" rel=""nofollow"">Xsplit</a> is a popular software choice for people recording and/or streaming video games. It allows multiple inputs and can overlay video on top of video, as you'd need for the PiP effect. There is a small fee to purchase this, but would highly recommend it if you are serious about it.</p>

<p>Another popular solution is <a href=""http://obsproject.com/"" rel=""nofollow"">Open Broadcaster Software (OBS)</a> which is similar to Xsplit, but is completely free. Here is a <a href=""http://obsproject.com/forum/viewtopic.php?f=18&amp;t=402"" rel=""nofollow"">guide to using OBS</a>. When you setup a ""Streaming Profile,"" you can just set it to locally record.</p>

<p>With both services, you have the option to record footage locally (such as to edit/produce and then upload to YouTube) and/or stream directly to an online streaming service (such as Twitch or Ustream). </p>
","8821"
"Reduce File Size of Dxtory Video Without Causing Lag","8373","","<p>I'm making videos while playing on my computer. Everything is working well, however 15 minutes of video consumes 50GB, which is too much space. I use Dxtory with Lagarith Lossless Codec.  If I use the default Codecs I get lag.</p>

<p>How can I reduce this 50GB without causing lag?</p>
","<p>The problem with compression is that it is a double edged sword. It can reduce the space required to store video, however it also takes processing time to perform the compression.</p>

<p>When you are running a game, however, the CPU and GPU are already busy keeping the game running smoothly. In order to shrink the file size, you must either reduce the quality or increase the compression. If you increase the compression, the computer has to take more away from the game and you start seeing lag.</p>

<p>The only ways around this are to:</p>

<ul>
<li>a) reduce the resolution of the video you are recording.  Throwing
away information is free and will also make applying further
compression less processor intensive.  </li>
<li>b) move the encoding to another system.  Some recorders can send the data to another computer and have that computer perform the encoding.  I am not sure if Dxtory supports this or not though.</li>
<li>c) Continue to use the uncompressed files and transcode the video after you record a clip. </li>
</ul>

<p>Of these options, c is probably the easiest way to maintain maximum quality.  You can use a program like <strong>Handbrake</strong> to take the large file and compress it down to something of reasonable size, just so long as you don't have to record more than you have room for at a given time.  Note that this process can take quite some time depending on the quality level you choose and how fast your computer is.</p>
","10453"
"Adobe Premiere saves m4v instead of mp4","8359","","<p>I have problems with my Premiere Pro. It is first time this has happened, and I cannot understand why. </p>

<p>I am creating a timelapse from my images. When I want to export media, I choose format H.264 (not H.264 Blu-ray), dimensions 1920x1080. Output name is visible as ""Sequence02.mp4"". </p>

<p>After export is finished, there is a file ""Sequence02.m4v"" on disk. <strong>Why?</strong> What causes Premiere to change file format by itself? I used Premiere before and this never happened. Maybe I'm using some different setting, but what could cause this change?</p>
","<p>I encountered the same issue when I turned the sound off in the sequence.  When I re-enabled audio export, even though I didn't want to or need to, it saved as an mp4 again.</p>
","16531"
"Title not updating after creation in premiere pro","8336","","<p>I'm having this strange issue where I have created a title and when I try to update it (position or stroke) nothing happens. Either on or off the timeline, the title stays in its original position (off center) and it won't apply my changes.</p>

<p>Any ideas what could be causing this?</p>

<p>I'm running PP CC</p>
","<p>A simple restart did the trick. </p>
","10821"
"How to hold the last frame when using ffmpeg","8298","","<p>I'm cutting/converting some videos with ffmpeg.  What I'd really like is to be able to have the output hold the last frame for a few seconds (with silence for audio).  Is such a thing possible?  Or will I have to somehow get the last frame and make a new video from that?</p>
","<p>One method is to use the <a href=""http://ffmpeg.org/ffmpeg-filters.html#overlay"" rel=""nofollow""><code>overlay</code> video filter</a>. Assuming your video is 640x480, 30 seconds duration, 25 frame rate:</p>

<pre><code>ffmpeg -f lavfi -i nullsrc=s=640x480:d=35:r=25 -i video.mp4 -i audio.wav -filter_complex \
""[0:v][1:v]overlay[video]"" -map ""[video]"" -map 2:a -codec:a copy -shortest output.mkv
</code></pre>

<ul>
<li><p>I set the duration of the <a href=""http://ffmpeg.org/ffmpeg-filters.html#color_002c-haldclutsrc_002c-nullsrc_002c-rgbtestsrc_002c-smptebars_002c-smptehdbars_002c-testsrc"" rel=""nofollow""><code>nullsrc</code> source filter</a> to be 5 seconds longer than <code>input.mkv</code>.</p></li>
<li><p>The default behavior of overlay is to repeat the last frame of the overlaid source. See the <code>eof_action</code> option for other behaviors. </p></li>
<li><p>The audio in this example is being <a href=""http://ffmpeg.org/ffmpeg.html#Stream-copy"" rel=""nofollow"">stream copied</a> (re-muxed) instead of being re-encoded.</p></li>
<li><p>The downside is that this examples requires re-encoding since a filter is being used, but it may be simpler than other methods since it is just one command.</p></li>
</ul>
","10833"
"How to mirror a composition?","8230","","<p>I have a moving solid mask composition, which forms a white line that moves vertically from the middle to the left and back.</p>

<p>I now want to create a copy of that composition, mirror it and link the position. So that the mirrored comp will move from middle to right and back.</p>

<p>How could I achieve this? Setting the <code>width = -100%</code> did not work.</p>
","<p>Pre comp your layer, then apply the Mirror effect to the comp.  If your comps are 1920x1080, set the mirror centre to 960x540, and the reflection angle to 0.<img src=""https://i.stack.imgur.com/85RaD.png"" alt=""enter image description here""></p>

<p>I think your scaling to -100% didn't work because it's a masked layer.  You might have more luck using a shape layer to create your line, then you should be able to scale it as you suggested.</p>
","13436"
"What benefit would I get from using Apple Compressor instead of just exporting from Final Cut Pro X?","8197","","<p>I have Final Cut Pro X 10.1.4. I usually always use the ""share"" option to export my video and select whatever format I want to share with. Usually either ""Master File"" or ""YouTube"". I also see the option to ""Send to Compressor"". I researched Apple Compressor and I am trying to figure out if it is worth the money. Does it really export with better options than just what can be done with Final Cut Pro? Also, to convert to different formats I have used Miro Video converter and Handbrake. Does Apple Compressor have a much better system or output than either of these other video converters? Just want to know if it is really worth the $50 USD for Apple Compressor.</p>

<p>Thanks for any input.</p>
","<p>The export settings you are using in Final Cut Pro X are preset Compressor settings and the export itself is actually using the Compressor engine (but with no user control). Compressor allows you control over those settings and to save your own custom presets.</p>

<p>For instance, you may want to export an MPEG2 file with a particular bitrate, letterbox 16:9 footage in a 4:3 file, export to Vimeo with custom settings, or any other number of possibilities. If you wanted all of these outputs, you could batch process them. These, and other, codec/compression options can be configured in Compressor and saved as custom presets. Those saved custom presets then appear as available Share options in FCPX.</p>

<p>Compressor allows for encoding without requiring FCPX to be open, or for converting files that don't originate from a FCPX project among other benefits. It also provides a preview window that allows you to compare your source video to processed video before committing.</p>

<p>I still sometimes use Handbrake for h.264 files. It's very fast, clean, and free. In those cases, I will export a ProRes ""master file"" out of FCPX to bring into Handbrake.</p>

<p>Compressor is not a perfect program, but it is very useful. A lot of it depends on your workflow needs. If you like exporting directly from FCPX, but need custom share options, then Compressor is probably worth it  for you. If you are content with the presets or a workflow with other software, then perhaps not</p>
","13224"
"Rearange order of audio tracks in Premiere","8104","","<p>Let's say I have the following arrangement of my audio tracks:</p>

<pre><code>x ---------- 
y ---------- 
z ----------
</code></pre>

<p>and I want to have them arranged this way instead:</p>

<pre><code>x ----------
z ----------
y ----------
</code></pre>

<p>What's the easiest way for me to make this happen?</p>

<p>(As an example, in Reason, I would grab the audio track I wanted to move and then simply place it where I want it. This does not seem to work in Premiere.)</p>
","<p>As you have discovered Premiere Pro doesn't let you move the tracks as easily as that (I wish it did!)</p>

<p>Fastest method would be to <strong>create 1 or 2 empty tracks</strong> (right click on a track label and select <code>Add Tracks</code>) <strong>then use the Track Select Tool</strong>  ('A' on the Keyboard) to quickly select an entire track:</p>

<p><img src=""https://i.stack.imgur.com/ySPL7.png"" alt=""Premiere Pro Track Select tool""></p>

<p>(Click and hold lets you select and move the track at the same time).</p>

<p><em>You could also select an entire track, Cut it, move your other tracks around, then Paste it back into a new track (you can choose which track it gets pasted into by clicking on the Track labels, Premiere will paste into the top most highlighted track)</em></p>
","3920"
"Can a camera like Nikon D810 support 4K video with firmware update?","8075","","<p>Is it possible to update the firmware of a camera (like Nikon D810) to enable 4K video? Or, such features are built into the hardware?</p>

<p>Thanks, NN</p>
","<p>Yes and no.  Any camera with a sensor larger than 8mp (and the correct aspect ratio) is capable of recording 4k video frames.  Firmware can grab frames off the sensor as fast as the sensor can read out.  The ability to record video at a given frame rate is a limitation of the sensor's ability to read fast enough, the processor's ability to encode fast enough and the card reader that can write the data out fast enough.</p>

<p>Practically however, most cameras that don't have 4k are unlikely to have fast enough performance in all these areas to be able to hack 4k resolution in.  It's simply too much data to move and most cameras didn't design that kind of over-engineering to be able to deal with all 3 of those conditions.  Even the Canon 5D Mark iii, which is a high end professional DSLR which can be hacked to shoot RAW video at decent frame rates at 2 to 2.5k ends up having to be dropped down to around 12 or fewer FPS for trying to hack it in to shooting 3k or 4k video.</p>

<p>This is also partly compounded by the fact that most of the real time encoders are built for particular resolutions.  For example, the previously mentioned 5D Mark iii hack works by recording raw frames which takes a huge amount of data rate for storage.</p>

<p>So, in some cases, there may be ways to get some level of 4k support from a third party firmware, but for the most part, you are limited by what hardware was originally put in and often (most of the time) it simply isn't going to be possible to increase to 4k by third party hacks.</p>

<p>As for the Nikon D810 specifically, I'm not aware of any highly developed third party firmwares for Nikon bodies that are similar to Magic Lantern, so I don't think there is anyone really exploring that in depth at this time for Nikon bodies.  That series is similar in spec to the 5D mark iii, so it may be possible to get low framerate 4k video on very fast cards, but that's purely speculation.  I wouldn't count on seeing it any time soon or having it be highly usable though.</p>
","13021"
"Overlay an image onto live video feed","8014","","<p>I want to take a live video feed (say from a security camera) and overlay an image over the top of the live video. Is there hardware/software to do this realtime? I don't care about storing the data, and I can't store it to disk first, it needs to be live video in/out with overlay composited on top. Any suggestions?</p>
","<p>Gstreamer offers the ability to overlay images on videos and has also excellent live streaming capabilities.</p>

<p>Some useful links:</p>

<p><a href=""https://coaxion.net/blog/2013/10/streaming-gstreamer-pipelines-via-http/"" rel=""nofollow"">https://coaxion.net/blog/2013/10/streaming-gstreamer-pipelines-via-http/</a></p>

<p><a href=""https://developer.ridgerun.com/wiki/index.php/Fast_GStreamer_overlay_element#Picture_overlay_examples"" rel=""nofollow"">https://developer.ridgerun.com/wiki/index.php/Fast_GStreamer_overlay_element#Picture_overlay_examples</a></p>
","10840"
"What camera to use for classroom/lecture recording?","7958","","<p>We want to install HD IP cameras in our classrooms to record lectures and make them available online.</p>

<p>We are having a hard time finding the right one. Most are geared towards surveillance.</p>

<p>Any recommendations?</p>

<p>Room size: large classroom seating about 70 students</p>

<p>Camera specs required: Audio/in for wireless microphone, 1080p, data transfer over ethernet.</p>
","<p>have a look at these (I don't know your price range so I leave options in the link):</p>

<p><a href=""http://www.logitech.com/en-us/webcam-communications/webcams"" rel=""nofollow"">http://www.logitech.com/en-us/webcam-communications/webcams</a></p>

<p>Update:</p>

<p>For more professional solutions:</p>

<p><a href=""http://www.crestron.com/resources/product_and_programming_resources/catalogs_and_brochures/online_catalog/default.asp?jump=1&amp;model=CAPTURELIVEHD"" rel=""nofollow"">http://www.crestron.com/resources/product_and_programming_resources/catalogs_and_brochures/online_catalog/default.asp?jump=1&amp;model=CAPTURELIVEHD</a></p>

<p>combined with:</p>

<p><a href=""http://www.crestron.com/resources/product_and_programming_resources/catalogs_and_brochures/online_catalog/default.asp?cat=1058&amp;subcat=1506&amp;id=2220"" rel=""nofollow"">http://www.crestron.com/resources/product_and_programming_resources/catalogs_and_brochures/online_catalog/default.asp?cat=1058&amp;subcat=1506&amp;id=2220</a></p>
","5265"
"Premiere Pro: Simple fade for text?","7919","","<p>In most video editing softwares I have used, you can</p>

<pre><code>- Click Text Object
- Click Button: Fade In/Fade Out
- Done.
</code></pre>

<p>But in Premiere pro, I have to </p>

<pre><code>- Click Text Object
- Open Effects Tab
- Keyframe Transparency 0%
- Keyframe it again 100%
- Keyframe it again 100%
- Keyframe it one last time 0%
- Right Click Keyframe and Set Ease In/Out
- Right Click Keyframe and Set Ease In/Out
- Right Click Keyframe and Set Ease In/Out
- Right Click Keyframe and Set Ease In/Out
- Make sure I dont change duration of the text, or else I can repeat
  whole process again.
</code></pre>

<p>Is there no easy way to do this in premiere pro CC?</p>

<p>I know its supposed to be very flexible and fit everyones needs, but I really just want a simple fade effect for the text.</p>
","<p>Yes you can set a default transition that uses the ctrl-d key combo. I believe it's set by default to crossfade, which I use. You click your clip, control-d and it'll put the fade in and out on the item. </p>

<p>You can change the default transition in the preferences.</p>

<p>It really is simple. </p>

<p>For bonus points, Control-shift-d works for audio.</p>
","18477"
"Is it possible to select particular portion of a video in adobe premiere pro?,","7883","","<p>Im a beginner to video editing, kindly help me to select (free select, as like in Adobe Photoshop) particular portion of a video, to merge with another video?</p>
","<p>To build on AJ's answer in a different direction, you may want to look at After Effects, which is a popular compositing tool. In After Effects, you can use the pen tool (somewhat like the lasso tool) to create a custom shape. With that shape, you can create a mask for the video. This mask can then show only the video inside of the shape.</p>

<p>Further, the mask can change over time with your video. If the subject of your first video moves, you can adjust the mask's shape as well. In other words, the mask can be animated, though this process can take a lot of time.</p>
","10537"
"Youtube convert my video on bad quality","7809","","<p>I have some problem with video and its representation on youtube. When I upload video which was encoded with MPEG4 or H.264 codec, I get bad quality video after youtube processing.</p>

<p>What is this video. And what is my task. 
I get source video from Iphone, Ipad, Ipod. Format of this video is - mp4. Then, I decode this video on frames with Java Library - Xuggler. This library is a wrapper of ffmpeg. After some filters on frames, I encode this frames on final video. I use codec MPEG-4 and try H.264, final format - mp4. And this final video is fine. It has the same quality as source video, when I play this video on Windows Media Player. But, when I upload video on youtube, after processing I get video with bad quality.</p>

<p>This is characteristics of source video, after uploading on youtube it doesn't lose quality:</p>

<p><img src=""https://i.stack.imgur.com/920ki.jpg"" alt=""enter image description here""></p>

<p>And this is characteristics of final video. It loses quality after youtube processing:</p>

<p><img src=""https://i.stack.imgur.com/g71K4.jpg"" alt=""enter image description here""></p>
","<p>I'm going to repost a section of my answer from <a href=""https://video.stackexchange.com/questions/4250/upload-a-video-to-youtube-without-losing-contrast/10079#10079"">this question</a>, as it seems generally relevant:</p>

<blockquote>
  <p>YouTube (as well as Vimeo, and practically every other video website
  nowadays) works using the H.264 codec. Here are <a href=""https://support.google.com/youtube/answer/1722171?hl=en"" rel=""nofollow noreferrer"">YouTube's
  instructions</a> for how they'd like videos to be encoded for upload.</p>
  
  <p>The TL;DR version of that page:</p>
  
  <blockquote>
    <ul>
    <li>Container: .mp4</li>
    <li>Audio Codec: AAC-LC</li>
    <li>Video Codec: H.264</li>
    </ul>
  </blockquote>
</blockquote>

<p>When you upload the incorrect codec - like AVC-HD you are using above, YouTube has to re-encode your video, resulting in the loss of quality and contrast you have been experiencing.</p>
","10158"
"Fast subtitle workflow in Sony Vegas?","7802","","<p>In Vegas is quite time consuming having to insert text using only visual cues, for example: </p>

<ol>
<li>Inserting new text media requires to choose presets instead of having a default style on the go.</li>
<li>No keyboard shortcut for ""Insert Text Media...""</li>
<li>Copying and pasting existing Text in the timeline requires clicking on the ""Generated Media..."" icon to start editing the new instance.</li>
</ol>

<p>Is there a way to simplify or speed-up this process?</p>

<p>Keyboard shortcuts, third party plugins? Please give your thoughts.</p>

<p>Thank you.</p>

<p><img src=""https://i.stack.imgur.com/6FOyw.jpg"" alt=""enter image description here""></p>
","<p>In Vegas you can make a default text or preset by using the save preset feature. For instance you may add a text at the end of all your movies that looks like this:</p>

<p>Joe Blow Productions</p>

<p>Copyright Joe Blow 2014</p>

<p>After you make this text, give the text title a unique name ""Joe Blow"", then click the save preset button just to the right of the title window--looks like a floppy disk. The next time you go to the text generator it will be in the menu. Note, when you save any effect or text preset in Vegas you have to leave the current menu and then return to it so it refreshes the content of that menu.</p>

<p>Here is a short Vegas Tutorial to show you how to make your own presets:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/sQC2J7wQboI?start=0""></iframe>
            </div></div></p>

<p>Let's say you want to change the font of this preset, engage it, and then make your changes and save this new version with a unique name, like ""Joe Blow 2"".</p>

<p>You can do this with effects too. It's an easy way to build a custom library of effects and texts that you can use a baseline and then tweak/save as you need them.</p>
","9997"
"Premiere Pro sequence adds black stripes on both sides of the video","7796","","<p>I'm trying to edit a footage video with Premiere Pro CS6. The raw file is 1920x1080 HD shot with a wide lens camera. So I created a new project in Premiere Pro and I made a custom sequence using exactly the same numbers: 1920x1080</p>

<p>However, the result is not what one would expect. The resultant sequence has two black stripes on both sides of the video. I verified both the properties of the video file and the sequence and they both say 1920x1080 but you can see the result on the screen shot. The sequence is on the right with the black stripes on both sides. On the left, you can see the preview raw footage window that looks just fine... Obviously if I render out the video, what goes out is the one on the right and I do not want to have the black stripes on both sides but I can't get rid of them.</p>

<p>So my question is why is this happening and how can I fix it?</p>

<p><img src=""https://i.stack.imgur.com/xM5Yn.jpg"" alt=""The problem that I have using Premiere Pro sequences""></p>
","<p>Aj Henderson is right.
Your pixel aspect ratio is different in each video.
You can see that from the pool table, the one on the left is stretched out, while the one on the right is squished, it almost almost like you have 16% black bars on either end of the video on the right. Pixel aspect ratio, anamorphic 1.333. </p>

<p>2 ways to fix it:</p>

<h2>1) Square pixels</h2>

<p>If you wish to keep the pixels square
Crop the video (scale in) until the crop lines disappear, but of course you will also crop out some content on the top and bottom of the video</p>

<h2>2) Create new sequence</h2>

<p>Create a new sequence, I am assuming AVCHD, select 1080p folder and then select the item AVCHD 1080p24 Anamorphic (or the 25 frame one, dont know what video setting your source is at). Also note there is a little tab at the top called settings when creating a new sequence, you can click that to view your pixel aspect ratio.</p>

<p>That should fix the issue.</p>
","12039"
"exporting .mov vr to multiple still frames","7732","","<p>Ive got a quicktime VR in .mov format i need to strip the all still frames (in total there are 250) out of it and have them in as individual images, preferably pngs or jpegs.</p>

<p>I was looking at using quicktime pro for this, is that the right software to do it ? Would you suggest trying another method ? </p>

<p>thanks in advance. </p>
","<p><strong>Using <a href=""http://support.apple.com/kb/DL923"" rel=""nofollow noreferrer"">Quicktime Player 7</a> you can export any movie file as an Image Sequence</strong></p>

<ol>
<li><strong>Open your .mov</strong> file using Quicktime 7</li>
<li><strong>File > Export</strong></li>
<li>In the Export dropdown select <strong>Movie to Image Sequence</strong>
<img src=""https://i.stack.imgur.com/BAhJH.jpg"" alt=""Export dialogue box""></li>
<li>Open the options and set the <strong>export format</strong> (eg JPEG or PNG)
<img src=""https://i.stack.imgur.com/ZjtVD.jpg"" alt=""Image Sequence options""></li>
<li>If you want just the current frame leave the <strong>frames per second</strong> blank. Otherwise, enter hte videos frame rate and you will export ALL the frames. </li>
</ol>
","4034"
"Program to convert videos by interpolating 30fps video to 60 fps similar to Splash Pro video player","7696","","<p>If you have tried Splash Pro video player, you will see that it will play 30fps videos in 60 fps by using some kind of motion interpolation technology.</p>

<p>I want to somehow use that technology to process my 30fps videos to 60 or higher fps.</p>

<p>I tried Smooth Video Project(SVP) and I don't know what to do with it. It seems to only convert only Youtube videos. </p>

<p>How do I get this technology and apply it to my videos? Can I do it in Sony Vegas?</p>
","<p>If you're exporting your finished project to a 60 fps format, your editing or compressing software will generally do some sort of conversion for you. It's not usually necessary to conform all your clips before editing.</p>

<p>If you have different options for how to handle the conversion, using ""Optical Flow"" or ""Motion Estimation"" options will interpolate frames. Depending on how good the interpolation is and the nature of the footage, this may look perfectly acceptable or horrible. As mentioned, you can try frame blending, too.</p>

<p>If your software doesn't do the conform automatically, you could try 3rd party tools. I know many people who have had good success with <a href=""http://revisionfx.com/products/twixtor/"" rel=""nofollow"">Twixtor</a> from RE:Vision Effects. But there are others you can try as well depending on which application you're using.</p>
","12884"
"How to DISABLE--NOT REMOVE subtitle tracks from an MKV file from the command line?","7683","","<blockquote>
  <p><strong>ATTENTION:</strong></p>
  
  <p><strong>THIS QUESTION IS ABOUT DISABLING--NOT REMOVING SUBTITLE TRACKS FROM MKV FILES FROM THE COMMAND LINE. PERIOD.</strong></p>
  
  <p>It IS NOT about REMOVING subtitles by REMUXING with <code>mkvmerge</code> or any other tool. There are literally thousands of threads about removal by remuxing. That's precisely why I started this thread about DISABLING NOT REMOVING subtiles. I could not find a single thread about that.</p>
  
  <p>It is NOT about GLOBALLY DISABLING subtitles in any PLAYER. That is obviously trivial and doesn't need any explanation. I don't need/want to do that and, if I did, I obviously wouldn't have taken the time to research the topic at hand and publish my findings in this thread.</p>
  
  <p>I shouldn't have to add this disclaimer but there have been a number of, in most cases demeaning, 'answers' that reveal a distinct misunderstanding of the OP, suggesting irrelevant 'solutions', and/or criticizing this hack as being 'O-h-h-h-so-difficult' if you want to re-enable the subtitle track at a later date. As I pointed out numerous times, anyone who can understand my answer and who knows how to use the command line will find this task trivial. Those who don't SHOULD NOT ATTEMPT this hack.</p>
  
  <p>The answer has already been given as this is the entire point of this thread. So any new answers are either superfluous or, in SOME cases, irrelevant. I will no longer respond to irrelevant 'answers' or comments or demeaning remarks by users who, judging by their reputations, should know better. Any such abuse will be flagged accordingly.</p>
  
  <p>I welcome any CONSTRUCTIVE suggestions that ADHERE TO THE TOPIC, in a COMMENT, but any 'answers' or negative comments will be unanswered and flagged as inappropriate.</p>
</blockquote>

<p>Now that THAT is out of the way, let us begin</p>

<p>I couldn't find out how to do this anywhere so I thought I'd share this hack with others who, like me, want to <strong>DISABLE</strong>, NOT REMOVE, subtitles in MKV files. </p>

<p><strong>There are times when you don't want the subtitles that are embedded in a file and you don't want to have to disable subtitles every time you play such a file or globally disable subtitles in every player you happen to have. Or you may want to automatically use a subtitle (<code>srt</code>) file that is preferable.</strong></p>

<p>Sure, you can completely REMOVE subtitles with <code>mkvmerge</code>:</p>

<pre><code>mkvmerge --no-subtitles ""inputfile.mkv"" -o ""outputfile.mkv""
</code></pre>

<p>But this is heavy-handed and a real pain if you have a bunch of multi-GB files. It takes lots of time and disk space to de- and re-mux every file. So I turned to <code>mkvpropedit</code> which edits the properties of the subtitle track header in milliseconds. Note that this tool is installed when you install  <a href=""https://www.bunkus.org/videotools/mkvtoolnix/"" rel=""nofollow"">MKVToolNix</a>. Unfortunately, it is not at all obvious how to use this wonderful tool for the task at hand. Here's an example of the properties of a subtitle track as given by <code>mkvinfo</code>:</p>

<pre><code>| + A track
|  + Track number: 4 (track ID for mkvmerge &amp; mkvextract: 3)
|  + Track UID: 11363869179551629452
|  + Track type: subtitles
|  + Default flag: 0
|  + Lacing flag: 0
|  + Codec ID: S_TEXT/UTF8
|  + Name: CC
</code></pre>

<p>Despite the <code>default flag</code> setting <code>0</code> (false), the subtitle track is still active by default in players. I later read in the <code>mkvmerge</code> documentation ""<em>If no track is set to be the default track then mkvmerge will promote the first track of each type that it finds to be the default track. This is consistent with the behavior of various media players.</em>"" I'm not sure if this means that it sets the <code>default flag</code> to <code>1</code> or somehow otherwise arranges for the track to be the default, but it seems maybe the latter.</p>

<p>I tried adding the <code>track enabled</code> property set to false but it didn't work.</p>

<pre><code>mkvpropedit --edit track:4 --set flag-enabled=0 ""inputfile.mkv""

| + A track
|  + Track number: 4 (track ID for mkvmerge &amp; mkvextract: 3)
|  + Track UID: 11363869179551629452
|  + Track type: subtitles
|  + Default flag: 0
|  + Lacing flag: 0
|  + Codec ID: S_TEXT/UTF8
|  + Name: CC
|  + Enabled: 0
</code></pre>
","<p>I gave up and contacted the author of <code>mkvpropedit</code> to ask if it was possible to do what I wanted to do. The first answer I got was 'No'. But I persevered and asked if there was any setting that will trick the player into not recognizing a subtitle track? The response was to set the Codec ID to something the player doesn't support. Eureka!</p>

<pre><code>mkvpropedit --edit track:4 --set codec-id=DISABLED ""inputfile.mkv""

| + A track
|  + Track number: 4 (track ID for mkvmerge &amp; mkvextract: 3)
|  + Track UID: 11363869179551629452
|  + Track type: subtitles
|  + Default flag: 0
|  + Lacing flag: 0
|  + Codec ID: DISABLED
|  + Name: CC
</code></pre>

<p>This worked! The player doesn't recognize any subtitle track. In milliseconds I have achieved my goal. No time-consuming remux. Nice.</p>

<p>The procedure for doing this hack is as follows:</p>

<ol>
<li><p>Get track number of the subtitle track</p>

<pre><code>mkvinfo ""inputfile.mkv""

| + A track
|  + Track number: 4 (track ID for mkvmerge &amp; mkvextract: 3)
|  + Track UID: 11363869179551629452
|  + Track type: subtitles
|  + Default flag: 0
|  + Lacing flag: 0
|  + Codec ID: S_TEXT/UTF8
|  + Name: CC

</code></pre>

<p>OR</p>

<pre><code>mkvmerge --identify ""inpufile.mkv"" | grep subtitles 
Track ID 3: subtitles (SubRip/SRT)
</code></pre>

<p>Note that <code>mkvinfo</code> gives the proper 1-indexed track number that <code>mkvpropedit</code> uses, BUT <strong><code>mkvmerge</code> gives a 0-indexed number so you have to add <code>1</code>.  Be sure to do this or you'll mess up the wrong track!</strong></p></li>
<li><p>Change the <code>Codec ID</code> to some nonsense name so the track is not recognized as a subtitle track. (In this case the track number is 4 but it will be different for other files.)</p>

<pre><code>mkvpropedit --edit track:4 --set codec-id=DISABLED ""inputfile.mkv""

| + A track
|  + Track number: 4 (track ID for mkvmerge &amp; mkvextract: 3)
|  + Track UID: 11363869179551629452
|  + Track type: subtitles
|  + Default flag: 0
|  + Lacing flag: 0
|  + Codec ID: DISABLED
|  + Name: CC

Track ID 3: subtitles (DISABLED)
</code></pre></li>
</ol>

<p>If you think you may at some time want to enable the track, keep a copy of the original <code>Codec ID</code>. As suggested by Jim Mack, it would be expedient to just add an obvious character to the existing <code>Codec ID</code>, thus preserving the original ID.</p>

<p>This hack works on files that have multiple subtitle tracks. Just repeat the renaming procedure for each track.</p>
","15389"
"Is it possible to alter the playback *preview* speed?","7559","","<p>Is it possible to change the speed of Premiere's playback of the video without altering the speed of the produced version? Even if I could play it only 1.5x faster, it would mean a lot of time saved in the long run.</p>
","<p>Yes - while playing the video, press the <code>L</code> key to increase playback speed and press the <code>J</code> key to decrease it.</p>

<p>For further options, please reference the relevant <a href=""http://help.adobe.com/en_US/premierepro/cs/using/WS52A45F55-4237-480c-86F0-9A3869DBEF57.html"" rel=""noreferrer"">Premiere Pro documentation</a>.</p>
","8485"
"H.264 or VP9 for encoding for YouTube?","7552","","<p>I make short films which I distribute through YouTube. I know YouTube uses VP9 for streaming, but YouTube still recommends uploading with h.264. It makes more sense to me to upload in as similar a format as will be streamed by YouTube, but I might be missing something.</p>

<p>Also, would uploading in a higher quality codec like DNxHR or Prores (I'm not concerned about bandwidth or processing time) better preserve video quality after YouTube's encoding?</p>
","<p><strong>tl;dr:</strong> Since Youtube reencodes all videos regardless of the upload format, it really isn't that important. Just export your video with a high bitrate to preserve quality. Also see <a href=""https://video.stackexchange.com/a/15815/6273"">my answer here</a> regarding quality loss caused by Youtube.</p>

<p><strong>Long answer:</strong> Each reencoding of a video to a compressed format lowers the quality. Usually, that means you'll lose quality at two points: When you export the edited video from your editing software and when you upload the exported video to Youtube, at which point it is reencoded to a highly compressed, streaming-compatible format. You have no control over that second step, so what you can do to achieve the maximum quality possible is make sure you lose as little quality as possible during the first encoding.<br>
In theory, that would mean export to a <em>perceptually</em> (even though technically not) uncompressed format like Apple ProRes or DNxHD as you suggested. However, unfortunately, <a href=""https://support.google.com/youtube/troubleshooter/2888402?hl=en"" rel=""nofollow noreferrer"">Youtube doesn't support those formats</a>, so you'll have to use a compressed format. To minimize quality loss, set a high bitrate in your export settings (assuming rendering and upload time aren't an issue. If they are, you'll have to find some middle ground; exporting with a bitrate that is higher than the bitrate of the source material won't yield any more quality, so that is as high as I would go). If you do that, it doesn't really matter which codec you use, both are highly efficient regarding file-size/quality ratio (VP9 probably a bit more so, but that's more important when you're dealing with low bitrates). <a href=""https://support.google.com/youtube/answer/1722171"" rel=""nofollow noreferrer"">Youtube recommends H264</a>, so that's what I would use. However, the best advice I can give you is to try out both, i.e. export the same video as both H264 and VP9 with identical bitrates/other settings, upload both to youtube and check which one looks better to you.</p>
","16325"
"Batch video editing software that can trim without re-encoding","7545","","<p>What application is there that can open a video, modify it, and save it?<br>
(the software that I am using now, only allows the video to be used as a source, and then exported)</p>

<p>I have adobe premier, but it does not actually edit videos, you have to make a new project, and set the output type/settings, then import the video, then bring in the video, edit it, and then save it.</p>

<p>I also have iMovie, and again, project, import, edit, export.</p>

<p>Is there any application that I can open a mp4 video make edits (clipping segments) and save it in the same format/size/container/etc as it was opened in?</p>
","<p>If you can script yourself, all you need to do is call FFmpeg repeatedly:</p>

<pre><code>ffmpeg -ss [start-time] -i input.mp4 -c copy -t [clip-time] output.mp4
</code></pre>

<p>Here, start and clip times can be in the form <code>HH:MM:SS.mmmm</code>, or just in seconds. </p>

<hr>

<p>To give you a rough idea how that'd look like in Ruby, see <a href=""https://gist.github.com/2574696"" rel=""noreferrer"">this Gist of mine</a>. It contains a script that takes an edit list, a folder of videos, and cuts them accordingly. I've used it very often for cutting videos based on an input edit list.</p>

<p>The edit list would look like that, with the timestamps in the proper FFmpeg format (e.g. 00:00:24.240) and the difference passed to <code>-t</code> in the last column.</p>

<p><img src=""https://i.stack.imgur.com/M01G8.png"" alt=""""></p>

<p>Note that you can't expect it to work with any input/output without synchronization issues. If that's the case, you need to re-encode video and audio parts. I wouldn't use that script of mine though as it's not very  eloquent in that regard. Rather cut your video in a NLE.</p>
","5626"
"When making an animation how can I get a zooming effect?","7540","","<p>I want to add a transition to one of my animations that gives the impression of the camera rushing/zooming towards the animation. Is there a better means of doing this in After Effects than scaling all of the elements onscreen up until they screen is filled?<br>
It's an effect you see in a ton of animated infographic-style videos at the moment, but I am new to After Effects and am not sure about the best way to replicate it.</p>
","<p>Drag your composition (in the project window) onto the <strong>Make new Composition</strong> button, everything inside that first composition will be added as one layer in the new composition. Now you can just zoom this one layer!</p>

<p>(Say your original comp. is called <strong>1</strong>. You drag <strong>1</strong> onto the button. Now there will be a <strong>New Composition</strong> comp. with one layer inside. This layer is called <strong>1</strong>.)</p>
","3525"
"How to cut a single clip in a Sequence into multiple clips for use in multiple Sequences in Premiere Pro","7532","","<p>I have a single clip of a multi-speaker conference in Premiere Pro CS6 that I need to split into individual clips of each speaker, so that I can make a timeline of each clip with the end goal of exporting several individual videos, rather than one long video of the entire conference.</p>

<p>I've already split the clip up within a Sequence using the Razor Tool and removed extraneous footage, like so:</p>

<p><img src=""https://i.stack.imgur.com/fvPUm.png"" alt=""razor at work""></p>

<p>How can I get these split-up and renamed clips to their own individual Sequences for production as separate video files? (If there is a Premiere shortcut or tool specifically for this, I haven't found it)</p>
","<p>The easiest way I've found is to simply click the New Item button at the bottom-right of the Project window and select ""Sequence..."" to create a new one; you'll need one Sequence for each clip section you've created:</p>

<p><img src=""https://i.stack.imgur.com/AOaNf.png"" alt=""New item location""></p>

<p>From there, You can simply copy/cut and paste each section of video into its respective Sequence. You'll want to make sure the old sections are removed from the original Sequence if you want to use that one. Otherwise, you can just delete the original Sequence when you're finished.</p>

<hr>

<p>You could copy each section into your Project Window by dragging and dropping it, but that creates two extra steps (and can be confusing if you haven't already renamed your split-up files in the original sequence): </p>

<ul>
<li><p>The new file in the Project Window won't be aware that you've renamed it in the timeline, so you'll have to rename the new clip; it'll have the same name as the original source clip, <em>even if</em> you rename the original source clip to something else. </p></li>
<li><p>You'd still have to create new Sequences for each clip manually, so that you can drag and drop the new clips into their own respective Sequences.</p></li>
</ul>
","16721"
"Are there any downloadable samples of 48 fps video?","7512","","<p>I would like to compare 24fps to 48fps video, but haven't been able to find any video samples.</p>

<p>Does anyone know of such samples?  Being able to compare the same video at several framerates would be lovely!</p>
","<p>Here's a short mov to demonstrate the difference, that's all I've been able to find:</p>

<p><a href=""http://www.stopmotionpro.com/media/video/48fps_cameramove2.mov"" rel=""nofollow"">http://www.stopmotionpro.com/media/video/48fps_cameramove2.mov</a></p>
","3907"
"Fill shape layer with an image (after effects)","7428","","<p>How can I fill a shape, with an image in After effects? </p>

<p>For example, something like this:
<a href=""http://upload.wikimedia.org/wikipedia/commons/b/bc/Europe_flags.png"" rel=""nofollow"">http://upload.wikimedia.org/wikipedia/commons/b/bc/Europe_flags.png</a></p>

<p>I found an answer somewhere that I need to create two layers, and texture layer to parent the shape layer, and use alpha matte. I cannot find alpha matte in my AE CC. but that does not seem to work </p>

<p>Any link to tutorial will be accepted.</p>
","<p>If what you want to have as an end result is a flag texture inside a non-standard shape, can I suggest instead that you import the flag texture, and use a matte to change its shape? If you've already created a shape that you want to use, you could convert it into your matte by adding a key frame to the matte's path control, adding a keyframe to the shape, copying that key frame, and then pasting the key frame over the matte's path keyframe. 
This is probably easier than using unnecessary alpha channels etc.</p>
","13267"
"How do I recover truncated AVI/DV video?","7418","","<p><strong>Short version:</strong> I believe I have truncated but otherwise valid AVI/DV files that won't play in any player I've tried (Adobe Premiere, MPC HC, VLC). I assume the stream data is intact but truncated, and the container is damaged. Are the files still recoverable and how would I accomplish this?</p>

<p><strong>Long version:</strong> Having finished a recording from a Sony AnyCast workstation with two cameras to a FireWire hard disk drive, I accidentially unplugged the hard disk drive without ""disconnecting"" (unmounting) it first.</p>

<p>The result was a corrupted ext3 filesystem that would not mount under the Windows Ext2 IFS setup I was using to download the videos, so I connected the hard disk to a Linux laptop and ran e2fsck on it. After this, I was able to list the directory and copy the files, but the files appeared corrupted.</p>

<p>I'm working under the assumption that I've only lost some data at the end of the two video files. (This might prove false as e2fsck might have wrecked havoc on them.) That would mean I have 1,5 hours (x2) of unscathed DV stream. Unfortunately, my layman analysis concludes that these lost bits must have contained some information vital to the container format.</p>

<p>Note that all I have are the files copied to an NTFS volume from the fsck'ed ext3 volume. I no longer have access to the original hard disk drive.</p>

<p>Dear StackExchange, please help me find a way to recover and reconstruct these files! I suspect what I would need is some software that would guess or be told the information usually contained in the container header, copy the stream data and write it into a new, valid container file.</p>

<p><strong>Edit:</strong> I believe the format is DV instead of MPEG2.</p>

<p><strong>Edit2:</strong> It seems that the broken files do <em>not</em> in fact have an AVI header at all - they should begin with the bytes ""RIFF"" which they do not. <code>ffprobe</code> <em>does</em> recognize the stream format, though:</p>

<pre><code>[dv @ 0xefb8a0] Estimating duration from bitrate, this may be inaccurate
Input #0, dv, from '120610-1-001.avi':
  Duration: 02:08:36.84, start: 0.000000, bitrate: 28799 kb/s
    Stream #0.0: Video: dvvideo, yuv420p, 720x576, 28800 kb/s, PAR 64:45 DAR 16:9, 25 tbr, 25 tbn, 25 tbc
    Stream #0.1: Audio: pcm_s16le, 48000 Hz, 2 channels, s16, 1536 kb/s
</code></pre>

<p><strong>Edit3:</strong> Thanks to Matti ""Lumpio-"" Virkkunen, we've finally got the video files fixed. He wrote a good 400 lines of C that did exactly what @JEEB suggested: crafted an AVI header and searched for things that look like AVI chunks, copying them over to the output file. A long AVI file needs a new header between every gigabyte or so, and the program simply fixes the broken first section and then copies the following sections as-is. The <a href=""https://gist.github.com/2959531"" rel=""nofollow"">source code for <code>fixavi.c</code></a> can be found on Gist.</p>

<p>Please note that details such as codecs, audio sample rate and video frame rate are hard-coded in the source, so if you need to replicate this process, please adapt the source to your needs. I found <a href=""http://www.alexander-noe.com/video/documentation/avi.pdf"" rel=""nofollow"">Alexander Noe's AVI format guide</a> a good reference on the AVI file format.</p>

<p><strong>Edit4:</strong> And as <code>fixavi.c</code> does not create an index at the end of the AVI file, I simply used <code>./fixavi broken.avi recovered_without_index.avi; ffmpeg -i recovered_without_index.avi -acodec copy -vcodec copy recovered_with_index.avi</code> to make one.</p>
","<p>First of all, if I remember correctly, the DV format is intra-only, and thus if there is various damage that has happened to your video, there should be a relatively big chance of getting at least something out -- given that the file isn't completely broken in the most relevant of places.</p>

<p>A good way of checking if there's at least some kind of sanity left in the file is indeed by the aforementioned <code>ffmpeg|avconv -i input.avi</code> or <code>ffprobe|avprobe input.avi</code>. If ffmpeg/avconv and/or the related probe application can 'see' the format as well as other information there's at least some hope that the file actually still contains some of the things you're hoping it contains.</p>

<p>As for actual recovery attempts, I would actually first make a copy of the file (backups are generally useful, even if it's a backup of possibly broken data), and try to open it with Avery Lee's <a href=""http://www.virtualdub.org/"" rel=""nofollow"">VirtualDub</a>, as I remember it having one of the good general AVI reconstruction mechanisms I've seen so far. If you are lucky and there's enough data for VirtualDub to actually do something with the file, it should ask if you want to repair/reconstruct the index and so forth. After that you can set both audio and video to 'Direct stream copy', and just remux with another file name, and see if that works.</p>

<p>Another way to try and salvage it is to use ffmpeg/avconv, and use the stream copying function there (of course this might not work at all in case ffmpeg/avconv or the related probe application cannot see what container/video/audio format the file is). <code>ffmpeg|avconv -i input.avi -vcodec copy -acodec copy out.avi</code></p>

<p>A yet another way to try and salvage something is by trying out the now-discontinued <a href=""http://www.alexander-noe.com/video/amg/"" rel=""nofollow"">AVI-Mux GUI</a>. I have not tried it myself, but I have some hazy memories of people fixing their broken AVI files by remuxing with it a few years ago.</p>

<p><strong>Edit</strong>:
And of course the so-called final destination would be to see if there are any chunk-like entities in the file, and manually recreating the AVI header and friends before it either via scripting or with a hex editor, looking at the related <a href=""http://wiki.multimedia.cx/index.php?title=Microsoft_Audio/Video_Interleaved"" rel=""nofollow"">technical documentation</a>.</p>
","4216"
"How do I detect scene changes in Adobe Premiere?","7403","","<p>I have a single clip with multiple edits in it. Is there an easy, automatic way to detect these? Ideally I'd just like them labelled in some way with support for ""jump to the next label"".</p>

<p>I realise it won't be 100% reliable since it would need to detect scene changes automatically, but perhaps there's something that can considerably speed up the process of splitting such a clip into separate scenes?</p>
","<p>This isn't part of Premiere, but <a href=""http://www.scenalyzer.com"" rel=""nofollow"">Scenalyzer</a> is an external tool for this that I've had some good luck with.  It can scan a large video file and split it into smaller ones based on frame change detection.  Then you can work with the smaller files much more easily.  There's a freeware version, and a more featured paid edition.</p>
","3470"
"Merge .ass-subtitles into movie","7332","","<p>I produced a video which will include subtitles. The movie is edited in Premiere Pro and (currently) rendered to a .mov file. I've created subtitles with Aegisub which creates a .ass file in order to keep the existing font- and position settings. </p>

<p>Is there anyway in which I can merge the .mov and the subtitles together? I've tried many different programs but none of them seems working. The only one that worked a bit was SubMerge but that ignored the font-settings and positions of the subtitles.</p>
","<p><strong>Will refer to Premiere Pro as PP and After Effects as AE from here on:</strong></p>

<p><strong>Here is a solution which requires AE</strong> that <em>I think</em> works but can't test it because my demo of PP and AE has finished. Luckily for you, <strong>they have trials for many products including AE which you <a href=""http://www.adobe.com/cfusion/tdrc/index.cfm?product=designweb_premium"" rel=""nofollow"">can download from their site</a>.</strong> I contacted a friend of mine who works as a video editor about this and he mentioned something that sounded a lot like this method, but without the conversion I will mention. At least I'm pretty sure since some of what he said went over my head.</p>

<p>Anyways, he initially said that <strong>.ass files are not compatible with Premiere Pro.</strong> Luckily though he pointed out that <strong>you can easily convert .ass files into .srt files, which can be imported into PP (CS4 and above).</strong></p>

<p><a href=""http://sub-converter.hostei.com/"" rel=""nofollow""><strong>Here is a website that does this for free</strong></a></p>

<p>This is where he began to loose me, but mentioned using the analyze content feature to make sure the content roughly matches up with the video. The video below will explain more and clarified stuff for me.</p>

<p><a href=""http://www.youtube.com/watch?v=4LAnZPDUOiI"" rel=""nofollow""><strong>Here is the video explaining how to do this and how to automate the process</strong></a></p>
","4954"
"After Effects: Create an isometric/orthographic Camera","7292","","<p>I know that its not possible to create an non-perspective camera in after effects. But I read about a way to fake the effect of an isometric camera in after effects. I just didn't find something useful in the web so far so I wanted to post the Question about how to create an isometric camera in after effects here so you guys can share your ideas on how to realize this problem.</p>

<p>I just cant get over the fact that its not possible in a mighty program like After Effects.</p>

<p>Thanks for your sharing your opinions and ideas in advance!</p>
","<p>Use a camera with a long focal length.  The longer the focal length, the closer to isometric.  Just back the camera far enough away to frame the shot.</p>
","15198"
"Cool Edit Pro alternative for Mac?","7213","","<p>I just can't find a program similar to Cool Edit Pro for the Mac. Any idea?</p>
","<p>Cool Edit Pro is now Adobe Audition. They have started a beta, so you're in luck.</p>

<p><a href=""http://labs.adobe.com/technologies/audition/"">http://labs.adobe.com/technologies/audition/</a></p>
","543"
"Similar to ""levels adjustment"" brightness improvement with ffmpeg","7174","","<p>I'm composing a video from set of 16bit tiff pictures. The pictures are very dark, and needs to be brightened. Currently I'm applying <code>-vf ""mp=eq2=1.0:1.0:0.3:1.0:1.0:1.0:1.0:1.0""</code>
filter to make pictures brighter, as said <a href=""http://www.mplayerhq.hu/DOCS/man/en/mplayer.1.html#VIDEO%20FILTERS"">here</a>, but they becomes too ""faded"".</p>

<p>I used to rescale their histogram with ""levels adjustment"" operation in image editor to make them brighter, but now I need to make a video, not a single image file. Also, preffered way of applying such operations to image - is to applying it to each color band separatly, because they have different brightness attributes initialy.</p>

<p>Can I do something similar with ffmpeg?</p>
","<p><img src=""https://i.stack.imgur.com/yFWHf.jpg"" alt=""original"">
<img src=""https://i.stack.imgur.com/x4sPP.jpg"" alt=""with curves filter""> </p>

<p>You could possibly use the <a href=""http://ffmpeg.org/ffmpeg-filters.html#curves"">curves filter</a>. It has a <code>lighter</code> preset:</p>

<pre><code>ffmpeg -i input -vf curves=preset=lighter -c:a copy output
</code></pre>

<p>The red, green, and blue components can be adjusted separately. The following is the same as what the <code>lighter</code> preset uses:</p>

<pre><code>curves=r='0.4/0.5':g='0.4/0.5':b='0.4/0.5'
</code></pre>

<p>Additionally you can make your own curves preset in Photoshop, export it as an <code>.acv</code> file, and then the curves filter can utilize it:</p>

<pre><code>curves=psfile=/path/to/ps_curves_file.acv
</code></pre>
","13110"
"The ProRes of Premiere Pro?","7151","","<p>I'm going to be editing more and more DSLR videos, most notably music videos for my band as we start releasing singles and want to know if there is an intermediate format for both cutting and exporting that rivals the kind of features found in ProRes 422 used in Final Cut?</p>

<p>Is it possible to use ProRes in Premiere Pro anyway (with the same advantages in editing speed and bit-depth)?</p>

<ul>
<li>I-frame encoding/decoding for speed.</li>
<li>Higher sample depth for making image modifications.</li>
<li>Intended for 720p 60fps H.264 footage captured from Canon EOS 60D.</li>
</ul>

<p>If ProRes itself can be used in Premiere Pro, I have all the tools I need to begin transcoding. Just wondering if anyone has some workflow/codec tips for using an intermediate editing/exporting format in Premiere instead of directly editing .MOVs.</p>

<p>I'm not using an overly powerful Mac but it seems to do the job with editing MOVs and AVCHD files directly (mostly using preview renders). 60fps shots and other HD with effects applied start to make it crumple without rendering previews. It's a MacBook Pro 13"" 2.53Ghz Core 2 Duo, 8GB DDR3, 1TB internal 2.5"" drive (slow).</p>

<p>I intend to use ProRes or similar footage on an external 7200rpm FireWire drive.</p>
","<p>There's a philosophical difference between delivery codecs (e.g. mpeg4, avchd), editing codecs (e.g. DNxHD, ProRes, Cineform), and capture codecs (e.g. r3d, DVCPROHD).  While almost any codec can be used for each of these three stages, your workflow needs will help you decide which are best suited for each stage.</p>

<p>The question you seem to be asking, is ""Which editing codec should I transcode to for an online edit?"".  Editing codecs are generally less compressed and larger on disk, to make playback simpler from a processing perspective.  It's definitely possible to use ProRes in Premiere, and coming from DSLR (as of July 2011) 422 is probably (technically) overkill, but it is a reasonable default choice.  Another editing codec option to consider is Cineform, since you can get the 1920x1080 codec for free now, as part of the GoPro Cineform Studio package.</p>

<p>There are even specialized tools to help you with transcoding batches of files, such as Magic Bullet Grinder.  If you're concerned about editing speed, there's even a chance that Premiere and your computer will be powerful enough to slog through your capture files as-is without transcoding, saving you from this step altogether.</p>

<p>As always, try it for yourself, and test, test, test, your workflow before putting it into production.</p>
","1732"
"Windows Live Movie Maker Output Video Quality Low","7031","","<p>I'm using Windows Live Movie Maker on Windows 7.</p>

<p>I made a screen capture video with a third party app which produces <strong>.avi</strong> files. Its quality is perfect. Its properties: </p>

<ul>
<li>Resolution: 1280 * 720 (720p)</li>
<li>Frame Rate: 15</li>
<li>Kbps: ~10.000</li>
<li>Video Size: ~23 MB</li>
</ul>

<p>I imported it to Movie Maker. And without any modification I just exported it with these settings:</p>

<p><img src=""https://i.stack.imgur.com/4a1eU.jpg"" alt=""enter image description here""></p>

<p>The resulting video was 1.5 MB and of course in <strong>.wmv</strong> format.</p>

<p>The quality drastically lower.</p>

<p>Than I tried increasing frame rate (till 30), bit rate (til 60.000), resolution (till 1080p). No change!</p>

<p>I don't care about the size (in MB), I want a big high quality video.</p>

<p>I'll add lots of videos and do lots of editing and Windows Movie Maker is a very practical tool for an amateur like me.</p>

<p>So <strong>is there a way to get a very high quality video from Movie Maker?</strong></p>
","<p>You can't do much about it. Movie Maker can only export in WMV format - and WMV is created for small video sizes (at the cost of quality).</p>

<p>You will need another software package to export in other formats than WMV. Check out <a href=""http://www.pinnaclesys.com/PublicSite/us/Home/"" rel=""nofollow"">Pinnacle products</a> for alternative - they target amateur producers.</p>
","5767"
"How to animate the outlines of text (After Effects)","6979","","<p>Is there a method in After Effects of animating the outlines of text as seen in the example below?</p>

<p><img src=""https://i.stack.imgur.com/9gIXZ.gif"" alt=""Outlines""></p>
","<p>Simplest way is to use the <code>Stroke</code> Effect on a solid layer with separate letters as masks.</p>

<ul>
<li>Create a new solid</li>
<li>Write your text in Illustrator</li>
<li>Convert it to a path</li>
<li>Seperate all letters  with the path finder or ungroup it </li>
<li>Setup same document size as in After Effects   </li>
<li>Import your paths into after effects, pasting should also work with <kbd>CTRL</kbd>+<kbd>V</kbd>. Requirement is to have all of your <strong>paths from Illustrator as seperate masks on the solid</strong> in After Effects.</li>
<li>Note: you can create seperate paths from an image of letters within After Effects with the <code>auto trace</code> option <em>(Layer > Auto Trace)</em> - use black and white option and set it to the current frame <em>(Note: Tracing letters isn't really accurate)</em></li>
<li>Apply the stroke effect to the solid layer which should be full of masks now</li>
<li>Tick <code>All Masks</code> option to make use of all masks</li>
<li>Set the paint style to <code>transparent</code></li>
<li>Go to your <strong>first frame</strong> and set the <code>End</code> value of the effect to <code>0%</code></li>
<li>Click the stop watch of it to enable animation</li>
<li>Go to your <strong>last frame</strong> and set the <code>End</code> value to <code>100%</code></li>
</ul>

<p>You can play with the settings of the effect, but in general this should do what you want to achieve. </p>

<p>Note: The order of masks determines the order of the animation. To reverse the effect simply duplicate the first keyframe and move it behind the second one.  </p>
","14909"
"How to do a video live stream through a VPS?","6946","","<p>I've been looking at a million resources and I can't find a definitive answer to this. </p>

<p>I'm looking to do a small scale (max 50 viewers possibly), low quality live video stream, with nothing but my existing equipment (mac and firewire camera), a low end ubuntu vps and free software. Stream from the mac and host the stream on a website on the vps.</p>

<p>I don't want to use services like justin.tv or ustream because the advertising is very intrusive, and I'd also like to learn how to do it myself. 
I have some server admin skills, but this is a new world to me, and I can't make sense of how all the pieces fit together. I spent all night reading about rtmp, rec5, wowza, ffmpeg and now i don't understand anything at all. </p>

<p>Can someone give me possible workflows to piece the 3 parts together?ie, streaming from local computer, receiving in vps, broadcasting on website. </p>
","<p>It varies a little from one server to another, but the basic components are an encoder/streaming client on the local client that takes input from the camera and turns it in to a stream that can be sent to the stream server on the VPS.  </p>

<p>The job of the stream server (such as Red5) is then to provide a publishing point that relays the stream to viewers.  The streaming server can either pull from the client or the client can push to a target location on the stream server in order to provide content to the publishing point.</p>

<p>Each viewer then connects to the published RTMP stream on the streaming server and is provided with a copy of the stream that is being uploaded.  Note that every copy has to be sent out separately unless you are on a private network where you can use multicast, so bandwidth needs rise quickly, though it should be possible to do 50 SD or lower quality streams from a single VPS provided it has a reliable 100Mbps connection.</p>

<p>The website simply contains a viewer that hooks up to the RTMP (or similar) stream and is actually operating client side, not server side.</p>

<p><strong>So to quickly recap, client uploads to back end of streaming server, streaming server replicates out to every viewer requesting to watch it and the web server simply gives users a client side viewer that connects to the video stream.</strong></p>

<p>To briefly hit on what each one of the component you mentioned.</p>

<ul>
<li><strong>RTMP</strong> is real time media protocol, which is a common streaming protocol used for actually relaying the video stream content.</li>
<li><strong>Red5</strong> is a fairly popular free streaming media server software used for replicating out RTMP and similar media streams to clients.</li>
<li><strong>Wowza</strong> is another streaming media server software, however it is a commercial, paid software product.  If you are limited to free software, Wowza is not an option.</li>
<li><strong>FFMpeg</strong> is a popular open source video encoder.  It runs on the client and can be configured to publish a stream that can be sent to Red5 or similar streaming media server.</li>
</ul>

<p>Additionally, you will need an end-point viewer capable of consuming the stream.  Software like Windows Media Player or Quicktime should be able to view a stream, but it is often best to include an embedded cross platform player in the website that people are going to for the stream.  There are many options for this component with different strengths and weaknesses, but any should work for your purposes.</p>
","10664"
"Why does Premiere Pro 6 Generate Peak Files?","6857","","<p>I am having a challenge with Premiere Pro 6. Every time I open a project I get a little progress bar in the lower right hand corner of the window that says:</p>

<blockquote>
  <p>Generating Peak File...</p>
</blockquote>

<p>This can take a fair amount of time with a lengthy project. Is this normal? I have see comments on Adobe forums about issues with this.</p>

<p>Another possibly related issue is the project getting corrupted. Before I noticed this I tried making some changes while PP6 was Generating and ending up getting the dreaded project corruption message...</p>

<p>With that as background, is the Generating Peak File normal? What can I do about it and do I just need to be careful while it is going on not to make any changes?</p>

<p>Thanks!</p>
","<p>Peak file creation as well as creating alternate forms of various assets is perfectly normal background behavior for Premiere.  When you load new assets into a project, Premiere automatically starts a number of background processes to analyze the clip and make it easier for Premiere to do its job smoothly.</p>

<p>This should not cause any trouble and should not be a concern to you.  It is just the program functioning as it should.  It shouldn't cause any problems if you start working with it prior to completion of the work, but some functions may work more slowly or provide incomplete information while the processes are still going on.</p>
","8094"
"Lossless extracting of JPEGs from an MJPEG video","6808","","<p>Using the Timelapse app on my Sony NEX-6 camera, I get an AVI which contains an MJPEG stream.</p>

<p>However, I'd like to take all the individual frames into Lightroom as normal photos, to do my post-processing. Is there a way to extract the individual JPEG frames in a lossless fashion, e.g. <em>without resaving them</em>? As I understand it, MJPEG format is essentially just a lot of JPEG files contained within (in this case) an AVI container.</p>

<p>I would prefer to do this on a Mac (I normally use Lightroom and Photoshop), but any application that does it would be fine, I'll find an OS to run it!</p>
","<p>The easiest way is to use ffmpeg or avconv.</p>

<p>The command:</p>

<pre><code>ffmpeg -i mjpegvideo.avi -vcodec copy frame%d.jpg
</code></pre>

<p>will create a series of jpeg without reencoding.</p>
","6851"
"What's the difference between 3G-A SDI and 3G-B SDI?","6800","","<p>Can someone point me at a description of the difference between a 3G-A SDI signal and a 3G-B SDI signal.  </p>

<p>I have a rack with a BlackMagic ATEM 2 M/E Production Studio 4K and a bank of 3 SWT Pro-HD monitors.  The ATEM is set up to run 1080p59.94. The monitors show ""No-sync"" when fed from the Aux outputs of the ATEM.</p>

<p>If I insert a Decimator MD-HX between the Aux output and the monitor's SDI input, it shows the SDI input to the MD-HX running 3G-B 1920x1080p59.94.
The SDI output of the MD-HX is switchable between 3G-A SDI and 3G-B SDI. Switching he output to 3G-A makes the signal appear on the monitor.</p>

<p>The problem does not appear if I run the ATEM at 720p59.94.</p>

<p>Wikipedia's article on SDI does not mention the A and B variants of SDI.</p>
","<p>Wikipedia does <a href=""https://en.wikipedia.org/wiki/SMPTE_424M"" rel=""nofollow"">mention</a> it - with a reference <a href=""https://tech.ebu.ch/docs/techreports/tr002.pdf"" rel=""nofollow"">link</a> included:</p>

<blockquote>
  <p>Within this standard there are three formats known as Level A, Level B
  Dual Link (B-DL) and Level B Dual Stream (B-DS). The Level A format is
  the direct mapping of uncompressed 1080p (up to 60 fps) video into a
  serial digital interface at the nominal 3 Gbit/s. The Level B-DL
  format is the mapping of dual-link HD-SDI/SMPTE 372M (i.e.: 1080p up
  to 60 fps) in a single serial digital interface at the nominal 3
  Gbit/s. The Level B-DS format is the dual-stream carriage of two
  independent HD-SDI/SMPTE 292M signals (720p up to 60 fps or
  1080i/1080p up to 30 fps) in a single serial digital interface at the
  nominal 3 Gbit/s</p>
</blockquote>

<p>The PDF reference has the descriptions starting on pg. 17.</p>
","18253"
"What is good software for camera recording under Linux?","6794","","<p>I use Linux(Mint Linux Petra) and will probably switch to Ubuntu 14 soon.</p>

<p>I need to record video with my camera on my laptop(imagine I use a web-camera for simplification) and I need to see real-time what I am shooting and record the stream.</p>

<p>I tried <code>camera-app</code> and <code>cheese</code> but both fail on my distribution. </p>

<p>Also, I plan to use external camera for better quality, any suggestions on interfaces, etc.? I prefer to see the stream directly on my laptop screen.
(If you want to know why - it will save me some time while editing.)</p>
","<p>I was going to recommend <a href=""http://www.kinodv.org/"" rel=""nofollow"">Kino</a> which is the one I used when I needed to do that, but according to their web page the project is no longer mantained, so you can either use it the way it is or try the other software that is recommended on their web page:</p>

<p>Shotcut, Kdenlive, Flowblade, OpenShot, PiTiVi, LiVES, and LightWorks.</p>
","12226"
"Screen refresh rate above 100 Hz noticeable?","6720","","<p>Is a refresh rate higher than 100 Hz still noticeable for the human eye?</p>

<p>Tv manufacturers are still taking these refresh rates to new heights, but is there really a noticeable difference between a refresh rate of 100 Hz and 200 Hz, Let alone 400 Hz?</p>

<p>Are there any applications when a refresh rate should be as high as possible?</p>
","<p>No, there is no practical limit that we know of yet to what would be best, there is however a practical limit to what we can capture and display.  </p>

<p>In tests with airforce pilots, subjects were able to <strong><em>identify</em></strong> a plane from being shown a frame for only 1/220th of a second.<a href=""http://www.100fps.com/how_many_frames_can_humans_see.htm"">1</a>  They eye is able to pull information out of extremely short periods of time, but unfortunately our eyes are also very good at concealing lack of information (which is actually why video works in the first place), so it is extremely hard (possibly impossible) to determine the actual point at which we wouldn't benefit from more information.</p>

<p>Either way, it is almost certainly upward of the current 1/600 mark and often theorized to be beyond the 1/1000 mark.  The problem comes not from how much information we present, but that we don't have enough information to present.  </p>

<p>Generally videos are normally not played back faster than 48 frames per second at the fastest and video games normally don't go beyond actually displaying 120 or so.  This is due to a lack of being able to store sufficient data and stream it in an efficient enough manner to display in an affordable manner.  </p>

<p>Instead, to make things appear more smoothly, the TV interpolates additional frames to fill the gaps.  It looks at where one frame is and where the next frame will be and generates frames in-between to smooth it out.  The problem is, not all motion is perfectly smooth and interpolation isn't a perfect process.  The result is that unnatural artifacts appear in the video which we can detect and end up being unsettling and eventually start causing more harm to the persistence of vision than the extra frames help.</p>

<p>This is why, when you move in to higher refresh rates it is sometimes better to use a lower setting without the interpolation.  Video is about simulating reality for your eyes.  In reality, there is no refresh rate, any time your eyes chose to process, they get whatever is there at the time.  Thus, higher and higher frame rates will more closely emulate reality as long as the information is actually based around reality rather than something invented by electronics on the fly.</p>

<p>Eventually, we may exceed the ""refresh rate"" of the eye, but as of yet, we don't definitively know what that is and are pretty certain we haven't hit it yet.</p>
","12128"
"What is the difference between ffmpeg and ffmbc now?","6693","","<p>Two years ago I've used ffmbc to encode videos to prores, but nowadays ffmpeg also supports <a href=""http://en.wikipedia.org/wiki/Apple_ProRes"">prores</a> and <a href=""http://de.wikipedia.org/wiki/Advanced_Video_Codec_High_Definition"">avchd</a>. What is the difference between <a href=""http://www.ffmpeg.org/"">ffmpeg</a> and <a href=""https://code.google.com/p/ffmbc/"">ffmbc</a> now?</p>
","<h1>Usage differences</h1>

<p><sub>Note: <code>ffmbc</code>/<code>ffmpeg</code> will refer to the tools, while FFmbc/FFmpeg will refer to the projects themselves and/or each whole collection of tools and libraries.</sub></p>

<ul>
<li><p>There are syntax differences including:</p>

<ul>
<li><p><code>ffmbc</code> uses the old, confusing <code>-newaudio</code> option while <code>ffmpeg</code> can just use <code>-map</code>.</p></li>
<li><p><code>ffmbc</code> does not support <code>-codec</code> or <code>-c</code>, so instead all stream types must be explicitly defined if you do not want to rely on the defaults. For example <code>-vcodec copy -acodec copy -scodec copy</code> vs just <code>-c copy</code>.</p></li>
<li><p>Probably lots more that I didn't think of. Feel free to edit answer and add more.</p></li>
</ul></li>
<li><p><code>ffmbc</code> will automatically relocate some data after encoding to MOV, MP4, etc while <code>ffmpeg</code> requires the <code>-movflags +faststart</code> option.</p></li>
<li><p><code>ffmbc</code> does not support <code>-filter_complex</code> so filtering is different. For example, using the overlay video filter:</p>

<ul>
<li><code>ffmbc</code>: <code>-i video.mp4 -vf ""movie='overlay.png' [movie]; [in][movie] overlay [out]""</code></li>
<li><code>ffmpeg</code>: <code>-i video.mp4 -i image.png -filter_complex ""[0:v][1:v]overlay""</code></li>
</ul></li>
<li><p>FFmbc seems to support more broadcast formats.</p></li>
<li><p>FFmpeg has more features and filters.</p></li>
<li><p>FFmbc dropped <code>ffplay</code> and <code>ffserver</code>.</p></li>
</ul>

<h1>Development differences</h1>

<ul>
<li><p>FFmbc is GPL, and FFmpeg is mostly LGPL which makes porting features from FFmbc to FFmpeg more difficult.</p></li>
<li><p>FFmbc is basically the project of a former FFmpeg developer with a few contributions from others. FFmpeg has many developers and contributors.</p></li>
<li><p>FFmpeg often merges things form Libav, while FFmbc ignores Libav. Libav cherry-picks occasionally from FFmpeg, but ignores most stuff from FFmpeg (Libav is often joked to have ""<a href=""http://en.wikipedia.org/wiki/Not_invented_here"" rel=""nofollow"">NIH syndrome</a>"").</p></li>
</ul>
","14748"
"Is it possible to speed up a video using handbrake?","6456","","<p>Is it possible to speed up a video using handbrake ?</p>

<p>I am looking for a software like handbrake to convert videos for faster playback. It should speed up the video up to 30x.</p>

<p>Is it possible with handbrake or are there similar tools ?</p>
","<p>You can use <a href=""https://ffmpeg.org/download.html"">ffmpeg</a>, a free command-line tool, to do this.</p>

<p>The basic command is</p>

<pre><code>ffmpeg -i input.mov -vf ""setpts=(PTS-STARTPTS)/30"" -crf 18 output.mov
</code></pre>

<p>The <code>30</code> indicates the factor by which the video will be sped up.</p>
","18470"
"Audio and Frames Not in Sync in RAM Preview","6414","","<p>Asked this question before... having same problem.</p>

<p>When I go to the RAM preview of After Effects, the audio of the video will play on sync for a while, then will get screwy. Sometimes the audio will jump ahead or before it should. It is making the sound choppy and my characters sound stupid.</p>

<p>Please, I need a solution!</p>
","<p>Sounds like After Effects is having a problems reading the audio from the file. I'd suggest converting the audio to a separate <code>.wav</code> or <code>.mp3</code> and importing it back into After Effects: </p>

<blockquote>
  <ol>
  <li>Open <strong>Adobe Media Encoder</strong> </li>
  <li>Drag the file in question into the que</li>
  <li>Under Format select <strong>Waveform Audio File</strong></li>
  <li>Leave the preset at its default</li>
  <li>Click on the yellow text under the Output File column to set the new files destination</li>
  <li>Click the Triangle to begin encoding</li>
  </ol>
</blockquote>

<p>Then use this new file in After Effects and it should work fine. </p>
","3967"
"Reverse the blade tool (merge clips together)","6411","","<p>I'm trying to make a transition to Final Cut Pro X from iMovie. Although I'm finding it quite simple and I'm in love with the more advanced features such as how easy keying is, I seem to have a problem I'm not sure how to get around.</p>

<p>I want to be able to ""undo"" the blade tool, in effect merging clips together. How can I do this?</p>

<p>I'm aware of the compound clip feature, but that lacks the ""I'm done here, move on"" feeling I get from a clip that doesn't have a bunch of visible edits to it.</p>
","<ul>
<li>If that's a cut within the same clip, you can move the right edge of the first clip to the end of the second clip, to restore the formerly length. </li>
<li>If you want to combine different clips to make some pivotal changes or append centrical effects. You have to create a compound clip.</li>
<li>If it was a cut within one clip, you can simply undo it. But i hope you know that.</li>
</ul>

<p>I dont' know what you mean with ""I'm-done-here-move-on-feeling"", but a typical project has hundreds and thousands cuts. That's absolutely normal.</p>
","3944"
"Removing an undesired object from a clip in Final Cut","6410","","<p>I have a screen-cast clip. On a white background, I have an icon which I'd like to cover with a shape or something. In fact, I've tried using a vector shape and it partially does its job. The problem I'm having is that the clip starts with a fade in/out transition and I'd like to ""hide"" the icon also during those frames. Adding the same transition to the vector shape didn't help, so maybe I'm just using the wrong approach.</p>

<p>How could I hide that icon so that it looks natural also during the transition frames?</p>
","<p>In the end, I've kept using a vector shape. Just, I've setup two keyframes - one at the beginning of the transition and one at the end - for the ""color"" property, and used the ""Select Color"" tool so the colour matches the current background. When needed, I've added extra keyframes. It now looks fine.</p>
","1882"
"How to compile FFmpeg with libfdkaac into a single static binary?","6381","","<p>My target is to build a single static FFmpeg binary with libfdkaac and x264 support for OSX and Linux. First, I tried to build one on OSX, I follow the instruction on FFmpeg web:
<a href=""https://trac.ffmpeg.org/wiki/CompilationGuide/MacOSX"" rel=""noreferrer"">https://trac.ffmpeg.org/wiki/CompilationGuide/MacOSX</a></p>

<p><strong>Here is the configuration settings I applied to build a static binary:</strong></p>

<pre><code>./configure pkg_config='pkg-config --static' --prefix=/usr/local --extra-version=ntd_20150126 --disable-shared --enable-static --enable-gpl --enable-pthreads --enable-nonfree --enable-fontconfig --enable-libfreetype --enable-libass --enable-libfdk-aac  --enable-libmp3lame --enable-libopus --enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-filters --enable-runtime-cpudetect
</code></pre>

<p>After build process, FFmpeg is around 14Mb, which means that it doesn't include the third party libraries. In fact, it requires to link to dynamic libraries located at <code>/usr/local/lib</code>. If I removed those dynamic libs, the FFmpeg will show an error like:</p>

<pre><code>dyld: Library not loaded: /usr/local/lib/libSDL-1.2.0.dylib
Referenced from: /usr/local/bin/ffmpeg
Reason: image not found
Trace/BPT trap: 5
</code></pre>

<p>Is it the right way to make all external libraries into single static binary?</p>

<p>I would be really appreciate for your help!</p>
","<p>In my CentOS 5.11 and FFmpeg 3.0, I have to use options</p>

<pre><code>--pkg-config-flags=""--static""
--extra-cflags=""-I$HOME/ffmpeg/include -static""
--extra-ldflags=""-L$HOME/ffmpeg/lib -static""
</code></pre>

<p><strong>--enable-static</strong> tell a complier to create the ""static libraries"" (libav*.a). We can be combine FFmpeg API in the other standalone (static) application.</p>

<p><strong>--disable-shared</strong> tell a complier not to create the ""dynamically linked shared object libraries"" (libav*.so). These type of libraries can be load and use FFmpeg API by the other application.</p>

<p>These 2 options doesn't complie FFmpeg as standalone static executable.</p>
","17722"
"How to apply track mattes to multiple layers?","6370","","<p>I want to create a track matte/mask that is applied to multiple layers.
The layers should not be precomposed!</p>

<p>How could I achieve this, that I can just change the matte shape and this is reflected to any of the layers. Because currently only the layer right below the matte is masked accordingly, other layers not.</p>
","<p>You can use the <strong>silhouette alpha</strong> or <strong>stencil alpha</strong> modes on a matte layer, which will matte any layers below it. </p>

<p>Or if that doesn't work for the composition you can use expressions. Here's how:  make your first matte layer, with its mask, and then duplicate it and put it over another of the layers you want to matte. Now with both matte layers selected press <strong>m</strong> to show the <strong>mask path</strong> property for both. Select only the new layer, and while holding <em>alt/option</em> click on the stopwatch next to the <strong>mask path</strong> property to add an expression to it. You'll see a pick-whip (the spiral thing) next to the expression. Click and drag the pick-whip to the <strong>mask path</strong> property of the original layer. It should say something like <strong>thisComp.layer(""the name of the original matte layer"").mask(""mask1"").maskPath</strong>
<img src=""https://i.stack.imgur.com/6uGVI.png"" alt=""expression linking mask shapes"">
To make those layers follow the original matte layer they shoul be parented to it as well.</p>

<p>Of course the simpler, faster to render way is to precompose the matte layer on its own and use copies of that comp as your matte layers. Why was it you don't want to precomp?</p>
","13391"
"Certain audio tracks not rendering","6366","","<p>I'm not sure the best way to explain the scenario so I'll do my best, please don't hesitate to ask if I'm missing something obvious to help build the picture.</p>

<p>My problem is I've edited a video in Premiere (CS6) and when I export it part of the audio is missing and I cant seem to find any reason why.</p>

<p>The audio that is missing is at the start and end of the video, it's a stereo piece of music. The rest of the audio is mono (and renders out no problem). When I'm playing back the preview everything plays as expected, nothing seems to be muted or turned down. The settings on the export are set to match the quality of the music track that is missing. </p>

<p>Any ideas why I can't get that audio to render? It's really frustrating! Cheers.</p>

<p>EDIT: I've tried rendering in different formats and sample rates. No joy.</p>
","<p>Managed to dive in to it and root out the problem. Despite all the audio playing in the preview there was actually a channel solo'd in the audio mixer. I only noticed this when I ran an update on Premiere, de-authorised and re-authorised it (because of another issue that happened) the audio in the preview started to play back correctly as per the track solo status.</p>

<p>Thanks for those who looked at the question and tried to help.</p>
","5374"
"Camera features useful when recording a live high school basketball game?","6344","","<p>What features should I be looking for in a camera (typically found in cameras less than $1000) for shooting a high school basketball game?  The intent is to use the video for ""Game Film"" type analysis (so picture quality for rebroadcasting purposes is not as important).  </p>

<p>Some things I have noticed already (borrowing cameras from friends) </p>

<ul>
<li>Tripod is very helpful</li>
<li>Most cameras wont cover the whole court (from top bleacher position) in a single shot. (Is there a feature of the camera or tripod that would make this less painful, wide angle?)</li>
<li>Long battery life (1.5 hours)</li>
</ul>
","<p>Generally I see four options:</p>

<ul>
<li>DSLR / system cam (e.g. Canon EOS, Sony Alpha)</li>
<li>Compact camera (e.g. Fuji Film)</li>
<li>Action cam (e.g. GoPro)</li>
<li>Camcorder (e.g. Samsung HMX or Sony HDR)</li>
</ul>

<p>Before talking about solutions I would propose to think about the requirements. The following topics come to my mind:</p>

<ul>
<li><strong>Field of view</strong> - Do you want to cover the whole field only, or do you also want to zoom in on single players? Camcorders and compact cams may be limited in wide angle but often offer great zoom. Action cams normally don't zoom at all. DSLR is most flexible with the right objectives, but zooming may not be as easy to operate as with camcorders. Some compact cams stop sound recording while zooming, if that's an issue.</li>
<li><strong>Light</strong> - I suspect it may be a little dark in the gym? So high ISO values and good quality at high ISO values may be important, especially when zooming in.</li>
<li><strong>Unattended operation</strong> - Do you need to leave the cam unattended? You'd probably wouldn't want to leave a DSLR unattended.</li>
<li><strong>Recording time</strong> - battery life time is helpfull, the option to record with power supply may be even better. Also some cams stop recording after some minutes (e.g. EOS 600D after 12 mins HD recording).</li>
<li><strong>Focus</strong> - Many cameras focus on the nearest objects in their field of view. This could be the spectators, not the players. So the camera should offer a deep depth of focus, or the option to manually set the focus.</li>
<li><strong>Quality</strong> - Do you need still images? Which would require you to set a short shutter time. High ISO values would be even more important then.</li>
<li><strong>Display</strong> - Where do you want to watch the recordings? If directly from the camera, you'll need an HDMI output or something similiar. Some Camcorder even offer an integrated beamer, so you could watch the recording directly in the fitting room without any further equipment. And you'll probably don't want to show the whole game, just parts of it? So you might want convenient fast-forward/rewind features. Some cams even offer very limited postprocessing capabilities, so you could cut away the uninteresting parts of the game.</li>
</ul>

<p>Accessories:</p>

<ul>
<li><p>A tripod may definitely be a good idea, but it needs its space. A smaller device to fix your camera to e.g. a fence may be even more appropriate (e.g. Gorilla pod). If you go for a tripod make sure it's high enough and strong enough to hold the weight of your cam.</p></li>
<li><p>Memory Card - it must be fast enough. If it's not, it may stop the recording in the middle of the game. How fast? Depends on the video format of your recording.</p></li>
<li><p>Replacement batteries</p></li>
</ul>

<p>Summary:
Assuming you don't want to zoom in and you want to leave the camera unattended somewhere, I tend to agree with AJ Henderson: An action cam might be the best way to go. Add a gorilla pod to fix the camera somewhere, get a big and fast memeory card and make sure the recording/battery life time fits your needs.
For more sophisticated use cases a Camcorder with integrated beamer, power supply, strong wide angle, high ISO plus a tripod may the right choice.</p>
","10865"
"YouTube Compression: Why?","6301","","<p>There's a lot of advice on the internet on encoding settings to apply to your video before uploading to YouTube (<a href=""https://support.google.com/youtube/answer/1722171?hl=en"">https://support.google.com/youtube/answer/1722171?hl=en</a>, <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/2DuHhd3lwOs?start=0""></iframe>
            </div></div>, ...). They say that not only will compression allow your video to upload faster but it will make it look better when played back on YouTube (an example of this is at 0:18 in the YouTube link above). </p>

<p>What I also read is that YouTube re-encodes any video that you upload (example: ""Telestream Episode product manager Kevin Louden started by noting that YouTube re-encodes all video uploaded to the site"" - <a href=""http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/Encoding-for-YouTube-How-to-Get-the-Best-Results-83876.aspx"">http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/Encoding-for-YouTube-How-to-Get-the-Best-Results-83876.aspx</a>).</p>

<p>So the obvious question for me is if YouTube re-encodes anything that it's asked to upload, why should different codecs make a difference to final playback quality on YouTube? To get the best playback quality on YouTube, shouldn't I upload a lossless file and let YouTube compress it however it wants?</p>

<p>I don't care about upload time or file size, I just care about playback quality on YouTube.</p>
","<p>I've answered a similar question some time ago. YouTube added a few codecs since then but all the info there still applys: <a href=""https://video.stackexchange.com/questions/5318/what-codec-will-my-youtube-uploads-be-output-in-and-what-codec-should-i-use-to-u/5715#5715"">How does YouTube encode my uploads and what codec should I use to upload?</a></p>

<p>Short answer: Yes if you are concerned about maximum quality a lossless codec or visually lossless codec is the way to go. Re-encoding always means loss of information if you re-encode with a lossy codec like h264.
Saying that videos will look better with compression is just a wrong statement.
Compression reduces file size, that's the only benefit you get from it. Best case you don't loose information with that compression but with most codecs like h264 you do because they discard information and re-arrange it in order to achieve a high compression.</p>

<p>Though different codecs do have an affect on the outcome, encoding a video with MPEG2 or h264 makes a huge difference. The former will usually have way more compression artifacts depending on your encoding settings than an h264 encoded video with similar bitrate.
So if your source is already having artifacts the second encode from YouTube won't make it look any better but worse.</p>
","12401"
"What is the Marketshare of the big three video editing (NLE) software packages?","6236","","<p>Cannot find a source to see market share or change for top video editing software tools used today.</p>

<p>The big three are:<br>
Avid<br>
Final Cut Pro<br>
Adobe Premiere Pro (after effects)<br></p>

<p>I know Avid is still used by large film and movie productions.<br>
But from the grass roots perspective, every day users, it seems to be overwhelmingly Adobe products. But this is based on my anecdotal personal perspective. Wanted something more concrete.</p>
","<p>This kind of data is actually remarkably hard to come by.  There are a few industry reports available for thousands of dollars, but the best free resource I could find when I was searching was an informal poll that Dave Dugdale did.  It was an imperfect test by his own admission because it allowed both voting up and voting down, but it gives at least some kind of an idea of the rough market share.</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/5zgdYW6rh3o?start=0""></iframe>
            </div></div></p>
","12130"
"How to fade in/out a video/audio clip with unknown duration?","6193","","<p>I'm working on a project where I'm using ffmpeg to batch convert some video files, and adding fade in/out effects at the head and tail of each clip.</p>

<p>I've been using this bit of code to add the fade/out effect on clips with known durations:</p>

<p><code>
ffmpeg -i clip.mp4 -filter:v 'fade=in:0:30,fade=out:9650:30' -c:v libx264 -crf 22 -preset veryfast -c:a copy fadeInOut.mp4
</code></p>

<p>My two questions are:</p>

<ol>
<li>How can I simultaneously fade the audio in/out?</li>
<li>Can ffmpeg automatically figure out the duration of the clip and be told just fade out the last 30 frames?</li>
</ol>

<p>Thanks!</p>
","<p><strong>#1</strong> To simultaneously fade the audio in/out:</p>

<pre><code>ffmpeg -i clip.mp4 -vf 'fade=in:0:30,fade=out:960:30'
                   -af 'afade=in:st=0:d=1,afade=out:st=32:d=1'
       -c:v libx264 -crf 22 -preset veryfast fadeInOut.mp4
</code></pre>

<p>The afade times are in <em>seconds</em>.</p>

<p><strong>#2</strong> Automatically? No. But see workaround below</p>

<p>You can first run ffprobe to get duration.</p>

<pre><code>ffprobe -i clip.mp4 -show_entries stream=codec_type,duration -of compact=p=0:nk=1
</code></pre>

<p>You'll get something like this:</p>

<pre><code>video|13.556000
audio|13.816000
</code></pre>

<p>You can then use the above to place your fades. These times are in seconds.</p>

<p><strong>Workaround</strong></p>

<pre><code>ffmpeg -i clip.mp4 -sseof -1 -copyts -i clip.mp4 -filter_complex
       ""[1]fade=out:0:30[t];[0][t]overlay,fade=in:0:30[v];
        anullsrc,atrim=0:2[at];[0][at]acrossfade=d=1,afade=d=1[a]""
       -map ""[v]"" -map ""[a]"" -c:v libx264 -crf 22 -preset veryfast -shortest fadeInOut.mp4
</code></pre>

<p>FFmpeg has a <code>sseof</code> option that allows one to seek an input from the end. We can use that to accomplish our goal. So we feed the input twice, with the 2nd time ingesting only the last second. We tell FFmpeg to preserve the timestamps, so that ffmpeg preserves the temporal position of this tail portion. </p>

<p>We apply a fade out to this tail and then overlay the result onto the full input. Since they are the same media file, the foreground completely covers the background, and since <code>copyts</code> was applied, the overlay happens upon the corresponding identical frame in the background input.</p>

<p>For audio, we create a blank dummy audio of duration 2 seconds, and then apply an audio crossfade from the main audio to this dummy audio. Since the 2nd audio is blank, this is, in effect, a fade-out for the main input. The <code>-shortest</code> is added to leave out portions of the dummy audio after the crossfade has occurred.</p>
","19871"
"Why is my video alternating between blurry and sharp?","6169","","<p>I notice this on a regular basis while watching videos I've downloaded. The image will alternate pretty quickly between blurry and sharp and sometimes the effect is quite drastic. Below is an example image of a tree and the difference between two frames.</p>

<p><img src=""https://i.stack.imgur.com/peOzz.jpg"" alt=""Two frames less than a second apart""></p>

<p><strong>What is this effect called and why does it happen</strong>? I'm fairly certain it has to do with compression but what is going on?</p>

<p>If you receive a video like this and there's nothing you can do with how it was rendered or processed, <strong>is there anything you can do after the fact to minimize the effect</strong>? Settings in your video player maybe?</p>

<p>This particular screen shot was taken in Windows Media Player on Windows 7 playing an AVI file. The effect is still visible in VLC and I first noticed this while playing video through my PS3 off a harddrive. I can't say for sure, but I'm fairly certain that it's happened on multiple file types as well.</p>
","<p>The effect that you've seen is a side effect of the compression algorithm that was used to encode the video.</p>

<p>The most common compression algorithms (MPEG-4 and MPEG-2 among them) compress different frames with different methods, and that is why two very close and relatively similar frames may look so different.</p>

<p>The frame that looks pretty good is likely a <em>keyframe</em>. This is a frame that has all its pixels represented in the encoded stream. The compression introduces some loss of quality, but since the whole image is encoded the result typically looks good.</p>

<p>The image that looks bad is a frame that is not fully represented in the encoded stream. To make videos smaller, for certain parts of the picture the encoder references parts of previous frames that the player/decoder keeps stored in memory, so only the parts that are different are stored for the frame. Because of this these frames end up being a sort of collage. If the bitrate is too low, then you end up with really washed up images like the one you show. But increasing the bitrate enough helps dissimulate this effect, to the point that it is barely or not noticeable. Consider that all the TV that you watch (digital over the air, satellite, FIOS, cable, etc.) is encoded in this way.</p>

<p>For MPEG-2 it is common practice to insert a keyframe in the stream every half a second (15 frames for 29.97fps video, 12 frames for 25fps). For H.264 sometimes they go even longer than half a second.</p>

<p>In general, the image quality of a compressed video is at its highest at the keyframe points, and then slowly starts to degrade, going up again with the next keyframe.</p>

<p>For a more detailed explanation of frame types used in compression algorithms see the Wikipedia article <a href=""http://en.wikipedia.org/wiki/Video_compression_picture_types"" rel=""nofollow"">Video compression picture types</a>.</p>

<p>Unfortunately a player cannot improve the quality of the video, since the problem is that the information to reproduce the frame with better quality isn't there. The only ways to address this is by increasing the bitrate when encoding and/or increase the number of keyframes. </p>

<p>On the player side I think the only thing you can do is play the video in a smaller window, so then these artifacts will be less visible.</p>
","4060"
"How to set up a 3-screen video installation to sync correctly?","6107","","<p>I need to set up a three-screen HD video installation with sound for a friend's art exhibition. The videos will likely be provided as separate video files, each with a different video but exactly the same length and intended to be played synchronously. I'm looking for advice on how to set up the installation so that the three screens will be synchronised correctly and played on an endless loop. What's the best way to achieve this with consumer-grade equipment?</p>

<p>I can hopefully borrow three HD (720p) projectors - not sure yet which model, I'll hopefully find that out soon - or can possibly purchase three cheaper projectors specifically for this. I've been told it's important to have the same model (&amp; year?) to ensure the color will be effectively matched.</p>

<p>I can borrow a laptop or mac mini to drive the video or could purchase one or more cheap devices if needed. I can also borrow blu-ray players if that's any use. Again, I need advice about what device to borrow/buy.</p>

<p>Sound is two-track stereo, so can be played from just one of the videos. That should be the easy bit. </p>
","<p>The Matrox graphics card is probably the easiest and most versatile approach. I use the TripleHead2Go, which as you'd assume sends a signal(s) to up to three monitors/projectors.</p>

<p>Multiple computers will never stay in sync. I found this out the hard way the night of an event.</p>

<p>In terms of software, if you're on OS X check out <a href=""https://vidvox.net/"" rel=""nofollow"">VDMX</a>. There's a fully functional demo that you can grab. Setting it up for your needs would be fairly trivial after a quick search for the right approach.</p>

<p>PS. I would have just commented on the other answers, but I'm new here and don't have the credentials.</p>
","15375"
"Build linux render farm in order to render adobe premiere projects","6043","","<p>I edit my videos on Adobe Premiere (or even Edius) and I want to build a linux based render farm for rendering my projects. </p>

<p>The concepts I've found in my research are 'render manager' and 'frame serving' which won't do a cross platform stuff in the way I desire or please tell me that I'm mistaken!</p>

<p>How do I set up a render farm for this task?</p>

<p><em>Note that I have a linux server which means it doesn't have a Graphical User Interface.</em></p>
","<p>You can do this by using ffmpeg. Here are the steps:</p>

<ol>
<li><p>First do steps of the second method (""Use a frameserver"") <a href=""https://trac.ffmpeg.org/wiki/Encode/PremierePro"" rel=""nofollow noreferrer"">here</a>.</p></li>
<li><p>Beginning this step you should be streaming your Premier timeline through Debugmode and Avisynth on a specific IP and port. Now open a command line and run this ffmpeg command:</p></li>
</ol>

<p><code>ffmpeg -i frameserver.avs -f mpegts  tcp://[IP address of your server]:[open port on your server]</code></p>

<p>By running this command, you are sending the Avisynth output (streaming) to a destination machine (your Linux server) through ffmpeg.
After this step all you have to do on the client server (where you edit your videos) is done. Next, set up the server side.</p>

<ol start=""3"">
<li>On the server (your Linux server), make sure you have ffmpeg installed and run the following command:
<code>ffmpeg -i tcp://[your server IP]:[The same port you entered in step 2]?listen -c:v libx264 -preset medium -crf 23 -pix_fmt yuv420p -c:a libfdk_aac -vbr 4 output2.mp4</code></li>
</ol>

<p>Through this command, ffmpeg will be listening on a specified IP and port and receiving sent packets and encode them with your favorite options!</p>
","15799"
"Can't add Motion Blur to layer imported from Photoshop","6011","","<p>I can't enable Motion Blur on this layer:</p>

<p><img src=""https://i.stack.imgur.com/6moQz.png"" alt=""enter image description here""></p>

<p>Any ideas what could be wrong?</p>
","<p><strong>Thats because you have the collapse transformations button ticked:</strong> 
<img src=""https://i.stack.imgur.com/1lkHW.png"" alt=""Collapse Transformation""></p>

<p>When <a href=""http://help.adobe.com/en_US/AfterEffects/9.0/WS064964FC-424C-4e5a-A5C3-2160B3DFBCA4a.html"" rel=""nofollow noreferrer"">collapse transformations</a> is ticked After Effects takes the options of the layers <strong>inside</strong> the 'loader' composition. So you will need to open up the 'loader' composition (double click) and then you can enable motion blur on the layers inside.</p>

<p>When you import Photoshop documents into After Effects as compositions, any groups in the PSD become pre-comps within AE. </p>
","3965"
"Is there a way to pause and resume FFmpeg encoding?","6011","","<p>I record a few hours of video footage every day, which I set in queue for encoding with <code>ffmpeg</code>. I use the <code>veryslow</code> preset with <code>x265</code>, so a single hour of video may take up to 20 hours or more to encode.</p>

<p>Since my computer is working day and night, I'm wondering if <code>ffmpeg</code> has a way to <strong>pause and resume</strong> encoding? I'm using <code>Terminal</code> for OS X.</p>

<p>If so, can I also continue encoding even after shutting down Terminal and rebooting the computer? I think the Terminal session is restored when you log in again in OS X, at least the Terminal history is.</p>
","<h1>Suspend</h1>

<p>A simple method is to suspend it with <kbd>ctrl</kbd>+<kbd>z</kbd>. Or you could get the PID with <code>pgrep ffmpeg</code> then use <code>kill -s SIGSTOP &lt;PID&gt;</code> to suspend.</p>

<p>Then resume with <code>fg</code> command or <code>kill -s SIGCONT &lt;PID&gt;</code>.</p>

<p>Unfortunately this will not survive a reboot.</p>

<h1>VM</h1>

<p>If you use a virtual machine, with something like VirtualBox, you could perform your encoding in a guest VM. It will allow you to ""save the machine state"" at any time which can survive a reboot. It can also allow you to assign max CPU resources for the guest so your host will always have available resources.</p>

<p>Possibly overkill, but if you're familiar with VMs or using one already it is a possible solution.</p>
","17069"
"How to extract MOOV atom/metadata from MP4 file?","5979","","<p>Can one ""dump""/extract the MOOV atom from an MP4 file? If so, how? Hoping for an obscure FFMPEG command.</p>

<p>I need to be able to process/play a stream without searching for the MOOV atom over and over again. For very long streams, this can run into several megabytes of data. It's rather strange, I don't think the MOOV atom is huge, I just think that it is scattered thruout the first N megabytes (i.e. toplevel data at the begnning, with references to further atoms that are later in the stream).</p>

<p>Thus, it would make sense for me to extract the MOOV atom out of the file, keep it </p>
","<p>You can use <a href=""http://atomicparsley.sourceforge.net/"" rel=""noreferrer"">AtomicParsley</a> to parse the metadata of a MP4-file. For example</p>

<pre><code> AtomicParsley /path/to.mp4 -T 1
</code></pre>

<p>will print the whole atom tree.</p>
","15121"
"RG6 vs RG59 HD-SDI cables for live stream","5903","","<p>It seems there are lots of discussions on the web, none recently though, on <code>RG6</code> vs <code>RG59</code> and the distances that they can carry signal.</p>

<p>For situations when quality matters, such as live streaming (as opposed to something like CCTV), is <code>RG6</code> the only way to go?</p>

<p>A <a href=""http://www.bhphotovideo.com/c/product/415820-REG/Canare_L_4CFB_984_L_4CFB_75_Ohm_RG59.html"" rel=""nofollow"">984' RG59</a> cable costs just about as much as a <a href=""http://www.bhphotovideo.com/c/product/863724-REG/Canare_200_HD_SDI_Video_Coaxial.html"" rel=""nofollow"">200' RG6</a> cable. Assume that the signal is powerful enough to send the signal over 900'. Will the <code>RG59</code> cable experience significant and noticeable signal loss compared to the <code>RG6</code> cable? </p>

<p>There is a significant price different between the two types, so if I can get away with a <code>RG59</code> cable, certainly I will.</p>
","<p>Unfortunately there is no perfect or exact answer. Your question topic mentions HD-SDI, which is a digital signal. Those tends to degrade 'cliff-wise', unlike analog signals where degradation is gradual. It will partly depend on the quality of the receiving device -- whether or not, or how well, it can capture the signal as the eye pattern turns to mush.</p>

<p>Having said that, 900 feet over RG59 is bound to be trouble, even over the premium shielded variants like Belden 1505a. For 720p, that's rated for maybe 200 feet. Your footage may vary (-:</p>

<p>RG6 will be better, but again, shielding matters so you're better off with 1694a or equivalent. Even then, I wouldn't gamble on 900' without testing it in place.</p>
","7605"
"Video Capture using ffmpeg (V4L2 indev) Results in Bad A/V Sync","5633","","<p>I actually contributed to a bug report on this issue a year and a half ago ( <a href=""https://trac.ffmpeg.org/ticket/692#comment:15"" rel=""nofollow"">https://trac.ffmpeg.org/ticket/692#comment:15</a> ). Basically, the problem is that using <code>ffmpeg</code> to record from a USB video capture device leaves the audio and video tracks out of sync. Depending on which input you list on the command line first, the sync problem will be different.</p>

<p>Since contributing to that report, <code>ffmpeg</code> has undergone many revisions, and the bug report has been updated to say that the problem was fixed by adding new command line options to the V4L2 indev (<code>-ts default|abs|mono2abs</code>). However, I have no idea what these new options mean or how they're meant to be used. I tried again tonight using <code>ffmpeg</code> to record from the video capture device. While the audio and video aren't as badly out of sync as they were 18 months ago, they're still out of sync.</p>

<p>Clearly, someone thinks this is fixed, leading me to think that I need more magic on the command line. Here's my latest incantation:</p>

<pre><code>ffmpeg  -f alsa -i hw:1  -f v4l2 -i /dev/video0  -acodec libfaac -b:a 128k -vcodec libx264 -b:v 12M -g 1 -preset ultrafast  SYNCTEST.mp4
</code></pre>

<p>Adding various <code>-ts</code> options to the line doesn't seem to accomplish anything. I'm using <code>ffmpeg</code> version 2.1.4. Can anyone provide further insights?</p>

<p><strong>Console Output:</strong></p>

<pre><code>$ ffmpeg  -f alsa -i hw:1  -f v4l2 -i /dev/video0  -acodec libfaac -b:a 128k -vcodec libx264 -b:v 12M -g 1 -preset ultrafast  SYNCTEST.mp4
ffmpeg version 2.1.4 Copyright (c) 2000-2014 the FFmpeg developers
  built on Feb 24 2014 08:21:48 with gcc 4.8 (Debian 4.8.2-16)
  configuration: --prefix=/usr --extra-cflags='-g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security ' --extra-ldflags='-Wl,-z,relro' --cc='ccache cc' --enable-shared --enable-libmp3lame --enable-gpl --enable-nonfree --enable-libvorbis --enable-pthreads --enable-libfaac --enable-libxvid --enable-postproc --enable-x11grab --enable-libgsm --enable-libtheora --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libx264 --enable-libspeex --enable-nonfree --disable-stripping --enable-libvpx --enable-libschroedinger --disable-encoder=libschroedinger --enable-version3 --enable-libopenjpeg --enable-librtmp --enable-avfilter --enable-libfreetype --enable-libvo-aacenc --disable-decoder=amrnb --enable-libvo-amrwbenc --enable-libaacplus --libdir=/usr/lib/x86_64-linux-gnu --disable-vda --enable-libbluray --enable-libcdio --enable-gnutls --enable-frei0r --enable-openssl --enable-libass --enable-libopus --enable-fontconfig --enable-libpulse --disable-mips32r2 --disable-mipsdspr1 --disable-mipsdspr2 --enable-libvidstab --enable-libzvbi --enable-avresample --enable-libiec61883 --enable-libfdk-aac --enable-vaapi --enable-libdc1394 --disable-altivec --disable-armv5te --disable-armv6 --disable-vis --shlibdir=/usr/lib/x86_64-linux-gnu
  libavutil      52. 48.101 / 52. 48.101
  libavcodec     55. 39.101 / 55. 39.101
  libavformat    55. 19.104 / 55. 19.104
  libavdevice    55.  5.100 / 55.  5.100
  libavfilter     3. 90.100 /  3. 90.100
  libavresample   1.  1.  0 /  1.  1.  0
  libswscale      2.  5.101 /  2.  5.101
  libswresample   0. 17.104 /  0. 17.104
  libpostproc    52.  3.100 / 52.  3.100
Guessed Channel Layout for  Input Stream #0.0 : stereo
Input #0, alsa, from 'hw:1':
  Duration: N/A, start: 1394670862.629223, bitrate: 1536 kb/s
    Stream #0:0: Audio: pcm_s16le, 48000 Hz, stereo, s16, 1536 kb/s
Input #1, video4linux2,v4l2, from '/dev/video0':
  Duration: N/A, start: 575244.424820, bitrate: 165722 kb/s
    Stream #1:0: Video: rawvideo (YUY2 / 0x32595559), yuyv422, 720x480, 165722 kb/s, 29.97 fps, 29.97 tbr, 1000k tbn, 1000k tbc
No pixel format specified, yuv422p for H.264 encoding chosen.
Use -pix_fmt yuv420p for compatibility with outdated media players.
[libx264 @ 0x1d1b0a0] using cpu capabilities: MMX2 SSE2Slow SlowCTZ
[libx264 @ 0x1d1b0a0] profile High 4:2:2 Intra, level 3.0, 4:2:2 8-bit
[libx264 @ 0x1d1b0a0] 264 - core 142 - H.264/MPEG-4 AVC codec - Copyleft 2003-2014 - http://www.videolan.org/x264.html - options: cabac=0 ref=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=0 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=1 keyint_min=1 scenecut=0 intra_refresh=0 rc=abr mbtree=0 bitrate=12000 ratetol=1.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=0
Output #0, mp4, to 'SYNCTEST.mp4':
  Metadata:
    encoder         : Lavf55.19.104
    Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv422p, 720x480, q=-1--1, 12000 kb/s, 30k tbn, 29.97 tbc
    Stream #0:1: Audio: aac (libfaac) ([64][0][0][0] / 0x0040), 48000 Hz, stereo, s16, 128 kb/s
Stream mapping:
  Stream #1:0 -&gt; #0:0 (rawvideo -&gt; libx264)
  Stream #0:0 -&gt; #0:1 (pcm_s16le -&gt; libfaac)
Press [q] to stop, [?] for help
frame=  436 fps= 30 q=-1.0 Lsize=     706kB time=00:00:14.91 bitrate= 387.9kbits/s dup=0 drop=4    
video:448kB audio:240kB subtitle:0 global headers:0kB muxing overhead 2.682203%
[libx264 @ 0x1d1b0a0] frame I:436   Avg QP: 0.00  size:  1051
[libx264 @ 0x1d1b0a0] mb I  I16..4: 100.0%  0.0%  0.0%
[libx264 @ 0x1d1b0a0] final ratefactor: -25.57
[libx264 @ 0x1d1b0a0] coded y,uvDC,uvAC intra: 0.0% 0.0% 0.0%
[libx264 @ 0x1d1b0a0] i16 v,h,dc,p: 97%  0%  3%  0%
[libx264 @ 0x1d1b0a0] i8c dc,h,v,p: 100%  0%  0%  0%
[libx264 @ 0x1d1b0a0] kb/s:252.03
</code></pre>
","<p>The first thing that I noticed is that your camera is delivering <code>yuv422p</code> - which isn't bad in and of itself, but you could try forcing it to yuv420 by adding <code>-pix_fmt yuv420p</code> to your command. (This is nice if you ever plan on displaying your video on anything that isn't linux.)</p>

<p>The second thing that I noticed is that the start times are wildly different - which can often be a signal that things are going to sync wrong.</p>

<p>This is the command line that I used  that was able to reproduce your lag.</p>

<pre><code>ffmpeg -f alsa -ac 2 -i hw:0,0 -f v4l2 -i /dev/video0 -r 25 \
       -acodec libfdk_aac -b:a 128k -pix_fmt yuv420p -vcodec libx264 SYNCTEST.mp4
</code></pre>

<p>by adding a <code>-force_key_frames 00:00:00.000</code> I was able to sync them correctly.</p>

<pre><code>ffmpeg -f alsa -ac 2 -i hw:0,0 -f v4l2 -i /dev/video0 \ 
       -force_key_frames 00:00:00.000 -r 25 -acodec libfdk_aac \ 
       -b:a 128k -pix_fmt yuv420p -vcodec libx264 SYNCTEST.mp4
</code></pre>
","12338"
"After effects: Moving text along 3D path","5606","","<p>After Effects beginner here.</p>

<p>The effect I'm after is where text is appearing from behind an object. So far, I'm using the position and rotation parameters.</p>

<p>But the text is rigid. I found something about having text along a path which is what I want except I haven't found out how to work it in 3D.</p>

<p>Here is what I have so far. I need the text to look like it's bending as it comes around the ... well, bend!</p>

<p><img src=""https://i.stack.imgur.com/rwTdy.png"" alt=""enter image description here""></p>
","<p>to accomplish this, we need to cheat something. I assume you are trying to do something like this in the following image:</p>

<p><img src=""https://i.stack.imgur.com/9tzNo.png"" alt=""enter image description here""></p>

<ol>
<li>First, create a camera in your comp (i see you already have one)</li>
<li>Then create a text layer and make it 3D (you have this as well)</li>
<li>Select your layer and select <code>Ellipse tool (Q)</code> but do not apply
anything.</li>
<li>While your text layer selected, double click the Ellipse tool in
your toolbar. This will create a circular mask on your text layer.</li>
<li>Rotate your Text layer -90 degrees on X Axis.</li>
<li>Twirl down the controls of the Text layer and under Text controls
you will see <code>Path Options</code>. In these options select your mask. And
change your settings as you wish.</li>
<li>Now expand all the controls of that layer. There you will see a text
label <code>Animate:</code> with a small circled arrow.</li>
<li>Select Enable Per-character 3D.</li>
<li>Then in the same menu, select Rotation.</li>
<li>There will be an <code>Animator 1</code> section, expand it and change the X
 Rotation to 90 degrees.</li>
</ol>

<p>Now check with your camera, orbit around, you should see your text bends through your mask.</p>

<p>To animate this text along that path, get back to the <code>Path Options</code>. Keyframe the <code>First Margin</code> option or add an expression by <code>Alt+Left Click</code>, and add this value and edit as you like:</p>

<pre><code>time * 500
</code></pre>

<p>This will get the current frame and multiply it with 500 and give this value to <code>First Margin</code> property.</p>

<p>I am also adding the link for the project, but you need at least CS5.5 to open: <a href=""http://www.mediafire.com/?13dl3x67w4v5yod"" rel=""nofollow noreferrer"">http://www.mediafire.com/?13dl3x67w4v5yod</a></p>
","5379"
"Which is better quality, a larger 480p video or a smaller 1080p video?","5533","","<p>On the Internet, I sometimes see lower resolution videos that have a larger file size than a higher resolution version.</p>

<p>Which of these should have a better apparent quality when displayed on a 50'' FHD TV?
A 4GB 480p or a 2GB 1080p ???</p>
","<p>Well you CAN make rough assumptions. For a live action film a bit-rate of 3-8 Mbit/s is very advisable at 1080p. At 3Mbit/s you will very likely have noticable artifacts, thats usually an advisable bitrate for 720p video.
A file at SD resolution 480p/567p that is around 4GB in size you probably deal with a DVD that wasn't trans-coded for archiving.</p>

<p>Which is better is hard to say though, in case of a DVD encode you probably have MPEG2 or MPEG4-v2 encoded video and with HD/Blu-ray you will very likely have h264 which is more efficent than MPEG2/MPEG4-v2/Xvid (h264 is MPEG4-v10). So a low bitrate with h264 is not as bad as a low bitrate with MPEG2/MPEG4-v2/Xvid.</p>

<p>Generally I would probably go with the 1080p with less bitrate if its smaller in size, just because its smaller in size and its very likely that on a 1080p monitor/TV the video will not look worse than the upscaled DVD.</p>
","12355"
"How can I seamlessly loop with turbulent displace filter in AE?","5509","","<p>Please have a look at my website <a href=""http://www.solstice.co.il"" rel=""nofollow noreferrer"">solstice.co.il</a> on a chrome browser.  On the left you see a looping movie of the tree wobbling around. This was made by tweening the evolution filter of the turbulent displace filter in After Affects. </p>

<p><img src=""https://i.stack.imgur.com/OAl5X.jpg"" alt=""enter image description here""></p>

<p>The problem is that because of the fractal nature of the effect, I could not find a way to seamless loop the animation, which led me to resort in an ugly cross-dissolve. </p>

<p>How can I create a more pleasing seamless loop with this effect?</p>

<hr>

<p><strong>update</strong></p>

<p>I've redone the animation manually using the puppet tool, which is closer to what I wanted but not perfect, so the question still stands. The old animation for reference can be found here:
<a href=""http://solstice.co.il/videos/tree_old.webm"" rel=""nofollow noreferrer"">http://solstice.co.il/videos/tree_old.webm</a></p>
","<p>In order to get a fluid and seamless loop, it's key to <strong>enable Cycle Evolution</strong> and <strong>move the last keyframe 1 frame beyond the end</strong> of the composition.</p>

<ol>
<li>Add the <strong>Turbulent Displace</strong> Effect onto the footage</li>
<li>Open up <strong>Evolution Options</strong></li>
<li>Enable <strong>Cycle Evolution</strong> </li>
<li>Go to the <strong>first frame</strong> in the timeline</li>
<li><p>Click on the stop watch of the <strong>Evolution value</strong>:</p>

<p><img src=""https://i.stack.imgur.com/0nxoM.png"" alt=""enter image description here""></p></li>
<li><p>Go to the <strong>last frame</strong> in the timeline and set the <strong>Evolution Times</strong> value to <code>1x</code>, this is equal to 360 degree (one revolution)</p></li>
<li><p>Move the second keyframe <strong>1 frame beyond the end</strong> of the composition:</p>

<p><img src=""https://i.stack.imgur.com/47sEr.jpg"" alt=""enter image description here""></p></li>
</ol>

<h3>Example</h3>

<p><img src=""https://i.stack.imgur.com/jsS5Z.gif"" alt=""enter image description here""></p>

<ul>
<li>Composition length: <code>51</code> frames</li>
<li>First evolution keyframe value: <code>0x</code> at frame <code>0</code></li>
<li>Second evolution keyframe value: <code>1x</code> at frame <code>51</code></li>
</ul>

<p><strong>Note:</strong> This seems to work with all fractal effects.</p>
","15816"
"How to parent a mask to another element in AfterFX?","5502","","<p>I have this video whereby text is revealed one letter after another. A text cursor is moving along with each letter to simulate the fact of writing the text on a word processor.</p>

<p>I have separate animations for the text being revealed: a mask expands, for the cursor: the position. Keyframes are aligned and the animation is linear.</p>

<p>However, the cursor is overlapping the text at the start of the animation as you can see here:</p>

<p><img src=""https://i.stack.imgur.com/evbit.png"" alt=""enter image description here""></p>

<p>I also tried adding a solid layer that moves with the text and hiding everything to the right of the cursor. That works except when the background is not a solid color as is my case.</p>

<p>So my question is if I can parent the mask's movement to the cursor, or vice-versa? Alternatively, is there a better way to do it?</p>
","<p>At the moment parenting a mask or its vertices to another object like a null is not possible by default. Although it is <a href=""http://blogs.adobe.com/aftereffects/2012/12/top-feature-requests-for-after-effects-in-2012.html"" rel=""nofollow noreferrer"">one of the most wanted features</a>, probably there is no official solution for this. But in the same link you can find a link to a forum post where scripting master Dan Ebberts <a href=""http://forums.adobe.com/thread/881618"" rel=""nofollow noreferrer"">offers a solution</a>. This solution has some limitations though, yet it might work in your project.</p>

<p>My solution would be more manual, this is not the fastest way but it works. If you are using a text layer, try adding an animator for the opacity, based on the characters.</p>

<p><img src=""https://i.stack.imgur.com/8uZHI.png"" alt=""Screenshot""></p>

<p>Then animate your cursor layer's position accordingly. Or better, parent it to a null and animate your null and use hold keyframes.</p>

<p>This option is better than animating a mask on text layers, as it reveals each character. In your screenshot, there is this situation that your letters are revealed as if a linear wipe is used.</p>
","5503"
"Can H264 achieve ""equivalence"" with ProRes 422?","5487","","<p>Has anyone done or seen any tests comparing Apple ProRes 422 with high-bitrate H.264?</p>

<p>We use 422 as a delivery format to go to DCP for theatrical versions of trailers.
The bitrate is around 150mbps.</p>

<p>We are wondering if it is possible to reach adequate quality with H.264. Say, with bitrates of 50-100mbps, and tweaking the GOP size/structure. (E.g., only using ""I"" frames.)</p>

<p>Assuming H264 is a more ""efficient"" codec, we are hoping to get smaller file sizes.</p>

<p>Or does H.264 just not have the color depth to compete?</p>

<p>It would be great to see if someone has done some real-world testing.</p>

<p><strong>Update 1:</strong> I just did a quick test using 50mbps, GOP size 1, I-frames only ... and ""by eye"" I see no difference whatsoever. Same level of noise, same colors. The ProRes is 1.92GB, the H264 is 566MB. How would one ""test"" the difference technically? Or measure the color information?</p>

<p><strong>Update 2:</strong> I did some more testing ... with Adobe Media Encoder, using MPEG2, and the ""4:2:2"" profile, the ProRes file went from <strong>1,920MB to 290MB</strong>! And I would say, subjectively, that is is 99% as good. This is remarkable. (With H264 I was down to 500-700MB)</p>
","<p>Well going by the numbers h264 has a lesser bit-depth and color accuracy than ProRes 422. PR422 has 10bit and 4:2:2 chroma sub-sampling, h264 has 8bit and 4:2:0 unless you encode in the Hi422P Intra profile which isn't very well supported in the wild but offers 10bit and 4:2:2.
So in that case I don't think you will have any difference what so ever between the two formats but a better compression ratio than with ProRes.</p>

<p>E: If you want to go real ape shit you can also encode in the 4:4:4 Intra Profile, that supports up to 14 bit of color depth. So technically superior to ProRes4444. Though you probably wont find any commercial application that supports it.</p>

<p>On another note, I don't think content delivery for cinema should be done in h264 nor ProRes 422 especially when you plan to encode to a lossless codec (JPEG2000/DCP) afterwards. It just doesn't make much sense to do so, all you loose is quality even though there is no need to do so, you only save space until you encode your DCP. There are other good lossless codecs, that offer very good compression, to use before encoding the DCP.</p>

<p>You could for example go directly to DCP, the Adobe Media Encoder offers export to 2k DCP since CC 2014, you can skip an encoding step and have a lossless codec with very good compression ratio.</p>

<p>Another great intermediate codec is <a href=""http://www.videohelp.com/tools/Ut-Video-Codec-Suite"" rel=""nofollow"">UtVideo</a> aswell as <a href=""http://diracvideo.org/schrodinger-faq/"" rel=""nofollow"">Schrdinger</a> (an implementation of the Dirac codec from BBC, its available through <a href=""https://www.ffmpeg.org/download.html"" rel=""nofollow"">FFmpeg</a>).</p>

<p>In the end though theory always differs from practice and if you don't deliver for nation wide cinemas with excellent projectors and what not, that quality difference in your delivery chain will be neglectable.
The only thing that could be pitfall is that h264 is very complex an by that always prone for some weird visual defects, so final inspection is always a necessity with h264.</p>
","12499"
"Convert variable input formats to black and white MP4","5459","","<p>I am currently using an FFMPEG command like:</p>

<p>ffmpeg -i input.mov -vf scale=1920:1080 output.mp4</p>

<p>to convert varying input format file to a fixed resolution MP4 output file.  One final step I need is to make the output Black and White (the input files will be full color).</p>

<p>I have tried both 
    -pix_fmt gray 
and
    -vf format=gray,format=yuv422p</p>

<p>but both just result in a black output video.</p>

<p>After much trawling of google, it looks like the FFMPEG exe I downloaded (for windows) wasn't compiled with the ""-enable-gray"" option, but I don't know if this makes any difference and don't know how to obtain a windows FFMPEG with this option enabled (building FFMPEG myself looks like a lot of hard work).
Any advice welcome!  Thanks.</p>

<p>PS I should add that the conversion I'm currently doing and any grayscale conversion could be separate steps if required.</p>
","<p>Yes, you do need a ffmpeg build with gray pixel format enabled. You should get your binary from the Windows link at <a href=""https://ffmpeg.org/download.html"" rel=""nofollow"">ffmpeg</a>. Download the latest 32-bit static build.</p>

<p>You can check if your build has gray enabled by running </p>

<pre><code>ffmpeg -pix_fmts | findstr /r gray
</code></pre>

<p>It should list an entry for gray. </p>

<p>And use <code>format=gray</code> in your <code>vf</code> chain and <code>-pix_fmt yuv420p</code> together for broad compatibility.</p>
","18053"
"how to blur a short scene in a video","5425","","<p>I need a software which can blur a scene from a video file. I have thought about VirtualDub and Adobe Premiere but I am not sure if they provide the desired functionality.</p>

<p>My requirement is to blur a some-seconds scene for example blur the scene from 10:12 to 10:45 in a video file.</p>
","<p>I fulfilled my requirement in Adobe Premiere Pro by following the below procedure:</p>

<ol>
<li><p>Load the complete video in Timeline panel.</p></li>
<li><p>Using 'Razor' tool cut the section which I need to blur. When we cut using 'Razor' tool, Adobe Premiere Pro marks it as a logical separate clip.</p></li>
<li><p>Drag the <em>Video  Blur</em> effect on the desired clip and set the Gaussian blur settings.</p></li>
<li><p>Repeat the above steps for all other sections of the original video which I need to blur.</p></li>
<li><p>In the end, Export the complete sequence.</p></li>
</ol>
","8309"
"How can I change the aspect ratio of the sequence preview pane?","5411","","<p>I'm wondering how I can change the aspect ratio and size of the preview pane on the right in the following image:</p>

<p><a href=""https://i.stack.imgur.com/jvDnj.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jvDnj.jpg"" alt=""enter image description here""></a></p>

<p>The source video (seen on the left) is 1280x720 but the preview pane seems to be set to 1:1. I've tried playing with the options in the right pane (changing 50%, 1/2 etc) but nothing seems to change the aspect ratio and size correctly.</p>

<p>For what it's worth, I can create a new sequence and set it's aspect ratio, but I'm wondering how can I change the aspect ration on an existing sequence?</p>
","<p>The program panel's sequence preview is mainly determined by the sequence settings. You can change those via <strong>Sequence</strong>  <strong>Sequence Settings</strong>. However, some parameters of a sequence can not be altered after creation, including resolution and aspect ratio (an exception is the automated adjustment of the sequence settings that is offered to you when you drop a clip in an empty sequence that doesn't match the sequence's settings). </p>

<p>To fix your issue, create a new sequence with correct settings. Then go back to your old sequence, select everything (<kbd>CTRL</kbd> + <kbd>A</kbd>), copy it (<kbd>CTRL</kbd> + <kbd>C</kbd>) and paste it into the new sequence (<kbd>CTRL</kbd> + <kbd>V</kbd>). To create a sequence with correct settings automatically, <strong>right-click</strong> on of your source clips and select <strong>New Sequence From Clip</strong> (I recommend this if you don't know what some or most of the options in the sequence creation menu mean).</p>

<p>The size of the program monitor's preview can be adjusted with the <strong>Select Zoom Level</strong> dropdown button in the lower left of the panel. The other dropbown button that you mentioned (the one in the lower right) controls the <strong>Playback Resolution</strong>.</p>
","16652"
"Can I get Handbrake to apply additional filters such as brightness/contrast?","5335","","<p>I have some scripts which pass video into <strong>Handbrake</strong> to transcode it for the web.<br>
What's the easiest way to apply additional filters for things like brightness/contrast?  (ie. things not covered by <a href=""https://trac.handbrake.fr/wiki/PictureSettings"" rel=""nofollow"">the 5-6 built-in filters</a>.) I'd prefer to avoid creating an intermediate file if possible.</p>

<p>So... should I look at:</p>

<ul>
<li>Frameserving from VirtualDub or something?</li>
<li>AviSynth scripts, which apparently are a little tricky to get working with Handbrake</li>
<li>Replacing Handbrake with other tools - perhaps FFmpeg or AviDemux or something?</li>
</ul>
","<p>Like you suggested yourself you might want to use FFmpeg instead. It can utilize x264 as well which is the library that Handbrake is using for encoding. While x264 itself can do some very limited color correction via color space conversion (which can be used in the Handbrake CLI) I wouldn't recommend it if you want advanced manipulation of the video look.</p>

<p>FFmpeg isn't hard to use and not all too different from the Handbrake CLI.
The advantage you have with FFmpeg is a lot more formats you can output to and read from and you have the option to use many advanced filters on your video aswell as audio:<br>
<a href=""http://ffmpeg.org/ffmpeg-filters.html#Video-Filters"" rel=""nofollow"">http://ffmpeg.org/ffmpeg-filters.html#Video-Filters</a></p>

<p>It's a very powerful tool for automated video editing and conversion.
The downside to FFmpeg is that you don't have all that many excellent presets but you should be able to use the x264 presets from Handbrake aswell.
<a href=""https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping"" rel=""nofollow"">https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping</a></p>
","10848"
"Does raising the framerate while not changing the bitrate result in quality loss?","5304","","<p>AFAIK, the bitrate is the number of bits <em>per second</em>. So let's say I have two videos, both using the same codec, bitrate settings and resolution. If the first one has a framerate of 25fps, but the second one runs at 50fps, does that mean that the second one will have a drastically lower quality since the available bits for each second of the video have to be divided amongst twice as much frames?</p>

<p>Or is this effect diminished by motion compensation (e.g. by having more P-frames and less I-frames) or something like that?</p>
","<p>If the result of increasing the frame rate is that more pixels are displayed per second, then yes, keeping the bit rate the same will almost certainly mean a loss in overall quality. Not all such losses are objectionable or even necessarily noticeable. For example, if the bit rate is 30 Mb/s and you reduce it to 15 Mb/s, probably not many people would notice.</p>
","12460"
"How can I stitch together several m2ts video files into one large file?","5275","","<p>I have recorded several community theater productions with a Panasonic TM900 camera. The settings on the camera were to record at 1080p @ 60 FPS.  However, I noticed that the camera produces  several video files (.m2ts). </p>

<p>While the included Panasonic HD Editor software plays the videos seamlessly that is not the case with other video players such as Windows Media Player.</p>

<p>I want to be able stitch together two or more m2ts video files without losing the quality of the original footage. What software should I use to do this? I do not need fancy editing capabilities though I would like to cut some scenes from the video such as during the play's intermission.</p>

<p>Open source software or at least free software is preferred but commercial software is not out of the question if it works well and it is easy to use.</p>
","<p>I believe you can join MPEG ts (transport stream) files simply by joining them together.  In Linux:</p>

<pre><code>cat file1.m2ts file2.m2ts file3.m2ts &gt; joined_file.m2ts
</code></pre>

<p>In Windows/DOS:</p>

<pre><code>copy /b file1.m2ts + file2.m2ts + file3.m2ts joined_file.m2ts /b
</code></pre>

<p>As long as the input files are split properly, and each new file begins with a key frame (and I would expect your camera does this), this should work just fine. And even if your camera is not careful about splitting right before a keyframe (again, I'd be surprised if it doesn't), as long as you re-join in the exact same order, it ought to work.</p>
","2794"
"How can I make professional video using an iPad?","5219","","<p>I am willing to make an educational guidance video. And since I want to make it using an <a href=""http://en.wikipedia.org/wiki/IPad"" rel=""nofollow"">iPad</a>, what things should I keep in mind?</p>

<p>And which specific camera app will be better over the inbuilt camera app? With that how could I improve audio quality of my video?</p>
","<p>Trade in the iPad and buy a desktop or laptop that you can edit on.  You will not produce professional results entirely on an iPad.  I am not aware of any good video editing options for iPad, certainly none of the big names have a product available.  There simply isn't enough horsepower on a tablet to perform the hard, complex operations involved in video editing in a smooth and timely manner. </p>

<p><em>(Minor correction: There is a version of iMovie available and Pinnacle does have a video editor available as well.  These are both makers of some of the better consumer focused stuff and could be considered big names, particularly since Pinnacle was previously held by Avid for a while, but I would still consider these cumbersome and limiting.  It probably <strong>IS</strong> possible to produce a professional quality result, but you'd have to work really hard at it.)</em> </p>

<p>You could use the iPad for shooting the video and get decent results with proper usage, but you will still need a lot of other equipment to compensate for the iPad's camera short comings.  </p>

<p>Lighting will be super important.  You will need lots of light, placed well so that there is sufficient light for the sensor and no shadows that the camera's dynamic range can't deal with.  This is also more than just having lamps, but also the stands to position them where you need them and modifiers to adjust the way the light falls on the scene to make good, even lighting.</p>

<p>Sound will also be another issue.  You will want an external mic.  People <a href=""http://www.bhphotovideo.com/explora/audio/tips-solutions/using-usb-microphone-ipad"" rel=""nofollow"">have reported</a> being able to use USB mics with the iPad Camera Connection Kit, but I couldn't find anything verifying if it would work with professional audio interfaces or a microphone more designed for video situations.  Another option (which is what you will commonly see in more professional productions) is to use a separate audio recorder, such as a Zoom h4n to record audio and sync it up in post production.  Even if you really wanted to stick with your plan to use the iPad for the editing, you could transfer the files off the SD card using the previously mentioned camera connection kit.</p>

<p>You will also need a stand and tripod that can hold the iPad securely.  There are numerous options available and I don't have any particular experience with any particular model, but being able to shoot stable smooth video is an absolutely critical aspect of any high quality video shoot.  If you shot is purely static, you might be able to prop it up like Jason suggested, though it also then runs the risk of slipping or more easily getting jostled, and as he points out, you won't be able to do a shot with any movement, so I'd highly recommend a tripod.</p>

<p>If you can handle those three things, that should allow for decent quality footage to be shot on the iPad, but you are probably looking at between $450 ($200 dirt cheap lighting, $50 tripod adapter, $50 dirt cheap tripod, $100 - $150 relatively cheap usb consumer audio mic) and $1300+ ($500 ok lighting, $250 basic pro audio recorder, $150-$200 decent video mic, $50 tripod adapter, $250-$350 ok tripod) of gear to supplement it.</p>

<p>I want to emphasis that the $450 is about the cheapest you could possibly go while still calling it ""professional video"".  You can make something that looks ok for a consumer project for less, but really pulling off professional video is unfortunately not particularly cheap even if costs have dropped like a rock in the last 10 to 15 years.  (You can now do for $15,000 what used to cost $150,000+.)</p>

<p>Finally, practice and experience is key.  There is more to producing professional results than the right equipment, you also need to know how to use it properly.  There is far too much to doing this to summarize in a single QA post, but pre-planning is really key.  If you know your shots and plan the video out before you shoot, you can make sure you get all the shots you need and make sure that they flow well from one to the next.  You can plan the angles to shoot from and what you want the end product to look for.  Even with hundreds of thousands of dollars of high end professional gear, the result will look like someone's home movies without planning and organization to accomplish a consistent vision for the video.</p>
","12116"
"Video converted by FFMPEG has a different duration, why?","5105","","<p>I convert videos using FFMPEG. My goal is to convert them to the MP4 container format (<em>MPEG-4 Part 14</em>) with <em>AAC</em>-encoded audio stream and <em>MPEG-4 part 10</em>-encoded video stream.</p>

<p>I use the following line to convert the videos:</p>

<pre><code>ffmpeg -y -i ""{inputFile}"" ""{outputFile}""
</code></pre>

<p>The converted video looks fine, however the duration of streams in the converted file and the input file doesn't always match.</p>

<p>I've made some experiments and the difference in the duration is undoubtedly there, however, it is not that much - anyway I'm testing with small videos.
Here are my results:</p>

<pre><code>| InputFile  | InputAudio | InputVideo | OutputAudio | OutputVideo  |
|------------|------------|------------|-------------|--------------|
| h.avi      | 3s 631ms   | 3s 567ms   | 3s 668ms    | 3s 567ms     |
| h.flv      | 3s 631ms   | 3s 558ms   | 3s 668ms    | 3s 567ms     |
| h.mov      | 3s 532ms   | 3s 533ms   | 3s 682ms    | 3s 534ms     |
| h.mp4      | 3s 605ms   | 3s 534ms   | 3s 682ms    | 3s 567ms     |
| h.mpg      | 3s 605ms   | 3s 533ms   | 3s 563ms    | 3s 534ms     |
| h.wmv      | 3s 620ms   | 3s 633ms   | 3s 659ms    | 3s 567ms     |
</code></pre>

<p>Since I would build a software on the top of FFMPEG, I would be happier if I could at least understand the reason of this difference. Is it because of some unnecessary transcoding?</p>

<p>In this case, can I turn this transcoding off to prevent FFMPEG to resample my input video file?</p>

<p>If I cannot turn it off, how can I be sure (besides testing) that this difference is not proportional to the size of the video?</p>

<p>If I convert for example a 10-hour video, a difference of multiple seconds or even minutes is not suitable for me.</p>
","<p>Hope this explanation is what you're looking for:</p>

<ul>
<li><p>When you transcode to an encoding such as H.264 (MPEG-4 part 10) you necessarily also resample the video, that's part of H.264 compression technique. Nontheless, I doubt if this is the reason you experience a timing gap since the resampling doesn't necessarily influence the clock rate of the media. So, I wouldn't worry too much about resampling, it can cause some variance, but probably very marginal.</p></li>
<li><p>The container formats you listed are kinda irrelevant, because they define how the compressed stream is packaged, whereas the the source of the timing difference is the compression itself. The <code>.flv</code> file, for example, may contain a stream encoded by the legacy Flash Sorenson codec or the newer H.264. In the first case you would be transcoding the video stream, but in the latter it is possible the you're not - depending on the audio codec used. The <code>.avi</code> and <code>.wmv</code> containers are codec agnostic, so there's no way of even guessing their content's encoding.</p></li>
<li><p>You didn't mention how the duration was tested. Note that ffmpeg by default shows you the duration that appears in the file's <strong>metadata</strong> and not a computed value. If your list is based on the data that ffmpeg dumps as part of its splash notices then you should note that this is explicitly <strong>metadata</strong> and not an actual measured value.</p></li>
<li><p>The delta in the durations you presented are within the range of one or two frames in a 25 or 30 fps range. It is reasonable for codecs to pad-with or strip-from streams blank frames according to their algorithm (or developer's tidiness...). It shouldn't influence timestamping when you properly concatenate streams.</p></li>
<li><p>There are only two reasons I can think of that can substantially change your media's duration, neither applicable in your specific case:</p>

<ol>
<li><p>Re-encoding at a different target speed. Sometimes this happens unintentionally due to bad metadata in the input file. But not in your case, which, as noted above, correspond with a single frame dropped or gained.</p></li>
<li><p>When you apply a codec that recreates either stream. Examples include ad-stripping, silence detection, noise cleansing, etc.</p></li>
</ol></li>
</ul>

<p>Bottom line, if you are concerned what's gonna happen with a 10-hour video - just run an actual test. If you run into trouble and seek assistance then keep in mind to post the input file's codec details and the method that you measured the stream duration.</p>

<p>Hope this helps.</p>
","15591"
"Best Lossless Video Encoder For Editing","5027","","<p>I'm looking for a good video encoder to compress <strong>HD files</strong> for editing within <strong>Sony Vegas Pro and Adobe After Effects</strong>. </p>

<p>What video codec would provide the <strong>fastest possible encoding and decoding</strong>? Also, what are some recommended encoding settings?</p>

<p>The video files must be usable in Sony Vegas Pro and Adobe After Effects.</p>

<p><strong>EDIT:</strong> Source Clip Info:</p>

<pre><code>Format                           : AVC
Format/Info                      : Advanced Video Codec
Format profile                   : High@L5.1
Format settings, CABAC           : Yes
Format settings, ReFrames        : 8 frames
Bit rate                         : 4 334 Kbps
Width                            : 1 920 pixels
Height                           : 1 080 pixels
Display aspect ratio             : 16:9
Frame rate                       : 23.976 fps
Color space                      : YUV
Chroma subsampling               : 4:2:0
Bit depth                        : 8 bits
Scan type                        : Progressive
</code></pre>

<p>The container is Matroska.</p>
","<p>I'll start by pointing you to <a href=""https://video.stackexchange.com/questions/1718/the-prores-of-premiere-pro/1732#1732"">another answer</a> I wrote before, roughly covering the different kinds of codecs.  </p>

<p>(Nice edit on your question, by the way, removing the file size limitation helped figure out what you were after.)</p>

<p>My recommendation (again) will be to either use ProRes 422 or Cineform as your intermediate, probably via Magic Bullet Grinder or GoPro Cineform Studio respectively.</p>

<p>Either of these codecs will give you good decoding speed, and both will work with Vegas and After Effects.  Any workflow that involves transcoding and not capturing natively to your editing codec (such as with ProRes on an Arri Alexa or a Cinedeck), is going to sacrifice time for processing, but in your case, it seems like that's a necessity. </p>
","1790"
"Combine multiple videos as separate streams in one mkv file","5026","","<p>So I have multiple videos of the same scenery from different angles. These are stored as separate files. Can I combine these as multiple video streams within one file? </p>

<p>For example I know this is possible with MKV, I just don't know how to ""join"" them.</p>

<p>Is this possible using <em>ffmpeg</em>? If so, which command sequence do I use?
If not, what other application would be able to do this?</p>
","<p>Yes, this is possible using ffmpeg and the <code>mkv</code> container.</p>

<p>An example command sequence would be:</p>

<pre><code>ffmpeg -i input1 -i input2 -c copy -map 0 -map 1 output.mkv
</code></pre>

<p><sub><a href=""http://ffmpeg.gusari.org/viewtopic.php?f=12&amp;t=828"">source</a></sub></p>

<p>Breaking down the command:</p>

<ul>
<li><code>-i input1</code> This selects the first input file. Could be something like my <code>video_track1.mov</code>.</li>
<li><code>-i input2</code> Here you can specify the second input file. You can add another entry <code>-i input3</code> if you have more input videos. I don't know what the maximum amount of videos is you can place in one <code>mkv</code> container.</li>
<li><code>-c copy</code> This copies all the streams (audio and video) in their original encoding.</li>
<li><code>-map 0</code> This maps all the streams (audio and video) of the first input file to the first track of the output file.</li>
<li><code>-map 1</code> This maps all the streams (audio and video) of the second input file to the second track of the output file.</li>
</ul>

<p>More information on the <code>-map</code> option can be found <a href=""http://ffmpeg.org/ffmpeg.html#Advanced-options"">in the documentation of ffmpeg</a>.</p>
","12868"
"Big difference in the video file size after the conversion (.mov to .mp4)","5015","","<p>After converting from .mov into .mp4, a video file is greatly reduced in size (.mov - 75,2mb .mp4 - 11,9mb) - does this mean that the converted video degraded (loss of color information, or the information in the highlights/shadows, or something else) or such difference in file size can be lossless?</p>
","<p>Let me explain you from scratch.</p>

<p>MOV,MP4,AVI,MKV etc all these are just containers, It is just kind of wrapper which contains audio,video,text,metadata file together as one file. It does not play much role in size of file. </p>

<p>Size of the file heavily depends on audio and video codec and bit-rate of that audio video codecs.</p>

<p>So there is possibility for file size reduction during conversion.</p>

<p><strong>1.</strong>  Reduce in video codec bitrate. While converting video if default/changed bitrate of video codec is less than source file, than it will lead to reduction in file size as well as quality for sure.[ May not be perceivable by eye ] </p>

<p><strong>2.</strong> Change video codec. While converting video if change video codec to advance video codec of the stream, than it will reduce size but quality will not be degraded.</p>

<p>For example : 
Video codec of source file is MPEG-2 with bit-rate 20mbps and during conversation we convert to H264/AVC with bitrate 10mbps it will have same quality with half of the size. </p>

<p>So, Below are the popular video codecs generation and improvement.</p>

<p>MPEG-2 <strong>-></strong> H264 (40-50% bitrate reduction than MPEG-2) <strong>-></strong> H265/HEVC (40-50% bitrate reduction than H264) </p>

<p>Though the more advance codecs are computationally expensive during video playback. e.g H65 is ~300% more computationally expensive than H264.</p>

<p>During video conversation point 1. &amp; 2. both can be applied </p>

<p>You can view details of audio/video file can be see using free tool mediainfo <a href=""https://mediaarea.net/en/MediaInfo/Download"" rel=""nofollow"">https://mediaarea.net/en/MediaInfo/Download</a> 
It will show properties of stream like video codec, bitrate,fps etc.</p>
","18584"
"Audio randomly cuts out for remainder of clip - Adobe Premier Elements 11","4941","","<p>I have a Sony Handycam HDR-XR260, and haven't had any trouble before the last two weeks.</p>

<p>While editing, occasionally a clip's audio will suddenly disappear a few minutes in, and not return until the next clip. Could be a 10 minute clip, could be a 20 minute clip.</p>

<p>The microphone is built in, so it isn't a matter of disconnected mic equipment. I'm not doing anything different from one clip to another, during filming or during uploading.</p>

<p>Super frustrating - any thoughts on the cause?</p>

<p>EDIT</p>

<p>I checked the original files on my computer, and the audio is still there. So for whatever reason, the clip isn't importing into Premiere with the audio, even when I try again.</p>
","<p>Through more research, I learned that Adobe struggles with .m2ts files (proprietary Sony format) despite being a supported file extension.</p>

<p>I updated my codecs for this extension with k-lite, but it didn't solve the issue.</p>

<p>I ended up using some free conversion software to convert to .mp4, and was able to bring in the audio finally! I only did this extra step for clips that lost their audio, since the conversion added additional time.</p>
","9174"
"How to delete an edit in Final Cut Pro (un-blade)","4933","","<blockquote>
  <p>I want to be able to ""undo"" the blade tool, in effect merging clips
  together. How can I do this?</p>
</blockquote>

<p><em>(Answered <a href=""https://video.stackexchange.com/questions/3937/how-to-delete-an-edit-in-final-cut-pro-un-blade/3939#3939"">another</a> question without noticing it was for Final Cut X, not FCP 7, but here is the answer for anyone using Final Cut Pro 7)</em></p>
","<p>If you have made an edit and you want to remove it, you can <strong>click on the edit</strong> then <strong>press <kbd>delete</kbd></strong> and the edit will be removed. </p>

<p>fig1. selected edit:</p>

<p><img src=""https://i.stack.imgur.com/SLZ3j.png"" alt=""Selected edit in Final Cut 7""></p>

<p><em>Note that this only works if the time on either side of the edit hasnt changed, as indicated by the red triangles.</em> If you have cut out any video, you cant delete the edit:</p>

<p><img src=""https://i.stack.imgur.com/pL8N6.png"" alt=""Indicator""></p>
","3939"
"What title(s) should a one-person film crew receive in the credit role?","4870","","<p>I recently worked on an interview series that had the following contributors:</p>

<ul>
<li>Producer</li>
<li>Interviewer: Person who contacted and arranged the interviews</li>
<li>Me, the ""predator"" (one-person film crew) who set up mics, selected angles and composed shots (one per interview for 8 interviews) like a Cinematographer/DP, operated the camera during the interviews, filmed B-roll, and edited the footage.</li>
</ul>

<p>Now I'm filling in the credits and getting ready to suggest what to call me to the client. I was thinking of this (in which I am named as Cinematographer and Editor):</p>

<p><img src=""https://i.stack.imgur.com/mOH1z.jpg"" alt=""Here&#39;s what the credit""></p>

<pre><code>Interviews by [name]
Cinematographer [me]
Editor [me]
Music by [name]
Producer [name]
</code></pre>

<p>Does this seem appropriate? Should I limit myself to one line in the credit role? Is there only a Cinematographer when that's a separate person from the camera operator? <strong>In other words, what title(s) should I be credited with?</strong></p>

<p>My reasoning is this: I did the professional work of a DP/Cinematographer and also ran the camera. But I don't want to look back on this once I've been doing this longer and think, ""Wow, that was kind of pompous.""</p>
","<p>Sounds like you directed it. Would be simple to just put Director, I think one can assume you did everything then if there is no editor or DP credited.  </p>

<p>The director is responsible for the overall vision of the flow of the piece.  They oversee filming and work with a cinematographer, if applicable, to determine the right look for the film, work with actors to get the right performance and then work with the editor to make sure it flows.  They are basically the person chiefly responsible for adapting the screenplay or concept of the film in to a finished movie.</p>

<p>For Example:</p>

<p>If you create a documentary completely by yourself, lets say you spend 20 days in Antarctica, you film everything you narrate, when you get back you edit it, and then do sound and music.  At the end of the day you directed that documentary, which for all intensive purposes is the highest credit you could receive, if the film ends and says directed by x.  People watching are not going to stand up and go ""Wait who shot this film?"" It is implied that you did it since you directed it and nobody was credited under you.</p>

<p>Another way this is sometimes credited to avoid confusion is simply doing created by</p>
","12731"
"Why don't phones correct vertical filming?","4865","","<p>Many people get really mad at filmers suffering from the <a href=""http://knowyourmeme.com/memes/vertical-video-syndrome"" rel=""noreferrer"">Vertical Video Syndrome</a> and I find it hard to disagree. Phone-shot videos filmed vertically are really annoying to watch. </p>

<p>But there's one thing I really don't get; why doesn't the phone correct that? The lens itself is always perfectly round and obviously doesn't really have an up- or downside. As far as I can imagine, shooting every video as if the phone was held horizontally would just be a formality in the software.</p>

<p>Is there any reason why that's never implemented? Is there any use in vertical filming? </p>
","<p>There is indeed no technical limitation that would prevent us from preventing vertical videos in software. The issue is more of an UX/usability problem.
While the sensor would allow us to capture virtually any part of it (video has usually always a lower resolution than the sensor can provide, when making videos the phone just records a small portion of the sensor area) we kind of don't want an auto correcting orientation as its rather unintuitive. The emphasize is on kind of.</p>

<p>Your phone screen is 9:16 so in order to show 16:9 video on your phone screen you either need to turn your phone by 90 or show the video with giant black bars on the top and bottom, which would be very annoying when recording a video. It's quite unintuitive to record horizontally while in a vertical position, so from a UX design perspective you would say the user wants to shoot vertically when holding its phone vertically because thats what you would usually expect your phone to act like.
Hold it vertical you shoot vertical, hold it horizontal you shoot horizontal, a pretty intuitive concept.</p>

<p>Though its still no reason to not at least implement an option to auto correct this and my guess why phone manufacturers aren't doing this is probably largely due to increased engineering costs in the camera module (afaik it can't be done purely in software, you would probably need to alter the functionality of your camera module a bit).</p>

<p>Users who shoot vertically don't really care about the fact that they have a horrible aspect ratio as they mostly just watch their video on a phone anyway. Vine is a good example for that, it really promotes doing vertical video because the aspect ratio of the device that your video will be watched on most of the time has the same vertical aspect ratio.
So the demand isn't really that huge so far to warrant such a feature but I'm certain we will see this at some point.</p>
","12589"
"Software to capture high quality screen-casts","4864","","<p>I want to record video tutorials but I can't get the program to capture my screen with high quality, and not just that, when I try to edit it in After Effects scaling and positioning the fonts and window seem pixelized and doesn't look sharp. Thanks!</p>
","<p>There are a lot of screen recording software, for example:</p>

<ol>
<li><a href=""http://camstudio.org/"">CamStudio</a> (free)</li>
<li><a href=""http://vegasaur.com/Screenpresso"">Screenpresso</a> (free/paid versions)</li>
<li><a href=""http://screencapture.movavi.org/"">Movavi Screen Capture Studio</a> ($49.95)</li>
<li><a href=""http://www.techsmith.com/camtasia.html"">Camtasia Studio</a> ($299.00)</li>
</ol>
","5274"
"Reducing the size of content within a composition in After Effects","4767","","<p>I'm having playback issues with a video I've rendered in After Effects. I need to adjust the composition and all the content to a smaller size. However, when I adjust the comp size the footage stays the same. Is there a way to reduce the whole lot together?</p>

<p>Thanks</p>
","<p>You can scale down everything in your composition by pre-comping everything and then shrinking the size of that pre-comp. I'm not entirely sure what you mean by ""when I adjust the comp size the footage stays the same"". If you mean that when you change the composition size, the footage extends beyond the new border of the comp, then pre-comping and scaling down the pre-comp should fix this.</p>
","12373"
"Canon 600D/ T3i Video Output and Converting?","4709","","<p>I purchased Canon 600D Few days ago. I am very new to the kind of photography and Videography .</p>

<p>I generally use my camera for taking pictures in auto mode .But yesterday I was in the party I shoot some good video which is important to me . When I was previewing that video in my camera it was good but when I import that to my system . It was very choppy I played in almost every video player . (. mov) This file works great on my iPod Touch .I read somewhere I have to convert the file so I use some simple convertor in Ubuntu . But I get poor quality video in .avi format. In short</p>

<p><em>I just want to know 3 things here :</em></p>

<blockquote>
  <p><strong>1) Is it nessecary to Convert video which I shoot from my T3i .</strong></p>
  
  <p><strong>2) How do I convert video simply without losing its HD quality.(Which
       format is good for HD videos</strong>)</p>
  
  <p><strong>> 3) I am using Sandisk Extreme 4 GB card it is Getting full in 10 min
  in 1920*1080p 24fps.Why it is taking too much space</strong></p>
</blockquote>
","<p>The default video files made by most Canon DSLRs is an H264 video file in an MOV container.  Decoding high definition video at the data rates that most Canon cameras use is intensive for both memory, disk and CPU unless you have a dedicated decoder chip.  Your iPod Touch uses flash storage (which is fast) and has H264 decoding capability to make it run smoothly.</p>

<p>On the other hand, a lower end desktop in particular is going to have slow hard drives and may not have sufficient power to decode the file on the fly, thus resulting in lag during playback.  Based on the quality level you describe, data retrieval from disk is most likely the problem you are experiencing.  So now with some groundwork set, lets move on to your direct questions.</p>

<p>1) It isn't necessary to convert video you shoot from your T3i.  It's a standard format and will play or be editable on any system capable of playing it.  If you are having issues with the H264 playback lagging on a particular system, you can try converting it to a lower data rate or less CPU/Memory intensive compression. </p>

<p>Either of these options might make the issue worse if you go in the wrong direction (for example, most algorithms that are less CPU intense take more space and vice versa, so if you were CPU limited and you use a higher compression, it might be more jumpy).  You can also simply sacrifice quality to make it play faster and take less space, but this isn't really ideal.</p>

<p>2) Any good transcoder can do the job.  The files that come off the camera are generally pretty high quality and thus should transcode well.  Your choice of encoder depends a lot on the platform you are on.  FFMPEG(MPEG based formats, including H264) isn't that user friendly, but is freely available and works cross platform.  Other options include the free Windows Media Encoder(WMV) on Windows, Quicktime(several formats) on Mac or Windows, Adobe Media Encoder(numerous formats) on Mac or Windows or Microsoft Expression Encoder(WMV).  </p>

<p>The basic key for quality conversion is to keep the native resolution (number of pixels) in the video and use a high enough data rate to support the quality.  VBR 2 pass is better than VBR 1 pass which is generally better than Constant Bit Rate, but each takes more time than the latter to encode.</p>

<p>3) Video is BIG.  Really, REALLY, BIG.  Since the camera needs to record fast, fairly limited compression is used.  It sounds like you are probably using All I frame video which is fairly high data rate video.  Keep in mind that you are effectively storing 24 pictures every second, continuously.  If it was truly uncompressed, it would go even faster.  RAW, uncompressed HD video produces 261 megabytes of information PER SECOND! (14 bits per color, 3 colors, 1920 x 1080 pixels per picture, 24 pictures a second.)  Makes 10 minutes on a 4GB card sound a lot better doesn't it?  </p>

<p>If you don't need as high of a quality, you can use IBP frame video which will drastically reduce (about 1/5 to 1/6 the size) the amount of space required though it also drops the quality of the video.</p>
","8122"
"Display multiple clips in DaVinci Resolve 12","4695","","<p>I'm very new to DaVinci Resolve and I'm still trying to figure out what DaVinci Resolve is good at and for what effects I should use other software.</p>

<p>The result I want to achieve is to see multiple clips at the same time (in the render product, not only while editing). Each quarter of the screen should show another clip.</p>

<p>My approach is hardly satisfying:
To achieve that I tried to have a video in a video.
I resized the first clip and made it small. But that makes the rest of the screen black and I couldn't find out how make the black part transparent in order to show another resized clip.</p>

<p>There must be an easier way (though my approach should work, otherwise it wouldn't be possible to replace a green screen). Any suggestions?</p>

<p><em>BTW</em>: Where can I learn such basic things? There must be a place to go.</p>

<p><strong>Note:</strong> I'm aware of the <code>split screen</code> feature, but this is not what I was looking for (because it splits the screen only to let the user compare different color versions).</p>
","<p>You are on The right track! There is not a specific effect to accomplish what you want to achieve.  </p>

<p>The only way (unless there is a 3rd party plugin, I don't know about) is to stack the 4 clips on separate tracks in the edit tab. Select one of them open the inspector (panel on the top right of the edit page) scale down and adjust the placement.  The video clip on the video track below should become visible. </p>

<p>[Added by Michael Tiemann as a friendly amendment: In the panel where the scaling information is displayed, there are also parameters for the anchor point.  By default, images are scaled from the center, which means you have to add in X and Y offsets to get each clip going to each corner.  Instead, you can set the upper left anchor point to be the upper left corner of the video, the upper right to be the upper right, etc.  Then, when you scale, each video clip will retreat to its own corner.  If you want to animate this effect, you can have one clip expand from its corner and go fullscreen by scaling back to 100%, and you can return it to its corner by scaling down to 50%.]</p>

<p>Good luck.</p>
","17129"
"How do I decide if 14ms response time in IPS LCD monitor enough?","4679","","<p>I was considering buying an IPS LCD monitor (<a href=""http://www.newegg.com/Product/Product.aspx?Item=24-005-364"" rel=""nofollow"">this one</a>; though <strong>the question is generic</strong>).</p>

<p>It lists the response time of 14ms.</p>

<p><strong>How do I judge if that is enough?</strong></p>

<p>I am slightly concerned since some of NewEgg reviews for that monitor mention issues with playback of movies, but I'm not sure if their attributing the problems to the response time is the correct call. Researching online (TomsHardware forums, Yahoo answers, some other forums) keeps giving contradictory answers, some say don't go above 5ms, some say 14 is enough unless you do FPS sensitive high end games.</p>

<p>The intended usages of the monitor include light fare (productivity, web browsing, Youtube, some light gaming where FPS isn't an issue; and - what seems to be the most taxing to me LCD wise - watching movies encoded as DivX or MP4, with resolutions between 720p and 1080p).</p>

<ul>
<li><p>Is there danger with ghosting or any other image issues due to 14ms response time given such usage patterns?</p></li>
<li><p>What other things may need to be considered that would either mitigate or exacerbate the response time?</p></li>
</ul>

<p>A second sub-question is whether this response time will become a serious problem if the user of LCD becomes interested in some light video editing (it's a child, so nothing professional).</p>
","<p>At 14ms, you can still get an effective 71 frames per second.  It should be fine for working with video.  The frames might not show up exactly at the start of their time interval, but it still should be ok.  </p>

<p>Faster response is still better, but it isn't going to be critical for anything other than really high frame rate gaming.  5ms is very VERY fast for a high end screen.  Cheap screens go faster because they use simple techniques that are fast and cheap but produce (greatly) inferior image quality.  Better screens like IPS and PVA are by nature slower and the last time I was seriously in the market (which was a couple years ago), 6 to 8ms was considered fast for an IPS or PVA.  sub-5 is completely unnecessary.  14ms is probably on the slower end, but should still be fine.  Even 20ms would likely be ok, though it might start getting more noticeable.</p>

<p>As far as if ghosting would be an issue, at 71 refreshes per second, a frame won't be able to off by more than 1/3 or so of a frame.  That's 1/90th or so of a second, so would be pretty minor.  It might make for a slight stutter, but isn't that likely to be noticeable.  I think most of the arguments against it come from the faster is better mantra and the twitch reflex crowd.  You might or might not be able to notice the difference, but it shouldn't be a significant problem and I'd personally much rather have a good quality display with a slower refresh than a display I can only view from one exact angle and even then have color differences from the angle of one eye to the other, but have ""crisp"" frame rendering.</p>
","8139"
"Repeating Flashing Text in After Effects","4662","","<p>I'm using After Effects CS6, and I'm deliberately trying to recreate the look of a cheap camcorder. I want that stereotypical red ""REC"" text and big red dot in a corner of the screen, and I want it to blink off and on. I want the ""ons"" and ""offs"" spaced one second apart, like so:</p>

<p>ON ==[one second]=> OFF ==[one second]=> ON ==[one second]=> OFF</p>

<p>And so forth. However, animating this by hand is a huge pain. How can I automate this animation? Would an expression work, or is there a way to tell After Effects to loop certain keyframes?</p>

<p>Thank you.</p>
","<p>This can be done easily with a mixture of time remapping and an expresison. Simply precomp the text animated as once on, once off (keyed out manually), then drop it into your main comp and apply the following expression:</p>

<pre><code>loop_out(""cycle"",0)
</code></pre>

<p>Then, just extend the composition for as long as you would like it to loop.</p>
","4546"
"H264 framerate information?","4635","","<p>Does raw H264 data (<a href=""https://supportforums.cisco.com/blog/149421/general-replay-video-packet-capture"" rel=""nofollow"">stripped from an RTP stream</a>) contain information about the framerate?</p>

<p>Or is the framerate derived either from RTP timestamps for stream reproduction or written the container files (avi, mkv...) when playing a video from a file?</p>
","<p>RTP is just a protocol for data transfer, it doesn't contain any specific information about the internals of the data transmitted.</p>

<p>The information should be contained in the .h264 stream. The SPS (Sequence Parameter Set) is what you want to look at, it should contain all the meta information you need. Here is a very nice detailed look at how the SPS is set up: <a href=""http://www.cardinalpeak.com/blog/the-h-264-sequence-parameter-set/"" rel=""nofollow"">http://www.cardinalpeak.com/blog/the-h-264-sequence-parameter-set/</a></p>

<p>Though in theory you should be able to just mux this into a mp4 and it should play fine.</p>
","10883"
"How to resolve ffmpeg error cannot load default config file?","4600","","<p>I have got this error message when using ffmpeg</p>

<pre><code>[Parsed_drawtext_0 @ 03bdc300] Could not load font ""/Library/Fonts/DroidSansMono.ttf"": cannot open resource
Fontconfig error: Cannot load default config file
[Parsed_drawtext_0 @ 03bdc300] impossible to init fontconfig
[AVFilterGraph @ 03d9b840] Error initializing filter 'drawtext' with args 'fontsize=15:fontfile=/Lib
rary/Fonts/DroidSansMono.ttf:timecode=00\:00\:00\:00:rate=25:text=TCR\::fontsize=72:fontcolor=white:
boxcolor=0x000000AA:box=1:x=860-text_w/2:y=960'
Error opening filters!
</code></pre>

<p>Does anyone know how can I resolve this issue?</p>

<p>I'm using ffmpeg on Windows 7. I've downloaded ffmpeg from ffmpeg.zeranoe.com/builds</p>

<p>This is what I entered and what came up:</p>

<pre><code>c:\ffmpeg\bin&gt;ffmpeg -i in.mts -vf ""drawtext=fontfile=/usr/share/fonts/truetype/DroidSans.ttf: timec
ode='09\:57\:00\:00': r=25: \x=(w-tw)/2: y=h-(2*lh): fontcolor=white: box=1: boxcolor=0x00000000@1""
-an -y out.avi
ffmpeg version N-70634-g3bedc99 Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 4.9.2 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzli
b --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libblu
ray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-
libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrw
b --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr -
-enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --
enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enab
le-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-decklink --enab
le-zlib
  libavutil      54. 20.100 / 54. 20.100
  libavcodec     56. 26.100 / 56. 26.100
  libavformat    56. 25.101 / 56. 25.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 12.100 /  5. 12.100
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mpegts, from 'in.mts':
  Duration: 00:00:23.42, start: 1.040000, bitrate: 9937 kb/s
  Program 1
    Stream #0:0[0x1011]: Video: h264 (High) (HDMV / 0x564D4448), yuv420p, 1440x1080 [SAR 4:3 DAR 16:
9], 25 fps, 25 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x1100]: Audio: ac3 (AC-3 / 0x332D4341), 48000 Hz, stereo, fltp, 256 kb/s
    Stream #0:2[0x1200]: Subtitle: hdmv_pgs_subtitle ([144][0][0][0] / 0x0090), 1920x1080
[Parsed_drawtext_0 @ 02ac2f20] Could not load font ""/usr/share/fonts/truetype/DroidSans.ttf"": cannot
 open resource
Fontconfig error: Cannot load default config file
[Parsed_drawtext_0 @ 02ac2f20] impossible to init fontconfig
[AVFilterGraph @ 03cdb840] Error initializing filter 'drawtext' with args 'fontfile=/usr/share/fonts
/truetype/DroidSans.ttf: timecode=09\:57\:00\:00: r=25: x=(w-tw)/2: y=h-(2*lh): fontcolor=white: box
=1: boxcolor=0x00000000@1'
Error opening filters!

c:\ffmpeg\bin&gt;
</code></pre>
","<p>Got it. The error is cuased by the font location in the command is of a Linux computer. The font location need to be different for windows.</p>

<p>The error caused by this line in the command: <code>drawtext=fontfile=/usr/share/fonts/truetype/DroidSans.ttf</code></p>

<p>The above line does not work in Windows. </p>

<p>For windows change to: <code>drawtext=fontfile=/Windows/Fonts/arial.ttf</code></p>
","15369"
"How can I increase the frame size of the titles in Adobe Premiere?","4572","","<p>In Adobe Premiere Pro CC 2015, I did hours of work on a video that is 640x360 pixels.  I added a bunch of ""titles"" of that same resolution.</p>

<p>Then I later realized that I wanted to export my project at 1920x1080 even though the source video was 640x360.  (At least then my titles would be more legible on high resolution monitors.)</p>

<p>I easily changed the resolution of the video portions of my project, but <strong>how can I change the frame size of the titles?</strong>  </p>

<p>I don't want to just ""scale"" them up to 1920x1080; I want them to actually BE that size and to be clear.</p>
","<p>As far as I know, unfortunately you can't take an old title which was based on whatever frame size, and switch its size to something else. The only way I know for your situation with the small titles is:</p>

<ol>
<li>Double click on a title that you want to change (either on the timeline or in the project).</li>
<li>In the title editor window, click on the top left button of 'New title based on current title'.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/GFges.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GFges.jpg"" alt=""Premiere title editor screen capture""></a></p>

<ol start=""3"">
<li>A pop up shows - type in the size you want for the NEW title AND don't forget to check (or change) the name of that new title, just that later on you know which title is which. Click OK.</li>
<li>You have just created a new title, you can find it in your project. It has the old text with all its preferences, inside a new size frame; the visual result is the same as you would have got with scaling, but w/o the quality loss.</li>
<li>That should do the trick for you, but you'll still have to do it one by one on all the titles, and then to put each one in the appropriate place on the timeline. </li>
</ol>

<p>I hope that helps. Good luck!</p>
","17904"
"Is it possible to control GoPro recording without the official app?","4538","","<p>Is it possible to start recording with a GoPro without the official app or from an unsupported device?</p>
","<p>For the GoPros with wifi (Hero3, Hero3+ or HD Hero/HD Hero 2 with a wifi bacpac), yes it is possible. They essentially work as a web server, so you can connect to that, and control the camera. It seems this is not officially documented anywhere, but a number of people have figured out how it works.</p>

<p>So turn on the wifi on the camera, then you can connect to it from your computer.
You can view the webserver by going to <code>http://10.5.5.9:8080</code> in a browser. This will let you view photos or videos stored on the camera.</p>

<p>To control the camera, you can send HTTP requests. eg to turn the camera on: <code>http://10.5.5.9/bacpac/PW?t=password&amp;p=%01</code> (where password is the GoPro wifi password). And to trigger the shutter: <code>http://10.5.5.9/bacpac/SH?t=password&amp;p=%01</code></p>

<p>All of the commands are listed in this Github: <a href=""https://github.com/KonradIT/goprowifihack"" rel=""noreferrer"">KonradIT/goprowifihack</a></p>

<p>There are also several unofficial apps for connecting to and controlling the GoPro, some of these are open source. eg <a href=""https://github.com/konradit/heroproapp"" rel=""noreferrer"">KonradIT/HeroProApp</a> or <a href=""https://github.com/moohtwo/Android-GoPro-Streaming"" rel=""noreferrer"">moohtwo/Android-GoPro-Streaming</a>.</p>
","10796"
"Are there only a few standard video frame rates?","4536","","<p>In modern digital video, can a video file be marked with any arbitrary frame rate? Or are only a few specific frame rates widely supported? By ""modern"" I mean software players like Quicktime, VLC, Roku, game consoles, etc. I'm curious both what the video standards themselves say are allowed frame rates and then what actually works in practice.</p>

<p>I understand that 24 fps, 25 fps, 30 fps, 50 fps, and 60 fps are widely supported standards. HandBrake also offers 5, 10, and 15; are those standard options? Could I use any FPS number I want?  And what about the non-integer rates like 23.976 and 29.97; are they actually treated differently by software than 24 and 30? Also I see references to ""variable frame rate"" in H.264 streams; does that actually work and if so, what uses it?</p>

<p>My specific question is what the best way is to encode some 8mm film scans. The source is 16 fps, the 8mm film standard. Right now I'm doubling every other frame to bring it up to 24 fps which works OK, but I'm wondering why I can't just mark the video as 16 fps. FWIW I've produced H.264 mp4 files with Handbrake at 15 fps and found they only played back correctly in VLC. Mac Quicktime played them too fast, probably 24 fps.</p>
","<p>There are several ""standard"" frame rates, but there are getting to be so many that supporting arbitrary framerates is easier than specifically supporting many specific ones.  This is especially true for software players, like VLC.</p>

<p>More and more support exists for VARIABLE fps.  (VFR, variable frame rate).  This is where the interval between frames within the same video isn't a constant.  Many video container file formats (like Matroska (<code>.mkv</code>) or MPEG-4 (<code>.mp4</code>, closely related to Apple's <code>.mov</code>)) don't even store a FPS number, but rather a time base (e.g. 1/30th of a second), and then each frame has a timestamp as a multiple of that time base.  It just so happens that the interval between each frame is one or a small integer number of units of the time base in a CFR (constant frame rate) video.</p>

<p>Security-camera footage with near-duplicate frames dropped would be an obvious use-case for VFR.  Even moreso if it's being compressed with a simplistic video codec that doesn't take good advantage of temporal redundancy (with inter (p and b) frames).  (play around with <code>ffmpeg -vf mpdecimate</code> to drop near-dup frames.  Use <code>-vsync 2</code> if outputting to mp4, because for some reason it isn't the default for that muxer, but it is for mkv.)</p>

<p>Another case is modern smartphones.  For example, my brother's Moto G (2nd gen) records VFR videos.  It lowers the frame rate when the sensor needs more light.  Some of the output from running mediainfo on an mp4 created by the phone's software, recorded indoors:</p>

<pre><code>Bit rate                                 : 9 999 Kbps
Width                                    : 1 280 pixels
Height                                   : 720 pixels
Display aspect ratio                     : 16:9
Rotation                                 : 90
Frame rate mode                          : Variable
Frame rate                               : 16.587 fps
Minimum frame rate                       : 14.985 fps
Maximum frame rate                       : 30.030 fps
</code></pre>

<p>Playback of a single VFR video stream is not hard.  The software just gets the next frame ready to display, sleeps until it should be displayed, then wakes up and displays it.</p>

<p>Things get a bit more complicated when you take into account the fact that humans can only see video frames when a monitor displays them.  VFR monitors exist, but are still rare.  (google for g-sync freesync).</p>

<p>Changing the displayed image while it's being scanned out to the monitor results in ugly tearing of the video (commonly seen when playing a game with vsync off).  This limits a player to changing the displayed image at 50 or 60Hz.  (CRTs support arbitrary vrefresh rates, within a range, but it's complicated to cook up modes with all the timings correct, so most people just used a few fixed refresh rates.  And now people have LCDs which only support a fixed refresh rate anyway.  Until freesync monitors are more widespread anyway.  I'm really looking forward to that. :)</p>

<p>So with video frame rates that aren't a multiple or a factor of the monitor refresh rate, some frames will be displayed for 3 monitor refreshes, and some for 2, for example, even if the video is meant to be at a constant 25FPS (on a 60Hz monitor).</p>

<p>Things get more complicated when you want to work with multiple clips and fade between them, or picture-in-picture, or various other effects.  It's a LOT easier to write video editting software if you can assume that all the clips have a new frame at the same time.  (They force clip alignment to snap to whole frames).</p>

<p>This is why NLEs (like kdenlive or pitivi, to pick random Free software examples), are more likely to force you to a fixed FPS, and drop/duplicate frames from your clips to make them match that frame rate.  The CFR you choose can be arbitrary, but it typically has to be constant for the whole ""project"".</p>

<p>(Do any NLEs fully work with VFR clips, and produce VFR output in that case?)</p>

<p>So in summary, once we have variable-sync monitors and OSes, the only thing holding us back will be video editting, I guess.  And broadcasting, since apparently CFR is a big deal for that, too?</p>

<p>In case you're wondering, the 29.970 (actually 30000/1001) and 23.976 (actually 24000/1001, from telecining) annoying non-integer frame rates are the fault of color NTSC.  <a href=""https://en.wikipedia.org/wiki/NTSC"" rel=""nofollow noreferrer"">search for 1.001</a>.  If only they'd been willing to risk a few B&amp;W sets not being able to handle an extra 0.1% frequency for the audio subcarrier, the world would have been spared this nonsense.  (I think I saw another article somewhere that made it sound more like many sets would have been fine, but they weren't sure about perfect compat.  Wikipedia makes it sound like no sets would have handle a 0.1% higher audio subcarrier.  IDK the facts.)</p>

<p>Annoying frame rates are one of the lesser sins of broadcasting, though.  It's really interlacing that's been the bane of video quality on modern (all pixels lit at once) screens, and that wouldn't have changed.  I still don't get why interlacing was kept around for HDTV.  Why was 1080i60 was ever defined, instead of using 720p60 to get the same temporal resolution for sports and stuff?  It's similar to 1920x540p60, but with a stupid vertical offset between odd and even fields that requires a lot of computation on the receiving end to make it not look horrible.</p>

<h2>edit:</h2>

<p>For your use-case, I'd absolutely suggest archiving at the native FPS.  Don't throw away any information by dropping frames.  Don't dup frames and make your files bigger (or make your h.264 encoder spend more time noticing the duplicates and outputting a frame full of skip macroblocks that only takes 20 bytes for the whole frame).   </p>

<p>In the future when we hopefully all have freesync displays that can play any framerate, you'll want to undo your pullup to 24fps so your video plays more smoothly!  Or if freesync somehow doesn't catch on, or the display that comes after LCDs is CFR, then rate conversion is probably best done at playback time anyway.  It's not like 24fps even plays perfectly on a 60Hz monitor.  (I don't visually notice the fact that some frames are displayed for 3 * 1/60th while some are displayed for 2 * 1/60th, but it's true).</p>

<p>If you're having problems with Quicktime, then IDK.  Maybe make sure Handbrake is making files with the right framerate set in the h.264 bitstream, as well as the container.  (Yes, h.264 headers can apparently store a frame rate, separate from what the container says.  See the docs for <code>mkvmerge --fix-bitstream-timing-information</code>.  And try using it with <code>--default-duration 16fps</code> to make an mkv file.  Then mux that back to mp4 and see if that fixes quicktime?)  Or maybe there's a way to do it with mp4 tools in the first place.  See for example: <a href=""https://askubuntu.com/questions/370692/how-to-change-the-framerate-of-a-video-without-reencoding"">https://askubuntu.com/questions/370692/how-to-change-the-framerate-of-a-video-without-reencoding</a></p>

<p>I can guarantee that arbitrary frame rate mp4 is valid, and even variable framerate mp4 is valid.  If Quicktime plays it wrong, it could well be Quicktime's fault.  Or maybe Handbrake's fault for making the file wrong.  I usually just use ffmpeg directly, because I'm a command line ninja.</p>
","14742"
"How to move a layer only in one direction?","4536","","<p>Is it possible to just move a layer along one axis, eg <code>x</code>?</p>

<p>I need to move many layers individually in a 3D space, but only along one axis. It would be much quicker if I could lock all axis but one, and then move along that.</p>
","<p>It's easy to do with a quick expression:</p>

<p><kbd>Alt/Option</kbd>-click the stop watch for the position property. You'll see the default expression <strong><em>transform.position</em></strong> is automatically inserted. The position property is an array  it contains the X, Y and Z value components. We want to lock it so that only one of the values can change. So we have to pick the array apart into its components, and then create a new array using some constant values.</p>

<p>So say we want to lock the layer to movement only on the X axis. We'll use the X component of the <strong><em>transform.position</em></strong> array and then put in a constant value for the Y and Z components. Say the layer's Y position is 123 and its Z position is 345. We'd use this expression:</p>

<pre><code>[transform.position[0], 123, 345]
</code></pre>

<p>note that the enclosing square brackets are part of the expression - they turn the three values back into an array. The bit where it says <strong><em>transform.position[0]</em></strong> selects just the first component of the array (just to be annoying, most computer languages <a href=""https://stackoverflow.com/questions/7320686/why-does-the-indexing-start-with-zero-in-c"">count array components from 0</a>, so the first component is <strong>[0]</strong>, the second <strong>[1]</strong> and so on).</p>

<p>You could refine this to make the layer easier to edit:</p>

<pre><code>[transform.position[0], transform.position.valueAtTime(0)[1], transform.position.valueAtTime(0)[2]]
</code></pre>

<p>here <strong><em>transform.position.valueAtTime(n)</em></strong> is a function that returns the value of the position property at time <em>n</em>. The round brackets are the input to the function, in this case the start of the comp or 0. Then as we want just one part of the position, we then use the square brackets to stipulate which bit: <strong><em>[1]</em></strong> for Y and <strong><em>[2]</em></strong> for Z (remember <strong><em>[1]</em></strong> means the second value in the array and <strong><em>[2]</em></strong> means the third). So the layer will be locked to the Y and Z position it has at the start of the comp, and will be free to move on the X position.</p>

<p>To do the same with the Y position or the Z, or the scale, or rotation or whatevs, you just use the same form, but change the array index (the number in the square brackets). So to lock the movement to only the Y axis:</p>

<pre><code>[transform.position.valueAtTime(0)[0], transform.position[1], transform.position.valueAtTime(0)[2]]
</code></pre>

<p>or Z axis:</p>

<pre><code>[transform.position.valueAtTime(0)[0], transform.position.valueAtTime(0)[1], transform.position[2]]
</code></pre>

<p>By now you should see the pattern.</p>

<p>Note that for 2D layers, the array only has two components, so you'd leave out the third part altogether, so to lock a 2D layer to the X axis you'd use:</p>

<pre><code>[transform.position[0], transform.position.valueAtTime(0)[1]]
</code></pre>

<p>Expressions are your friend!</p>
","13408"
"After Effects renders with no audio","4505","","<p>I've been using after effects cs6 to make <a href=""https://www.youtube.com/watch?v=7-AAl2SrHNo"" rel=""nofollow"">intros</a> in my youtube template. They always render without sound. I've been exporting them as <code>avi</code> files. I'm currently making one in an mpeg4 format (assuming that is another name for mp4) since my other intros all have sound and they're mp4 files.</p>

<p>I clicked ""lossless"" in the render queue and I changed the file format to avi. I checked ""audio output"" and didn't touch the settings inside that submenu and then I clicked OK and render. after waiting a while, AE spit out an intro file thats around a gigabyte in size and has no sound. What is the problem?</p>
","<p>The output file doesn't have any sound because there <strong>is no audio</strong> in the timeline that you are rendering.</p>

<p>The output format (MP4 vs. AVI) doesn't have any effect; both formats can contain an audio track or not. Checking the box to output audio only has any effect if there is already an audio track in the timeline. If there's no audio track, the output will have audio but it will be silent (as you're experiencing).</p>

<p>The solution is to add some audio file(s) to the project and then put it into your timeline.</p>
","16201"
"ffmpeg: explicitly tag h.264 as bt.601, rather than leaving unspecified?","4497","","<p>I want to tag video as explicitly bt.601, rather than ""unspecified"".  I know players typically choose bt.601 as the default for videos less than 1280 pixels wide, but I want to make it explicit that bt.601 is the correct colour matrix for a video.</p>

<p>The sticking point is that there is no <code>bt601</code> option for ffmpeg or libx264, only bt709 and some others.</p>

<p>I have some videos that were downscaled from HD, and are still in bt.709, but aren't tagged properly.  So when I watch a video, I sometimes have to <a href=""https://github.com/mpv-player/mpv/issues/1804#issuecomment-92411790"" rel=""noreferrer"">manually toggle my player to bt.709</a>.  Tagging explicitly as bt.601 will communicate to future viewers (e.g. myself) that it's definitely bt.601, and not a mis-tagged bt.709 video, when I look at it with <code>mediainfo</code>.</p>

<p>Another use-case for this would be if you upscaled a bt.601 video to 1280 or higher without doing a colormatrix conversion (using <code>-vf colormatrix</code>), players would incorrectly assume bt.709 if you left the color information unspecified.</p>

<p>FFMpeg's <code>colormatrix</code> video filter does support <code>bt601</code> as an input or output color matrix, but doesn't also set tags.  (Since it doesn't have the side-effect of setting ffmpeg's color options, it refuses to even operate with src and dest the same, like <code>-vf colormatrix=bt601:bt601</code>.)</p>

<hr>

<pre><code>ffmpg in  -color_primaries bt709 -color_trc bt709 -colorspace bt709  out
</code></pre>

<p>will tag the output video as using bt.709 for yuv&lt;->rgb.  The tags end up inside the video bitstream itself, not just in the container (since this is seems to be a codec-specific thing, not a container thing, for mp4 and mkv containers at least).</p>

<p>e.g. <code>mediainfo</code> output:</p>

<pre><code>Complete name             : out.mkv
...
Writing application                      : Lavf57.14.100
Writing library                          : Lavf57.14.100


Video
ID                                       : 1
Format                                   : AVC
...
Writing library                          : x264 core 148 r2638+4 afcf21c
Encoding settings                        : cabac=1 / ref=8 / ...
Language                                 : English
Default                                  : Yes
Forced                                   : No
Color primaries                          : BT.709
Transfer characteristics                 : BT.709
Matrix coefficients                      : BT.709
</code></pre>

<hr>

<p>So bt.709 is easy to tag.  The problem is, I don't see bt.601 anywhere.  Is one of other names an alias for it, or is there really no way to specify it explicitly?</p>

<pre><code>x264 --fullhelp output:

  --colorprim &lt;string&gt;    Specify color primaries [""undef""]
                              - undef, bt709, bt470m, bt470bg, smpte170m,
                                smpte240m, film, bt2020
  --transfer &lt;string&gt;     Specify transfer characteristics [""undef""]
                              - undef, bt709, bt470m, bt470bg, smpte170m,
                                smpte240m, linear, log100, log316,
                                iec61966-2-4, bt1361e, iec61966-2-1,
                                bt2020-10, bt2020-12
  --colormatrix &lt;string&gt;  Specify color matrix setting [""???""]
                              - undef, bt709, fcc, bt470bg, smpte170m,
                                smpte240m, GBR, YCgCo, bt2020nc, bt2020c
</code></pre>

<p><code>ffmpeg -h full</code>  shows the same set of options for its color options, which ffmpeg's <code>-c:v libx264</code> video codec passes on to x264.  Is any of these an alias for bt.601, or have a numerically equivalent colour matrix?</p>
","<p>PAL and NTSC have different color primaries, so </p>

<p>NTSC = SMPTE 170M = BT 601 525</p>

<p>PAL = BT 470 BG = BT 601 625</p>

<p>See the rows for value 5 &amp; 6 on the table on page 387 of the active H.264 <a href=""http://www.itu.int/rec/T-REC-H.264-201402-I/en"" rel=""nofollow"">standard</a>.</p>

<hr>

<p>So the right args for ffmpeg are:</p>

<p>NTSC:</p>

<pre><code># NTSC
ffmpeg -i input  \
-color_primaries smpte170m -color_trc smpte170m -colorspace smpte170m

mediainfo:
Color primaries                          : BT.601 NTSC
Transfer characteristics                 : BT.601
Matrix coefficients                      : BT.601
</code></pre>

<p>PAL:</p>

<p><code>-color_trc</code> doesn't accept <code>bt470bg</code>, but <code>ffmpeg -h full</code> shows that <code>gamma28</code> means ""BT.470 BG"" for that option.</p>

<pre><code># PAL
ffmpeg -i input  \
-color_primaries bt470bg -color_trc gamma28 -colorspace bt470bg

mediainfo:
Color primaries                          : BT.601 PAL
Transfer characteristics                 : BT.470 System B, BT.470 System G
Matrix coefficients                      : BT.601
</code></pre>
","16842"
"Pinnacle Studio 17 vs Adobe Premiere Pro CC","4497","","<p>I have my own YouTube channel that I am currently developing graphics for. I use Adobe Photoshop CC to create the graphics and I have been using a trial of Premiere Pro for video editing and I love it.</p>

<p>However, I'm lucky enough to of got Photoshop on a year contract cheap from a special offer on Adobe but Premiere Pro will cost me twice as much each month for a year. I've seen Pinnacle Studio 17 Ultimate Movie Editor in a local store for 75 and that's a one-off cost.</p>

<p>So my question is: would I be able to use Pinnacle Studio as well as I used Premiere Pro?</p>

<p>Needs:</p>

<ul>
<li>Accepts video formats that Premiere Pro does including .MP4</li>
<li>Multiple video &amp; audio layers</li>
<li>Adjust position and size of separate layers</li>
<li>Chroma key</li>
<li>Accept .psd files (if possible)</li>
<li>Change audio layer volumes independently</li>
</ul>
","<p>Both Pinnacle Studio 17 and Premiere Pro CC will suit the needs you listed, but only Premiere Pro accepts .psd files, Pinnacle Studio does not. Furthermore, Premiere Pro has an option to directly export frames to Photoshop, edit them there, and import them back to Premiere Pro. That sounds like a feature you will find useful.</p>
","10593"
"Smoothcam filter vs Warp Stabilizer","4497","","<p>To smooth out shaky footage I want to know which of the following apps is the best: </p>

<ul>
<li>Final cut Pro X</li>
<li>Adobe Premiere Pro CS6</li>
<li>Adobe After Effects CS6</li>
</ul>
","<p>I found this video comparing the adobe warp and Final cut: <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/MW3qU6pXBtA?start=0""></iframe>
            </div></div></p>

<p>I think warp stabilizer looks way better, but it might also be relative to the type of footage and the specific settings applied.</p>
","12021"
"Freeze frames in DaVinci Resolve","4484","","<p>How can I take a single frame of footage and extend it over several seconds on the timeline?</p>
","<p>Right click on the clip (or open the Clip menu with the clip selected) and select ""Change Clip Speed"". A popup will open:</p>

<p><a href=""https://i.stack.imgur.com/tNKLW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tNKLW.png"" alt=""Change Clip Speed popup""></a></p>

<p>Check the ""Freeze frame"" check box and then hit ""Change"". The clip can now be resized and it will only show that frame for its duration.</p>
","20844"
"120fps 720p footage with real time playback on Ubuntu? + editing?","4452","","<p>I have a laptop with an i7 CPU, 1.73 GHz, 8GB RAM, Ubuntu 10.04 with an ""ATI Mobility Radeon HD 5800 Series"" graphics card. I have compiz enabled (which may be one of the culprits with the problem below).</p>

<p>I have recorded a few short videos in 720p resolution at 120fps (GoPro Hero 3), stored on my local HD. The mp4 videos range from 30MB to 500MB, so quite small.</p>

<p>I tried a variety of players, none of which seem to respond in real time  . In fact the response time is so bad that I get a few frames quite fast at the start but then only the odd frame every second to 5 seconds and the sound seems spliced! One of my 8 cores runs into the ground (100%), the others are essentially unused, with the video player process running at 98%. So obviously this is my limiting factor (can't multi-thread?).</p>

<p>(mplayer, xine both try to play in real time but drop many frames; vlc plays the sound, but shows impressionist rendering and then gets stuck on one frame quite early on; avidemux plays every frame but about 1/6th of real-time speed; cinelerra plays some of the frames only and quite slowly; kdenlive does not allow me to create a 120fps project; Kino converts and down-samples; Lives can't follow; OpenShot plays the sound Ok, but gets stuck on the first frame)</p>

<p>Playing standard 1080p 30fps footage uses about 65% of one core. I did expect that playing 120fps would drop frames, but not to the extent that I only see one every few seconds. In fact I thought that it would play about 1 in 4, which would be great for my purpose.</p>

<p>So I'd really appreciate if any of you had any hint as to how I should configure my video card, system or whatever to get better playback ability, i.e. real-time playback with my raw footage (without conversion or down-sampling prior to viewing).</p>

<p>This also affects my ability to edit the videos as I can't tell what I'm looking at in real time, which is very inconvenient when editing. I do not want to down-sample for the first stage of editing as I want to get a feel for the full video first and choose which sections to slow down for slow-motion and which sections to down-sample for normal real-time play.</p>
","<p>While it is true that 720p video only has about half the data of a 1080p feed, the other thing you have to realize is that when you push the system beyond it's limit, it may spend a lot of time trying to process frames that it doesn't finish in time.  Depending on how the player is configured, it may give up and try to catch up rather than finish rendering the frame and get further behind.</p>

<p>Because of this, as soon as you get behind, you may now end up finding it is much harder for it to catch up.  You also run in to problems with keeping the data flowing quickly enough since the level of compression sounds like it may be higher (harder to decode) and the CPU is so busy that it can't be working on moving data up to be ready to read.</p>

<p>You probably should do a transcode of the video to a lower framerate and work with the transcoded copy to make editing decisions and then swap in the high framerate video only at the end.  After all, the only difference 120fps video will make on final output is anywhere that you end up doing a slow motion effect since you aren't going to have any display (including your computer screen) that can actually load 120 frames per second in to the image buffer.  Even 240hz screens are not displaying that many frames from a source.</p>
","10326"
"Create a DIY Green Screen / Chroma Key","4349","","<p>What are some good resources for people attempting to setup their own green/blue screen, light, record and pull a quality key from it? Particularly on a low budget, or with materials that aren't specifically designed for that purpose. Such as,   </p>

<ul>
<li>What sorts of fabric/material give good results? Both general tips and specific examples/links are helpful.</li>
<li>Is it possible to light it adequately without a full lighting kit?</li>
<li>What sort of encoding is recommended when filming it? (i.e. is it possible to use a DSLR with h.264?)</li>
<li>What software would you recommend for post processing the key? Does some work better with homebrewed screens than others? </li>
<li>If you have one, post an example of the results you were able to achieve with your suggestions. </li>
</ul>
","<blockquote>
  <p>What sorts of fabric/material give good results? Both general tips and specific examples/links are helpful.</p>
</blockquote>

<p>Great Resources for tips on lighting and materials:
<a href=""http://provideocoalition.com/index.php/alindsay/story/greenscreen_primer_part_1/"">http://provideocoalition.com/index.php/alindsay/story/greenscreen_primer_part_1/</a>
<a href=""http://rebelsguide.com/forum/viewtopic.php?t=2432"">http://rebelsguide.com/forum/viewtopic.php?t=2432</a></p>

<p>Underexposure (not lighting enough) is your biggest enemy, followed closely by uneven falloff across the screen.  Always keep your talent/target a good distance from the screen.</p>

<blockquote>
  <p>Is it possible to light it adequately without a full lighting kit?</p>
</blockquote>

<p>Not easily, but don't dismay, it always takes a skilled hand to get greenscreen lit well, so in many cases, even <strong>with</strong> a full lighting kit, results will be sub-par.  It's a tough thing to get right, because lighting needs vary a lot depending on your shooting situation.  Your best bet for getting good, even light, is to have an on-set monitor setup (such as a computer with OnLocation), and test, test, test, before you start shooting.</p>

<blockquote>
  <p>What sort of encoding is recommended when filming it? (i.e. is it possible to use a DSLR with h.264?)</p>
</blockquote>

<p>Great question.  This is showing that you've done enough research already to be aware of the issues.  ""Possible"" isn't the word I'd use to describe your capture options, because it's more of a spectrum from good idea, to difficult to work with, almost entirely because of <a href=""http://en.wikipedia.org/wiki/Chroma_subsampling"">chroma subsampling</a></p>

<p>Difficult: DSLR on-camera recording, consumer capture codecs (AVCHD, HDV, all of which probably will be 4:2:0)</p>

<p>Better: Output to an HDMI recorder at 4:2:2</p>

<p>Good idea: Uncompressed S-Log 4:4:4 (from something like an F3)</p>

<blockquote>
  <p>What software would you recommend for post processing the key? Does some work better with homebrewed screens than others? </p>
</blockquote>

<p>Some ordinary options are KeyLight (in Premiere and After Effects) and Primatte (which is a real pleasure to use).  Keying tools won't care <strong>how</strong> you created the screen, just how well it was exposed, and how much color separation there is between subject and background.</p>

<p>Finally, a more-radical-than-it-should-be thing to consider is using a neutral gray background instead of green or blue, and relying on a combination of luma keying and roto.  One bad example I've always remembered was about the keying in Forrest Gump. Stu Maschwitz once explained the effects artists had to roto every single frame to take care of green spill and edges.  Their lives would've been easier if they'd just gone with a neutral gray.  </p>

<p>Green does not equal magic movie effects, it's just one of many options.</p>
","1799"
"What factors are important for DSLR lenses for shooting video?","4324","","<p>I'm about to upgrade my DSLR camera from a Canon XSi (450D) to either a Canon 7D or T3i (600D).  I'm largely interested in the video features of the camera. I have several mid/high-end lenses that I already use for still photography, but I don't have any ""general purpose"" zoom lenses (since I prefer to use a prime lens for most of my stills).</p>

<p>So, I'd like to get a general-purpose zoom lens (something that covers at least the 20-70mm range, or so) to use for shooting video.</p>

<p>My primary question is, how important is lens quality for videography? I know that ""quality"" can mean many different things, so let me break this question down a little bit more.</p>

<ol>
<li><p>How important is sharpness? 1080p video is barely 2 megapixels, so it would stand to reason that an ultra-sharp lens is not really necessary to get sharp-looking video. Is this an accurate assumption?</p>

<p>My approach for buying lenses for still photography has usually been to buy the best lens I can afford for the application I'm interested in. I'm wondering/hoping that it might be reasonable to buy a cheaper lens for video, and not feel like I'm sacrificing image quality, since the output resolution is so much lower than with stills. Is this just wishful thinking?</p></li>
<li><p>Obviously a large aperture is a bonus for video, since long exposures can't be used. But when shooting in low-light, again, it would stand to reason that I might be able to use a higher ISO to compensate for a slower lens, with less noticeable impact, since I'm recording at a much lower resolution than I would be for still photography. Is this also true? How fast of a lens do I need to be able to record in an average indoor lighting setting (no studio lighting), without a high ISO becoming noticeable?</p></li>
<li><p>Is image stabilization a pro or a con for hand-held video recording? I can imagine the sound of the motor might be a nuisance if picked up by the microphone. What impact does it have on the video itself?</p></li>
<li><p>I'm guessing a USM for auto focus probably won't matter a whole lot, since what I've read suggests that live auto-focus doesn't work very well in video mode. But even so, I suppose a USM AF would be a bonus, if/when it does work.</p></li>
<li><p>The ability for a lens to retain focus when zooming seems like it might be of particular importance when shooting video on a zoom lens.</p></li>
</ol>

<p>Are there other factors I ought to consider when selecting a lens <em>specifically for video</em>?</p>

<p>I'm not really interested in pointers on which focal lengths I ought to use for what type of shooting, or specific lens recommendations... I'm interested in more general pointers, that will apply to all video shooting.</p>
","<blockquote>
  <p>How important is sharpness? 1080p video is barely 2 megapixels, so it would stand to reason that an ultra-sharp lens is not really necessary to get sharp-looking video. Is this an accurate assumption?</p>
</blockquote>

<p>It depends on the way the DSLR is capturing it's video from the sensor. The first method is the most obvious one, take the image and scale it, but there's also another; use only the pixels across the sensor's area that approximately ""line up"" with the pixels in the final resolution. I.e. in a 1080p image only every third or fourth scan line would be used resulting in one sensor pixel being used to produce one pixel in the video image.</p>

<p>I would hazard a guess that an ""acceptably sharp"" lens would be okay for video, though not ideal and probably for more reasons than image sharpness.</p>

<blockquote>
  <p>My approach for buying lenses for still photography has usually been to buy the best lens I can afford for the application I'm interested in. I'm wondering/hoping that it might be reasonable to buy a cheaper lens for video, and not feel like I'm sacrificing image quality, since the output resolution is so much lower than with stills. Is this just wishful thinking?</p>
</blockquote>

<p>As my above answer suggested, there are other aspects of the lens you need to keep in mind when choosing a video lens and these may be:</p>

<ul>
<li>Maximum aperture for low-light.</li>
<li>Fixed aperture for consistent exposure when zooming.</li>
<li>Image stabilisation is especially useful when not using a steadicam or other stabilising rig or device.</li>
<li>Ease of use, especially when it comes to the focus ring. Canon USM lenses in particular have a very smooth, usually well placed and grippy focus ring. Kit lenses aren't so good in comparison.</li>
</ul>

<blockquote>
  <p>Obviously a large aperture is a bonus for video, since long exposures can't be used. But when shooting in low-light, again, it would stand to reason that I might be able to use a higher ISO to compensate for a slower lens, with less noticeable impact, since I'm recording at a much lower resolution than I would be for still photography. Is this also true? How fast of a lens do I need to be able to record in an average indoor lighting setting (no studio lighting), without a high ISO becoming noticeable?</p>
</blockquote>

<p>I think you'll find with most typical video DSLR's is that the ISO performance for video isn't quite as great as you might assume, but it in no way bad. I'd say ISO 1600-3200 lives up to most expectations on the 18mp APS-C sensors, but beyond that it's quite noticeable.</p>

<blockquote>
  <p>Is image stabilization a pro or a con for hand-held video recording? I can imagine the sound of the motor might be a nuisance if picked up by the microphone. What impact does it have on the video itself?</p>
</blockquote>

<p>For music videos and other videos where sound is not used, IS is fantastic and goes a long way to ensuring the video is jitter free, but if you need the sound and it's potentially a quiet environment, it might be prudent to use an external, higher quality mic attached to say, the hot shoe via a mount.</p>

<blockquote>
  <p>I'm guessing a USM for auto focus probably won't matter a whole lot, since what I've read suggests that live auto-focus doesn't work very well in video mode. But even so, I suppose a USM AF would be a bonus, if/when it does work.</p>
  
  <p>The ability for a lens to retain focus when zooming seems like it might be of particular importance when shooting video on a zoom lens.</p>
</blockquote>

<p>As above, Canon USM lenses (whether L or standard) have very usable focus rings. Provide good grip and are very smooth to focus with.</p>

<hr>

<p>I personally use a Canon EOS 60D with an EF-S 17-55mm F/2.8 IS USM lens and find it a great combination at those focal lengths. The IS in that lens is quite remarkable and the USM focus ring makes it very smooth and accurate to focus. When I turn the IS off, I can instantly see the jittery movement from my hands trying to hold a 2kg camera setup.</p>
","1711"
"Can't browse Photos library in Final Cut Pro X","4311","","<p>I can't browse the Photos library from within FCPX. It just says ""Open Photos to see your photos in this list"". Yes, I've closed and reopened both Photos and FCPX.</p>

<p>I had everything working, but now I'm using a different Photos library on a different harddrive, and it has stopped working as before.</p>

<p>Any ideas?</p>
","<p>In the Photos app (not FCPX), go to Preferences, then click ""Use as System Photo Library"". Then FCPX will be able to find the library file.</p>
","16851"
"convert horizontal side by side to vertically stacked 3D video","4294","","<p>I have seen <a href=""https://stackoverflow.com/a/33764934"">this</a> and <a href=""https://video.stackexchange.com/questions/4563/how-can-i-crop-a-video-with-ffmpeg?newreg=08cf71ca624344a09f50ef640118e6cd"">this</a> post explaining how to crop and stack videos using <code>ffmpeg</code>.</p>

<p>My question is - is there an easy (preferably one step) way to turn a horizontally stacked video into a vertically stacked one?</p>

<p>And is there a Linux-way to re-encode given 3D video formats into others?</p>

<p><strong>Edit:</strong></p>

<p>with Mulvyas help I wrote a <a href=""https://github.com/frans-fuerst/magic/raw/master/LR2TB.py"" rel=""nofollow noreferrer"">script</a> which does the converting stuff and creates a text file with meta information needed by the Gear VR video player.</p>

<p>Note: you need a recent version of <code>ffmpeg</code> for the <code>vstack</code> filter - version 2.6 which is installed with Fedora 22 is too old. Version 2.8.3 has <code>vstack</code> available.</p>
","<p>Just integrate all filtering into one command:</p>

<pre><code>ffmpeg -i input.mp4 \
-filter_complex ""[0:v]crop=in_w/2:in_h:0:0 [top]; \ 
[0:v]crop=in_w/2:in_h:in_w/2:0[bottom]; \
[top][bottom]vstack[outv]"" \
-map ""[outv]"" -map 0:a -c:a copy output_3dv.mp4
</code></pre>

<p>Edit: this command below scales and pads the output to 2000x2000</p>

<pre><code>ffmpeg -i input.mp4 \
-filter_complex ""[0:v]crop=in_w/2:in_h:0:0 [top]; \
[0:v]crop=in_w/2:in_h:in_w/2:0[bottom]; \
[top][bottom]vstack,\
scale=iw*min(2000/iw\,2000/ih):ih*min(2000/iw\,2000/ih), \
pad=2000:2000:(ow-iw)/2:(oh-ih)/2[outv]"" \
-map ""[outv]"" -map 0:a -c:a copy output_3dv.mp4
</code></pre>
","17132"
"How to make the rest of the mask transparent in Premier Pro CS6?","4293","","<p>I'm trying to highlight a part of my video. Since I'm totally new to video editing on Premier Pro - I'm not sure if this is the best way of doing that, but I'm using masks.</p>

<p>I have a title sitting in <code>video 2</code> row, in it a have a rectangular mask (100% opacity)</p>

<p><img src=""https://i.stack.imgur.com/Y1YBT.png"" alt=""title""></p>

<p>I then apply a <code>Track Matte Key</code> effect on the video I'm trying to highlight, and get the following result:</p>

<p><img src=""https://i.stack.imgur.com/zf0JQ.png"" alt=""enter image description here""></p>

<p>This seems like a step in the right direction, but I can't find the place to change the settings of the mask to enable transparency. The intended result is to have dimmed out frame, with only the masked area fully shown.</p>

<p>This is the <code>Effect Controls</code> screen for my video:</p>

<p><img src=""https://i.stack.imgur.com/9XndY.png"" alt=""enter image description here""></p>
","<p>Ok, now that I was able to reproduce your technique thus far, I can fill in the missing pieces.  Your use of the Track Matte Key produces a layer that is transparent everywhere other than where the mask is present.  You need to layer it overtop of the video that you want behind it and you need to apply the dimming effect to the background layer.</p>

<p>In your example, Move video 2 to Video 3 and copy Video 1 to Video 2.  Leave the track matte key on Video 2 and remove it from Video 1.  This will layer the highlighted area (Video 2) over the full video (Video 1).  You can then either reduce the opacity or apply a brightness reduction to Video 1 to make it appear more subdued and get the look you are going for.  </p>
","8015"
"Awful Audio in Video post-render (Premiere Pro CC)","4276","","<p>I'm trying out Adobe Premiere Pro CC 2014 and I have a video with three audio tracks. The quality of each track is as follows (listed as the ""Source Audio Format"" in Premiere): </p>

<ol>
<li>48,000 Hz 16-bit Mono </li>
<li>16,000 Hz 16-bit Mono</li>
<li>44100 Hz - compressed - Stereo</li>
</ol>

<p>The sequence settings for the audio is ""48,000 Hz, Stereo, and Audio Samples"". When I render what I'm trying to use is h.264 and for audio my settings are ""AAC, 48,000 Hz, Stereo, High, 320 Kbps, Bitrate""   </p>

<p>I've tried numerous other settings as well, including different encodings, and whenever I render out the video the audio sounds thin and echoes a ton. It is really awful. The only way I'm able to get all three tracks to play together nicely, is if I render them as an uncompressed .wav file, but obviously I'll get no video then. Another strange thing is that all the other tracks (in camera audio, music track) sound completely fine, but the one that's really important, the narrative voice over sounds awful. </p>

<p>What am I doing wrong?! </p>

<p>EDIT: Here are some pictures of the settings</p>

<p><img src=""https://i.stack.imgur.com/Fa02c.png"" alt=""Export Settings"">
<img src=""https://i.stack.imgur.com/ULtJe.png"" alt=""Sequence settings"">
<img src=""https://i.stack.imgur.com/4tS1R.png"" alt=""Track Settings""></p>
","<p>OK, after using different encodings, recording in different modes, trying different field recorders, trying different NLEs, I refreshed my PC (Windows 8 Refresh), reinstalled my drivers, updated, reinstalled AE and PP and it works. So it was a confliction with some sort of 3rd party program, driver, or errant system file, etc. </p>
","12300"
"Weird zooming in Premiere Pro preview-window","4272","","<p>After a hard time getting my Premiere Pro CC to work correctly,<br>
I stumbled upon a new problem.</p>

<p>In the image below, you can see how the frame is about 200% zoomed in the preview window, and normal in the ""source""-window. I've checked the magnification level, but it's ok. Resolution is set to ""Full"".</p>

<p>The preview was a square and zoomed at first, so I created a new sequence, but now it's widescreen and zoomed.</p>

<p>How do I make the preview normal again? </p>

<p><img src=""https://i.stack.imgur.com/r4Zk8.png"" alt=""Weird zooming""></p>
","<p>Make sure the resolution in the sequence is the same as the resolution of your clip. Otherwise you'll need to scale the video to fit correctly.</p>
","15441"
"Copying h.264 video from TS into MP4 changes frame rate and time","4239","","<p>I have large MPEG-TS file that I am converting to MP4 using the below command.  If I seek to the same time in the video in both the MP4 and the TS file,  the MP4 will be a few frames behind the TS file.  This get progressively worse the further into the video I seek. </p>

<pre><code>c:\&gt;ffmpeg -y -r 30 -i full-ts.ts -c:v copy -r 30 -an full.mp4
ffmpeg version N-69040-gb23a866 Copyright (c) 2000-2015 the FFmpeg developers
  built on Jan 12 2015 22:02:37 with gcc 4.9.2 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --
enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinge
r --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-
libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-decklink --enable-zlib
  libavutil      54. 16.100 / 54. 16.100
  libavcodec     56. 20.100 / 56. 20.100
  libavformat    56. 18.101 / 56. 18.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5.  7.100 /  5.  7.100
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mpegts, from 'full-ts.ts':
  Duration: 00:11:25.75, start: 1.424000, bitrate: 3407 kb/s
  Program 1
    Metadata:
      service_name    : Service01
      service_provider: FFmpeg
    Stream #0:0[0x100]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 30 fps, 30 tbr, 90k tbn, 60 tbc
    Stream #0:1[0x101]: Audio: aac (LC) ([15][0][0][0] / 0x000F), 48000 Hz, stereo, fltp, 125 kb/s
Output #0, mp4, to 'full.mp4':
  Metadata:
    encoder         : Lavf56.18.101
    Stream #0:0: Video: h264 ([33][0][0][0] / 0x0021), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 30 fps, 30 tbr, 15360 tbn, 30 tbc
Stream mapping:
  Stream #0:0 -&gt; #0:0 (copy)
Press [q] to stop, [?] for help
frame=20419 fps=0.0 q=-1.0 Lsize=  251660kB time=00:11:25.59 bitrate=3007.0kbits/s
video:251446kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.084929%
</code></pre>

<p>I'm confused why the TBN is so large in the output stream.  I've tried -copyts,  -copytb, all the -vsynch options,  but I cannot get seek times to match up on the MP4.   Also,  the MP4 shows a frame rate of 29.78,  whereas the TS file is solid 30/1 using ffprobe.  </p>

<pre><code>ffprobe -v error -of flat=s=_ -select_streams v:0 -show_entries stream=avg_frame_rate full.mp4
streams_stream_0_avg_frame_rate=""5227264/175529""

ffprobe -v error -of flat=s=_ -select_streams v:0 -show_entries stream=avg_frame_rate full-ts.ts
programs_program_0_streams_stream_0_avg_frame_rate=""30/1""
streams_stream_0_avg_frame_rate=""30/1"" 
</code></pre>

<p>Any idea how I can get the seeking on the both containers to show the same frame?  Please help!</p>
","<p>As per a comment here, removing the -r flag ought to do this properly. You have another -r flag for the input, also. Remove both of them, and it should properly copy the stream as it is into a new container; you may need to remove that -an (disable audio) switch, too. I'm not sure on that:</p>

<pre><code>ffmpeg -y -i full-ts.ts -c:v copy full.mp4
</code></pre>

<p>If that fails, also per a comment here, you could try a different output container format.</p>
","18782"
"I changed the .mp4 extension of my video to .mpg and it still opens and plays fine (in VLC and in Windows Media Player). Why is this so?","4236","","<p>I'm uploading a file to an external system that accepts only mpg or mpeg extension files. I had a mp4 file so I changed the mp4 extension to mpg and I was able to upload the file into the system. Would other using the system who have access to this uploaded mpg file be able to view my video correctly? </p>

<p>Why is it that I can change the extension from .mp4 to .mpg and the video still plays fine? Should it not give me an error because the extension is not correct?</p>
","<p>The <code>.mpg</code> and <code>.mpeg</code> extensions are typically associated with MPEG-1 and MPEG-2 files. The structure of these files is different than the .mp4 format used for H.264 video, part of the MPEG-4 family of formats.</p>

<p>I suspect VLC and WMP can play the file because they must not be using the file extension to determine file type, they probably parse the file with all the supported parsers until one works. To prove this point, I took a .wmv movie and renamed it to .mp4. When I played it in my Windows 7 PC with WMP I got a warning window saying that the extension did not match the format, but I told it to proceed anyway and the file played just fine.</p>

<p>If this online service requires MPEG-1 or 2 then you should transcode your video to one of those formats before uploading. Unless the online service transcodes all uploaded videos to some standard format. In that case it does not really matter what the format of the uploaded video is, as long as the service can read it and transcode to its own favorite format.</p>

<p>Good luck.</p>
","4706"
"Error FFMPEG when convert video to MP4 with baseline presset","4069","","<p>I have this command in ffmpeg:</p>

<pre><code>ffmpeg -i '/var/www/sites/default/files/private/videos/original/reel para web en loop_2_2.mp4' -vf 'scale=640:480' -strict experimental -vco
dec 'h264' -vpre 'libx264-baseline' -acodec 'aac' -vprofile 'baseline' -ac '2'
 -y /tmp/1379483589-52393fc5e233e.mp4
</code></pre>

<p>And I have the following pressets:</p>

<pre><code>libvpx-1080p50_60.ffpreset  libvpx-720p50_60.ffpreset  libx264-ipod320.ffpreset
libvpx-1080p.ffpreset       libvpx-720p.ffpreset       libx264-ipod640.ffpreset
libvpx-360p.ffpreset        libx264-baseline.ffpreset
</code></pre>

<p>And this is the result:</p>

<pre><code>ffmpeg version N-56279-g5295407 Copyright (c) 2000-2013 the FFmpeg developers
  built on Sep 15 2013 03:43:12 with gcc 4.7 (Ubuntu/Linaro 4.7.3-1ubuntu1)
  configuration: --prefix=/usr --enable-gpl --enable-libass --enable-libmp3lame
--enable-libopus --enable-libtheora --enable-libvorbis --enable-libvpx --enable-
libx264 --enable-nonfree --enable-version3
  libavutil      52. 43.100 / 52. 43.100
  libavcodec     55. 31.101 / 55. 31.101
  libavformat    55. 16.102 / 55. 16.102
  libavdevice    55.  3.100 / 55.  3.100
  libavfilter     3. 84.100 /  3. 84.100
  libswscale      2.  5.100 /  2.  5.100
  libswresample   0. 17.103 /  0. 17.103
  libpostproc    52.  3.100 / 52.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/var/www/sites/default/files/private/vi
deos/original/reel para web en loop_2_2.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: mp42mp41
    creation_time   : 2013-09-12 17:30:22
  Duration: 00:00:39.32, start: 0.000000, bitrate: 3190 kb/s
    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv), 720x4
80 [SAR 40:33 DAR 20:11], 2991 kb/s, 29.97 fps, 29.97 tbr, 29970 tbn, 59.94 tbc
(default)
    Metadata:
      creation_time   : 2013-09-12 17:30:22
      handler_name    : Mainconcept MP4 Video Media Handler
    Stream #0:1(eng): Audio: aac (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 18
9 kb/s (default)
    Metadata:
      creation_time   : 2013-09-12 17:30:22
      handler_name    : Mainconcept MP4 Sound Media Handler
[NULL @ 0xab9d980] [Eval @ 0xbfd87b9c] Undefined constant or missing '(' in 'wpr
ed'
[NULL @ 0xab9d980] Unable to parse option value ""wpred-dct8x8""
[NULL @ 0xab9d980] Error setting option flags2 to value -wpred-dct8x8.
Output #0, mp4, to '/tmp/1379483589-52393fc5e233e.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: mp42mp41
    Stream #0:0(eng): Video: h264, yuv420p, 640x480 [SAR 15:11 DAR 20:11], q=-1-
-1, 90k tbn, 29.97 tbc (default)
    Metadata:
      creation_time   : 2013-09-12 17:30:22
      handler_name    : Mainconcept MP4 Video Media Handler
    Stream #0:1(eng): Audio: aac, 48000 Hz, stereo, fltp, 128 kb/s (default)
    Metadata:
      creation_time   : 2013-09-12 17:30:22
      handler_name    : Mainconcept MP4 Sound Media Handler
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 -&gt; libx264)
  Stream #0:1 -&gt; #0:1 (aac -&gt; aac)
Error while opening encoder for output stream #0:0 - maybe incorrect parameters
such as bit_rate, rate, width or height
</code></pre>

<p>What is the problem in my command? I have baseline profile in ffmpeg. Or I need do one more step?
I need that this file playing in iPhone in MP4. Because I use JW Player.</p>
","<p>This looks to me like an actual issue with the scripts in the preset vs your build of FFMpeg.  It appears that the syntax of options being passed to the encoder are incorrect (particularly something related to ""wpred"" and a missing parens) and since they are not specified in your command line, I can only assume it is the preset that is the problem.  I would suggest trying either a different version, a different build or a different source for your copy of FFMPEG and see if you have better luck.</p>
","8994"
"Copying All Settings Of One Composition in AE","4039","","<p>I want to copy the whole settings of the composition to the other, all keyframes, transition points, 3d layer etc. In Adobe After Effects</p>
","<p>This is not possible to do in one step, it is only possible to <strong>duplicate compositions</strong>:</p>

<ul>
<li>Select the composition (<em>Import Footage Panel</em>)</li>
<li>Press <kbd>Ctrl</kbd>+<kbd>D</kbd> (or go to <em>Composition > Duplicate Composition</em>)</li>
<li>As (almost) everywhere in AE, hit <kbd>Enter</kbd> on the selected composition to <em>rename the duplicate</em></li>
</ul>

<hr>

<p>When you need to <strong>copy the properties of a layer</strong>:</p>

<ul>
<li>Select the <em>Layer</em></li>
<li>Click on the tiny Triangle <kbd>></kbd> left beside the layers name to open up the properties</li>
<li>Select <em>Transform, Effects or Mask</em></li>
<li>Press <kbd>Ctrl</kbd>+<kbd>C</kbd></li>
<li>Switch to the <em>Second Composition</em></li>
<li>Select the <em>Layer</em></li>
<li>Set the <em>playhead</em> to the animation <em>start frame</em> (first keyframe)   </li>
<li>Press <kbd>Ctrl</kbd>+<kbd>V</kbd> to <em>overwrite all properties</em> and <em>insert all copied keyframes</em></li>
</ul>

<p>To check the result, press <kbd>U</kbd> on the <em>Layer</em>, this will open up all animated properties of the layer. </p>

<p>Note: Copying the <strong>same properties of different layers</strong> is also possible by <em>selecting multiple layers</em>, <em>open up the properties simultaneously</em>, and select them by <em>holding down</em> <kbd>Ctrl</kbd> or <kbd>Shift</kbd>. </p>

<hr>

<p>Related: <a href=""https://video.stackexchange.com/questions/17326/after-effects-shortcut-for-showing-all-transforming-properties-of-a-layer"">After Effects: Shortcut for showing all transforming properties of a layer</a></p>
","15787"
"How to handle Rode VideoMic Pro during transport?","3993","","<p>Unlike, for example, Zoom H5 which is shipped in a rock-solid plastic box, Rode VideoMic Pro is shipped in a cheap plastic piece which holds it in the box. The piece is much bigger than the microphone and is difficult to handle, since its only purpose is to be used during the original shipping.</p>

<p>Since I need to transport the microphone in my bag, one of the concerns is that I can't just put it directly in the bag together with lenses and camera, because it is too fragile for that.</p>

<p>What I have tried:</p>

<ul>
<li>I went to nearly every supermarket in the town, searching for the box of appropriate size with no success.</li>
<li>I searched on Rode website; they doesn't sell any boxes for their microphones.</li>
<li>B&amp;H accessories don't contain anything related to transport.</li>
<li>A search on Amazon was also unsuccessful.</li>
<li>Companies which make custom-sized boxes are mostly centered on large boxes for shipping or storage. There is <a href=""http://www.mycasebuilder.com/"" rel=""nofollow noreferrer"">MyCaseBuilder</a> which is great, but I live in Europe, so $50 shipment plus taxes is too expensive.</li>
</ul>

<p>I hardly doubt I'm the first one encountering this issue. How other people solved it?</p>

<p>The original packaging of Rode VideoMic Pro looks like this:</p>

<p><img src=""https://i.stack.imgur.com/DDvo5.jpg"" alt=""enter image description here""></p>

<p>By comparison, here's a box for Zoom H5:</p>

<p><img src=""https://i.stack.imgur.com/mbi58.jpg"" alt=""enter image description here""></p>
","<p>I am owner of Rode VideoMic Pro and have to say I just throw it in my bag together with dslr and lens. Wind cover gets a bit deformed sometimes, but it gets into original shape in five minutes after taking it out of the bag. I have one of the cheaper Lowepro bags and I can fit in my Nikon D5200 with one lens on, second lens and said microphone just fine. Mic fits vertically to the corner of the bag next to the second lens. I have been carrying it like this around for almost a year and I haven't encountered any issues.</p>

<p><em>I would therefore suggest to just throw it in the bag nicely, if it wont wiggle and bump around during carrying, it will be just fine without any dedicated case.</em></p>
","12696"
"Crop video in ffmpeg without re-encoding","3974","","<p>I have videos I want to split across multiple screens, so the solution I've found so far is to just crop the input video 4 times with ffmpeg (is this optimal? Is there some other way so I only need to use one command?)</p>

<p>Anyways, in ffmpeg crop is a video filter, and it seems that I can't specify the video codec as copy when using a video filter. Why is this the case? It seems like nothing should change if I just remove some of the video. I should be perfectly capable of just copying the same video over, with just part of it removed.</p>

<p>Is there any way for me to do a crop in ffmpeg without having to do a re-encode? The extra encoding time makes my processing times really long and the new bitrates caused by the re-encode makes the video parts out of sync and unable to be played properly.</p>

<p>Thanks</p>
","<p>A videofilter in ffmpeg always has to modify pixel information hence the reason why you need to re-encode.
It seems logical at first that you wouldn't have to do this when cropping but the way lossy video codecs work makes this pretty impossible without re-encoding everything.
They usually don't see an array of pixel information but a much more complex representation of pixel blobs that are described in a way that takes up less space than the original pixel blob, you can't really ""crop"" that encoding, to do a crop you would first need to decode the information to know what you are dealing with but then you need to re-encode to have it compressed again. Thats the main stepping stone.
Then they also use things like motion estimation that depends on information before and after certain frames, if that information changes all of the sudden the whole thing is screwed up and you end up with tons of artifacts when decoding the video during playback.</p>

<p><strong>tldr;</strong> no its not possible to crop without re-encoding in any tool.</p>

<p>Unless you deal with some very specific lossless non-consumer intermediate codecs that would enable it but in that case transcoding wouldn't be an issue.</p>
","12764"
"what graphics card features effect NVIDIA NVENC hardware encoding speed?","3957","","<p>I have a web project I am working on that requires transcoding of multiple short video clips (6-10 seconds) uploaded by users into a web friendly H.264 format on my linux server.  I have been able to enable NVIDIA NVENC hardware acceleration with ffmpeg and a 4 year old GeForce GTX 670, and I'm getting hardware encoding speeds twice that of my software encoding (Xeon E5-1620 v3).  With an $800 video card budget, I'd like to be able to transcode these short video clips as fast as possible because I will have multiple users uploading them simultaneously.  </p>

<p>The NVENC engine does have <a href=""http://developer.download.nvidia.com/assets/cuda/files/NVENC_DA-06209-001_v07.pdf?autho=1452972945_d6791efa67031b38127384c5c437f683&amp;file=NVENC_DA-06209-001_v07.pdf"" rel=""nofollow"">licensing limitations</a> when implemented on a consumer level NVIDIA card: only 2 video transcoding threads can be run simultaneously, even if you have multiple cards.  If I decide to go with one of the pricy quadro card line, then I am only limited by the other hardware elsewhere in my system in terms of how many threads I can run.  However, with my specific project I'm a lot better off transcoding these clips in a series rather than parallel because the clips will be viewed in the order they're uploaded. Clips later in the queue can be transcoded as earlier clips are viewed. If clips are transcoded in parallel on the same card, performance is inversely proportional to the number of simultaneous threads.</p>

<p>Having stated this, my plan is to set up two NVIDIA cards and run a single thread on each one to maximize throughput.  The <a href=""https://developer.nvidia.com/nvidia-video-codec-sdk"" rel=""nofollow"">NVIDIA codec SDK</a> is vague on NVENC performance difference between various cards, but it seems that there is a <a href=""http://developer.download.nvidia.com/assets/cuda/files/NVENC_DA-06209-001_v07.pdf?autho=1452974230_cc98539d0a11948f1978940adf52093e&amp;file=NVENC_DA-06209-001_v07.pdf"" rel=""nofollow"">major difference</a> between GPU generations Maxwell Gen 2> Maxwell Gen 1> Kepler.  I can find no reliable benchmarks for NVENC encoding (opposed to CUDA benchmarking, which is easy to find).</p>

<p>In the absence of hard benchmarking data comparing various cards, what features of the currently available cards would have to largest impact on single thread NVENC encoding speed?  Since the actual GPU is not utilized fully during transcoding, does GPU and memory clock speed affect this function much?  I have $400/card to spend, but if the entry level Maxwell Gen 2 GeForce GTX 960 is as good as the more recent cards, then I will put extra money into other aspects of the server (CPU/RAM/SSD etc).  I know this might seem like a subjective question (what's <em>best</em>), but I am trying to make a self-educated guess based on an understanding of the transcoding hardware.</p>
","<p>The main two features in transcode with NVENC: A <strong>video memory</strong> if you want to transcode a many streams and the second one is a <strong>GPU</strong>. Maxwell Gen 2 this is best at the moment. <br>
<em>Regarding licensing limitations</em>: only 2 video transcoding threads can be run simultaneously on a consumer level NVIDIA card, but this regulated on driver level. And with a strong desire this limitation may be removed then a maximum transcoding threads will be depend of a video memory size and Video Engine utilization, but for one transcoding thread for different video cards is necessary different video memory size. For example, one ffmpeg thread cost is 100 MB video on QUADRO K 4200(4GB) and 170 MB on GTX 980 TI(6GB). <br> <br>
<em>It may be interesting for you, my results in transcoding SD sources in a real time:</em><br>
<em>QUADRO K4200(4GB):</em> one transcoding thread cost is 100 MB and we can run in parallel about 36 threads, but the bottleneck is a video engine utilization. I can run about 30 parallel threads with -preset hp -profile:v -vcodec nvenc_264 <br>
<em>GTX 980 TI(6GB)</em>: the bottleneck is video memory. I can run about 32 threads (32*170=5440) in parallel. I did it for tests, of course. But video engine results is better then on K4200 in 2.5 times. This 32 threads with -preset slow -profile:v -vcodec nvenc_264 -vf yadif=0 and Video Engine utilization is only 80%. <br>
I tested this on <em>GTX 660(2GB, Maxwell Gen 2)</em> is about 15 parallel threads due to video memory.</p>

<p><strong>My conclusion:</strong> If you will transcode no more then 2 threads in parallel GTX 960 is a good variant, but you can save some money to another hardware and look to another video card with Maxwell Gen1. If you will go to the hack way then GTX 960 is a good variant but only with 4GB video memory.    </p>
","17943"
"FCP vs Motion 5","3926","","<p>I searched around and I have not come across a good answer.
What does FCP have that Motion5 does not? I have been looking at the two and they look relatively the same. Here is some of what I am doing/care about:</p>

<p>I love YouTube, I want to become a video maker. I want to do productions from Minecraft ExplodingTNT to Smosh to Productions and more... I need a video editor to make up for iMovie because I am sick of the export errors, slowness, and overall limitations of it. My budget is low so I don't really want to get FCP, Avid, or Adobe Production Suite until I can get the money to get them. I am also looking into 3d stuff like Cinima 4D and Autodesk Entertainment  Creation Suite (or a few of the products in that suite) but that can be for another time. I am also experienced in audio creation and editing. I have Ableton and am planning to get more software (if that is helpful to know). Really, I want to know if Motion is all that is needed for a pretty experienced, but still new person to video creation. Or if I should go strait to the big stuff.</p>

<p><strong>I still want to know some big differences regardless of my needs.</strong></p>

<p><em>If I did something wrong or you want more information on something, feel free to leave a comment because I am new to AVP.</em></p>
","<p>There's actually a <em>big</em> difference between the two. If you're on a budget and don't mind that editing video is not as straight forward, then Motion is actually not a bad route to take.  </p>

<p>Final Cut allows you to use things you've created in Motion as graphical templates. Motion in and of itself is a 2D(/3D) animation app, it wasn't made for video editing. </p>

<p>So, although you <em>can</em> edit video in Motion, doesn't mean it's very comfortable. For instance, you can just press a shortcut to render your timeline in Proxy, High Quality format. If you'd do the same thing in Motion, you would have to encode the material by yourself and tell Motion manually to use the Proxy's instead of the original material. </p>

<p>Final Cut also has some media management tools that allow you to quickly find the footage you want to edit. So if you imported tons of clips, it's easier to find a particular clip in FCP than in M5. </p>

<p>Both apps work well together, but in itself, Motion wasn't made to edit video. If you come from iWork. Editing in Final Cut Pro is almost equal (if not the same). </p>

<p>I can't talk about Avid, Premiere, but they work similarly to the ""old"" way of editing video, where the timeline is static. When there is a clip deleted between two clips, they don't ripple together. They stay at their current position. Some people prefer to edit that way. Personally I can't see myself going back to that way of editing, it is simply too slow.</p>

<p>Now in regards to 3D Motion is a 2D motion graphics application. It can do 3D too, but it misses most features ""real"" 3D apps have like fluid or explosion calculations of material. Material can also have a real material. So the app knows that wood explodes differently from glass. If you are new in 3D animation and just want to tip your toes in, go for Blender. It is hard to learn, though, because it's free, there are tons of tutorials available online to get you quick up and running. </p>

<p>After Effects is the industry leader in motion graphics. It is said that nothing can't be done in After Effects. Motion tries to an After Effects. It is also said that Motion can do everything that After Effects can, but, again, with more effort. </p>

<p>As a summary. If you're on a budget and need a great 2D motion graphics app, Motion is not a bad choice. It works well together with Final Cut Pro, a video editing app that is similar to iMovie. Though 3D animation can be done in Motion, it is better to use a dedicated 3D animation app like Blender. If you really want the most top-notch effects, templates and biggest community (and can afford the price of a monthly/yearly Cloud membership) go with Adobe. </p>

<p>Hope this helps.</p>
","5827"
"How to prevent users from downloading videos?","3924","","<p>We want to provide VOD service via http/https, but we dont want users download a video by finding out the url directly from the web player or using a developer plugin like firebug. </p>

<p>We are told that only option for us is to implement DRM to protect video content.
Is it true that DRM is the only option?
If there are other options, please suggest it.
Any expert opinion is most welcome on this.
Thanks in advance.</p>
","<p>Unfortunately, no, that's the entire reason DRM exists.  It is a bit like trying to prevent someone from recording an on-air broadcast.  When you send video data over the internet to someone's player, they can simply store the information being sent to the player unless you obfuscate it and make it so that the player will only work under certain circumstances and will not share the data.  This, by definition, is DRM.</p>

<p>What DRM attempts to do is control the reading of the data entirely, so that it can not be copied.  This has varying degrees of success and rarely, if ever, works particularly well.  It may keep honest people honest, but if you are sending someone data in a way that they can access it, measures to try to stop them from copying it are... difficult.  The most advanced systems use special display drivers and encrypt the data right up to the point it is being displayed on the screen (HDCP), that way other software on the computer can't directly pull the information off the frame buffer being prepared for the screen.</p>

<p>There may be some ways you can mildly obfuscate the access to your video, but ultimately, if you send it in the clear, it is trivial for a knowledgeable viewer to store the datastream.  If you use DRM, it is substantially harder, but still likely to be able to be worked around by a dedicated attacker.</p>
","17187"
"Create a movie file from a set of pdf files","3907","","<p>I have a set of files from which I would like to create a movie file with a certain frame rate. I can, however, not seem to find software that is able to do this.</p>

<p>I'm working under Ubuntu Linux. Ideally the movie would maintain the vector graphics format from the pdf. But I do not know if software exists which makes this possible. If this is impossible, I could also convert the pdf files to a nonvector format and produce a movie from it.</p>

<p>Does anybody have experience with this?</p>
","<p>Using the latex package <code>animate</code>, a movie consisting of vector graphics can be made.</p>

<p>Construct a set of, for instance, pdf files with a base name <code>img-</code> and construct the file <code>main.tex</code> containing the code</p>

<pre><code>\documentclass{standalone}
\usepackage{animate}
\begin{document}
    \animategraphics[controls,autoplay,loop]{10}{img-}{1}{20}
\end{document}
</code></pre>

<p>. Compiling the <code>main.tex</code> file produces a pdf file with an embedded vector graphics movie. For playback, either <code>Adobe Reader</code> or <code>PDF-XChange Viewer</code> is needed. For more details, consider the manual of the <code>animate</code> package at <a href=""http://ctan.math.utah.edu/ctan/tex-archive/macros/latex/contrib/animate/animate.pdf"" rel=""nofollow noreferrer"">The animate Package</a>. For Linux users, I believe only Adobe Reader is an option. Follow the instructions at <a href=""https://askubuntu.com/questions/89127/how-do-i-install-adobe-acrobat-reader"">How do I install Adobe Acrobat Reader?</a>.</p>
","12273"
"Microphone suggestions or software to avoid background noise in audio","3867","","<p>I am afraid this is a bit off topic but I really need to find a microphone recommendation to record home videos without background noise. If this site is not the place and there is a site for searching these specific microphones, please let me know.</p>

<p>I have tried <a href=""http://www.microsoft.com/hardware/en-us/p/lifechat-lx-3000"" rel=""nofollow"">Microsoft LiveChat LX 3000 microphone</a> but it is adding a wierd background noise into my video recording.</p>

<p>Also if there is a known software that reduces background noise, please share its name.</p>
","<p>Your most critical need is going to be a professional low noise analog to digital converter and pre-amp with a decent quality microphone to go with it.  There are a few options you can pursue for this depending on your interests.</p>

<p>Since you are currently working with a computer directly, you could go for any of a number of professional audio capture devices with XLR and phantom power capabilities, such as the X2U adapter from Shure that you were talking about or a device like an MAudio audio interface unit.  </p>

<p>Since these only provide inputs you would also need a microphone.  Depending on how close you mind the mic being, something as simple as an SM58 would produce nice clean audio for you, this has the added advantage of being usable really close to your mouth and focusing the pickup in a cardioid pattern that mostly picks up stuff in front of the microphone, thus it will pick up less background noise.</p>

<p>Alternately, you could use a condenser mic or shotgun mic that could be placed further away, however a condenser mic would pick up more of the room sound and a shotgun mic would require careful aiming in order to make sure it picks you up and not the room.  A Lapel mic or LAV is also an option.  A lapel mic is a type of condenser that can clip to your shirt and therefore pick up a little less noise, but will still be in the shot, though it is less obvious than a dynamic mic like an SM58.</p>

<p>Yet another option to get additional flexibility is to go with a device like an H4n recorder.  This is what I use for my video work.  It is a stand alone recorder which can also be used as an audio interface for the PC and also has built in stereo condenser mics.  The device runs a little bit more expensive than an audio interface, but is normally a bit cheaper than an interface and a condenser mic pair.  It also can be used away from a computer for any project.</p>

<p>Additionally, the H4n includes XLR and 1/4"" inputs so that it can be used with any of the mics you could use with an audio interface if you later decide to upgrade or change to a different kind of microphone for a particular recording task.</p>
","11954"
"Video in After Effects composition panel is pixelated during scrubbing","3851","","<p>I am new to AE and following a Lynda.com essentials course.  In the course, the video in the composition panel stays sharp during scrubbing, as below:</p>

<p><a href=""https://imgur.com/bhYwrZv"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/bhYwrZv.png"" title=""source: imgur.com"" /></a></p>

<p>But in my AE application, it shows as pixelated during scrubbing, as below:</p>

<p><a href=""https://imgur.com/Gc3oHgI"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/Gc3oHgI.png"" title=""source: imgur.com"" /></a></p>

<p>My AE CC 2014.0 installation is brand new, without any configuration tweaks.  I am on Windows 8.1, using an Acer V5-571 laptop (i5).</p>
","<p>It's because you're using Adaptive Resolution, where AE will attempt to let you scrub through quickly, by reducing the resolution it shows in the viewer.  Higher powered computers will be able to show higher resolution in the viewer more quickly.</p>

<p>There's an explanation in Adobe's help here:
<a href=""https://helpx.adobe.com/after-effects/using/previewing.html"" rel=""noreferrer"">https://helpx.adobe.com/after-effects/using/previewing.html</a></p>

<p>There are also many things you can do to adjust the quality of your preview, depending upon what you're working on.. For example, if you're checking the movement of something, maybe you don't need as high resolution an image but full frame rate... Whereas if you're checking the pixel-perfect quality of a matte, maybe you don't need as high frame rate during preview.</p>

<p>Also bear in mind that there are different render engines in AE now - if you're using the 3D raytrace renderer for 3D work, previews will be very slow unless using a super-fast machine.</p>
","15097"
"Who owns raw footage?","3850","","<p>I am working at a Multimedia Production placement for my college course for a company that wants a promotional video. I shot about 4 hours of video.</p>

<p>They believe claim that I have no right to use the raw footage that I shot for anything (including my own portfolio) and that they have the right to full access of the raw &amp; audio footage that I shot.</p>

<p>In order to avoid escalating the situation, I didn't say anything because right now, I am the sole holder of all the raw footage files.</p>

<p>They want to be able to use the footage I shot for their projects in the future and I doubt credit will be given to me for the footage.</p>

<p>Do I have the right to put a watermark on the footage before I give it to them?
Do they have the right to the raw footage, or ONLY the finished product which I am producing for them?</p>

<p>I signed a generic contract when I went into this placement, but the form mentioned nothing about media (as this is a hospital). So what is the default rule of thumb about the rights to this footage?</p>

<p>I am not sure if it makes a difference, but I am in Canada</p>
","<p><em><strong>I am not a lawyer, but I played one on TV.</em></strong></p>

<p>If the contract wasn't specific than you might have some wiggle room. Typically contract law (varies by state) says that if the contract is unclear then it favors the person who didn't write it. So if they provided you an unclear contract it favors you when it isn't clear.</p>

<p>Unfortunately if they hired you to create the ""work"" of the footage, then it becomes a ""<a href=""http://en.wikipedia.org/wiki/Work_for_hire"" rel=""nofollow""><strong>work for hire</strong></a>"":</p>

<blockquote>
  <p>A ""work made for hire"" is (1) a work prepared by an employee within the scope of his or her employment; or (2) a work specially ordered or commissioned for use as a contribution to a collective work, as a part of a motion picture or other audiovisual work, as a translation, as a supplementary work, as a compilation, as an instructional text, as a test, as answer material for a test, or as an atlas, if the parties expressly agree in a written instrument signed by them that the work shall be considered a work made for hire. (17 U.S.C.  101)</p>
</blockquote>

<p><em>Read up on work for hire, but generally speaking, unless the contract states otherwise, if you are hired / contracted to create something, the party paying you owns your work.</em></p>

<p>BTW, to be a work for hire doesn't mean they paid you cash. They could be paying you in work experience. </p>
","3501"
"720p 60fps playing in slow motion need real time speed?","3820","","<p>I just got a kodak playsport zx5 and shot some video of a volleyball game at 720p 60fps. I figured since the ball was moving so fast i'd need the 60fps to capture the games clearly. I just downloaded the videos and playing them in KMPlayer and they are in slow motion. How can I get them to play in real time?</p>

<p>Thanks</p>
","<p>The reason it plays back in slow-motion is because you recorded it in slow-motion mode. From <a href=""http://store.kodak.com/store/ekconsus/en_US/pd/ThemeID.3925700/PLAYSPORT_Video_Camera__Zx5/productID.221644700"" rel=""nofollow"">Kodak's web page</a> (my emphasis):</p>

<blockquote>
  <p>720p at 60 fpsfor fast action and <strong>super slow motion playback</strong> </p>
</blockquote>

<p>Your computer have no problem playing back the video if CPU usage is just 30% and have a full frame rate of 60 fps.</p>

<p>What you need to change this is to re-time your video.</p>

<p>You can do is in many ways - I would suggest a video editor such as Adobe Premier Pro (try their 30 days free trial if you don't have it already), select a project that fits the format (HD720P) but with 30 fps.</p>

<p>Then drop your video on the time line, right-click and select change playback speed (or something in those words). Set 200% or what will suit your video. Render out to final video and the problem will be gone.</p>
","5576"
"Are there any red filters that fit an SJ4000","3778","","<p>Are there any red filters to fit the SJ4000 camera? Is is possible to use one compatible with a gopro hero 3?</p>
","<p>While I can't confirm it.  It appears that people online have had success with using GoPro filters of the type that just slide between the case and the lens.  You could also probably just order some appropriately shaded lighting gel or optical filter material and cut it to size yourself.</p>
","11925"
"After Effects: Parent a Camera's position to a Null-Object with specific Off-Set","3728","","<p>I am just struggling with a simple expression thing I guess.</p>

<p>I have a camera looking at an Object in 3D Space. Actually The Object is at [0,0,0]. The Camera's position is at [29,-1637,-2248].</p>

<p>What I am trying to do is, to move the camera. For a better workflow I want to parent the camera's position to the position of a Null-Object, also at [0,0,0]. The thing is the camera has to keep its offset from [0,0,0] and just move parallel with the null at [0,0,0].</p>

<p>Can somebody help me?
Thanks in advance!</p>
","<p>To drive an animation by another layer use the pick whip or the selection list to <strong>parent</strong> it. In the following example the <code>Camera (layer)</code> adopts all animation properties of the <code>Null</code>.</p>

<p><img src=""https://i.stack.imgur.com/6WZW1.jpg"" alt=""enter image description here""></p>

<p>Note: <code>Camera</code> is a 3d layer in your case, make sure <strong>3d layer toggle</strong> is enabled for the <code>Null</code> too.</p>
","15177"
"WMV vs MP4 Streaming & Download","3695","","<p>I work for a company that outputs videos in many formats.</p>

<p>We currently stream movies in FLV format, offer downloads in a WMV and MP4 format.</p>

<p>However in the interest of saving server space and offering the user a better service, we are preparing to offer our streams in MP4 instead.</p>

<p>What I am wondering is: What format would be good for downloads:</p>

<ul>
<li>Download size</li>
<li>Encoding time</li>
<li>Format compat: Will it work out of the box on both a Mac and Windows machine.</li>
</ul>

<p>To my knowledge, WMV works out of the box on Windows and MP4 works out of the box on Mac. Is there a format that works for both and has a reasonable file size?</p>
","<p>Your comparison of WMV to MP4 is a little bit confusing, because you're comparing apples and oranges.</p>

<ul>
<li><p>MP4 is a container format, which may contain a variety of audio and video formats.  Most commonly, an MP4 file will contain wither an MPEG-4 Part 10 (aka H.264) or MPEG-4 Part 2 video stream, although it can contain MPEG-2 or MPEG-1 video streams.  There are a number of audio codecs that can be stored in an MP4 file as well. See <a href=""http://en.wikipedia.org/wiki/Mp4#Data_streams"" rel=""nofollow"">Wikipedia</a> for more information.</p></li>
<li><p>WMV is an actual codec (akin to H.264 or MPEG-4 Part 2 mentioned above), and files that end in .WMV are generally stored in an ASF container (<a href=""http://en.wikipedia.org/wiki/Windows_Media_Video#Container_format"" rel=""nofollow"">see here</a>).</p></li>
</ul>

<p>What this all boils down to, is you need to use a <em>codec</em> that both Mac and Windows can play.</p>

<p>According to <a href=""http://support.microsoft.com/kb/291948"" rel=""nofollow"">Microsoft</a>, as of July 2010, Windows Media Player for Windows XP supports (among others):</p>

<ul>
<li>Microsoft MPEG-4 Standard Video Codec</li>
<li>Vivo H.264 Video CODEC</li>
</ul>

<p>Which suggests that it ought to play back the two common video formats contained in .MP4 files.  Note the caveat:</p>

<blockquote>
  <p><strong>NOTE:</strong> Not all of the supported codecs are installed by default. These codecs will be downloadable when you play content that requires the codec.</p>
</blockquote>

<p>So it's possible that your users may be prompted to install a codec the first time they try to play one of your videos.</p>

<p>You'll also want to make sure you use an audio codec supported by Windows Media Player. The most common (but by no means only) one is probably MPEG-1/MPEG-2 Audio Layer III (aka MP3), and this is in the list of supported codecs.</p>

<p>So based on this information, if you use MP4 streaming, Windows users should have no trouble playing back your videos.  Although you'll surely want to test your specific output files on a fresh Windows (and Mac) install before you commit to this decision.</p>
","2587"
"Is there a video type for vector animation?","3690","","<p>Is there a vector-based video type, preferably open-source, so that an animated clip could be produced using ""rules"" instead of pixel-compression?</p>

<p>This would mean lossless resolution-independent video. If it does exist - a link to the specifications would be helpful.</p>
","<p>There was one, but it fell in to disuse and isn't used very often anymore, largely because of the lack of mobile support, but also due to security issues it created.  It was called Flash.</p>
","13305"
"FFMPEG output is blocky","3664","","<p>I'm attempting to extract the individual frames from a raw AVC video stream. I'm doing this via the following ffmpeg command: <code>ffmpeg -q 1 -i video_test.avc -f image2 frames/frames_%04d.jpg</code></p>

<p>The problem I'm encountering is the first few frames look good. I'm including an example below.
<img src=""https://i.stack.imgur.com/9KWDv.jpg"" alt=""Example of good frame""></p>

<p>The extracted frames then get progressively worse and by the time I get to the 17th frame, they're really blocky.
<img src=""https://i.stack.imgur.com/imKyK.jpg"" alt=""Example of blocky image""></p>

<p>You get get the raw video from this <a href=""https://dl.dropboxusercontent.com/u/21682354/Internet%20Posts/ffmpeg%20issue/video_test.avc"" rel=""nofollow noreferrer"">link</a>.</p>

<p>Is there an option I'm missing? I've noticed that FFMPEG on my Mac works as expected and generates nice frames, but FFMPEG on my production server that runs Ubuntu exhibits this problem. </p>
","<h3>Option placement matters</h3>

<blockquote>
  <p>Is there an option I'm missing?</p>
</blockquote>

<p>No, but option placement matters. Options before the input apply to the input, and options before the output generally apply to the output. The exception are global options. See the <a href=""http://ffmpeg.org/ffmpeg.html#Synopsis"" rel=""nofollow noreferrer"">FFmpeg synopsis</a> and <a href=""http://ffmpeg.org/ffmpeg.html#Description"" rel=""nofollow noreferrer"">FFmpeg description</a> for more details on option placement.</p>

<p>Therefore, you must move your <code>-q</code> (alias for <code>-qscale</code>) option:</p>

<pre><code>ffmpeg -i video_test.avc -q:v 2 frames/frames_%04d.jpg
</code></pre>

<ul>
<li><code>-f image2</code> is superfluous here since ffmpeg knows that your outputs are images.</li>
<li>A sane range for <code>-q:v</code> for the encoder <code>mjpeg</code> is 2-5; lower is higher quality.</li>
<li>It is recommended to be explicit as to which stream you want to apply <code>-q</code> to since ffmpeg can not know if it should be applied to video or audio, so in your case use <code>-q:v</code>. This is good practice, although it does not matter for your input since you have no audio.</li>
</ul>

<h3>A fake ""ffmpeg""</h3>

<blockquote>
  <p>I've noticed that FFMPEG on my Mac works as expected and generates nice frames, but FFMPEG on my production server that runs Ubuntu exhibits this problem.</p>
</blockquote>

<p>Ubuntu no longer uses FFmpeg:</p>

<ul>
<li><a href=""https://stackoverflow.com/a/9477756/1109017"">Who can tell me the difference and relation between ffmpeg, libav, and avconv?</a></li>
<li><a href=""http://blog.pkh.me/p/13-the-ffmpeg-libav-situation.html"" rel=""nofollow noreferrer"">The FFmpeg/Libav situation</a></li>
</ul>

<p>Since I do not use the fork my answer may not work for you.</p>

<h3>Getting ffmpeg</h3>

<p>FFmpeg development is very active and it is always a good idea to use a recent version. Two main ways of getting it are:</p>

<ul>
<li>Follow a step-by-step <a href=""http://trac.ffmpeg.org/wiki/UbuntuCompilationGuide"" rel=""nofollow noreferrer"">guide to compile ffmpeg</a></li>
<li>Use an already compiled <a href=""http://ffmpeg.org/download.html#LinuxBuilds"" rel=""nofollow noreferrer"">ffmpeg build</a> (<a href=""https://askubuntu.com/a/270107/59378"">instructions</a>)</li>
</ul>
","8622"
"Improving frame/image quality on Handbrake?","3660","","<p>I created a .cdr file from a DVD and converted it to an .mp4 with <a href=""https://handbrake.fr/"" rel=""nofollow noreferrer"">Handbrake</a>. My only issue is Handbrake's default settings create a somewhat grainy picture. The file size went from 1.24Gb to ~100Mb so I assume that explains it. </p>

<p>I'm unfamiliar with the determinants of frame image quality. Here's a screenshot of Handbrake's interface, might someone point me in the direction of the thing to toggle that might improve the .mp4 quality?</p>

<p><img src=""https://i.stack.imgur.com/WFO1X.png"" alt=""enter image description here""></p>
","<p>Under the video tab, you'll see two options. Video Codec, which is how the video is encoded (h.264 is the standard for .mp4 files), and Quality, which controls the level of video compression that occurs.</p>

<p>It's the Quality slider that you're mostly worried about if you want to keep it an mp4. Drag that slider to the right to improve the quality (this will also increase the file size) or to the left to decrease the file size (this will also decrease the quality). Or if you know exactly the size of file you want, divide the desired file size by the number of seconds and you can input the kb/s that you want the video to be compressed to (smaller file sizes means higher compression and lower quality, higher file sizes equals less compression and higher quality).</p>

<p>In order to test to make sure that you like your quality selection and file size, you can choose a single chapter of the film (if your cdr has that stored) and run that through the encoder and test the quality.</p>

<p>Another thing that is available to choose is the video resolution, but in my experience with handbrake, it's quite temperamental, so I only recommend messing with that if you really need to.</p>
","13252"
"Final Cut Pro 7: Zoom without using scale","3648","","<p>Here's what I want to do:</p>

<p>I have footage witch I want to divide into several different squares at the same time (think ""24""), while also cropping the footage so that I can show certain parts of the original material. All this I can do with the ""crop"" and ""scale"" functions under ""motion"". However, I would also like to zoom in, within one of these frames, during the video. If I use scale for this, the whole frame gets bigger. That's not what I want.</p>

<p>I've circumvented the problem by creating a white square, placed it on a video track higher in the hierarchy than the one I want to manipulate, then using color key to let the footage shine through the white parts. However, this isn't especially confortable to use, especially since I haven't found any good way to feather the edges of the box.</p>
","<p>I do this all the time. It's super easy:</p>

<ol>
<li><p>Set up the 'shape' of your frame with the crop tool. </p></li>
<li><p>Apply the effect 'Basic 3D' (Effects > Video Filters > Perspective > Basic 3D).</p></li>
<li><p>In the controls for that effect, use the 'Scale' parameter to zoom your image. It will not affect your crop marks!</p></li>
</ol>

<p>Good luck!</p>
","3394"
"Burn MKV with subtitles on a DVD","3623","","<p>I have a movie on an mkv file, the movie has video, audio and the subtitles all in the mkv file but I cannot find a way to burn it to dvd including the subtitles. I need to get VIDEO_TS and AUDIO_TS files out of the mkv file so I can burn those on dvd with a program like ImgBurn.</p>

<p>Please tell me which program I can use to get those VIDEO_TS and AUDIO_TS files (including working subtitles) out of my MKV file, I would like to keep the quality the MKV file has, and I need a program that is 100% free, I don't want a free trial.</p>

<p>Thank you for reading.</p>

<p>Best Regards,
Bas</p>
","<p>Use <a href=""http://www.freemake.com/free_video_converter/"" rel=""nofollow"">Freemake Video Converter</a>, it's 100% free and you can just burn MKV files on dvd, on the dvd, the subtitles also work if you set that up properly in the <a href=""http://www.freemake.com/free_video_converter/"" rel=""nofollow"">Freemake Video Converter</a> program.</p>
","15697"
"Is there a way to capture RTMP-stream and output it to SDI?","3614","","<p>Is there any way to capture an RTMP stream and output it to SDI?</p>

<p>Here is my situation: We specialize in producing local sports for web streaming. Occasionally we are asked to deliver a stream to a local TV-station. Paying for uplink truck and the whole satellite thing is way beyond our budget.</p>

<p>I'm not really familiar with IP-based workflows, that's why capturing RTMP-stream was my first idea.</p>
","<p>I have succesfully used Wirecast software in the past to bring in two RTMP sources and switch between them for live streaming an event from two different venues across the country. It worked wonderfully. You could then use a cheap BlackMagic device to get that out of your pc into an SDI feed.</p>

<p>Wirecast sources: <a href=""http://www.telestream.net/wirecast/devices.htm#ip"" rel=""nofollow"">http://www.telestream.net/wirecast/devices.htm#ip</a></p>
","16894"
"Are there any good video editing programs with a command line interface?","3601","","<p>I am currently looking for a video editing program which I can control via the command line. The main feature I need is the ability to add some overlay text. Is there anything out there that can do this?</p>

<p>My ideal platform would be Linux, but I would be open to using Windows or OSX if need be.</p>
","<p><a href=""https://ffmpeg.org/"" rel=""nofollow noreferrer"">FFmpeg</a> (<a href=""http://ffmpeg.org/trac/ffmpeg/wiki"" rel=""nofollow noreferrer"">wiki</a>) is one option; you can achieve what you want either with the <a href=""http://ffmpeg.org/trac/ffmpeg/wiki/How%20to%20burn%20subtitles%20into%20the%20video"" rel=""nofollow noreferrer"">subtitles filter</a> (see also <a href=""https://ffmpeg.org/ffmpeg-filters.html#subtitles-1"" rel=""nofollow noreferrer"">here</a>) or the <a href=""https://ffmpeg.org/ffmpeg-filters.html#drawtext-1"" rel=""nofollow noreferrer"">drawtext filter</a>.</p>

<p>The subtitles filter requires ffmpeg to be compiled with <code>--enable-libass</code> and drawtext requires it to be compiled with <code>--enable-libfreetype</code>. If you're on Linux, the former is fairly likely to be the case, though the latter may not be. Also, beware than Debian and its derivatives (including Ubuntu et al) are using the forked tool avconv, from the libav project (see <a href=""https://stackoverflow.com/questions/9477115/who-can-tell-me-the-difference-and-relation-between-ffmpeg-libav-and-avconv/9477756#9477756"">here</a> for a summary of the situation); some people have reported problems with avconv vs ffmpeg. I would recommend using ffmpeg, mainly because there seem to be more ffmpeg users than avconv users on the SE network, so you'll be able to get better help here.</p>

<p>If the version in your repos is not compiled to do these, you can grab a static build <a href=""https://ffmpeg.org/download.html"" rel=""nofollow noreferrer"">from the FFmpeg website</a>, or you can compile it yourself, <a href=""http://ffmpeg.org/trac/ffmpeg/wiki/CompilationGuide"" rel=""nofollow noreferrer"">following one of the guides on the ffmpeg wiki</a>, making sure to use the correct flags.</p>
","7465"
"ffmpeg pixel format definitions","3583","","<p><code>ffmpeg -pix_fmts</code> lists many pixel formats. In my ffmpeg, there are 66 different pixel formats that start with <em>yuv</em>. A few of them are familiar to me (e.g., yuv422p), but most of them are not (e.g., yuva422p16be).</p>

<p>Where are these pixel formats defined?</p>
","<p>If your question was</p>

<blockquote>
  <p>Where are these pixel formats defined?</p>
</blockquote>

<p>Go to <a href=""http://ffmpeg.org/download.html"" rel=""nofollow"">http://ffmpeg.org/download.html</a> and download the source of ffmpeg. Then unpack it</p>

<pre><code>tar -xavf FILENAME.tar.*
</code></pre>

<p>and descend to the folder in the source code that is called <code>libavutil</code></p>

<pre><code>cd ffmpeg-*/libavutil/
</code></pre>

<p>and open the file <code>pixfmt.h</code> where you will find a description of all pixel formats. <strong>A few sample lines</strong></p>

<pre><code>AV_PIX_FMT_YUV420P,   ///&lt; planar YUV 4:2:0, 12bpp, (1 Cr &amp; Cb sample per 2x2 Y samples)          
AV_PIX_FMT_YUYV422,   ///&lt; packed YUV 4:2:2, 16bpp, Y0 Cb Y1 Cr
AV_PIX_FMT_RGB24,     ///&lt; packed RGB 8:8:8, 24bpp, RGBRGB...
AV_PIX_FMT_BGR24,     ///&lt; packed RGB 8:8:8, 24bpp, BGRBGR...
AV_PIX_FMT_YUV422P,   ///&lt; planar YUV 4:2:2, 16bpp, (1 Cr &amp; Cb sample per 2x1 Y samples)
</code></pre>

<p>Additionally you can read <a href=""https://en.wikipedia.org/wiki/YUV"" rel=""nofollow"">https://en.wikipedia.org/wiki/YUV</a> and its cited sources and external links.</p>
","16402"
"Do video formats support colour profiles?","3541","","<p>Recently, I was reading a wiki, dedicated to one popular cartoon show and stumbled upon <a href=""http://mlp.wikia.com/wiki/Forum:Australian_iTunes_rips"" rel=""nofollow noreferrer"">an interesting problem</a>: <img src=""https://i.stack.imgur.com/s0Pvv.jpg"" alt=""frames from the different regional iTunes stores""></p>

<p><em>From left to right: frames from the US iTunes, Australian iTunes and US iTunes version with CC in a video player</em> </p>

<p>As you may see, it's the same frame, but colours are different. So, it makes me wonder: does H.264 standard / MP4 container supports any kind of colour profiles (like, ICC in JPEG) and if yes, does any player support it? And what about DVD/Blu-Ray disks? Or there is no way to watch the same version of any video, as it was shot by it's creators? Thanks. :)</p>
","<p>No most video formats (nearly all) do not allow custom ICC color profiles to be embedded. (Improvemet/correction: MP4s can have color profiles tagged in the metadata of a video file.) most video on the consumer end is intended for the REC.709 color space. sRGB is similar and uses the same primaries. transcoding (including compressing) will almost always result is a slight shift in color accuracy, saturation, gamma curve, or black and white level shift. This is made worse due to converting between RGB and YCbCr color spaces.</p>
","12100"
"Remove audio tracks from multiple MKV files","3532","","<p>I've got a multitude of MKV files each containing two audio tracks that correspond to two languages. In each of the files, the English language is the second track and it's the only track I need for these MKV files. So how do I easily discard the first audio track in each of the MKV files.</p>

<p>N.B. My primary OS is Windows but if there really is no easy way to have it done on Windows, I could be interested in a Linux answer too.</p>
","<p>I don't know Windows, but I have just gone through this in Mac OS X (Linux Applicable).</p>

<p><a href=""http://www.bunkus.org/videotools/mkvtoolnix/"" rel=""nofollow"">mkvtoolnix</a> is your friend here, and is available for Windows.</p>

<p>I used the GUI to check (a sample of) my files to make sure the audio tracks that I wanted to keep and delete were in the same order in the files. Then I manipulated the options in the GUI to match what I wanted.</p>

<p>Finally, I used the ""copy to clipboard"" button to copy the command line output to the clipboard and massaged it into a bash script to run on a directory of the files I wanted strip the audio from. </p>

<p>I know it's quick and dirty (and maybe not to your liking), but you can probably use it as a starting point to massage into what you <em>really</em> want to do.</p>

<pre><code>#!/bin/bash

for file in *.mkv; do mkvmerge -o ""$file-noeng.mkv""  --language ""1:jpn"" --track-name ""1:AVC-HD"" --default-track ""1:yes"" --forced-track ""1:no"" --display-dimensions ""1:1920x1080"" --language ""3:jpn"" --track-name ""3:AAC"" --default-track ""3:no"" --forced-track ""3:no"" --language ""4:eng"" --track-name ""4:ASS"" --default-track ""4:no"" --forced-track ""4:no"" -a ""3"" -d ""1"" -s ""4"" --attachments ""1"" -T ""--no-global-tags"" ""$file"" --track-order ""0:1,0:3,0:4"";

done
</code></pre>
","4569"
"Origin of ""desktop capture engine""","3525","","<p>I'm trying to locate a desktop capture driver.</p>

<p>I am using Flash Media Encoder 3.2 and had a device listed as ""<code>desktop capture engine</code>"" in my list of devices. Allowed me to stream my desktop.</p>

<p>I had no idea where it came from so I tried to uninstall some programs to see which one ""removed"" it. Bright idea, I Know!</p>

<p>So after removing RealVNC and Super! from my computer, it's gone, but re-installing those programs doesn't re-add the device. I'm wondering if I had a different version of RealVNC installed. I don't remember downloading from their website before.</p>

<p>Anyone know what the origin of this driver might be?</p>
","<p>I have installed SUPER today, and Desktop Capture Engine has appeared on my computer.</p>
","9208"
"Final Cut Pro- Reopen Library and Entire Timeline is Gone!","3520","","<p>I was working with a library, arranged all my clips, audio, etc. and had a great 6 minute long video ready to go.</p>

<p>When I closed the library, I shut my computer off. After I rebooted, I went to reopen the library and all of the files are still part of the library, but all of the clip arranging and everything I did to it- the entire timeline- is gone!</p>

<p>It's just completely blank as if all I did was drop files into the library and never started working on anything from within the timeline.</p>

<p>What happened!? Is there any way to view all of my timeline? That's like 12 hours worth of work!</p>

<p>Thanks</p>
","<p>The timeline for which you are looking is actually called a ""Project"".  Projects are stored within events, which are in turn stored within a library.  Just opening a library (from within the finder, for instance) will not necessarily open your project.  To do that, you need to twirl down the disclosure triangle next to your Library in the event manager, select the event upon which you were working, and then double click the Project name to load it in the timeline.  Also, make sure that you are connected to any drives or network storage locations that you might have stored media to when you created the project.</p>
","12515"
"Correct font properties for subtitles","3466","","<p>I'm editing a video using Adobe Premiere CC, I need to put subtitle. What is the best font properties?
I'm using: </p>

<p>Font Family- <strong>Arial Rounded MT</strong>;
Font Size- <strong>15</strong>;
Color- <strong>#FFF600</strong>;
Strokes > Inner Strokes- color <strong>#000000</strong></p>

<p>Is this the correct way?</p>
","<p>For some reason the default subtitle colour often seems to be yellow; I can't find any definitive reason why though. I hate yellow subtitles with a passion, they always look hideous and distracting, so I always make mine white. To separate them from light coloured backgrounds I use a soft black drop shadow if possible, or otherwise a black border. As you can see below they are readable even over light backgrounds.</p>

<p>The size depends on the viewing medium. The one below looks a bit wee in the size it's displayed on this page (on a retina screen on a 15"" laptop), so if I was doing subtitles for handheld devices where it might be displayed at this size I'd make it larger, but on a TV it's plenty big enough. In a cinema it might be a bit too large. Resolution is a factor, too. If this was going to standard definition I'd have to bump up the font size a bit too.</p>

<p>Sans-serif is my default choice, usually Arial because it's so bland, so it doesn't distract from the art direction. Serif fonts are also a problem if the video is ever going to be displayed on interlaced screens (which applies if it is to be broadcast, at least until all the old interlaced TVs have gone to the verge), because the thin horizontal lines flicker between fields. Also at low resolutions the thin parts of the letters get lost. So while serif fonts generally have greater readability they might not be the best choice just yet.</p>

<p><img src=""https://i.stack.imgur.com/FhjDd.jpg"" alt=""enter image description here""></p>
","14670"
"Simulating a camera flash","3433","","<p>Can anyone give me some tips on simulating a camera flash in After Effects. I am trying to make it keyframing the scale and opacity of a white shape layer but I can't get it to look right.</p>
","<p>As a photographer, a flash is simply a very short burst of white light (as you're no doubt aware). The maximum time at full power is usually around 1/250th of a second, which is equal to 1/10th of a frame. In that case, if you were to accurately represent a flash being fired, you would simply boost the exposure of a single frame by several stops (to make it almost completely white, whilst retaining a little bit of detail).</p>

<p>Something as simple as this would suffice.</p>

<p><img src=""https://i.stack.imgur.com/qjGwN.jpg"" alt=""enter image description here""></p>

<p>For one frame.</p>

<p><img src=""https://i.stack.imgur.com/zKx91.jpg"" alt=""enter image description here""></p>

<p>If you really wanted to be technically accurate, you could simulate the CMOS flicker of a flash by only boosting the exposure on a horizontal portion of the frame, say the top 2/3rds or something.</p>
","3542"
"Extract streams from WebM","3419","","<p>I hope my question fits the theme of this SE. </p>

<p>Is there any software that can extract streams from WebM videos? I don't want to convert (re-compress), I want the streams intact. Thanks.</p>
","<p>Quote: ""Since WebM is a Matroska subset, mkvtoolnix should let you demux the files. It's open source, cross platform, and the author provides binaries for Windows.""</p>

<p>Source: <a href=""https://superuser.com/questions/412890/lossless-extraction-of-streams-from-webm"">https://superuser.com/questions/412890/lossless-extraction-of-streams-from-webm</a></p>
","3764"
"zooming animation is not smooth in Premiere CS6","3414","","<p>Animatedly, I tried to zoom into a video clip in Premiere by creating two Position keyframes and two Scale keyframes</p>

<p><img src=""https://i.stack.imgur.com/XUx2g.png"" alt=""Premier transform effect animation""></p>

<p>But <a href=""https://www.dropbox.com/s/h3qwy01xxksy1tm/hammering-removal.flv"" rel=""nofollow noreferrer"">the result is very shaky for some reason?</a></p>

<p>What am I doing wrong with this implementation, or should I achieve this zooming effect by other means?</p>

<h3>Same shaky result using linear interpolations</h3>

<p>If I use just Position keyframes (Scale keyframes deleted for troubleshooting), and if I set the keyframes' <strong>Temporal Interpolation</strong> to <strong>Linear</strong> and also the <strong>Spatial Interpolation</strong> to <strong>Linear</strong>, <a href=""https://www.dropbox.com/s/me0kvm6fqb3upgd/position-only-linear-animation.flv"" rel=""nofollow noreferrer"">the resultant video is still jerky, as evidenced here</a></p>

<ul>
<li>I set the temporal and spatial interpolations to Linear for both keyframes i.e. for both the starting and middle keyframes</li>
</ul>

<p><img src=""https://i.stack.imgur.com/L78ny.png"" alt=""Premier transform effect linear animation""></p>

<p>There are a number of other interpolation options, maybe one of them would help?</p>
","<p>Turns out that one should use <a href=""http://crawfordschools.org/dshepard/Broadcast%20Video/PSA/p4_09_howto_motion_effect.pdf"" rel=""nofollow"">the Motion effect</a> and not the Transform effect for zooming in Premiere, even though they have exactly the same Position and Scale controls!</p>

<p><strong><a href=""https://www.dropbox.com/s/64lc85thyb6jm3k/zooming-with-motion-effect.flv"" rel=""nofollow"">Here is the result with the Motion effect</a></strong></p>

<p>p.s. <a href=""http://help.adobe.com/en_US/premierepro/cs/using/WS52DEF9DC-C740-41f3-865A-46DB6827BCC0.html"" rel=""nofollow"">The Anti-Flicker filter</a> I discovered under the Motion effect was left to the default of 0, so that was not needed.</p>

<p>p.p.s. I also found <a href=""http://forum.videohelp.com/threads/303107-Premiere-CS4-My-zoom-PNG-image-is-stutter!"" rel=""nofollow"">this discussion</a> of a similar issue</p>
","7276"
"Why is MP4 black background gray in Internet Explorer 11?","3408","","<p>I exported a Flash movie with a black background to MOV, and converted the MOV to mp4 (H.264). In Quicktime player, the background of the MOV and mp4 are black. In native players in Firefox and  Chrome (desktop) and Safari and Chrome (ipad), the background of the mp4 is black. In IE11... it's dark gray. </p>

<p>The actual gray is rgb 16 16 16, hex #101010. 
Any ideas?</p>
","<p>The solution was to turn off IE-11's ""hardware acceleration"" feature which (<a href=""http://support2.microsoft.com/kb/2528233"" rel=""nofollow"">says here</a>) ""lets Internet Explorer move all graphics and text rendering from the CPU to the Graphics Processing Unit (GPU)"".</p>

<p>I disabled that under Tools -> Internet Options-> Advanced by selecting ""Use software rendering instead of GPU rendering"".  <a href=""http://support2.microsoft.com/Library/Images/2685831.png"" rel=""nofollow"">It's pictured here</a></p>

<p>Then, after obligatory re-start (of IE), all the grays were black and colors bright in IE-11. 
Now, how I communicate this knowledge to IE users viewing my washed-out videos is a tough new question.</p>
","12801"
"How to get smooth motion when changing clip speed?","3401","","<p>Let's say I have a 30fps video created by a camera panning at a constant speed while filming some static scenery.</p>

<p>If I watch the video, the motion in the video is perfectly smooth.
If I double the speed, the motion continues to be perfectly smooth.
If I half the speed, the video looks choppy but again the motion continues to look smooth.</p>

<p>The problem comes when I try to change the speed by irregular numbers.
Let's say I want to change the speed of the video to 120%. I then export the video at 30 FPS. The motion in the video then becomes visually choppy, jerky.</p>

<p>I quickly figured out what is happening: by increasing the speed, my 30 FPS video becomes 30*1.2=36FPS. But I am exporting at the standard 30FPS so every 6th frame is dropped. This means that every 5 frames the camera moves at double speed. If I look at the motion frame by frame the camera movement is: slow, slow, slow, slow, FAST, slow, slow, slow, slow, FAST, slow, slow, slow, slow, FAST, etc.</p>

<p>So finally my question is what option do I have to get smooth motion or hide this problem when changing the the speed of the video by small intervals?</p>

<p>Shooting at a higher shutter speed is not an option in my opinion, as it just blurs the motion until the problem is not visible.</p>

<p>I use Adobe Premiere and Adobe After Effects.
Will the Time Warp plugin in After Effects be able to create smooth motion by using motion vectors? What if there is active movement in the scene? will it still look good?</p>

<p>I have yet to see people truly understand this problem. How is this effect called? Does anyone have any link or article explaining what I described?</p>
","<blockquote>
  <p>Shooting at a higher shutter speed is not an option in my opinion, as it just blurs the motion until the problem is not visible.</p>
</blockquote>

<p>I don't get that part. Isn't hiding the problem until it is no longer visible what you want?<br>
Also, a higher shutter speed results in less blurred frames, not the other way around. </p>

<blockquote>
  <p>I quickly figured out what is happening: by increasing the speed, my 30 FPS video becomes 30*1.2=36FPS. But I am exporting at the standard 30FPS so every 6th frame is dropped.</p>
</blockquote>

<p>This is the default behaviour of the Media Encoder. Adobe calls it Frame Sampling. As you said, since frames will be dropped at regular intervals, it will produce curious results.</p>

<blockquote>
  <p>So finally my question is what option do I have to get smooth motion or hide this problem when changing the the speed of the video by small intervals?</p>
</blockquote>

<p>Use one of the other Time Interpolation modes Premiere Pro has to offer. At the bottom of the Export Settings dialog, there's a dropdown menu containing three options: Frame Sampling, Frame Blending and Optical Flow. You will find the same dropdown menu in the Speed/Duration panel that can be accessed via the context-menu to change the interpolation mode of individual clips. </p>

<p>From the <a href=""https://helpx.adobe.com/premiere-pro/using/duration-speed.html"" rel=""nofollow noreferrer"">documentation</a>: </p>

<blockquote>
  <p>The Optical Flow feature in Premiere Pro uses frame analyses and pixel motion estimation to create brand new video frames, resulting in significantly smoother speed changes, time-remapping, and frame-rate conversion. [...]</p>
  
  <p>Frame Sampling repeats or removes frames as needed to reach the desired speed. Frame Blending repeats frames, and it also blends between them as needed to help smooth out the motion. </p>
</blockquote>

<p><a href=""https://blogs.adobe.com/creativecloud/optical-flow-time-remapping-tips-tricks-for-best-results/"" rel=""nofollow noreferrer"">Here's another helpful article on using the Optical Flow interpolation.</a> </p>

<p>I suggest you try all the interpolation modes and see what works best for your footage.</p>

<p>One more thing, you probably don't want to hear that, but 30 fps video is really bad for doing slow or fast motion effects (except maybe timelapses with 200% speed or more). As you explained yourself, there's not that many frames to go on so some frames will need to be dropped, synthesised or blended in some way which looks subpar at best. The interpolation algorithms do a decent job, but it will never look as good as it would with footage that was shot at a higher frame rate in the first place. </p>
","20285"
"How do I batch replace video clips in Premiere to graded copies? (from Da Vinci Resolve)","3397","","<p>My color grader and I are working in parallel.  I'm editing in Adobe Premiere, and he is grading in Da Vinci Resolve (version CC 2015.2 and 12, respectively).  I gave him an early rough cut of the film to start grading.</p>

<p>A lot has changed in the cutting of the film since I exported him the rough cut XML of the project. The typical round-trip workflow I find online is to export XML projects back and forth between the NLE and Resolve, but that doesn't work for us because the Premiere project has changed so much and will continue to change.</p>

<p>Instead, we're trying to have Resolve render graded replacement video files for all of the source clips used, and tell Premiere to point to a different source clip.  The following technique seems to work, but has to be done clip by clip: ""Project Pane > Right click > Replace Footage"".</p>

<p>Is there an easier way to do this transition in Premiere and Da Vinci Resolve?  Or a better workflow in general for a colorist to work in parallel with an editor?</p>
","<p><strong>Method 1</strong>
First, with the Premiere project closed, move the original files that you're editing with to a new folder or other location. Open your project and it will warn you of missing files. Click the <strong>Relink others automatically</strong> checkbox and then hit <strong>Locate</strong>.</p>

<p><a href=""https://i.stack.imgur.com/m1rvw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m1rvw.png"" alt=""enter image description here""></a></p>

<p>Navigate to the folder with the new graded footage, find the first clip and press Ok. It will then re-link all the other files to the matching files in new folder.</p>

<p><strong>Note</strong> that this will only work if the files <strong>have the same name</strong>. Different extensions are ok, from my experience it's smart enough to deal with it if your original footage was an MXF and you're replacing it with an AVI for example.</p>

<p><strong>Method 2</strong>
In your project window, or in your timeline, select all the files you want to replace, right-click and choose <strong>Make Offline</strong>. Generally you want to choose ""leave the media files on disk"" in the dialogue that opens, because it permanently deletes the file if you don't.</p>

<p>Once your clips are offline then you can re-link. With all the files still selected, right-click and choose ""Link Media"". This will open up the same dialogue as method 1, so as before you choose <strong>Relink others automatically</strong> and navigate to your graded footage.</p>

<p>Again, this only works if the graded clips are <strong>named the same</strong> as the originals.</p>

<p><strong>Method 3</strong> 
With the project closed, move the original clips from where they are on your hard drive, and replace them with the graded clips. When you open the project it will link to the new clips, again, provided that they are named the same. You will need to re-render any rendered clips in your timeline.</p>

<p>Another note is that if your footage contained audio you need the graded footage to contain audio, even if you're not using it. Premiere won't accept them otherwise. So in Resolve, make sure you check the ""Render Audio"" box.</p>

<p>I use this workflow when I'm working with BlackMagic Cinema Camera footage. DaVinci Resolve gives me much better results than the Adobe grading tools, either in Premiere or Speedgrade. The only thing that does nearly as good a job is using Adobe Camera Raw by opening the footage in After Effects and rendering out intermediates, but that is extremely tedious and dog-slow.</p>
","18293"
"What ffmpeg/avconv format and container will Adobe Premiere Pro accept, out-of-box?","3361","","<p>All of these converted, but Premiere Pro only loads the audio.  I use Ctrl-I to import the videos, and they come up as waveforms, and can only be placed on audio tracks.</p>

<p>These examples use avconv, but I also have ffmpeg, and don't really care which as long as it works  :-)</p>

<pre><code>avconv -i v1raw.AVI  -c:a copy -c:v mpeg2video  v1conv.mpeg
avconv -i v1raw.AVI  -c:a copy -c:v mpeg2video  v1conv.avi
avconv -i v1raw.AVI  -c:a copy -c:v mpeg4  v1conv.m4v
avconv -i v1conv.avi  -c:a copy -c:v copy  v1conv_copy.mpeg
</code></pre>

<p>Here's the data for my input file (incase it helps).</p>

<pre><code>Input #0, avi, from 'v1raw.AVI':
  Duration: 00:05:28.53, start: 0.000000, bitrate: 9592 kb/s
    Stream #0.0: Video: h264 (Main), yuv420p, 1920x1080, 30 fps, 30 tbr, 30 tbn, 60 tbc
    Stream #0.1: Audio: adpcm_ms, 44100 Hz, 1 channels, s16, 176 kb/s
</code></pre>

<p>And my available video encoding formats from avconv.</p>

<pre><code>a64multi, a64multi5, asv1, asv2, bmp, cljr, dnxhd, dpx, 
dvvideo, ffv1, ffvhuff, flashsv, flv, gif, h261, h263, h263p, 
huffyuv, jpegls, libdirac, libschroedinger, libtheora, libvpx, 
libx264, libxvid, ljpeg, mjpeg, mpeg1video, mpeg2video, 
mpeg4, msmpeg4, msmpeg4v2, pam, pbm, pcx, pgm, pgmyuv, png, 
ppm, qtrle, rawvideo, roqvideo, rv10, rv20, sgi, snow, svq1
</code></pre>

<p><sup><a href=""http://pastebin.com/raw.php?i=z4qsWpdh"" rel=""nofollow"">see full codec info</a></sup></p>

<p>I'd be happy to post anything else that has the slightest chance of helping.</p>
","<p>Premiere Pro CS5 should be able to import H.264 video, IIRC, however it may be having trouble with the audio and/or container format. With ffmpeg try re-muxing the video stream without the audio:</p>

<pre><code>ffmpeg -i input -an -codec:v copy output.mp4 -an -codec:v copy output.avi
</code></pre>

<p>If both <code>output.mp4</code> and <code>output.avi</code> work then we know that the issue lies with the audio (although H.264 with b-frames in AVI probably isn't a great idea). You can re-encode the audio and keep the video:</p>

<pre><code>ffmpeg -i input -codec:v copy -codec:a pcm_s16le output.avi
</code></pre>

<p>If the first command does not work with Premiere, then you may need to re-encode. This is a good use case for an editor friendly, lossless intermediate file:</p>

<pre><code>ffmpeg -i input -codec:v utvideo -codec:a pcm_s16le output.avi
</code></pre>

<p>Close Premiere, install <a href=""http://umezawa.dyndns.info/archive/utvideo/"" rel=""nofollow"">Ut Video</a> (scroll to the bottom of the page), open Premiere, and import your video. You can do your editing, and then export to your final format using the crappy Adobe Media Encoder, or export to another lossless file, and then use ffmpeg to provide whatever you need. Alternatively, if you're using Windows you can use <a href=""http://www.debugmode.com/frameserver/"" rel=""nofollow"">Debugmode FrameServer</a> and encode directly with ffmpeg from Premiere. See <a href=""https://ffmpeg.org/trac/ffmpeg/wiki/FFmpegPremierePro"" rel=""nofollow"">How to encode with ffmpeg from Adobe Premiere Pro</a> for instructions.</p>

<p>If you need a recent ffmpeg for Windows see <a href=""http://ffmpeg.zeranoe.com/builds/"" rel=""nofollow"">Zeranoe FFmpeg builds</a>.</p>
","5425"
"How to fit 720p video on a 1080p container?","3356","","<p>I am using Adobe Premier Pro to edit our short film and give some generic editing to it. </p>

<p>At the time of shoot we wanted a slow motion effect in a shot, so we shot at 720p at 60fps <em>(We had limitations, our camera doesn't supported 60fps at 1080p)</em>. </p>

<p>Now when post processing all the videos fit in the 1080 container but our slow motion video shows black blank space around them. How to overcome this?</p>
","<p>You can make a clip scale to the resolution of the sequence by right-clicking on it and pressing <strong>Set to frame size</strong> (For older versions of Premiere, use <strong>Scale to frame size</strong>). To set this at the standard behaviour for new clips that aren't the same resolution as the sequence, go to <strong>Preferences</strong>  <strong>General</strong> and check <strong>Default scale to frame size</strong>.</p>

<p>However, keep in mind that upscaling a 720p video to 1080p won't yield you any more quality, worse case, it will look worse. Also, exporting to 1080p when your video is actually only 720p is kind of tricking the viewer. If you upload an upscaled video to Youtube and flag it as 'Full HD', I will so downvote your video ... So if your source material is shot in 720p, I would highly recommend you export to 720p.</p>
","16250"
"AE Error : Imager Buffers exceed internal limits. Decrease the memory requirements for the rendering of this frame. (12804)","3339","","<p><br />
I'm kinda new to AE and having a weird issue while trying to render a 1920x1080 Composition.</p>

<p>After rendering about 10seconds, i get the following error:
<img src=""https://i.stack.imgur.com/DijTN.png"" alt=""enter image description here""></p>

<p>I've checked all of my objects and i have no composition with the large dimensions mentioned in the error. <br /><br /></p>

<p>Did anyone ever bump into this error ? It would greatly help me to hear from the more experienced one.</p>

<p>Cheers,<br />
Shai.</p>
","<p>It would be good to know what OS your machine is running and if it's 32 or 64 bit, and the resources as in type of processor (Dual Core, i3, i7?) and how much DRAM is on the motherboard. I am not an After Effects user (yet) but this error appears to be a resource error, which is common to many other video editors.</p>

<p>It would also be helpful to have you describe what you are trying to render in AE. For instance how long is the piece, how many video tracks, what effects are used, etc.</p>

<p>Without knowing your computer's resources is not a show stopper, here are some items to check.</p>

<p>Are you running anything else when AE is trying to render, if so try your render again with nothing running but AE.</p>

<p>If you have a 32 bit machine you may have a 2GB limit on the RAM, which is going to be an issue. Have you tried this on a 64 bit machine with more memory?</p>

<p>What resources do you have AE using besides the requested render? For instance do you have the preview screen on 'auto' size mode, you might try to make the preview smaller and in a 1/4 or 1/2 mode so AE doesn't have to figure out each display frame size thereby freeing up some cycles.</p>
","3234"
"vary the color of repeated shape in After Effects","3335","","<p>I have a shape, in a shape layer, let's say a triangle. I apply a repeater to the shape, and I get lots of them. I can change the distance between them and their rotation ...<br>
My question is: Is there a way to change their color? I mean to control what fill or stroke color each of the cloned ones have? either a random color or even specified color for each?  </p>
","<p>I don't think it's possible with repeater. You could do this with expressions and multiple duplicates of the shape however.</p>

<p>Here's an example I came up with:</p>

<p><img src=""https://i.stack.imgur.com/wilKM.png"" alt=""enter image description here""></p>

<p>And here's how I did it:</p>

<ul>
<li><p>Firstly make your initial shape. Now duplicate that shape (with the shape selected hit<br>
<kbd>ctrl / </kbd>+<kbd>D</kbd>). You should see this:</p>

<p><img src=""https://i.stack.imgur.com/spxW8.png"" alt=""enter image description here""></p>

<p>Note that we'll be making as many copies as we want later, but for the moment only make one, and get it set up so that we can duplicate it and not have to apply expressions to dozens of copies individually.</p></li>
<li><p>For each of the properties you want to alter with every repetition we'll add an <strong>Expression Control</strong> effect, as a handy way to be able to control the whole thing from a single slider.<br>
Say we wanted to alter the position, scale and rotation: we'd add a <strong>Point Control</strong> effect to control position another for scale, and an <strong>Angle Control</strong> for rotation. Expression controls are in <strong>Effects>Expression Controls></strong><br>
<img src=""https://i.stack.imgur.com/7pCNy.png"" alt=""effect controls""></p></li>
<li><p>For ease of use rename each effect suitably. I've changed <em>""Point Control""</em> to <em>""offset""</em> and <em>""scale ctrl""</em>, and changed <em>""Angle Control""</em> to <em>""rotation""</em>:<br>
<img src=""https://i.stack.imgur.com/XxZ2H.png"" alt=""enter image description here""></p></li>
<li><p>We're going to add expressions to control the <strong>transform>Position</strong> property, the <strong>Transform>Rotation</strong> property and the <strong>Transform>Scale</strong> property of each shape object. Note that this is the property inside the shape, not the Transform properties for the layer.<br>
<img src=""https://i.stack.imgur.com/me1yb.png"" alt=""enter image description here""></p></li>
<li><p>In order to increment the rotation and scale etc. for each duplicate of the Polystar shape we have to find its index, or its serial number so to speak. With layers this is relatively easy, each layer has an <strong>index</strong> property, but shapes within a layer are layer sub-objects so we have to use a bit of magic to get the index of each. <a href=""https://forums.creativecow.net/thread/227/25458"" rel=""nofollow noreferrer"">This thread</a> is where I got the info I needed to work it out.</p>

<p>the key bit of the expression you need is this:</p>

<p><code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code></p>

<p>The variable shapeIndex now gives you a unique index for each duplicate that you create. This index can then drive the value of the various properties.</p></li>
</ul>

<blockquote>
  <p><em>Skip this section if you're not interested in how it works</em>  </p>
  
  <p>The expression <code>propertyGroup(2)</code> returns the string <code>""Polystar 2""</code> and we
  use javascript string function <code>split()</code> to split it on the space
  which returns an array <code>[""Polystar"", ""2""]</code>.<br>
  We select the second member
  of that array (or member[1], since javascript numbers arrays starting
  at 0) which returns <code>""1""</code>. If javascript wasn't so sloppy with types we'd have to
  explicitly convert the string <code>""1""</code> into the integer <code>1</code>, but it does the
  conversion without even being told. Yay Javascript!</p>
</blockquote>

<ul>
<li><p>So to add an offset to each duplicated shape layer we then use the shapeIndex variable and the expression control. </p></li>
<li><p>Firstly we'll add an expression to the shape's <strong>Transform>Position</strong> property. <kbd>alt</kbd>-click on the stopwatch next to the property to set an expression for that property. It will look like this:<br>
<img src=""https://i.stack.imgur.com/2EEKH.png"" alt=""enter image description here""></p></li>
<li><p>Delete what is there by default and paste in the shapeIndex variable definition mentioned above:<br>
<code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code><br>
hit <kbd>return</kbd> (not <kbd>enter</kbd>) to start a new line</p></li>
<li><p>Now use the pickwhip to get the value of the Expression Control Effect that we created for the offset:<br>
<img src=""https://i.stack.imgur.com/MvTmH.png"" alt=""don&#39;t spare the pickwhip""></p></li>
<li><p>Now we multiply that offset by the shapeIndex variable. The expression should now look like this:  </p>

<p><code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code><br>
<code>effect(""offset"")(""Point"")*shapeIndex</code></p>

<p>So if your offset is [20, 20], then the fist duplicate will have an offset of [20,20], the next [20,20]*2 which is [40, 40], then [60, 60] and so on. You'll be able to change the value of this point control and have the duplicates update as you do it, without having to type any more code.</p></li>
<li><p>The same applies to rotation:</p>

<p><code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code><br>
<code>effect(""rotation"")(""Angle"")*shapeIndex;</code></p></li>
<li><p>And scale. Scale is a bit different because we want to increment the difference between 100 and the value of the controll - if the control is 110% we want to add 10% to the scale. And once we start adding vectors and scalars it gets more complicated - we have to unpack the array, and the integer and pack it up again. So we use this expression:</p>

<p><code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code><br>
<code>[100+ (effect(""scale ctrl"")(""Point"")[0]-100)*shapeIndex, 100+ (effect(""scale ctrl"")(""Point"")[1]-100)*shapeIndex];</code></p>

<p>If we only wanted uniform scale we could use a slider Expression Control effect, and the the same sort of expression we use for rotation:</p>

<p><code>shapeIndex=thisProperty.propertyGroup(2).name.split("" "")[1]-1;</code><br>
<code>(effect(""scale ctrl"")(""Slider"")/100)*shapeIndex;</code></p></li>
<li><p>Lastly we want to control the colour. Colour in AE is a four-element array [R, G, B, Alpha]. The individual components of the array are decimals from 0 to 1, where 1 is full on and 0 is off. So red is [1,0,0], yellow is [1,1,0] and mid grey is [0.5, 0.5, 0.5]. What you want to do with colour is up to you, but here's a demo. It fades out from red to green, with random variation on the blue channel. </p>

<p>Note that the shapeIndex definition in this expression is different. That's because the <code>color</code> property is nested down a level inside the <code>fill</code> property, so we have to use <code>propertyGroup(3)</code> instead of <code>propertyGroup(2)</code>.</p>

<p><code>shapeIndex=thisProperty.propertyGroup(3).name.split("" "")[1]-1;</code><br>
<code>seedRandom(shapeIndex, timeless = true);</code><br>
<code>r=value[0]-shapeIndex/25.6;</code><br>
<code>g=value[1]+shapeIndex/25.6;</code><br>
<code>b=random()</code><br>
<code>[r , g, b, 255]</code></p>

<p>The <code>seedRandom</code> function creates a new, unchanging random number for each copy. If the <code>timeless=true</code> part is changed to <code>timeless=false</code> the random number will change with every frame. Another randomising function is the <code>wiggle</code> function, that softly changes values over time.</p></li>
<li><p>So now your shape layer should have two shape objects in it, and the second one will have expressions like this:<br>
<img src=""https://i.stack.imgur.com/cB29y.png"" alt=""enter image description here""></p>

<p><strong>Here's where the magic happens</strong></p></li>
<li>Now duplicate that second shape. If all goes well you should see new copies with offset, rotation and scale controlled by the expression control effects (make sure you have sane amounts for these values - the offset for example will default to half the width of the comp, which will move your duplicates out of sight. Try setting it to something like 50, 50 to start with). It should look something like the image at the start. Below is how the layer looks. 
<img src=""https://i.stack.imgur.com/0H7ID.png"" alt=""enter image description here""></li>
</ul>

<p>One of the cool things about doing it this way is that the expression is actually based on the name of the shape object. So the layer order doesn't matter. As you can see here, the copies are in front of the original, I can move them around so that they're behind, or individually move them up or down. Using the name is a little bit kludgy though. If I rename one of the shapes I might break all the expressions that control it.</p>

<p>If that's all too hard to read, <a href=""https://onedrive.live.com/redir?resid=5408802B7CF7224B!47711&amp;authkey=!ACkuHMqDVQ6SPYA&amp;ithint=file%2Czip"" rel=""nofollow noreferrer"">here's the project</a></p>
","15260"
"Premiere Elements ""Debug Event"": Keyframe error","3298","","<p>I have encountered a probable bug in Premiere Elements 10, and fortunately I've also found a workaround which is why I wrote my own answer below. But since I haven't been able to solve the underlying problem, I'd be very glad for additional ideas on how to fix the problem. Now here's the question:</p>

<p>How can I salvage a Premiere Elements project file (.prel) that can't be opened because of the following error:</p>

<pre><code>Premiere Elements Debug Event
Premiere Elements has encountered an error:
[d:\pre\mediacore\mediafoundation\api\inc\Keyframe/Keyframe.h-142]
</code></pre>

<p>This error appears to occur sporadically on non-US versions of Windows. There is no previous indication during the editing session that anything may be wrong (saves and auto-saves work fine), but on the next restart, the project can't be opened. Other projects appear to be unaffected. </p>

<p>According to other users who have seen the same error (Google for ""premiere debug keyframe"" and you'll find some), there might be some connection with the introduction of DVD menus into the project, but I'm just speculating here.</p>
","<p>Analyzing the files that fail to open in contrast to those that do open just fine shows one glaring problem: All lines in the <code>project.prel</code> file (which is just an XML file) where there should be decimal numbers in <code>&lt;StartKeyframe&gt;</code> or <code>&lt;Keyframe&gt;</code> tags are damaged. Example: </p>

<p>Correct version:</p>

<pre><code>&lt;StartKeyframe&gt;-91445760000000000,100.,0,0,0.,0.,0.,0.&lt;/StartKeyframe&gt;
</code></pre>

<p>Defective version:</p>

<pre><code>&lt;StartKeyframe&gt;-91445760000000000,100,,0,0,0,,0,,0,,0,&lt;/StartKeyframe&gt;
</code></pre>

<p>So apparently Premiere Elements at some point got confused by European locale settings and decided to save decimal values using the European decimal separator (comma) instead of the US decimal separator (dot). Of course it's then impossible to open that file correctly ever again.</p>

<p>Fortunately, at least in my tests, there are only three kinds of decimal values in <code>.prel</code> files:</p>

<ol>
<li>integer floats: <code>1.</code>, <code>0.</code>, <code>100.</code></li>
<li>the single value <code>0.5</code> </li>
<li>double precision floats: <code>0.16666666666666666</code>, <code>1.000000000000000000000000</code></li>
</ol>

<p>This makes it possible to fix the file using a regular expression replace. If your editor supports Perl-style regular expressions, you can search for</p>

<pre><code>,(?:(?=[,&lt;]|\d{10})|(?&lt;=\b0,)(?=5\b))
</code></pre>

<p>and replace all with</p>

<pre><code>.
</code></pre>

<p>This will replace all erroneous decimal commas (i. e. those that are either followed by another comma, an opening angle bracket, a decimal fraction of at least 10 digits, or the number 5 (but only if that comma is preceded by the single digit 0)) with a decimal point.</p>

<p>After that (be sure to back up your <code>.prel</code> file before you do this!), the file can be opened again correctly. However, as soon as it's saved again, the error is re-introduced.</p>

<p>Therefore, if someone has an idea how to actually fix this problem instead of patching it up with an admittedly wonky regex, I'd be most grateful.</p>

<hr>

<p>One hint as to what the underlying problem might be: Around the time the error was introduced, Premiere spontaneously opened a little dialog window that said (in German) something like</p>

<pre><code>""Premiere Elements is updating a component. Information is being transmitted...""
</code></pre>

<p>While this window was displayed, two Win7 UAC boxes popped up, asking me to allow the Windows Regserver to update my system. The actual commands it needed approval for were</p>

<pre><code>C:\Windows\System32\regsvr32.exe /s 
   ""C:\Program Files\Adobe\Adobe Premiere Elements 10\mc_dec_mpa_ds.ax"" 
   ""C:\Program Files\Adobe\Adobe Premiere Elements 10\mc_dec_mp2v_ds.ax""
</code></pre>

<p>and</p>

<pre><code>C:\Windows\SysWOW64\regsvr32.exe /s 
   ""C:\Program Files\Adobe\Adobe Premiere Elements 10\32\mc_dec_mpa_ds.ax"" 
   ""C:\Program Files\Adobe\Adobe Premiere Elements 10\32\mc_dec_mp2v_ds.ax""
</code></pre>

<p>(line breaks inserted for clarity). </p>

<p>The first time around, I allowed these actions (which in retrospect may have been a bad idea...); now this happens every time I'm trying to open a ""patched"" project file. It doesn't matter whether I now allow or refuse it - either way, Premiere writes a corrupted project file when I save my project.</p>
","4026"
"Do any iOS video apps record in log color space?","3278","","<p>Is there an app for iPhone or iPad that will let me record log color space video? It seems like with such an abundance of instagram-ish ""filter"" apps, developers probably have access to raw sensor data. Are there any apps designed to record it in a flat color profile for postproduction grading?</p>
","<p><a href=""https://9to5mac.com/2017/01/11/filmic-pro-log-profile-iphone-7-plus-color-graded/"" rel=""nofollow noreferrer"">This announcement article dated January 11, 2017</a> indicates that there will be an upgrade to <a href=""https://itunes.apple.com/us/app/filmic-pro/id436577167"" rel=""nofollow noreferrer"">the app FiLMiC Pro</a> which may accommodate what you are looking for. Note that the FiLMic Pro App Store web page indicates the latest release is 12/23/2016 - I do not know when the update the article speaks of will be available:</p>

<blockquote>
  <p>The forthcoming update will introduce a new log mode that allows
  filmmakers to shoot videos with a flat picture profile containing
  additional stops of dynamic range.</p>
</blockquote>

<p>Also check out <a href=""http://www.filmicpro.com/"" rel=""nofollow noreferrer"">FiLMic's website</a> - I couldn't find any details, but I was interested to read that they develop it with <a href=""https://developer.apple.com/swift/"" rel=""nofollow noreferrer"">Apple's Swift language</a>.</p>

<p>Of note, they have apparently released a beta version for testing and demo of the LOG capture. Compare in particular the dark area of the dashboard in the lower left hand corner in this example of <a href=""https://youtu.be/BnLkdxYCtRU"" rel=""nofollow noreferrer"">source video</a> vs. the same <a href=""https://youtu.be/et8-jtZauXM"" rel=""nofollow noreferrer"">footage after grading</a> with BlackMagic Design's <a href=""https://www.blackmagicdesign.com/products/davinciresolve"" rel=""nofollow noreferrer"">DaVinci Resolve</a>. Disclaimer - these videos are from the article and the quality of the color grading vs youtube compression and such is relative. Also, as an example of the extra detail shooting this way may afford, note in the third shot, the shadow detail of the hat before and after.</p>

<p>Likely you don't need this information, but here is <a href=""http://www.newsshooter.com/2015/07/27/looks-picture-profiles-luts-and-log-why-when-and-how-you-should-use-them/"" rel=""nofollow noreferrer"">a recent article which provides a decent overview</a> of what it means to shoot in log color space , or, record a ""flat color profile."" Also, <a href=""https://video.stackexchange.com/q/12199/17685"">some related VideoSE Q&amp;A</a> about shooting considerations.</p>
","20651"
"Encoding DNxHD using ffmbc","3264","","<p>I have decided to re-encode my footage as DNxHD for editing. I am using ffmbc (<a href=""http://www.videohelp.com/tools/ffmbc/old-versions"" rel=""nofollow"">downloaded here</a>) to do the encoding. My source footage is HDV 1080i50.</p>

<p>Therefore I think the best format to trancode into would be DNxHD 1080i50. According to <a href=""http://itbroadcastanddigitalcinema.com/ffmpeg_howto.html#Encoding_VC-3"" rel=""nofollow"">this document</a> ffmpeg (which ffmbc is derived from) has a limited selection of formats of which 1080i50 is one.</p>

<p>Here is the command I am using (%1 and %2 are the in and out filenames):</p>

<pre><code>ffmbc -i %1 -s 1920x1080 -vcodec dnxhd -b 185M -mbd rd -tff -acodec copy -threads 0 %2
</code></pre>

<p>Questions:</p>

<ol>
<li><p>To encode interlaced media I need to specify -tff or -bff. I don't really understand the difference. Which would be the best to choose? Currently I have chosen tff as that is how the source is encoded.</p></li>
<li><p>Is there any benefit to doing a two pass encode? My understanding of a two pass encode is that it helps keep the bit-rate constant. Is that helpful for editing. I can do a two pass encode with the following command, but it's so slow I'm wondering if it's necessary?</p>

<p>ffmbc -i %1 -s 1920x1080 -vcodec dnxhd -b 185M -mbd rd -tff -an -f rawvideo -an -threads 0 -y NUL
ffmbc -i %1 -s 1920x1080 -vcodec dnxhd -b 185M -mbd rd -tff -acodec copy -threads 0 %2</p></li>
<li><p>My source is actually encoded as 1440x1080i. Can any one comment on the drop in quality when converting to 1920x1080i. (I believe that the image will still be the same size but the source has wider pixels)</p></li>
</ol>
","<p>1) If you're not going to deinterlace it then stick to the source's field order.</p>

<p>2) I wouldn't bother with the two pass encoding (if indeed it even does anything) - dnxhd will only encode at certain specific fixed bit rates anyway so you're actually pretty constrained for options.</p>

<p>3) You'll see some softening, but it's unavoidable.  ffmbc's filtering is ok though.</p>

<p>What software are you editing in?</p>

<p>And the 185M bitrate might be overkill if your source is HDV.  Try 120M and save yourself a bit of space.</p>
","2938"
"Cross-dissolve in real time between end and start point of looping clip","3248","","<p>I'm trying to achieve the following: I have a short movie clip that I want to loop for an indefinite amount of time. However, as of now, it doesn't particularly good every time the clip starts over (it just sort of jitters back to the starting point without any form of smooth transition), so I would need a cross-dissolve effect to be applied to it in realtime.</p>

<p>Is there any (preferably free) software solution that could help me accomplish what I want to do here? Preferably, I'd like some kind of free solution that works both across Windows and OS X, but tips relating to the platforms individually are appreciated as well. I don't care whether I need some stand alone software to achieve this (as opposed to using some kind of plug-in for a program that I'm already using, like VLC).</p>
","<p>You can't seamlessly loop a clip that wasn't designed to loop. Some suggestions below, but all require video editing software. No plugin for a player that I know of.</p>

<h3>Option 1: Clip has a distinctive/necessary beginning &amp; end that have to be seen, or has audio/narration that cannot be edited or shortened</h3>

<p>The best option in this case would be to edit the clip to fade-in from and fade-out to black (and/or an identical image) at the beginning &amp; end. </p>

<p>Now when it loops it at least appears the same (even if the player hiccups slightly). Black works well since most players go to black after finishing a clip.  </p>

<h3>Option 2: No audio, the clip's start/end location doesn't matter &amp; you want a seamless loop</h3>

<p>In this case you could try the following, using whatever software - iMovie, Premiere, After Effects, MovieMaker etc...</p>

<ul>
<li>Place the entire clip on a timeline.</li>
<li>Duplicate the clip.</li>
<li>Find a point in the clip that has as little motion/change as possible.</li>
<li>On the top copy of the clip, move 1 frame to the <em>left</em> and trim everything <em>after</em> this point.</li>
<li>On the bottom copy, go back to the chosen frame and trim everything <em>before</em> this point.</li>
<li>Move the <em>bottom</em> copy left so it starts at the beginning of the timeline 
<ul>
<li>Your chosen point now starts at 0:00:00 </li>
</ul></li>
<li>Shift the top copy to the right until it only overlaps slightly with the bottom copy.
<ul>
<li>The original start/end points are now overlapping somewhere in the middle</li>
</ul></li>
<li>Add a cross-dissolve to the beginning of the top copy
<ul>
<li>The original end point will be obscured by the dissolve. </li>
<li>Experiment w/the length and location of overlap/dissolve to find a suitable result. It could work w/just a few frames, or might require a longer overlap to not look jumpy. How well it works depends on how different the original start/end frames are. </li>
<li>Ensure the dissolve only occurs during overlapping frames or else you'll see clips jumping on &amp; off screen. </li>
</ul></li>
<li>Trim the end of the timeline so it stops exactly at the last frame of the top clip.<br>
<ul>
<li>Your edited video will be shorter by the length of the dissolve.</li>
<li>The new last frame is now exactly 1 frame <em>before</em> the new first frame at the beginning. </li>
</ul></li>
<li>Export timeline to your required format  </li>
</ul>

<p>The result of this process is that the beginning and end now only  differ by a single frame, making a loop with minimal jump when you restart playback. The cross-dissolve in the middle takes care of the smooth transition that you don't currently have, at the expense of ~1 second of original length.</p>

<p>You could also flip the directions around so the top clip shows the original ""end"" point clearly &amp; fades to the ""start"" of the bottom clip. Either way works...  </p>

<p>There still may be a slight stutter no matter what, depending on the delivery platform. VLC is decent about looping, but iTunes and WMP have a noticeable gap even in a clip that should be seamless. If your clip is for DVD there might be a slight pause, but visually it would look the same.</p>

<p>If the video is for web or mobile use, it'll depend on the platform. iOS and Android are horrible about looping videos natively (there's always a noticeable gap while it restarts, if looping is allowed at all). With HTML5 video or Flash you could code it to seek to the beginning immediately before it reaches the end, hopefully preventing much stutter and avoiding the need to go to all this editing hassle...</p>

<p>I hope this helps!  </p>
","14975"
"Burn a single HD video into a DVD without losing quality","3241","","<p>I created a video (slideshow of 14.2MP photos and music) using Windows Live Movie Maker. The video's resolution is 1440x1080 and is as WMV with 840MB.</p>

<p>Now what I want is to <strong>burn that HD video (no menus, no other videos) into a DVD so it can be playable on any DVD player and watchable on a TV</strong>. Is there any way to burn it without losing quality? I tried <em>Nero</em>, <em>Windows Live Movie Maker burn dvd option</em> and <em>Wondershare DVD Creator</em>. The video quality got worse in all of them (Nero's being the worse and Wondershare's the better) with loss of sharpness, sometimes minor jpeg artifacts and some color changes.</p>

<p>I believe there must be a way to do it because:</p>

<ul>
<li>the DVD still has 3.80GB free, so why compressing it?</li>
<li>the movie DVD's image quality is great so what are they using to do it?</li>
</ul>

<p>Is there any software to burn the video without losing the original quality? Should I compress the video to 720x480 using specialized software /which one?) before burning it?</p>
","<p>The short answer is ""No.""  DVD by definition is limited to 720x480 video with a fairly low bitrate, compressed specifically with MPEG-2.  There is absolutely no way to make your HD video look just as good with those constraints.</p>

<p>Some DVD players do let you play .mp4, .mkv, or whatever files that happen to be stored on a DVD, but that isn't a standard function of DVD players in general.</p>
","4506"
"container format requires global headers","3222","","<p>Using this FFmpeg:</p>

<pre><code>ffmpeg-20160127-git-9079e99
</code></pre>

<p>with <a href=""http://jplayer.org/audio/m4a/Miaow-02-Hidden.m4a"" rel=""nofollow noreferrer"">this file</a>, if I run this command:</p>

<pre><code>ffmpeg -i Miaow-02-Hidden.m4a -c copy outfile.m4a
</code></pre>

<p>I get this message:</p>

<pre><code>[ipod @ 0000000000694500] Codec for stream 0 does not use global headers but
container format requires global headers
</code></pre>

<p>How can I resolve this warning?</p>
","<p>This issue was fixed with <a href=""//github.com/FFmpeg/FFmpeg/commit/955b818"" rel=""nofollow noreferrer"">commit 955b818</a>:</p>

<pre><code>commit 955b818cf947473ec94a3fe8aa7f408b119fbbc9
Author: Clment Bsch &lt;clement@stupeflix.com&gt;
Date:   Thu Sep 1 16:48:45 2016 +0200

    ffmpeg: switch to codecpar
</code></pre>

<p>You can suppress the warning on older version like so:</p>

<pre>
ffmpeg -i Miaow-02-Hidden.m4a -c copy <b>-flags global_header</b> outfile.m4a
</pre>

<p><a href=""//github.com/svnpenn/a/blob/db3e32f/music/music-download.sh#L67-L68"" rel=""nofollow noreferrer"">Example</a></p>
","19954"
"Video stream with delay","3156","","<p>I need to capture a video stream from a web cam and then show it non-stop with a delay of 30 seconds or more.</p>

<p>A server side linux software would be the best but a Windows GUI application will be ok as well.</p>
","<p>What you are looking for is called a <code>Broadcast Delay Processor</code>. Unfortunately these comes as a hardware modules you feed your signal through, and therefor can be relative expensive.</p>

<p>I did look around for a software based broadcast delay for you, but was not able to find any.</p>

<p>Luckily though, you can use <a href=""http://wiki.videolan.org/VLC_command-line_help"">VLC</a> (free and available on Linux and Windows) to stream with a delay. Check out this command:</p>

<blockquote>
  <p>Display stream output</p>

<pre><code>  --sout-display-delay=&lt;integer [-2147483648 .. 2147483647]&gt;
                             Delay (ms)
      Introduces a delay in the display of the stream.
</code></pre>
</blockquote>

<p>In your case use it like with this value:  <code>--sout-display-delay=30000</code></p>

<p>You can also record your webcam with VLC, see this article for details:<br />
<a href=""http://newcome.wordpress.com/2010/01/17/recording-webcam-videos-with-vlc-media-player/"">Recording webcam videos with VLC Media Player</a></p>

<p>In other words, using VLC with the provided commands you should be able to both record your webcam and stream it with a delay (at no cost).</p>
","5149"
"Why not filming computer or TV screens?","3149","","<p>I see this all the time: a character is doing something on a computer or watching TV. Instead of filming the actual content of the screen, they film the scene with a fixed camera and add a layer with the content they want to show during post production.</p>

<p>Here is an example (from a TV series named Underemployed, Season 1 episode 1 at 18:42). <img src=""https://i.stack.imgur.com/hX76B.png"" alt=""screen changed at post production""></p>

<p>In this case, you can see that the perpective correction is wrong (the bottom right corner doesn't match the actual corner of the screen), and I'm really wondering what I'm looking at. It looks like a deformed iPad application displayed on a cheap PC, I really don't know.</p>

<p>What kind of problems occur when you film an LCD ? Is it a technical issue, or is there another problem I'm not thinking of ? Maybe a logistic problem (like.. the actor doesn't know how to use a computer, or they should synchronize the actors play with the content played on the TV..).</p>
","<p>There are several factors that comes to play with recording computer screens.</p>

<ul>
<li><p>As filzilla already mentions, sync is a major factor. In the old days CRT monitors was very challenging to film even with sync-locking. Depending on if you filmed on silver or on video you did get different results. With the exception of LED screens, LCDs also offer sync problems.</p></li>
<li><p>The other factor is light conditions. For movie scene you usually have a large amount of light which a computer screen is unable to produce - so the screen turns very dark. You also get problems with color-correction/white-balance as the monitor typically have a different color temperature than the lights used to lit the scene.</p></li>
<li><p>The third factor is of more recent nature and has to do with the nature of LCD screen and it's polarization, referring to the limited angle they are visible within. You will get ""gradient"" effects and on more poor screens also have limited visibility from the sides, top and bottom.</p></li>
<li><p>Moir can be a problem. For video this isn't such a big problem because the light coming from the screen will blend the pixels. Unless you have a high resolution camera filming in low(er) light conditions, or are filming close-up (but not so close that you can actually see the screen pixels), the camera won't be able to separate the individual pixels. For movie cameras this is more present though due the higher resolutions and the requirement to light conditions.</p></li>
<li><p>Then there is content synchronization with the actor's interactions or the scene in general, the potential of typos, wrong clicks, having to reset for each take and so forth.</p></li>
</ul>

<p>It's more easy to put in a video in post which you can color correct and adjust, synchronize and so forth to the movie.</p>

<p>The process isn't very complicated. The computer screen is often recorded with a green or blue backdrop to eliminate the need for mask/matte rotoscoping. You just track the corners of the inner part of the screen and drop in a keyed replacement for the screen.</p>

<p>There are plugins too that do a pretty decent job automating this process (or instance the Mocha AE and its planar tracking).</p>
","5525"
"Create Project Template in Adobe Premiere cs6","3131","","<p>I'm a youtuber and I make a bunch of videos. Every time I make a vid I have to import the same files [watermarks, back ground music]. Is there anyway to make a template so when I create a new file they are already there?</p>
","<p>Certainly - you can simply create a new project with the appropriate files already imported. Save that project. Whenever you need to create a new video, go to your template project and save it as a copy. This way, you retain your original template and you can bypass the process of reimporting the necessary media.</p>
","8492"
"Using a Flash to Light Video","3131","","<p>I have a Nikon D3100. It has a built in flash.  Can I use it as a light when filming videos?</p>
","<p>Generally speaking, it is not possible to use a camera flash as a light for video.  A true flash is a high intensity discharge bulb which produces a very short (1/100 second or shorter), very bright burst of light.  It is not designed to be used for constant light output and has insufficient power, durability or cooling to be able to operate as a continuous light source.</p>

<p>Some devices, most often smartphones, use LED light sources as a ""flash"" but actually are not flashes, simply constant output lamps that can be used for video or as a low brightness ""flash"" for still images.  This is very rare to see with a point and shoot or DSLR.  When it is included with DSLR flashes, it is typically as a separate, distinct light on the same unit, such as the video light built in the Canon's 320EX flash.</p>
","12513"
"Key out everything but one color?","3086","","<p>Ok I know this might be very basic for many of you, but I couldn't quite figure out how to do this ...</p>

<p>So, in case I want something like a reverse color keying where everything <em>except</em> one particular color is keyed out, what would be the easiest or, even better, most customizable way to do this in Premiere Pro?</p>

<p>I want to be able to manually set the tolerance level for similar colors as well as soft edges.</p>

<p>Thanks in advance!</p>

<p>No answers starting with ""This is much easier with After Effects ..."" please!</p>
","<p>This is quite simple.  Use any keyer of your choice that can give you a good mask for the layer, then place an ""Alpha Adjust"" layer below it in the Effects control and click the ""Invert Alpha"" checkbox.  This will invert the alpha channel produced by the keyer and reverse the effect the way you are looking for.</p>

<p><img src=""https://i.stack.imgur.com/1motb.jpg"" alt=""enter image description here""></p>
","12815"
"No sound in Adobe Premiere Pro CC 2017","3058","","<p>I searched internet alot but after all those usless advices I feel upset.. </p>

<p>I'm importing .mp4 file which has music in it, but in Premiere there is no audio track at all. </p>

<p>I have restarted pc, installed k-lite codec pack, quicktime codecs but why? Why is it so hard to simply import .mp4 file with music to the video editor..</p>
","<p>Ok, here is <a href=""https://forums.adobe.com/message/9109650#9109650"" rel=""nofollow noreferrer"">answer</a>. 
Quote: </p>

<blockquote>
  <p>Please clear the Media Cache (keeping Premiere Pro closed). Find
  instructions here: FAQ: <a href=""https://forums.adobe.com/thread/2152942"" rel=""nofollow noreferrer"">How to clean media cache files?</a></p>
  
  <p>Navigate to the Media Cache location as specified in the Premiere Pro
  preferences and rename the folders.</p>

<pre><code>   MAC: Premiere Pro menu&gt;Preferences&gt;Media
   Windows: Edit Menu&gt;Preferences&gt;Media
</code></pre>
  
  <p>Please reply if your issue gets resolved after trying these steps.</p>
  
  <p>NOTE: Premiere Pro will conform all files after opening any/all
  project(s) and will generate the peak file for the audio. This might
  take long depending upon the number of Media used in the project(s).</p>
</blockquote>
","20957"
"Windows 7 - multiple programs with conflicting sound output","3049","","<p>I have a Windows 7 (32 bit, service pack 1) on my laptop and, whenever I launch my sound editor (Ableton Live 8), all other programs (Firefox, Chrome, Skype etc) stop outputting any sounds. Even when I try switching the order of steps (opening Ableton first and then launching a browser, for example) it doesn't work.</p>

<p>In Control Panel > Sound, I have the laptop speakers as the default output and, as soon as I close Ableton, all other programs are able to make noise again.</p>
","<blockquote>
  <p>Driver type <strong>ASIO</strong> Audio device <strong>ASIO4ALL v2</strong></p>
</blockquote>

<p>Aha, there we go. ASIO is designed to give only one program at a time access to the audio hardware. As I said, this is in principle a good thing, because it ensures the DAW can run trouble-free and without overly long latency or crippled sound introduced when the OS tries to combine the audio from different programs so some composite that's then usable for even the worse hardware that can be assumed. However, for just testing out the program, learning features etc. none of this is really a concern, so you might as well use one of the non-professional alternatives. Microsoft DirectSound should always work.</p>
","5131"
"Make video look like it was shot in a dark room","3042","","<p>So I'm working on a video that I want to look a bit cooler. It was shot in a normally illuminated room (no special equipment, only normal light bulbs), it looks like this:</p>

<p><img src=""https://i.stack.imgur.com/rkqOF.jpg"" alt=""enter image description here""></p>

<p>With a person sitting in front of the laptop. There are also over-the-shoulder shots with the laptop display being shown.
So, I want to make the video a bit darker and surreal. It does not have to look like it was actually shot in the dark, it would even benefit from looking a bit unnatural and obviously edited. To be more specific, I want the person sitting in front of the laptop (as well as the laptop itself) to be clearly visible, whilst the table, the chair, and everything in the background gets darker and unobstrusive.</p>

<p>I'm using the current version of Premiere Pro. What is the best approach to get that kind of effect?</p>

<p>And please, no lecture on good preparation and such. The recording was done in a rush and the whole video is kind of a last-minute thing. It doesn't have to look perfect, I'm open for suggestions ...</p>
","<p>You are lucky that you don't have extreme lighting conditions in your shot, also you're indoors. So it should definitely be possible to get the footage a bit darker looking without making it look weird.
I can recommend working in After Effects rather than Premiere, its far more suited for advanced lighting correction.</p>

<p>I would start by masking the already dark areas of your footage and make them even darker. Try to create a more focused looking light that's coming from the light bulb/screen or whatever light source you want to have. If you want to create new light sources you will also have to make some masks to emulate the way the light would shine on the surrounding objects.
The whole part behind the chair should be a lot darker.
Generally make everything that is already in a shadowy area even darker and lower the brightness of the image overall to a point that feels good.</p>

<p>Also something that should help is this (bit dated) tutorial by Andrem Kramer. Its not perfect but definitely a good help to figure out the workflow:
<a href=""http://www.videocopilot.net/tutorial/day_to_night_conversion/"" rel=""nofollow"">http://www.videocopilot.net/tutorial/day_to_night_conversion/</a></p>
","12705"
"Minimal working frame rate for H264 codec","3042","","<p>When making a video from single image files whereas each image file should be visible for about one second, it does make sense to encode a video with an extremely low frame rate such as 1 frame per second. For this type of application, every frame rate greater than this would be a waste of resources.</p>

<p>I am wondering if the H264 codec (or any specific implementation, such as x264) itself has any lower limit for the frame rate below which it comes to technical problems or some kind of instabilities. In case there is no problem with encoding, can we expect video players to properly deal with such an unusual low frame rate?</p>

<p>Thanks for sharing your experience!</p>
","<p>I'm with AJ. Unless you know the characteristics of every player that might view this, it would be unwise to rely on a small sample of test results. Using a standard frame rate like 24 fps with a keyframe interval of 24 frames will give you essentially the same thing with no compromise in compatibility. The intermediate frames will be minimally small because there will be no detectable changes to encode.</p>
","8746"
"How to convert LibreOffice Impress to video with custom animations?","3033","","<p>I am wondering whether it is possible to convert a LibreOffice Impress presentation into a video (to upload on YouTube), while preserving custom animations on slides.</p>

<p>There is a possibility to export the presentation to a .swf Macromedia Flash file, but the custom animations are not preserved.</p>

<p>Does anyone have a solution?</p>
","<p>I would use a screen recording / capture tool. With this solution you can even define your individual timing from slide to slide.</p>

<p>Which software to use? Depending on your operating system there are many free or low cost solutions available. There are even web-based services.</p>

<p>This <a href=""http://www.technonutty.com/2015/07/best-top-10-screen-recording-softwares-online-utilities.html"" rel=""nofollow"">blog post</a> lists 10 of them. I copied that list here for reference.</p>

<p>Desktop Software:</p>

<ul>
<li><a href=""http://filehippo.com/download_icecream_screen_recorder/"" rel=""nofollow"">IceCream</a> (Windows)</li>
<li><a href=""http://atomisystems.com/activepresenter/"" rel=""nofollow"">ActivePresenter</a> (Windows)</li>
<li><a href=""http://camstudio.org/"" rel=""nofollow"">CamStudio</a> (Windows)</li>
<li><a href=""https://www.techsmith.com/snagit.html"" rel=""nofollow"">Snagit</a> (Windows/Mac)</li>
<li><a href=""https://www.techsmith.com/jing.html"" rel=""nofollow"">Jing</a> (Windows/Mac)</li>
</ul>

<p>Web Services:</p>

<ul>
<li><a href=""http://www.webinaria.com/"" rel=""nofollow"">Webinaria</a></li>
<li><a href=""http://www.apowersoft.com/"" rel=""nofollow"">Apowersoft</a></li>
<li><a href=""https://www.screenr.com/"" rel=""nofollow"">Screenr</a></li>
<li><a href=""http://www.icyte.com/system/snapshots/fs1/6/1/3/6/6136fdcd5ad0da964634b9747f65ac5f2df32d5c/index.html"" rel=""nofollow"">ScreenToaster</a></li>
<li><a href=""http://screencastle.com/"" rel=""nofollow"">ScreenCastle</a></li>
</ul>

<p>I'm sure next year there will be different options. That's why this list can only be an overview and starting point for your own research.</p>

<p>You might need another tool to finally crop your recorded video to 1280x720 or 1920x1080 pixel, because the screen recorder might have recorded an odd rectangle that you have drawn with the mouse. My favorite is <a href=""http://virtualdub.org"" rel=""nofollow"">VirtualDub</a> for Windows.</p>
","16337"
"Gradient banding","3014","","<p>I have a video with a gradient background, with silhouette vector animations on top. The gradient therefore stays exactly the same throughout the video.</p>

<p>However, rendering the video as mp4, I'm having considerable trouble with banding in the gradient. There should be enough of a difference to have a color for each pixel, yet the gradient in the video looks like it's from the good ol' days.</p>

<p>Are there any settings or tricks I could use to avoid gradient stripes in my video? My aim is to match the video's gradient as closely as possible with the gradient background displayed on the website it's supposed to be embedded on.</p>

<p>Thanks in advance!</p>
","<p>Banding isn't uncommon in compressed video files, and can is exacerbated by the type and level of compression...there's also the bitrate of the original file and the final output. Lot's of points along the chain can cause banding. But I'd guess it's in your final output.</p>

<p>What software are you using for rendering/conversion, and what are the render settings you are using?</p>

<p>Are you keeping the video on Vimeo or YouTube? If so they both have great FAQs for how to best render/compress videos for uploading, but the settings are a decent balance of economy of space vs. video quality. <a href=""https://vimeo.com/help/faq/general_vimeo_questions#recommended_settings"" rel=""nofollow"">The FAQ on Vimeo</a> is my personal favorite, and should do you well for both platforms.</p>

<p><a href=""http://greyscalegorilla.com/blog/2009/10/how-to-remove-banding-artifacts-in-after-effects/"" rel=""nofollow"">Here</a> is a great video (which may not apply in this case) dealing with banding and fixes, particularly if the banding is in your original/source file.</p>
","3771"
"Import .flac to after effects cs6","2986","","<p>How can i import a simple .flac audio file to After Effects ? I didn't find one single plugin/presets and answer at adobe site.</p>
","<p>Sadly Adobe doesn't support FLAC in any of their video products. The only Adobe product that supports it so far is Audition.</p>

<p>Though there is this <a href=""http://valion.net/flacimporter64/"" rel=""nofollow"">great and open source plugin</a> that works just fine with CS6 even though it was made for CS5. Plugins made for CS5 are usually compatible with CS6 and CC.</p>

<p>For CS4 there is a 32bit version of the same plugin: <a href=""http://valion.net/flacimporter/"" rel=""nofollow"">http://valion.net/flacimporter/</a></p>
","10866"
"Blur whole text and then ""unblur"" part of it","2980","","<p>I have a text in after effects that I wish to first blur the whole thing and then animate it so that a part of the text is clear.</p>

<p>If I have the text <code>Maecenas sed diam eget risus varius blandit sit amet non magna.</code></p>

<p>First the whole text is blurred. Then it animates and at the end just the last part <code>non magna</code> is blurred.</p>

<p>So far I've used the blur function by going to my <code>Text Layer</code> > <code>Animate</code> > <code>Blur</code>. I've tried doing the thing I want by setting keyframes for the <code>Range Selector</code> properties but I can't get it right.</p>
","<p>What you need to do is to use a regular blur effect. You also have to apply the effect to an adjustment layer. I'll explain why later on. Why don't I just make a recipe for you to follow first, and then I'll explain what's going on! :)</p>

<blockquote>
  <ol>
  <li>Make your text layer and fill it with text.</li>
  <li>Make an Adjustment Layer. (Layer -> New -> Adjustment Layer)</li>
  <li>Make sure the Adjustment Layer is selected, and go Effect -> Blur -> Test some of them (Gaussian Blur is very common.)</li>
  <li>Go to the Effect Controls panel to set the blur to what you want it to be.</li>
  </ol>
</blockquote>

<p>What is happening now, is that the Adjustment Layer is blurring all the layers below it. So if you have other layers below, you may need to Pre-Compose the two layers so that the Adjustment Layer doesn't affect more than the text layer. And the reason why we are using an Adjustment Layer, is that now we can either animate the layer away to reveal the naked text layer, or we can animate a mask to do the same thing. If you use a mask, you can add a feather that will make it look much better in most cases.</p>

<p>Alternatively, you can make two copies of the text, one with the blur directly on it, and one without any blur at all. Then you could animate the mask of the naked text layer to reveal it, but this is a bit difficult if you are animating the text or something like that, because you'll have to do exactly the same on both layers...</p>

<p>Here I'll explain how to do it with the Adjustment Layer with a mask.</p>

<blockquote>
  <ol>
  <li>Select the Adjustment Layer and select the Rectangle tool from the top toolbar, and draw a rectangle covering the whole of the text.</li>
  <li>Press M to open the mask settings on the layer. Now you can set the feather to what you want it to be.</li>
  </ol>
</blockquote>

<p>Move the mask a bit to the right to see how the effect will look like. Remember also that when you are adding feather, you'll have to make the mask bigger because the mask will fade away in the corners...</p>

<blockquote>
  <ol>
  <li>Animate the mask as you please. Tell me if you want me to explain how to do this! :)</li>
  </ol>
</blockquote>

<p>If you need to have more layers below the text layer, you need to Pre-Compose it:</p>

<blockquote>
  <ol>
  <li>Select both the layers, right-click and select Pre-Compose.</li>
  </ol>
</blockquote>

<p>That's it! Good luck with your movie :)</p>
","7630"
"Do I need to convert 60fps to 30fps so it will be reproducible?","2979","","<p>I just bought a GoPro Hero 3 Black Edition.  I have been testing out two modes: 1080p at 30fps and 720p at 60fps.</p>

<p>I have noticed that when reproducing the videos with 720p at 60fps they are not really smooth, they seem a little bit weird (I have used VLC to reproduce them). So, a friend of mine suggested me to convert them to 30fps and then they will be slow motion, and later on speed them up 50% with Adobe Premier.  After doing that, they look better.</p>

<p>I am confused. Why does this happen? Why do I need to low them down to 30fps and then speed them up up to 50%? Wouldn't that mean that we are back to 60fps?</p>

<p>Thanks</p>
","<p>You're not back to 60fps, you're still at 30fps but with every other frame discarded (or blended, or interlaced, depending on how the speedup is accomplished). </p>

<p>When you discard every other frame, you're trading off temporal resolution. If you interlace, you're trading off spatial resolution.</p>

<p>Either way, if this appears smoother, something's wrong with the way the 60fps is being presented. Your playback system may not be able to keep up with the data rate or frame rate. If 1080/30 looks OK, then it's probably not a data rate issue.</p>
","8712"
"How can I add a transition between two cuts of the same clip in Premiere Pro CS5?","2974","","<p>I've run into what I think is a bug in Premiere Pro CS5.  I've got one large clip that was pulled from an analog camera, and I'm cutting it into scenes.  When I try to add a transition between two such scenes, it just doesn't work.  If I do a two second clock wipe, for instance, I can see the hand of the clock rotating, but I just see the left image for one second, then a sudden cut to the right image for one second, with no transition.  Can anyone confirm that this is a bug, and is there a workaround?  Thanks.</p>
","<p>Well, I never got this to work, but for some reason, Premiere did allow the fade to black transition, so I used that one.  Not ideal, but it sufficed for this project.</p>
","3532"
"Is it right time to buy 4K video camera?","2971","","<p>I want to casual shooting (family, travels etc.), but I do not want to sacrifice the quality.</p>

<p>Should I buy a camera equipped with 4K shooting? Is it the right time to buy a 4K camera or should I buy a 1080p now and may be after 2-3 years I will buy 4K.</p>

<p>From my perspective, 4K is still not evolved, it will take another 3-4 years to reach its peak and that's when I think more and more cameras will be available with affordable price.</p>
","<p>You will pay substantially more for a good 4k camera right now than you would for a good 1080p camera. Quality =/= resolution, there are phones that can shoot in 4k but the videos will look a lot worse than a 1080p video from a dedicated 1080p camcorder/dslr, the image quality depends on a lot of factors.
A bigger sensor for example has the ability to capture a lot more light and have much more accurate colors than a tiny sensor that only captures very few photons per pixel.
The video compression used by the camera is also a big factor, even with h264 there can be huge quality differences between cameras depending on the encoding settings of the camera.</p>

<p>The question you should also ask yourself, what do you get from a 4k resolution when you mostly shoot things that are meant to be memories that you don't already get with 1080p.</p>

<p>My advise would be to not think about 4k and look for a nice 1080p camera. Even in commercial video there is not that much to gain with 4k video right now and probably not even in the future, we are reaching resolutions that don't necessarily improve the image quality for every content. It really depends on what you shoot and most importantly how big your screen is and how far you sit away from it. Right now there are quite a few living room configurations where its already hard for a lot of people to tell the difference between 1080p and 720p.</p>
","12610"
"Adobe Media Encoder CC 2017 Will Not Launch on Mac OS Sierra","2956","","<p>I click the app to launch it, it starts to load in the doc, it even creates preference files in Documents, but then it exits. No error, no splash screen, nothing. I've tried every single suggestion out there from deleting slc files to deleting preferences, uninstalling, reinstalling, deleting the logs and prefs under documents, holding shift down when launch, holding alt, cmd, shift when launching. What else can I do? </p>

<p>For some reason this is required to export videos from Premiere, is there no way to export videos without it?</p>
","<p>When you choose Export Media from Premiere Pro CC 2017, there are options to either Queue the export request (which sends to Media Encoder, aka AME) or Export directly, which, as it says, directly exports media without going through AME.</p>

<p>Perhaps your Console log has some notes about what your system doesn't like about AME (or vice-versa).  On my system it launches without error (though it takes its time initializing all the bundles before it starts).</p>
","20160"
"How Can I Reduce Noise On a 5D Mk3?","2956","","<p>The internet is full to bursting with comparisons of noise on the 5D Mk3 at different ISOs, but there is next to nothing about how to reduce noise or work with it / around it.</p>

<p>I am experiencing noise across all ISO settings, but obviously worse the higher I go. I never shoot above 800 and try and stay as low as possible. I am careful not to underexpose, but I still get a fizzy noise in the midtones and shadows.</p>

<p>I appreciate that the 5D is flawed when it comes to video and I don't need to be told how a dedicated video camera will achieve better results; I am a stills photographer and I don't want to buy a second camera.</p>

<p><strong>So given the known limitations of the 5D Mk3, how can I shoot to reduce the noise and what workflows are best for removing noise in post?</strong></p>

<p><em>I am shooting Cinestyle and will soon be shooting RAW via Magic Lantern.</em></p>
","<p>Certainly using RAW on ML will help.  With 14 bit color, your noise reduction options are a lot more advanced.  Traditional noise reduction that you are used to from photos can be used with video as well using either color grading software or editing software.  (With RAW video, it would be in the color grading software before mixing down to processed clips.)  Additionally, with RAW, you won't have the encoding artifact which you may be misinterpreting as noise as well.</p>

<p>That said, how bad of noise are you getting.  I'm in the reverse situation of you.  I started as a video guy and switched to photography and got the 5D Mark iii because of it's strengths in both areas.  You should not be getting significant noise issues below 800 ISO on your 5D Mark iii unless something is wrong.  The 5D Mark iii shoots outstanding video and with RAW is compared fairly closely (though still noticeably different from) much more expensive cameras like the C300 or even some lower end ARRI cameras.</p>

<p>It is generally applauded for it's high video quality and good low light video capability.  Can you post a sample that demonstrates the problem you are seeing?  I wonder if there might be some other issue causing your problems as my 5D Mark iii doesn't give what I would consider unreasonable noise levels on video even at ISOs like 3200 or even 6400 in some lighting conditions.</p>

<p>ISO 3200 video frame grab</p>

<p><img src=""https://i.stack.imgur.com/iHzmX.jpg"" alt=""enter image description here""></p>

<p>There is some noise, but it's pretty limited, particularly considering this is in the shadows under a desk, inside, with no other light other than what was coming in through the window above the table.  There are some newer cameras in the same or lower price range that have slightly improved on the capability, but the 5D mark iii is still one of the top contenders last time I looked around.</p>

<p>Other than that, the easiest option, like Jason Conrad mentioned, is to use more light and lower numbered ISO.  </p>
","12691"
"Help with playback stutter in Sony Vegas Pro 11","2954","","<p>I've got 2 DSLRs in my kit, a Nikon D7000 and a Nikon D3S, for 1080i and 720p respectively. I experienced stuttering playback with footage from both cameras, and figured it was something with Nikon's codec until I had the same issue with HDV footage from a Sony camera (client was unsure of the model#). I recently upgraded from Vegas 8 to Vegas 11 to handle HD footage more cleanly. My machine has 6gb RAM, one of the recommended NVidia graphics cards, and my CPU, while slower than I'd like at 2.53GHHz, is still within recommended ranges for Vegas 11. My internal discs are spinning @ 7200RPM &amp; I'm running Win7 Pro. I've downloaded all the latest drivers, codecs and updates that I could find for my hardware and software.</p>

<p>So here's more detail on what I've tried:
Playback in Vegas 11 Pro stutters for both the Nikon and and Sony HD footage. 
SD footage plays back just fine. 
Only picture stutters, not sound. 
Stutter happens at all playback resolutions, from draft to highest quality. 
If I view the clips in an external player (Quicktime, VLC, DivX) playback is fine.
If I render out a clip to Quicktime, AVI, Windows Media, MPG2, MPG4 at HD resolution, or at least matching the camera clip as closest as I can, I can pull it back into Vegas and play it without any issues. </p>

<p>Based on the above facts:
It's appears to be a Vegas issue, and since playback after rendering THROUGH Vegas works fine, I suspect it's an issue with the Nikon codecs on the inbound side. But, what's up with the Sony playback stuttering? Maybe it's hard drive speed issue? Then why do rendered clips, some of which are larger than the original, not stutter?</p>

<p>I never did any setup from within Vegas to point it to the GPU. I assume that Vegas will FIND my GPU and assign whatever processes as it sees fit. DO I need to do something special?</p>

<p>Has anyone else had success with Nikon clips, and would you be willing to share your setup parameters?</p>

<p>Thanks. </p>
","<p>I have seen some trainwrecks by combining Nikon .mov with Vegas Pro 10. After tech calls into both Nikon and Sony it was determined that my machine didn't have enough resources.</p>

<p>I need to upgrade from 32 bit to a 64 bit version of Win7, plus I need to go from a quad core to an i7, and from 4GB to 12 GB or better. I can use my Nikon files now but only if I keep the total under 1 GB, short projects only but it works. One thing that may help your stuttered preview is to set the window to preview mode at 1/4 size, this helped a lot for me.</p>

<p>Here is a tutorial that covers everything I've tried and more of the things that may help you
get your playback smoother.</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/bFRVKJCHmUg?start=0""></iframe>
            </div></div></p>
","3990"
"Adobe Premiere Elements 10: File > Export is greyed out","2939","","<p>I have a new install of APE 10 on a Windows 7 64-bit machine.</p>

<p>I am a new user of APE.</p>

<p>I have edited a .MOV file, and now I want to save it in a format compatible with YouTube.</p>

<p>When I go to the File menu, <strong>Export is greyed out</strong>.</p>

<p>What are the requirements before being able to export in Adobe Premiere Elements 10?</p>

<p>Thanks.</p>
","<p>The Export function was removed a couple of generations ago but it is still in the file menu as its used for a couple of things such as tiles.</p>

<p>All video is output using the ""Share"" button.</p>

<p>If you want some extra information on this matter here is an answer from Adobe:
<a href=""http://helpx.adobe.com/premiere-elements/kb/export-option-unavailable-premiere-elements.html"" rel=""nofollow"">http://helpx.adobe.com/premiere-elements/kb/export-option-unavailable-premiere-elements.html</a></p>

<p>Hope that helps!</p>
","7224"
"Recommendable production codecs for Adobe video products","2936","","<p>I'm currently searching for new codecs that are better suited for production than the ones we are currently using. So far we used QuickTime Animation and PhotoJPEG. The latter always proved as something ""rocksolid"" and never made any problems. Though its lossy and has no alpha, thats when we used Animation but that codec is a dinosaur when it comes to compression and bit-depth.
I generally dislike all the built-in QuickTime codecs as they either dont deliver all the formats options or are just not pracitcal in a multi OS workflow. The whole Pro-Res family is a no go as it can only be encoded on a mac (except for the beta version of the ffdshow ProRes encoder).</p>

<p>So far I'm very happy with JPEG2000 using the j2k plugin from fnordware, the format has all the good things from legacy JPEG and Animation and has even more features but the downside is the fact that its an image sequence which is not so nice to handle as a single container file. The crippled JPEG2000 codec in QuickTime is no option.</p>

<p>So what I'm searching is a codec that can be used with container formats like .avi or .mov (or any other container, it really doesn't matter), with a good compressability in the range of lossless JPEG2000, atleast 16bit per channel and support for an alpha channel. E: And of course lossless if that wasn't clear so far. Otional lossy compression like JPEG2000 has, would be great but not necessary.</p>

<p>I hope someone can suggest me something, its no problem if the format needs a plugin for After Effects, Premiere etc. it just needs to be compatible with CS6 and not be restricted to Windows or Mac.</p>
","<p><a href=""http://umezawa.dyndns.info/archive/utvideo/?C=M;O=D"" rel=""nofollow"">Ut Video</a> is an editor friendly lossless video codec and is a good choice for in house production as an intermediary. It is fast, open source, actively developed, supports RGB(A)/ULRA (for your alpha channel requirement) and YUV colorspaces, and is available for Windows, OS X, and Linux. You can install it on Windows and it will show up in Adobe Media Encoder, After Effects, etc. I don't know if it will fit all of your needs, but it is worth a try.</p>
","5572"
"Why does HDMI out on my Canon 5D Mark III work with my TV but not my Lilliput monitor or BlackMagic Design Shuttle 2?","2902","","<p>Facts as I observe them:</p>

<ul>
<li>I can use a mini-HDMI to HDMI cord to mirror my Canon 5D Mark III on my consumer Samsung television</li>
<li>I can use a consumer HDMI device such as an Apple TV to power my Lilliput monitor and capture over my BlackMagic Design Shuttle 2 -> UltraStudio Mini Recorder setup</li>
<li>I cannot use my Canon 5D Mark III to run HDMI over the  Shuttle 2 -> Mini Recorder</li>
</ul>

<p>The video is set to 1920x1080 with 30fps. I know the BlackMagic devices like <code>720p59.94</code> and  and <code>1080p59.94</code>.  I have up-to-date <code>1.3.3</code> Canon firmware and have default settings except for <code>HDMI output + LCD</code> setting which is set to <code>Mirroring</code>. I am set to <code>P</code> mode. </p>

<p>I've established a Computer&lt;->Mini Recorder setup in multiple applications using the working input so I believe the issue to be between the camera and the devices receiving its HDMI out. The behavior of the camera seems to change when I plug its output into one of the non-working sources, apparently leaving movie mode. When connected to the monitor and I turn it off, the display returns from black.</p>

<p>My guess here is that the TV is capable of some wildly varying HDMI standards while my camera and/or BlackMagic gear is not. It may not be capable of handling whatever <code>1920x1080 30fps IPB</code> output ends up being.  <a href=""https://forums.creativecow.net/thread/280/376"" rel=""nofollow"">This seems relevant</a>:</p>

<blockquote>
  <p>-Has nothing to do with HDCP compliance, not required, so that is good.</p>
  
  <p>-If you have a display that does not work at 1080 in the live view mode with the Canon 5D Mark II it is probably attributable to one of
  two things: Native preferred timing not set to 1920x1080i in monitor's
  EDID or that the display type is listed as RGB color and default color
  space is listed as sRGB in the monitor's EDID.</p>
</blockquote>

<p>Can anyone tell me what may be causing this apparently flippant behavior and for bonus karma suggest a workaround? </p>
","<p>Which Lilliput monitor do you have and what exactly is the output format of the 5d Mkiii? </p>

<p>Not all Lilliput screens can display progressive resolutions. Sometimes cameras provide ""fake"" interlaced on the monitor out (PsF) even if recording progressive to avoid that problem. It is a common problem with the BM Pocket Cinema Camera and (some) Lilliput screens since the BMPCC only outputs 24p/25p/30p.</p>

<p>Here is a list of hdmi modes the different Lilliput models support: <a href=""http://lilliputdirect.com/index.php?_route_=supported-hdmi-modes"" rel=""nofollow"">http://lilliputdirect.com/index.php?<em>route</em>=supported-hdmi-modes</a></p>

<p>There is no quick workaround to that problem since the hyperdeck shuttle does simple hdmi pass-through and no image conversion. </p>
","18061"
"Strike through text After Effects CS6","2892","","<p>In most Adobe CS6 apps, the character palette has a strikethrough button. In After Effects CS6, this seems to be  missing. </p>

<p>Is there a way to do strikethrough in After Effects CS6. Most the tutorials I have searched for show how to do animated strikethroughs. I just want a simple text strikethrough (without animation) <del>like this</del>. </p>
","<p><strong>Text control is very limited in after effects.</strong> There is no functionality in the <em>Character Panel</em> to achieve a strikethrough at the moment. </p>

<hr>

<h3>Using Illustrator</h3>

<p>For better <strong>text control</strong> I'd suggest create your text in <strong>illustrator</strong> instead:</p>

<p><a href=""https://i.stack.imgur.com/d16t0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d16t0.png"" alt=""enter image description here""></a></p>

<p>and import the illustrator file into <strong>after effects</strong> afterwards:</p>

<p><a href=""https://i.stack.imgur.com/18wPn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/18wPn.png"" alt=""enter image description here""></a></p>

<p><strong>Note:</strong> Editing the text in illustrator is still possible.</p>

<hr>

<h3>Using a shape layer</h3>

<p>In order to create a <strong>simple stroke</strong> above your <em>Text Layer</em>:</p>

<ol>
<li>Enable the <em>Pen Tool</em> (<kbd>G</kbd>) </li>
<li>With no layer selected, click into the <em>Viewer</em> (this will create the 'first point' of the stroke)</li>
<li>Hold down <kbd>Shift</kbd> to make sure that the line, which you are going to create is 'straight'</li>
<li>Click into the viewer again to set the 'end point'</li>
</ol>

<p>This will create a new <strong>Shape Layer</strong>. If the layer is selected you can adjust the 'stoke color' and the 'thickness' in the <em>Tool Bar</em>:</p>

<p><a href=""https://i.stack.imgur.com/GsmNM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GsmNM.png"" alt=""enter image description here""></a></p>

<p>Another way is to roll out the <em>Properties of the layer</em> by clicking on the tiny triangle <kbd>></kbd> left beside the layers name, go to <em>Contents > Shape(Number) > Stroke(Number)</em> to adjust the 'color' and the 'width'. </p>

<hr>

<p>However a shape layer provides <strong>much more control in terms of animation</strong>:</p>

<ol>
<li>Add a <strong>Trim Paths</strong> property</li>
<li>Create a first keyframe by clicking on the <em>stopwatch</em> of the <code>End</code> value and set it to <code>0</code></li>
<li>Move a few frames forward</li>
<li>Create a second keyframe and set <code>End</code> value to <code>100</code></li>
</ol>

<p><a href=""https://i.stack.imgur.com/rGISq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rGISq.png"" alt=""enter image description here""></a></p>

<p>Related: <a href=""https://graphicdesign.stackexchange.com/questions/49643/how-to-use-after-effects-trim-path-with-an-illustrator-path/49975#49975"">https://graphicdesign.stackexchange.com/questions/49643/how-to-use-after-effects-trim-path-with-an-illustrator-path/49975#49975</a></p>

<h3>Result</h3>

<p><a href=""https://i.stack.imgur.com/5lBT2.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5lBT2.gif"" alt=""enter image description here""></a></p>
","16322"
"mp4 file will not play video, only audio on quicktime, fine on VLC","2890","","<p>I have an mp4 file that will not play video, only audio, on QuickTime (10.4 on Yosemite) - and by extension does not show the correct icon in Finder not will it play properly on iOS devices.</p>

<p>Others play fine. Video encoding is H264, audio is AAC, so it <em>should</em> work. I tried remuxing it with ffmpeg, no luck. </p>

<p>Mac console shows ""QuickTime Player: ava error: fail in SeqAndPicParamSetFromCFDictionaryRef""</p>

<p>Happy to remux (prefer not to re-encode, but I will if I have to) using ffmpeg, mp4box, whatever.</p>

<p>Below is file information:</p>

<pre>
 * Movie Info *
    Timescale 600 - Duration 02:15:51.210
    Fragmented File no - 2 track(s)
    File Brand M4A  - version 0
    Created: GMT Sun Nov 27 14:25:55 2011

File has root IOD
Scene PL 0xff - Graphics PL 0xff - OD PL 0xff
Visual PL: AVC/H264 Profile (0x15)
Audio PL: AAC Profile @ Level 2 (0x29)
No streams included in root OD

Track # 1 Info - TrackID 1 - TimeScale 48000 - Duration 02:15:51.210
Media Info: Language ""Undetermined"" - Type ""soun:mp4a"" - 382088 samples
MPEG-4 Config: Audio Stream - ObjectTypeIndication 0x40
MPEG-4 Audio AAC LC - 2 Channel(s) - SampleRate 48000
Self-synchronized

Track # 2 Info - TrackID 2 - TimeScale 24000 - Duration 02:15:51.143
Media Info: Language ""Undetermined"" - Type ""vide:avc1"" - 195432 samples
MPEG-4 Config: Visual Stream - ObjectTypeIndication 0x21
AVC/H264 Video - Visual Size 1216 x 544 - Profile High @ Level 4.1
NAL Unit length bits: 24
Pixel Aspect Ratio 1:1 - Indicated track size 1216 x 544
Synchronized on stream 1
</pre>

<p>I cannot imagine it is the order of tracks, audio before video. I have a suspicion it has something to do with the ftyp, but unsure?</p>

<p>EDIT:</p>

<p>After the suggestion by #ProfessorSparkles below, I changed the Profile Level using subler... but it still doesn't work, same problem. Sound is fine, no video. Here is the updated mp4 info:</p>

<pre>
* Movie Info *
    Timescale 600 - Duration 02:15:51.210
    Fragmented File no - 2 track(s)
    File Brand M4A  - version 0
    Created: GMT Sun Nov 27 14:25:55 2011

File has root IOD
Scene PL 0xff - Graphics PL 0xff - OD PL 0xff
Visual PL: AVC/H264 Profile (0x15)
Audio PL: AAC Profile @ Level 2 (0x29)
No streams included in root OD

Track # 1 Info - TrackID 1 - TimeScale 48000 - Duration 02:15:51.210
Media Info: Language ""Undetermined"" - Type ""soun:mp4a"" - 382088 samples
MPEG-4 Config: Audio Stream - ObjectTypeIndication 0x40
MPEG-4 Audio AAC LC - 2 Channel(s) - SampleRate 48000
Self-synchronized
Alternate Group ID 1

Track # 2 Info - TrackID 2 - TimeScale 24000 - Duration 02:15:51.143
Media Info: Language ""Undetermined"" - Type ""vide:avc1"" - 195432 samples
MPEG-4 Config: Visual Stream - ObjectTypeIndication 0x21
AVC/H264 Video - Visual Size 1216 x 544 - Profile Main @ Level 3.1
NAL Unit length bits: 24
Pixel Aspect Ratio 1:1 - Indicated track size 1216 x 544
Synchronized on stream 1
</pre>
","<p>You will have to re-encode or use a tool to change your h264 level, f.e. this one <a href=""http://coolsoft.altervista.org/en/h264leveleditor"" rel=""nofollow"">http://coolsoft.altervista.org/en/h264leveleditor</a> but its windows only, there might also be a Mac tool that does the same. Your video is encoded with High Profile Level 4.1. iOS only support Level 3.1 and probably the same with QuickTime on Yosemite.</p>
","13180"
"What is the most lossless way to rip a laserdisc to a Mac?","2887","","<p>I want to rip a LD to my Mac as a digital video file. I will create a DVD from this file eventually, but I'm primarily interested in playing it on my laptop. What video output from the Laserdisc player would you use? And what hardware and software would you use to import to a Mac? What settings (image resolution, audio sample rate) would capture the best qualitywithout going over the top? (Clearly 4k would be overdoing it for the analog LD, but MPEG-2 might introduce compression artifacts that I don't want to see unless I make a DVD.)</p>

<p>An accepted answer will include: </p>

<ul>
<li>Recommendation for video output from LD player</li>
<li>Recommended capture hardware</li>
<li>Recommended capture software</li>
<li>Recommended file format</li>
</ul>
","<p>Generally, I would expect that just about any solution will work pretty well now.  The capabilities of even cheap modern hardware so far outpace the capability of laserdisc that you aren't likely to lose much.  Certainly a professional quality capture system similar to the ones Matrox sells would do a superb job, but I'd hazard that even a cheap $30 USB to composite capture device would do the job sufficiently well.  You should be able to use whatever software comes with the capture device.</p>

<p>You will want to capture using the composite output rather than the S-video output since the Laserdisc is natively Composite.  I would capture to a standard DV codec if you have the option for one.  If not, a basic h.264 file is also a fine option.  You may also want to run a 2 pass h.264 compression from your DV capture to minimize the file size while keeping quality high.  (DV files are rather space consuming, but they are very high quality SD files.)  You can use FFMPEG or Handbrake for the transcoding if you want a free option.  x264 is a highly popular h.264 codec available in both.</p>

<p>Truly the most lossless way would be to use an actual lossless format, but that is excessive given the highly limited quality of Laserdisc to begin with.  That's why I recommended DV as a balance of space and quality.</p>
","10455"
"How to make the ""wipe"" transition with vertical line in Premiere pro cs6?","2884","","<p>How to make the ""wipe"" transition with vertical line dividing the image during transition in Premiere pro cs6? </p>

<p>Transitions like that is most often used in the demoreels, showing process of creating shots ""in layers""?</p>

<p>Thanks in advance!</p>
","<p>Depends on your idea. Simplest way is using a <strong>Linear Wipe</strong> Effect (on the strip above). For more complex transitions I'd suggest to animate a <strong>rectangular mask path</strong> in order to get most control, see the manual: <a href=""https://helpx.adobe.com/premiere-pro/using/masking-tracking.html"" rel=""nofollow"">https://helpx.adobe.com/premiere-pro/using/masking-tracking.html</a></p>
","15729"
"link multiple audio tracks to one video track in Premiere CC","2871","","<p>I shoot TMZ-type webisodes, where multiple people are throwing jokes around a news room.  I'm considering buying 4 lavaliere mics for my 4 performers so I can isolate their audio during editing.</p>

<p>I want to keep the 4 audio tracks synched with the video track as I cut and rearrange the footage. Using Premiere CC, how would I link the multiple audio tracks to the single video track?</p>
","<p>You can link multiple clips as you would link two clips. First, put all the audio and video tracks you want in your timeline and arrange them using the timecodes, manual adjustments or however you prefer to align the individual recordings (I'm assuming you have a method for that, since you removed that part of the question in the last edit). Then select all clips that you want to link, right-click on one of them and select ""link"" (if some of them are already linked, you will only see an ""unlink"" option, in that case unlink them, select them again and then link them alltogether). You could also choose to group them, which yields a slightly different effect.</p>

<p>Check the <a href=""https://helpx.adobe.com/premiere-pro/using/editing-audio-timeline-panel.html#link_and_unlink_video_and_audio_clips"" rel=""nofollow"">Premiere Pro Support Page</a> for more information on linking clips, as well as <a href=""https://helpx.adobe.com/premiere-elements/using/grouping-linking-disabling-clips.html"" rel=""nofollow"">this page</a> on linking and grouping clips in Premiere Elements (it's works quite similar in Premiere Pro).</p>

<p>One more tip: In the program preferences under ""Generel"", you have the option ""Display out of sync indicators for unlinked clips"", which will show you when clips you linked at some point are out of sync even if you unlinked them later on. This might be helpful for an editing process with several independent audio clips.</p>
","15294"
"How to encode 60p MP4 video to play on TVs?","2858","","<p>I have 60fps MP4 videos shot with a Canon G7x camera.  They playback find on my laptop, however I would like for the videos to be playable on TVs that have video file playback capability (such as my Samsung Smart LED TV.) I have tried converting to .MOV using QuickTime, but that format also does not work.</p>

<p>Is there a way to prepare such videos to allow them to play directly on TVs?</p>
","<p>Unfortunately, smart TVs are still a relatively young market and the ability to play back video files is not yet uniform.  Each TV may support or not support a variety of formats, resolutions and frame rates.  Your best bet is likely to stick to formats used by popular services such as youtube and NetFlix as these platforms increase the likelihood that a manufacturer will support that specific format.</p>

<p>Alternately, if you know what model TV you are targeting, you can look at the documentation for the TV for the specific codecs and frameratest that are supported by the TV and pick one of them to match.</p>
","13023"
"Converting MLV to Prores","2849","","<p>I have some Magic Lantern Raw footage that I want to convert to Prores 444 for use in this workflow<a href=""http://www.cineticstudios.com/blog/2014/08/the-easy-way-to-use-magic-lantern-raw-straight-to-prores.html"" rel=""nofollow"">enter link description here</a>.</p>

<p>I understand the process, other than the use of LUTs.</p>

<p>In the process, an application called <a href=""http://www.magiclantern.fm/forum/index.php?topic=9560.0"" rel=""nofollow"">MLRawViewer</a> is used to convert .mlv files to ProRes444 files. In this conversions process, a LUT is selected. My confusion lies in the selection of the LUT. </p>

<p>I want the ProRes file to be as flexible as possible for grading later in the process, and I have no desire to use a LUT to give the footage a particular look. </p>

<p><strong>So which LUT should I choose? What should inform this decision?</strong></p>
","<p>I'd pick something that looks flat and unsaturated.  Those are usually the ones designed to retain detail.  If you see clog or slog2, those are good.  Rec 709 will throw out info, so don't use that.</p>

<blockquote>
  <p>What should inform this decision?</p>
</blockquote>

<p>Trial and error.  A lut is really just a starting point for your grade.  Try some different ones out.  If it absolutely butchers the color, then it's probably not a good place to start.  If you see clipping or banding, then you've chosen a lut which was meant for a different situation.  As you try each one, look at the trace on a waveform monitor and vector scope.  If anything goes out of bounds, pick something else.  Do you notice any kind of granularity, banding, or stair-stepping in the traces?  Look for one that's more even.</p>
","14525"
"Software to edit .mov files from a Canon 7D","2837","","<p>I have some video files I recorded using a Canon 7D (.mov encased H.264). The recording is full HD resolution at 30fps. I need to... </p>

<ul>
<li>Adjust the white balance and saturation </li>
<li>Combine several short clips into one longer clip, and </li>
<li>Export to a format that will allow smaller file sizes. </li>
</ul>

<p>Anything that would be ready to upload to Vimeo would be a plus. Due to budget constraints I need to do this as inexpensively as possible. Is there an open source, free, or inexpensive program out there that can do what I need? </p>

<p>I use a Windows 7 (64) system.</p>
","<p><a href=""http://virtualdub.sourceforge.net"" rel=""nofollow"">VirtualDub</a> is a very powerful video editor. However, it's not very intuitive to use it and it has a steep learning curve. On the plus side, there's a large internet community that supports it, so lots of online resources are available.</p>

<p>For your specific problem I've made a short tutorial below. I've done all steps in Windows XP with two .mov files from a Canon 550D. Unfortunately I failed to find an easy way of adjusting white balance with VirtualDub.<br>
I've used various on-line resources to compile this tutorial. There might be a more optimal way of doing this, but I found this the most straight forward.</p>

<h3>Installing VirtualDub + plugins</h3>

<ol>
<li>Download the <a href=""http://sourceforge.net/projects/virtualdub/files/virtualdub-experimental/1.10.3.35390/VirtualDub-1.10.3.zip/download"" rel=""nofollow"">VirtualDub 1.10.3</a>.</li>
<li>Download <a href=""http://code.google.com/p/ffinputdriver/downloads/list"" rel=""nofollow"">ffinputdriver</a> to enable loading .mov files.</li>
<li>Drag the VirtualDub folder to a convenient location. You don't need to install the program, it's click and run.</li>
<li>Place the contents of the plugin folder of ffinputdriver in the plugin folder found in the VirtualDub folder.</li>
<li>Start VirtualDub</li>
<li>First you'll need to convert your video to AVI. That's what VirtualDub works with natively. A warning: my 1280x720 .mov file of 25MB became a 600MB .avi file.
The transcoding speed is dependent on your machine.</li>
</ol>

<h3>Transcoding .mov to .avi</h3>

<ol>
<li><code>File -&gt; Open video file</code>  </li>
<li><code>File -&gt; Save segmented AVI</code>  </li>
<li>Repeat for all your files.  </li>
</ol>

<h3>Adjust Saturation</h3>

<ol>
<li><code>File -&gt; Open video file</code>. Choose one of the AVI files you generated in the previous step.</li>
<li>The video file opens in two screens. The left is the original, the right one is the processed video. At first they should be identical. You can scale the video views by right-clicking on them and choosing an appropriate zoom percentage.</li>
<li>Go to <code>Video</code> and check if <code>Full processing mode</code> is on.</li>
<li>Go to <code>Video -&gt; Filters -&gt; Add -&gt; HSV Adjust</code>.</li>
<li>Adjust the saturation to your liking and hit <code>OK</code>. Hit <code>OK</code> again in the filter window.</li>
<li>Click <code>File -&gt; Save segmented AVI...</code> and save the edited file somewhere.</li>
<li>Repeat the process for all your videos in need of saturation adjustments.</li>
</ol>

<h3>Combine clips into one longer clip</h3>

<ol>
<li><code>File -&gt; Open video file</code>. Choose the first file of your sequence.</li>
<li><code>File -&gt; Append AVI segment</code>. The selected video file will be appended to the end of the open file.</li>
<li>Repeat this with all the video files.</li>
</ol>

<h3>Export to smaller file</h3>

<ol>
<li>Follow the steps outlined <a href=""https://frapsforum.com/threads/virtualdub-save-direct-to-mp4-fraps-to-youtube-with-external-encoders.2446/"" rel=""nofollow"">here</a>.<br>
From a 25MB .mov file I got a 600MB .avi file, which got transformed to a 2.7MB .mp4 file with the steps given in the linked tutorial.</li>
</ol>
","7972"
"Post-production workflow: VFX before color correction?","2824","","<p>I'm shooting my first internet video tomorrow. It has a spaceship chasing and shooting at a superhero in it. With knowledge accumulated between ~a year ago and now and more focused research over the past week, my idea of post-production workflow for a VFX video in the style of FreddieW or Corridor Digital YouTube channels is something like this (prerequisite: I am talking about color correction as getting all shots into a uniform look color-wise, and color grading as getting the feeling you want from the color):</p>

<ol>
<li>Edit raw shots into a sequence, and lock them there.</li>
<li>Do music, sound and VFX. Go through each shot in the video and add VFX elements, matching them to the shot's lighting, exposure, color, shadows, ...</li>
<li>Go through each composited shot and color correct and grade it.</li>
<li>Replace shots in locked edit with composited and color corrected and graded shots.</li>
</ol>

<p>I have now these questions:</p>

<ol>
<li>Is this workflow ""correct""? Are there many ""correct""'s (is it mutable)?</li>
<li>You're supposed to shoot in as low contrast and saturation as you can (this is called flat?) to make color correction easier, like you have a blank, unbiased canvas to ""pull"" the colors out of, right?</li>
<li>This is the real core of my question: how can you match VFX elements to flat footage when there's no amplified color detail to work with? Can I color correct before VFX, and leave color grading where it is?</li>
</ol>
","<p>Any workflow that works for you is ""correct"".  I don't see any obvious problems with your proposed workflow, though I would think you would want to do sound after VFX so that it can be properly timed to the VFX.  Depending on the software you are using, you may not have to lock the shots but may actually be able to move directly in to working on the shots.</p>

<p>As far as low contrast and low saturation, I would personally disagree with shooting for low contrast/low saturation.  This limits the depth of your available color space too much.  Your camera has a limited number of colors it can describe, if you ignore a large portion of that color space, then you can't describe as many colors and the resolution of your color after boosting the saturation and contrast will be very greatly reduced.  That said, if you have issues with lacking detail in your shadows, exposing it up in to mid-tones can help since your format doesn't actually support using that portion of the dynamic range anyway, so you wouldn't be losing anything meaningful.  </p>

<p>The key is that you want to make strong use of the color space your camera can capture and encode that way you have the highest possible use of the color space as long as you are careful not to exceed the color space (color clipping needs to be avoided).  It is always easy to make colors more subdued and to lower contrast as this is moving from a high color resolution to a low one, but you can't move the other way because you don't have the detail to fill in.</p>

<p>What flat means is not necessarily low contrast or low saturation, but rather that the colors are not heavily tinted and that details are well represented within the usable portion of the color space.  They can be fairly vibrant, but they have a neutral response and gamma.  In other words, you want it to evenly capture detail across the DR, but you also want the DR to be fully utilized.  Think of flat as the way the curve looks in the Photoshop curves tool.  Flat would be a straight line rather than a curved one.  It can still have very strong contrast and saturation since the DR is being fully used.</p>

<p>For the third part, it really doesn't matter.  Either you mix to subdued colors and boost everything or you mix to vibrant colors and bring it down.  Either way, as long as it fits at some point, it will fit when you adjust the color grade of the entire frame if it fit well before.</p>
","10543"
"How do I PROPERLY slow down a 60 fps clip in Vegas Pro 11?","2817","","<p>Let me start at the very beginning:</p>

<p>My project frame rate is 23.976 fps (standard film). When I create such a project in Adobe Premiere and then want to add a clip to the timeline, I can set the frame rate of the clip manually. Therefore, when my clip is 60 fps shot with a Canon DSLr (actually I believe and hope it's 59.94 fps), I can override its fps to 23.976 and then drop it to the timeline, thus getting the perfect slow-motion = as slow as possible without any frames missing in-between.</p>

<p>Now, I don't know how do I do this in Vegas Pro 11. All I can do is drop the clip to the time-line and then set its playback rate to 0.4x original speed (59.94 * 0.4 = 23.976). However, I don't know how safe this is. Is it safe to assume that the playback speed of the clip will be EXACTLY 23.976 fps and there won't be any frames missing in-between, thus creating the perfect slow-motion?</p>
","<p>You should be safe setting playback rate at 0.400. 59.94 is actually 60/1.001 and 23.976 is actually 24/1.001, so the 0.4 multiplier is technically exact. Switch off resample to be sure. Of course, how Vegas actually handles this internally is anybody's guess.</p>

<p>One way to test this would be to generate a frame sequence just containing an incremental numeral display (1,2,3...) in AE or similar, then render that as a 59.94p video. Import in vegas with playback 0.4 and resample off and then cycle through the frames to see if there are any duplicates or missing.</p>
","3352"
"Concate two video file with fade effect with ffmpeg in linux","2812","","<p>is there anyway to concate two video file with fade effect with <code>ffmpeg</code> in <code>linux</code>?<br>
something like this:  </p>

<pre><code>ffmpeg -f concat -i input.txt -c copy ""--with:fade:out-in"" output.ogg
</code></pre>
","<p>Your present command specifies stream copy, but since you wish to alter the video frames during the transition, that won't work. </p>

<p>It can be done if you know the duration of the two videos and the duration of fade. Also, for the command below, the resolutions of the two videos should be the same.</p>

<pre><code>ffmpeg -i first.ogg -i second.ogg \
-filter_complex \
""color=black:WxH:d=Video1Length+Video2Length-FadeDuration[base]; \
 [0:v]setpts=PTS-STARTPTS[v0]; \
 [1:v]format=yuva420p,fade=in:st=0:d=FadeDuration:alpha=1, \
      setpts=PTS-STARTPTS+((Video1Length-FadeDuration)/TB)[v1]; \
 [base][v0]overlay[tmp]; \
 [tmp][v1]overlay,format=yuv420p[fv]; \
 [0:a][1:a]acrossfade=d=FadeDuration[fa]"" \
-map [fv] -map [fa]
output.ogg
</code></pre>

<p>Replace</p>

<p><code>WxH</code> with the resolution of the videos i.e. <code>640x360</code>,</p>

<p><code>Video1Length</code> <code>Video2Length</code> <code>FadeDuration</code> with their value in seconds.</p>
","17504"
"How can I combine whiteboard animation, stop motion and live video without massive import/export and adjustments?","2794","","<p>I have watched videos from <a href=""http://www.youtube.com/user/AsapSCIENCE"" rel=""nofollow"">AsapSCIENCE</a>. These videos combine stop motion, whiteboard board animation (drawing effects) and real life scenes.</p>

<p>I found a whiteboard animation software, VideoScribe, that lets me achieve drawing like effects without any drawing knowledge, however it does not let me add any video or edit the video track.</p>

<p>Suppose I </p>

<blockquote>
  <p>draw a man -> drag him -> walking animation -> draw a girl -> drag and
  animate again -> draw a dog jumping towards them -> erase dog -> drag
  both to come close -> draw rain -> raining -> draw umbrella -> ...</p>
</blockquote>

<p>I'll prefer to use videoscribe for drawing effect and flash for rest. But I need to adjust frames everytime. And if I later on realize that dog should look more cute. I would have to rework since It may again require adjusting scenes, platform and position of other elements.</p>

<p>I was wondering if Flash has some actionscript which can put sketch/drawing/painting effect to a SVG image then we at least combine video scribing with Flash only. It'll save a lot of time in importing/exporting and editing. Especially when we commit a mistake in past work. </p>

<p>Is there any software or Action Script to achieve this without needing advanced knowledge?</p>

<p>Any end-to-end <strong>tutorial</strong> which can teach me how to make videos like this may also help me.</p>
","<p>After Effects is probably the easiest all inclusive solution that could handle everything they do in those videos, but they are doing multiple things and there isn't really a good way to give a tutorial for all of them.  </p>

<p>You would need to shoot the video on a green screen so that it can be extracted and layered in.  You can then animate the layer than those green screened clips and animate them with keyframe animation to make them move as needed on top of the white board.</p>

<p>The whiteboard itself can also be made using Write On effects or path animation.</p>

<p>Similar things can also be done with other decent NLEs like Premiere, Final Cut Pro, Vegas, Media Composer or Lightworks, but path animation and motion graphics (which this is a class of) are most easily done in After Effects.</p>

<p>After Effects is to video what Flash is to Internet games and basic scripted animation.  It is primarily done without programing and through more traditional vector animation workflows.  It is probably the closest to what you are looking for if you like the behavior of flash but want it to support more video stuff and require less scripting.</p>
","10764"
"How to make a video where an actor writes facing the camera?","2775","","<p>I'd like to know how to produce this kind of instructional video, where an actor is writing on an invisible wall facing the camera:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/jlLlxgwCt6o?start=0""></iframe>
            </div></div></p>

<p><em>Introduction to Limits (mathbff)</em></p>

<p>What equipment/tools and which setup is required to shoot this video?</p>
","<p>You need a few simple things to do this:</p>

<ol>
<li>a camera</li>
<li>a large piece of glass or plexiglass</li>
<li>dry erase markers</li>
<li>editing software capable of basic transform controls</li>
</ol>

<p>Record your actor writing on the glass.  To the camera, the writing will appear backwards.  Use your editing software to flip the image horizontally.  The writing will now appear normal*.  Most editing software is capable of this operation.  If there's not a ""flip horizontal"" command, try setting the horizontal scale to -100%.</p>

<p>*Pro tip:  Make sure your actor's clothes don't have any writing on them, or it will appear backwards in the final result.</p>
","16456"
"How to fix a 24fps shot that looks jerky when rendered at 30fps frame rate?","2736","","<p>I accidentally shot one scene at 24fps instead of the 30fps I intended.  I want to render it at 30fps (to fit the rest of the video), but the render from Premiere looks choppy, regardless of whether I use Frame Blending or not.  I know this is expected, especially since I shot at 1/60 shutter speed, but are there any effects / settings I can apply to smooth things out?  Blur perhaps?</p>

<p>For what it's worth, it's a moving shot of some dancing, so both the subject and the camera are moving throughout.</p>

<p>Thanks!</p>
","<p>If you change the frame rate of everything to 60fps you'll get an even frame blend every 2.5 (60fps) frames.  If you change the frame rate to 120fps you can eliminate the stutter completely, as 30 fps will repeat frames 4x and 24fps will repeat frames 5x.  But few systems can play 120fps, whereas many can play 60fps (including YouTube).</p>
","16477"
"Automatically split takes into individual files in Final Cut Pro","2700","","<p>One of my 'favourite' features in Adobe Premiere Pro CS family is <a href=""http://help.adobe.com/en_US/PremierePro/4.0/WS1c9bc5c2e465a58a91cf0b1038518aef7-7f9fa.html"" rel=""nofollow"">automatic scene detection</a> - its  ability to spit out individual clips when capturing from tape using pause/stop information. As far as I know, Final Cut Pro has no automnatic way of doing this. Splitting clips during import manually is so tedious. Is there something I'm missing out, or a plugin perhaps that provides this feature in FCP 7?</p>
","<p>There is a  pop-up menu in the General tab in User Preferences that has all the options in dealing with timecode breaks. There is an option where you choose Make New Clip from the On timecode break. This option will make a new clip in the bin when a timecode break is reached.</p>

<p>SRC: <a href=""http://documentation.apple.com/en/finalcutpro/usermanual/index.html#chapter=20&amp;section=5&amp;tasks=true"" rel=""nofollow"">http://documentation.apple.com/en/finalcutpro/usermanual/index.html#chapter=20%26section=5%26tasks=true</a></p>
","1724"
"What is the most effective way of slowing down footage?","2696","","<p>I want to experiment with slowed down footage and I see several ways in doing that with my current set-up.</p>

<p>I have the following hardware/software at my disposal:</p>

<p>After Effects CS5.5,
 Twixtor Pro,
Canon 550D (capable of shooting 50fps 1280x720 footage)</p>

<p>I can think of several ways to slow-down my 50fps footage, I'm interested to hear what the most effective way to go is in terms of <strong>quality</strong> and what the best way is to slow my footage down <strong>as much as possible</strong>.<br>
The methods I could think of are written down below.</p>

<ol>
<li>Interpret the footage in CS5.5 as 25fps or lower. 18fps being the minimum, stutter is noticeable below that.</li>
<li>Interpret as 50fps and then do a time-remapping to slow down the footage. Add frame blending to compensate for any stutter.</li>
<li>Interpret as 50fps and use Twixtor Pro to slow down the footage.</li>
</ol>
","<p>Depending on how much you are looking to slow down.  Interpret as 50fps, and then once in your timeline, say it is a 25fps timeline.  Then you can stretch the footage using time remapping, then you can turn on frame blending to the solid line...this will be good in most instances up to about 25% speed of original...</p>

<p>Less than that the same, 25fps timeline, interpret footage at 50fps, then use twixter to get that super slomo feel.  Not that twixter does not always work and is going to give varying results based on the footage.</p>
","5551"
"Can you mask a parent null object in Adobe After effects?","2693","","<p>Is it possible to create one mask or track matte mask on a parent null layer that applies to all child layers? Only way I've figured out is to apply a track matte to every layer I want masked which seems very inefficient. </p>

<p>Cheers.</p>

<p><img src=""https://i.stack.imgur.com/Y253B.png"" alt=""enter image description here""></p>
","<p>Well, there a lot of ways to get result quicker, but each one depends on desirable result. =)</p>

<p>Here's one way:</p>

<ol>
<li>Make desired null's mask</li>
<li>Make any mask for it's child (it doesn't matter what there)</li>
<li>Alt+click on child's mask path 'clock' icon (it'll open script settings)</li>
<li>Clear edit field or select all the contents</li>
<li>Drag 'link' icon (spiral) of mask path expression over Null's mask Path (it should add some kind of ""thisComp.layer(""Null 1"").mask(""Mask 1"").maskPath"" to expression field)</li>
<li>Copy this new mask to all childs</li>
</ol>

<p>p.s.: this way is just a start, since it's not includes shifts of child and parent layers, but if pivots aligned it'll work</p>
","12055"
"How to trim out black frames with ffmpeg on windows?","2690","","<p>I have a short video and I'm trying to cut-out all the blank or near blank frames.
This isn't a case where I care about the breaks in a video that could be dark.  I literally want all near black-frames out.</p>

<p>What I didn't realize with ffmpeg at first was that the 'blackframe' or 'blackdetect' filters, is that they didn't actually filter.  They just seem to show you what you can possibly filter, with other command(s).  </p>

<p>What is the best way in 'Windows' to actually filter out the resulting found frames?</p>

<p>So with ffmpeg I'm actually concat-(ting) the forward and reverse of the same file than I'm running some filters on those files.  See my current command:</p>

<pre><code>ffmpeg.exe -y -i ""2013-10-14_14-30-55.mov"" -filter_complex ""[0:v]transpose=3,split[tp][tp2];[tp]reverse[vr];[tp2][vr]concat=n=2:v=1:a=0[vbf];[vbf]blackframe=98:32[v]"" -map ""[v]"" -vcodec wmv2 -q 10 -trellis 2 -r 16 ""g:\gd5.wmv""
</code></pre>

<p>Then I get this output: (Just the tail end of it anyway...)</p>

<pre><code>[Parsed_blackframe_4 @ 00000000003bf320] frame:650 pblack:99 pts:21821999 t:21.821999 type:B last_keyframe:639
[Parsed_blackframe_4 @ 00000000003bf320] frame:651 pblack:99 pts:21855366 t:21.855366 type:P last_keyframe:639
[Parsed_blackframe_4 @ 00000000003bf320] frame:652 pblack:99 pts:21888733 t:21.888733 type:B last_keyframe:639
[Parsed_blackframe_4 @ 00000000003bf320] frame:653 pblack:99 pts:21922099 t:21.922099 type:B last_keyframe:639
[Parsed_blackframe_4 @ 00000000003bf320] frame:654 pblack:99 pts:21955466 t:21.955466 type:I last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:655 pblack:99 pts:21988833 t:21.988833 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:656 pblack:99 pts:22022199 t:22.022199 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:657 pblack:99 pts:22055566 t:22.055566 type:P last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:658 pblack:99 pts:22088933 t:22.088933 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:659 pblack:99 pts:22122299 t:22.122299 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:660 pblack:99 pts:22155666 t:22.155666 type:P last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:661 pblack:99 pts:22189033 t:22.189033 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:662 pblack:99 pts:22222399 t:22.222399 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:663 pblack:99 pts:22255766 t:22.255766 type:P last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:664 pblack:99 pts:22289133 t:22.289133 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:665 pblack:99 pts:22322499 t:22.322499 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:666 pblack:99 pts:22355866 t:22.355866 type:P last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:667 pblack:99 pts:22389233 t:22.389233 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:668 pblack:99 pts:22422599 t:22.422599 type:B last_keyframe:654
[Parsed_blackframe_4 @ 00000000003bf320] frame:669 pblack:99 pts:22455966 t:22.455966 type:I last_keyframe:669
frame=  362 fps= 14 q=10.0 Lsize=    9004kB time=00:00:22.62   bitrate=3260.2kbits/s dup=1 drop=309
video:8922kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.918756%
</code></pre>

<p>So what do I do with this blackframe information in Windows?  Or is there a better method with the 'blackdetect' filter?</p>
","<p>Here's a way to do it with the <code>blackdetect</code> and <code>trim</code> filters.</p>

<p>First, a better way to get the <code>blackdetect</code> output is via <code>ffprobe</code> because it is capable of writing structured data such as <code>XML</code>, <code>JSON</code> or one <code>key=value</code> per line. </p>

<p>You do this using metadata injection:</p>

<p><code>ffprobe -f lavfi -i ""movie=/path/to/input.mp4,blackdetect[out0]"" -show_entries tags=lavfi.black_start,lavfi.black_end -of default=nw=1 -v quiet
TAG:lavfi.black_start=0
TAG:lavfi.black_end=5.42208
TAG:lavfi.black_start=73.4067
</code></p>

<p>As you can see it returns the start and end time of each black section. You use the returned info to <code>trim</code> the segments between each <code>end</code> and the next <code>start</code>:</p>

<p><code>ffmpeg -i /path/to/input.mp4 -filter_complex ""[0:v]trim=start=5.42208:end=73.4067,setpts=PTS-STARTPTS[v1];[0:a]atrim=start=5.42208:end=73.4067,asetpts=PTS-STARTPTS[a1]"" -map [v1] -map [a1] output.mp4</code></p>

<p>You can chain multiple <code>trim</code> filters in the same command. See <a href=""https://superuser.com/questions/681885/how-can-i-remove-multiple-segments-from-a-video-using-ffmpeg"">this answer</a> for a detailed example.</p>

<p>To automate this in Windows you can install and use <a href=""https://en.wikipedia.org/wiki/Windows_PowerShell"" rel=""nofollow noreferrer"">PowerShell</a> or another scripting language of your choice (PHP, Python etc).</p>
","16571"
"Best bitrate to responsive (web) video","2677","","<p>So, I am a multimedia developer: I develop websites and that nerd things... My last site is responsive (fluid work in all devices - mobile, computer, television,...).</p>

<p>A responsive website needs <strong>4 video files</strong>:</p>

<ol>
<li>2 <code>MP4</code> - <strong>1)</strong> for big screens and <strong>1)</strong> for small screens</li>
<li>2 <code>WEBM</code> - for same purposes</li>
</ol>

<p>So, recently I checked my server disk usage and I realized that videos are consuming a big percentage of it, so I guess that it's because of bitrate.</p>

<h2>DEFAULT BITRATE OF MY VIDEOS</h2>

<ol>
<li><strong>HD MP4</strong> - 2,10 Mbps - 1280X720 (~77,5MB) - <strong>Duration:</strong> 04:42</li>
<li><strong>SD MP4</strong> - 694 kbps - 640X360 (~25,57MB) - <strong>Duration:</strong> 04:42</li>
</ol>

<p>So, am I using right bitrate? Are the files small enough?</p>
","<p>Those are the expected file sizes for those data rates.  4 minutes and 42 seconds of video is 282 seconds of video, at 2.1 Mbps, that's 592.2 Mbits.  That's around 72 megabytes or so, so your 77.5 megabyte file is the correct size (there can be some variation and sound is also generally factored in separately.)</p>

<p>Those are also pretty close to the minimum bitrates that you can use without getting substantial quality loss.  2.1 Mbps is around the same bitrate that Hulu uses for their 720HD streaming and they are the lowest bandwidth of the major streaming sites that I'm aware of.</p>

<p>If you plan to host video, you really need more disk space and bandwidth.  Hosting is available very cheaply for multiple gigabytes of hosting space with unlimited bandwidth if you look around a bit, though I haven't been in that segment of the market for a while so I couldn't tell you who is best anymore.  (I self host on a dedicated server because I need a lot of space and high performance.)</p>
","9768"
"FFMpeg and HLS, videos need to load in their entirety before they start playing","2673","","<p>I am using ffmpeg to generate a segmented list of files in order to stream them to an iOS app. The list of files is generated fine but when it comes time to play them, the video needs to be downloaded in its entirety before playback starts. This behaviour seems to be the case on iOS, Safari and VLC.</p>

<p>Does anybody know why this is happening and how I can improve the performance of playback? I have complete control over how the files are recorded in iOS, as well as how they are processed. Here is a sample stream:</p>

<p><a href=""http://www.bytesizecreations.com/storie-test/hls.m3u8"" rel=""nofollow"">http://www.bytesizecreations.com/storie-test/hls.m3u8</a></p>

<p>Here are my ffmpeg commands to generate the segments from the file:</p>

<pre><code>ffmpeg -i joined.ts -flags -global_header -vcodec copy -acodec copy -map 0 -f segment -segment_time 2 -segment_list hls.m3u8 -segment_list_size 999999 -segment_format mpegts out%03d.ts
Here is the output of ffprobe on the file:

  libavutil      54.  7.100 / 54.  7.100
  libavcodec     56.  1.100 / 56.  1.100
  libavformat    56.  4.101 / 56.  4.101
  libavdevice    56.  0.100 / 56.  0.100
  libavfilter     5.  1.100 /  5.  1.100
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  0.100 /  3.  0.100
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  0.100 / 53.  0.100
Input #0, mpegts, from 'joined.ts':
  Duration: 00:00:07.96, start: 1.441667, bitrate: 3899 kb/s
  Program 1 
    Metadata:
      service_name    : Service01
      service_provider: FFmpeg
    Stream #0:0[0x100]: Video: h264 (High) ([27][0][0][0] / 0x001B), yuv420p, 1280x720, 24 fps, 24 tbr, 90k tbn, 180k tbc
    Stream #0:1[0x101](und): Audio: aac ([15][0][0][0] / 0x000F), 44100 Hz, stereo, f
</code></pre>
","<p>So it turns out after a lot of research that this is the desired behaviour for Apple's AVPlayer component. Basically, my Internet is so slow here where I live, the component waits until the video has fully downloaded before it starts playing. The best and recommended way around this, is to generate alternate streams for the video before trying to play it. Here is documentation on the m3u8 format and HLS streaming guidelines:</p>

<p><a href=""https://developer.apple.com/library/ios/documentation/networkinginternet/conceptual/streamingmediaguide/Introduction/Introduction.html"" rel=""nofollow"">https://developer.apple.com/library/ios/documentation/networkinginternet/conceptual/streamingmediaguide/Introduction/Introduction.html</a></p>

<p>I followed the above and generated two extra streams from the original: a 200kbps, 400kbps, and playback now starts almost immediately. Your mileage may vary depending on the speed of your Internet connection.</p>

<p>Alternatively, if you don't have to use something like HLS, you can use plain mp4 files. Make sure that they have <code>-faststart</code> enabled if you are processing the file with ffmpeg.</p>

<p><strong>Update:</strong> To simplify this for other developers, I have released an SDK that is able to upload your mov or mp4 files to a cloud service and convert the file to an HLS compatible stream. The SDK can be installed via Cocoapods and you can access documentation and the framework here: </p>

<p><a href=""https://github.com/Storie/StorieCloudSDK"" rel=""nofollow"">https://github.com/Storie/StorieCloudSDK</a></p>
","13193"
"Convert Uncompressed AVI to JPEG2000 in MXF","2628","","<p>I'm looking to convert raw, uncompressed video (currently in avi container) into lossless motion jpeg2000 encoded video in an mxf container. I've tried FFMPEG, but don't think it can do this. What's the best method for transcoding on Linux?</p>

<p>FFMPEG command tried:<br />
 <code>ffmpeg -i Glueck_Newsreel.avi -vcodec libopenjpeg -acodec copy ms0001.mxf</code></p>

<p>Errors:<br />
<code>[mxf @ 0x1f43a00] track 0: could not find essence container ul, codec not currently supported in container</code><br />
<code>Could not write header for output file #0 (incorrect codec parameters ?)</code></p>
","<p>The JPEG 2000 codec in FFMPEG is still experimental. To use it you would have to compile FFMPEG by yourself with a special option enabled.</p>

<p>But you could use <a href=""http://opendcp.org/"" rel=""nofollow"">OpenDCP</a>, which has it's <a href=""http://code.google.com/p/opendcp/wiki/Documentation"" rel=""nofollow"">documentation here</a>. It needs TIFF images and WAV files as input, that you can produce with FFMPEG. The resulting MXF can be tested with the <a href=""http://www.iis.fraunhofer.de/en/bf/bsy/download/easydcpplayer.html"" rel=""nofollow"">easyDCP player</a>. </p>

<p>OpenDCP is available on all mayor platforms, but the easyDCP player only on Windows/Mac. But perhaps this is still an option for you, since you only need to run the easyDCP player once (on a friend's PC), just to be sure that the OpenDCP output is ""according to the standard"".</p>
","7166"
"How to create simple animation with transparent background?","2618","","<p>I'd like to create basically a HUD (head up display) using simple animations.  The following are three different videos:</p>

<ul>
<li>animated checkmark</li>
<li>animated circle</li>
<li>animated underline</li>
</ul>

<p>Each video just draws out the symbol.  I'd like transparent backgrounds so these small videos can be used in other videos for effects.</p>

<p>As an example, in some video, I can introduce the animated checkmark.  You'll see the checkmark next to something I want to emphasize but everything under the checkmark (in the host video) is still visible (because of transparency in the checkmark video).</p>

<p>How can this be done in AE?  I'm not sure how to create a checkmark that basically draws itself out.  Are there any tutorials (youtube) that show something like this?</p>
","<p>All of the animations you want to do there are achievable with masks and the <code>Stroke</code> effect.</p>

<p>I suggest you start to generally learn to use After Effects, these are very simple things you want to do there and you shouldn't need any tutorial once you understand the very basics of After Effects.</p>
","11936"
"create subtitles track by automatic timing of existing text","2609","","<p>1
00:00:01,000 --> 00:00:04,000</p>

<p>I am using Adobe Premiere to produce videos in English by gluing together multiple clips put together from lots of different footage or sources. I do have a text transcript of what is being said though, in English, so talking about word-for-word English subtitles for the voice in English.</p>

<p>2
00:00:04,250 --> 00:00:06,250</p>

<p>How would I go about creating a subtitles file for the final video?</p>

<ul>
<li>I do not wish to render the captions into the video</li>
<li>The transcript I have does not have any time information and that is the crux of this question - <strong>what is the easiest way to generate semi-accurate timing records, as automatically as possible</strong>?</li>
<li>So the question is NOT about how to sync existing subtitles with existing timestamps</li>
<li>I could do it in Premiere, or Adobe Audition, or <a href=""http://nikse.dk/SubtitleEdit/"" rel=""nofollow"">Subtitle Edit</a>, Subtitle Workshop or <a href=""http://en.wikipedia.org/wiki/Subtitle_editor"" rel=""nofollow"">any other subtitle editor</a> that may provide relevant synchronization functionality?</li>
<li>The end result I'm hoping for is a flv file with closed captioning (.sub, .srt or similar) that could be turned on or off in a web-browser / Flash player</li>
<li><a href=""http://www.youtube.com/watch?v=hbmf0bB38h0"" rel=""nofollow"">Like this lecture</a>, for example, when you click on CC and choose the embedded English subtitling (<a href=""http://support.google.com/youtube/bin/static.py?hl=en&amp;topic=2734693&amp;guide=2734661&amp;page=guide.cs"" rel=""nofollow"">not the automatic captions that YouTube offers</a>)</li>
</ul>
","<p>If you select your clip and go to the Clip menu, then choose Analyse Content, Premiere will attempt to identify speakers and even attempt to dictate what is being said.  You can assist this analysis by providing a script file to it by adding either an Adobe Story Script file or a TXT file to the Reference Script option.  I don't have any experience with how well this feature works, but it's certainly worth a shot if you already have an accurate script file to try it with.</p>
","7650"
"Adding metadata for films/TV shows on Linux","2602","","<p>I am looking for an open-source solution for automatically pulling films/TV shows informations. I've found MetaX (Windows only) and Subler (MacOSX only) from a post <a href=""https://apple.stackexchange.com/questions/91790/adding-metadata-for-films-tv-shows-in-itunes"">here</a>. Apparently metadata sources can be found from: TVDB, TheMovieDB, Amazon, and tagChimp, as well as IMDB, ref <a href=""http://www.danhinsley.com/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I know I can use <a href=""http://gpac.wp.mines-telecom.fr/mp4box/"" rel=""nofollow noreferrer"">MP4Box</a> (or libav/ffmpeg) to easily add metadata (including Cover Art), but really what I would like is a script and/or GUI to easily <strong>retrieve</strong> metadata (plot summary, cast and crew, genre, release data, Cover Art as JPEG...).</p>

<p>Some people suggested using <a href=""http://www.tweaking4all.com/home-theatre/adding-movie-information-metadata-to-mp4-m4v/#linux_vlc"" rel=""nofollow noreferrer"">VLC</a> on Linux. But I cannot get anything to work over here (using debian/jessie amd64). Neither the <em>Download Cover Art</em>, nor the <em>Add Cover Art</em> seems to work.</p>

<p><img src=""https://i.stack.imgur.com/ciiFR.png"" alt=""VLC Screenshot""></p>

<p>The first one does nothing, the second one print this on the console:</p>

<pre><code>[00007f3fc4567d38] taglib meta writer error: File /tmp/big_buck_bunny_480p_h264.mov can't be opened for tag writing
</code></pre>

<p>Bonus point: if this could be integrated to <a href=""https://handbrake.fr/"" rel=""nofollow noreferrer"">Handbrake</a>, that would be awesome.</p>
","<p>After some research, I discover this <a href=""https://stackoverflow.com/questions/1966503/does-imdb-provide-an-api"">post</a>.</p>

<p>So far the API for <a href=""http://www.omdbapi.com"" rel=""nofollow noreferrer"">http://www.omdbapi.com</a> has proven to be exactly what I wanted. For completeness, I found those command line tools helpful:</p>

<ul>
<li><a href=""https://github.com/xbgmsharp/allocine"" rel=""nofollow noreferrer"">https://github.com/xbgmsharp/allocine</a></li>
<li><a href=""https://github.com/bgr/imdb-cli"" rel=""nofollow noreferrer"">https://github.com/bgr/imdb-cli</a></li>
</ul>

<p>And on debian:</p>

<pre><code>$ sudo apt-get install python-imdbp
</code></pre>
","15267"
"FFmpeg - dropping duplicate frames","2590","","<p>FFmpeg knows about duplicate frames in the input video stream as it outputs a message like this:</p>

<blockquote>
  <p>More than 1000 frames duplicated</p>
</blockquote>

<p>Is there some way to tell it: ""Not include duplicates into the output stream, please?""</p>

<p><strong>EDIT (ADDED):</strong></p>

<p>I tried this command (thanks to <em>Mulvya</em>):</p>

<pre><code>ffmpeg -i scene.mkv -profile:v baseline -level 3.1 -vsync 0 -map 0:v  scene.mp4
</code></pre>

<p>and obtained the target video <code>scene.mp4</code> with the same duration and (almost) identical in playback with original.</p>

<p>The command with full output of it is here:</p>

<pre><code>&gt;ffmpeg -i scene.mkv -profile:v baseline -level 3.1 -vsync 0 -map 0:v  scene.mp4
ffmpeg version N-83585-ga5c1c7a Copyright (c) 2000-2017 the FFmpeg developers
  built with gcc 5.4.0 (GCC)
  configuration: --enable-gpl --enable-version3 --enable-cuda --enable-cuvid --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-nv
enc --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libblur
ay --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-
libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-lib
rtmp --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amr
wbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enabl
e-libxvid --enable-libzimg --enable-lzma --enable-zlib
  libavutil      55. 47.100 / 55. 47.100
  libavcodec     57. 80.101 / 57. 80.101
  libavformat    57. 66.102 / 57. 66.102
  libavdevice    57.  2.100 / 57.  2.100
  libavfilter     6. 73.100 /  6. 73.100
  libswscale      4.  3.101 /  4.  3.101
  libswresample   2.  4.100 /  2.  4.100
  libpostproc    54.  2.100 / 54.  2.100
Input #0, matroska,webm, from 'scene.mkv':
  Metadata:
    encoder         : libmkv 0.6.5
    TITLE           : New Project
    DIRECTOR        : Big Mac
    COMPOSER        : Big Mac
    DESCRIPTION     : This video is about New Project
  Duration: 00:03:11.93, start: 0.000000, bitrate: 1025 kb/s
    Stream #0:0(eng): Video: h264 (High), yuv420p(tv, smpte170m/smpte170m/bt709, progressive), 640x480 [SAR 1:1 DAR 4:3], 30 fps, 30 tbr
, 1k tbn, 180k tbc (default)
    Stream #0:1(eng): Audio: aac (LC), 48000 Hz, stereo, fltp (default)
    Stream #0:2(eng): Audio: ac3, 48000 Hz, stereo, fltp, 224 kb/s
[libx264 @ 00000000005c6c40] using SAR=1/1
[libx264 @ 00000000005c6c40] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX
[libx264 @ 00000000005c6c40] profile Constrained Baseline, level 3.1
[libx264 @ 00000000005c6c40] 264 - core 148 r2762 90a61ec - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.h
tml - options: cabac=0 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 t
rellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 i
nterlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40
 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, mp4, to 'scene.mp4':
  Metadata:
    DESCRIPTION     : This video is about New Project
    TITLE           : New Project
    DIRECTOR        : Big Mac
    COMPOSER        : Big Mac
    encoder         : Lavf57.66.102
    Stream #0:0(eng): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 640x480 [SAR 1:1 DAR 4:3], q=-1--1, 30 fps, 15360 tbn, 30
 tbc (default)
    Metadata:
      encoder         : Lavc57.80.101 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))
Press [q] to stop, [?] for help
frame= 4602 fps=185 q=-1.0 Lsize=   10688kB time=00:03:11.90 bitrate= 456.3kbits/s speed= 7.7x
video:10651kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.349428%
[libx264 @ 00000000005c6c40] frame I:24    Avg QP:20.07  size: 27706
[libx264 @ 00000000005c6c40] frame P:4578  Avg QP:21.30  size:  2237
[libx264 @ 00000000005c6c40] mb I  I16..4: 20.4%  0.0% 79.6%
[libx264 @ 00000000005c6c40] mb P  I16..4:  2.0%  0.0%  2.4%  P16..4: 22.9%  4.2%  1.3%  0.0%  0.0%    skip:67.3%
[libx264 @ 00000000005c6c40] coded y,uvDC,uvAC intra: 47.9% 84.7% 30.2% inter: 4.5% 23.4% 0.6%
[libx264 @ 00000000005c6c40] i16 v,h,dc,p: 36% 26% 11% 26%
[libx264 @ 00000000005c6c40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 23% 13%  5%  7%  8%  5%  5%  3%
[libx264 @ 00000000005c6c40] i8c dc,h,v,p: 39% 26% 27%  8%
[libx264 @ 00000000005c6c40] ref P L0: 82.8% 10.5%  6.7%
[libx264 @ 00000000005c6c40] kb/s:454.58
</code></pre>
","<p>Don't see any frames duplicated. The message you quoted - <em>More than 1000 frames duplicated</em> - is not ffmpeg's analysis of the input. FFmpeg duplicates or drops frames when the input and output framerates differ. You need to use the <a href=""https://stackoverflow.com/a/37089629/5726027"">mpdecimate</a> filter to remove duplicates in the input:</p>

<pre><code>ffmpeg -i input.mp4 -vf mpdecimate,setpts=N/FRAME_RATE/TB out.mp4
</code></pre>
","20959"
"Automatically restart ffmpeg on error from inside bash script","2564","","<p>I use <a href=""https://ffmpeg.org/"" rel=""nofollow"">ffmpeg</a> to livestream an Axis camera feed to <a href=""https://support.google.com/youtube/answer/2474026?hl=en"" rel=""nofollow"">youtube livestream</a>. I have a beefy server with 32 GB of ram and plenty of CPUs doing the processing, but still the stream errors out from time to time.</p>

<p>Would it be possible to change my bash script so it restarts automatically on error? </p>

<p>This is my ffmpeg command, and I run it from a <code>screen</code> session:</p>

<pre><code>ffmpeg -re -thread_queue_size 512 -rtsp_transport tcp -i ""${source_url}"" \
   -acodec libmp3lame  -ar 44100 -ac 2 -acodec pcm_s16le -f s16le -ac 2 \
   -i /dev/zero -vcodec libx264 -preset medium -s 1280x720 -g 6 -r 3    \
   -f flv ""rtmp://a.rtmp.youtube.com/live2/${youtube_stream_key}""
</code></pre>

<p>These are some of the error messages which cause the encoding to error and stop:</p>

<pre><code>Failed to update header with correct duration.
Failed to update header with correct filesize.
</code></pre>

<p>this error is rare since I raised the <code>thread_queue_size</code> parameter:</p>

<pre><code>Thread message queue blocking; consider raising the thread_queue_size option
</code></pre>

<p>I am looking for answers on how to make ffmpeg automatically restart. </p>
","<p>I found <a href=""https://serverfault.com/a/80897/83850"">this method on serverfault.com</a> and it seems to restart ffmpeg on error:</p>

<pre><code>cmd=ffmpeg -re -thread_queue_size 512 -rtsp_transport tcp -i ${source_url} \
   -acodec libmp3lame  -ar 44100 -ac 2 -acodec pcm_s16le -f s16le -ac 2 \
   -i /dev/zero -vcodec libx264 -preset medium -s 1280x720 -g 6 -r 3    \
   -f flv rtmp://a.rtmp.youtube.com/live2/${youtube_stream_key}

until $cmd ; do
        echo ""restarting ffmpeg command...""
        sleep 2
done
</code></pre>
","18499"
"Video tutorials: how to add labels, comments, arrows etc","2561","","<p>I'm planning to make a video tutorial about one application. I would like to make it without speaking, but I would like to add comments to video. </p>

<p>Is there a video software that allows you to add comments, labels, arrows etc.. to clarify and highlight some things in the video.</p>

<p>Wink seemed like one, but it is only available for linux as 32 bit binary.</p>
","<p>My solution was to use Youtube's video editing tools to add annotations and comments. Thanks to other comments!</p>
","5357"
"Is there any way to play corrupted .MOV video file","2547","","<p>Today I recorded presentation I gave to my development team, part of this recording was screencast. Right after I finished, screen capturing tool that I used collapsed and I was left with <a href=""http://yadi.sk/d/pv2PMVjc02Los"" rel=""nofollow"">corrupted .MOV file (160Mb)</a></p>

<p>Here is header:</p>

<pre><code>0000000      0000    1400    7466    7079    7471    2020    0000    0000
0000020      7471    2020    0000    0800    6977    6564    0000    0000
0000040      646d    7461    2021    4003    1c68    4e21    3bff    785a
0000060      d8ac    c326    6d06    bf0e    5c5d    0010    0000    2000
0000100      4c45    9987    9691    8948    9b15    dbfd    000e    0000
0000120      c700    c774    7541    a2c6    3fd5    8a92    7880    677d
0000140      ed57    87ff    b783    00ae    7dfc    7e6f    78f0    fbfe
0000160      0678    6839    eefb    230f    8ca7    0100    2387    977e
0000200      a44f    61e5    f5d5    073e    1d2f    88b7    0400    4bd7
0000220      0e3d    ef59    faef    d1f3    808d    6f00    dd67    1d37
</code></pre>

<p>Is it corrupted?
Is there any way to play such file?</p>

<p>I can not recapture my talk so I really hope you could help me.</p>
","<p>Found tiny tools called <a href=""http://slydiman.narod.ru/rus/mmedia/recover_mp4.htm"" rel=""nofollow"">recover_mp4</a> , emailed it's author who modified it to conform my case. iShowYou suppor has not answer me yet.</p>

<p>There is also cool online tool called <a href=""http://mp4repair.org/pwt6/preview.html"" rel=""nofollow"">mp4repair</a> that is frontend to video repair service. </p>
","4991"
"Display Contents Outside of Composition Boundary?","2527","","<p>I have an icon I'm animating within it's own composition. I want the comp size to be comparable to the final size of the icon, so when I lay it down in other projects, I have a good understanding of where the icon will end up.</p>

<p>As the icon animates in, it extends beyond the size of its composition. Can I set the composition to render display elements outside of its boundary box?</p>

<p>Thanks,
Andy.</p>
","<p>Make the layers in the initial composition 3D layers. Then nest it in the other comp and make the layer it creates 3D, and turn on continuous rasterisation (the sun icon) for that layer. Now elements protruding beyond the edge of the nested comp will appear. </p>

<p>This will not create any problems if your comp is 2D, but it might create problems if you're using 3D layers in either the nested comp or the final one. The other problem is that the bounding box shown in the comp window is dynamic for a continuously rasterised layer, so it will grow to suit the size of all the visible layers.</p>

<p>A less kludgy way of achieving this result would be to make the nested comp big enough to fit all the action, and to add a layer to act as guide to show the size of the final frame. You could create a solid, or a shape layer with an outline or whatever you want. Unfortunately you could not use a proper guide layer, as they do not show up in nested compositions, only in the composition in which they reside. So you'd have to turn off the pseudo-guide layer before rendering.</p>
","12810"
"Preventing flicker when recording live presentations","2524","","<p>Say someone's giving a presentation with the projector image next to them. What I see when recording this is the screen flickers, kind of like top-to-bottom scan refresh that's actually visible. What can be done to fix this? Recording is done with Canon 5DIII.</p>
","<p>The reason why this happen has to do with synchronization, or rather, lack there-of.</p>

<p>The projector displays its video-stream at one frame-rate and starting at one point, while your Canon records at a perhaps different frame-rate and starting when the projector image maybe has drawn part of the image.</p>

<p>The ideal approach is to use a common sync - in this case that is very unlikely to help in any case as the Canon don't have a sync input and the projector probably doesn't have one either.</p>

<p>To reduce the problem you need to make sure as a minimum that the Canon and the projector work on the same frame-rate. Ask someone or ask to look at the projector if possible. The start-point (for when the image lines start to draw) is worse - here you need to cross fingers and hope that when you turn on your camera that it will sync with the vertical blanking. As a rule of thumb: use 60i or 30p fps in America and Japan, 50i or 25p in Europe. However, some projectors can show at an internal rate rather than the one based on the electrical system.</p>

<p>Secondly, lower shutter-time (slower) and of course increase the f-stop (add ND filter if necessary). This will reduce the flicker but you can still get ""bands"" in the video. It will also be more sensitive to movement so here you need to balance/compromise. Too low and you get a strobo-effect instead.</p>

<p>Also try progressive record mode to see if that helps.</p>

<p>However, the best option in cases such as these is to ask for a copy of the presentation file so you can grab stills/video from that in post and super-impose those on the video, or cut to the presentation when editing.</p>

<p>Ask in advance so you can rig your camera in a better position to capture the presenter rather then the screen, but keep track of where in the presentation he is so you get the correct ""slide"" in post.</p>
","5745"
"Video mixer from multiple sources into one","2516","","<p>I want to take two video inputs say one from camera through hdmi/dvi etc and one through laptop(some presentation running in it). One, I want to mix these two video streams and take it out as a single video with both the videos running parallely. Videos running side by side or picture in picture format. Secondly how can I webcast this video?</p>
","<p>Your best bet for this is to use streaming software.  <a href=""http://obsproject.com"" rel=""nofollow"">Open Broadcaster Software</a> for example is able to take a video capture device input (such as using an HDMI capture device and an HDMI camera, or simply using a high end web cam) and combine it with a screen capture (built in feature) and turn it in to a video stream on the fly.  It can then relay this stream to a stream service of your choice, such as YouTube or you can use your own media server such as a RED 5 instance (though running your own server is far more complicated.)</p>
","12791"
"What is the ffmpeg filter equivalent of ""Automatic levels"" for colors?","2487","","<p>Looking at the documentation about filters there seems to be a <a href=""http://www.ffmpeg.org/ffmpeg-filters.html#curves-1"" rel=""nofollow"">curve</a> filter to manipulate color with the following presets:</p>

<ul>
<li>color_negative</li>
<li>cross_process</li>
<li>darker</li>
<li>increase_contrast</li>
<li>lighter</li>
<li>linear_contrast</li>
<li>medium_contrast</li>
<li>negative</li>
<li>strong_contrast</li>
<li>vintage</li>
</ul>

<p>Unfortunately the documentation doesn't mention what these presets do or what their parameters are. I'm looking for a simple option to take a video and auto level colors for all the frames. Does any of these presets do that?</p>
","<p>Here's the details on what the presets do:</p>

<pre><code>[PRESET_COLOR_NEGATIVE] = {
    ""0/1 0.129/1 0.466/0.498 0.725/0 1/0"",
    ""0/1 0.109/1 0.301/0.498 0.517/0 1/0"",
    ""0/1 0.098/1 0.235/0.498 0.423/0 1/0"",
},
[PRESET_CROSS_PROCESS] = {
    ""0.25/0.156 0.501/0.501 0.686/0.745"",
    ""0.25/0.188 0.38/0.501 0.745/0.815 1/0.815"",
    ""0.231/0.094 0.709/0.874"",
},
[PRESET_DARKER]             = { .master = ""0.5/0.4"" },
[PRESET_INCREASE_CONTRAST]  = { .master = ""0.149/0.066 0.831/0.905 0.905/0.98"" },
[PRESET_LIGHTER]            = { .master = ""0.4/0.5"" },
[PRESET_LINEAR_CONTRAST]    = { .master = ""0.305/0.286 0.694/0.713"" },
[PRESET_MEDIUM_CONTRAST]    = { .master = ""0.286/0.219 0.639/0.643"" },
[PRESET_NEGATIVE]           = { .master = ""0/1 1/0"" },
[PRESET_STRONG_CONTRAST]    = { .master = ""0.301/0.196 0.592/0.6 0.686/0.737"" },
[PRESET_VINTAGE] = {
    ""0/0.11 0.42/0.51 1/0.95"",
    ""0.50/0.48"",
    ""0/0.22 0.49/0.44 1/0.8"",
}
</code></pre>

<p>For each preset, each <code>x/y</code> pair maps input <code>x</code> to output <code>y</code>, where the range is <code>0-1</code>. If not set, the filter automatically sets <code>0/0</code> and <code>1/1</code> i.e. input black = output black and input white = output white. Whereas <code>0/1</code> would mean set input black pixels to white in output. Where you have multiple pairs in quotes, separated by commas, the sequence is <code>""R"",""G"",""B""</code> mapping. The interpolation between the points is <em>natural cubic spline</em>. </p>

<hr>

<p>For auto leveling, a crude method would be use the <code>autolevels</code> subfilter of the <code>pp</code> filter.</p>

<pre><code>ffmpeg -i input.mp4 -vf pp=al output.mp4
</code></pre>

<p>What this does is stretch luminance to full range.</p>

<hr>

<p>To check levels before (and after) any adjustment you do, you can generate a RGB parade for a frame like so</p>

<pre><code>ffmpeg -i input.mp4 -vf ""format=rgb24,waveform=c=7:d=parade,scale=1200x512"" -vframes 1 frame1parade.png
</code></pre>

<p>For a specific frame, use</p>

<pre><code>ffmpeg -ss 12.4 -i input.mp4 -vf ""format=rgb24,waveform=c=7:d=parade,scale=1200x512"" -vframes 1 frameNparade.png
</code></pre>
","17944"
"Can't export .mov with premiere pro cs5.5?","2477","","<p>I'm trying to export a mpeg format sequence to .mov (I need it to be quicktime for a ceratin reason). When I choose the format, e.g. quicktime animation or quicktime H.264, It opens an 'encoding' window, and stays on 0% forever.</p>

<p>Exporting other formats works great. What could be the problem?</p>

<p>I'd be happy to include more information as soon as I know which info is relevant. </p>
","<p>Sounds like your Quicktime install has been messed up, possibly by iTunes or something else installing Quicktime.  Try reinstalling Quicktime Player or the whole of Premiere if you can face it...</p>
","8531"
"Kung Fury VHS effect","2426","","<p>I hope everybody seen this <strong>amazing</strong> trailer of upcoming Swedish masterpiece:</p>

<p><img src=""https://i.stack.imgur.com/rTx3x.jpg"" alt=""KUNG FURY""></p>

<p><a href=""https://www.youtube.com/watch?v=72RqpItxd8M"" rel=""nofollow noreferrer"">FULL HD TRAILER</a></p>

<p>So, I wonder, how to achieve this cool video effect. Please, pay attention on the edges:</p>

<p><img src=""https://i.stack.imgur.com/SKTBh.jpg"" alt=""enter image description here""></p>

<p>The image some kinda decomposes into 2 channels.
How to reproduce this?</p>
","<p>Ok, after a research I found the <em>solution</em>. 
Watch this <a href=""https://www.youtube.com/watch?v=5PGkWjqAZ9s"" rel=""nofollow noreferrer"">tutorial</a>.</p>

<p><strong>At a glance:</strong></p>

<ol>
<li>Create <strong>Adobe After Effects</strong> project.</li>
<li>Add <strong>""Channel set""</strong> effect.</li>
<li>Duplicate layer 3 times.</li>
<li>Adjust each layer to show only one channel (Red/Green/Blue).</li>
<li>Set <strong>Blending Mode</strong> as <strong>Screen</strong>.</li>
<li>Add transformation for each layer. Play with rotation/scale.</li>
</ol>

<p>Voila! Check my example.  </p>

<p><strong>Original frame (sorry for low bitrate)</strong>
<img src=""https://i.stack.imgur.com/SovkT.png"" alt=""enter image description here""></p>

<p><strong>RGB shifted</strong>
<img src=""https://i.stack.imgur.com/2tukC.png"" alt=""enter image description here""></p>

<p>I think it's cool as hell!</p>
","13048"
"What is the most compatible compression/Codec to in Handbrake use for video that will be played in Chrome and Safari browsers?","2414","","<p>I am going to re-sample a bunch of videos to be played through web browsers.
(Current video is large, 3 MB for a 4s clip. Hoping to compress by 90%).</p>

<p>Use:  These videos will be played in a web-based eduational app. They'll be up to about 5 seconds and be played in the latest Chrome or (on iOS/Mac) Safari browser.</p>

<p>I'm using Handbrake (.9.9.5530, latest) . These must be playable in <strong>Chrome and Safari</strong> (latest versions) browsers without any pluggins (and would be nice to be playable in the latest FireFox and IE).
(I'm also open to using another free or cheap encoding software that will let me do batch processing.</p>

<p>What are the recommended settings (Video Codec, Quality (and constant or avg bit rate, etc.)
My top priority is compatibility.</p>
","<p>If compatibility is your top priority, then you should include two alternative versions of your video on your website, like in this <a href=""https://developer.mozilla.org/en-US/docs/HTML/Supported_media_formats"" rel=""nofollow"">HTML example</a>.</p>

<p>As for the exact formats I would suggest:</p>

<ul>
<li><strong>H.264 and AAC in MP4</strong>: Chrome, Firefox 22+ on Windows, IE9, Safari 3.1</li>
<li><strong>VP8 and Vorbis in WebM</strong>: Firefox fallback for Mac and older versions on Windows.</li>
<li>(If you need support for even older browsers (IE6-8), and mobile (Android, WP7), you could add <a href=""http://mediaelementjs.com"" rel=""nofollow"">mediaelement.js</a> to your website.)</li>
</ul>

<p>Make sure to put them in that order to make the fallback work. H.264 or MPEG-4 are recommended because they are the only formats to have hardware acceleration on newer devices. </p>

<p>Compression settings: start by using <strong>constant quality</strong> with default settings for video and <strong>128 Kbps VBR</strong> audio. For H.264 make sure to not exceed <strong>Main Profile</strong> and <strong>Level 4.0</strong>. And since Handbrake doesn't do the WebM format, you could try <a href=""http://mirovideoconverter.com"" rel=""nofollow"">Miro Video Converter</a>.</p>
","10225"
"How do you subtitle a video?","2412","","<p>I am making a small video (not really a film) that has muffled/hard to understand dialogue. The video goes for about 2 minutes and I only need to subtitle roughly 2 sentences worth - so it's not a lot. </p>

<p>I'm using Final Cut Pro 7 and I know how to insert text. What I need to know is what font is typically used for subtitles and what size should this font be?</p>

<p>Subsequently, if two people are talking simultaneously, what is the best way to alter the subtitle so the viewer is aware which subtitle belongs to which character?</p>
","<p>Subtitling is a horrible horrible thing, and there are loads of regulations and standards and other bull which can be found <a href=""http://www.ofcom.org.uk/static/archive/itc/itc_publications/codes_guidance/standards_for_subtitling/subtitling_1.asp.html"">here.</a> </p>

<p>As for the font, it really depends on your opinion - if I'm not mistaken Trebuchet Sans and Deja Vu Sans are as close as you can get to the standard DVD fonts. </p>

<p>But all I just said depend on your artistic point of view - if the subtitles makes part of the movie it shouldn't follow the general guidelines - I would strongly recommend using a font that makes sense with the movie (Example: Consolas would be great for a movie like Primer or Tron) and use free style subtitles - like the ones in classic adventure games. </p>

<p>If you're not aware about these games, check out Full Throttle Opening on youtube.</p>

<p>Then again, here's some ideas about people talking simultaneously:</p>

<p>If they are from opposite sexes you can use different colors for each character. 
One character's lines could be on the right side of the screen while the other is on the left. 
You can even use different fonts (not recommended) </p>

<p>But, according to the usual pattern you should identify the characters who are speaking with brackets and simply show both lines at the same time. </p>

<pre><code>[John Doe] I love you Mary. 
[Mary Doe] GTFO, I hate you. 
</code></pre>

<p>But, as mentioned before, I <strong>strongly</strong> recommend creating a unique style for your movie, specially since it's only a few lines. </p>
","3298"
"Canon XF100 live stream -- additional hardware needed","2406","","<p>Question: How to get the XF100 to live-feed into a laptop for streaming?</p>

<p>Details:
I currently have a Canon XF100 HD video camera, and I need to use it for live streaming of some upcoming events.  I also need to do a HD stream (opposed to SD).  The outputs on the camera are HDMI, YPbPr, and SD RCA plugs.</p>

<p>I would think HDMI or YPbPr with red/white audio would be best, however the part I need to know is, how do I get this into a laptop?  A desktop is not an option in this case.   We do have some laptops around, but we also might be able to get a new laptop if needed.</p>
","<p>You need a capture card.  If you have either USB3 or Thunderbolt, then you can use something like the <a href=""http://www.blackmagicdesign.com/products/intensity"" rel=""nofollow"">Black Magic Intensity</a> or <a href=""http://www.matrox.com/video/en/products/mxo2_mini/streaming/"" rel=""nofollow"">Matrox MXO2</a>.  You could also consider a stand alone device that is capable of handling the streaming for you directly, such as the <a href=""http://www.matrox.com/video/en/products/monarch_hd/"" rel=""nofollow"">Matrox Monarch</a> or similar devices that don't have to rely on the computer's processing to send the stream.</p>
","9434"
"How to re-edit and/or duplicate an in-progress edit with YouTube Video Editor","2395","","<p>After uploading a bunch of clips to YouTube, I go into their editor and begin making cuts, combining clips together, etc. The problem is, after I published it, there is no way (that I can see) to go back and open up that project in the Video Editor again to make changes to the edit.</p>

<p>This wouldn't be so bad if I also couldn't figure out how to duplicate an edit project. So for example, if Im in the middle of editing a bunch of clips together, named ""Project 1"", how could I think ""Save as..."" or duplicate that project to ""Project 1b.""</p>

<p>Any ideas? The google help center on this is pretty sparse.</p>
","<p>The YouTube video editor is just not designed for this sort of thing - it has very basic functionality, just enough to allow you to tweak a couple of things. Once the edit is complete it has no ongoing concept of a 'project' - you just have a new video.</p>

<p>I would always recommend editing locally in a proper video editor and then uploading.</p>
","4328"
"Timelapse Countdown Timer in After Effects","2388","","<p>I'm sure someone out there must have done this before, but how do I create a countdown timer with hours, minutes and seconds that starts from a specific time and counts down at 71.928x speed?</p>

<p>71.928x because I took one frame every 3 seconds and I'm playing it back at 23.976fps.</p>

<p>I've tried using text expressions but to no avail.</p>
","<p>Here's what I did in the end (as an expression on the  text layer)</p>

<pre><code>countspeed = 144;
clockStart = thisComp.duration * countspeed * -1;

function times(n) {
    if (n &lt; 10)  return ""0"" + n else return """" + n
}

clockTime = clockStart + countspeed * (time - inPoint);

if (clockTime &lt; 0) {
    minus = """";  clockTime = -clockTime;
} else {
    minus = """";
}

t = Math.floor(clockTime);
h = Math.floor(t/3600);
min = Math.floor((t%3600)/60);
sec = Math.floor(t%60);
ms = clockTime.toFixed(3).substr(-3);
minus + times(h) + "":"" + times(min) + "":"" + times(sec) 
</code></pre>
","5376"
"quickly extract video frames with ffmpeg","2375","","<p>I am using <code>ffmpeg</code> to extract frames from videos. I have the following code thus far:</p>

<pre><code>ffmpeg -i vid.mp4 -vf fps=1/10 images%05d.png
</code></pre>

<p>My issue is that it takes awhile! 6 seconds per image on average. Is there a way to speed this up? I am working with tens/hundreds of videos and I'm wanting to write up a shell script to automate the whole thing, but I don't want it to take days!</p>

<p>I have seen other programs that can do this but they have to literally play through the video to reach the frames to extract. That would be far too slow. Perhaps <code>ffmpeg</code> is doing something like this?</p>

<p>Any ideas how to speed this up?</p>

<p>The videos at hand are 1920x1080 GoPro videos and I'd like full resolution images.</p>

<p>Here's the output from the console:</p>

<pre><code>ffmpeg version 2.7.2 Copyright (c) 2000-2015 the FFmpeg developers
  built with Apple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)
  configuration: --prefix=/opt/local --enable-swscale --enable-avfilter --enable-avresample --enable-libmp3lame --enable-libvorbis --enable-libopus --enable-libtheora --enable-libschroedinger --enable-libopenjpeg --enable-libmodplug --enable-libvpx --enable-libspeex --enable-libass --enable-libbluray --enable-lzma --enable-gnutls --enable-fontconfig --enable-libfreetype --enable-libfribidi --disable-indev=jack --disable-outdev=xv --mandir=/opt/local/share/man --enable-shared --enable-pthreads --cc=/usr/bin/clang --enable-vda --arch=x86_64 --enable-yasm --enable-gpl --enable-postproc --enable-libx264 --enable-libxvid --enable-nonfree --enable-libfdk-aac --enable-libfaac
  libavutil      54. 27.100 / 54. 27.100
  libavcodec     56. 41.100 / 56. 41.100
  libavformat    56. 36.100 / 56. 36.100
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 16.101 /  5. 16.101
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  2.100 /  1.  2.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'vid.mp4':
  Metadata:
major_brand     : mp42
minor_version   : 1
compatible_brands: mp41mp42isom
creation_time   : 2015-09-18 03:38:20
  Duration: 00:01:18.23, start: 0.969178, bitrate: 20388 kb/s
Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuvj420p(pc, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 20004 kb/s, 29.97 fps, 29.97 tbr, 180k tbn, 59.94 tbc (default)
Metadata:
  creation_time   : 2015-09-18 03:38:20
  handler_name    : Core Media Video
  encoder         : GoPro AVC encoder
Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 127 kb/s (default)
Metadata:
  creation_time   : 2015-09-18 03:38:20
  handler_name    : Core Media Audio
[swscaler @ 0x7fdb12000000] deprecated pixel format used, make sure you did set range correctly
Output #0, image2, to 'images%05d.png':
  Metadata:
major_brand     : mp42
minor_version   : 1
compatible_brands: mp41mp42isom
encoder         : Lavf56.36.100
Stream #0:0(und): Video: png, rgb24, 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 0.10 fps, 0.10 tbn, 0.10 tbc (default)
Metadata:
  creation_time   : 2015-09-18 03:38:20
  handler_name    : Core Media Video
  encoder         : Lavc56.41.100 png
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; png (native))
Press [q] to stop, [?] for help
frame=    9 fps=0.2 q=0.0 Lsize=N/A time=00:01:30.00 bitrate=N/A    
video:17647kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown
</code></pre>
","<p>I have no idea if what you're doing is optimal, but ""playing the video"" to extract frames is essentially what has to happen, at least a little, if the frame you're extracting is not an I-frame (most frames are not). </p>

<p>The chosen frame is likely represented as the accumulated differences since the last complete frame, which is what interframe compression does. In order to extract it, a program must read and reconstruct the intervening frames -- not from the beginning of the video, just from the most recent complete frame</p>

<p>Depending on how many frames you need from each video, it might make sense to first convert to an ""all-I"" codec like MJPEG, where every frame stands alone. Then extracting any given frame would be much faster. When done you can delete the converted file.</p>
","16445"
"Can FCPX work with vector graphics?","2356","","<p>I've tried to import <strong>.eps</strong> and <strong>.ai</strong> files unsuccessfully. Are there any vector formats supported by FCPX?</p>
","<p>No.</p>

<p>Or at least not yet. According to this post:
<a href=""https://discussions.apple.com/thread/2323932"" rel=""nofollow noreferrer"">https://discussions.apple.com/thread/2323932</a></p>

<p><strong>EDIT:</strong>
Here's the <a href=""https://support.apple.com/kb/PH12754?locale=en_ZA"" rel=""nofollow noreferrer"">offical supported media formats for FCPX</a> from the Apple support site. It shows that those formats are not supported.</p>
","15842"
"Change sequence frame size Final Cut Express 4","2353","","<p>I have a sequence in Final Cut Express 4 that has 1080p video in it, but when I export it, it exports as 720p video, because the sequence settings are set to 720p video. How do I change the frame size?</p>
","<p>I think Menu -> Sequence -> Settings -> Load Sequence Preset..., then picking choosing a 1080p  preset might do what you want.  You'll probably have to delete and re-render anything you've pre-rendered.</p>

<p>You can set the default sequence preset for all new sequences with Easy Setup under the application menu.</p>
","2299"
"Best way to batch rotate 3000+ h.264 videos?","2350","","<p>I have approximately 3000 short video clips, each about 3 seconds in duration. I need to rotate each of them 90 degrees. What is the best way to do this? I have Adobe Creative Cloud subscription, as well as access to both Windows and Mac machines. Looking for the easiest solution to automate/batch process the rotation of so many videos.</p>

<p>If you were wondering - the videos are part of an e-learning course on lumber. The learner views each randomly selected video, which is just a short clip of a piece of lumber rotating. The videos were shot so that the lumber (2x4s, etc.) were horizontal. But now the client wants the lumber to be vertical in the clips.</p>
","<p>Use <code>ffmpeg</code> and the <a href=""https://ffmpeg.org/ffmpeg-filters.html#transpose"" rel=""nofollow noreferrer"">""transpose"" filter</a> as <a href=""https://video.stackexchange.com/a/15405/3138"">""poor"" suggested</a>. For example, if your files are mp4 and are in the current directory:</p>

<pre class=""lang-bash prettyprint-override""><code>mkdir -p rotated
for f in *.mp4; do ffmpeg -i ""$f"" -vf transpose=2 -c:a copy -metadata:s:v rotate="""" rotated/""$f""; done
</code></pre>

<ul>
<li><p>The transpose value can be ""1"" or ""clock"" for clockwise rotation, ""2"" or ""cclock"" for counter-clockwise rotation. There are other values which will also apply a flip.</p></li>
<li><p>The audio will be <a href=""https://ffmpeg.org/ffmpeg.html#Stream-copy"" rel=""nofollow noreferrer"">stream copied</a> to avoid unnecessary re-encoding.</p></li>
<li><p>The video stream rotation metadata is cleared in this example. Otherwise <code>ffmpeg</code> will copy it from the input to the output resulting in an incorrect rotation for players that pay attention to this metadata (some do, and some don't).</p></li>
</ul>

<p>Another example, more verbose and flexible, and which keeps the rotated file in the same directory with a new name:</p>

<pre class=""lang-bash prettyprint-override""><code>ext=mp4
for f in /some/dir/*.$ext; do
    new=${f/%.$ext/-rotated.$ext};
    ffmpeg -i ""$f"" -vf transpose=2 -c:a copy -metadata:s:v rotate="""" ""$new""
done
</code></pre>
","15411"
"Command Line Tool for Converting Image Sequence to Quicktime .mov Video","2343","","<p>I have large number of numbered .tif files and I'd like to convert them to a .mov/Quicktime video file for further editing in Final Cur Pro X. Therefore no artefacts should be added in the process.</p>

<p>Is there a good way to do this conversion. I'd prefer using a command line tool.</p>
","<p>You can use wonderful <a href=""http://ffmpeg.org/"" rel=""nofollow"">ffmpeg</a> for your desired workflow. There is a guide <a href=""http://www.warpwood.com/wiki/ffmpeg/"" rel=""nofollow"">here on this page</a> but once you have the ffmpeg installed, you can simply cd into your folder with sequence, and type in your terminal:</p>

<pre><code>ffmpeg -f image2 -pattern_type glob -i '*.png' -vcodec qtrle -r 25 -s 1920x1080 test.mov
</code></pre>

<p>To understand the options, you can check the <a href=""http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion"" rel=""nofollow"">documentation</a> but here is the explanation:</p>

<pre><code>When importing an image sequence, -i also supports expanding shell-like
wildcard patterns (globbing) internally, by selecting the image2-specific
-pattern_type glob option.
</code></pre>

<p>In this specific command, <code>-r</code> is for framerate, and <code>-s</code> is our image size. <code>-vcodec qtrle</code> tells ffmpeg to use QuickTime Animation. For ProRes, you can use:</p>

<pre><code>-vcodec prores -profile 2
</code></pre>

<p>Here <code>-profile</code> refers to ProRes Profiles (0, 1, 2, 3), 0 being Proxy, and 3 being High Quality (<a href=""http://forum.doom9.org/showthread.php?p=1538476#post1538476"" rel=""nofollow"">source</a>).</p>

<p>Besides these, if you are going to use ffmpeg, there are lots of detailed bits of commands online, or we can figure out specific commands for your workflow here.</p>
","5578"
"Is it possible to recreate VirtualDub Deshaker functionality in Adobe Premiere/After Effects?","2339","","<p>I'd like to automatically stabilize some poorly-shot shaky footage in Adobe Premiere or After Effects.</p>

<p>VirtualDub Deshaker analyzes frame motion, computes smoothing, adjusts frame position, scale and rotation, and does smart filling of the area outside a frame.</p>

<p>Is anything similarly powerful achievable using Adobe software, possibly via third-party plugins?</p>
","<p>After Effects CS5.5 has the <a href=""http://tv.adobe.com/watch/after-effects-cs55-new-creative-techniques/ae-cs55-warp-stabilizer-instant-gratification/"" rel=""nofollow"">Warp Stabilizer</a> (also check the video on the main <a href=""http://www.adobe.com/products/aftereffects.html"" rel=""nofollow"">Adobe After Effects</a> site).</p>
","1744"
"iMovie- drawing?","2324","","<p>We have some movie, in which we would like to draw live on . So for example, you see an object in the movie, then a circle is being drawn around it , and a few small images jumps out of it.</p>

<p>Can this be done with iMovie ? something like picture in picture effect with the movie and some images ? or should we make the animation separately ?</p>

<p>I know iMovie has this frame by frame effect , but for this to work we would need to have many parts of a circle, then run them frame by frame to the movie- which seems problematic.</p>

<p>Then if we create the animation outside of a movie editor, it wouldn't be transparent - like around an object .</p>

<p>Whats the simplest way ?</p>
","<p>This <a href=""https://graphicdesign.stackexchange.com/questions/36520/how-to-make-a-color-transparent-in-gimp"">answer</a> in the Graphic Design forum explains how to make an image background transparent in either of two ways.  If you have an animation you have captured (perhaps by screen-casting your drawing session into a series of images), you can use an image editing tool that can do batch processing, such as <a href=""http://www.gimp.org/"" rel=""nofollow noreferrer"">The GIMP</a>, to create the transparency type most appropriate for your particular scenario.</p>
","17399"
"What can be done with .CPI or .clpi files?","2320","","<p>In BDAV's M2TS there are .clpi files, in AVCHD the same files are labeled as .CPI. As i understand, some sort of metadata is written into these files. The question is twofold:</p>

<ol>
<li>What metadata exists in these files? (both 'by the standard' and 'de facto', if there are differences)</li>
<li>Are there any free/libre/open-source software tools that are able to extract this metadata from them?</li>
</ol>

<p>An example of such a file is <a href=""https://dl.dropboxusercontent.com/u/1033657/00074.CPI"" rel=""nofollow"">here</a>.</p>
","<p>As <a href=""https://video.stackexchange.com/users/3295/professor-sparkles"">Professor Sparkles</a> has correctly pointed out (I've just checked and confirm)  the <a href=""https://mediaarea.net/en/MediaInfo"" rel=""nofollow noreferrer"">MediaInfo</a> tool can extract some metadata from these files. Here is a sample output from one of mine:</p>

<pre><code>General
Complete name                            : /Volumes/CAM_SD/PRIVATE/AVCHD/BDMV/CLIPINF/00119.CPI
Format                                   : Blu-ray Clip info
File size                                : 502 Bytes

Video
ID                                       : 4113 (0x1011)
Format                                   : AVC
Format/Info                              : Advanced Video Codec
Width                                    : 1 920 pixels
Height                                   : 1 080 pixels
Display aspect ratio                     : 16:9
Frame rate                               : 50.000 fps

Audio
ID                                       : 4352 (0x1100)
Format                                   : AC-3
Format/Info                              : Audio Coding 3
Channel(s)                               : 2 channels
Sampling rate                            : 48.0 KHz
Compression mode                         : Lossy
Language                                 : English

Text
ID                                       : 4608 (0x1200)
Format                                   : PGS
Language                                 : English
</code></pre>

<p>(But note that it can output more details in the --full mode.)</p>

<p>Also, it may be helpful to note that the AVCHD standard is proprietary and its spec is not publicly available. Attempts to reverse-engineer its components exist, and one of them is in this forum thread (from 2008): <a href=""http://forum.doom9.org/showthread.php?t=141361"" rel=""nofollow noreferrer"">http://forum.doom9.org/showthread.php?t=141361</a></p>
","15094"
"Can you measure the width of a character in an AE expression?","2319","","<p>I have a type effect that requires a couple of layers, one with the type, and another moving behind the letters as they appear (simulating the little text bubbles you get when typing on an iPhone). It's very tedious animating the position of the bottom layer to match the text, so I'm wondering if it's possible to use any javascript tools to measure the characters and drive the position value.</p>

<p>Currently I have an expression driving the opacity like this, to automatically turn the background off for spaces:</p>

<pre><code>theOffset=thisComp.layer(1).text.animator(""Animator 1"").selector(""Range Selector 1"").offset;
if (thisComp.layer(1).text.sourceText.substr(theOffset, 1) == "" "" ){0} else {100}
</code></pre>

<p>and another that folows the text animator range selector so that I can retime if I want without having to nudge keyframes till doomsday:</p>

<pre><code>theOffset=thisComp.layer(1).text.animator(""Animator 1"").selector(""Range Selector 1"").offset;
try {
    transform.position.key(theOffset + 1).value
}
catch(error){[-10000,0]}//effectively hide it on errors (like the index going out of bounds)
</code></pre>

<p>but this relies on me going through and creating keyframes to match the spacing of all the letters in the text, and frankly I'd rather be doing something else with the time that takes. so I'm trying to find a string method that I could use like this:</p>

<pre><code>xOffset = thisComp.layer(1).text.sourceText.substr(1, theOffset).getTextWidth()
</code></pre>

<p>but <strong>getTextWidth</strong> obviously isn't a valid method. Is there a JS method that can get the size of a string on screen? There seems to be several solutions for this when the client is a web browser, but I can't find any that work for AE expressions.</p>
","<p>Looks like Adobe have come to the party on this one. A new update as of After Effects CC 2014.2 (13.2) has a sourceRectAtTime() method that does exactly what I'm wanting. To quote <a href=""http://blogs.adobe.com/aftereffects/2014/12/after-effects-cc-2014-2-13-2.html"" rel=""nofollow"">their blur</a>b:</p>

<blockquote>
  <p>You can now read the rectangle bounds of a layers content, including
  the corrected bounds of a text layer, for any time in a composition.
  The sourceRectAtTime() method from the After Effects scripting API is
  now accessible in expressions as a read-only layer object attribute.</p>
</blockquote>

<p>Yee-hah!</p>
","13311"
"remove black frames from vob files with ffmpeg","2318","","<p>I have a few <code>.vob</code> files with ""black frames"" (it seems that an issue during the acquisition from VHS did that).<br>
I can get where these black frames occur with the command</p>

<pre><code>ffmpeg -i source.vob  -vf blackdetect=d=0:pic_th=0.70:pix_th=0.10 -an -f null - 2&gt;&amp;1
</code></pre>

<p>which gives me </p>

<pre><code>[...]
[blackdetect @ 0x7fab5b40a3e0] black_start:137.68 black_end:137.72 black_duration:0.04
[blackdetect @ 0x7fab5b40a3e0] black_start:161 black_end:161.04 black_duration:0.04
[blackdetect @ 0x7fab5b40a3e0] black_start:181.96 black_end:182 black_duration:0.04
[...]
</code></pre>

<p>(black_duration is always 0.04s for some reason)</p>

<p>I've built a script that cuts the parts between black frames and creates one file for each.<br>
basically, for the three lines above, it runs:  </p>

<pre><code>ffmpeg -i ""/Users/simon/Documents/projects/encodage VHS/source.vob"" -ss 00:02:17.720 -t 23.28  -f avi -c:v libx264 -c:a libmp3lame -crf 0 -preset veryslow -vf ""setpts=PTS-STARTPTS"" 13.mpeg;
ffmpeg -i ""/Users/simon/Documents/projects/encodage VHS/source.vob"" -ss 00:02:41.040 -t 20.92  -f avi -c:v libx264 -c:a libmp3lame -crf 0 -preset veryslow -vf ""setpts=PTS-STARTPTS"" 14.mpeg;
</code></pre>

<p>then I merge those files with the concat plugin of ffmpeg.</p>

<p>My issue is: I'm sure the values I use to cut are right, but running the blackdetect filter again, I can see black frames at the beginning of some fragments.</p>

<p>Do you know why I'm still getting those black frames ? Is there a better way to remove them than cut and concat? I'm using a Mac.</p>
","<p>This is the script I used to solve this.
1/ It finds black frames and put them in a file
2/ It reads the file and compute a (really long) command line which will remove those frame with the ""select"" filter.</p>

<p>It's not beautiful, but it gets the job done!</p>

<pre><code>ffmpeg -i ""$1"" -vf blackframe -an -f null - 2&gt;&amp;1 | ack  ""(?&lt;=frame:)[0-9]*(?= )"" -oh &gt; blacks.txt


printf ""ffmpeg -i \""$1\""""
printf "" -f avi -c:v libx264 -c:a libmp3lame -crf 23 -preset veryslow""

printf "" -vf \""select=""

cat blacks.txt | while read a; do
printf ""not(eq(n\,$a))*""
done
printf ""\""""
printf "" ""
printf ""$2""
printf ""\n""
rm blacks.txt
</code></pre>
","10631"
"Windows Software to record from Webcam & Screen","2318","","<p>I'm looking for software which can simultaneously record video from my computer's web cam at the same time as recording the content displayed on screen.</p>

<p><strong>More Detailed Requirements</strong></p>

<ul>
<li>Run on Windows 7</li>
<li>Be cheap (&lt; ~70 USD)</li>
<li>Be simple (i.e. easy to learn for basic recording features at least)</li>
<li>Record videos up to 3 hours in length.</li>
<li>Quality is not overly important so long as it's possible to see and hear what's going on.</li>
<li>The video will be published as a file; we don't need any broadcast/streaming facilities</li>
<li>Basic post production video editing tools (examples below) would be useful, though not essential
<ul>
<li>adjust audio levels in sections of the video </li>
<li>cutting and splicing facilities </li>
<li>ability to switch between cam, screen, and composite feeds</li>
</ul></li>
</ul>

<p><strong>Background</strong></p>

<p>My specific use case: we're about to have a number of software demos for various products and would like to record these.  We can then make these videos available to the end users so they can see what was said &amp; get an idea of what the products looked like without them needing to sit through each demo (saving them time, easing the logistics of organising the demos and keeping number of people present down to avoid too many tangents).  We're likely to also use this tool for some user training videos once the eventual products are selected.</p>

<p><strong>Research</strong></p>

<p>So far I've found a number of potential solutions, but have no personal experience with any.  Reading their sites though they claim to record from various input sources it's not clear if simultaneous input (i.e. cam &amp; screen) recording is possible, or if there are limits on video length.</p>

<ul>
<li>NCH Debut:   <a href=""http://www.nchsoftware.com/capture"" rel=""nofollow"">http://www.nchsoftware.com/capture</a></li>
<li>Open Broadcast Software:   <a href=""http://obsproject.com"" rel=""nofollow"">http://obsproject.com</a></li>
<li>Many Cam:   <a href=""http://manycam.com"" rel=""nofollow"">http://manycam.com</a></li>
<li>XSplit:   <a href=""http://www.xsplit.com"" rel=""nofollow"">http://www.xsplit.com</a></li>
</ul>

<p>Any recommendations would be much appreciated; thank-you in advance.</p>
","<p>Personally I use <a href=""http://www.techsmith.com/camtasia.html"">Camtasia Studio</a> and <a href=""http://www.techsmith.com/snagit.html"">SnagIt</a>. TechSmith, the vendor, has created their own codec for their software which IMO works excellent.</p>

<p>You will probably find Camtasia a bit stiff when it comes to the price compared to your budget, but I mentioned SnagIt for this reason although it's primarily to capture screenshots it do have built-in video recording as well using the same codec as its big brother.</p>

<p>It's simpler and you will probably want to convert the result and edit it in a third-party editor (Camtasia Studio comes with its own editor which is in part why the price is higher).</p>

<p>I never recorded 3 hours but I did more than an hour which was without problem. The key factor here is to have enough space on the disk you record to.</p>

<p>My 2 cents..</p>
","9569"
"How to make a follow the bouncing ball video?","2293","","<p>I found easier to teach some score by reading it with the student while playing, and I saw that many people online use this priciple on videos as well:</p>

<p>Do you know how can I make that kind of karaoke-style animations for scores?
Is there a software better suited for it?</p>

<p>For reference, I'll put here a link to a video with the goal I'd like to reach:
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/NfnKwDNd6oY?start=76""></iframe>
            </div></div></p>
","<p>There's a tutorial on how to make a bouncing ball karaoke-style lyric follower using After Effects here:</p>

<p><a href=""http://aescripts.com/beat-assistant/"" rel=""nofollow"">http://aescripts.com/beat-assistant/</a></p>

<p>The example you give should be simpler, as the ball only bounces on each quarter-note beat -there aren't any lyrics to worry about.</p>
","13446"
"FFmpeg : How to replace video background color with transparency? (while capturing)","2272","","<p>I want to capture a window and overlay it on another video.</p>

<p>This is my command:</p>

<pre><code>ffmpeg -i MainVideo.avi 
-f gdigrab -framerate 25 -video_size 300x200 -i title=""MyWindow"" 
-filter_complex ""[0]setpts=PTS-STARTPTS[b];[b][1:v]overlay=(main_w-overlay_w):main_h-overlay_h[v]""
-map ""[v]"" -c:v libx264 -r 25  out.mp4
</code></pre>

<p>background of my window is black.</p>

<p>How can I replace this black color with transparency?</p>

<p>is this possible by ffmpeg? (I think it is possible by chromakey/colorkey filters but I'm not sure, and I don't know how to use those filters)</p>

<p>Thanks.</p>

<p><strong>UPDATE:</strong></p>

<p>Suppose this is a frame of video that I want:
<a href=""https://i.stack.imgur.com/zAuuX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zAuuX.png"" alt=""enter image description here""></a></p>
","<p>GDIgrab supplies a RGB feed so the colorkey filter can work here.</p>

<p>Try</p>

<pre><code>ffmpeg -i 1.mp4 -f gdigrab -framerate 25 -video_size 300x200 -i title=""MyWindow""
-filter_complex
""[1]split[m][a];
 [a]format=yuv444p,geq='if(gt(lum(X,Y),0),255,0)',hue=s=0[al];
 [m][al]alphamerge[ovr];
 [0][ovr]overlay=(main_w-overlay_w):main_h-overlay_h[v]""
-map ""[v]"" -c:v libx264 -r 25 out.mp4
</code></pre>

<p>You may have to use a value of near 0, if 0 doesn't capture all the transparent area.</p>
","19703"
"Mono to Stereo using FFMPEG","2272","","<p>I have an unusual problem that I hope someone can help me solve.
I have 45 video files that I'm batch encoding as MP4.
Some of those files have their audio channel position set to Front L R and others have have it set to Front C.
However, the distribution system which these videos are sent to (and which I have no control over) does some re-processing of these videos.
All the videos that have their Channel Position set to Front C are coming back with distorted audio. </p>

<p>My question is:</p>

<p>Does anyone know how to encode a video with ffmpeg and force the audio channel positions to be Front L R?
(I have carefully studied this page (<a href=""https://trac.ffmpeg.org/wiki/AudioChannelManipulation#monostereo"" rel=""nofollow noreferrer"">https://trac.ffmpeg.org/wiki/AudioChannelManipulation#monostereo</a>) but it was not obvious what the solution might be)</p>

<p>Thank you!</p>
","<p>If you run <code>ffmpeg -layouts</code>, you will see that stereo indicates the presence of two channels, one assigned as Front Left and the other as Front Right. If you use <code>-ac 2</code> in your encode, ffmpeg will create a stereo layout output by default. FFmpeg does not display the individual channel assignments in the readout of the file.</p>
","20119"
"Record video without Live View / Turning off Monitor while Recording","2263","","<p>I am a brand new amateur photographer, so I really needing a lot of input from other users, for example..</p>

<p>I just notice yesterday while recording some video with my Nikon D5200 that I can't record video using only the eyecup view !</p>

<p>Only when I was using the live view mode, I could press the record button and record video, why is this happening?</p>

<p>My problem is that, the monitor is one of the camera functions that drain most of my battery!!! I record three videos of 20 minutes and I it's dead, because the screen is always on.</p>

<p>So, actually I have two questions:</p>

<p>1) Can I record video using only the eyecup mode (without live-view)?</p>

<p>2) If it's not possible, then, can I use live view mode but just turn off the LCD monitor?</p>

<p>Thank you so much for your help!
Looking forward for your answer, I am loving learning photography, still need a lot of information :)</p>
","<blockquote>
  <p>I can't record video using only the eyecup view</p>
</blockquote>

<p>D5200 is an SLR, that is a <em>single-lens reflex camera</em>.</p>

<p>As opposed to mirrorless cameras, SLRs use a mirror which sends the image which passes through the lens to the top where, with the help of a pentaprism, the light is redirected to the viewfinder.</p>

<p>When you shot a photo, the mirror is raised, letting the light through. Behind this mirror is situated the film, or, in digital SLRs, the sensor.</p>

<p>Depending on the position of the mirror, the light arrives to either the sensor, or the viewfinder. Never both. You can, by the way, see it very easily: shot a photo at 1/2 of a second while looking through the viewfinder: you'll see that it becomes dark for half a second.</p>

<p>When you're recording a video, the mirror is raised during the entire recording: the light which passes through the lens hits directly the sensor. The same thing happens with Live Viewthe only difference with video recording is just that the signal received by the sensor is not recorded to the memory card.</p>

<p>Could you disable the LCD screen when shooting a video? I don't remember seeing anything like that in the options, and it wouldn't be particularly useful, since it's kinda useful to be able to watch what you're shooting. However, if you're using a remote controller (such as a tablet or a PC), then the LCD screen on the camera is turned off.</p>

<p><strong>Why wouldn't the mirror move up and down constantly when shooting the video, letting you look through the viewfinder?</strong></p>

<p>There are a lot of reasons for that, including the following:</p>

<ul>
<li><p>The image will look darker, since a part of the light will need to reach the sensor.</p></li>
<li><p>Flipping the mirror up and down twenty-five to sixty times per second would make a lot of noise, and lead to camera shake.</p></li>
<li><p>Looking through the viewfinder when shooting a video isn't very practical.</p></li>
</ul>

<p>The last point, specifically, leads me to another aspect. I imagine that your original question is motivated by the fact that it's difficult to view anything on the LCD screen in a bright day. For that, different accessories exist. Most popular ones are <a href=""https://www.bhphotovideo.com/c/product/682687-REG/Zacuto_Z_FIND_PRO2_Z_Finder_Pro_2_5x.html"" rel=""nofollow noreferrer"">the viewfinder which fixes itself directly on the LCD screen</a> like that:</p>

<p><a href=""https://i.stack.imgur.com/gVGDQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gVGDQ.jpg"" alt=""enter image description here""></a></p>

<p>as well as the external monitors which look like that:</p>

<p><a href=""https://i.stack.imgur.com/xKNyQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xKNyQ.jpg"" alt=""enter image description here""></a></p>

<p>There is a much less expensive alternative: use a tablet as an external monitor. However, the quality of a professional external monitor is much higher than a tablet, especially noticeable when you have to work outside on a sunny day.</p>

<blockquote>
  <p>My problem is that, the monitor is one of the camera functions that drain most of my battery!!! I record three videos of 20 minutes and I it's dead, because the screen is always on.</p>
</blockquote>

<p>No, it's not. While the LCD screen consumes some power, what actually drains the battery is the recording itself, i.e. the processing of the light received by the sensor into a video file on your memory card.</p>

<p>Note that there are accessories for that too. <a href=""http://www.nikonusa.com/en/nikon-products/product/power-adapters/eh-5b-ac-adapter.html"" rel=""nofollow noreferrer"">Power adapters</a> make it possible to use the camera without battery, so if you're shooting videos in a studio, this could be an option.</p>

<p>For some DSLRs, Nikon also sells battery grips which are connected to the bottom of the camera. It looks like this:</p>

<p><a href=""https://i.stack.imgur.com/vDEqv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDEqv.jpg"" alt=""enter image description here""></a></p>

<p>However, it seems that Nikon doesn't have grips for D5200. There are third-party ones, but be careful when purchasing them, since low quality ones can damage your camera. High quality ones are usually rather expensive ($200 to $600).</p>

<p>Another solution consists of buying spare batteries. When one is depleted, you use another one, while recharging the first. Having three or four batteries is not that unusual.</p>
","19907"
"Black frame strobing during CC export","2215","","<p>I have a sub-3-minute sequence in Premiere CC, containing a single 4k DCI 23.98 AVC stream from a Sony FS7, with a couple static PNGs and 29.97 Quicktime Animation lower thirds. </p>

<p>I need to export to a quicktime wrapper around some sort of mpeg-4 codec; I've tried several configurations of H.264 and MPEG-4 with the same result.</p>

<p>My timeline plays and looks normal, but after export, I see the following result via Imgur videotogif:</p>

<p><a href=""https://imgur.com/I27CHk5"" rel=""nofollow noreferrer"">http://imgur.com/I27CHk5</a></p>

<p>Because I know some of you will ask, here is a screenshot of my export settings:</p>

<p><a href=""https://i.stack.imgur.com/duFTl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/duFTl.png"" alt=""enter image description here""></a></p>

<p>Changing resolution, codec (within variants of Quicktime), with/without previews, max render quality - all has no effect on the strobing issue.</p>

<p>Anyone have any thoughts on what the problem might be here?</p>

<p>I'm having the same issue with a visually similar sequence in the same project for the same client, and had it once before as well (you guessed it, same client, same content specs). I've never experienced this ever before so I'm inclined to think there's some little gremlin in there somewhere - but I'm dashed to figure out what it is. </p>

<p><strong>[edit]</strong></p>

<p>I've also verified that the issue occurs when exporting to P2/MXF as well as ProRes.</p>

<p>Per @Mulvya's question about source properties, here's a screencap of the properties window on one of the clips. All the footage was shot in the same session, same camera, etc, and should be completely identical:</p>

<p><a href=""https://i.stack.imgur.com/5sq37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5sq37.png"" alt=""enter image description here""></a></p>
","<p>Are you using the ""Mercury Playback Engine GPU Acceleration (CUDA)"" as your renderer inside of media encoder? I've read about and had a few cases where software only rendering can cause similar issues.</p>
","18426"
"How to show audio keyframes in Adobe Premiere CC?","2210","","<p>How to show audio keyframes in Adobe Premiere CC?</p>

<p><code>Show Audio Keyframes</code> option is <code>ON</code> although I can't see any keyframes on the timeline. </p>

<p>I am sure, keyframes are there, since volume controls change in <code>Audio Clip Mixer</code> as I move play head.</p>

<p><a href=""https://i.stack.imgur.com/tnIcW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tnIcW.jpg"" alt=""enter image description here""></a></p>
","<p>You need to expand the track to be large enough to see the waveforms and overlaid keyframes.  Mouse over the horizontal line that divides A2 from A3, drag downward, and your waveforms and keyframes should be revealed.</p>

<p>If you right-click in the waveform area, you have the option as to which keyframes it will display in the window.  By default it's Volume:Level, but it could be Panner:Balance (or others).</p>
","17253"
"FFmpeg MP4 to FLV too slow","2200","","<p>I am converting .mp4 files to .flv using ffmpeg in centos. But the conversion time is extremely too slow! It takes about 1.5 hours to convert 500MB .mp4 file into .flv. I am using a linux vps with 2 GBs dedicated RAM.</p>

<p>I am using the following code:</p>

<pre><code>ffmpeg -i source.mp4 -c:v libx264 -ar 22050 -crf 28 destination.flv
</code></pre>

<p>Is there any way I can fasten the conversion time? Any help/suggestions will be appreciated.</p>
","<p>Given your are using a VPS this is no suprise (guessing you only have a single core with 1-3GHz) and you won't be able to crunch the conversion down to a few minutes. You can try using <code>-c:v libx264 -presets ultrafast</code> but I'd guess it will still take you about 45-60 minutes to encode.</p>

<p>Also remove the <code>-crf</code> option when using a preset (they choose a default that produces a very good balance between speed and quality, I only recommend to modify that value when you know what it really does)</p>

<p>Be advised that the quality you get from that preset is not exactly great. If you NEED fast encoding speed over everything then go for it, if it's not THAT important I'd suggest using <code>fast</code> oder <code>faster</code> instead of the <code>ultrafast</code> preset.</p>
","12151"
"Should I just get a gaming PC for audio/video production?","2195","","<p>I was looking around for computers aimed at AV production, and there doesn't seem to be many. It seems you largely have the option of SOHO PCs, business machines, or gaming machines. </p>

<p>Is a gaming machine optimal for AV production? What aspects of a gaming machine are unnecessary for AVP? What aspects are missing?</p>

<p>I plan on mostly doing audio production, with some graphic design, and maybe a little video production (mostly 2D).</p>
","<p>I researched this when building my own computer for HD video editing about a year ago - I don't remember all of my sources, but there's <a href=""http://forums.adobe.com/thread/433549"" rel=""nofollow"">a hardware guide</a> on the Adobe Primiere Hardware Forum (very helpful place) that was a big help. That guide is several years old, so the specific hardware recommended is out of date, but the principles are still there.</p>

<p>Basically, most gaming PCs are going to have a more powerful graphics card than you need and less RAM than you'll want (and possibly inadequate hard drives). It's better to build yourself, but if you're going to buy an off-the-shelf model, a gaming PC will probably serve you better than one that's not (since most of the manufacturers' high-end stuff is geared toward gamers). There are companies that build PCs specifically for video editing, but they are VERY expensive, especially compared to what you can do yourself. I built mine for around $1000, which is what I would've paid for the Dells and HPs I was looking at, but I was able to get exactly what I wanted.</p>

<p>I don't know about audio, but for video, I'd recommend:</p>

<ul>
<li>The fastest processor that fits your budget (use <a href=""http://www.cpubenchmark.net/high_end_cpus.html"" rel=""nofollow"">benchmark scores</a>, not clock speeds, to compare)</li>
<li>Fast hard drives. An ideal setup from what I've read (and based on <a href=""http://ppbm5.com/DB-PPBM5-2.php"" rel=""nofollow"">Premiere benchmarks</a>) is to have a small SSD for your boot drive and a RAID array for the actual editing. Those are fairly expensive, but at minimum, you should have 2 hard drives, one to boot and one to work in (and do NOT get 5400 RPM models).</li>
<li>A mid-level graphics card. I've got a GeForce GTX 550 Ti and it handles my 1080p/60fps video just fine.</li>
<li>RAM will depend on what software you're using (as will all of these, but RAM more so). For Premiere CS5+, for instance, you probably want at least 12 GB; I use Lightworks, which is still a 32 bit program as of this writing and therefore can't use more than 4 GB.</li>
</ul>
","4725"
"Why there are no full lenght independent cartoon animated movies created in Blender?","2184","","<p>I have been searching for an indie movie that is completely done in Blender, but the longest animation I have found was about 15 minutes long.</p>

<p>Are there any full length animated cartoons/movies (~90 minutes)?</p>

<p>I have found only this:
<a href=""http://www.blender.org/features-gallery/movies/"" rel=""nofollow"">http://www.blender.org/features-gallery/movies/</a></p>

<p>You can tell me also commercial titles if there are any, but I haven't found any so I am interested if they are any or not?</p>

<p>Most of the movies from big studios are using Maya, so I am curious if any indie companies are using Blender or not, because I think that if a group of enthusiasts invest 2 or 3 years in something like Big Buck Bunny (it's only 10 minutes long, but created in Blender and I think it is really nice ely done, especially if you consider it was done about 5 years ago): <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/YE7VzlLtp-4?start=0""></iframe>
            </div></div> </p>
","<p>Simply put, to make a full length movie requires a lot of man hours and generally a decent budget.  It becomes more cost effective to get real animation software when dealing with a full length film than trying to struggle through with something like Blender.</p>

<p>Don't get me wrong, Blender is great for what it is as a free product, but, at least as of the last time I was able to seriously work with it a few years back, it was nothing at all compared to the capabilities of one of the high end systems like Maya or SoftImage.</p>

<p>Both the ease of working with the tools and the improved collaboration workflows make it so that using a commercial system may likely be cheaper in the long run due to time saved for a long project like a full length movie.  It's certainly easier, even if costs are just close to comparable.</p>

<p>It's also worth noting that Maya isn't the only platform used by the big studios.  Several use their own in-house systems, Maya, SoftImage and Cinema4D are also all used pretty extensively.  3D Studio Max is also sometimes used, though it has found more of a niche in the video game market.</p>
","8642"
"Layer dimensions in after effects","2172","","<p>How can I <strong>find the dimensions in pixels</strong> of a layer in adobe after effects.</p>
","<p>To find the pixel dimensions of a layer you can use this expression:</p>

<blockquote>
<pre><code>layer.sourceRectAtTime(t = time, includeExtents = false)
</code></pre>
  
  <p><strong>t</strong>: the time index, in seconds. A floating-point value.</p>
  
  <p><strong>includeExtents</strong>: true to include the extents, false otherwise. Extents apply to shape layers only, increasing the size of the layer
  bounds as necessary.</p>
</blockquote>

<p>This is only for AE version > 13.2 (CC 2014.2). See the announcement <a href=""http://blogs.adobe.com/aftereffects/2014/12/after-effects-cc-2014-2-13-2.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>For earlier versions <a href=""https://video.stackexchange.com/questions/13001/can-you-measure-the-width-of-a-character-in-an-ae-expression"">this question</a> is relevant, in the answers there is a link to an expression that achieves the same result, albeit a lot slower.</p>
","14557"
"Adobe Premiere Pro / After Effects Hard Drive Scratch Disk and Cache set up in temporary situation.","2148","","<p>I've exhausted all options before asking for opinions in trying to figure this out, and I'm running out of time by doing all possible combinations and running benchmarks, or setting up projects to see which hard drive set up is working best as I have a job that needs to finished today, with what I have available. </p>

<p>So my usual hard drive set up for Premiere Pro and After Effects is not possible right now, as my main SSD is not working, so I'm left with the hard drives I have left to try and put together a system that will get me through the day as pain free as possible. </p>

<p>I'm making a short introduction to an interview using Premiere Pro and After Effects (not necessarily at the same time for this project), and working with Raw footage converted to ProRes proxies to save me some precious space and speed while editing. I'm running on Mac (El Capitan) and using CC 2015 if that matters to anyone.</p>

<p>Currently for available hard drives at my disposal I have-</p>

<ul>
<li><p>My internal 2 TB SSD. (System drive for OS, programs, etc.)</p></li>
<li><p>A RAID 0 configured set of 7200 RPM 1 TB (combined size) Seagate's connected through USB 3. I'm getting speed test results of around 147MB/s Write and 148MB/s Read. </p></li>
<li><p>A cheap 1TB Western Digital My Passport through USB 3 that is getting speed test results of around 60MB/s Write and 75MB/s Read, although I've seen the write speed jump all the way up to 75, and all the way down to 35, so I've never sure. </p></li>
</ul>

<p>So thats what I'm working with for at least the rest of the day. How would you configure your Scratch Disks in this situation? </p>

<p>I was thinking having my Project files on the RAID 0 HD, along with the Source Video, while the Cache ran off the internal SSD. That leaves the Video and Audio previews. Will the My Passport be fast enough to handle them or should I use either the internal or RAID 0 for them as well? </p>

<p>Should I move the cache to the RAID 0 and have the Source Video files and Previews run off of the Internal SSD? </p>

<p>I can't decide so some input would be amazing. Basically the question is, what needs the most and least speed- the Cache, the Previews, or the Source Video files? </p>

<p>Thanks for your help!
-P.J. </p>
","<p>Personally, I would ditch the usb3 drive for editing. Assuming you can't buy a third drive I would do your setup like this. </p>

<p>Do a two drive setup.
C: OS and media cache
d: (raid) Media, projects, previews and exports.</p>

<p>I've always followed the advice here and its worked great for me.</p>

<p><a href=""https://forums.adobe.com/servlet/JiveServlet/showImage/2-2906955-31395/Guideline+Disks.jpg"" rel=""nofollow"">https://forums.adobe.com/servlet/JiveServlet/showImage/2-2906955-31395/Guideline+Disks.jpg</a></p>
","18479"
"repeat crossfade in a sequence of images","2148","","<p>I am trying to animate a series of drawings. I have 10 drawings of 10 states of an object and I would like to have a crossfade between each states and this repeated for about 2 min back and forth.</p>

<p>I am using After effects and I have imported the image sequence, made a composition (30fps, 2 min long) changed the time remapping so each drawing is displayed during 15 frames (0.5 seconds). </p>

<p>My issue is that the change between drawings snaps and I need a crossfade between them. The other option I can think of to is create a composition with all the drawings and manualy create key images for opacity but this is going to be very long as it has to go on for 2 minutes.</p>

<p>Is there a way to crossfade each drawing into the next one in Affter effects?</p>
","<p>There's always more than one way to do things in AE:</p>

<p><strong>Method1</strong></p>

<p>Import the drawings as separate source files, rather than as an image sequence (in the import dialog uncheck the image sequence checkbox). Once you've got all your images imported select them all and drag them into a new composition. </p>

<p>They will appear as a stack of layers with only the top layer visible, but don't fret. Next step is to trim them to the length you need. The easiest way to do this is to select them all and move the playhead to the last frame you want (frame 15 in your case) and hit <kbd>alt/option</kbd><kbd>]</kbd> which is the shortcut for trimming the end of a layer.</p>

<p>Now right click one of the selected layers and from the contextual menu choose <strong>Keyframe Assistant > Sequence Layers</strong>. To get a crossfade you need to check the overlap box, and use the <strong>Dissolve Front Layer</strong> transition (I have never been able to work out why you would <em>ever</em> use the <strong>Dissolve front and back layers</strong> option, because it means that halfway through both layers will be 50% transparent, meaning that the background will show through). You might have to go back and add extra frames to your layers to allow for the overlap, so if you want a 5 frame overlap on a 15 frame layer you'll need to make it 20 frames.</p>

<p><img src=""https://i.stack.imgur.com/XivGC.png"" alt=""enter image description here""></p>

<p>This is the built-in tool for doing exactly what you want to do.</p>

<p><strong>Method 2</strong></p>

<p>Do what you're doing, but make sure that frame blending is turned on for your image sequences. It's the little filmstrip icon in the switches column (you may have to hit <kbd>f4</kbd> to toggle the visibility. </p>

<p><img src=""https://i.stack.imgur.com/l6xLd.png"" alt=""enter image description here""></p>

<p>There are two values for frame blending, you want the first one, which is basically dissolving between frames  the second one does a kind of morph between successive frames which can do a good job in some circumstances, but can look very weird in others.</p>

<p>Now because frame blending is a bit computationally expensive, AE doesn't show it in the comp window by default. You'll have to turn it on to see it, and you do that by hitting the filmstrip button at the top of the layers area in your timeline:</p>

<p><img src=""https://i.stack.imgur.com/fHwDu.png"" alt=""enter image description here"">
Click it and it will go blue, and you'll see your frames blending.</p>

<p>Although this might be easier to set up, it does have the disadvantage that the frames are going to be constantly dissolving, with only one frame each where they are 100% from the individual drawing.</p>
","14940"
"Create a random number each second","2141","","<p>In After Effects I am trying my hands at this <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/KLsk3lGbiVc?start=0""></iframe>
            </div></div> and would like to add a random number between 45 and 55 in the upper right corner, which should change each second.</p>

<p>If I use Math.round(random(45,55)) I am getting pretty close, but the number updates maybe 20 times a second making it unusable. Is there a way to control how often a new number is generated? Is there another way to do this?</p>

<p>Just to clarify, I want sort of what is already in wiggle, where wiggle(1,30) wiggles once a second.</p>
","<p><code>random</code> updates at the sub-frame level (so its value actually changes even during a frame, for example when motion blur is on. You can see this as you zoom in on the graph editor of a property with a random value, it keeps changing as it zooms and the time is sub-sampled more).</p>

<p>To force it to update only once a second you need to use the <a href=""http://adobe.ly/1DZW3md"" rel=""nofollow noreferrer""><code>seedRandom</code></a> function, with the <code>timeless</code> attribute set to <code>true</code>, and a <code>seed</code> that changes every second. Using <code>Math.floor(time)</code> will return a value that increases once every second, on the second.</p>

<p>So something like:</p>

<pre><code>seedRandom(Math.floor(time), timeless = true);
random(45,55);
</code></pre>

<p>will create a random value that stays constant for 1 second, and </p>

<pre><code>var holdTime = 5;
seedRandom(Math.floor(time/holdTime), timeless = true);
random(45,55);
</code></pre>

<p>will stay constant for 5 seconds - you can change the value of the variable <code>holdTime</code> to control how long the value holds for. For example to change once every frame, but not during sub-frame intervals set it to <code>holdTime = thisComp.frameDuration;</code></p>
","16215"
"ffmpeg - video with timer","2135","","<p>I am currently working with the <code>drawtext</code> and <code>drawbox</code> filters to recreate a similar video appearance to what is shown below in the image. I am trying to see of a possible ways to add a count down timer to display video duration. The timer could either be placed in the upper left or right hand corners. I am able to generate a test video showing a timer this way <code>ffplay -f lavfi -i testsrc</code> OR  <code>fmpeg -f lavfi -i testsrc -vf crop=29:52:256:94 -t 10 timer1.mp4</code>. However, how can I display a timer ? </p>

<p><em>drawbox/drawtext filter settings</em></p>

<pre><code>ffmpeg -i ""/media/test/test.mp4"" -vf drawtext=""fontfile=/usr/share/fonts/truetype/open-sans/OpenSans-Regular.ttf:text='Title of this Video':x=(w-tw)/2:y=(h-th)/2""  /media/test_edited.mp4""
</code></pre>

<p><em>desired output</em></p>

<p><img src=""https://i.stack.imgur.com/DmScO.png"" alt=""enter image description here""></p>
","<p>This is possible but will require scripting. <code>ffprobe</code> will be used to determine the duration of the input, then the countdown and background box will be created by the <a href=""https://ffmpeg.org/ffmpeg-filters.html#drawtext"" rel=""nofollow noreferrer""><code>drawtext</code></a> filter.</p>

<p><img src=""https://i.stack.imgur.com/84Gwd.jpg"" alt=""reverse duration countdown timer""><br>
<sub>A 30 second input will start from 30 and count down to 0.</sub></p>

<p>Example:</p>

<pre><code>input=input.mp4
duration=$(ffprobe -loglevel error -show_entries format=duration -of default=nw=1:nk=1 ""$input"")
ffmpeg -i ""$input"" -vf ""drawtext=fontfile=OpenSans-Regular.ttf:text='%{eif\:$duration-t\:d}':fontcolor=white:fontsize=24:x=w-tw-20:y=th:box=1:boxcolor=black@0.5:boxborderw=10,format=yuv420p"" -c:v libx264 -c:a copy -movflags +faststart output.mp4
</code></pre>

<ul>
<li><p>If you prefer rounded corners you may be able to make a PNG file with alpha, and then place it in your desired location using the <a href=""https://ffmpeg.org/ffmpeg-filters.html#overlay"" rel=""nofollow noreferrer""><code>overlay</code></a> filter. Doesn't seem worth it to me though.</p></li>
<li><p>If you find the duration from <code>ffprobe</code> to be inaccurate then you will have to completely decode the file to get the actual duration and then parse the console output. One method would be: <code>ffmpeg -i input -f null -</code>.</p></li>
<li><p>This may be possible to do without <code>ffprobe</code> but I don't have an alternative method at the moment.</p></li>
<li><p>You can combine this filterchain with the one from <a href=""https://video.stackexchange.com/a/15554/1760"">ffmpeg drawtext filter - create transparent background with text</a> to get exactly what you want. An oversimplified command could then look like:</p>

<pre><code>-vf ""format,drawbox,drawtext,drawtext,format""
</code></pre></li>
</ul>
","15562"
"Split one video file into small files made up of its scenes","2134","","<p>I'm trying to come up with a workflow to take completed short films, and split them up into individual files based on where each shot that makes up the film starts and ends.</p>

<p>I've used the scene detection feature in Adobe SpeedGrade to figure out where shots should start and end, but I'm having trouble taking that information and actually rendering out individual shots based on it. I had expected to be able to bring this info back into Premiere Pro using an EDL (Edit Decision List) but none of the ""cuts"" that SpeedGrade marked seem to be reflected when I try this. Either I'm doing something wrong or Premiere/SpeedGrade can't support this sort of workflow.</p>

<p>Also to keep in mind, this is expected to be a repetitive process, and I'm developing this workflow for people who may be inexperienced in video editing, so it really should be automated as much as possible. Manually entering the start and end times of each shot isn't really a viable option because it would be ridiculously time consuming, even after SpeedGrade determines these times. </p>
","<p>Here's the workflow I decided to use. It's a little slower than what I had hoped, but it seems to fit my needs.</p>

<ol>
<li>Convert videos into MOV or AVI</li>
<li>Import into SpeedGrade</li>
<li>Run Scene Detect</li>
<li>Manually confirm correct scene detection.</li>
<li>Export each shot individually from Speed Grade</li>
<li>Convert to a web-ready MP4 using Media Encoder (MP4 is required in my use case but it might not be in everyone's)</li>
</ol>

<p>Some background--apparently EDL's with 'dumb cuts' (i.e. cuts where the video before and after the cut is from the same clip) aren't recognized in most video editors, so importing the EDL into premiere won't work for that reason.</p>
","12653"
"How to cut large video without video editing software (Windows)","2122","","<p>I have a large video that I need only a small portion of online, and then I want to edit and cut out some parts inside that portion of video online. How do I accomplish that without any special software except perhaps VLC or the likes? </p>

<p>I wary of downloading AVIdemux and other ""free"" stuff from the web. </p>

<p>I've had bad experience with Softonic, CNet and the likes and even SourceForge who used to be great but then started installing stuff that you can't get rid of, and that affect your system like changing your search page etc. (For some reason many times they are not defined as malware, but in some cases are, and then its even worse because online instructions tell you to download ""removal"" software which usually like a Trojan just wreaks havoc). </p>

<p>Maybe there's something on GitHub?</p>
","<p>If you need to make multiple cuts and then join them or edit those cuts, you'll need a video editor to do it efficiently. Try <a href=""http://www.shotcut.org/"" rel=""nofollow"">Shotcut</a>.</p>
","17243"
"What does a phantom technician do on a production set and why would a director need one?","2121","","<p>I just saw a video and in the credits is listed ""phantom technician"" ?
What are this person's duties, and why would a director need one on a production set?</p>

<p>Here is the video reference:
<a href=""https://vimeo.com/23141142"">https://vimeo.com/23141142</a></p>
","<p>This was shot with a <a href=""http://www.visionresearch.com/Products/High-Speed-Cameras/Phantom-HDGOLD/"">Phantom HD</a> camera. It's a special high-speed digital camera that can shoot at up to 1,500 fps (which is what the creators say they were shooting at). So he's likely the tech who setup this camera rig.</p>
","3860"
"What's a good camera to shoot a low-budget short film with?","2120","","<p>My budget is around $750. So far I have come up with <em><a href=""http://www.dpreview.com/reviews/canoneos550d"" rel=""nofollow"">Canon 550D</a></em> and I think it's a great choice to shoot a low-budget short film. I am just seeking your advice to see if there is any better alternative I can use with this budget. Cameras with <em>Flip-out screen</em> are preferred. Any advice would be appreciated!</p>
","<p>The short answer is anything that works for you. IMO, the 550D is a fine choice if it works for you.</p>

<p>The long answer? Even if you need to postpone the film, make sure your script is FLAWLESS. Shooting with a <a href=""http://www.red.com/"">Red</a> can't save an awful script, but I've seen some ~awesome~ shorts &amp; features shot with a <a href=""http://en.wikipedia.org/wiki/PXL-2000"">Fisher Price PXL-2000</a> and the <a href=""http://www.amazon.com/Pure-Digital-Camera-Photo/b?ie=UTF8&amp;node=373705011"">Flip</a>.</p>

<p>Beyond that, understand lighting. Know where your light is coming from and how to use it to make your scenes less muddy or grainy (unless that's what you're going for stylistically). Consumer camcorders are less light-sensitive than their professional brethern. </p>

<p>Also, make sure you can record decent sound. In most cases, on-board microphones are crap. That being said, bad sound makes a mediocre script awful. amazing sound can make mediocre scripts a touch less painful to watch. </p>

<p>One other thing you may want to try: network with other filmmakers and see what they're using. See if you can rent or borrow cameras to see what you like to work with. Who knows? I've shot films and donated my gear to the cause because the script was AWESOME. You may run into someone who's willing to support your vision with a camera donation.</p>

<p>But start with your script.</p>
","4375"
"Why more than 10 FPS is needed","2111","","<p>As we know that out brain can process at most 1/10th of a second then why we go beyond the 10 FPS as our brain should not be able to process more than the 10 FPS of a scene. whats is the logic behind ?</p>
","<p>The brain can process an image displayed for as little as 13 ms if the surrounding content is blank.  10 to 12 images per second is only the number of images that appear distinct to the eye.  Beyond 10 to 12 images a second,  the brain starts to try to piece them together, but it is doing a lot of work to make that go and the motion isn't as smooth as it could be.</p>

<p>Using higher frame rates means that the brain has to do less filling in the gaps and motion can appear more accurate and smoother since the brain is actually fully capable of processing images faster than 10 times a second.</p>

<p>The 12fps animation threshold is only a minimum to get the appearance of motion, not the maximum amount of information our brain can take in.  We can process far more images per second to determine motion, but require at least 10 to 12.</p>

<p>You can test this for yourself if you want by taking a 30 fps video and inserting a black frame at random in the video.  You'll easily notice the distortion from the missed frame because it abruptly changes to something other than the expected image.</p>

<p>Basically your brain knows what it expects to see and can recognize that it didn't see what it expected, it just can't recall exactly what it is that it saw that it didn't expect since it was taking a shortcut to reduce the amount of processing needed.</p>

<p>The Wikipedia article on <a href=""http://en.wikipedia.org/wiki/Frame_rate"">frame rate</a> is a good source of additional information on the topic.</p>
","10750"
"Setup for Premiere Pro on Laptop with SSD/HDD","2098","","<p>I am about to install Premiere Pro on a laptop with 500gb SSD and 750gb HDD, how can I optimise it in terms of where to install the OS, other apps and where to put the video files. I also have an external drive where I store footage, am not sure if that should come into the mix.</p>

<p>Thanks</p>
","<p>The question is whether disk performance is critical in your normal workflow, in which case you'd want to place the footage on the SSD.</p>

<p>I have a similiar setup with Sony Vegas and I observed that, disk performance is rather uncritical during rendering, because it's always the processor that's the bottle neck. But your situation may differ from that. Especially, if you have a very fast CPU and a very slow HDD.</p>

<p><strong>Alternative 1:</strong></p>

<p>The following configuration works well for me:</p>

<ul>
<li>OS on SSD to have fast boot time</li>
<li>Vegas/Premiere on SSD to have fast startup time</li>
<li>Video footage on HDD to have all footage in a single location.</li>
<li>External drive for backuping your footage</li>
<li>All other stuff: where-ever there is room left. Just make sure there is enough storage for your videos on HDD. 750GB fill up very quickly with HD footage ;-)</li>
</ul>

<p><em>Disadvantage</em>: May be slow with fast CPU's and slow drives. Project opening times may be slow as well.</p>

<p><em>Advantage</em>: Fast boot and programm start. All footage in one place.</p>

<p><strong>Alternative 2:</strong> </p>

<p>Of course, if OS+Premiere+Video fits to the SSD, put it there. This will give you the fastest experience. Plus the option to have a very fast backup to the HDD and a second slower backup to the external drive.</p>

<p><em>Advantage</em>: Fast. Double backup.</p>

<p><em>Disadvantage</em>: Not much storage for footage.</p>

<p><strong>Alternative 3</strong>: </p>

<p>Put OS+Premiere to SSD and video to HDD. Place the one project that you're currently working on to SSD. This configuration is also very fast, but I sometimes experience issues with moving projects across the drives, so I try to avoid that. </p>

<p><em>Advantage</em>: Fast.</p>

<p><em>Disadvantage</em>: Possibly issues with moving footage/projects around.</p>
","11961"
"Remove noise in video post production","2097","","<p>I have a video, shot with a recent but low-end camcorder (Samsung HMX-Q10) in Full HD 1920x1080/50i.</p>

<p>The problem is that there wasn't enough light (I guess) and the camcorder added some ""parasites"" to the video on the zones in the shadow (blacks are quite okay, but dark zones are not).<br>
The kind of parasites that you may see on <a href=""http://www.youtube.com/watch?v=GQ3fPl6eNkM"" rel=""nofollow"">that video</a> (on walls for instance) (this is not a video shot with that camcorder, nor my video).</p>

<p>So, I don't seem to be able to remove them when shooting, whatever the parameters of the cam are. I still need to try with more light, but I'd like to know if it is possible to correct that with a post-production effect on my existing video, especially with Sony Vegas Pro.</p>

<p>Moreover, is there a way to prevent the cam to add those artificial elements to the video ? This looks like some numeric elements added to the video, like the cam tries to correct the lighting. Can't the camcorder write exactly the image it gets from the lens, even if it's too dark ?<br>
Can I achieve <a href=""http://www.youtube.com/watch?v=hz6k-38jtUQ"" rel=""nofollow"">this kind of dark image</a> with my cam (or maybe it is darkened in post production ?)</p>
","<p>The parasites you talked about is called noise. The setting you can change to get less is called the ISO. It corresponds to the old ASA value we had before the digital age.</p>

<p>We can't see a thing, we can only see light. That's the way the universe is built up. Same with the camera. It sees the light reflected off surfaces. So white objects reflects all light and dark objects no light. And absence of light, as you know, means darkness. Where it is dark it has to ""work extra hard"" to figure out what's actually there, because there is almost no information there given from the light. Therefore you will get some noise. A better camera will produce less noise.</p>

<blockquote>
  <ol>
  <li>Turn down the ISO as much as possible without compromising the aperture or ""shutter speed"" you want.</li>
  <li>Remove the noise left in post.</li>
  </ol>
</blockquote>

<p>It is also no problem to edit it away in post on your computer! The only compromise is that you will get smoother edges and blurred images the more noise you remove. I once saw a movie shot entirely by moonlight. It was full of noise but they removed everything in post!</p>

<p>There are two plugins you can use:</p>

<blockquote>
  <ol>
  <li><p>Neat Video's Noise Removal. I've never tried this, but you can use it with Sony Vegas. They claim to be the best in noise removal.</p></li>
  <li><p>Red Giant's Magic Bullet DeNoiser. I've used this and am very satisfied with it. Does a really good job. I don't know if this works in Vegas, but it certainly works great in Adobe After Effects and Premiere Pro!</p></li>
  </ol>
</blockquote>
","3438"
"Upload a video to Youtube without losing contrast","2095","","<p>I've been trying to upload a video to Youtube which had been created in Vegas Movie Studio HD 11 and rendered as an MP4 file. You may notice the text has become sharper and the watermark is now almost impossible to see:</p>

<p><strong>Video on PC (Played in Quicktime)</strong></p>

<p><img src=""https://i.stack.imgur.com/1Gyhw.png"" alt=""enter image description here""></p>

<p><strong>Video on Youtube</strong></p>

<p><img src=""https://i.stack.imgur.com/Np9G6.png"" alt=""enter image description here""></p>

<p>The entire video is affected by not just the first slide. I have tried to increase the colour settings in Sony Vegas and with Youtube's built-in editor. I have also tried using the built-in Vegas Youtube uploader.</p>

<h2>Video Properties</h2>

<ul>
<li>Created and rendered in Vegas Movie Studio HD. </li>
<li>Rendered as HDV 720-25P (Matches project settings)</li>
<li>Audio: 384 Kbps, 48,000 Hz, 16 Bit, Stereo, MPEG</li>
<li>Video: 25 fps, 1280x720 Progressive, YUV, 18.3 Mbps</li>
<li>Pixel Aspect Ratio: 1.000</li>
</ul>
","<p>I worked out a satisfactory solution to this problem. It involves adding the 'Computer RGB to Studio RGB' Video FX to each video track in your Vegas Studio project. This effects the rendered file and project quite a lot but appears normal when uploaded to Youtube:</p>

<p><strong>Quicktime</strong></p>

<p><img src=""https://i.stack.imgur.com/aEMVi.png"" alt=""enter image description here""></p>

<p><strong>Youtube</strong></p>

<p><img src=""https://i.stack.imgur.com/c1KBR.png"" alt=""enter image description here""></p>

<p>It's possible to disable the track Video FX whilst you work on your project so you don't have to deal with dimmed video. The effects will need to be be re-enabled before you render the project. </p>
","4279"
"Is there a program for automatic clip creation from scene detection/recognition result?","2093","","<p>I want to parse a large set of movie/advertisement mixture in one video file, i.e. some hours of video, and separate not only movie and advertisement, but also split/cut different advertisements into clips (I'm only interested in advertisement clips separated into different files). Is there a tool which allows to do such thing without interaction (most tools, e.g. <code>kdenlive</code>, have great scene recognition, but only split the clips into logical units for editing in the GUI)? Of course, scene detection isn't deterministic and some clips will require cutting, but that shouldn't matter.</p>

<p>The important thing is the separation of clips into files and the automatization (might be done within the program or on the console with Linux command pipes or an API for - lets say - <code>python</code>).</p>

<p><strong>EDIT 1:</strong> The program should run on Linux and be free software.</p>
","<p>I found out about <code>melt</code> and created <a href=""https://github.com/krichter722/video-splitter"" rel=""nofollow"">video-splitter</a> which is a command line wrapper around the former. There might even be a very elegant way to do everything in a command pipe with two or three commands or even everything with one <code>melt</code> command.</p>
","14562"
"How can I sync two videos together in Corel VideoStudio Pro X3","2080","","<p>I have two videos of a dance at a dance competition taken by two different people. The videos are of the same dance but are from different angles.</p>

<p>I want to create a single video that contains both videos side by side and synchronised to the same point. The videos start and finish a few seconds apart from each other and thus are slightly different length.</p>

<p>My video application is Corel VideoStudio Pro X3. How can I do this? I imagine that somehow using the audio this could be done. The music should act as a good guide for syncing them together.</p>
","<p>There is no way to do this automatically. It has to be done manually by matching the frames together from the two videos. This can be done by placing the videos in separate overlay tracks and works for two or more videos.</p>
","3790"
"How do I set up an RTMP stream server and URL endpoint?","2079","","<p>I've got a Haivision Barracuda, which can stream RTMP to a URL (secured by username/password) set in its interface, but have very little experience of streaming video. What software do I need (Windows or Linux, but ideally free or open source at the moment) to set up a server that can provide this URL on which to receive the stream, please?</p>

<p>Can I then rebroadcast that (also as RTMP, to a remote URL endpoint), and how can I view the live stream to test that it's working?</p>
","<p>Turns out we had the free version of Adobe Flash Media Server installed on a Linux server that could receive the stream and show it via players connecting to the URL and the inbuilt test applications. We didn't have time to take down one of our servers to test the Windows Streaming Media path, but thanks for the suggestion, AJ Henderson.</p>
","7895"
"Scaling ""Stroke"" Effect ""Brush Size"" while scaling composition?","2062","","<p>I've got a 100x100px composition with a Mask containing a Stroke effect used to animate the drawing of a vector. The problem I'm having is when I drop this composition into a larger one and attempt to scale it up, the Brush Size in Stroke remains the same (e.g. the graphic is drawn with thin lines, instead of the thicker looking ones in the smaller comp). </p>

<p>I'm thinking some sort of expression magic might be the fix to this, but any input on how to maintain line thickness at any resolution/scale would be much appreciated. Thanks!</p>

<p><strong>EDIT:</strong> I've attached a small demo project showing the issue I'm running into below:
<a href=""http://hellocld.com/stuff/stroke-issue.aep"" rel=""nofollow"">http://hellocld.com/stuff/stroke-issue.aep</a></p>
","<p>Easy fix, you see the small sun icon on the imported comp layer? Disable that, that basically means that the comp should act like the layers within it should act like they are in the parent comp. It negates the transformation your are applying to it and the effect will just keep its pixel value its supposed to have, you are not ""blowing it up"" like you would expect.
If you disable it the imported comp will act like a rendered video.</p>

<p>If you have huge pixelation going on just go to full resolution in the preview and it will be fine, though the layer will not act like a vector, you will have a rasterized image so keep in mind to have a big enough resolution if you want to scale it up.
Better to have it bigger and scale it down than having not enough.</p>

<p><img src=""https://i.stack.imgur.com/Ned9M.png"" alt=""enter image description here""></p>
","10885"
"Use A6000 as webcam type camera","2050","","<p>I need to do a live stream and I'm looking for a way to connect my Sony A6000 to my computer so that it can be used as a camera for Facebook Live. Any ideas?</p>
","<p>For real time output you only have hdmi (the USB is for file transfer) so all you need is an hdmi input card for your PC.</p>

<p>Then you can grab that input as a source for applications.</p>
","20422"
"How much disk space does 1 mini DV tape digitised in HDV take?","2047","","<p>I'm looking out to buy some storage for a session of logging a lot of tapes, and I want to guage how much space I will need. How much disk space does 1 mini DV tape digitised in HDV take?</p>
","<p>mini DV is already digital format. The bitrate is 25Mbps.
So, 1 hour occupies approximately 11 Gigabytes</p>
","13157"
"reasons for After Effects CC deprecating wmv and H.264","2041","","<p>I've recently upgraded to After Effects CC from CS6 and I discovered that in the render queue, options for wmv and H.264 have been deprecated.  I know codecs come and go, but I'd like to understand the reasons why Adobe eliminated these options.  I know they're still available in Media Encoder CC, but why not After Effects CC?</p>
","<p>While I'm not aware of any release of the official reasons for this, I can offer a strong theory based on my knowledge of the way workflows are intended with After Effects.  After Effects is an effects and composition package, it is not designed to be placed at the end of a processing pipeline, but rather in the middle of it.  </p>

<p>After Effects is designed to work on clips that will be edited in to a finished product.  As such, it doesn't make a lot of sense to export from After Effects in a format that is targeted at end viewers, such as h.264 or WMV.  It makes much more sense to target high quality, low compression intermediate formats that will minimize iterative quality loss or simply to use embedded compositions directly in Premiere.</p>

<p>Since it isn't a designed use case to be doing final output from After Effects, it doesn't make a lot of sense to put in extra development time maintaining a feature that is redundant and not part of the main intended work flow of the product.  If you want to do a final output in that format, you can still use Media Encoder to do it and then they only have to license and maintain the encoders for one application rather than two.</p>

<p>It is unfortunate in the cases where you wanted to do a one off render of a clip, but that isn't their main use case that they are designing around.  They probably decided that maintaining that use case simply wasn't worth the cost of maintaining.</p>
","10314"
"Does version control have a part in the video production workflow?","2027","","<p>I'm a software developer and I'm also interested in photography (for four years) and video production (for a few months only).</p>

<p> In software development, there is an important rule <em>every</em> developer follows on <em>every</em> project: <strong>everything should be under version control</strong>: source code, configuration files, database schema, documentationeverything which enables building the project from scratch. This has two pleasant consequences:</p>

<ol>
<li><p>In an event of a disaster when you lose everything except the version control repository, you should be able to continue as if nothing happened.</p></li>
<li><p>In an event of some stupid change which negatively affects the project, the developer can come back to an earlier revision.</p></li>
</ol>

<p> In photography, <strong>every change I make to photos is stored forever in Lightroom catalog</strong>, making it possible to revert to previous state at any moment. With virtual copies feature, Lightroom also enables to do what is called a <em>branch</em> in version control: the ability to test something different, and either keep both results or remove one of them later.</p>

<p>The catalog doesn't store the RAW photos themselves, but they don't change anyway.</p>

<p> In video production, things seem different. I work with Premiere Pro, After Effects and Soundbooth.</p>

<ul>
<li><p>None seem to store history permanently: if I do an action by mistake and notice it only the next day, there is no way to recover the previous version.</p></li>
<li><p>Soundbooth also changes directly the WAV files, which requires an additional effort to keep separate the original recordings from the modified ones.</p></li>
<li><p>Version control is rarely mentioned, and I haven't found anyone telling how does he actually use version control in his workflow. Moreover, nobody mentions which version control should be used, and since most version control systems are optimized for text files, not binary ones, this creates an additional challenge.</p></li>
<li><p>Video.SE doesn't have <a href=""/questions/tagged/version-control"" class=""post-tag"" title=""show questions tagged &#39;version-control&#39;"" rel=""tag"">version-control</a> or <a href=""/questions/tagged/revisions"" class=""post-tag"" title=""show questions tagged &#39;revisions&#39;"" rel=""tag"">revisions</a> tags.</p></li>
</ul>

<p>Thus, I have two questions:</p>

<ol>
<li><p>Does version control have a part in the workflow of a person who works with video production? How is it integrated?</p></li>
<li><p>Would migrating to Adobe Creative Cloud help? Are there specific features which allow, in Creative Cloud, to track successive revisions of a Premiere Pro or After Effects project?</p></li>
</ol>

<p>Remark: to avoid off-topic answers, I highlight that <strong>my question is unrelated to the backups</strong>, and is specifically about storing successive revisions of my work, not having an on-site/off-site backup of the data.</p>
","<p>Version control in the sense of Git isn't very practical in the video world. You would need to make a specific version control tool for every audio and video tool out there as all work with their own project formats.
But being able to read these formats is just one thing, then you also need the render engine of that tool to show diff's.</p>

<p>Though all of these tools work in a non-destructive way unless you pre-render some stuff (compare that to compiling a dll/lib of a piece of your code and work with that from now on), so you ususally can just go back to an old revision by doing ctrl+z or using the history tool in some programs.</p>

<p>Though saving sub versions is usually the way to go. Like stib described in his answer or by doing it manually.</p>

<p>Something I like to do and works well with every software is putting my project files (no source footage) into Dropbox. If you have a somewhat fast upload speed (~1 Mbit/s) and your project file isn't 100MB+, you can upload your project before you save the next time. An average Premiere/AE/FCP project is around 10-20MB so your recently saved files will be uploaded in 1-2 minutes. Even faster if you have more upload bandwidth.</p>

<p>Then if you have to go back you can access Dropbox's history of your files and download or restore to that revision. Dropbox saves file revisions forever* on a paid account (*atleast when they had the pack rat option, now its a year I think) and for 30 days on a free account. I'm sure that there are other cloud hosters that offer similar features.
It's a bit like using a super limited version of git that handles binary files very well and is headache free.
This has the advantage that you don't clutter the folder with tons of files and have a backup at the same time.</p>

<p>Most cloud hosters also offer Team Memberships so you can work with multiple editors. Or you share the project folder with other team members.</p>
","12574"
"flattening a sequence in Adobe Premiere (collapsing included sequences)","2023","","<p>I'm using Adobe Premiere -- I'm trying to convert a project to use a single sequence, with all the media clips in that sequence.</p>

<p>At the moment I'm in Adobe Premiere 5.5, but I have Adobe CC available.</p>

<p>Any leads are welcome!</p>

<h1>Question</h1>

<p><strong>Q: What is the fastest and easiest way to replace these subsequences in my main sequence, with their component clips, such that the main sequence still renders to exactly the same video?</strong></p>

<p>If you have any leads or even partial ideas, please feel free to leave a comment.  Also, please mention which version of Premiere you are using.</p>

<h2>Current State of the Project</h2>

<p>This is a simplified view, the actual number of clips and subsequences is much larger!</p>

<ul>
<li>Main Sequence A

<ul>
<li>Clip 1</li>
<li>Clip 2</li>
<li>Sequence B</li>
<li>Clip 3</li>
<li>Sequence C</li>
</ul></li>
<li>Sequence B

<ul>
<li>Clip 4</li>
<li>Clip 5</li>
</ul></li>
<li>Sequence C

<ul>
<li>Clip 6</li>
<li>Clip 7</li>
</ul></li>
</ul>

<h2>Desired State of the Project</h2>

<ul>
<li>Main Sequence A

<ul>
<li>Clip 1</li>
<li>Clip 2</li>
<li>Clip 4</li>
<li>Clip 5</li>
<li>Clip 3</li>
<li>Clip 6</li>
<li>Clip 7</li>
</ul></li>
</ul>

<h2>Difficulties In Flattening</h2>

<ul>
<li>Sequence A doesn't use all of Sequence B, and it doesn't start at the beginning</li>
<li>Suppose I just copy the clips from Sequence B</li>
<li>...pasting them in Sequence A where Sequence B is now</li>
<li>the clips will not start at the same time they do when in Sequence A</li>
</ul>

<h1>Flattening Techniques</h1>

<h2>Attempt 1: Nesting</h2>

<ul>
<li>Select the clips in Sequence A, including Sequences B and C </li>
<li>choose ""Nest""</li>
<li>now only one sequence is in Sequence A!</li>
<li>...however that sequence still has nested sequences.  </li>
</ul>

<h2>Attempt 2: Trim project</h2>

<ul>
<li>I selected only Sequence A and chose Project Manager...</li>
<li>Premiere made an entirely new, simplified (""trimmed"") project</li>
<li>the media used in that Sequence A, and therefore B and C as well, were included in the sequence, and no others</li>
<li>the media used were copied to the new location (so they are in two places on this drive now)</li>
<li>...but when I opened the new project, Sequence A STILL has nested sequences.</li>
</ul>

<h2>Attempt 3: brute force</h2>

<p>Why bother?  Why not fix all these manually?  Well... </p>

<p>Here's the process I have at the moment to replace a single sub-sequence.  Bear in mind this is a large project and I'll have to do this dozens of times.</p>

<ul>
<li>in Sequence A, select Sequence B and duplicate to another layer</li>
<li>...we'll call this ""Sequence B2""</li>
<li>extend Sequence B2 in length to the very beginning of its earliest point </li>
<li>cut B2 to where Sequence B begins</li>
<li>the new first half of B2 is the length of B that elapses before showing the part used in Sequence A.  

<ul>
<li>This half we'll call B2a</li>
<li>the part that plays in Sequence A we'll call B2b</li>
</ul></li>
<li>now, in Sequence B, use timecode/frame arithmetic to go to the exact point in the clip</li>
<li>make the cut in the same place in Sequence B as it began in Sequence A... we'll call this B3, which is theoretically starts at the same point as B2b</li>
<li>copy the cut clip B3 in Sequence B...</li>
<li>...and paste in Sequence A on top of B2b and use a separate layer </li>
<li>set B3 to a partial opacity to verify Sequence A is EXACTLY THE SAME as it appeared before B3 was pasted in!

<ul>
<li>if it's off, you'll see B3 and B2b as two distinct and overlapping images</li>
</ul></li>
</ul>
","<p>(1.) Open the master sequence containing the nested sequences.</p>

<p>(2.) MAKE SURE that the ""Insert or overwrite sequences as nests or individual clips"" selector (the leftmost icon under the timecode display in the sequence panel) is NOT highlighted. This will force a nested sequence in the Source Monitor to be edited into the sequence as the individual clips contained in the nested sequence.</p>

<p>(3.) Park the playhead in the middle of one of the nested items in the main sequence</p>

<p>(4.) Choose ""Marker / MARK CLIP"" (I have this programmed to ""X"" on the keyboard."") You need to make sure your track selection toggles are correctly set to target the track containing the nested item.</p>

<p>(5.) Choose ""Sequence / MATCH FRAME"" (I have this programmed to ""F"" on the keyboard.) This opens the nested sequence into the Source Monitor.</p>

<p>(6.) Choose ""Clip / OVERWRITE"" (I have this programmed to ""B"" on the keyboard, like an Avid.) This should have replaced the nested sequence CLIP ITEM with the clips contained in the nested sequence. This overwrites (from IN to OUT as selected in step 4) with the contents of the nested sequence.</p>

<p>(7.) move the playhead to the next clip and press X, F, B</p>

<p>(8.) repeat step 7 until you're done.</p>

<p>[This is tested on Premiere CC 2015.2]</p>
","18571"
"Is firewire better than USB for live webcasting - Video","2022","","<p>I am building a 2 camera, 4 mic webcasting setup. </p>

<p>Using firewire to connect the 2 cameras to a laptop is more involved as it requires 2 analog to digital converts and 2 firewire ports, whereas most laptops usually have at least 2 usb ports.</p>

<p>I need the video quality to be good (possibly HD), stable and reliable.</p>

<p>Which route should i take?</p>
","<p>For the best quality and/or High Definition you want to use the fastest computer input you have that your streaming app will accept. That would normally be Firewire. Most online streaming sites will accept both USB or Firewire.</p>

<p>Take a look at <a href=""http://www.youtube.com/user/TEDxTalks/search?query=tedxsanantonio+2011"" rel=""nofollow"">http://www.youtube.com/user/TEDxTalks/search?query=tedxsanantonio+2011</a>. These are TEDx Talks we shot using a SONY PMW-EX1 with HD/SDI out and a few other SONY HDV cameras with SDI adapters ($200 each I think) into a Newtek Tricaster that was streaming and recording 1080i. The HD files were uploaded to YouTube.</p>

<p>Hope that helps</p>
","3423"
"Difference between 4K Edition and 4K Cineform","2021","","<p>On this page I found this here:</p>

<p><a href=""http://www.timescapes.org/products/"" rel=""nofollow"">http://www.timescapes.org/products/</a></p>

<p>And I don't get it, because 4k Ediition is only 25gb and 4k cinema is 330gb.</p>

<p>What is the difference if both are 4K (4096 x 2304) ?</p>

<p>Could 12-bit (color resolution) make that huge difference in size?</p>
","<p>The simple answer is yes for a couple of reasons:</p>

<ul>
<li>if the number of pixels is the same*, the colour depth can have a huge impact on the size. 12 bit = 4096 in decimal. So if the difference between the two formats is 13.2 (330/25) that could be easily explained as the difference between 8 bit and 12 bit is could be a 16x difference.</li>
<li>the content on each of the formats may be different - you have no way of knowing from this product list.</li>
</ul>

<p>(*there are some differences - see table below from the <a href=""http://en.wikipedia.org/wiki/4K_resolution"" rel=""nofollow noreferrer"">Wikipedia 4K page</a>)</p>

<p><img src=""https://i.stack.imgur.com/POJGX.png"" alt=""enter image description here""></p>
","5811"
"What are the outer dimensions of the GoPro 3+ lens holder?","1999","","<p>I need to know the outer dimensions of the GoPro 3+ lens on the waterproof housing (The square black ring).</p>

<p>Does anyone know where I can find them or does anyone have the camera themselves that they can measure?</p>
","<p>I don't have the Hero 3+ but you can measure it with the help of a front picture of the cam and the dimensions of the camera.
Taking the dimensions from here: <a href=""http://www.dpreview.com/articles/8137555963/hands-on-with-The-gopro-hero-3-black-edition"" rel=""nofollow noreferrer"">http://www.dpreview.com/articles/8137555963/hands-on-with-The-gopro-hero-3-black-edition</a>
(5.8 x 3.9 x 2 cm)
And using this picture:
<img src=""https://i.stack.imgur.com/O2yeW.png"" alt=""enter image description here""></p>

<p>And measure how big the lens casing is compared to the size of the camera in pixels (for example with the Photoshop ruler tool)
We get 35.4 mm for the width for example. So I'd go with a solid 35mm (width).</p>

<p>I measured 537 for the camera and 327,5 for the black lens casing. So the Black Lens casing is 61% the size of the camera. 61% of 58mm is 35.38mm.
You can do the same with the other dimensions.</p>
","11959"
"Is there a 4K action camera solution that does NOT have perspective distortion?","1987","","<p>I was looking at alternatives to the GoPro (such as the Sony FDR-X1000V), but they all seem to only shoot wide-angle with significant perspective distortion (fisheye). Are there action cameras which do <em>not</em> introduce this sort of distortion?</p>
","<p>There are two major advantages to recording fisheye distortion rather than rectilinear distortion.  First, it is optically easier to do, which means one can make lenses that are better in other metrics (such as chromatic aberration, resolution, vignetting, etc) while remaining small enough and light enough to fit into the body of an action camera.  Second, the relative size of an object doesn't change radically as its moves from center to edge when the camera is not under tight control.  For that reason, it is possible to define a crop area that follows the subject as it bounces around the full frame and then use motion tracking to create a relatively stable view of the object, which can then be de-fished.  If it were rectilinear, the stabilization process might have to do some major zoom and tilt corrections as well, which would be very distracting.</p>
","19444"
"Audio from PA system to Camcorder","1985","","<p>I've been asked to video tape a Q/A session with a speaker, and I plan to use my HD camcorder (http://www.amazon.com/Panasonic-HDC-TM700K-Camcorder-Control-Internal/dp/B0035LD0EY).</p>

<p>The issue is that my camcorder only has the 1/8th inch speaker cable input, and a long cable from the PA system to my camcorder could pick up a lot of noise.  Wondering what my options are as to how to get the audio from the PA system into my camcorder.  I thought of two options:</p>

<ol>
<li>Wirelessly transmit the audio from the PA system into the camcorder </li>
<li>Get an XLR &lt;--> 1/8th inch adapter for my camcorder and use an XLR cable to get the audio from the PA system.</li>
</ol>

<p>Are there any other ways?  If not, could you please recommend product options for 1 and 2. Not too expensive (around $100 if possible).  This isn't professional, but I do want decent audio quality.</p>

<p>Thanks</p>
","<p>An XLR cable used with an adapter won't give you the audio quality that an all-XLR cable will. If audio quality is really important, I'd buy or borrow an external recorder that takes XLR cables directly. If this isn't an option, pick up an XLR to 1/8"" adapter cable. I suggest taping the adapter to the camcorder so it doesn't move around much. </p>

<p>(Mic cables <em>do</em> move around, particularly if you have a boom operator. However, with an interview, you may be able to use on-screen mics in stands or a single, overhead mic on a stand or hung from the ceiling.) </p>

<p>If you need two mics for the interview, you'll need a small mixer which you can then feed into the camera. </p>

<p>Wireless only really makes sense if you want to use lavalier mics on the actors; wireless systems introduce additional complexities, as they're less plug-and-play than a standard cabled mic. </p>

<p><strong>Balanced versus unbalanced audio:</strong></p>

<p>Any signal chain will have line noise. An unbalanced system (like an XLR cable going into an 1/8"" stereo adapter would be) will have more of it. XLR cables (with three connectors) are good because they deliver <em>balanced audio</em>, which is another way of saying that they can do long cable runs without much noise. </p>

<p>A 1/8"" cable has also three connectors, but usually because they carry a <em>stereo signal</em>, which is not the same as an unbalanced one. I assume the stereo cable jack on your camera means that the cable run is <em>not</em> balanced - repurposing that third prong from ground to a stereo signal is what does it. </p>

<p>Although I think it's technically possible to have a balanced 1/8"" jack, I've never heard of one. (A stereo signal carried through XLR would neccesitate two XLR cables.)  </p>

<p>According to <a href=""http://shop.panasonic.com/shop/model/HDC-TM700K?t=specs&amp;support#tabs"" rel=""nofollow"">Panasonic's specs</a>, your camera uses a ""stereo mini"" jack, not a balanced or TRS jack. Connector quality would be even more important in that case, since you're feeding an unbalanced analog signal to the camera. </p>

<p>If you have a little time, perhaps getting two adapters - one high-end and one inexpensive - and doing a test may be in order. </p>

<p><strong>Conclusion:</strong></p>

<p>In your situation, if buying or renting an external recorder isn't an option (around $300US), a good microphone feeding into an XLR to 1/8"" adapter <em>will</em> introduce line noise, but will also give you getter audio quality than most onboard camera mics.</p>
","4243"
"Object moving to a position I didn't specify","1982","","<p>I'm trying to do the following:</p>

<ol>
<li>At 0;00;00;00 orange triangle is offscreen and begins to move towards the center.</li>
<li>At 0;00;03;00 orange triangle arrives at position 243, 135.</li>
<li>At 0;00;03;00 orange triangle begins to move to the left.</li>
<li>At 0;00;07;00 orange triangle arrives at position 137, 135.</li>
</ol>

<p>From 0;00;03;00 to 0;00;03;05 the object begins sliding to the right, reaching the position 246, 135. I didn't tell it to do that, nor to even go in that direction. Any idea why it's happening? I've attached a video and a picture below that hopefully should help explain.</p>

<p><img src=""https://i.stack.imgur.com/IbTqJ.png"" alt=""Graph Editor view showing weird position""></p>

<p><a href=""https://vimeo.com/117806531"" rel=""nofollow noreferrer"">Vimeo link</a></p>

<p>Thanks for any help.</p>
","<p>It's because of Spatial Interpolation.
You need the spatial interpolation to be set to linear, and by default it is set to Auto Bezier.</p>

<p>Right click on the keyframe at 3'00, and select <a href=""http://helpx.adobe.com/after-effects/using/keyframe-interpolation.html"">Keyframe Interpolation</a>.  Change the Spatial Interpolation to linear, and it should solve the problem.</p>

<p>What's happening:</p>

<p>Instead of drawing straight lines (linear lines) between your positions on the screen, after effects is drawing curves (bezier curves).  When you tell your object to ""double back"" on itself, AE tries to draw a curve to this effect, causing a little knot or bounce forward at the 3'00 mark.  </p>

<p>You can choose how AE interprets changes in space, as described, but you can also choose how it interpolates time too (Temporal interpolation).  Setting temporal interpolation to bezier often produces nicer results than linear, as it allows objects to slowly come to a stop, rather than suddenly stopping at a keyframe, which is how objects in the real world respond (due to weight, friction, gravity etc).</p>
","14697"
"ALL-I vs IPB. Which one to use","1973","","<p>I'm fairly new do filmmaking. I've been trying to figure out which is best, but people seem to have very different opinions and everyone has his own arguments. My guess is that ALL-I should be better because they are all I-Frames, whereas IPB should be used when storage is a concern. However, some people are saying IPB is actually better.  </p>

<p>Any thoughts?</p>
","<p>First we need a primer on I, P and B frames.  An I frame is a frame that is a completely independant picture of the frame.  It doesn't depend on any other frames and is a frame which B and P frames are based on.  A P frame stores only the changes from the previous frame and a B frame stores only the changes in either direction.  IPB uses a mix of all 3 frame types in a set sequence to capture video with much lower data usage, but every P and B frame is an approximation of what the frame that was originally captured was.  They are not exact.</p>

<p>For the same size I frame, all I will ALWAYS be better quality, however, it could be possible for IPB to be better if the camera has a set limit rate for I frame that could end up with larger I frames for the IPB due to the saved space, but that would depend on the exact data rates used by the camera.  The theory behind it would be to say we have all I with 3k per frame (making this number up completely) then we have 30 pictures of whatever quality we can get out of 3k.  Now, if the IPB frame, if we had say 1k per frame on average, but the P and B frames are very small, then it might be possible for the I frame to use 5k of data and the P and B frames to use only .5k each and result in a higher overall quality.</p>

<p>This would depend on the quality of the motion and the data rates being used by the camera however.  In most cases, recording all I on the camera is probably going to produce I frames equal or larger in size than those in IPB.  However, when doing a final output, the highest quality for a given size will almost always be compressing to IPB.  All I will also capture fast motion better since the difference between frames will be larger.</p>
","8297"
"How to create a new stereo mix from one channel in Premiere pro cs5.5","1969","","<p>I have a video with stereo audio, but one channel is completely silent. So the audio only plays on the left stereo. How is it possible to duplicate the left channel and create a new stereo mix for the same clip? Or alternatively force the sound in a mono audio track?</p>
","<p>Hey try the audio effect Fill left or Fill right!
(I use a zoom h4n and have to constantly do that )
<a href=""http://www.mediacollege.com/adobe/premiere/pro/audio/fill.html"" rel=""nofollow"">http://www.mediacollege.com/adobe/premiere/pro/audio/fill.html</a> !</p>

<p>Happy editing!
(remember fireballs always make video better!)</p>
","4165"
"How to gradually colourize an object in after effects","1950","","<p>I'm rather new to AE but I believe I have the rotoscoping under control.
How can I gradually (each frame more) colourize an object in a video. So that at first the whole video is black and white and then one object starts to get more and more colour starting from a certain point and allow the colour to spread back to the whole object.</p>

<p>Thanks a lot!</p>
","<p>I can't give you AE specifics, but one general method would be to have a mono layer and an otherwise identical color layer, keying the color over the mono using a matte layer that you draw on. As portions of the matte layer are turned white, or shades of white, the color layer is revealed.</p>
","7469"
"Batch conversion into a new folder with ffmpeg","1942","","<p>I'm trying to batch encode 45 video files, and copy the encoded files into an existing subfolder called ""encode""</p>

<p>I've used the suggested code from this question (<a href=""https://stackoverflow.com/questions/5784661/how-do-you-convert-an-entire-directory-with-ffmpeg"">https://stackoverflow.com/questions/5784661/how-do-you-convert-an-entire-directory-with-ffmpeg</a>) But, I don't know how to capture the original file names (with spaces and punctuation) and use that as the new file name in the new directory.</p>

<p>Here's the code I'm using so far:</p>

<p><code>
for i in *.mp4;
  do name=`echo ""${i%.*}""`;
  echo $name;
  ffmpeg -i $i -vf ""scale='if(gt(a*sar,16/9),640,-1)':'if(gt(a*sar,16/9),-1,360)',pad=640:360:(ow-iw)/2:(oh-ih)/2,setsar=1"" -vcodec libx264 -b:v 1600k -bufsize:v 1600k -r 30000/1001 encoded/$name.mp4
done
</code> 
It's giving me an ""invalid argument"" error message when I run it - probably because I haven't specified the names correctly(?)</p>

<p>The ffmpeg code that I'm using does work (I can encode individual file) but I haven't figured out how to batch encode the entire directory.</p>

<p>If anyone can let me know what I'm doing wrong, I would love to know!</p>

<p>Thank you!</p>
","<p>The accepted answer in the question you're referring to uses additional unnecessary processes, legacy code, and doesn't prevent globbing or word splitting.</p>

<p>See the <a href=""https://stackoverflow.com/a/33766147/1109017"">second answer</a> which is shorter, simpler, and more efficient:</p>

<pre><code>for f in *.mp4; do ffmpeg -i ""$f"" &lt;additional options&gt; ""encoded/${f%.*}.mp4""; done
</code></pre>

<p>Secondly, don't use fancy quotes which will cause errors, so instead of <code>$varible</code> use <code>""$variable""</code>.</p>
","19862"
"How to shoot a scene of someone riding a bike?","1941","","<p>I'm shooting a short film and there is a scene I'd like to film where a guy is biking and the camera follows him through his bike ride. All I have to film on is a Sony a3000 camera, I don't own anything fancy. How can I film this scene without buying any expensive equipment?</p>
","<p>Well, its a bit of a generic question.
Almost like asking how to take a movie like shot using your camera.</p>

<p>The answer is equally generic.</p>

<ol>
<li><p>Lighting, to prevent blowout or shadows take the shot when its cloudy, or near sunrise or sunset. (Unless you wanted a grunge harsh hot feel and maybe over exposed is the effect you are after, so take it at noon).</p></li>
<li><p>Make sure the light is behind you so you capture the image flush.</p></li>
<li><p>Make sure you take shots in scenes. A closeup of peddling (running alongside with a macro lens can work). Take a closeup shot. Take a shot that first places them in the scene (unless you are adding mystery and sometimes you place that scene at the end).</p></li>
<li><p>Tell a story, have an element in the foreground like a thumbs up to hitch a ride, out of focus is the rider coming up, then comes into focus the thumb is blurred. Then cut reaction shot, or maybe maybe headphones not seeing shot. etc, I mean just go with the story, and tell the story with your shots.</p></li>
<li><p>I guess I could also go with settings, I am a big fan of large apertures (low f stops) like f2.0 - f4.0. And high shutter speed 1/125th or higher to minimize blur on fast moving objects, again depending how you want the end result to look, all these rules can be broken. Large apertures (low f stops)  tends to create a bokeh effect (out of focus the background if focus is on subject) making it obvious to the audience what is the focus of the story.</p></li>
</ol>
","14857"
"Using ffmpeg to cut out a scene by original timestamp","1941","","<p>I have a recording (a simple, unprocessed TS from my satellite receiver, not encrypted) that has uneccessary stuff at the beginning and the ending. I want to cut out the main feature now with ffmpeg.</p>

<p>So I started with ffplay to get the timestamps for the beginning and the ending and crop detection:</p>

<pre><code>ffplay -i ""recording_xyz.ts"" -vf ""cropdetect=24:16:0""
</code></pre>

<p>This was the output for the first and the last frame I need:</p>

<pre><code>[Parsed_cropdetect_0 @ 000000000625baa0] x1:0 x2:1279 y1:1 y2:718 w:1280 h:704 x:0 y:8 pts:7888683985 t:81953.624278 crop=1280:704:0:8
...
[Parsed_cropdetect_0 @ 000000000625baa0] x1:0 x2:1279 y1:1 y2:718 w:1280 h:704 x:0 y:8 pts:7888683985 t:87259.194348 crop=1280:704:0:8
</code></pre>

<p>The stream itself starts at timestamp <code>81825.820733</code>:</p>

<pre><code>Duration: 01:38:00.24, start: 81825.820733, bitrate: 12748 kb/s
</code></pre>

<p>With all that information, I tried to cut the desired part out (and also removing streams I don't need) and convert everything into an MKV. It turned out I don't need to crop, so I can simply copy everything:</p>

<pre><code>ffmpeg -i ""recording_xyz.ts"" -ss 81953.0 -to 87259.0 -c:v copy -c:a copy -c:s copy -map 0:0 -map 0:3 -avoid_negative_ts 1 -reset_timestamps 1 ""archive_file.mkv""
</code></pre>

<p>The result is an empty MKV file.</p>

<p>What would be the correct way to use ffmpeg to cut a specific scene identified by the timestamps out?</p>

<p><strong>For reference:</strong></p>

<p>ffmpeg version I'm using:</p>

<pre><code>ffmpeg version N-64477-g5864069 Copyright (c) 2000-2014 the FFmpeg developers
  built on Jul  6 2014 22:10:36 with gcc 4.8.3 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx--enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-decklink --enable-zlib
    libavutil      52. 90.101 / 52. 90.101
    libavcodec     55. 68.101 / 55. 68.101
    libavformat    55. 45.100 / 55. 45.100
    libavdevice    55. 13.101 / 55. 13.101
    libavfilter     4. 10.100 /  4. 10.100
    libswscale      2.  6.100 /  2.  6.100
    libswresample   0. 19.100 /  0. 19.100
    libpostproc    52.  3.100 / 52.  3.100
</code></pre>

<p>ffmpeg's identification data for the original input stream:</p>

<pre><code>Input #0, mpegts, from 'recording_xyz.ts':
  Duration: 01:38:00.24, start: 81825.820733, bitrate: 12748 kb/s
  Program 11110
  Program 11130
  Program 11140
    Stream #0:0[0x190a]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, t709), 1280x720 [SAR 1:1 DAR 16:9], 50 fps, 50 tbr, 90k tbn, 100 tbc
    Stream #0:1[0x1914](deu): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, stereo, s16p, 253 kb/s
    Stream #0:2[0x1915](mis): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, s16p, 190 kb/s
    Stream #0:3[0x1916](deu): Audio: ac3 ([6][0][0][0] / 0x0006), 48000 Hz, stereo, fltp, 448 kb/s
    Stream #0:4[0x1917](mul): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, stereo, s16p, 190 kb/s
    Stream #0:5[0x191e](deu): Subtitle: dvb_teletext ([6][0][0][0] / 0x0006)
    Stream #0:6[0x181a]: Unknown: none ([5][0][0][0] / 0x0005)
</code></pre>

<p>The things that appear on the console while trying to encode:</p>

<pre><code>[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] non-existing PPS 0 referenced
    Last message repeated 1 times
[h264 @ 0000000002cb9c80] decode_slice_header error
[h264 @ 0000000002cb9c80] no frame!
[h264 @ 0000000002cb9c80] mmco: unref short failure
[mpegts @ 0000000002c784c0] PES packet size mismatch
    Last message repeated 3 times
[mpegts @ 0000000002c784c0] Could not find codec parameters for stream 6 (Unknown: none ([5][0][0][0] / 0x0005)): unknown codec
Consider increasing the value for the 'analyzeduration' and 'probesize' options
</code></pre>

<p>... stream identification data goes here (see above) ...</p>

<pre><code>[matroska @ 00000000047c0980] Using AVStream.codec.time_base as a timebase hint to the muxer is deprecated. Set AVStream.time_base instead.
    Last message repeated 1 times
Output #0, matroska, to 'archive_file.mkv':
  Metadata:
    encoder         : Lavf55.45.100
    Stream #0:0: Video: h264 (H264 / 0x34363248), yuv420p, 1280x720 [SAR 1:1 DAR  16:9], q=2-31, 50 fps, 1k tbn, 90k tbc
    Stream #0:1(deu): Audio: ac3 ([0] [0][0] / 0x2000), 48000 Hz, stereo, 448 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (copy)
  Stream #0:3 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
frame=    0 fps=0.0 q=-1.0 size=       1kB time=00:00:00.00 bitrate=N/A
</code></pre>

<p>After some time, ffmpeg finishes:</p>

<pre><code>video:0kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown
</code></pre>
","<p>It turned out that it is quite easy to cut by the original timestamp. Let's stick with the example I've given in the question. <code>ffplay -i recording_xyz.ts -vf ""cropdetect=24:16:0""</code> gives you the following information about the stream:</p>

<pre><code>Input #0, mpegts, from 'recording_xyz.ts':
  Duration: 01:38:00.24, start: 81825.820733, bitrate: 12748 kb/s
  Program 11110
  Program 11130
  Program 11140
    Stream #0:0[0x190a]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, t709), 1280x720 [SAR 1:1 DAR 16:9], 50 fps, 50 tbr, 90k tbn, 100 tbc
    Stream #0:1[0x1914](deu): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, stereo, s16p, 253 kb/s
    Stream #0:2[0x1915](mis): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, s16p, 190 kb/s
    Stream #0:3[0x1916](deu): Audio: ac3 ([6][0][0][0] / 0x0006), 48000 Hz, stereo, fltp, 448 kb/s
    Stream #0:4[0x1917](mul): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, stereo, s16p, 190 kb/s
    Stream #0:5[0x191e](deu): Subtitle: dvb_teletext ([6][0][0][0] / 0x0006)
    Stream #0:6[0x181a]: Unknown: none ([5][0][0][0] / 0x0005)
</code></pre>

<p>Most important here is the <code>start: 81824.820733</code> information from the second line. Keep this number in mind.</p>

<p>Now we need our desired start and stop timestamps. <code>ffplay</code> can be paused by the space key. Pause at the desired start and the desired end. You can ""navigate"" the stream by clicking with the mouse in the ffplay video window. The beginning of the stream is left, the end is at the right border of the window.</p>

<p>When paused, you can read the following on the console:</p>

<pre><code>[Parsed_cropdetect_0 @ 000000000625baa0] x1:0 x2:1279 y1:1 y2:718 w:1280 h:704 x:0 y:8 pts:7888683985 t:81953.624278 crop=1280:704:0:8
</code></pre>

<p>You see again a timestamp here, marked with <code>t:</code>. In this example, the value is <code>81953.624278</code>. Same goes for the desired end of the stream. In our example, this is <code>87259.194348</code>.</p>

<p>With this information, you can easily calculate the values for ffmpeg's <code>-ss</code> and <code>-to</code> parameters:</p>

<ul>
<li>Let <code>b</code> be the stream start timestamp (here <code>81824.820733</code>)</li>
<li>Let <code>s</code> be the desired start timestamp (here <code>81953.624278</code>)</li>
<li>Let <code>e</code> be the desired end timestamp (here <code>87259.194348</code>)</li>
</ul>

<p>The relative starting point (in seconds) is <code>s - b = 81953.624278 - 81824.820733 = 128.803545</code>. For the relative ending point (in seconds): <code>e - b = 87259.194348 - 81824.820733 = 5434.373615</code>. Convert the seconds now to the format <code>hh:mm:ss.msec</code>. For convenience, you might want to use this little Python script:</p>

<pre><code>#!/bin/env python3

def ts_format(d: float) -&gt; str:
    hrs = int(d)
    rest = float(d - hrs) * float(60)
    mins = int(rest)
    rest = float(rest - mins) * float(60)
    secs = int(rest)
    rest = float(rest - secs) * float(1000)
    msec = int(rest)
    return '{:02d}:{:02d}:{:02d}.{:03d}'.format(hrs, mins, secs, msec)


def main() -&gt; bool:
    base = input('Base timestamp: ')
    starting = input('Desired start timestamp: ')
    ending = input('Desired end timestamp: ')
    start_hr = (float(starting) - float(base)) / float(3600)
    end_hr = (float(ending) - float(base)) / float(3600)
    print('Start: {:s} | End: {:s}'.format(ts_format(start_hr), ts_format(end_hr)))


if __name__ == '__main__':
    main()
</code></pre>

<p>With the example above, you'll receive the following output:</p>

<pre><code>Start: 00:02:08.803 | End: 01:30:34.373
</code></pre>

<p>With this information, you can use ffmpeg:</p>

<pre><code>ffmpeg -i recording_xyz.ts -ss 00:02:08.803 -to 01:30:34.373 -c:v copy -c:a copy -c:s copy -map 0:0 -map 0:3 -avoid_negative_ts 1 -reset_timestamps 1 ""archive.mkv""
</code></pre>

<p>I tried this with several recordings, and it always worked well. It is also very easy to incorporate cropping if necessary:</p>

<pre><code>ffmpeg -i recording_xyz.ts -ss 00:02:08.803 -to 01:30:34.373 -c:v libx264 -crf 18 -preset fast -tune film -vf ""crop=1280:704:0:8"" -c:a copy -c:s copy -map 0:0 -map 0:3 -avoid_negative_ts 1 -reset_timestamps 1 ""archive.mkv""
</code></pre>

<p><strong>Potential pitfalls</strong></p>

<ul>
<li>If the recording goes over midnight, you might experience a lower timestamp at the end of the stream compared to the beginning timestamp. You need to find the timestamp values around the break: The highest one before the break, and the lowest after the break. For calculating the desired end, you need to add the value at the end, the distance between the stream start and the highest value before the break and the distance between the lowest value after the break and the desired ending timestamp.</li>
<li>The method for finding your desired start and end point is very unprecise. In my humble opinion, this is OK for TV recordings.</li>
<li>The method is not suitable for removing commercials.</li>
</ul>
","13384"
"Why is there a 2D disc in every Blu-Ray 3D package?","1925","","<p><a href=""https://en.wikipedia.org/wiki/Blu-ray_Disc#Blu-ray_3D"">Wikipedia</a> says:</p>

<blockquote>
  <p>MPEG4-MVC compresses both left and right eye views with a typical 50% overhead compared to equivalent 2D content, and can provide full 1080p resolution backward compatibility with current 2D Blu-ray Disc players.</p>
</blockquote>

<p>So if a Blu-Ray 3D is completely backwards compatible to a regular Blu-Ray, why do all consumer Blu-Ray-3D packages contain a 2D as well as a 3D disc? Is that just marketing (so people will even buy it if they dont yet have a 3D-capable setup) or is there some technical reason behind this?</p>
","<p>Disk space.  The 3d content takes more space so there is generally not enough room for special features.  The 2d special features are only on the 2d disk.  They take up the room that the second eye takes on the 3d disk.  They could make a special features only disk, but this is generally more expensive since it requires another master and another production line.  It's more cost effective and better marketing to just include the 2D copy.</p>
","9561"
"How to tune a musicbox","1916","","<p>I want to try to make my own musicbox. The picture below shows what I mean:
<img src=""https://i.stack.imgur.com/UvfPs.jpg"" alt=""enter image description here""></p>

<p>I was wondering how to determine the length of the metal tone generating plates (shown in dark grey). I assume it is a function of the length and thickness, I just don't know the exact relationship.</p>

<p>Could someone please provide me with a formula relating the plate dimensions with the frequency?</p>
","<p>The tuned teeth (or lamellae) of the steel comb is typically set to a chromatic scale. To change the tuning would require adding or subtracting material to an individual tooth with some trial and error and a good ear. Changing the length would cancel the action of the pins on the revolving cylinder so that is not a good idea.</p>

<p>If you think the scale you have meets your tuning definition then it would be far easier to make a new scroll to change to the melodic and harmonic content you desire. If you are starting from scratch and have the tools you should make a system to be able to change the scrolls so you have a diversity of songs.</p>

<p>Here is a reference to tuning the reeds on a harmonica since this is adding and subtracting material related:</p>

<p><a href=""http://www.angelfire.com/music/harmonica/mikesretuning.html"" rel=""nofollow"">http://www.angelfire.com/music/harmonica/mikesretuning.html</a></p>

<p>Information on how to tune lamellae as used in a Karimba:</p>

<p><a href=""http://www.nscottrobinson.com/mbiratunings.php"" rel=""nofollow"">http://www.nscottrobinson.com/mbiratunings.php</a></p>

<p>Here's a place where you can make your own music box tunes by using 
punchable paper strip musical movement:</p>

<p><a href=""http://www.deanorgans.co.uk/order_musicalmovements_mbm30hp.htm"" rel=""nofollow"">http://www.deanorgans.co.uk/order_musicalmovements_mbm30hp.htm</a></p>

<p>Math part:</p>

<p><a href=""http://en.wikipedia.org/wiki/Tuning_fork#Calculation_of_frequency"" rel=""nofollow"">http://en.wikipedia.org/wiki/Tuning_fork#Calculation_of_frequency</a></p>

<p>UPDATE:</p>

<p>I am not entirely certain that using tuning fork math is the right place for a series of metal bars as found in a music box comb so I have done some more searches and found a reference that covers much more here:</p>

<p><a href=""http://windworld.com/features/tools-resources/exmis-free-bar-length-calculator/"" rel=""nofollow"">http://windworld.com/features/tools-resources/exmis-free-bar-length-calculator/</a></p>
","5499"
"What is the video codec name of videos made through the Canon 600d DSLR","1903","","<p>I need to know the name of the codec?
They have the file extension of .mov  but i need to know what codec that is.</p>

<p>What is the video codec name of videos made through the Canon 600d DSLR?</p>
","<p>All Canon DSLRs, including 600D use the same video codec: </p>

<ul>
<li><em>Codec</em>: MPEG-4 Part 10 / AVC / H.264</li>
<li><em>Profile</em>: Baseline</li>
<li><em>Level</em>: 5.0</li>
<li><em>Bitrate mode</em>: CBR  40 Mbit/s maximum average</li>
<li><em>PAL</em>: 1080p25, 1080p24, 720p50, 480p50</li>
<li><em>NTSC</em>: 1080p30, 1080p24, 720p60, 480p60 </li>
<li><em>Colorspace</em>: 8-bit YUV 4:2:0 Rec. 709</li>
</ul>

<p>Many of the higher-end models have an additional <strong>All-I mode</strong>:</p>

<ul>
<li><em>Profile</em>: High (without CABAC)</li>
<li><em>Level</em>: 5.1</li>
<li><em>Bitrate mode</em>: VBR  100 Mbit/s maximum average</li>
</ul>
","10291"
"Lossless conversion of yuv420p10le to yuv420p","1899","","<p>I have an mp4 movie, containing an <code>h264</code> video stream (pixel format <code>yuv420p10le</code>), and an <code>AAC</code> audio stream.</p>

<p>My favorite player cannot read the <code>yuv420p10le</code> pixel format. I was wondering if it is possible to convert losslessly the pixel format of the movie from <code>yuv420p10le</code> to <code>yuv420p</code>. More specifically, is it <em>possible</em> (theoretically speaking), and if so, can any program do it and how?</p>

<p>I've tried the following with <code>ffmpeg</code>, but it does not work:</p>

<pre><code>ffmpeg -i super_movie.mp4 -vcodec copy -acodec copy -pix_fmt yuv420p super_movie_yuv420p.mp4
</code></pre>

<p>Thanks in advance for any help.</p>
","<p>No this is not possible. Looking at this theoretically disregarding the codec you can not take away information and call it lossless, thats already a terminology problem.</p>

<p>Applying a new color space without transcoding is also not possible
It's not so easy to say whether this is even possible with certain codecs other than h264, it really depends on how you define certain terminology.</p>

<p>Regarding h264 its simply not possible in the way the codec works, color information is essential in how the codec encodes frames as its based on motion estimation and the color information between frames is what defines the in between frames that don't hold the information of a whole frame but only the differences between keyframes.
So just applying different color values to each existing pixel is not really possible. It would result in major artifacts.</p>
","12213"
"ffmpeg encode in all-i mode h264 and h265 streams","1891","","<p>Which params I can use to obtain a transcode all-i based? No B and P frames?</p>

<p>I know that this is not efficient for compression purposes but this is not my case now.</p>

<p>I guess i have to fiddle a bit with <strong>-x264-params</strong> or <strong>-x265-params</strong> but I don't know the switches to do it :)</p>
","<p>For H264</p>

<pre><code>ffmpeg -i input -c:v libx264 -intra output
</code></pre>

<p>For H265, seems no alias or preset has been set yet</p>

<pre><code>ffmpeg -i input -c:v libx265 -x265-params frame-threads=4:keyint=1:ref=1:no-open-gop=1:weightp=0:weightb=0:cutree=0:rc-lookahead=0:bframes=0:scenecut=0:b-adapt=0:repeat-headers=1 output
</code></pre>

<p>H265 code <s>stolen</s>borrowed from <a href=""https://superuser.com/q/935930/114058"">here</a>.</p>
","16959"
"Video for Windows Codec x264 and VP80","1891","","<p>I'm looking for VfW codec to encode H264 and VP80.</p>

<ul>
<li>I already found: <a href=""http://sourceforge.net/projects/x264vfw/"" rel=""nofollow"">x264vfw</a> </li>
<li>And <a href=""http://www.videohelp.com/tools/VP8-VFW-Codec"" rel=""nofollow"">VP80vfw</a> </li>
</ul>

<p>The 1st is bugged and lost the start and the end. The 2nd do not install on Windows 8.</p>

<p>Why VfW ? to use then in a C# sofware using EmguCV, recording frame by frame. I can't use ffmpeg.</p>

<p>Any Idea ?</p>
","<p>There isn't really an alternative to x264vfw. Are you sure the error is with the codec and not your application? It's in wide use, such a major bug would definitely be found.</p>

<p>Also be aware that h264 doesn't work in a frame by frame basis unless you encode only intra frames.
VP80 is NOT h264, just a similar codec.</p>

<p>If you need to develop an app you probably want to turn to a commercial h264 implementation like MainConcepts Codec SDK: <a href=""http://www.mainconcept.com/products/sdks/codec-sdk/video/h264avc.html"" rel=""nofollow"">http://www.mainconcept.com/products/sdks/codec-sdk/video/h264avc.html</a>
Should also come with a vfw codec.</p>

<p>Alternatively you can turn to GStreamer which offers C# bindings: <a href=""http://cgit.freedesktop.org/gstreamer/gstreamer-sharp"" rel=""nofollow"">http://cgit.freedesktop.org/gstreamer/gstreamer-sharp</a></p>

<p>Another option could may be <a href=""http://www.videohelp.com/tools/ffdshow"" rel=""nofollow"">ffdshow</a> it adds quite a few codecs to VfW and I believe also x264.</p>
","12168"
"Compress or convert .mov to lighter format","1888","","<p>My camera records videos in <code>.mov</code> format, what can I use to edit out the audio and lower it's size in a Windows environment?</p>
","<p>You didn't mention whether you're using a Mac or Windows, but...</p>

<p>You could use a tool like <a href=""http://www.squared5.com"" rel=""nofollow"">MPEG Streamclip</a> to change the format of the video, or strip off the audio.  Works on both Mac or Windows.</p>
","2616"
"ffmpeg concat introduces a/v sync problem","1888","","<p>I have one 2-hour video file and want to add a 10-second title sequence to its start. The long file has perfect a/v sync from start to finish. However, when I use <code>ffmpeg -f concat -i mylist.txt -c copy outfile.mp4</code> to concatenate the title file to the long file, the audio in the long file gradually goes out of sync, progressively becoming worse until at the end of 2 hours, the audio is a full second or two behind the video.  I can understand why the <code>concat</code> might break the synchronization, but I don't understand why it would  cause the synchronization to become progressively worse. </p>

<p>I want to avoid re-encoding the long file to maintain quality. If necessary, I can re-encode the title file because it's just white text on a black background.</p>

<p>I'd greatly appreciate any suggestions. I'm a digital video semi-noob and have faced a steep learning curve, but I am learning. I'm using Ubuntu Linux 16.04 and ffmpeg 2.8.6-1ubuntu2, which I think is the latest ffmpeg version.  I have about 20 old 2-hour family video cassettes that I am digitizing. </p>

<p>File background:</p>

<ol>
<li><p>The long file is an MP4 file using H264 and AAC. It originated as a .ts file and I use ffmpeg to encode it into the mp4 format. Before concatenating the title file, it is in perfect a/v sync at all times. It's 720x480, with 127 audio bitrate.  I captured the video from a Sony Handycam Video8 tape using a Hauppauge Live-2 USB converter, using the VLC CLI to generate the original .ts file. It's about 120 minutes long.</p></li>
<li><p>The title file I created in Blender, and tried to duplicate the long file's format: 720x480, 127 audio bitrate, MP4 containing H264 and AAC. It actually has no audio, as it's just two ""slides"" of text. (I know that there probably is actually an audio track generated by Blender, but it's just silence as I don't add any audio tracks to the simple titles.) It's only 10 seconds long.</p></li>
</ol>

<p>The results of <code>ffmpeg -i longfile.mp4</code> is:</p>

<pre><code>Metadata:
major_brand     : isom
minor_version   : 512
compatible_brands: isomiso2avc1mp41
encoder         : Lavf56.36.100
Duration: 01:17:06.58, start: 0.013000, bitrate: 2134 kb/s
Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p,
720x480 [SAR 32:27 DAR 16:9], 1997 kb/s, 29.97 fps, 29.97 tbr, 29971
tbn, 59.94 tbc (default)
Metadata:
handler_name    : VideoHandler
Stream #0:1(unk): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, 
fltp, 127 kb/s (default)
Metadata:
handler_name    : SoundHandler
</code></pre>

<p>The results of <code>ffmpeg -i titlefile.mp4</code> is</p>

<pre><code>Metadata:
major_brand     : isom
minor_version   : 512
compatible_brands: isomiso2avc1mp41
encoder         : Lavf56.40.101
Duration: 00:00:10.01, start: 0.012000, bitrate: 421 kb/s
Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 
720x480 [SAR 1:1 DAR 3:2], 284 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 
59.94 tbc (default)
Metadata:
handler_name    : VideoHandler
Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, 
stereo, fltp, 127 kb/s (default)
Metadata:
handler_name    : SoundHandler
</code></pre>
","<p>Run</p>

<pre><code>ffmpeg -i titlefile.mp4 -vf setdar=16/9 -video_track_timescale 29971 -ac 1 newtitle.mp4
</code></pre>

<p>and then run concat with the new title video.</p>

<p>Modern containers like MP4 have <strong>P</strong>resentation <strong>T</strong>ime<strong>S</strong>tamps, which are denominated with reference to a timebase. So, if the timebase value is 1/500 and a frame's PTS is 200, then that tells the video player to show that frame at 200*(1/500) = 0.4 seconds. The <code>tbn</code> values shown in the readouts are the reciprocals of this timebase. Now, the concat demuxer, due to a design oversight (or choice!?) does not rescale the PTS values so that all inputs have frames with PTS using the same timebase. Your two videos have different TBs, and so the long video after the concat is being sped up. The difference is perceptually invisible - except for the audio drift. That <code>tbn</code> value is for the video stream. Audio streams have their own timebase, related to the sample rate, which is the same here. </p>
","19238"
"How to change output filename in ffmpeg batch?","1887","","<p>I'm using windows and ffmpeg to convert video mp4 into audio mp3. I've come up with the following batch line that will do the job:</p>

<pre><code> for %f in (*.mp4); do ffmpeg -i %f -vn -ar 44100 -ac 1 -b:a 32k -f mp3 %f.mp3
</code></pre>

<p>However, the <code>%f</code> variable captures the whole filename with the extension, so my output looks like <code>filename.mp4.mp3</code>. I'd rather it look like <code>filename.mp3</code>.</p>

<p>I've tried all sorts of brackets and truncation methods I know of but I cannot get anything to work.</p>

<p>How can I change the filenames in an ffmpeg batch conversion on Windows?</p>
","<p>You can extract the file name directly from the <code>for</code> replacement parameter. No need to set a new variable at all.</p>

<pre><code>for %f in (*.mp4) do ffmpeg -i ""%f"" -vn -ar 44100 -ac 1 -b:a 32k -f mp3 ""%~nf.mp3""
</code></pre>

<p>If you want to do more complicated <a href=""https://technet.microsoft.com/en-us/library/ee692804.aspx"" rel=""nofollow noreferrer"">string munging</a>, it's much more intuitive in Powershell:</p>

<pre><code>ls *.mp4|foreach{
    ffmpeg -i $_ -vn -ar 44100 -ac 1 -b:a 32k -f mp3 $_.name.replace(""mp4"", ""mp3"")
}
</code></pre>
","20475"
"How to get FFMPEG output the same quality as Vimeo?","1882","","<p>I like Vimeo, but I rather host a player and video myself. So I was wondering if someone knows how to get the closest result to Vimeo quality, but from FFMPEG?</p>

<p>It seems that Vimeo uses 1280 pixels width for it's HD videos, but I'm wondering if there are presets or settings out in the wild that can specify bitrate, quality etc for FFMPEG?</p>
","<h1>Download</h1>

<p>Assuming the downloaded video is the same as the one you'd watch in your browser:</p>

<pre><code>$ youtube-dl https://vimeo.com/123456789
</code></pre>

<h1><code>strings</code> &amp; <code>grep</code></h1>

<p>Assuming <code>x264</code> is used to encode, and the encoding settings weren't stripped out, you can see what version was used and view some of the particular settings. <strong>However, it does not mean that you should attempt to emulate everything verbatim</strong>.</p>

<pre><code>$ strings input.mp4 | grep x264
x264 - core 144 r11 40bb568 - H.264/MPEG-4 AVC codec - Copyleft
2003-2014 - http://www.videolan.org/x264.html - options: cabac=1 ref=3
deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00
mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 
deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=72
lookahead_threads=8 sliced_threads=0 nr=0 decimate=1 interlaced=0
bluray_compat=0 stitchable=1 constrained_intra=0 bframes=3 b_pyramid=2
b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2
keyint=infinite keyint_min=25 scenecut=40 intra_refresh=0
rc_lookahead=40 rc=crf mbtree=1 crf=20.0 qcomp=0.60 qpmin=5 qpmax=69
qpstep=4 vbv_maxrate=4950 vbv_bufsize=13500 crf_max=0.0 nal_hrd=none
filler=0 ip_ratio=1.40 aq=1:1.00
</code></pre>

<ul>
<li><p>It seems as if some (older?) videos have been stripped of this info.</p></li>
<li><p>Some videos seem to use 2pass rate control and others seem to use crf. I'm guessing the crf method is their current one. Both methods are using VBV (in <code>ffmpeg</code> this is set with <code>-maxrate</code> and <code>-bufsize</code>) but that doesn't mean you need to.</p></li>
<li><p>Looks like the <em>medium</em> preset was used, but there is no reason for you to use the same exact preset. If you have the patience for a slower preset, then use a slower preset.</p></li>
<li><p>You can see some additional info about the video stream, audio stream, and container format with: <code>ffprobe -v error -show_format -show_streams input.mp4</code>.</p></li>
</ul>

<h1>Encoding example</h1>

<p>There is no need to overcomplicate things, so for simplicity, and ""to get the closest result to Vimeo quality"", just use <code>-crf 20</code> with the slowest <code>-preset</code> that you have patience for. Or even better, use the highest crf value that still looks good to you.</p>

<p>This example is assuming your build supports the AAC audio encoder <code>libfdk_aac</code>. If not then you can use the native FFmpeg AAC encoder: <code>-c:a aac -strict experimental -b:a 192k</code>.</p>

<pre><code>ffmpeg -i input -c:v libx264 -preset slower -crf 20 -c:a libfdk_aac -vbr 4 \
-ac 2 -movflags +faststart output.mp4
</code></pre>

<h1>Also see</h1>

<ul>
<li><a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"" rel=""nofollow"">FFmpeg Wiki: H.264 Video Encoding Guide</a></li>
<li><a href=""https://trac.ffmpeg.org/wiki/Encode/AAC"" rel=""nofollow"">FFmpeg Wiki: AAC Audio Encoding Guide</a></li>
</ul>
","15182"
"ffmpeg transcoding H264 interlaced and keep interlacing via H264_NVENC codec","1876","","<p>I have the following interlaced H264 file:<br></p>

<blockquote>
  <p>Stream #0:0[0x335]: Video: h264 (Main) ([27][0][0][0] / 0x001B),
  yuv420p(tv, bt470bg, top first), 720x576 [SAR 16:11 DAR 20:11], 25
  fps, 50 tbr, 90k tbn, 50 tbc</p>
</blockquote>

<p>And this is a frame from the source video:
<a href=""https://i.stack.imgur.com/I6fBO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I6fBO.png"" alt=""enter image description here""></a></p>

<p>When I use:</p>

<blockquote>
  <p>ffprobe -show_frames -i source.mpg | grep interlaced_frame</p>
</blockquote>

<p>All frames are interlaced:<br>
interlaced_frame=1<br>
interlaced_frame=1<br>
         .<br>
         .<br>
         .<br>
interlaced_frame=1<br></p>

<p>I whant to transcode this file and keep interlacing and try to do it with the following command:</p>

<blockquote>
  <p>ffmpeg -i source.mpg -vcodec h264_nvenc -flags +ildct+ilme -acodec copy -f mpegts out.mpg</p>
</blockquote>

<p>after transcode the same frame from the ""out.mpg"" file is looks like follow: </p>

<p><a href=""https://i.stack.imgur.com/q8Qbz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q8Qbz.png"" alt=""enter image description here""></a></p>

<p>This is a console output:<br></p>

<pre><code>[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] decode_slice_header error
[h264 @ 0x2b546c0] no frame!
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] decode_slice_header error
[h264 @ 0x2b546c0] no frame!
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] decode_slice_header error
[h264 @ 0x2b546c0] no frame!
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] SPS unavailable in decode_picture_timing
[h264 @ 0x2b546c0] non-existing PPS 0 referenced
[h264 @ 0x2b546c0] decode_slice_header error
[h264 @ 0x2b546c0] no frame!
[h264 @ 0x2b546c0] mmco: unref short failure
    Last message repeated 1 times
[mpegts @ 0x2b4fcc0] PES packet size mismatch
Input #0, mpegts, from 'source.mpg':
  Duration: 01:00:00.95, start: 44661.794367, bitrate: 3336 kb/s
  Program 2101 
    Metadata:
      service_name    : ? 
      service_provider: TricolorTV
    Stream #0:0[0x335]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, bt470bg, top first), 720x576 [SAR 16:11 DAR 20:11], 25 fps, 50 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x336](rus): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, s16p, 192 kb/s
    Stream #0:2[0x337](rus): Subtitle: dvb_teletext ([6][0][0][0] / 0x0006)
Output #0, mpegts, to 'out.mpg':
  Metadata:
    encoder         : Lavf57.56.100
    Stream #0:0: Video: h264 (h264_nvenc) (Main), yuv420p, 720x576 [SAR 16:11 DAR 20:11], q=-1--1, 2000 kb/s, 25 fps, 90k tbn, 25 tbc
    Metadata:
      encoder         : Lavc57.64.100 h264_nvenc
    Side data:
      cpb: bitrate max/min/avg: 0/0/2000000 buffer size: 4000000 vbv_delay: -1
    Stream #0:1(rus): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, 192 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (h264_nvenc))
  Stream #0:1 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
[h264 @ 0x2b91360] reference picture missing during reorder
[h264 @ 0x2b91360] Missing reference picture, default is 2147483647
[h264 @ 0x2bf1f40] mmco: unref short failure
    Last message repeated 1 times
[h264 @ 0x2bf1f40] number of reference frames (0+5) exceeds max (4; probably corrupt input), discarding one
[h264 @ 0x2c7bf80] reference picture missing during reorder
[h264 @ 0x2c7bf80] Missing reference picture, default is 65736
[h264 @ 0x2c979e0] mmco: unref short failure
[h264 @ 0x2ccefa0] mmco: unref short failure
[h264 @ 0x2bf1f40] mmco: unref short failure
frame= 1232 fps=169 q=34.0 Lsize=   13581kB time=00:00:51.04 bitrate=2179.7kbits/s speed=6.99x  
</code></pre>

<blockquote>
  <p>$ffprobe -i out.mpg<br>
  Stream #0:0[0x100]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(top first), 720x576 [SAR 16:11 DAR 20:11], 25 fps, 25 tbr, 90k tbn, 50 tbc</p>
</blockquote>

<p><strong>But when I tried the same command with libx264 codec everythin looks fine:</strong></p>

<blockquote>
  <p>ffmpeg -y -i source.mpg -vcodec libx264 -flags +ildct+ilme -acodec copy -f mpegts libx264_out.mpg</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/zsM4X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zsM4X.png"" alt=""enter image description here""></a></p>

<p>And console output:</p>

<pre><code>[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] decode_slice_header error
[h264 @ 0x22ef6c0] no frame!
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] decode_slice_header error
[h264 @ 0x22ef6c0] no frame!
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] decode_slice_header error
[h264 @ 0x22ef6c0] no frame!
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] SPS unavailable in decode_picture_timing
[h264 @ 0x22ef6c0] non-existing PPS 0 referenced
[h264 @ 0x22ef6c0] decode_slice_header error
[h264 @ 0x22ef6c0] no frame!
[h264 @ 0x22ef6c0] mmco: unref short failure
    Last message repeated 1 times
[mpegts @ 0x22eacc0] PES packet size mismatch
Input #0, mpegts, from 'source.mpg':
  Duration: 01:00:00.95, start: 44661.794367, bitrate: 3336 kb/s
  Program 2101 
    Metadata:
      service_name    : ? 
      service_provider: TricolorTV
    Stream #0:0[0x335]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, bt470bg, top first), 720x576 [SAR 16:11 DAR 20:11], 25 fps, 50 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x336](rus): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, s16p, 192 kb/s
    Stream #0:2[0x337](rus): Subtitle: dvb_teletext ([6][0][0][0] / 0x0006)
[libx264 @ 0x2332720] interlace + weightp is not implemented
[libx264 @ 0x2332720] using SAR=16/11
[libx264 @ 0x2332720] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX
[libx264 @ 0x2332720] profile High, level 3.0
Output #0, mpegts, to 'libx264_out.mpg':
  Metadata:
    encoder         : Lavf57.56.100
    Stream #0:0: Video: h264 (libx264), yuv420p, 720x576 [SAR 16:11 DAR 20:11], q=-1--1, 25 fps, 90k tbn, 25 tbc
    Metadata:
      encoder         : Lavc57.64.100 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
    Stream #0:1(rus): Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, stereo, 192 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))
  Stream #0:1 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
[h264 @ 0x2327140] reference picture missing during reorder
[h264 @ 0x2327140] Missing reference picture, default is 2147483647
[h264 @ 0x23757e0] mmco: unref short failure
    Last message repeated 1 times
[h264 @ 0x23757e0] number of reference frames (0+5) exceeds max (4; probably corrupt input), discarding one
[h264 @ 0x2415500] reference picture missing during reorder
[h264 @ 0x2415500] Missing reference picture, default is 65736
[h264 @ 0x2430f60] mmco: unref short failure
[h264 @ 0x2468520] mmco: unref short failure
[h264 @ 0x23757e0] mmco: unref short failure
[mpegts @ 0x22eacc0] PES packet size mismatchme=00:59:59.40 bitrate=1368.5kbits/s speed=4.97x    
[h264 @ 0x2483fe0] error while decoding MB 16 8, bytestream -6
frame=89975 fps=124 q=-1.0 Lsize=  601846kB time=01:00:00.68 bitrate=1369.3kbits/s speed=4.97x    
</code></pre>

<blockquote>
  <p>$ffprobe -i libx264_out.mpg<br>
  Stream #0:0[0x100]: Video: h264 (High) ([27][0][0][0] / 0x001B), yuv420p(top first), 720x576 [SAR 16:11 DAR 20:11], 25 fps, 25 tbr, 90k tbn, 50 tbc</p>
</blockquote>

<p><strong>How to correct transcode H264 file to H264 and keep interlacing via h264_nvenc codec?</strong></p>
","<p>It's an issue with new video drivers. Everything is fine with video driver <a href=""http://www.nvidia.com/Download/driverResults.aspx/98373/en-us"" rel=""nofollow noreferrer"">361.28</a> and lower.
I already sent this issue to NVIDIA Support with Reference Number 161118-000164 <br><br>
UPDATE: This issue has been resolved from the <a href=""http://www.nvidia.com/Download/driverResults.aspx/114708/en-us"" rel=""nofollow noreferrer"">375.39</a> driver.<br> It allows keep interlacing on Pascal 10x video cards.</p>
","19877"
"FFmpeg: What is an acceptable value for the q= parameter?","1867","","<p>I'm trying out encoding video with ffmpeg using <code>x264</code> on a NAS machine with a 2.4 GHz processor. This yields a terminal output that has a high number in the <code>q=</code> parameter:</p>

<p><code>frame=  236 fps=0.2 q=29.0 size=    1038kB time=00:00:08.82 bitrate= 963.4kbits/</code></p>

<p>When I encode using <code>x265</code> on my standard computer, I get something that looks like this:</p>

<p><code>frame=67350 fps=0.7 q=-0.0 size=  416991kB time=00:37:28.41 bitrate=1519.3kbits/</code></p>

<p>It is my understanding that <code>q=</code> stands for quality, and should probably be as close to <code>0</code> as possible, while a low number close to <code>q=31</code> indicates poor quality and that ffmpeg has difficulties reaching an acceptable bitrate.</p>

<p>Is this something specific to <code>x264</code> or is it that my settings for the NAS machine are too demanding? The video sources are 1080p at 30 fps.</p>

<pre><code>ffmpeg -i ""$i"" -c:v libx264 -preset veryslow -crf 23 -af ""volume=25dB, highpass=f=200, equalizer=f=50:width_type=h:width=100:g=-15"" -c:a aac -strict experimental -b:a 192k
</code></pre>

<p>Should I be concerned about this, and what can be done about it?</p>
","<p>Your rate control mode is CRF, so there's no target bitrate to hit. And for a value of 23, QP of 29 is what I normally see used. Ultimately, the test is whether the video <em>looks</em> acceptable.</p>
","17091"
"Is it possible to find a webcam with large aperture?","1861","","<p>I'm looking for a Webcam with large aperture and shallow depth of field, similar to the F 1.8 Canon lenses, so far It has been really difficult since most of the specifications for any webcam are meant for home users.</p>

<p>I do video casting with green screen, and it does help tremendously having large aperture to diffuse the green screen (color evenness) and clarity at night with inexpensive lighting equipment. Also editing time is reduced considerably since I wouldn't have to re-sync audio and video or import files from the DSLR to my computer. </p>

<p>Please give your thoughts.</p>
","<p>Actually, your best bet is to use a PCIe or USB 3.0 HDMI input card, like a <a href=""http://www.blackmagicdesign.com/au/products/intensity"" rel=""nofollow"">Blackmagic Intensity</a>, which will take the clean HDMI output of many high quality cameras (e.g: a Nikon D600 or D800, a Canon C100 or C300, etc.) and make it available to other apps on your computer. This is what professional productions do for green screen work to ensure they're getting the full quality image out of the camera before it hits the camera's compression software.</p>
","9920"
"How to export a .AVI as a sequence of images in Avidemux","1859","","<p>I'm trying to simply export every frame of an <strong>.AVI</strong> movie as a sequence of images (<strong>.tiff</strong>) or a folder of <strong>.jpeg</strong> images.</p>

<p><a href=""http://www.digital-photo-software-guide.com/avi-to-jpeg.html"" rel=""nofollow"">This web page</a> says I can simply select all frames in Avidemux, then go to:</p>

<p><em>File --> Save --> Save JPEG image</em></p>

<p>But that only ends up saving one frame instead of the entire selection. Any thoughts? </p>

<p>I'm using <strong>Avidemux version 2.6.10</strong></p>
","<p>You need to downgrade to 2.5 since 2.6 does not support <a href=""http://avidemux.org/smif/index.php?topic=11632.0"" rel=""nofollow"">image sequence</a> export.</p>

<p>Alternatively, you could download <a href=""http://ffmpeg.org/download.html"" rel=""nofollow"">FFmpeg</a> and run</p>

<pre><code>ffmpeg -i input.avi -q:v 1 in%d.jpg

ffmpeg -i input.avi in%d.tif
</code></pre>
","18798"
"Premiere distorts picture while paused","1858","","<p>Premiere is distorting footage while paused, making color grading ridiculously challenging. </p>

<p>Whether the media is playing in the source monitor or the sequence monitor, the picture is pixelated and tinted red and yellow. While playing, the picture looks normal. Here are two screenshots from the Black Magic Cinema Camera. The same happens with the 5Dmk3 H.264 footage.</p>

<p><em>Paused</em>
<a href=""https://i.stack.imgur.com/db5Us.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/db5Us.jpg"" alt=""paused""></a></p>

<p><em>Playing</em>
<a href=""https://i.stack.imgur.com/hqaHb.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hqaHb.jpg"" alt=""playing""></a></p>

<h3>Troubleshooting</h3>

<p>Here's what I've tried and the results so far:</p>

<ul>
<li>I <strong>reinstalled</strong> Premiere Pro CC 2015.</li>
<li>I tried from <strong>different cameras.</strong> The issue persists in every case. I've tried with H.264 from a Canon 5Dmk3 as well as with ProRes 422 HQ from a Black Magic Cinema Camera.</li>
<li>I tried footage from <strong>different shoots.</strong> Same problem.</li>
<li>I created a brand <strong>new project</strong> and imported new footage. It still distorted while paused.</li>
<li>I rebooted my Mac in <strong>safe mode,</strong> created a brand new project, and the picture wasn't distorted. It was, however, cropped down to only show 25% (the lower-left corner of the picture).</li>
<li>I tested in Final Cut Pro X 10.2.1. No problems here.</li>
<li>I tested in Speedgrade CC 2015. Same problem as Premiere.</li>
</ul>

<p>Normally it renders OK, but when I've tried to render to MPEG the distortion persists in every 2nd frame.</p>

<h3>My setup</h3>

<p>Here's the hardware and software combination I'm running on. If I hadn't successfully eliminated the distortion in safe mode, I'd think my OS was the issue. It may be, but this issue did not occur in 10.11's safe mode.</p>

<ul>
<li>Premiere CC 2015</li>
<li>OS X 10.11.5 (the public beta of El Capitan)</li>
<li>MacBook Pro (Retina, 15-inch, Late 2013)</li>
<li>2.6 GHz Intel Core i7</li>
<li>16 GB 1600 MHz DDR3</li>
<li>NVIDIA GeForce GT 750M 2048 MB</li>
</ul>

<h3>My question</h3>

<p>What next step(s) should I take to resolve this issue? (Or, what else can I do to understand it more deeply?)</p>
","<p>According to Adobe, it's a bug. Until Adobe fixes this bug with an update, the workaround is to <strong>switch off GPU acceleration</strong> in Project Settings. </p>

<p><a href=""https://i.stack.imgur.com/BhM4J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BhM4J.png"" alt=""Project Settings""></a></p>
","16246"
"Delete unused video clips in Vegas","1857","","<p>After finishing editing a video with Sony's <em>Movie Studio Platinum 12.0</em>, I'm left with a giant folder of uncompressed rushes. I would like to delete some of them to save disk space.</p>

<p>Is there an easy way to keep only the video files that I have used in a project ?</p>
","<p>I use Vegas Pro, but I think this applies to the Movie Studio version as well. As far as I know there is no magic clean up button that removes the unused files from your folder. However, if you go to the 'project media' tab in Vegas, you will see reference icons of all the files that were used during your editing session including jpegs, and .wav files. This is an accumulation, so if you pulled a clip in and then decided to delete it from the timeline it will still show up here until you click the button with the lightning bolt icon, this updates this folder so it will show you everything that is currently on the timeline.</p>

<p>This can be your reference to manually remove from the folder outside of Vegas. Caution: if you delete a file that is in the timeline, Vegas will complain and ask you for the location of the file the next time you launch this session. Likewise, if you move or rename any clips that were in the timeline, Vegas will treat this like a lost file and you will have to show Vegas where it is.</p>
","8034"
"Which software gets the fastest rendering times?","1854","","<p>I'm sure there are benchmarks out there, but I'm not sure where to find them. Circa 2006 I was holding on to Vegas 6 without letting go because I could see very noticeable differences in its rendering times. (It's super fast!) And I've made fun of people who tough it out with Final Cut or Premiere for its atrociously slow rendering times. I'm just curious if anyone benchmarks this stuff.</p>
","<p>I'm not sure that it is necessarily possible to get meaningful benchmarks for video editing render times now.  With the advent of system's using GPU processing for rendering, the already complicated field got even tougher.  There are simply too many variables.  It depends on the performance of particular effects and plugins and what kind of hardware and bandwidth needs there are within the project.  Since the needs of projects differ so greatly, I don't see how a meaningful benchmark could be established unless it was to try benchmarking individual filters within the applications.</p>

<p>In general, the new render engine in Premiere is very fast though.  Even for software only it works quick, but with GPU acceleration, it gets pretty crazy.  Running software only, I can run a full resolution all I frame 1080p 24fps signal with color correction applied through my tower with fairly minimal frame drops.  I'm unfortunately PC only so I can't compare to FCPX.</p>
","7644"
"Software for video cut automation / scripting","1853","","<p>I am very new to a/v editing and need to automate a simple process. Basically what I need is a way for my mom to capture video from her camera and input a series of numbers like</p>

<pre><code>1:02 1:30
1:55 2:34
3:10 10:00
</code></pre>

<p>then the software to process the full video, cut out only the sequences specified by these time signatures and mix everything into one file.</p>

<p>I am a software developer by trade so ideally that would be some software that has a scripting language which I can learn and make a script in. I can semi-automate the process up to the point where it needs to cut out the desired sequences. I would do it myself for her but we don't live in the same town.</p>

<p>Note that no special editing is required, only cutting out sequences and stitching them together. I have googled around after ""adobe premiere automation"" and similar but found nothing. I am sort of familiar with AP, but it can be any other software.</p>

<p>If all else fails I would have to use some sort of desktop automation, but I would rather avoid this.</p>
","<p>You can also try to use <a href=""http://www.avidemux.org"" rel=""nofollow"">Avidemux</a> -- its free and open source program + you can use it on Windows, Linux and Mac.</p>

<p>Here is the <a href=""http://www.avidemux.org/admWiki/doku.php"" rel=""nofollow"">documentation wiki on Avidemux</a>, where you can find section ""<a href=""http://www.avidemux.org/admWiki/doku.php?id=tutorial%3ascripting_tutorial"" rel=""nofollow"">Scripting</a>"": </p>
","5026"
"How can I add fade in and fade out effects to a video?","1829","","<p>I tried searching for freeware on Google which can do this. But I have not been able to find a software which clearly mentions <code>fade in</code> or <code>fade out</code> in its specifications.</p>

<p>I wanted to have a scene with music in the background. At <code>fade out</code> both the music and the video should fade out (or grow darker to be </p>
","<p>Windows Live Movie Maker is a free app that can do both audio and video fades (both in and out).  You probably already have this application installed on your computer, and if you need further help I would imagine you can find a number of YouTube tutorial videos.</p>

<p>Once you import your video, click on the Edit tab and you'll see the audio fades immediately.</p>

<p>Then, click on the Visual Effects tab, and click the down arrow at the right of the single row of effects to expand these.  Scroll down to ""Motion and Fades"".  In this group you'll find what you're looking for.</p>

<p>If you don't already have WLMM on your computer, there are a jillion places where you can download it.  Incidentally, the latest version is called ""Windows Movie Maker 2012"", for Windows 7 and 8, so you'll want to look for that unless you're still running Vista or XP.</p>
","4902"
"Video Editor with custom effect masking and shape/drawing overlays","1827","","<p>I've spent a very long time on this and have been unimpressed with 90% of the main stream video editors, so I'd really appreciate some expert advice on this.</p>

<p>I'm looking for a video editor (hopefully within the 100-150 range if possible) that has the following options:</p>

<ol>
<li>""Effects Masking"" - ability to draw custom masks for blurring small segments of text on screen captures. A lot of the editors I see allow you to import a png file for this, but I really would rather have this in the editor, as I have a lot of footage and making a new png for every time the text changes would take forever.</li>
<li>Customizable basic shapes/drawing tools for overlays - NOT cheesy animated overlays; I want to be able to highlight areas of the screen by being able to draw boxes or circles around them.</li>
</ol>

<p>These seem like basic requirements, but I've had a really hard time with this, so I would REALLY appreciate your help!</p>
","<p>That isn't really rotoscoping, just mask drawing.  Rotoscoping is more on a frame by frame basis generally.  Either way, After Effects will do what you are looking for very easily, but is not in your price range (not even nearly).  Largely, you get what you pay for though and I'm not sure that there are a whole lot of cheaper options that allow you to draw your masks in the editor.</p>
","10013"
"Easy way of editing keyframes in adobe Premiere","1818","","<p>I'm a first time adobe premiere user. I can get the results I want after a fashion, but I find the methods I use way to cumbersome. I was wondering if you guys could point me some shortcuts.</p>

<p>I have two video streams from two camera's (slides &amp; stage). I fade over from one camera to the next depending on what happens. I have added both streams to the sequence and just turn the opacity of the top stream to 0 when I want to show the bottom stream.</p>

<p>I have a large bunch of timecodes for when I want to make a fade-over. However, the default fade is way to slow. So I have to now: </p>

<ol>
<li>go to the time code</li>
<li>add a keyframe</li>
<li>drag the frame to 0 or 100</li>
<li>right click selecte bezier</li>
<li>drag the bezier handle upwards enough that the fade speed is acceptable</li>
</ol>

<p>That's a lot of mouse work, fiddlings and clickings. I have found the effect controls which allow me to fine tune the controls, but still with a lot of imprecise mouse moving.</p>

<p>I would love to have a way to just specify all keyframes and bezier parameters in a text file or so, so I can quickly edit them change all bezier parameters for all keyframes at the same time and be able to twiddle the results. Another possibility would be to copy keyframe settings, apply settings to multiple keyframes at once or to define keyframe 'classes' or so.</p>

<p>Even more so, I'm just learning premiere so I sometimes make rooky mistakes. For example by picking the wrong sequence framerate (I think) i managed to create a result that flickered heavily. I can't edit sequence settings so I had to create a new sequence. But I can't find how to copy markers or keyframes from one sequence to another. So I have to set them all over again :(</p>

<p>Any ideas on how I can improve the efficiency of this process? Or maybe I shouldn't be using keyframes and opacity at all?</p>

<p>thanks!</p>
","<h2>Adjusting the default video transition duration</h2>

<p>Go to <code>Preferences -&gt; General</code> and change the <code>Video Transition Default Duration</code> to a frame amount of your choice.</p>

<h2>Optimising current workflow</h2>

<p>I can think of some optimalisations regarding your current workflow. It's not a drastic change, but might help you do the task faster.</p>

<ol>
<li><p>I would suggest splitting the timecode list per videotrack (slide &amp; stage) according to the following scheme:</p>

<p>List 1: slide fade-outs<br>
List 2: slide fade-ins<br>
List 3: stage fade-outs<br>
List 4: stage fade-ins  </p>

<p>List 1 should match list 4 and list 2 should match list 3.</p></li>
<li><p>Go to the first fade-out on a list. Create a keyframe pattern you like for the fade-out, for example the one shown below:
<img src=""https://i.stack.imgur.com/ZYtyT.png"" alt=""enter image description here""></p></li>
</ol>

<p>In the effects panel it looks like this:
<img src=""https://i.stack.imgur.com/X8RfW.png"" alt=""enter image description here""></p>

<p>A linear keyframe followed by a bezier keyframe followd by a hold keyframe.</p>

<ol start=""3"">
<li><p>Now select those three keyframes in the effect panel by dragging a rectangle around them or clicking each keyframe while holding <code>ctrl</code> (Windows) <code>cmd</code> (Mac).</p></li>
<li><p>Right click and press <code>Copy</code></p></li>
<li><p>Now go to each fade-out timecode on both video tracks (so list 1 and 3) and in the effects panel right click and press <code>Paste</code>. Of course you can also hit <code>ctrl+V</code> or <code>cmd+V</code>.</p></li>
<li><p>Repeat this process for list 2 and 4.</p></li>
</ol>

<p>In this way you only have to set the bezier speed once and just copy paste the keyframe sequence at each timecode.</p>

<h2>Exporting keyframe locations to textfile</h2>

<p>I'm not aware of a way to do this directly, but if you export your project to a Final Cut Pro XML or to EDL (Select sequence, then: <code>File -&gt; Export</code>) you have the ability to change the position of /add keyframes through a text editor by editing the <code>.edl</code> or <code>.xml</code> files . You can then import the <code>.edl</code> or <code>.xml</code> file into Premiere Pro as a new sequence.
However, these formats do not support bezier keyframes.</p>
","6903"
"ffmpeg - Unable to find a suitable output format","1801","","<p>This is a part of my command:</p>

<pre><code>-r 60 -preset medium -codec:v libx265 -ar 48000 -acodec aac -shortest -strict experimental -sn -vsync 1 -pix_fmt yuv420p -b:v 30000000 -movflags +faststart  -x265-params high-tier=0:pmode=1:wpp=1:tune=fastdecode :bitrate=30000:fps=60:keyint=360:min-keyint=180:vbv-bufsize=30000:vbv-maxrate=30000:scenecut=0  -metadata title
</code></pre>

<p>and when i run it i get the following error:</p>

<blockquote>
  <p>Unable to find a suitable output format for
  ':bitrate=30000:fps=60:keyint=360:min-keyint=180:vbv-bufsize=30000:vbv-maxrate=30000:scenecut=0'
  :bitrate=30000:fps=60:keyint=360:min-keyint=180:vbv-bufsize=30000:vbv-maxrate=30000:scenecut=0:
  Invalid argument</p>
</blockquote>

<p>I think there might be a missing space or something like that somewhere but i'm not too sure what's wrong with this syntax?</p>

<p>Thanks!</p>
","<pre><code>...tune=fastdecode :bitrate=30000... 
                 ^^^
</code></pre>

<p>There's a space that should not be there.</p>
","19079"
"How to re-encode a video file's audio, but leave the video stream with ffmpeg?","1796","","<p>Does anyone know how to take a video and re-encode the audio, but just copy the video data using FFMPEG?</p>
","<p>So Apparently FFMPEG can do this, I had to use <code>-c:v copy</code> and that will leave the video stream intact. I actually used an app called iFFMPEG to figure this out.</p>
","9803"
"How to keyframe a crop in Premiere Pro CC","1795","","<p>I am working on a school project, and it starts off with a 5 second count up that takes up the full screen. I am using the prebuilt <em>insert timecode</em> effect built into Premiere. What I have done is cropped all but the minutes and seconds out of the view.</p>

<p>What I would like to do next is to transition it to the bottom right corner while shrinking it.</p>

<p>Any help/advice would be welcome. If theres another program that does it easier, then feel free to let me know.</p>

<p><strong>EDIT:</strong></p>

<p>Here is what I have:
<img src=""https://i.stack.imgur.com/N4mCo.png"" alt=""Cropped Big Version""></p>

<p>Here is it without the crop:
<img src=""https://i.stack.imgur.com/nIqgd.png"" alt=""Non-Cropped Big Version""></p>

<p>Here is what I would like to have the timer transition to:
<img src=""https://i.stack.imgur.com/xOz85.png"" alt=""Cropped Small Version""></p>

<p>Here is the non-cropped version of the smaller timestamp:
<img src=""https://i.stack.imgur.com/rySiy.png"" alt=""Non-Cropped Small Version""></p>
","<p>Before I start I'd like to mention that this would be much easier in After Effects. If you plan on doing this more often, you should consider learning AE to be able to do things like that faster.</p>

<p>So, to follow your approach I created a transparent layer, put the timecode effect on it and cropped it using the crop effect. It looks like this right now:</p>

<p><img src=""https://i.stack.imgur.com/Q8DAs.png"" alt=""stage 1""></p>

<p>Since you want to resize and move it, you'll need to activate the animation for two parameters in the effects panel: The ""size"" parameter of the timecode effect and the ""position"" parameter under the motion effect (which is applied to each clip by default). You can do this by clicking the clock symbol next to the parameters name (see the next image). Make sure that your playhead (in the timeline) is at position 00:00, since activating the animation will create a keyframe at the current position.</p>

<p><img src=""https://i.stack.imgur.com/AgaRV.png"" alt=""stage2""></p>

<p>Now, move the playhead to the point where you want the animation to end (i.e. the timecode has arrived at your desired position and size). First, alter the percentage value of the ""size"" parameter of the Timecode effect. A keyframe will automatically be created (see the small diamond shape in the timeline next to the effect settings panel). Then, adjust the ""position"" parameter of the motion effect. Again, a keyframe will automatically created. Adjust both parameters until you're happy with size and position. Then you're basically done.</p>

<p><img src=""https://i.stack.imgur.com/QBncX.png"" alt=""stage 3""></p>
","15058"
"Grading Colour To Black And White In Davinci Resolve","1786","","<p>I use Photoshop / ACR a lot and I'm well used to they way they handle conversion from black and white. I have control over reds, greens, blues, yellows, cyan and magentas, which allows a huge flexibility in conversion.</p>

<p>I'm trying to transfer this knowledge to Resolve, but I'm struggling to find a comparable workflow. The Saturation setting in the Primary settings doesn't give any such control, so I'm assuming I need to break the footage in it's RGB components and work on each separately, however I can't find a way to split and recombine them so that I end up with a monochrome image.</p>

<p>I can find almost nothing relating to grading colour footage to black and white in resolve, so what is the most flexible approach?</p>
","<p>There is actually a specific option for this in Resolve.</p>

<ol>
<li>In your primaries pane (bottom left of the color tab) navigate to the ""rgb mixer"" tab.</li>
<li>In the top right of the tab, click the options menu (3 dots)</li>
<li>Check the 'make monochrome' command.</li>
</ol>

<p>Now you'll see 6 from the 9 rgb mixer bars turn gray and you'll be able to use the other 3 to mix you black and white the way you want to. </p>

<p>Good luck! </p>
","17089"
"How can i check if a particular sound I used in my video was copyrighted?","1786","","<p>Been using royalty free music for some time, however youtube shows that I am using some copyrighted material. So just wanted to know if there is any way by which I can check if a song is copyrighted or not?</p>
","<p>In nearly all countries, copyright protects creative works automatically. Some exceptions and restrictions apply, but unless the author has voluntarily added his work to the public domain or the copyright has expired (common terms are seventy and ninety years after publication or after death of the author), it's safe to assume all songs are copyrighted, including the royalty-free music you've been using.</p>

<p>Whether or not it's okay to upload material that contains this music to youtube, that would depend on the author's consent. Sometimes, the music is released under a blanket licence that allows redistribution. If not, contact the author to obtain permission.</p>
","10038"
"How to concatenate multiple videos with ffmpeg?","1786","","<p>Need to:</p>

<p>concatenate 3 mpg4 video clips (each around 1 minute long, and 800x500 size) + background mp3 -> into a 1-minute long (1920x1080) video composed of the first 20 seconds from each of the 4 videos.</p>

<p>So each clip would need to be enlarged (from 800x500 to 1920x1200 to keep aspect ration), and then the height (cropped down to 1920x1080 from the bottom of the video).</p>

<p>Then each video cut down to the first 20 seconds.</p>

<p>Then the 3 videos are concatenated (along with a 1 minute long mp3 file to play in the background of the final video).</p>

<p>Questions:</p>

<ol>
<li>Is ffmpeg capable of doing this correctly and how?</li>
</ol>
","<p>Sure. Command is</p>

<pre><code>ffmpeg -i 1.mp4 -i 2.mp4 -i 3.mp4 -i bg.mp3
-filter_complex
""[0]trim=0:20,scale=1920:-1,crop=1920:1080:0:0[a];
 [1]trim=0:20,scale=1920:-1,crop=1920:1080:0:0[b];
 [2]trim=0:20,scale=1920:-1,crop=1920:1080:0:0[c];
 [a][b][c]concat=3[v]""
-map ""[v]"" -map 3:a
-shortest out.mp4
</code></pre>

<p>Skip the <code>-shortest</code> if the audio is slightly longer than 1 minute. Alternatively, adjust one of the trim endpoints to make up for the shortfall.</p>

<p><em>to play in the background of the final video</em> -> I assume final here means the output and not the 3rd video.</p>
","20431"
"ffmpeg: how to convert to DNxHR in Windows 10?","1781","","<p>I have installed ffmpeg version <code>3.2.2</code> from the ffmpeg.org download page.</p>

<p>When I enter <code>ffmpeg formats</code>, I see <strong>dnxhd</strong> but not <strong>dnxhr</strong>.  What do I need to do to be able to manipulate <strong>dnxhr</strong>?</p>

<p>I have downloaded and installed <a href=""http://avid.force.com/pkb/articles/en_US/download/Avid-QuickTime-Codecs-LE"" rel=""nofollow noreferrer"">Avid Codecs LE 2.7.3</a> as well but that didn't seem to help.</p>
","<p>There's a single decoder/encoder which reads/converts to both DNxHD and DNxHR.</p>

<p>You have to set the correct profile switch. Available options are</p>

<pre><code> dnxhd                        E..V....
 dnxhr_444                    E..V....
 dnxhr_hqx                    E..V....
 dnxhr_hq                     E..V....
 dnxhr_sq                     E..V....
 dnxhr_lb                     E..V....
</code></pre>

<p>Basic minimal syntax would be</p>

<pre><code>ffmpeg -i in.mp4 -c:v dnxhd -profile:v dnxhr_hq out.mov
</code></pre>

<hr>

<p>The Avid codecs package has no interaction with ffmpeg.</p>

<p><strong>[19-1-2017]</strong> At this time, the encoder does not support 444 or HQX profiles.</p>

<p><strong>[4-2-2017]</strong> Now, it does.</p>
","20434"
"Can Sony Vegas Pro be used or compared to the output produced by Final Cut Pro for professional use?","1775","","<p>I am a PC user and use Sony Vegas Pro. I really like it for editing purposes. I do not like Adobe Premier or Avid which work on Windows and believe that the top 3 tools are AVID, Sony Vegas, and Premier. Can Vegas be used for professional productions today or semi pro; is the quality and tools up to notch? Can it compare to Final Cut Pro which I see professionals use?</p>

<p>The criteria can vary but in general if it is feasible. </p>
","<p>Vegas is a mature, full-featured NLE. I use it regularly for professional work from spots, to corporate video to feature films. I've also used it for semi-professional things like editing a video of my stepdaughter's choir concert for a Christmas DVD.</p>

<p>Here's where I run into problems. My producing partner is all about FCP. There's no clean way to export an EDL between our two systems so that we can BOTH edit. We work around it. If there's something with Sony's proprietary HD format that's somewhere between HDV and XDCAM, or DSLR footage from a Nikon, I edit because Vegas imports those clips easily. When FCP imports those formats, it ranges from not recognizing the file to displaying interlace artifacts. On the other hand, my Vegas setup chokes on Canon HDV, so that gets edited on FCP.</p>

<p>Whether using FCP or Vegas, the output is comparable, I'm not saying identical because of inherent differences in how MacOS/FCP and Windows/Vegas implement the various output codecs. Needless to say, though, our material from FCP and Vegas has been used in every media from Broadcast to family videos. </p>

<p>The tool, in and of itself, it not the issue. Professionalism comes from a.) the footage and b.) the skill of the editor. Shoot a corporate video with the onboard mic, no tripod and no lights and Eisenstein himself couldn't make that a professional looking edit. Conversely, light and mic a two year old's dance recital, shoot it with three Steadicams for proper coverage, and you'll get something worthy of a broadcast dance show. </p>

<p>It's just that for some projects, I need a phillips head and for some I need a flathead. It doesn't matter if Vegas in the flathead or the phillips and FCP the other. Which one is the best for the job? No one's comparing MSPaint to Vegas...it's just a tool, and if you use it right, you'll be very pleased wit the results. </p>
","4111"
"Why must a sample rate by at least twice a frequency in order to reproduce that frequency accurately?","1769","","<p>I read this:
To reproduce a given frequency, the sample rate must be at least twice that frequency.
Why is this so?</p>
","<p>That's Nyquist-Shannon sampling theorem. Think about it in the other way: imagine you are given a set of samples (the black dots in the image).</p>

<p><img src=""https://i.stack.imgur.com/05b9o.png"" alt=""http://i.stack.imgur.com/ld5VI.png""></p>

<p>The most intuitive way to recover the original analog signal would be by joining these samples. But you can easily notice that there is not a unique way of joining them. In fact, there are infinite possible signals that would contain the whole set of samples. Obviously, given the sample set, you would join them the easiest way, i.e. the blue signal.</p>

<p>Now, what about sampling? In the image you also have a red signal (which frequency is 1 Hz) that has been sampled too slowly, resulting in the black dot sample set. To have a clue about the real signal, we should have sampled it at, at least, 2 Hz. That way, we also would have samples (dots) at 0.5, 1.5, 2.5, etc seconds. And, as you can draw in Paint, the easiest path to join all the samples would be, again, the red signal. So we would have recovered our original signal successfully.</p>
","6888"
"Keynote video pausing/playing","1755","","<p>I have an idea for linking a keynote presentation with a video (exported from a 3d tool).  The idea is that each time you 'click' (as you would to move to the next slide/transition) it would play x more frames of the video then pause.  So frame 1-30 might be the first animation, and it would pause on frame 30.  Then, you click again and it plays 31-60, stopping on 60.</p>

<p>Is such a thing possible and if so how would I achieve it?  Assume a video file is available in the required format.</p>

<p>Many thanks.</p>
","<p>The easiest way I can think of would be to actually do it as multiple videos and stills.  Have the first video play and then go to a still of the last frame.  The next would be the next video clip followed by the still.  Continue doing that for the length of it.  Not sure if it works like that in Keynote or not, but that's how I'd do it in any of my sequencing/presentation packages.</p>
","7961"
"Downscaling 4k to 1080p in Premiere Pro, Image Zooming 2x","1736","","<p>When I drop 4k footage onto the timeline in Premiere Pro, I get a 4k sequence. When I change the sequence to 1080p, the image zooms in. This makes sense. I then scale the image back 50% and I'm back to normal.</p>

<p>In every single tutorial on downscaling 4k footage in premiere, no one ever has to do the 2nd step that I do. I thought I solved his issue by going to:
""Edit > Preferences > General"" and ticking the box ""Default scale to frame size"" but it's still happening.</p>

<p>How can I avoid this second step that <a href=""https://youtu.be/9oEWvBsN63U?t=91"" rel=""nofollow"">no one seems to have to do</a>?</p>
","<p>Changing the preference doesn't change any clip settings retroactively.  But once you have set it, and then create a new project and a new sequence, and when you freshly import your 4K clip and drop that freshly imported clip onto your freshly created sequence in your freshly created project, it should work as you expect.  And if you right click on the clip, it should have a checkmark next to ""Scale to Frame Size"".</p>
","18581"
"Configure file naming & presents in Adobe Media Encoder","1728","","<p>I'm running the latest version of Premiere Pro &amp; Encoder (CS6). Encoder has a watch folder capability to automatically encode files, but it defaults to Flash output with a specific preset &amp; gives the file the name of the original sequence selected in the Premiere project.</p>

<p>What I'd like to do is drop a Premiere project file in a watch folder and have my rendering server pickup the file and start encoding it but I want to configure a specific output file type, present &amp; name the output file the name of the Premiere project file. I can't figure how to do this or if it is even possible (aside from the watch folder &amp; encoding).</p>

<p>The current project I'm on has over 100 video outputs and we're looking at 1 Premiere project per output. This is just the first project... looking at many more in the future so working on a workflow now.</p>
","<p>You can change the format, once you add a watch folder to your watch folders window, click the F4V under the format tab, and you can configure how you want each video from that watch folder to be set as...</p>

<p>Furthermore if you click the output folder destination, you can change the filename and destination in which the movie will be saved to.</p>

<p>Hope this helps</p>
","6953"
"Export to wide screen (16:9) from Final Cut pro X","1725","","<p>Apologies for the noob question but I've just started trying to learn FCP and couldn't find the settings to export my movie in 16:9 aspect ratio.</p>

<p>Can anyone tell me the quickest way to go about it?
(source material has a variety of different aspect ratios)</p>
","<p>This answer assumes that the video that you're editing is already in a 16:9 ratio, otherwise when you export there will be black bars on the sides or above and below your video.</p>

<p>A 16:9 ratio is the standard for HDTV and Full HD. You're probably familiar with the Youtube video qualities - 360p, 480p, 720p, 1080p, etc. These are all 16:9. Therefore, all you have to do is export in one of these resolutions - 1080p is Full HD and 720p is HD.</p>

<p>For more reading on common aspect ratios, reference this <a href=""http://en.wikipedia.org/wiki/16:9#Common_resolutions"" rel=""nofollow"">Wikipedia page</a>.</p>
","8630"
"How do you install Adobe Production Premium CS5 on a different hard drive in windows?","1699","","<p>Is there a way of installing Adobe Production Premium CS5 on a hard drive other than C:\? I tried changing the install path in the installer but after installing parts of the program don't work and it can't find any plug-ins I add. I have windows 7.</p>

<p>After a lot of messing around I figured out how to get it to work and will post the answer.</p>

<p>The reason I actually want to do this is because my main OS drive is an SSD and there's not really enough space to install the whole Adobe suite on it.</p>
","<p>After a long time of messing aroung I got it to work by tricking the software into thinking it was actually in the default location it likes at 'C:\Program Files\Adobe' by creating directory junctions. In order for it to work the hard drive must be formatted with the NTFS filesystem.</p>

<p>I used <a href=""http://code.google.com/p/symlinker/"" rel=""nofollow"">http://code.google.com/p/symlinker/</a> to create the directory junctions. You need to map the following folders to folders on the other hard drive you want to install the suite on. Run the symlinker software as an administrator.</p>

<ul>
<li>C:\Program Files\Adobe</li>
<li>C:\Program Files\Common Files\Adobe</li>
<li>C:\Program Files (x86)\Adobe</li>
<li>C:\Program Files (x86)\Common Files\Adobe</li>
</ul>

<p>Make sure that you select <strong>Directory Junction</strong> in the dropdown for type of link otherwise it won't work.</p>

<p>In the symlink program the box which says 'give a name to the link' is the folder name. So for example to set up the first path you would enter 'C:\Program Files' in the top box. Then 'Adobe' as the link name and then '[New Drive]:\Adobe From Link' (as an example) in the third box and then do the same for the other 3 and they all need to go to different folders.</p>

<p>Then when you've done this and checked that the folders are being mapped correctly you can simply install the adobe suite into C:\Program Files\Adobe (where it wants to go) and it will actually set itself up on your other hard drive.</p>

<p>I know it's not common to even think about putting it on another hard drive but I had to and I hope this will be useful if there's someone else out there that ever needs to and will save a lot of time!</p>
","3721"
"How to connect a camera to a computer 100ft away?","1696","","<p>We are trying to live stream our worship service on Sunday morning.  We have located an ideal spot for the video camera. Unfortunately it is 100 ft away from the sound and network connections which are ""behind"" the front wall of the worship space.</p>

<p>What can we use to connect a consumer level ($400) video camera to a computer that is 100 or so feet away.  My understanding is that FireWire will run 10 ft max, and composite video no more then 30 feet without the image becoming soft and having color bleed issues.</p>
","<p>As you have Wi-Fi access you could buy a Wi-Fi capable camera. For example the <a href=""http://www.ebay.com/itm/HP-T450-Wireless-HD-Camcorder-With-Live-Streaming-Video-Wi-Fi-/400348234610"" rel=""nofollow"">HP T450</a> costs $134 and is able to stream to <a href=""http://www.ustream.tv/"" rel=""nofollow"">Ustream</a> via Wi-Fi. The pages 39-42 of <a href=""http://h10032.www1.hp.com/ctg/Manual/c03561403.pdf"" rel=""nofollow"">the manual (PDF)</a> elaborate on the use of Wi-Fi to stream to Ustream.</p>

<p>This is just one of many Wi-Fi capable camera's. <a href=""http://store.sony.com/p/Sony-Live-Streaming-Video-Camcorder-MP4-Recorder-Video-Footage-Camera/en/p/MHSTS55/S#features"" rel=""nofollow"">Sony's Bloggie</a> ($150) can also stream over Wi-Fi, but it uses <a href=""http://www.qik.com"" rel=""nofollow"">Qik</a> as its footage server.</p>

<p>If your Wi-Fi signal turns out to be too weak to stream over Wi-Fi, both camera's have a HDMI out so you could use a <a href=""http://www.amazon.co.uk/Component-YPbPr-Audio-Video-Converter/dp/B00622MA6W"" rel=""nofollow"">HDMI -> RGB converter</a> and utilise AJ Henderson's answer.</p>

<p><em>I'm not affiliated with any sellers/companies listed above</em></p>
","7222"
"Loudness control LUFS level adjustments","1695","","<p>Is there a tool that can adjust the loudness level of a video? </p>

<p>for example I need to make the loudness to be -23 LUFS with max deviation of 0.5 lu.</p>

<p>is there a simple tool/cloud service that doesn't cost too much, that can do this ? </p>
","<p>FFmpeg's <a href=""https://ffmpeg.org/ffmpeg-filters.html#loudnorm"" rel=""noreferrer"">loudnorm</a> filter can be used. Basic syntax is</p>

<pre><code>ffmpeg -i in.mp4 -c:v copy -af loudnorm=I=-23:LRA=1 -ar 48000 out.mp4
</code></pre>

<p>The loudness range (LRA) should be <code>2 x max deviation</code>.</p>

<p>Also see the <a href=""https://ffmpeg.org/ffmpeg-filters.html#ebur128-1"" rel=""noreferrer"">ebur128</a> filter for measuring loudness.</p>
","20505"
"open-source or free AVCHD conversion to image sequence","1684","","<p>I am looking for a good and <em>free option</em> to convert AVCHD (Full-HD from a Panasonic Lumix GH1) to image sequences. I am currently using <em>Blender</em> but it is cumbersome in that it only allows me to convert the videos if I disable the preview - should the preview be open in my workspace, Blender halts for about 1-2 mins until it shows a preview image - any further click on the preview halts it again.</p>

<p>It can very well be a <em>command line tool</em> (I am willing to craft the command line options myself if needed). </p>

<p>The output images can be any common format (PNG, JPG, even TIFF). The conversion to image sequences is asked for because Blender (that I use as editing software) has no problems with image sequences and other formats but AVCHD seems to still be painful.</p>

<p>Thanks!</p>
","<p>Wonderful <a href=""http://ffmpeg.org/"" rel=""nofollow"">ffmpeg</a> command line utility is the solution. Just check their <a href=""http://ffmpeg.org/documentation.html"" rel=""nofollow"">documentation</a>, but something like the following will probably work for your case:</p>

<pre><code>ffmpeg -i video.mts video%05d.png
</code></pre>

<p>This will take your video, and create video00001.png, video00002.png... files in the same folder.</p>

<p>You can find a lot of guides for ffmpeg, or ask here for a specific soltion. We can try to figure out.</p>
","5590"
"ffmpeg: pillarbox 4:3 to 16:9","1680","","<p>I'm confusing myself over how the video filters work, particularly how they work together, I think.  I have two MKVs, pulled from DVD, that are respectively 16:9 and 4:3.  What I would like to do is ultimately combine them (mkvmerge widescreen.mkv + fullscreen.mkv) and to do that I need matching display aspect ratios.</p>

<p>So I would like to pillarbox the 4:3 video as I encode it, and then hopefully append it to the 16:9 video.  I've tried a number of filters to accomplish this, and the closest I was able to get was a slightly wider than expected video, otherwise pillarboxed properly.</p>

<pre><code>ffmpeg -i fullscreen.mkv 4:3 -vf ""pad=853:480:66:0,scale=720x480,setdar=16:9"" [etc.]
</code></pre>

<p>The pixel dimensions are 720x480, the display dimensions are 853x480, and the two MKVs can be successfully merged (after reencoding the widescreen.mkv to the same video codec, but without video filters).</p>

<p>However, the resulting display of the fullscreen video is pillarboxed, yet slightly horizontally stretched.  In 1:1 pixels it measures 720px across, not 640.</p>

<p>I figure I'm doing the math wrong, or misunderstanding how the filters are supposed to operate, or something simple, but my understanding of ffmpeg video filters is basically nothing.</p>
","<p>Assuming both MKVs are rips of NTSC DVDs, these are the commands you need:</p>

<p><strong>For 4:3 video</strong></p>

<pre><code>ffmpeg -i ""input43.mkv"" -vf ""scale=640x480,setsar=1,pad=854:480:107:0"" [etc..]
</code></pre>

<p><strong>For 16:9 video</strong></p>

<pre><code>ffmpeg -i ""input169.mkv"" -vf ""scale=854x480,setsar=1"" [etc..]
</code></pre>

<p>(Note H.264, the codec you are likely using, needs dimensions to be even, so specifying <code>853</code> as width will lead to ffmpeg feeding <code>854</code> pixels to the encoder and then auto-setting a funky SAR.</p>
","17344"
"Simple Video Editing Tool?","1678","","<p>Can someone suggest free tools for video editing and video making? If you can include the feature much better. Hope to be able to get some of your advice. I need it badly. I am really not into technology but greatly needed to make a simple video. Thanks!</p>
","<p>Why not try this list. All of this are basic software and very easy to use and learn. Hope this helps!</p>

<ol>
<li>Movie Maker</li>
<li>Magix</li>
<li>Media Composer</li>
<li>Premiere Pro</li>
<li>After Effects</li>
</ol>

<p>Try also to visit this link <a href=""http://www.techyv.com/questions/software-required-making-vedios-and-editing"" rel=""nofollow"">http://www.techyv.com/questions/software-required-making-vedios-and-editing</a> I believe this will going to be very helpful to you too.</p>
","4234"
"YouTube 3gp encoding settings","1671","","<p>I'm building a new media site and I'm taking cues from YouTube for which video file formats I should offer for widespread usability among mobile and desktop clients.  I'm doing all transcoding using <code>ffmpeg</code>.  My trouble is that YouTube's .3gp files have a Codec ID of ""3gp6"", a Video Format of ""MPEG-4 Visual"", and a Video Format Profile of ""Simple@L1"" (using <code>mediainfo</code> to interrogate the files.) I can't figure out which <code>ffmpeg</code> settings I can use to reproduce this.</p>

<p>If I use <code>-vcodec libx264 -profile:v baseline -level 1</code> in my <code>ffmpeg</code> command, then the file comes out with the right Codec ID, but the Video Format is ""AVC"" and the Profile is ""Baseline@L1.0"".</p>

<p>Alternately, if I use <code>-vcodec mpeg4 -profile 0 -level 1</code>, then the Video Format and Profile are correct, but the Codec ID is ""3gp4"" instead of ""3gp6"".</p>

<p>So my question is, how can I use <code>ffmpeg</code> to arrive at a Codec ID of 3gp6 along with a format of MPEG-4 Visual and a profile of Simple@L1?  I've been unsuccessfully Googling and experimenting on this for hours.</p>
","<p>I asked on the ffmpeg-users mailing list, and it was suggested that I add <code>-brand 3gp6</code> to my encoding command.  This achieved the result in <code>mediainfo</code> that I was looking for.  However, it remains to be seen whether this really reaches the same compatibility with older hardware that YouTube's 3GP files have.  I don't have any old hardware to test with, lol.  Anyway, as far as I'm concerned, problem solved for now!</p>

<p>If anyone is interested, here is the command I ended up with for reproducing YouTube's ""Format 36"", which is a 360x180 3GP video with about 230 Kbps overall bitrate:</p>

<pre><code>ffmpeg -i input-file.mov -vcodec mpeg4 -s 320x180 -qscale:v 20 -brand 3gp6 \
    -vf 'scale=iw*sar:ih,pad=max(iw\,ih*(16/9)):ow/(16/9):(ow-iw)/2:(oh-ih)/2' \
    -acodec libfaac -ar 22050 -ac 1 -qscale:a 70 output-file.3gp
</code></pre>

<p>And here is what I came up with for reproducing its ""Format 17"", which is a 176x144 3GP video with about 82 Kbps overall bitrate:</p>

<pre><code>ffmpeg -i input-file.mov -vcodec mpeg4 -s 176x144 -qscale:v 12 -r 10 -level 8 -brand 3gp6 \
    -vf 'scale=iw*sar:ih,pad=max(iw\,ih*(11/9)):ow/(11/9):(ow-iw)/2:(oh-ih)/2' \
    acodec libfaac -ar 22050 -ac 1 -qscale:a 45 output-file.3gp
</code></pre>

<p>In case you're wondering, that massive ""-vf"" argument automagically detects mismatches in aspect ratios between the input and output, and boxes the output if they differ.</p>
","12692"
"What to do with a VMP video file","1667","","<p>I recieved a VMP video file to convert to avi or mpeg or something. I searched the web but I cannot find any association with this file.</p>

<p>Can anyone help? Which program does this file come from?
What can I do with it?</p>
","<p>This is likely a video from a Sony/Vaio product having a vmp extension.  I don't know of any third party converters, however you should be able to export the video file from your Vaio/Sony product itself.  It'll automatically create an mpeg video file for you. </p>
","10174"
"ffmpeg split screen two video sources","1666","","<p>I am not able to figure out how to get ffmpeg
to make a video 
that combines (muxes) half-height-cropped  from two video sources. 
The latest generation of ffmpeg (3.x) has a ""vstack"" filter. </p>

<p>My screen resolution is set to 1280 by 720 pixels. </p>

<p>I have a webcam that is set to provide video at that same resolution. </p>

<p>I want the top half of the webcam's input/feed/stream to be layered on top of the 
<em>bottom</em> half of the screen-capture-recorder (recording my desktop).</p>

<p>Both source video streams, plus the end output result will have a width of 1280 pixels.  The combination of the half-height 2 video stream inputs will end up filling up 720 lines of vertical resolution.</p>

<p>This should be simple.</p>

<p>I'll paste a few of the commands that I've tried and that have failed to work.</p>

<p>ffmpeg.exe -f dshow -i video=""screen-capture-recorder"" -f dshow -i video=""Microsoft LifeCam HD-6000"" -vf ""crop=1280:360:1280:360"" -filter_complex vstack -vcodec libx264 -pix_fmt yuv420p -preset ultrafast 6000screen_take1.mkv</p>

<p>and</p>

<p>I even tried adding : 
""[0:v][1:v]vstack[v]"" -map ""[v]""
to that , above (previous) command.</p>

<p>See my Google Doc for complete output (what was in my cmd.exe window) :</p>

<p><a href=""https://docs.google.com/document/d/1SqtjEY_gM3rdeKPyRzUq_jJhPQUPCK5L4E5HPttwS3s/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/document/d/1SqtjEY_gM3rdeKPyRzUq_jJhPQUPCK5L4E5HPttwS3s/edit?usp=sharing</a></p>
","<p>If you're using filter_complex, then all filtering has to happen within that complex. So,</p>

<pre><code>ffmpeg.exe -f dshow -i video=""screen-capture-recorder"" \
           -f dshow -i video=""Microsoft LifeCam HD-6000"" \
-filter_complex ""[0:v]crop=1280:360:0:0[v0];[1:v]crop=1280:360:0:0[v1];[v0][v1]vstack[v]"" \
-map [v] -vcodec libx264 -pix_fmt yuv420p -preset ultrafast 6000screen_take1.mkv
</code></pre>

<p>This stacks the top half of the webcam video below the top half of the desktop capture.</p>

<hr>

<p>Having taken a look at the console output, you should actually try this:</p>

<pre><code>ffmpeg.exe -f dshow -rtbufsize 9000000 -i video=""screen-capture-recorder"" \
           -f dshow -i video=""Microsoft LifeCam HD-6000"" \
-filter_complex ""[0:v]crop=1280:360:0:0[v0];[1:v]scale=1280:-1,crop=1280:360:0:0[v1]; \
                 [v0][v1]vstack[v]"" \
-map [v] -vcodec libx264 -pix_fmt yuv420p -preset ultrafast 6000screen_take1.mkv
</code></pre>
","17894"
"Puppet tool - how do I make the arms stay attached to the same points on the body?","1659","","<p>Trying to make following character's body wiggle a bit, using the <strong>puppet tool</strong>:</p>

<p><img src=""https://i.stack.imgur.com/HTdIg.png"" alt=""enter image description here""></p>

<p>The face and arms are on separate layers as they will need to move independently. When the body moves via the puppet pins, the face and arms stay in their original positions. <strong>How can I 'attach' them to their assigned positions on the body?</strong> </p>
","<p>If you put a puppet pin at the point where the arms attach, then you can tie the position of the arm layer to the position of the puppet pin using an expression. This will attach the layer to the puppet pin, allowing you to move the pin and the layer it is on and have the arm follow.</p>

<p>It's easy to do, with no knowledge of scripting really required.</p>

<p><img src=""https://i.stack.imgur.com/QgB9y.png"" alt=""enter image description here""></p>

<p>So say your body layer is called <code>""Body""</code> and it has a puppet pin called <code>""L_Shoulder""</code>, which is on the body at the point where you want to attach the left arm. The left arm layer should have its anchor point at the point that you want it to pivot on, which coresponds to the <code>L_Shoulder</code> on the body layer.</p>

<p><img src=""https://i.stack.imgur.com/g81hW.png"" alt=""enter image description here""></p>

<p>Here's what you do:</p>

<ul>
<li>First, make the arm layer a child of the body.</li>
<li>Now apply an expression to its position property (hit <kbd>alt/opt</kbd> and click on the stopwatch). </li>
</ul>

<p>The expression we want is this:</p>

<pre class=""lang-js prettyprint-override""><code>thisComp.layer(""Body"").effect(""Puppet"").arap.mesh(""Mesh 1"").deform(""L_Shoulder"").position
</code></pre>

<p>This looks very difficult to type, but fortunately we don't have to. Next to the expression editor text box is a pickwhip, that we can use to select properties and values. After you <kbd>alt/option</kbd>-click the stopwatch, drag the pickwhip to the position property of the puppet pin, and it will fill it all in for you.</p>

<p><img src=""https://i.stack.imgur.com/j88bh.png"" alt=""pickwhip""></p>

<p>Because the puppet pin positions are relative to the layer this works well. As the arm is a parent of the child, setting its position relative to the body layer is exactly what you want.</p>

<p>If the arm is not a child of the body it gets much more complicated: you have to use the <code>toComp</code> method of the body layer to get the relative position of the pin to the composition space, and then you have to apply expressions to link the other transformation properties of the body layer, but it can be done.</p>
","15580"
"FFMpeg: ""Invalid audio stream. Exactly one MP3 audio stream is required""","1654","","<p>I am trying to adapt this FFmpeg script that encodes all video files in a directory, to instead convert mp3 files present in that directory with similar preferences.</p>

<h2>The original script:</h2>

<p><em>This works for .MOV -> .MOV encoding.</em></p>

<pre><code>cd /Convert; for i in *.MOV; do ffmpeg -i ""$i"" -c:v libx265 -preset veryslow -crf 23 -af ""volume=25dB, highpass=f=180, lowpass=f=15000, equalizer=f=50:width_type=h:width=100:g=-15"" -c:a aac -strict experimental -b:a 192k ""${i%.MOV}-ENCODED.MOV""; done
</code></pre>

<h2>Adapted for mp3 re-encoding:</h2>

<pre><code>cd /Convert; for i in *.mp3; do ffmpeg -i ""$i"" -af ""volume=25dB, highpass=f=180, lowpass=f=15000, equalizer=f=50:width_type=h:width=100:g=-15"" -c:a aac -strict experimental -b:a 192k ""${i%.mp3}-ENCODED.mp3""; done
</code></pre>

<p><em>Throws errors:</em></p>

<pre><code>Invalid audio stream. Exactly one MP3 audio stream is required.
Could not write header for output file #0 (incorrect codec parameters ?): Invalid argument.
</code></pre>

<p>What is wrong with the script?</p>
","<p>You're trying to store an AAC stream in a MP3 container, would be my guess.</p>

<p>Either store the result as <code>""${i%.mp3}-ENCODED.aac""</code> or switch <code>-c:a aac</code> to <code>-c:a libmp3lame</code></p>
","17930"
"How to double video frames without any duplicate frames","1642","","<p>My video details : DV-PAL 720x576i , 25FPS , duration=20s , frames=500</p>

<p>I need to increase (double) the video frames without any duplicate frame.</p>

<p>I used the following commands to increase (double) frame rate of the video.</p>

<pre><code>ffmpeg -i in.avi -r 50 out.avi
ffmpeg -i in.avi -r 50 -filter:v setpts=2*PTS out.avi
</code></pre>

<p>But not works properly.</p>

<p>Please tell me How to double video frames without any duplicate frames?</p>

<p><strong>Note</strong> : tool (ffmpeg or mencoder or etc.) and the duration and speed are not matter for me, Because I want to extract frames.<br>
(Sorry for my English)</p>
","<p>Use</p>

<pre><code>ffmpeg -i in.avi -r 50 -filter:v ""setpts=0.5*PTS"" out.avi
</code></pre>

<p>or</p>

<pre><code>ffmpeg -i in.avi -r 50 -filter:v ""setpts=N/50/TB"" out.avi
</code></pre>

<hr>

<p>If you want to <em>interpolate</em> new frames, use</p>

<pre><code>ffmpeg -i in.avi -r 50 -filter:v ""yadif,framerate=50"" out.avi
</code></pre>

<p><code>yadif</code> is a deinterlacing filter, which is needed since the framerate filter does not operate on interlaced media.</p>
","18606"
"What's the difference between a DVCAM tape that is mini and a MiniDV tape?","1640","","<p>What exactly is the difference between a DVCAM tape that is mini and a MiniDV tape? They look pretty much the same in terms of size. It seems like Sony is making DVCAM tapes that are the same size as MiniDV tapes now, and its getting hard to tell what, if anything, is different. Im cataloging videos that have either been captured on tapes that say DVCAM, MiniDV, and some tapes that actually have both DVCAM and MiniDv printed on them. Im basically trying to figure out what I should put as the ""media format."" Any ideas?</p>
","<p>Originally there was miniDV which is a standard consumer tape format (using ME tape) holding DV (at the time DV wasn't really a file format in it's own right).</p>

<p>Sony released a professional version of this called DVCAM with locked audio and which records at a shallower angle, running tape faster at the expense of running time, to reduce dropout.  </p>

<p>You could use either DVCAM tapes (now known as DVCAM-L) which as well as holding more tape for longer runtimes used higher quality MP tape, or you could record on miniDV tapes in either DV mode (to remain compatible with miniDV cameras) or in DVCAM mode to take advantage of the better format.  (The larger tape format uses the same width tape so the same head is used for either.) More recently DVCAM mini tapes have been available which use the better quality tape in the smaller package.</p>

<p>So DVCAM encompasses a file format, a recording format, the larger tape format and a marque of professional quality tape in the mini size tapes.  DVCAM mini and miniDV tapes are interchangable but they are branded towards their intended use and there is a general difference in quality/price between the two.</p>

<p>In your case regardless of the brands written on the actual tape you should probably use miniDV if it holds DV and DVCAM or DVCAM mini if it holds DVCAM.  If there is nothing recorded on the tape then it is suitable for either.</p>
","2462"
"For time-lapse videos, what software should one use?","1620","","<p>I have the following starting position:</p>

<p>I have a GoPro Hero 2 and have done some timelapse series. I took them and stitched them together with the software that go pro provides. This all worked well.</p>

<p>The Problem:</p>

<p>As soon as i want to watch these videos somewhere else, I can't. The software uses some codec apparently belonging to GoPro and obviously most common players won't play anything.
I already tried to convert the video to a wider accepted format(codec) but I wouldn't get anything I could watch. It either only used the first view pictures and then went black or 
the video was entirely black.</p>

<p>The Question:</p>

<p>Now my question is should I use an other program or is there any easy way to convert my current work into a format(codec) that is wider spread. What software would you suggest to use in such a case. This is only a hobby project so I'm not prepared to spend unreasonable amounts on fully fledged software suites.</p>
","<p>So I finally got time and dug around. I unearthed multiple options to do a timelapse video, but the one I found best is actually the following:</p>

<p>The answer is so simple, I just didn't know a time-lapse video could be created with <a href=""http://www.virtualdub.org/"" rel=""nofollow"">VirtualDub</a>.</p>

<p>Here is how you do it and you can install and use your very own and wanted codec: <a href=""http://timelapseblog.com/2009/08/04/using-virtualdub-for-time-lapse/"" rel=""nofollow"">manual</a></p>

<p>You can then work on the time-lapse movie with a video editor of your choice.</p>
","4037"
"Premiere Pro PC timeline only plays for a few frames and then no Audio after installing Miraizon DNxHD / ProRes Codec","1620","","<p>Since installing Miraizon DNxHD / ProRes Codec I am not able to play my timeline in real-time. It gives me a few frames and then the video freezes and audio keeps playing. I changed from Performance mode to Memory mode and now it will play through, however if I stop and start a couple times. It reverts to the former behavior. Specs of my system and project below.</p>

<p>I have Premiere Pro CC 2014 on a Windows 8 PC i7-4930 16GB RAM. Drive structure: Win/Programs on 256GB SSD, Project/Media on 1TB SSD, Preview/DB on 5TB RAID 0 7200 Seagates.</p>

<p>I'm working on a shortform project, basically 5 sequences comprising 18mins of a 22min show. Two streams of prores from from a 5D,6D 2 cam shoot, and I'm putting slides, stockfootage, and titles in.</p>

<p>I was able to play my timelines in real-time just fine (preview format DNx 115 1080p 23.976) until installing Miraizon DNxHD / ProRes codec. I got it because the editor putting the final show/QC together is on FCP7.. anyhow after installation I notice my preview files were gone -not surprising-. If I delete my preview files I get a couple clean playbacks (after they regenerate) and then the original behavior, same if I restart PPro.</p>

<p>I tried running the QT installer and ""repairing"" that after someone suggested that the QT SDK might be the issue. No difference. Makes more sense to re-install DNxHD codec from avid probably- I'll do that next. </p>

<p>Has anyone had a similar issue or Can you suggest what might be going on?</p>
","<p>According to Miraizon support this is a peculiarity in Premiere Pro handling of ProRes that causes it to interact ""very inefficiently"" with the the codec. There is an update forthcoming but until then they suggested this as a work-around: 
Move the AppleProResDecoder.qtx from the /Quicktime/QTSystem/ folder to a temp folder.</p>

<p>I still get some inconsistencies with various files under these circumstances such as ""Media Pending"" messages that aren't resolved and spotty playback.
This is currently working for me: 
For editing:
Leave the AppleProResDecoder.qtx in /Quicktime/QTSystem/ and move the MProResCodec.qtx from the /Quicktime/QTComponents/ folder to a temp folder. Restart PPro.<br>
When I'm ready to export my ProRes:
Then when I'm ready to export to ProRes, I move the MProResCodec.qtx back to the /Quicktime/QTComponents/ folder and move the AppleProResDecoder.qtx into the temp folder.
Restart PPro.</p>
","12259"
"Smartphone vs camcorder","1619","","<p>Okay, so I'm a young A/V tech, and I'm looking into picking up some equipment for video recording. The guy who is mentoring me said that for the kind of work I do, which is primarily speaking events indoors, with varied lighting, I want a camera with at least "" sensor.</p>

<p>The Canon XA20 Professional Camcorder seemed to be the best choice for the money, but then I got thinking, phones have come a long way, so when I looked it up, the Samsung Galaxy S6 has a "" sensor and is capable of shooting 2160p@30fps or 1080p@60fps video, which compared to the camcorders 1080 @ 59.94p seems like it's pretty good, minus the fact that you can't get the same zoom on the phone.</p>

<p>So for the same price of the camcorder, I could pick up two S6s plus accessories and ask for roughly the same price or less that the camcorder with no accessories.</p>

<p>Am I missing anything here? Would love to hear what I'm missing being an amateur and all. If anyone would have a better suggestion on how to maximise our invest my money, I'd love to hear them. Thanks in advance.</p>

<p><strong>Edit 1:
Okay, so to add some clarity, I was just asking about video. My experience is almost completely in audio, and I always use separate equipment for that.
Also, when I'm talking about speaking events, I'm currently only dealing with small groups under 75 people, so I can get the cameras pretty close to the speakers without intruding on the group's attention. My only major issue to date is lack of lighting.</strong></p>
","<p>There are a few things you are missing, and a few more you just aren't fully appreciating how much of a difference the things you mentioned make.</p>

<p>First, the S6 sensor is not over 1/2 inch, it is 1/2.6 inch, which is substantially smaller than a 1/2 inch sensor, particularly since sensors are measured diagonally.  </p>

<p>Second, lenses matter a lot more than the sensor.  The inability to zoom optically is a deal breaker for what you are trying to do with covering speaking events.  Typically, video is a secondary concern to a live audience and camera positions will be far off to avoid impacting visibility.  Without strong optics, you will not be able to get the footage you need.</p>

<p>Third, most (all?) cellphones do not use a global shutter, while most camcorders do.  Global shutters ensure that an entire frame is captured at the same time instead of scanning line by line while the frame is exposing.  The lack of a global shutter results in the ""jello"" effect that you can see when you pan a camera quickly.  Global shutter is a bit less of a concern for fixed camera position shooting at a relatively static target, but it is still work mentioning.</p>

<p>Forth, reliability.  Smartphones are more likely to crash than a stand alone video camera, they are subject to recording length limits that may interfere with your recordings.  They will have lower battery life while shooting which may also cause issues and the S6 in particular does not have a replaceable battery at all, so when you are out of juice, you are done unless you have an external battery pack that can keep up with sufficient juice.  Even then, it's a disadvantage to the much more compact battery placement directly on a camcorder.</p>

<p>Fifth, stability.  While you can get tripod mounts for a smartphone, camcorders are built from the ground up to support tripod mounting in a secure way.  You can work around that on the smart phone with complicated cases and mounting systems, but those also drive up the cost.  When shooting off a tripod, camcorders also still have a stability advantage.  You can hold them up to your eye and form a 3 point support that is easily held steady.  Accomplishing the same with a phone would require expensive rigging running up in to hundreds or possibly over $1000 unless you want to try to do custom fabrication yourself.</p>

<p>There are probably a few other things I'm missing too as I've just been writing these from what has come to mind as I go.  Smartphones are up to the point where they can make a pretty decent (though less durable and more expensive) alternative to a go pro, but they are really not an alternative for a camcorder by any stretch of the imagination.</p>
","15941"
"Software to combine two videos of the same scene?","1613","","<p>I think this software idea seems simple enough to have been created, but it might be so specific that no one has done it. I'm just wondering if it already exists.</p>

<p>I'm looking for some program which takes two videos which have identical backgrounds (or nearly identical, there are likely slight intensity differences in pixels) and allows you to combine the dynamical elements of each into a single video. For example, If I have a video of a room where I'm standing on the left side of the room, then another video from the same view where I'm standing on the right, and then the program would combine the two so I'm standing on both left and right. I guess things like shadows and whatnot would cause some problems here, but the general idea shouldn't be too ridiculous. Does something like this exist?</p>

<p>Thank you much!</p>
","<p>I thought I might expand a bit on the previous answers here. They're both correct, just not very informative.</p>

<p>What you are after is a technique often called the cloning or twin effect. As you have already stated, it consists of filming two different clips against the same background and then layering them over eachother.</p>

<p>This can be achieved in almost any conventional video editing program (even Windows Movie Maker and iMovie) - so which one you choose depends only on your own preferences and other needs.</p>

<p>However, <a href=""http://www.youtube.com/watch?v=xxX5eVFye4I"" rel=""nofollow"">here is a video tutorial</a> of how to do it in Premiere Elements. This should help show you the basic technique. If you need more specific help with whatever other piece of software you want to use, simply googling it will give you some great links. ;) Also, if anyone wants to specific video tutorials on how to do this in other video editing programs - feel free to edit (or comment on) this post.</p>

<p>I hope that was of help to you - and if you need more help I'll gladly assist.</p>
","5602"
"Matching a camera in a 3D scene based on a photo","1608","","<p>I have a Sketchup scene with a 3D model and a geolocation, but I also have a photo for the scene. </p>

<p>How cam I match/create a virtual camera in the 3D application to match the viewpoint of the physical camera's viewpoint ? </p>

<p>Would be cool if the solution is also in Sketchup, but can be done using other 3D packages as well.</p>

<p>Thanks!</p>
","<p>Production Software Used:</p>

<p><a href=""http://www.thepixelfarm.co.uk/product.php?productId=PFTrack"" rel=""nofollow"">PfTrack</a>, <a href=""http://www.thepixelfarm.co.uk/product.php?productId=PFMatchit"" rel=""nofollow"">PfMatchit</a>, <a href=""http://www.vicon.com/boujou/"" rel=""nofollow"">Boujou</a>, <a href=""http://www.ssontech.com/synovu.htm"" rel=""nofollow"">Syntheyes</a>, <a href=""http://www.thefoundry.co.uk/products/nuke/"" rel=""nofollow"">Nuke Camera Tracker</a>...</p>

<p>Student free use:
<a href=""http://usa.autodesk.com/maya/"" rel=""nofollow"">Autodesk Matchmover</a></p>

<p>All of the above software is used to 3D Match Move Cameras to video sequences. Most of them are very expensive. PfTrack is my personal favorite.</p>

<p>Although all of the above mentioned software is used to solve cameras for video sequences they work just as well with stills. PfTrack has tools to orient a scene based on hints that you give it about scale, x-axis, y-axis,z-axis, FOV and so on. The solve is easier with more reference images that have been taken from different angles looking at the same scene.</p>

<p>As was mentioned above looking for key signs in the photo that give hints for y-axis and z-axis will help you align your scene. If you Place grids down you can try to ballpark the perspective of the shot and give yourself a good horizon line. You can also place proxy geometry of some of the objects in your scene to guess depth and so on. Many of these techniques are what is used to match CG to video.</p>

<p>In the end the more information you have about the original physical scene is what will make matching CG easier. such as actual dimensions of the scene and as many measurements as possible. If you have access to any of the above software you can undistort your stills and have assistance on producing good solutions to your scenes 3D camera.</p>
","6841"
"FFmpeg/Avconv unable to copy subtitles; Unsupported codec?","1607","","<p>I've got an mp4 file with these streams (<code>avconv -i file.mp4</code> or <code>ffmpeg -i file.mp4</code>):</p>

<pre><code>Stream #0.0(eng): Audio: aac, 48000 Hz, stereo, fltp, 125 kb/s (default)
Stream #0.1(eng): Audio: ac3, 48000 Hz, 5.1, fltp, 384 kb/s
Stream #0.2(eng): Video: h264 (Main), yuv420p, 1280x568, 4027 kb/s, PAR 1:1 DAR 160:71, 23.98 fps, 2997 tbn, 50 tbc (default)
Stream #0.3(eng): Subtitle: c608 / 0x38303663, 0 kb/s
Stream #0.4(und): Subtitle: text / 0x74786574
Stream #0.5: Video: mjpeg, yuvj420p, 667x1000 [PAR 72:72 DAR 667:1000], 90k tbn
</code></pre>

<p>I am trying to strip out the first audio stream from the file like so:</p>

<pre><code>avconv -i file.mp4 -map 0 -map -0:0 -codec copy file-out.mp4

# OR #

ffmpeg -i file.mp4 -map 0 -map -0:0 -codec copy file-out.mp4
</code></pre>

<p>The problem is, I get this error:</p>

<pre><code>Could not write header for output file #0 (incorrect codec parameters ?): Operation not permitted
</code></pre>

<p>So, I checked if something's wrong with the mp4 file itself:</p>

<pre><code>avprobe file.mp4

# OR #

ffprobe file.mp4
</code></pre>

<p>And the output says:</p>

<pre><code>Unsupported codec with id 0 for input stream 3
Unsupported codec with id 94213 for input stream 4
</code></pre>

<p>I have no idea what that means. (Apparently something's wrong with the subtitles, but I got the video from iTunes.)</p>

<ol>
<li><p>What's wrong?</p></li>
<li><p>Can I simply force avconv/ffmpeg to do what I asked of it (i.e. strip the audio stream &amp; copy the rest as it is)? If so, how?</p></li>
<li><p>Is there a better alternative for remuxing, that's command-line &amp; actively developed?</p></li>
</ol>

<hr>

<p>Apparently I missed the other errors/warnings in the console output before the final error message, like @aergistal suggested:</p>

<pre><code>[mp4 @ 0x2645420] Codec for stream 0 does not use global headers but container format requires global headers
[mp4 @ 0x2645420] Codec for stream 1 does not use global headers but container format requires global headers
[mp4 @ 0x2645420] Codec for stream 2 does not use global headers but container format requires global headers
[mp4 @ 0x2645420] Codec for stream 3 does not use global headers but container format requires global headers
[mp4 @ 0x2645420] Codec for stream 4 does not use global headers but container format requires global headers
[mp4 @ 0x2645420] track 2: could not find tag, codec not currently supported in container
</code></pre>
","<p>The fourth stream apparently is a <a href=""https://en.wikipedia.org/wiki/EIA-608"" rel=""nofollow"">CEA-608</a> subtitle stream, which during the days of analog television signal transmission was caption data <em>embedded within</em> the video data. Apparently FFmpeg can extract it but can't <a href=""https://trac.ffmpeg.org/ticket/1778"" rel=""nofollow"">mux</a> it to a new container.</p>

<p>As for stripping only the audio, <a href=""https://gpac.wp.mines-telecom.fr/mp4box/"" rel=""nofollow"">MP4Box</a> may be of help.</p>
","17009"
"Troubleshooting XDCAM import into Premirere Pro CC 2014","1605","","<p>I am in the middle of editing a TV spot we create bi-weekly and therefore on a tight timeline. I am using Premiere Pro CC (2014) just upadted last week on Win8.1, 4790k, 16GB, OS SSD 256GB, Project Drive 1TB SSD, Export and Preview Drive 3TB Seagate 7200. They shot this particular spot in a studio with different cameras.  So I recieved files in: XDCAM HD 1080 422 i60 50mb CBR 1920x1080 - 1888x1062  </p>

<p>My understanding is that XDCAM is supported natively in PPro, however, when I try to import into Premiere or add to Media Encoder I recieve error: ""Codec missing or unavailable"".</p>

<p>I asked a friend to re-encode it as ProRes rather than spend time figuring out -what I assumed was a codec issue. But the ProRes 422 LT that he exported
I get error: ""The importer reported a generic error"". I'll say that I've been using prores on this same system a bunch of times, however it was a different possibly less savey person who did this transcode and so it is possibly still suspect.</p>

<p>These files were live edited on some box so the file is over an hour long.. could it be a RAM issue? I've jogged my page file up to a static 120GB..</p>

<p>I am able to play the prores files on a second laptop that has no adobe suite installed.. but still cannot export them from quicktime as something else. I get an error.</p>
","<p><strong>Since Adobe CS5 XDCAM files are supported natively</strong>, make sure the folder structure is the same like on your SXS Card, open up the <a href=""https://pro.sony.com/bbsc/ssr/micro-xdcam/resource.downloads.bbsccms-assets-micro-xdcam-downloads-XDZP1TC.shtml"" rel=""nofollow""><strong>Sony Clip Browser</strong></a> and try to load up the files - this should work immediately. If you can't see the files, there must be an encoding error or the files are corrupt.</p>
","15604"
"3D composition shows up as 2D in main project view?","1600","","<pre><code>Camera
[comp 3D]
  ellipse mask in z-axis
  layer1 2D
  layer2 2D
  layerN 2D
</code></pre>

<p>The layers are aligned in the Z-axis along the ellipse, forming a 3D text by the layers.
When I'm inside the composition, I can actually see a aligned layers in the 3D space.</p>

<p>But when I go to my main project view just seing the ""comp 3D"" layer, everything is plain 2D! Also moving the camera I can just see a plain 2D image, even though inside the comp it is 3D. Why?</p>

<p>Update:
<img src=""https://i.stack.imgur.com/VTGGe.png"" alt=""enter image description here""></p>

<p>There are 3 lights being aligned differently in the z-axis (see TOP view).</p>

<p>When I move a camera around inside the ""light comp"", everything is fine and I can see also my lights moving.</p>

<p>When I move the camera in my main layout, the lights are static! No movement is reflected to the light comp layer! I can just see the camera mask moving, not the comp content.</p>
","<p>The issue could be Optical Flares, it doesn't fully integrate into AE's 3D space but has its own coordinate system. The usage of AE lights is somewhat of an workaround for that.
Have a look at this plugin/script, that could help:
<a href=""http://www.videocopilot.net/tutorials/3d_pre-compose_script/"" rel=""nofollow"">http://www.videocopilot.net/tutorials/3d_pre-compose_script/</a></p>

<p>It's exactly made for what you want to do.</p>
","13411"
"Differences Tamron f/2.8 28-75 / 24-70","1591","","<p>I'd like to buy one of those two lenses for filming with a EOS 600D:</p>

<ul>
<li>Tamron SP AF 28-75mm F/2.8 XR Di LD Aspherical [IF] MACRO</li>
<li>Tamron SP 24-70mm F/2.8 Di VC USD</li>
</ul>

<p>I am aware of the following differencies of the two lenses.</p>

<p>Advantages 28-75mm:</p>

<ul>
<li>Some websites claim this lense to be parfocal, which might be interesting for filming.</li>
<li>It's lighter (510g vs 825g)</li>
<li>Slightly lower minimum object distance (33 vs 38cm)</li>
<li>It costs less than half of the 24-70</li>
</ul>

<p>Advantages 24-70mm:</p>

<ul>
<li>Vibration compensation (which I always switch off during filming)</li>
<li>USD motor (which doesn't seem to be important for filming)</li>
</ul>

<p>From that the 28-75 clearly seems to be the better choice. </p>

<p>Is there anything I am missing? Are there any other points to consider?</p>
","<p>Yes, there is a lot you are missing.  </p>

<p>The 24-70 has substantially better transmittance (more light makes it through the lens), slightly better sharpness, significantly less chromatic aberration, 2 more diaphragm blades (better, more round bokeh), full time manual focus (can auto-focus at the start of a shot but still adjust after starting shooting).  </p>

<p>Also, image stabilization can be very helpful when shooting video.  If you are on a tripod you shouldn't, but if you are shooting free-hand, it is a good thing unless you need quick, lag free panning (which would be a problem due to your rolling shutter on a 600d anyway).</p>

<p>While I don't recommend putting too much stock in the overall score that DXoMark produces, there individual discreet stat comparisons are generally reasonably good.  You can find the comparison for these two lenses on the 700D listed <a href=""http://www.dxomark.com/Lenses/Compare/Side-by-side/Tamron-SP-24-70mm-F28-Di-VC-USD-Canon-on-Canon-EOS-700D-versus-SP-AF28-75mm-f-2.8-XR-Di-LD-Aspherical-IF-Canon-on-Canon-EOS-700D___884_870_297_870"" rel=""nofollow"">here</a>.  (Sadly, they did not have data for the 600D specifically.)</p>
","12061"
"iOS - App To Change Aspect Ratio of a Video","1584","","<p>I'm a new iOS user, and previously on my Android device, from time to time I needed to change the aspect ratio of a video to 1:1 (to fit with Instagram).</p>

<p>So at that time, I was using this <a href=""https://play.google.com/store/apps/details?id=com.silentlexx.ffmpeggui&amp;hl=fr"" rel=""nofollow noreferrer"">app</a> that let's you directly type ffmpeg commands as if you were using a computer, so I it was simple to do it by using :</p>

<pre><code>-vf ""scale=720:720,setsar=1:1""
</code></pre>

<p>But unfortunately I didn't find any alternative on iOS.</p>

<p>So, does anyone know an app that will let me change the aspect ratio of a video?</p>
","<p><strong>Videon</strong></p>

<p>It's a great app for the iPhone. </p>

<p>Not only does it have some great camera features; like stabilization, and manual exposure/WB, but you can also use it's post feature on any clip in your Photos folder. </p>

<p>You can crop, change aspect ratio, framerate, adjust rotation, pretty much anything... along with a ton of filters; color correcting tools and so on. </p>

<p>Very easy to use. And real time playback of your file as you correct it. </p>

<p>When finished, you can output your file by exporting it to a variety of bitrates and sizes. </p>

<p>A must have!</p>
","21419"
"Auto split comp into separate render items?","1583","","<p>Is there a script/plugin that will either (a) take layers from a comp and create separate render queue items for each layer; or (b) use markers in a comp to generate separate renders?</p>

<p>I thought there used to be one ... ""RenderLayers"" or something ...</p>

<p>Essentially I need to split up a long comp into separate output files.</p>

<p>thanks!</p>
","<p>There is a script called ""rd_RenderLayers"" from redefinery, maybe thats what you mean. It does exactly that, adding every layer to your render queue.</p>

<p>Unfortunately his website is on hiatus but the downloads still work, you can download the latest version <a href=""http://www.redefinery.com/xae/scripts/rd_RenderLayers_v3.1.zip"" rel=""nofollow"">here</a>.</p>
","12422"
"Convert variable framerate ~60 FPS to constant 60 FPS losslessly","1574","","<p>I have gameplay footage recorded with ShadowPlay (GeForce Experience) at 60 FPS, which as I understand results in variable frame rate, because the game drops frames sometimes. However, trying to use this video in Adobe Premiere results in the game audio severely falling behind the visuals, because Premiere treats it as if every frame was the same length.</p>

<p>I have tried HandBrake with these settings:</p>

<p>Quality: constant, RF 0 (lossless)<br>
FPS: 60, constant<br>
Detelecine: Off<br>
Decomb: Off<br>
Denoise: Off<br>
Deblock: Off</p>

<p>But the output video is not only much smaller than the source video, but it's massively worse in terms of visual quality. Why isn't it lossless when I've set it to what it claims is lossless?</p>

<p>Input video info: <a href=""http://pastebin.com/7Tss4brj"" rel=""nofollow noreferrer"">http://pastebin.com/7Tss4brj</a><br>
Output video info: <a href=""http://pastebin.com/DKFEzDKk"" rel=""nofollow noreferrer"">http://pastebin.com/DKFEzDKk</a>
(from Mediainfo)</p>

<p>How can I convert a variable framerate video to a constant framerate video losslessly?</p>
","<p>To start with: your version of Handbrake is old. Version 1.0.3 came out very recently, it's got some improvements over that older version.</p>

<p>Second, CQ0 is not <em>actually</em> ""lossless."" It's aiming for ""lossless"" quality, but it's going to bump up against the data rate limitations of the <a href=""https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Levels"" rel=""nofollow noreferrer"">H.264 Level</a>, so while it might need, say, 100MbPS at a given point to achieve ""lossless"" compression, Level 4.0 might be capping it at 20MbPS or something like that. Note in the Encoding Settings readout on your export file it lists ""vbv_maxrate=20000,"" that's where your bitrate is capped. So to get as close to lossless as possible you'd need something like Level 5.2.</p>

<p>So ratcheting up your H.264 level will improve quality, but Premiere may end up chugging very hard on it. H.264 is really bad for editing. It's very computationally complex, largely because it doesn't store each frame separately (called Intraframe compression) but it takes batches of frames (called a Group of Pictures, or GOP) and compresses them together, in such a way that any one frame within a GOP will depend on the surrounding frames in order to reproduce a complete image. This is called Interframe compression, and the number of frames in a GOP depends on the parameters set at encode time, and may even be variable (not typical), but you can see what those parameters in MediaInfo by noting the line that says ""Format settings, GOP"" (""N=30"" means 30 frames per GOP), or looking at the encoding settings where it notes ""keyint"" (Keyframe Interval). So needing to decode all 30 frames to show you any one frame is <em>very</em> CPU and RAM intensive.</p>

<p>If you've got the storage I'd recommend considering using DNxHD, which is a ""mezzanine"" (editing friendly) codec that preserves quality, considered ""virtually lossless,"" but has low CPU/RAM impact. The catch is very high bitrates. So we're talking ~130-200GB/hr at 1080p59.94. The other catch is that <a href=""https://en.wikipedia.org/wiki/List_of_Avid_DNxHD_resolutions"" rel=""nofollow noreferrer"">DNxHD has very limited supported resolutions and frame rates</a>. There's also CineForm which is just as high a quality, but far more flexible, and both CineForm and DNxHD are natively supported by Premiere (CineForm in 2015 and later) but last I checked CineForm support in other tools, like ffmpeg, is lacking.</p>

<p>So are more editing-friendly solution would be to use ffmpeg to make DNxHD with a command line the following:</p>

<pre><code>ffmpeg -i (input file).mp4 -c:v dnxhd -b:v 290M -c:a pcm_s16le -r 60 (output).mxf
</code></pre>

<p>That would make DNxHD with uncompressed audio in an MXF Op1a file. You could also whack .mov on the end there and make a Quicktime file instead just as easily. Note that -r sets the output frame rate. You could try using H.264 in ffmpeg by changing the video codec (-c:v), but you'll run into the same limitations as you would working with Handbrake, so I don't know if it's really worth it to go into that here.</p>
","20830"
"green screen to transparent with ffmpeg","1572","","<p>ffmpeg does a very good job of getting rid of a green screen, with a command like this:</p>

<pre><code>ffmpeg -f lavfi -i color=c=black:s=606x1080 
       -i input.mp4  
       -filter_complex 
            ""[1:v]chromakey=0x70de77:0.1:0.2[ckout];
            [0:v][ckout]overlay[out]"" 
       -map ""[out]""  output.mp4
</code></pre>

<p>but it replaces the green screen with solid black, whereas I would like it replaced with a transparent background, alpha = 0.</p>

<p>I tried setting the color to <code>#00000000</code> (8 zeroes indicating a zero alpha), but that still resulted in a black background.</p>

<p>Strangely, when I set the color to say red, it gives a red tint to many unfiltered areas.</p>

<p>So I would like a transparent background, but I'm also curious what the color option is doing.</p>
","<p>Output by MP4 defaults to the H.264 codec for the video. H.264 does not support alpha.</p>

<p>You can output to <code>.mov</code> with a video codec <code>-c:v</code> of <code>png</code> or <code>qtrle</code>. There are others but they have restrictions on resolution and framerate.</p>

<p>You can't use the overlay filter as it doesn't output to a pixel format with alpha. So, just the chromakey filter, and no color source.</p>
","21399"
"ffmpeg- What's the difference between ""NN000/1000 fps"" and ""NN000/1001 fps""?","1565","","<p>I'm trying to select the correct capture options for <code>ffmpeg</code> and I'm confused by this kind of listing:</p>

<pre><code>[decklink @ 00000000010564c0]   17      3840x2160 at 24000/1001 fps
[decklink @ 00000000010564c0]   18      3840x2160 at 24000/1000 fps
[decklink @ 00000000010564c0]   19      3840x2160 at 25000/1000 fps
[decklink @ 00000000010564c0]   20      3840x2160 at 30000/1001 fps
[decklink @ 00000000010564c0]   21      3840x2160 at 30000/1000 fps
</code></pre>

<p>Based on the fact that 24 and 30 have two entries and 25 only has one, I assume that the two 24 options are true 24fps and 23.9fps, and the two 30 options are true 30fps and 29.97fps.</p>

<p>But which is which?  And why is this kind of notation used?</p>
","<p>Yes, <code>24000/1000</code> is 24 fps and <code>24000/1001</code> is 23.976fps. Some refer to the X/1001 frame rates as ""drop down"" (as in ""dropped down from the integer"") but this is easy to confuse with ""pull down"" which often refers to the cadence of frames when fitting 24 or 25 fps material into a 30 fps program.</p>

<p>You can also think of these notations as <code>24,000 divided by 1,000 equals 24</code> and <code>24,000 divided by 1,001 equals 23.976023976 repeating</code>. This notation is also the exact same thing as saying <code>24 divided by 1 equals 24</code> and <code>24 divided by 1.001 equals 23.976023976 repeating</code>. It is, of course, quite ridiculous to articulate the repeating decimal values.</p>

<p>The <code>1001</code> options and their resulting notation are a legacy from analog color television systems. As Mulvya points out ""it's because that's the smallest set of numbers that represent the value as integers"". These standards are still around today mainly for backwards compatibility (with black and white television sets) and 'cause that's just the way TV grew up. Prior to color television, the <a href=""https://en.wikipedia.org/wiki/NTSC"" rel=""nofollow noreferrer"">NTSC</a> frame rate was actually exactly thirty frames per second.</p>

<p>As for the <code>30000/1000</code> and <code>30000/1001</code> notation it comes from the NTSC video standard. ""30""fps NTSC is actually displaying 30/1.001 frames per second (sometimes referred to as ""29.97"" or ""30DF"" for short, but to be precise 29.97002997... repeating frames per second). Due to this slight difference (.01%), NTSC uses either Drop-Frame (DF) or Non Drop-Frame (NDF) time-code counting. Drop-Frame ""drops"" frame counts in a pattern such that every ten minutes the time-code duration will correspond to the actual playback duration. No image frames are dropped and this was mainly a consideration for broadcasters fitting their programs with commercial breaks. Non Drop-Frame on the other hand more accurately represents a count of the total number of frames in a program (for those who enjoy counting in base 24:60:60:30...)</p>

<p>As for <code>25000/1000</code> there's no 25/1.001fps standard because 25fps is a <a href=""https://en.wikipedia.org/wiki/PAL"" rel=""nofollow noreferrer"">PAL</a> standard and PAL video systems were engineered using 50Hz AC power which did not have the same issues as NTSC when color came about.</p>
","20527"
"how accurate frequency does a PC speaker produce","1564","","<p>I was wondering how accurate frequencies do PC speakers produce. I'm talking about the old small things in computer that usually beep.</p>

<p>I used the <code>beep</code> linux command to generate some frequencies. But I noticed something interesting. When I use the hardware PC speaker I stop hearing anything on 1700Hz. Actually I barely hear up to 16850Hz and I feel it more like a pain in the back part of my head.</p>

<p>Emulating PC speaker with the integrated sound card and listening through the laptop speaker I hear up to 20KHz and it is nothing like a pain.</p>

<p>So I am wondering if any of these two can be trusted for producing frequencies above 8Khz? I'm more inclined to trust the old PC speaker because I expect to hardly be able to hear such high frequencies. Not having experience with known correct equipment it is hard to judge though.</p>

<p>P.S. I don't think there is any spec I can read about frequency response.</p>
","<p>The <a href=""http://en.wikipedia.org/wiki/PC_speaker"" rel=""nofollow"">classic PC Speaker</a> is a very simple system that was designed to be inexpensive and use off-the-shelf parts of the early 1980s.</p>

<p>The core problem is that the PC speaker system consists of a cheap speaker, a timer, and digital pulses. Using this system, you can get quite a lot of interesting sounds out that weren't originally intended. But you are limited to square waves and the harmonic content that comes with them.</p>

<blockquote>
  <p>I don't think there is any spec I can read about frequency response.</p>
</blockquote>

<p>The best you can find is specs for the timer and then run experiments with the actual hardware. The early IBM PCs had discrete timers, but at some point that functionality got rolled into the VLSI chipsets that PC makers used to build motherboards.</p>

<p>If you have an actual audio chip built into the motherboard (fairly common, these days, much less common even in the late 1990s), then it is likely that the PC speaker functionality is emulated there. So the behavior will be different from the original hardware.</p>

<p>Your integrated sound card should produce the most accurate sound. But at the upper end of the sound range, you may be limited by the physical capabilities of your laptop speakers. If you drive the speaker faster than it can move, then you won't get the sound you expect.</p>
","7336"
"Tranferring an iMovie project from iPhone to MacBook Pro","1554","","<p>Is it possible to transfer an iMovie <em>project</em> from my iPhone to my MacBook Pro?</p>

<p>Right now, my project consists of clips that I shot directly using iMovie, but they don't appear in my Camera Roll on my iPhone. Is there a way to make them appear on my Camera Roll (or other folder) so that I can simply copy and paste the <em>individual</em> clips onto my computer?</p>

<p>I have tried the new update of iMovie 10.1, but it gives a strange error when I try to import. What I have done:</p>

<ol>
<li>Goto the projects in iMovie for iPhone, open a project, export to iTunes.</li>
<li>Download the ""projectname.iMovieMobile"" file from iTunes onto my computer.</li>
<li>Open iMovie version <strong>10.1</strong> (the new update). File > Import iMovie iOS Projects... > select the project.iMovieMobile.</li>
<li><p>When I try to import them, it gives an error after a few seconds:</p>

<blockquote>
  <p>Error in iOS Project Import
  Failed to import iOS project. The project's version is too old. Please update the iMovie app on your iOS device.</p>
</blockquote></li>
</ol>

<p>However, I have checked that I have the correct versions. I will try updating my iOS and get back to here.</p>

<hr>

<p><em>Additional Info</em>:</p>

<ul>
<li>Mac Book Pro (Late 2011)

<ul>
<li>I just installed the 'El Capitan' update, but it doesn't to appear to actually be working; will try reinstalling.</li>
</ul></li>
<li>iPhone 6S, iOS version 9.0.2 (latest)</li>
<li>iMovie (on MacBook Pro) v. 10.1</li>
<li>iMovie (on iPhone) v. 2.2</li>
</ul>
","<p>Turns out I finally found my own solution:</p>

<ol>
<li><strong>Make sure you have the latest versions of iMovie, <em>especially on iOS</em></strong>. This was the main problem causer for me, as iMovie claimed to be updated and would not show up in the updates in the App Store until I restarted several times.</li>
<li>From your iPhone: export the project you wish to transfer to iTunes. Connect your iPhone to your computer via a USB cable and download the <code>filename.iMovieMobile</code> file to your computer.</li>
<li>Open iMovie on your computer, and click <code>File &gt; Import iMovie iOS Projects...</code>.</li>
<li><p>You should have the project imported to iMovie on the computer! Now, you can even transfer to Final Cut Pro X (see <a href=""http://help.apple.com/imovie/mac/10.1/#/movcbf7e2a3f"" rel=""nofollow"">here</a> for info on how to do that).</p>

<blockquote>
  <p>Make sure you are running the most recent versions of iMovie!</p>
</blockquote></li>
</ol>
","16714"
"How many NVIDIA NVENC engines a particular GPU have?","1553","","<p>There is a <a href=""https://developer.nvidia.com/nvenc-application-note"" rel=""nofollow noreferrer"">NVIDIA VIDEO CODEC SDK APPLICATION NOTE - ENCODER</a> in which we can read the following:</p>

<blockquote>
  <p>While Kepler and first generation Maxwell GPUs had one NVENC engine, certain
  variants of the second generation Maxwell GPUs and Pascal Generation GPUs have two/three NVENC engines physically present on the silicon.</p>
</blockquote>

<p>So, does anyone know how to determine the number of NVENC engines in a GPU or already know the numbers? Because there is no information in NVIDIA datasheets and they are annoyingly silent in their own developer forums.</p>
","<p>NVIDIA have just published <a href=""https://developer.nvidia.com/video-encode-decode-gpu-support-matrix"" rel=""nofollow noreferrer"">Video Encode and Decode GPU Support Matrix</a> that contains number of NVENC chips for Pro-level cards.</p>
","20026"
"Scrolling the panel in After Effects","1551","","<p>I'm on windows using After Effects CS6. </p>

<p>I'm new to this software and am having a problem scrolling the window vertically or horizontally, <strong>Where is the scroll bar?</strong></p>

<p>Alternatively I'm trying to scroll using the mouse wheel, but this seems to be zooming in and out. I read that I can hold the Alt or Ctrl button while moving the wheel, but that doesn't make any difference.</p>

<p>I just want to scroll the panel up and down so I can see what I'm doing.</p>
","<p>After tearing my hair out I found the ""hand cursor"", now I can drag the panel around however I want it :)</p>

<p><img src=""https://i.stack.imgur.com/m24ve.png"" alt=""After Effects Hand Cursor""></p>
","9180"
"How to split video into multiple files based on chapter markers?","1546","","<p>Is there a way to automatically split a video into multiple files based on chapter markers?</p>

<p>I don't want to re-encode the file, just a video/audio passthru.</p>

<p>The source file is a MKV container, but I guess it doesn't really matter.</p>

<p>Apple OS X, Windows either OS would be fine.</p>
","<p>mkvmerge, a part of <a href=""http://www.videohelp.com/software/MKVtoolnix"" rel=""nofollow"">MKVtoolnix</a>, will split a MKV file by chapters.</p>

<p>Basic syntax is</p>

<pre><code>mkvmerge -o chapter.mkv --split chapters:all input.mkv
</code></pre>

<p>but see <a href=""https://mkvtoolnix.download/doc/mkvmerge.html#mkvmerge.description"" rel=""nofollow"">Section 2.5 #7</a> of the documentation for more details.</p>
","17066"
"How to encode a video for the iPhone with Handbrake","1539","","<p>I've been trying to encode a video trailer for mobile Safari playback (iPhone). Using Handbrake for Windows, I selected the source .mov file and chose the iPhone 4 preset. I then checked off the Web optimized option and started the process. </p>

<p>The resulting video did not play on my iPhone's Safari browser, however.</p>

<p><a href=""http://mobilevideo.jt.citeeze.com/static/trailer.mp4"" rel=""nofollow"">http://mobilevideo.jt.citeeze.com/static/trailer.mp4</a></p>

<p>For reference, here is the original file:</p>

<p><a href=""http://mobilevideo.jt.citeeze.com/static/53159_high.mov"" rel=""nofollow"">http://mobilevideo.jt.citeeze.com/static/53159_high.mov</a></p>

<p>Thanks!</p>
","<p>Based on the <a href=""http://blog.zencoder.com/2010/09/30/how-to-encode-video-for-mobile-use/"" rel=""nofollow noreferrer"">link</a> given in <a href=""https://stackoverflow.com/a/5871802"">this</a> answer on stackoverflow I will try to explain how to encode a video in an iPhone friendly format. </p>

<p>The steps below are carried out on a Mac, but the windows version of Handbrake should be similar.</p>

<ol>
<li>Open Handbrake.</li>
<li>Go to <code>File -&gt; Open Source</code> and browse to your video.</li>
<li>Choose <code>MP4 File</code> as container. </li>
<li>Input the settings shown below for the respective tabs:</li>
</ol>

<blockquote>
  <p><strong>Picture tab</strong>:</p>
  
  <p>Keep aspect ratio - check<br>
  Width - 480 (set to 960 if encoding for the iPhone 4)<br>
  Anamorphic - None<br>
  Cropping - Automatic  </p>
  
  <p><strong>Video tab</strong>:</p>
  
  <p>Codec - H.264<br>
  Video quality - RF = 21<br>
  Constant framerate - 30  </p>
  
  <p><strong>Audio tab</strong>:</p>
  
  <p>Codec - AAC<br>
  Bitrate - 128<br>
  Sample rate - 44.1</p>
  
  <p><strong>Advanced tab</strong>:</p>
  
  <p>Add the following text in the textbox labeled <code>Current x264 Advanced Options String:</code> if you're encoding for a pre iPhone 4 device:</p>
  
  <p><code>cabac=0:ref=2:me=umh:bframes=0:weightp=0:subq=6:trellis=0:8x8dct=0:level=3</code>  </p>
  
  <p>If it's an iPhone 4 or higher the text should be:</p>
  
  <p><code>level=4.1</code></p>
</blockquote>

<p>The Advanced tab should look like this (for the iPhone 3 encode):
<img src=""https://i.stack.imgur.com/hpuiK.png"" alt=""enter image description here""></p>

<p>Here's a MediaFire link to the converted video for iPhones before the 4:</p>

<p><a href=""http://www.mediafire.com/?tlyzcg1hyygq8zf"" rel=""nofollow noreferrer"">http://www.mediafire.com/?tlyzcg1hyygq8zf</a></p>
","6925"
"Split video into equal duration segments","1539","","<p>I am currently using ffmpeg to split videos into <code>10</code> seconds segments. I have taken two approaches but neither seems to yield the desired results. Approach number one is using basic cutting with the -t and -ss options to get the segments. This gives close to accurate segments but  it has some issues with the frames in the ending of some videos. The second option is using segment: great and fast but it gives segments of not equal duration. Any idea how to get segments of the same duration and no issues in the frames? In other words if I play them in a list it should play smoothly</p>

<p>General way</p>

<pre><code>ffmpeg  i input.mp4  -ss &lt;start&gt; -t &lt;duration&gt; -vcodec copy -acodec copy output%03d.mp4
</code></pre>

<p>Using segment</p>

<pre><code>ffmpeg -i input.mp4 -c copy -map 0 -segment_time 8 -f segment output%03d.mp4
</code></pre>
","<p>The issue with the frames are very likely the results of using h264 and not re-encoding the video. h264 usually doesn't have single frames (unless encoded with an intra profile) but groups of frames (GOPs), ffmpeg will cut at a keyframe position ie. at the end or beginning of a GOP. Or not in the case of your first approach, giving you issues with the affected GOPs.</p>

<p>The issue should be resolved by re-encoding (-vcodec libx264 -preset slow) or if that also yields errors transcode to a lossless codec like jpeg2000 (<code>-vcodec libopenjpeg</code>) cut your video and then encode into h264 again (or any other codec you may prefer).</p>
","12823"
"Streamable Container for HEVC (H.265)","1524","","<p><code>flv</code> is a container which can be used to stream videos while they are being encoded. <code>flv</code> supports <code>h.264</code> codec. </p>

<p>However, <code>flv</code> doesn't support <code>hevc</code> codec. Is there any container for <code>hevc</code> which is streamable?</p>
","<p>MPEG -TS. See <a href=""https://gpac.wp.mines-telecom.fr/2013/04/26/mpeg-hevc-support-in-gpac/"" rel=""nofollow"">here</a>:</p>

<blockquote>
  <p>All MPEG-2 TS operations from GPAC (client and MP42TS) are supported on
    HEVC. MP42TS can be used to generate TS files usable for DASH or for
    injection in modulation chains; it can also be used to send the TS over an
    UDP or RTP stream in unicast or multicast mode</p>
</blockquote>
","16643"
"Lossless cutting video (MPEG-4)","1517","","<p>Is it possible to lossless clip out the video file (MPEG-4/H.264) that result chunks will be <em>equivalent</em> to the same of raw footage? I want to reduce size of the source (raw) material by cutting off the unnecessary, but desire to have remaining clips in the same quality as initial video.</p>
","<p>Yes, it is generally possible, but with a few limitations.  h.264 uses what is known as a group of pictures.  A group of pictures groups multiple frames together in a way that allows for further compression, but the entire group of pictures has to be decoded together.  As such, it is only possible to cut a video stream in between groups of pictures. This is often around 15 frames, but length can vary based on encoding options that were used.</p>

<p>As long as you are ok breaking it on a group of pictures, you can cut up the video to your hearts content and still use the same data stream.  At the end of the day, most video formats consist of some metadata that describes how the video is laid out and how it syncs with the audio.  It then contains one or more streams of video and one or more streams of audio.  When played back, this metadata is read and then the streams are used for playback.</p>

<p>Splitting the file simply means extracting part of the stream, broken up on a group of pictures, pulling out the relevant audio and then piecing the data back together in a new container that has metadata that corresponds to it.  As long as you break it at the points it is designed to break, then it ends up being two normal, otherwise unaltered streams.  There are numerous tools that can do this for you, including tools like FFMPEG, though I unfortunately do not have the exact command to perform the split as my FFMPEG skills are decidedly weaker than many others on here (though one of them will likely soon be able to post with the command line you need.)</p>
","12229"
"Change output using a .jsx script for After Effects CS6","1516","","<p>I've made a script that will loop through a (.txt) interchanging elements of a certain composition.</p>

<p>Within the script, I add each changed composition to the render queue and output it.</p>

<p>How can I pragmatically output the format in h.264 (.mp4) instead of it defaulting to (.avi).</p>

<p>I've discovered that you can just change the default output module via edit -> templates -> output module, but still.. is there anyway of doing this via script?</p>

<p>Thanks</p>
","<p>So, the answer to your question is 'yes, but...'</p>

<p>It's perfectly feasible, but you have to create an output module with the settings that you want, and then save it to your computer.  You can create this by going to Edit -> Templates -> Output Module.  Click on the 'New' button in the pop-up window, and then Edit it for the settings you need (quicktime, h.264, etc).  Save it as a useful name (""H264HighQuality"" or something like that).  Save the output module, and you'll be able to access it from your scripts from there on out.</p>

<p>In order to use it in your script, just use:
<code>app.project.renderQueue.items.add(myComp).applyTemplate(""H264HighQuality"");</code></p>

<p>As long as you've <em>made</em> the template first, you can access it in your scripts as many times as you want.  However, After Effects does NOT allow you to programmatically create Output Modules or alter them on the fly, unfortunately.  But you only need to make the template once -- as long as you have that template on the computer you're running the script, it should work just fine.</p>
","12392"
"Duplicate layers X times and move 1 frame each time","1515","","<p>I do a lightpainting video on After Effects (CS6 here) where I duplicate my layert to create continuous lines based on a moving light. I have a lot of comps to do and many layers to duplicate, then I search a solution to avoid this boring task</p>

<p>If you knows a script or something else which <strong>duplicates a layer X times and moves it 1 frame everytime</strong> (like stairs), it would be very welcomed!</p>

<p>Here is a screenshot of the current (but working) solution, by hand:</p>

<p><img src=""https://i.stack.imgur.com/aEbOv.jpg"" alt=""""></p>
","<p>Check the <a href=""http://aescripts.com/array/"">Array</a> script on <a href=""http://aescripts.com"">aescripts.com</a> website. It duplicates your layer in desired numbers and has more control for offsetting the properties like position, scale, rotation as well as time. It is really easy to use and has many great built in functions for creating really good looking and creative motion graphics. Especially for your case, playing around with the script might function as a brainstorm tool to create new looks. Just watch the provided tutorial video to see the features.</p>

<p>By default, AEScripts website will set a price but you can ""Name your own price"" for this particular script. Just register with the website and type zero in the price field and add to cart.</p>

<p>My personal thought on the pricing, if you are using the script in a commercial project, it is always a good thing to pay some amount for it. As this is a great and creative tool, it deserves some donations, think it as a tip. It is not compulsory but a way of showing appreciation.</p>

<p>By the way, check the other scripts on the website, it is a gold mine. There are lots of great tools to speed your workflow.</p>
","5432"
"Windows 10 miniDV import using Movie Maker","1515","","<p>I'd like to import my miniDV tapes.  I'm using Windows 10 and the built-in Movie Maker software.   My goal is to get the highest fidelity import from the miniDV tapes.   I did a trail run and it looks like Movie Maker stores the content in AVI files.  </p>

<p>My questions:</p>

<ol>
<li>Is the import lossless/uncompressed?</li>
<li>Is it possible to do a higher quality import?  If so, what software should I use?</li>
<li>I know AVI is an older container format.  Is AVI a suitable format going forward?  If not, what should I use?  I'm trying to future proof as much as possible.</li>
</ol>
","<p>DV is a compressed format to begin with. Unless you apply further compression at the time of capture within WMM, the captured stream is identical to that on the tape. The bitrate of the capture should be roughly 3.5 MB/s.</p>

<p>AVI is an old format, and for broader compatibility, you can store the video in Quicktime .MOV, but this will have to be done after capture, using a tool like <a href=""http://ffmpeg.org/download.html"" rel=""nofollow"">FFmpeg</a>. This process is quick since it's just rewrapping the data.</p>

<p>Command will be</p>

<pre><code>ffmpeg -i file.avi -c copy file.mov
</code></pre>
","18863"
"How much film do I need (in feet/meters)?","1514","","<p>I'm shooting 3-perf 35mm @ 24 fps. How many feet of film do I need if I'm shooting one hour?</p>

<p>Is there a good site/app for calculating this?</p>

<p>Note: This is a repost of <a href=""https://photo.stackexchange.com/questions/638/how-much-film-do-i-need-in-feet-meters"">an off-topic/closed question</a> I asked on Photography - Stack Exchange.</p>
","<p>4,051 feet of film.</p>

<p>For reference:</p>

<ul>
<li>8mm = 80.0 frames per foot</li>
<li>Super 8 = 72.0 frames per foot</li>
<li>16mm = 40.0 frames per foot</li>
<li>35mm 2-perf = 32.0 frames per foot</li>
<li>35mm 3-perf = 21.3 frames per foot</li>
<li>35mm = 16.0 frames per foot</li>
<li>65mm = 12.8 frames per foot</li>
</ul>

<p>The math: </p>

<blockquote>
  <p>60 minutes = 3600 seconds, 3600 sec x 24 fps = 86,400 frames, 86,400/21.333 = 4,050.06328</p>
</blockquote>

<p><strong>Tools for Calculating</strong></p>

<p>Kodak makes a free app for iOS called ""KODAK Cinema Tools"" which has a film calculator, among other features.</p>

<p><a href=""http://motion.kodak.com/US/en/motion/Tools/Mobile/index.htm"" rel=""nofollow"">http://motion.kodak.com/US/en/motion/Tools/Mobile/index.htm</a></p>

<p>Panavision has a simple website with a film footage, frame and running time calculator:</p>

<p><a href=""http://www.panavision.co.nz/main/kbase/reference/calcfootage.asp"" rel=""nofollow"">http://www.panavision.co.nz/main/kbase/reference/calcfootage.asp</a></p>
","3180"
"Thunderbolt 3 & eGPU viable for Video Processing?","1504","","<p>I am pondering about getting a (new) notebook at some point. Apparently TB3 (officially) supports external GPU's now. The problem is that the speed of TB3 is still much lower than Pci-x... So I was wondering - and maybe someone knowledgeable can give some insight - if a notebook with TB3 and an eGPU will be a viable solution for video processing with davinci resolve and other gpu accelerated software, or how important is the ""low"" IO speed of TB3 vs PCI in that context?</p>
","<p>I've looked into this recently myself, given that the MBP line is not exactly a workhorse when it comes to GPU processing. </p>

<p>eGPUs are actually being done now over TB2, with great success. I have a friend who purchased a similar unit to the <a href=""https://www.akitio.com/expansion/thunder2-pcie-box"" rel=""nofollow"">Akitio Thunder2</a> (the <a href=""https://bizon-tech.com/us/bizonbox2-egpu.html/"" rel=""nofollow"">Bizon-Tech</a> version), put a decent GPU in it (I think it was the GTX960 or similar), and experienced almost 50% faster render time in Premiere (the latest versions of CC support GPU acceleration for encode/decode with some codecs). </p>

<p>The math gets a little funky, but actually <a href=""http://www.tested.com/tech/457440-theoretical-vs-actual-bandwidth-pci-express-and-thunderbolt/"" rel=""nofollow"">this Tested article</a> comparing PCI Express to Thunderbolt indicates that the Thunderbolt2 spec, in practice, provides almost the equivalent of a PCIe3.0 interface. The bandwidth speed lost will be minimal. Further, the article indicates that <em>performance</em> of the graphics card that is lost by a slower transfer speed is, for all practical intents and purposes, negligible (as little as 5% in most cases).</p>

<p>I'm certain that the performance gained by an eGPU box, even over TB2 on current-gen hardware, would vastly outstrip the current performance you're getting on a CPU-accellerated-only laptop. </p>

<p>And, the best part is, you're out only around $1500 for a box and a good graphics card.</p>

<p><em>For the record:</em></p>

<p>I don't know what notebook you're using, but I'll assume a relatively recent MBP. I'm working here from a 2014 15"" Retina which happens to have dual GPUs (integrated Intel Iris and discrete GeForce GT 750M), and based strictly on CUDA cores, you'll have to pick up something in the K1200+ range if you want to see a significant improvement (the <a href=""http://gpuboss.com/graphics-card/GeForce-GT-750M"" rel=""nofollow"">GT750M has 384 cores</a>). However, it's relatively weak at only 2GB GDDR5 (roughly comparable to a K620 in terms of cuda cores and memory).</p>

<p>Improving both specs is important, particularly for 4K or RAW video. I would recommend something like a K2200 or K5000, or if you've had a good year, any of the Maxwell-class cards would be a beautiful, beautiful thing.</p>

<hr>

<p>TechInferno - <a href=""https://www.techinferno.com/index.php?/articles/guides/2015-13-macbook-pro-gtx97016gbps-tb2akitio-thunder2-win81osx1010-tranj10-r2/"" rel=""nofollow"">2015 MBP + GTX970 + Akitio Thunder2 via TB2</a></p>

<p>LinusTechTips - <a href=""https://linustechtips.com/main/topic/277270-macbook-pro-with-a-gtx-980-setup-guide-benchmarks-egpu-setup-windows-10/"" rel=""nofollow"">2012 MBP + GTX760 + Akitio Thunder2 via TB1</a></p>

<p>Bizon-Tech - <a href=""https://bizon-tech.com/us/bizonbox2-egpu.html/"" rel=""nofollow"">Akitio-based custom enclosure for high-wattage GPUs</a></p>

<p><a href=""http://www.tomsitpro.com/articles/nvidia-quadro-m6000,2-898.html"" rel=""nofollow"">A useful comparison of top-end Kepler vs Maxwell NVIDIA architecture</a></p>

<p>Anecdotally, in 2014 I saw a live demo in <a href=""http://www.awn.com/news/nvidia-showcases-4k-your-way-nab-2014"" rel=""nofollow"">NVIDIA's booth at NAB</a> of what the K6000 card is capable of - at the time, mindblowing performance. They put dual K6000's in a Z820 workstation and did realtime 4k debayering of RED Raw, <em>with</em> scale and blur effects, in Premiere. So yeah, duh - it's a powerful card, right? Point is, you don't have to look at the latest Maxwell cards to get performance improvements in an eGPU setup - even a single quality card will provide much more efficient and expedient results than CPU-only.</p>
","18065"
"How to make a video of an iPad (or other tablet) screen","1496","","<p>I have been making some movies of my iPad for promotional videos and they all range from bad to worse in terms of video quality. I have used compact cameras and camcorders with pretty similar results. </p>

<p>Most of the stuff I shoot is AVCHD 1920x1080 at 30fps. The camera is positioned to take in the whole screen of the iPad and little (or nothing) else.</p>

<p>The other day I borrowed a Canon REBEL T2i, thinking that the results were going to be fantastic. They were not. There is a weird strobey effect (NO idea why that would appear <em>more</em> with this camera shooting 30fps than with any other one), and the clarity is not great. In fact, it may look better with a $250 Sony pocket camera I have.</p>

<p>Which got me to thinking: I really haven't the slightest idea what I'm doing. </p>

<ul>
<li>Should the room be dark or light? Or should there be light coming from some specific direction?</li>
<li>Should the iPad's brightness be high, medium or low?</li>
<li>Should the camera be as close to the iPad as possible, while still being able to get focus, or a few feet away?</li>
<li>Since the DSLR has SO many manual settings, what adjustments should I be trying to make?</li>
<li>Since the DSLR has interchangeable lenses, what kind of lens is really appropriate for this sort of thing?</li>
</ul>

<p>I could go on, but I really don't even know the questions to ask. A bit of guidance or orientation or some links to read (that are not 400-page books, unless they are titled ""How to Make a Video of a Tablet"") would be appreciated. </p>
","<p>From what I recall </p>

<ul>
<li>The room should be dark, so the tablet is the light source.</li>
<li>Experiment with the brightness, but usually low is the way to go.</li>
<li>Camera doesn't need to be extremely close, but the more zoom you use the more obvious vibrations will be.</li>
<li>Put the DSLR in manual focus and adjust it so the tablet is just slightly out of focus - this will remove the moire effect.</li>
<li>You want a short depth of field. This is achieved with a lower F-Stop (larger aperture opening).</li>
</ul>

<p>I attended a session by <a href=""http://www.pocketpixels.com/"" rel=""nofollow"">Hendrik Kueck</a> (creator of <a href=""http://www.pocketpixels.com/ColorSplash.html"" rel=""nofollow"">Color Splash</a>) at <a href=""http://360iDev.com"" rel=""nofollow"">360iDev</a> on this topic specifically. It was called ""<em>Effectively using videos to explain and market your app</em>"". You can <a href=""http://360idev.com/session-videos#360idevaustin2010"" rel=""nofollow"">buy that video online</a> for $3.50. It goes into a lot more detail and explains everything really well (and may even correct me on a couple points.) It is a deal at 10x that price.</p>
","3500"
"Why isn't ffmpeg used in movie production or broadcasting tool chains?","1489","","<p>Is the assumption in the title true?</p>

<p>What is preventing the use of ffmpeg when producing a movie or broadcast quality material?</p>

<p>Are there any prominent or well known users of ffmpeg?</p>

<p>What is used instead of ffmpeg? Quicktime pro? Final Cut Pro/Adobe Premiere/Avid media composer?</p>

<p>Edit:</p>

<p>One interesting indication of ffmpeg usage could be job offer listings from major media firms where they are looking for ffmpeg skills.</p>
","<p>FFmpeg is probably being used more than you believe. I think the BBC uses it for some workflows, there is evidence that Laika and Weta may use it, and there is a fork called FFmbc which is targeted for professional broadcast usage.</p>

<p>YouTube probably uses FFmpeg to decode as shown by some unique decoding issues (but this was several years ago that I read about this and I'm not sure of the current status).</p>

<p>Also see the <a href=""http://ffmpeg.org/projects.html"">FFmpeg Projects</a> page for a small list of who uses FFmpeg in their projects.</p>
","9557"
"What are the minimum camera characteristics required to record a movie for cinema?","1486","","<p>What are the minimum characteristics required for a camera to project a movie taken by it on a standard cinema screen? I mean anything that affects the quality of the picture seen on the screen. What sort of camera should be used to shoot the video?</p>

<p><strong>Edit:</strong> To be more clear:<br>
If I record a movie with my cellphone's camera, there is no doubt its quality is not good enough to be projected on a cinema. So I should use a more professional camera! My question is about the minimum characteristics this professional camera should have. Like the number of frames it can record per second, the size of picture it records, the number of pixels per picture it records, etc.</p>
","<p>If I understood your question correctly, you want to project a movie on a cinema.</p>

<p>This would depend on the projector used by the movie theatre. Most movie theatres use film projectors, for which there really isn't a ""quality"", but rather you'd have to worry about getting a decent camera with decent film at the correct size. In this case I don't know much about transferring from digital to film. </p>

<p>If the movie theatre uses a digital projector though, they usually come in 2K (20481080) at 24 or 48 frames per second, or 4K (40962160) at 24 fps if they follow the DCI specification. Basically, get the best camera you can get. 2K should be enough to have a pretty good quality experience at the distance a cinema projection is seen from.</p>

<p>Also remember though, the quality does not only depend on the amount of pixels on screen, but also on the compression used, and the sensor of the camera itself, so be prepared for large files or a lot of processing to compress your movie to a high quality profile.</p>

<p>If you can't get a 2K camera, 1080p should do: it's not too far from 2k: it the same vertical size, <a href=""http://en.wikipedia.org/wiki/File%3aVector_Video_Standards2.svg"" rel=""nofollow"">just a little less wide</a>. </p>

<p>Links that might help: </p>

<p><a href=""http://www.mkpe.com/digital_cinema/faqs/tech_faqs.php#2K+4K"" rel=""nofollow"">http://www.mkpe.com/digital_cinema/faqs/tech_faqs.php#2K+4K</a>
http://en.wikipedia.org/wiki/Digital_cinema</p>

<p>Hope it helps!</p>
","3996"
"Lossless screen-recording software","1476","","<p>For a small project of mine I want to record short screen-casts which show features of my program. Currently, I'm using a small app on my Ubuntu box which creates directly animated gif output. The drawback is, that gif has some limitations in the number of colors. If <a href=""http://halirutan.de/start"" rel=""nofollow noreferrer"">you inspect some of the gifs</a> you may note that although it looks great, it's not perfect, especially when you look at the aliased fonts</p>

<p><a href=""http://halirutan.de/images/pluginBanner/B_SymbolDoc.gif"" rel=""nofollow noreferrer"">animated gif http://halirutan.de/images/pluginBanner/B_SymbolDoc.gif</a></p>

<p>There are some handmade solutions, for instance the demonstration on the <a href=""http://www.sublimetext.com/"" rel=""nofollow noreferrer"">sublime-text site</a>. To have something that nice, you first need to record your screen completely lossless. Most screen-casting programs I know unfortunately do compress the video.</p>

<p><strong>Question:</strong> I have Ubuntu and OSX at my disposal, what (if possible free) screen-casting solutions exists which let me create lossless videos?</p>

<p>Extra credit for programs where I can select the region or window to record and turn off audio.</p>

<h2>Update</h2>

<p>To make it clear what I mean with lossless: Usually, videos are compressed with a method which is perfect for <a href=""http://www.hdwallpaperstop.com/wp-content/uploads/2013/06/forest-landscape_pictures_hd_wallpapers_nature_backgrounds_autumn_forest_and_landscape_color.jpg"" rel=""nofollow noreferrer"">natural images/videos</a> but horrible for images/videos having <a href=""http://upload.wikimedia.org/wikipedia/commons/3/3b/Ringing_artifact_example.png"" rel=""nofollow noreferrer"">sharp edges and one-colored structures</a>. I made two screen-casts and zoomed a small part so that those effects are visible. The first one is done with <a href=""http://wiki.ubuntuusers.de/recordMyDesktop"" rel=""nofollow noreferrer"">gtk-recordmydesktop</a> with highest quality and the artifacts in the gray area are clearly visible</p>

<p><img src=""https://i.stack.imgur.com/Q5oNm.png"" alt=""enter image description here""></p>

<p>The second one is done with the small <em>Record your Desktop</em> tool which saves gif files directly and takes the exact pixels of the screen without compression.</p>

<p><img src=""https://i.stack.imgur.com/vK5GH.png"" alt=""enter image description here""></p>

<p>What I want is a screen-caster which saves its videos in this raw format without the limitations of gif.</p>

<p>Possibly related questions:</p>

<ul>
<li><a href=""https://video.stackexchange.com/q/4477/4310"">How does Salman Khan make his screencasts?</a></li>
<li><a href=""https://video.stackexchange.com/q/3324/4310"">creating a lossless mp4 screencast</a></li>
</ul>
","<p>FFmpeg with a lossless codec is one solution. I recall <a href=""http://boredzo.org/codec-comparison/"">a comparison of different codecs</a> which concluded that Apple Animation (known to ffmpeg as <code>qtrle</code>) gave the best quality for screen-recording.</p>

<pre><code>ffmpeg -f x11grab -r 25 -s 1024x768 -i :0.0 -c:v qtrle output.mov
</code></pre>

<p>Stop it by opening its terminal while it is running and pressing <code>q</code>. Obviously change the resolution (1024x768 here) to match your own. For selecting a section of your screen, see <a href=""https://ffmpeg.org/trac/ffmpeg/wiki/How%20to%20grab%20the%20desktop%20%28screen%29%20with%20FFmpeg"">how to grab the screen</a> on the ffmpeg wiki.</p>

<p>It is possible that the <code>ffmpeg</code> which comes with your Linux distribution does not have the <code>qtrle</code> codec included. In this case you have to <a href=""http://www.ffmpeg.org/index.html"">download and compile</a> your own version. </p>
","8315"
"Opteka CXS-2 vs Glidecam HD-2000","1469","","<p>Recently I have been looking at a lot of reviews of these two video stabilization products and now I simply can't make up my mind - which one is better for me, which one should I buy?</p>

<p>I mean, the Glidecam is amazing, but is it really worth the price? Perhaps Opteka shoulder rig would be enough if I'm filming just camps, public events and such things.</p>

<p>Perhaps you have had a similar experience. Perhaps there's something else that you can suggest for a low-budget starter in DSLR film making?</p>

<p>Ps. - I already have a really great tripod, but I would like to be able to move around while recording.</p>

<p>Opteka CXS-2: <a href=""http://cheesycam.com/optekas-new-cxs-2/"" rel=""nofollow"">http://cheesycam.com/optekas-new-cxs-2/</a></p>

<p>Glidecam HD-2000: <a href=""http://www.glidecam.com/product-hd-series.php"" rel=""nofollow"">http://www.glidecam.com/product-hd-series.php</a></p>
","<p>I have a glidecam 2000 and a chest harness for it.  I also have a shoulder mount similar to the opteka one you linked.</p>

<p>I use shoulder mount far more then the glidecam.<br>
It takes time and practice to get good at the glidecam.  Lots and lots of time.
After using it with a chest harness I would never use it by itself again.
Without any extra support just the glidecam is exhausting for your arm.  Guess what your arm does when it's tired, it shakes..</p>

<p>The shoulder mount rocks.  I can give that to a rooky camera op and they have very few issues using it.</p>

<p>It's ideal as a mobile shoot.  Unless you have good image stabilization neither can move very fast or far.  </p>

<p>Keep in mind neither can move far or fast, your not going to be running after your actor .</p>

<p>You will also need to shoot with a wide lens that has image stabilization using either of these pieces of gear.</p>
","4201"
"Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height","1463","","<pre><code> ffmpeg -i test.mp4 -acodec libvorbis -vcodec libvpx out.webm
ffmpeg version N-57647-g1880294 Copyright (c) 2000-2013 the FFmpeg developers
  built on Oct 30 2013 19:25:44 with gcc 4.4.7 (GCC) 20120313 (Red Hat 4.4.7-3)
  configuration: --prefix=/usr/local --enable-version3 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libvpx --enable-libfaac --enable-libmp3lame --enable-libtheora --enable-libvorbis --enable-libx264 --enable-libvo-aacenc --enable-libxvid --disable-ffplay --enable-shared --enable-gpl --enable-postproc --enable-nonfree --enable-avfilter --enable-pthreads --extra-cflags=-fPIC --arch=x86_64
  libavutil      52. 48.100 / 52. 48.100
  libavcodec     55. 39.100 / 55. 39.100
  libavformat    55. 19.104 / 55. 19.104
  libavdevice    55.  5.100 / 55.  5.100
  libavfilter     3. 90.100 /  3. 90.100
  libswscale      2.  5.101 /  2.  5.101
  libswresample   0. 17.104 /  0. 17.104
  libpostproc    52.  3.100 / 52.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'test.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf55.19.104
  Duration: 00:01:47.16, start: 0.023220, bitrate: 273 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 320x180 [SAR 1:1 DAR 16:9], 192 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: aac (mp4a / 0x6134706D), 44100 Hz, mono, fltp, 74 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
File 'out.webm' already exists. Overwrite ? [y/N] y
[libvpx @ 0xc4bd00] v0.9.0
[libvpx @ 0xc4bd00] Failed to initialize encoder: ABI version mismatch
Output #0, webm, to 'out.webm':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf55.19.104
    Stream #0:0(und): Video: vp8, yuv420p, 320x180 [SAR 1:1 DAR 16:9], q=-1--1, 200 kb/s, 90k tbn, 25 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: vorbis, 44100 Hz, mono, fltp (default)
    Metadata:
      handler_name    : SoundHandler
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 -&gt; libvpx)
  Stream #0:1 -&gt; #0:1 (aac -&gt; libvorbis)
Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height
</code></pre>

<p>Is my CLI wrong? Do I need to specify more commands.</p>
","<p>This is an issue with your ffmpeg installation.  You have a mismatch between the version numbers of the libraries you are trying to use.  Most likely, your version of the webm libraries is not built on the same version of ffmpeg as you are using.</p>
","9790"
"Orbits in After Effects","1460","","<p>I'm new to After Effects and I might be asking about obvious stuff, but I searched this site and elsewhere and can't find an answer. </p>

<p>Basically I want to orbit some points in a setup similar to the Sun - Earth - Moon (Sun is static, Earth goes around the Sun, and the Moon goes around the Earth), and I need each moving point to have its position change over time so that I can parent other stuff to them.  </p>

<p>I'm trying to animate this graphic:</p>

<p><img src=""https://i.stack.imgur.com/0QPsW.jpg"" alt=""enter image description here""></p>

<p>I've got the epicycle (the small circle) to rotate around point T (Terre), while V (Venus) is rotating on the epicycle. I have a beam between T and the center of the epicycle (C), but I would also need to add a beam between C and Venus. </p>

<p>I can get Venus to rotate on the epicycle either by making it an ellipse on it's own layer and then parenting it to the epicycle; or I can add it as another ellipse on the same shape layer as the epicycle, making V rotate with the epicycle. But in both cases the position of Venus remains invariable, so the beam can't follow.</p>

<p>How do I make point V rotate on the epicycle such that its own position changes over time allowing me to add a beam between it and the center of the epicycle?</p>

<p>I'm confused by all the different Position parameters, if I create V as an ellipse on its own shape layer there'll be 3 different position parameters and none if them will change over time if V is parented to the epicycle. I've also tried alt-pick-whipping these 3 to the position of the epicycle with no luck. </p>

<p><img src=""https://i.stack.imgur.com/OjvYK.png"" alt=""enter image description here""></p>
","<p>Ok, it's relatively straightforward using some expressions. But then again isn't everything?</p>

<p>Assuming you've got your original rotating beam layer, your epicycle layer which is parented to the original beam, and your Venus layer parented to that. You need to find the co-ordinates of the center of the Venus layer, relative to the Composition, in order to draw a beam to it. </p>

<p>Well you could work it out trigonometrically I'm sure, but it's way too late and I've been into the Irish Single Malt, so I'm not going to go there. Luckily there's a handy expression that does the job: <code>toComp(point)</code>. What it does is find the composition co-ordinates of a point on a layer. </p>

<p>First we apply a <strong>Point Control</strong> effect <em>Effects>Expression Controls>Point Control</em> to the Venus layer, we'll use the <code>toComp</code> expression to set its value. But before that, make sure the <strong>Anchor Point</strong> property of the Venus layer is showing  we'll need it in a second. </p>

<p>Now to apply an expression to the point control <kbd>alt/opt</kbd>-click the stopwatch on the <strong>Point</strong> property of that effect. In the expressions editor that opens up type <code>toComp(</code> and then drag the expression pick-whip to the <strong>Anchor Point</strong> property to fill in the value <code>Transfrom.AnchorPoint</code>. You can type it, but it's case sensitive and not at all forgiving of typos. Then close the brackets: <code>)</code> it should now read <code>toComp(transform.anchorPoint)</code>.</p>

<p><img src=""https://i.stack.imgur.com/KNJV8.png"" alt=""enter image description here""></p>

<p>So now you have a point that tells you where exactly the Venus Layer's anchor point is in the comp. Using an expression you could plug this straight into the <strong>End Point</strong> property of the <strong>Beam</strong> effect, but I'm allergic to the beam effect, I prefer to use shape layers, because vectors.</p>

<p>So I make a new rectangular shape layer with the stroke I want, and no fill, and I set all the transforms to 0. The size of the rectangle path is going to be the length between the start of the original beam, and the value of the <strong>Point Control</strong> point, AKA the position of Venus. You can do this by getting the square root of the sum of the squares of the X and Y offsets, but that's a bit 6th century BC, instead we'll use the handy expression <code>length(point1, point2)</code>. And we'll set the height to 0, so that it draws a line.</p>

<p>So on the rectangle shape layer, in the <strong>Contents>Rectangle 1>Rectangle Path 1</strong> Property we'll use this expression:</p>

<pre><code>origin=thisComp.layer(""origin"");
venus=thisComp.layer(""venus"")
beamlength=length(origin.transform.position, venus.effect(""Point Control"")(""Point""));
[beamlength,0]
</code></pre>

<p>layer <em>origin</em> is the original beam. You could do it all on one line, but it would be hella ugly:</p>

<pre><code>[length(thisComp.layer(""origin"").transform.position,thisComp.layer(""venus"").effect(""Point Control"")(""Point"")),0]
</code></pre>

<p>Beauty is truth. </p>

<p><img src=""https://i.stack.imgur.com/hEReD.png"" alt=""enter image description here""></p>

<p>Since by default the rectangle tool draws rectangles around a central point we need to offset it. In the <strong>Contents>Rectangle 1>Rectangle Path 1>Position</strong> Property (not the layer's position property, that should be the same as the original beam, or the shape's <strong>Contents>Rectangle 1>Transform: Rectangle 1>position</strong> property, that should be 0,0) we offset it by half the width, so that the rectangle's left edge is always at the origin, thus : <code>content(""Rectangle 1"").content(""Rectangle Path 1"").size/2</code> (did you notice that we're dividing a vector by a scalar? 100 Nerd points to you if you did.)</p>

<p>Now we've got a beam that is the right length, but we need to rotate it. Thank you and good night Pythagoras, hello Mr Aryabhata. Since we know the rise and the run, we can find the angle using Atan(rise/run). But there's some problems: when the run is negative Atan will return a positive value, and whenever the run is 0 the expression will crash. And lastly <code>Math.atan</code> returns a result in Radians, so we need to convert it to degrees. So we have to turn a relatively simple expression into this monster, which goes on the <strong>Contents>Rectangle 1>Transform: Rectangle 1>Rotation</strong> Property:</p>

<pre><code>s=thisComp.layer(""venus"").effect(""Point Control"")(""Point"");
o=thisComp.layer(""origin"").transform.position;
run=s[0]-o[0];
rise=s[1]-o[1];   
if (run==0){
 if (rise&gt;0) //pointing straight up
  {-90;}
 else //pointing straight down
  {90;}
} else{
 if (run&gt;0){
    radiansToDegrees(Math.atan(rise/run));
  } else {
    180+radiansToDegrees(Math.atan(rise/run));
  }
}
</code></pre>

<p>this is how my beam layer looks:
<img src=""https://i.stack.imgur.com/SiWGv.png"" alt=""enter image description here""></p>

<p>and below is the whole comp. I've used shape layers for everything and driven them all by expressions as is my wont, if you want to have a look at it, it's <a href=""http://adobe.ly/1xy8D30"" rel=""nofollow noreferrer"">here</a>.</p>

<p><img src=""https://i.stack.imgur.com/MWjw8.png"" alt=""enter image description here""></p>
","14636"
"How do you play a clip backwards in Sony Vegas?","1457","","<p>While editing some vocals the other day, I accidentally hit a key in Vegas and the scrub head started going backwards, playing my audio in reverse. I wasn't trying to do this at the time and got it playing normal again. Today I was thinking that this could be useful when trying to find a point where the vocalist takes a breath. Rather than guess, split the clip and play from a few seconds earlier to hear if it is gone, then repeat the process, you could just set the play head past the breath, play it backwards and find the exact point that they start their breath.</p>

<p>Does anyone know the key you hit to play the scrub head backwards in Sony Vegas?</p>

<p>(This is not flipping the clip to play in reverse using ""Right-click/Reverse"", I don't need that, I just need the play head to play in reverse temporarily, like scrubbing it backwards)</p>
","<p>Found it!</p>

<p>You can press the ""J"" key to play in reverse. Each time you hit the ""J"" it will jump a full speed up to x4. If you want to jump by speeds of .25x, hold the ""K"" button and hit ""J"". While it is play backwards, hitting the spacebar will jump you back to where you started playing, and hitting ""K"" will stop you where you are.</p>

<p>To adjust playback speeds going forward, you can use the ""L"" key in the same manor.</p>

<p>To find the exact point where a breath is taken, you place the play head about where the breath starts, then hit ""J"", it will play backwards for a bit. Hit the spacebar and adjust the playback spot by hitting the left or right arrow, then hit ""J"" again. Once you are exactly where you want to cut the clip, hit ""S"" and you will split the clip right there. </p>

<p>(The arrows will jump one ""frame"" according to the frame rate of your project, even if it is an audio project. If it is set to 24 fps, it will jump 1/24th of a second forward or backward)</p>
","5511"
"How can I create the blurring effect in a video like this?","1450","","<p>I am noticing more and more product videos being released using lots of blur and the focus shifting from one point to another.</p>

<p>This is an example of the technique I'm refering to:
<a href=""http://vimeo.com/43763778"" rel=""nofollow"">http://vimeo.com/43763778</a></p>

<p>How is the shot accomplished?</p>

<p>I am noob when it comes to video editing, so I want to ask what are the producers of this video most likely using, and how are they doing it?</p>

<p>Sorry if this question seems a bit open ended, but I just want to ask, what techniques have been apparently used on this video to make it what it is.</p>

<p>Thank you.</p>
","<p>There are no filters applied there.  It's pretty much basic shots, probably from a DSLR based on the feel of the video.  The shots with the one guy were shot with things overexposed and then they upped the contrast to try and do recovery, but it results in large overblown highlights where there was too much light to fit in the dynamic range (the amount of variation between brightest and darkest spot) that the camera could capture.  It otherwise all looks like standard footage though, nothing special done other than basic color grading.</p>

<p>Note that if you are talking about the shifting blur effect, that's called rack focus and is so apparent because of the use of a fast lens with a short depth of field.  It looks remarkably similar to footage I get with my 5D Mark iii and 24-70mm f/2.8 L lens.</p>

<p>Rack Focus can either be from out of focus completely to focus on a particular subject or from one subject to another. It generally uses a narrow depth of field and a follow focus may be used to help reach the focus points exactly and in a controlled manner.</p>

<p>The technique is often used either to introduce a subject from a dream or stunned like state (for example, if someone is waking up or in an accident, their pov might start out with a rack focus from being completely out of focus). It is also very frequently used to shift the viewer's focus from one subject to another in a dramatic way. In the more artistic sense, it can also be used to simply explore the depth of the scene by moving the viewer through the space.</p>
","7968"
"Waving flag in After Effects from vector","1442","","<p>Could anyone provide some insight on how to go about animating a vector flag (such as the one pictured below) to wave in the wind. Simultaneously, how could also animate what's ""printed"" on the flag to match? (Another example below). Thanks for any help.</p>

<p><img src=""https://i.stack.imgur.com/EEfpp.png"" alt=""Vector flag""></p>

<p><img src=""https://i.stack.imgur.com/Xd18b.gif"" alt=""Waving flag""></p>
","<p>The simplest way would be: </p>

<ul>
<li>Create your flag in Illustrator</li>
<li>Think about or simply test how many frames/steps are key for animation</li>
<li>Draw your <em>frames/steps</em> on seperate Illustrator layers in to get the same position for all of it</li>
<li>Paste your <em>frame/path</em> with <kbd>CTRL</kbd>+<kbd>V</kbd> on <strong>1 solid layer</strong> every 4-20 frames by switching between After Effects and Illustrator back and forth - time depends on your animation</li>
</ul>

<p><strong>Explanation:</strong> Through pasting the masks at a different time, After Effects automatically creates keyframes of the mask shape property. All copied shapes become interpolated keyframes like a roto shape animation to get soft transitions. </p>

<p><strong>Note:</strong> The only limitation is that the number of anchor points <em>per vector image</em> should remains the same to get soft interpolation between the <em>shape keys</em>.</p>
","14910"
"How can I make a video more seekable?","1429","","<p>I am looking at the factor that allows software video players to seek at any (or close to) position in the video.</p>

<p>Is this the keyframes or what? </p>
","<p>The problems with seeking are typically in the video player and not in the videos themselves, so you may want to look at different video player. A video player should be able to seek to any frame in the video, regardless of format and the encoder settings used. It is also technically possible to play any video backwards, even though very few players support this.</p>

<p>The problem is, seeking into frames that are not keyframes is difficult and CPU intensive. Frames that are encoded as keyframes can be decoded on their own, while non-keyframes depend on other frames to be decoded. If you are playing the movie left to right all the dependencies  for a given frame have been already decoded, but if you are seeking directly into the frame, then the player needs to search for all those dependencies, decode them and only then it can decode the target frame.</p>

<p>So I think I can summarize my advice as two different suggestions:</p>

<ol>
<li><p>Switch to a player that can seek to any frame.</p></li>
<li><p>if you can't switch to a better player, then encoding your video with more frequent keyframes will give you better seek accuracy, at the expense of picture quality (if you keep the bitrate the same) or file size (if you increase the bitrate to compensate).</p></li>
</ol>

<p>Good luck.</p>
","3680"
"How to avoid uploaded video being re-encode by Facebook","1425","","<p>I just uploaded a video to Facebook yesterday, and then I tried to download the video as HD, and I realized that Facebook actually re-encode the video even though the original video file doesn't rendered with high bitrate or super HD option.</p>

<p><strong>Original video</strong><br></p>

<blockquote>
  <p>File size: 50 Mb<br>
  Length: ~3 mins<br>
  Bitrate: <strong>2 Mbps</strong><br>
  Resolution: 1280x720<br>
  Frame rate: 25 fps<br>
  Reference frame: 8<br>
  Level: 4<br>
  Audio bitrate: <strong>128 Kbps</strong></p>
</blockquote>

<p><br>
<strong>Uploaded video</strong><br></p>

<blockquote>
  <p>File size: 20 Mb<br>
  Length: ~3 mins<br>
  Bitrate: <strong>750 Kbps</strong><br>
  Resolution: 1280x720<br>
  Frame rate: 25 fps<br>
  Reference frame: 5<br>
  Level: 3.1<br>
  Audio bitrate: <strong>48 Kbps</strong></p>
</blockquote>

<p>As you can see there's a big loss of quality in the uploaded video. How can I avoid Facebook leave my video untouched? Or it will re-encode the video no matter how small the file size is?</p>
","<p>You can't avoid the re-encode. They do this to be sure that every video they serve is in a standardized format, resolution and bitrate. It would be inefficient and risky to vet all diverse set of incoming files to check if they meet all their parameters, some of which are not easily available to set or tweak at the user's end.</p>
","17537"
"ffmpeg: can I crop using vf while also using filter_complex at the same time?","1416","","<p>I have an <code>ffmpeg</code> command using <code>-vf</code> to square crop a video:</p>

<pre><code>ffmpeg -i input.mp4 -vf \
""crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)', scale=720x720"" \
-an -c:v libx264 -profile:v high -level 4.1 \
-preset superfast -crf 20 output.mp4
</code></pre>

<p>I also have a separate <code>ffmpeg</code> command using <code>-filter_complex</code> to apply a slow motion ""profile"" (normal speed, then slow, then normal speed) to a video:</p>

<pre><code>ffmpeg -i input.mp4 -filter_complex \
""[0:v]trim=0:4.95,setpts=PTS-STARTPTS[v1]; \
 [0:v]trim=4.95:6.75,setpts=PTS-STARTPTS[v2]; \
 [0:v]trim=6.75:8,setpts=PTS-STARTPTS[v3]; \
 [v2]setpts=PTS/0.1[vslow2]; \
 [v1][vslow2][v3]concat=n=3:v=1:a=0[out]"" \
-map [out] -an -c:v libx264 -profile:v high -level 4.1 \
-preset superfast -crf 20 -r 30 output.mp4
</code></pre>

<p>If I try and put them together:</p>

<pre><code>ffmpeg -i input.mp4 -vf ""crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)', scale=720x720"" \
-filter_complex \
""[0:v]trim=0:4.95,setpts=PTS-STARTPTS[v1]; \
 [0:v]trim=4.95:6.75,setpts=PTS-STARTPTS[v2]; \
 [0:v]trim=6.75:8,setpts=PTS-STARTPTS[v3]; \
 [v2]setpts=PTS/0.1[vslow2]; \
 [v1][vslow2][v3]concat=n=3:v=1:a=0[out]"" \
-map [out] -an -c:v libx264 -profile:v high -level 4.1 \
-preset superfast -crf 20 -r 30 output.mp4
</code></pre>

<p>I then get:</p>

<pre><code>-vf/-af/-filter and -filter_complex cannot be used together for the same stream.
</code></pre>

<p>Is there a way of applying the <code>crop</code> filter to the entire stream, or will I have to apply it to each of the <code>[v1]</code>, <code>[v2]</code>, etc. streams?</p>
","<p>Use this:</p>

<pre><code>ffmpeg -i input.mp4  -filter_complex \
""[0:v]crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)',scale=720x720,split=3[1v][2v][3v]; \
 [1v]trim=0:4.95,setpts=PTS-STARTPTS[v1]; \
 [2v]trim=4.95:6.75,setpts=(PTS-STARTPTS)/0.1[v2]; \
 [3v]trim=6.75:8,setpts=PTS-STARTPTS[v3]; \
 [v1][v2][v3]concat=n=3:v=1:a=0[out]"" \
-map [out] -an -c:v libx264 -profile:v high -level 4.1 \
-preset superfast -crf 20 -r 30 output.mp4
</code></pre>
","18356"
"How to identify i-frame from idr-frame in the ffprobe show frames output?","1413","","<p>The ffprobe show frames output shows the following details:</p>

<pre><code>[FRAME]
media_type=video
key_frame=1
pkt_pts=42
pkt_pts_time=0:00:00.042000
pkt_dts=42
pkt_dts_time=0:00:00.042000
pkt_duration=N/A
pkt_duration_time=N/A
pkt_pos=799
width=1920
height=1080
pix_fmt=yuv420p
sample_aspect_ratio=1:1
pict_type=I
coded_picture_number=0
display_picture_number=0
interlaced_frame=0
top_field_first=0
repeat_pict=0
reference=3
[/FRAME]
</code></pre>

<p>I was reading <a href=""http://www.streaminglearningcenter.com/articles/everything-you-ever-wanted-to-know-about-idr-frames-but-were-afraid-to-ask.html"" rel=""nofollow"">this article</a> and was wondering how to differentiate between the two types of frames from the above output.</p>
","<p>Frames with key_frame <code>1</code> (will have picture type <code>I</code>) are IDR frames.</p>

<p><code>I</code> frames with key_frame <code>0</code> are not.</p>
","19252"
"What is the size of 1 minute video at 1920 x 1080 (60 fps)?","1411","","<p>I would like to know what is the size in MB for 1 minute video captured in the following format:</p>

<p>1920 x 1080 (60 fps) AVCHD</p>

<p>and</p>

<p>1920 x 1080 (60 fps) MPEG-4</p>

<p>with peak data rate of 24Mbit/sec</p>

<p>How to calculate it?</p>
","<p>Try those calculators, they should do the trick.</p>

<p><a href=""http://www.videospaceonline.com/"" rel=""nofollow"">http://www.videospaceonline.com/</a></p>

<p><a href=""http://www.digitalrebellion.com/webapps/videocalc"" rel=""nofollow"">http://www.digitalrebellion.com/webapps/videocalc</a></p>
","16764"
"How to time-remap with Twixtor in Adobe Premiere?","1409","","<p>I have problem to understand, how Twixtor works. </p>

<p>I want to use Twixtor plugin with my Adobe Premiere Pro. I can only change speed for whole sequence. Adobe has its own filter called ""Time remapping"", but that does not change speed so smoothly, as I saw in some videos.</p>

<p>Does Twixtor have keyframes? Where I can set start and end of slow down effect?</p>
","<p>Just like with any effect property you can animate it by clicking on the little stop watch on the left of the effect property name (see below).</p>

<p>You can apply the effect to just a single clip, you don't have to use it on the entire sequence, just drag and drop the effect from the effect list onto the desired clip and then animate the speed of the video via keyframes.</p>

<p>A screenshot from the ""Effects Control"" panel:</p>

<p><img src=""https://i.stack.imgur.com/K7ic8.png"" alt=""enter image description here""></p>
","12135"
"After Effects CC slant effect crops artwork","1408","","<p>I'm a newbie to AE so there's probably a simple answer to this. So I've imported some art from illustrator and dropped in on the stage. I then <code>Create shapes from vector layer</code>. Then I apply the effect <code>CC Slant</code> under the Distort menu under Effects and give the slant a value of 50.</p>

<p><img src=""https://i.stack.imgur.com/5RVMl.png"" alt=""enter image description here""></p>

<p>Looking good. Until I move it to the edge of the stage and then the artwork gets cropped. Notice how only the ""mple"" of ""Example"" are visible:</p>

<p><img src=""https://i.stack.imgur.com/NV8z2.png"" alt=""enter image description here""></p>

<p>Even after rendering the movie (with the position changing from center stage to left stage while slanted), the problem persists. I have tried re-importing my artwork, and applying the slant effect while the artwork is in different positions on the stage to no avail. I've also tried other effects like the warp effect, and the same thing happens. </p>

<p>Is there a way to keep the artwork from getting cropped on the edge of the stage?</p>
","<p>Depending on how you imported it, it is possible that it ended up nested as a nested composition or smart object.  If you applied the effect to the outer object but then dragged it within the inner object, you could end up dragging it off the edge of the outer object which would produce the result you are experiencing.</p>

<p>Effectively, you would have an outer object that is tilted and an inner object that is going off the edge of the outer.  Thus, the outer object displays only the half of the object you see and then the skew gets applied (which makes the otherwise vertical clipping to be angled.)</p>

<p>That's the only thing that occurs to me as a possible cause.</p>

<p>Even if that isn't it, a work around might be to put the original graphic in a composition and then apply the effect to the composition instead and then move the composition as you need.  This would probably work around whatever issue you are having here.</p>
","9352"
"Which decoder allows for fast seeking or how to configure k-lite codec pack?","1400","","<p>I am using mostly Haaly Media Splitter for input and ffdshow for decoding (libavcodec).
Actually I am ok with the default profile 7 installation of k-lite codec pack.</p>

<p>At the moment I have the default options in the ffdshow decoder configuration and in the Haali media splitter filter.</p>

<p>What I want is to make seeking as fast as possible. Any suggestions on how to detect the bottleneck or how to reduce seeking time?</p>
","<p>Let me describe what the bottleneck in seeking is.</p>

<p>You didn't say what format are your videos encoded in, but I'm guessing you are using H.264 or maybe MPEG-2. Am I correct?</p>

<p>To maximize quality, these codecs have different compression formats, and each frame gets encoded differently. This is a brief summary of the compression types:</p>

<ul>
<li>I-frames (I stands for intracoded) are frames that can be decoded easily, as they don't depend on any other frames.</li>
<li>P-frames (P stands for predictive) are frames that encode only the differences from one or more previous I or P-frames. To decode a P-frame you need to first decode all the I and P-frames that this frame depends on.</li>
<li>B-frames (B stands for bi-directional) are frames that encode differences like P-frames, but unlike P-frames, these differences can refer to previous or future I or P-frames. As with P-frames, To decode a B-frame you first need to decode all the past and future frames that are dependencies.</li>
</ul>

<p>So now I hope the performance problem with seeking is clear. You jump to the middle of the movie clip, and chances are, you are going to land on a B or a P-frame. So the decoder now needs to find all the references required by that frame, decode them in the correct order and only then it can finally decode the target frame.</p>

<p>To achieve better compression and higher quality, encoders create a lot less I-frames than P or B-frames. The idea is that the compressed stream starts with a really high quality I-frame, and then for a number of frames that follow, smaller P and B frames encode just the differences. Of course if you go for a long time like this quality starts to degrade, so at some point the movie will include another big and nice I-frame and the cycle starts again.</p>

<p>When a movie clip is played from start to end, the different frame types appear sorted in the file so that when playback reaches a given frame, all its references have already been seen by the decoder. This allows for very efficient playback at 1x speed. Unfortunately when you play at other rates or when you seek the amount of work the decoder needs to do to find referenced frames is much larger because frames are read out of sequence.</p>

<p>So going back to your question, the problem isn't really in the decoder. If you need to improve seek time you need to use videos that are encoded to minimize seek time. In essence this means videos that have more frequent I-frames, also called keyframes sometimes. The more I-frames you have, the less references will be required to decode P and B-frames so the decoder will have less work to seek.</p>

<p>I hope this helped understand the problem.</p>
","3182"
"How do I mute my monitors while leaving headphones on?","1389","","<p>Very simply, I want to be able disable my monitor speakers and leave the headphones on.  This is obviously important when recording in the same room.</p>

<p>All of the mixers I have come across place the main level controls before the phones output, which have their own level control.  This seems like a missed opportunity.</p>

<p>Sounds simple enough, but <a href=""http://www.homerecordingconnection.com/forum.php?action=view_thread&amp;id=17303&amp;frm=8"" rel=""nofollow"">this thread</a>, which is the closest I've found to a statement of the question, never addresses it.</p>

<blockquote>
  <p>1) Biggest thing I am not sure how to do is set up my
  monitors/headphones on the mixer so that I can go back and forth when
  recording</p>
</blockquote>

<p>Up to now, I have been driving my monitors from the 1&amp;2 bus of a mixing console.  This allows me to turn down the speakers using the group faders.  But this console is broken, and I would like to replace it with something simpler that is strictly for monitoring (since I don't need the preamps or EQ, etc.)  It seems like a simple line mixer should be able to handle my requirements.  But even the line mixers with 2 aux buses don't have single faders for those sends.</p>

<p>What am I missing here?  How is this normally done?</p>
","<p>you are in the correct track: a small mixer would do the trick. Basically you have the pfl button that would get the specific channel(s)/souce(s) selected to your headhones jack (monitors) without modifying at all your master (and viceversa). Another option would be something like the Mackie Big Knob, made specifically for what you want, but in this case you can only select to monitor a source, not individual channels).</p>
","3040"
"Is it possible to store subtitle srt files in an m3u playlist file?","1376","","<p>I have a folder that contains around 51 videos and another folder with their respective subtitles.  </p>

<p>I don't want to merge the folders into one folder so I was wondering if there's a way to store subtitle <code>srt</code> files location (or at least a folder location for all subtitles) in an <code>m3u</code> playlist file along with the video files locations. </p>

<p><strong>Note:</strong> Video file format is <code>flv</code>.</p>
","<p><code>m3u</code> playlists only contains links to the media itself that you're playing.</p>

<p>It doesn't actually consider the <code>srt</code> file at all. If the software used to play back the <code>m3u</code> file support <code>srt</code> files it will look in the same folder for the same file as the video only with the extension switched to <code>srt</code>.</p>

<p>You will therefor always need to store the <code>srt</code> together with the video file.</p>
","8027"
"How to make Vegas Pro open AVI files","1374","","<p>In using Vegas Pro 13 on a new Windows 10 computer, I find that I cannot open AVI files. It's not a codec issue; the file will not open at all because Vegas doesn't recognize it as a type of file it can open. I've used Vegas Pro for years and do not remember having this issue, but it has been five years since I got a new computer. <a href=""https://video.stackexchange.com/questions/9917/vegas-pro-will-not-show-video-for-avi-with-mp4v-fourcc-codec"">I've had trouble with the codecs in AVI files</a>, but they always opened.</p>

<p>How can I fix this issue?</p>
","<p>It seems that sometimes Vegas Pro's plugin that opens AVI files is corrupt or installed incorrectly (if this is the case, a reinstall may fix it). </p>

<p>There is a simple fix that involves replacing the DLL file that controls AVI files.</p>

<p>The file is called <code>aviplug.dll</code>. The file path will be similar to: <code>C:\Program Files\Sony\Vegas Pro 13.0\FileIO Plug-Ins\aviplug\</code></p>

<ul>
<li><p><a href=""https://www.mediafire.com/?1r1r3g5ut5quuub"" rel=""nofollow noreferrer"">The fixed dll can be found at Mediafire</a>.</p></li>
<li><p><a href=""https://www.youtube.com/watch?v=0h1rBtsqm4A"" rel=""nofollow noreferrer"">Here's a YouTube walkthrough</a>.</p></li>
</ul>
","20465"
"ffmpeg conversion losing frames","1362","","<p>I am using ffmpeg to load and mp4 and save an mp4. When I load the source mp4 (from android screen record) into blender, it will not import properly and unusable.</p>

<p>When I run the mp4 through ffmpeg, the problem is resolved. A new problem is unfortunately introduced.</p>

<p>The resulting video is at a noticeably lower framerate. It is distinguishable to the naked eye, and I have unanimously had three other people confirm that the output video is choppy where the source video is not.</p>

<p>The funny thing is, is that when loaded in VLC, it is reported they are both at 12.58 fps under tools>media info>codec. The source video still looks smooth though, where the output does not.</p>

<p>Does anyone have an direction that could be given in possibly rectifying this problem?</p>

<p>Thanks</p>

<p>Here is the command and the output</p>

<pre><code>ffmpeg -i android_record1.mp4 -crf 18 ../corrected/output.mp4
ffmpeg version N-78598-g98a0053 Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 5.3.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55. 18.100 / 55. 18.100
  libavcodec     57. 24.103 / 57. 24.103
  libavformat    57. 25.101 / 57. 25.101
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 34.100 /  6. 34.100
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'android_record1.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: isommp42
    creation_time   : 2016-02-24 20:17:38
  Duration: 00:00:33.54, start: 0.000000, bitrate: 921 kb/s
    Stream #0:0(eng): Video: h264 (Baseline) (avc1 / 0x31637661), yuv420p, 1280x800, 918 kb/s, SAR 1:1 DAR 8:5, 12.94 fps, 90k tbr, 90k tbn, 180k tbc (default)
    Metadata:
      creation_time   : 2016-02-24 20:17:38
      handler_name    : VideoHandle
[libx264 @ 00000000039fea40] using SAR=1/1
[libx264 @ 00000000039fea40] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2
[libx264 @ 00000000039fea40] profile High 4:4:4 Predictive, level 3.2, 4:2:0 8-bit
[libx264 @ 00000000039fea40] 264 - core 148 r2665 a01e339 - H.264/MPEG-4 AVC codec - Copyleft 2003-2016 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=0 mixed_ref=1 me_range=16 chroma_me=1 trellis=0 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=0 chroma_qp_offset=0 threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=2 keyint=250 keyint_min=12 scenecut=40 intra_refresh=0 rc=cqp mbtree=0 qp=0
Output #0, mp4, to '../corrected/output.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: isommp42
    encoder         : Lavf57.25.101
    Stream #0:0(eng): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 1280x800 [SAR 1:1 DAR 8:5], q=-1--1, 12.94 fps, 315k tbn, 12.94 tbc (default)
    Metadata:
      creation_time   : 2016-02-24 20:17:38
      handler_name    : VideoHandle
      encoder         : Lavc57.24.103 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))
Press [q] to stop, [?] for help
Past duration 0.725090 too large
Past duration 0.716301 too large
Past duration 0.818840 too large
Past duration 0.936516 too large   2196kB time=00:00:05.71 bitrate=3145.4kbits/s dup=66 drop=0 speed=9.91x
Past duration 0.644524 too large
Past duration 0.733879 too large
Past duration 0.888176 too large
Past duration 2.010735 too large
Past duration 0.915520 too large
Past duration 0.774406 too large   3675kB time=00:00:14.37 bitrate=2094.3kbits/s dup=149 drop=8 speed=  13x
Past duration 2.095695 too large
Past duration 1.697746 too large
Past duration 0.891594 too large
Past duration 2.141594 too large   4102kB time=00:00:22.02 bitrate=1525.6kbits/s dup=224 drop=18 speed=13.7x
frame=  436 fps=180 q=-1.0 Lsize=    4702kB time=00:00:33.69 bitrate=1143.2kbits/s dup=227 drop=225 speed=13.9x
video:4699kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.054633%
[libx264 @ 00000000039fea40] frame I:2     Avg QP: 0.00  size:316054
[libx264 @ 00000000039fea40] frame P:434   Avg QP: 0.00  size:  9630
[libx264 @ 00000000039fea40] mb I  I16..4..PCM: 53.7%  0.0% 46.3%  0.0%
[libx264 @ 00000000039fea40] mb P  I16..4..PCM:  1.3%  0.0%  0.4%  0.0%  P16..4:  3.5%  0.2%  0.1%  0.0%  0.0%    skip:94.6%
[libx264 @ 00000000039fea40] 8x8 transform intra:0.0% inter:25.9%
[libx264 @ 00000000039fea40] coded y,uvDC,uvAC intra: 52.8% 52.4% 51.4% inter: 2.1% 2.9% 2.9%
[libx264 @ 00000000039fea40] i16 v,h,dc,p: 50% 50%  0%  0%
[libx264 @ 00000000039fea40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 49%  6%  3%  3%  2%  2%  2%  3%
[libx264 @ 00000000039fea40] i8c dc,h,v,p: 21% 47% 32%  0%
[libx264 @ 00000000039fea40] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 00000000039fea40] ref P L0: 90.2%  1.2%  6.2%  2.4%
[libx264 @ 00000000039fea40] kb/s:1142.43
</code></pre>
","<p>Your source video is variable frame rate and then there's a great variance in the last third of the file. I estimate a peak framerate of 30 fps, based on a crude calculation, so try</p>

<pre><code>ffmpeg -i android_record1.mp4 -crf 18 -r 30 ../corrected/output.mp4
</code></pre>

<p>Experiment with higher values, if needed.</p>
","17756"
"After Effects - Making a compilation","1357","","<p>I have been practicing with Adobe After Effects for quite a while.  I've created some effects like gunshots, intros and such. </p>

<p>I now want to learn how to combine many videos together, and with additional effects - not only fade in and fade out, but more amazing stuff! </p>

<p>I have not found anything helpful when looking for where to start learning about this. </p>

<p>My main occupation is game programming.  Would you suggest that I go learn better video editing and Photoshop or rely on an artist to do so?</p>
","<p>If you are trying to do editing, you are using the wrong program.  Adobe After Effects is an effects and compositing program.  It is designed to make alterations to clips of video that can then be used together.  It is not designed for editing clips together.</p>

<p>If you wish to do video editing, you want a piece of software known as a Non Linear Editor (or NLE).  Adobe Premiere is the NLE in Adobe's software line.  Adobe Bridge allows for you to move back and forth between them, so you can include an After Effects composition as a clip in Premiere or open a clip from Premiere in After Effects to do effects or composition work on the clip.</p>

<p>As a bonus, note that this same kind of interaction can be used with Adobe Encore to add a Premiere Sequence directly to an Encore project if you wish to make a DVD or Bluray disk from your videos.</p>

<p>Update: Since you asked about it, here is a list of the major Adobe CC applications and their purpose.</p>

<ul>
<li><p>Photoshop: In depth Image Manipulation</p></li>
<li><p>Lightroom: Photo cataloging, Non-destructive RAW processing and basic touch up, workflow management, printing, publishing.</p></li>
<li><p>Premiere: Non-linear Video Editing</p></li>
<li><p>After Effects: Video effects and compositing</p></li>
<li><p>Encore: DVD/Bluray disk production</p></li>
<li><p>Prelude: Video/Asset intake</p></li>
<li><p>SpeedGrade: Color grading</p></li>
<li><p>Illustrator: Vector graphic creation and editing</p></li>
<li><p>Flash: Vector and sprite based animation, script-able and interactive capable as well as supporting video playback </p></li>
<li><p>Audition: Audio Editing DAW</p></li>
<li><p>InDesign: Print layout</p></li>
<li><p>Acrobat: Public document finishing, creates PDFs</p></li>
<li><p>DreamWeaver: Web design</p></li>
<li><p>Muse: simpler web design</p></li>
<li><p>Fireworks: Alternate image manipulation application, primarily targeting the web.</p></li>
<li><p>Bridge: Minor Adobe product that interconnects the applications to allow them to exchange nested data</p></li>
<li><p>Media Encoder: Minor Adobe product that allows for a shared render queue and encoding of final assets</p></li>
</ul>
","9573"
"What are the advantages camcorders have over zoom digital cameras?","1355","","<p>It seems that the latest models of compact digital cameras come with many features which used to be only in camcorders: image stabilization, low-light shooting mode and of course high-definition to name a few.
This <a href=""http://www.dpreview.com/reviews/Q311TravelZoomGroupTest/page9.asp"" rel=""nofollow"">article</a> shows some candidates which IMO give pretty good competition to modern camcorders.
I already have a decent SLR camera for taking still pictures, and now i am interested in buying a camera for shooting home videos. I thought of buying a digital camera instead of a  camcorders - but only if I will not be missing out anything that only camcorders have to offer.</p>
","<p><strong>The video capabilities of even the best digital cameras are but a small fraction of these of video cameras.</strong> The gap is smaller now but only marginally so.</p>

<p>Where <strong>digital cameras compare very well is in terms of image quality</strong>, particularly when it comes to low light shooting. For almost everything else, video cameras trump digital cameras.</p>

<p><strong>Camcorders have sophisticated and detailed control over audio recording.</strong> They can set levels and recording angle which almost no digital camera can do. You can plug an external audio source on some still cameras but that is about it.</p>

<p><strong>The optical zoom of camcorders is specially designed for video.</strong> It moves silently and smoothly to give a natural look. Camcorder lenses are built with a continuous variable apertures so that the brightness of images can vary smoothly rather than in steps (only a handfull of SLD lenses can do that). The focus system works equally smoothly and quietly while most digital cameras record AF noise in the audio track.</p>

<p><strong>Camcorders have tons of features which are meaningful to video.</strong> Fading, transitions and even video effects are the norm. They can also record for much longer both continuously and in sequence than still cameras. You can get up to a 128GB SDXC card for a still camera but it is costly while a camcorder can easily have a 200GB HDD built in.</p>
","2860"
"Simple, easy to configure streaming server for linux - just transcode & stream","1347","","<p>After spending a few hours setting up and configuring PlexMediaServer, I realized that it looks <em>awesome</em> for home use, but has some impediments for serving up content to the public at large. </p>

<p>I have 50 or so video presentations (genetics &amp; infant healthcare if you must know) that need to be served up.  The presentations will be available on a web page, and I would simply need a link to point back to a file that the streaming server would transcode &amp; play.  The HTML is not important - I can have simple href links for that.  </p>

<p>Are there any relatively easy to configure and set up streaming servers that can offer this basic functionality?   My server is CentOS 6.5.  </p>
","<p>Ended up with <a href=""http://www.vimp.com/en/web/vimp-community.html"" rel=""nofollow"">ViMP Community Edition</a>.  Took a little while to work through some dependencies but now that it is installed everything works exactly like I want. </p>
","9669"
"Are there any mathematical advantages for higher sampling rates?","1342","","<p>Given that most people can't hear so many frequencies above 20kHz anyways, I've never understood the exact arguments for using sampling rates above 48kHz.  At 48kHz, I understand that it is easier to construct a lowpass filter with a bit of higher bandwidth to remove aliasing, but I don't understand why anybody would want to record at 96kHz.  </p>

<p>For projects which are strictly digital, ie, using pure digital synthesis and not recording any material which would be converted from analog -> digital, is there any advantage to using sampling rates above 44.1kHz?</p>

<p>For everything else, is there any benefit at all to using 96kHz?  Is it beneficial when applying some particular type of DSP operation later on?  Or is it purely a placebo effect for the ear?</p>

<p>Note: There are other questions here asking about which sampling rates to use for various types of recording projects, but here I am asking for <em>real, hard facts</em> for any mathematical or DSP-related reasons supporting use of higher sampling rates.</p>
","<p>I always use double sampling rates if possible, for two important reasons.</p>

<p>First reason: to get rid of the characteristics of the anti-imaging filter when working with analog sound sources. What is an anti-imaging filter?</p>

<p>Let's say I am recording on 44100 Hz.<br>
If I would record a sine wave of less then 10 KHz, you could clearly see the sinewave when you plot the sample values in a graph.<br>
If I sample a sine wave of 0dB FS with a 22,5KHz frequency, the samples read 1 and -1 alternately.</p>

<p>Now, here's the problem. If I record a sine wave of 0dB FS with a 30 KHz frequency, and plot the samples, each sample is taking more than half a sine period, and - if you would play back the samples - it would return an 11KHz sinewave. (If you don't believe me, just make a simple drawing.) This behaviour is called the 'imaging effect'.</p>

<p>This means that before sampling the signal, we have to be sure that there are NO frequencies present what so ever above the so-called ""nyquist frequency"" (which is half the sampling rate). When using digital sound sources that provide their sounds already sampled, this is not really that big a deal, since they can sometimes just be programmed to never generate a signal above half the sampling rate, or they can filter everything out using a linear-phase brickwall filter that has no effect on the rest. </p>

<p>But, if you are sampling signal from an analog source, this filtering is done before the signal is sampled. The only way to filter analog sound is by use of an electronical circuit. And since the filter is supposed to have a very steep curve, <strong><em>it will affect the frequencies within the audible range</em></strong>, even though the filter was not designed for it. Now there are quite some good filters inside A/D converters, so the problem is minimal, but it gets relatively irritating to listen to when you are working several days on 44.1 KHz audio, compared to using 96KHz. The filter that is going to be applied when you downsample 96 back to 44.1 is of course a digital filter, and is probably of a much better quality. And, it is only applied when you are completely done with all the work, so it won't bother you.</p>

<p>Second reason: to get rid of the characteristics of the dithering signal. </p>

<p>When you are recording in 24 bits resolution and you plan to have your master at 16 bits, you will need a dithering signal to mask away the rounding errors. Now noise is not a pretty thing to have in your recording and while broadband noise is best for masking rounding errors, noise shaping can be a big improvement applied to the dithering signal in order to make it less disturbing. Now if the recording was made using 96KHz, you can noise shape most of the dithering signal to frequencies higher than 24KHz, so nobody will hear them. The dithering noise is at the end of the recording finally filtered out, at the moment you downsample your project back to 44.1 KHz.</p>

<p>So, bottom lines:
Is it useful when recording analog stuff:</p>

<ul>
<li>Yes, definitely. You have less disturbance from the anti-imaging filter and less disturbance from the dithering signal when used with proper noise shaping.</li>
</ul>

<p>Is it useful when working with digital stuff that came right of my softsynth?</p>

<ul>
<li>Yes, still useful if you plan to work with 24 bits, and mastering it down to 16 bits. You can gain a great deal with noise shaping the dithering signal.</li>
</ul>
","233"
"What is the point of the ethernet plug: SoundCraft Si Compact 32?","1341","","<p>After searching the internet, I can't find out what the ethernet plug for the mixer is for. Can you use it to remote control the mixer from a computer? Or what is it for?</p>
","<p>You can use the port in V2 software to trigger HiQnet Venue Presets programmed via London Architect and System Architect; the iPad app to directly control the Si Compact is still in BETA test so it's not a 'product' on the web site but you can register for information at <a href=""http://www.soundcraft.com/apps/visi-remote.aspx"" rel=""nofollow"">http://www.soundcraft.com/apps/visi-remote.aspx</a>.</p>

<p>If you have further questions please use the Soundcraft web site Facebook pages or contact your distributor to make  enquiries since Soundcraft do not actively participate and monitor any forums. </p>

<p>Regards, <br/>
Soundcraft-Studer Product Management Team</p>
","3956"
"Motion and shape tracking in After Effects CS5.5","1334","","<p>My footage consists of someone with black sunglasses moving his head tilting slowly back and forth. The glass of the sunglasses is visible in the footage. In the beginning the head is tilted back, the glasses are turned up from the camera, so only a small portion of the glass is visible. The more the head tilts forward, the more the glass approaches being parallel to the camera.</p>

<p>I want to track the shape and movement of the glass. So the movement would be saved to the position/rotation path and the shape to a mask shape. I have tried this using After Effects, but I only managed to track the motion of the area movement.<br>
How do I track the changing shape of the glass as well?  I also have access to Mocha and Boujou.</p>
","<p>I found a tutorial which uses Mocha for After Effects to accomplish my task exactly. Here is the link to the tutorial:</p>

<p><a href=""http://www.imagineersystems.com/videos/mocha-ae-paste-mocha-shapes-to-after-effects-mask-channel/view"" rel=""nofollow"">http://www.imagineersystems.com/videos/mocha-ae-paste-mocha-shapes-to-after-effects-mask-channel/view</a></p>
","4389"
"What's the difference between an Engineer and a Producer?","1324","","<p>People seem to be making a point of the distinction of the role of Engineer and Producer. What's the difference?</p>
","<p>The engineer is the ""equipment operator"".  His expertise is choosing the right equipment for the job and using it in the right way to achieve the desired effect.  His role is analogous to that of the cameraman in film production.</p>

<p>The producer is kind of like a project manager.  His job is to get all the individuals - musicians, vocalists, engineers - to each do their part to achieve the overall goal.<br>
For example, he might decide a certain song should be quiet and soothing, so he'll advise the musicians to tone down the accompaniment, the vocalist to soften her voice, and the engineer to add the necessary processing to achieve that soothing sound he's after.</p>
","102"
"Does HD necessarily mean clear, high quality images?","1323","","<p>I recently purchase an <a href=""http://www.axis.com/products/cam_m5014/"" rel=""nofollow"">AXIS network camera</a> that will be used at a scrap metal yard to photograph truck license plates. While the camera is probably sufficient for my purposes I was a bit confused because it was advertised as an HDTV 720p camera but the image quality (still or video) is nowhere near as good as, say, any of the leading smartphone cameras. Apparently, this is because each image is only around 1 MP. If the image is aliased and blurry, in what sense is it ""high definition""? It has ""more lines"" than my old TV, yet it looks worse than any other digital camera I've owned in the last 10 years. Isn't this term a bit misleading in this context?  </p>
","<p>HD simply refers to a given number of pixels in the signal, not some measure of quality.  Even 1080p TV is lower resolution than the photos on most camera phones.  Even the 4k video used in the latest Hollywood blockbuster movies is lower quality than the still photos of most camera phones.  Video takes up far FAR more data than stills.  If you think about it, 1 second of video contains between 24 and 60 individual photos depending on the format of video being used.  Now there are ways to shrink the video since a lot of the pictures are very similar, but the quality that can be recorded is still much smaller than what we can process for a still.</p>

<p>The think is that an 8mp camera phone still doesn't shoot any better than 1080p video at most, so while it may take 8mp stills, it only takes sub 2mega pixel video.  720p isn't all that much better than an old 480 display.  The main advantage is the progressive scan, but there still isn't that much more information (about double).</p>

<p>The other thing to note with security cameras is that they tend to be made to be durable more than high quality.  The optics are probably not that great and they favor infinity focus over sharpness.  They want everything in the scene to be reasonably in-focus instead of having any particular area really sharp.  So between the optics, the design goals and the limited resolution, it isn't all that surprising that it doesn't render things that clearly since it isn't the goal of a typical security camera.</p>

<p>The term is not at all misleading though and based on your description, it sounds like it behaves like I would expect for the device.  It is High Definition since 720p is a high definition standard even if the optics don't give you as sharp of an image as you'd prefer.</p>
","7786"
"Stretch or Change Timing in After Effects","1317","","<p>I am new to After Effects. I downloaded the free file for the Month of May from VideoHive and have been playing with it. Some of the text is displayed just a bit too short for my linking. I do not want to change the look of the effects but take everything and make it a half second to one second longer. Basically take the length from 35 seconds to about 55-60 seconds without messing up any of the transitions or effects.  </p>

<p>Is this possible? </p>
","<p>The solution will be found in the Timeline. Scroll through the items in the timeline until you find the track for the text object you want to change. The duration of the text on screen is defined by starting and ending keyframes on the object's track.  </p>

<p>Moving the last keyframe on the object's timeline will extend the duration of an object, so clicking and dragging the last keyframe for the text will extend how long it is on screen. </p>
","7843"
"ffmpeg - moov atom not found0/0","1315","","<p>I'm trying to play a movie (<code>.mp4</code>) that i've encoded with <code>ffmpeg</code> and i get the following error:</p>

<blockquote>
  <p>[mov,mp4,m4a,3gp,3g2,mj2 @ 0x7fa39a802800] moov atom not found0/0<br>
  /movie.mp4: Invalid data found when processing input</p>
</blockquote>

<p>Why does ffmpeg throw this error?</p>

<p>Thanks!</p>

<p><strong>Note:</strong>
It's an <code>.exr</code> sequence that i've encoded, but i don't think it's a problem with my encoding settings since i've encoded another <code>.exr</code> sequence with the same settings and it worked. </p>

<p><strong>Command:</strong></p>

<pre><code>ffmpeg -thread_queue_size 512 -y -loglevel info -threads 0 -f lavfi -i aevalsrc=0 -framerate 60 -i /image.png -start_number 000000 -apply_trc bt709 -framerate 60 -i /sequence.%06d.exr -r 60 -preset medium -codec:v libx265 -ar 48000 -acodec aac -shortest -strict experimental -sn -vsync 1 -pix_fmt yuv420p -b:v 31457280 -movflags +faststart  -x265-params high-tier=0:pmode=1:wpp=1:tune=fastdecode:bitrate=31457:fps=60:keyint=360:min-keyint=180:vbv-bufsize=31457:vbv-maxrate=31457:scenecut=0 
' -filter_complex ""[2:v]crop=3440:2227:0:2560, scale=3440:768, rotate=0[input_num0];[1:v][input_num0]overlay=0:0[output_num0]"" -map ""[output_num0]:0"" -map ""0:0"" output.mp4 
</code></pre>

<p><strong>Console output:</strong></p>

<pre><code>ffmpeg version 3.0.1 Copyright (c) 2000-2016 the FFmpeg developers
  built with Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/3.0.1 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-libtheora --enable-libvorbis --enable-libvpx --enable-ffplay --enable-libspeex --enable-libfdk-aac --enable-libopus --enable-libx265 --enable-nonfree --enable-vda
  libavutil      55. 17.103 / 55. 17.103
  libavcodec     57. 24.102 / 57. 24.102
  libavformat    57. 25.100 / 57. 25.100
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 31.100 /  6. 31.100
  libavresample   3.  0.  0 /  3.  0.  0
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, lavfi, from 'aevalsrc=0':
  Duration: N/A, start: 0.000000, bitrate: 2822 kb/s
    Stream #0:0: Audio: pcm_f64le, 44100 Hz, mono, dbl, 2822 kb/s
Input #1, png_pipe, from '/image.png':
  Duration: N/A, bitrate: N/A
    Stream #1:0: Video: png, monob(pc), 3840x1536, 60 tbr, 60 tbn, 60 tbc
Input #2, image2, from '/sequence.%06d.exr':
  Duration: 00:00:32.27, start: 0.000000, bitrate: N/A
    Stream #2:0: Video: exr, rgb48le(unknown/unknown/bt709), 5120x2560 [SAR 1:1 DAR 2:1], 60 tbr, 60 tbn, 60 tbc
[libx265 @ 0x7f9ee1003400] Unknown option: tune.
x265 [info]: HEVC encoder version 1.9
x265 [info]: build info [Mac OS X][clang 7.0.2][64 bit] 8bit
x265 [info]: using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX AVX2 FMA3 LZCNT BMI2
x265 [warning]: Limit reference options 2 and 3 are not supported with pmode. Disabling limit reference
x265 [info]: Main profile, Level-5.1 (Main tier)
x265 [info]: Thread pool created using 4 threads
x265 [info]: frame threads / pool features       : 2 / wpp(24 rows)+pmode
x265 [info]: Coding QT: max CU size, min CU size : 64 / 8
x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra
x265 [info]: ME / range / subpel / merge         : hex / 57 / 2 / 2
x265 [info]: Keyframe min / max / scenecut       : 180 / 360 / 0
x265 [info]: Lookahead / bframes / badapt        : 20 / 4 / 2
x265 [info]: b-pyramid / weightp / weightb       : 1 / 1 / 0
x265 [info]: References / ref-limit  cu / depth  : 3 / 0 / 0
x265 [info]: AQ: mode / str / qg-size / cu-tree  : 1 / 1.0 / 32 / 1
x265 [info]: Rate Control / qCompress            : ABR-31457 kbps / 0.60
x265 [info]: VBV/HRD buffer / max-rate / init    : 31457 / 31457 / 0.900
x265 [info]: tools: rd=3 psy-rd=2.00 signhide tmvp strong-intra-smoothing
x265 [info]: tools: lslices=8 deblock sao
Output #0, mp4, to '/output.mp4':
    encoder         : Lavf57.25.100
    Stream #0:0: Video: hevc (libx265) ([35][0][0][0] / 0x0023), yuv420p, 3840x1536, q=2-31, 31457 kb/s, 60 fps, 15360 tbn, 60 tbc (default)
    Metadata:
      encoder         : Lavc57.24.102 libx265
    Stream #0:1: Audio: aac (LC) ([64][0][0][0] / 0x0040), 48000 Hz, mono, fltp, 69 kb/s
    Metadata:
      encoder         : Lavc57.24.102 aac
Stream mapping:
  Stream #1:0 (png) -&gt; overlay:main (graph 0)
  Stream #2:0 (exr) -&gt; crop (graph 0)
  overlay (graph 0) -&gt; Stream #0:0 (libx265)
  Stream #0:0 -&gt; #0:1 (pcm_f64le (native) -&gt; aac (native))
Press [q] to stop, [?] for help
[image2 @ 0x7f9ee081f400] Thread message queue blocking; consider raising the thread_queue_size option (current value: 8)
</code></pre>
","<p>Try</p>

<pre><code>ffmpeg -y -loglevel info -threads 0 -f lavfi -i aevalsrc=0 -framerate 60 -loop 1 -i /image.png -start_number 000000 -apply_trc bt709 -framerate 60 -thread_queue_size 512 -i /sequence.%06d.exr -filter_complex ""[2:v]crop=3440:2227:0:0,scale=3440:768,rotate=0[input_num0];[1:v][input_num0]overlay=shortest=1,format=yuv420p[output_num0]"" -preset medium -c:v libx265 -c:a aac -ar 48000 -shortest -b:v 31457280 -x265-params high-tier=0:pmode=1:wpp=1:tune=fastdecode:bitrate=31457:fps=60:keyint=360:min-keyint=180:vbv-bufsize=31457:vbv-maxrate=31457:scenecut=0 -map ""[output_num0]"" -map 0:0 -movflags +faststart output.mp4 
</code></pre>

<p>There are still some redundant settings, but those can be checked later on.</p>
","19076"
"How to mask footage video with text?","1313","","<p>I want to make a footage video ""shine"" through a text, so that the letters serve as a mask and only what's inside from the footage will shine through. Also, 3D Camera movement should be possible afterwards.</p>

<p>How could I do this? I know I can create masks from text, and then copy that letter masks on my footage clip. But that will only work for 2D. How could I achieve the same for 3D?</p>
","<p>Thats rather simple if I understand correctly what you want to achieve. One way to do it is to right click your text layer and choose the ""Create Masks from Text"" option. Then pre-comp your video layer and copy the masks your just created (AE creates a new solid that contains the masks) onto the video comp layer. Now make the video comp layer 3D and you can animate your camera however you want in your master comp and also go into your video comp and animate the camera there as well for some parallax effect.</p>
","13360"
"How to do a ""deep"" / full copy of a nested sequence in Premiere Pro CS6? (i.e. constituent components are all copied)","1312","","<p>I created a title sequence: title, with glow/fade in then fade out and made this into a nested sequence. I wanted to reuse this nested sequence in other parts of my project, but use different text content for the title in each of the other instances. </p>

<p>So, I copied the nested sequence several times. But when I changed the title to different text, this affected all of the nested sequence copies. So it seems that the copy creates a new 'container' but references the same components (e.g. title) as the original.</p>

<p>So I'm looking for a way to entirely copy a nested sequence and make duplicates of all the consituent components. Is there a way?</p>

<p>By the way, there seem to be three different ways to make a copy: from the contextual pop-up menu for a sequence there is 'Copy', 'Duplicate' and another method (name escapes me at the moment). I've tried all three and none of them solve my problem. I'm hoping that there is a check box somewhere that I can tick/enable to say 'copy all components'.</p>
","<p>Duplicate both your Title and your sequence, call one sequence V1...and the other V2, then take your title and call it Title V1 and title V2...then replace the title in sequence 2 with V2, then from that point on any changes made within the V1 side will not affect V2 and vise versa.</p>
","5777"
"How does one edit and save clips once and reuse in multiple videos with Adobe Premiere?","1308","","<p>I have many clips which all need basic editing such as adjusting the start and end. I would also like to reuse these same clips in multiple videos without setting the start and end again. I would like a portfolio of these basic edits once and simply use them as is. Of course inside premiere i would edit them again if necessary and so on without affecting the original settings. Is this possible if so how ?</p>
","<p>Within a new project, create a sequence/timeline; call it 'Basic Edits' if you like, and make your trim edits on the source clips there. For each of your multiple videos, create a new sequence and copy-paste the trimmed clips from the first sequence.</p>

<p>Or if you want to use these clips in another project, you can import the sequence by the File -> Import menu dialog.</p>

<p>Of course, I presume that by Adobe Premiere, you don't mean Premiere Elements - the lite version - which doesn't feature multiple sequences in a project, as far as I know.</p>
","4472"
"Native SDI-output vs HDMI-SDI adapter for live stream","1305","","<p>I currently run live streams of amateur sporting events, mainly inside. Currently I am using several <code>Panasonic HS-900k</code>s connected to a PC desktop via RCA cables and several <code>Easycap DC60+</code>s (USB 2.0). As you can imagine, the quality from the Panasonic's gets degraded very quickly. Some of the RCA cables are carrying video over 100ft from source to destination.</p>

<p>I am looking to upgrade the connections to SDI via the <code>Blackmagic Decklink</code> products. That said, I will need cameras that support SDI output, or purchase an HDMI-SDI adapter (~$300). </p>

<p>My budget is approx. $3000 for a single camera, and since I'll only be outputting the stream at 720p max for the foreseeable future (don't want to overburden end-users bandwidth), I really don't need a fancier camera then that.</p>

<p><strong>Obviously there is a big price difference between a new camera and a HDMI-SDI adapter. Is there a big quality difference, or does HDMI-SDI not really lose much quality?</strong> Will it provide the same signal as a new camera such as a <code>Panasonic AG-HPX170 P2HD</code> that provides native SDI-output? I am not looking to output audio from these cameras, just video.</p>
","<p>For amateur events, SDI may be overkill compared to using simple Component.  The key for long runs is to separate the various components so that they don't degrade.  There is a similar question <a href=""https://video.stackexchange.com/questions/7218/how-to-connect-a-camera-to-a-computer-100ft-away/7220#7220"">here</a> about long distance runs. </p>

<p>As for HDMI to SDI, there should be no quality loss compared to HDMI as both are digital formats supporting full uncompressed HD streams.  The quality of the converter may make a difference, but there is no technical reason that you couldn't have a flawless conversion between formats.  Both support both YCbCr 4:2:2 and 4:4:4, so the color spaces should be compatible either way.</p>
","7308"
"Can I run this in one command line (ffmpeg)","1302","","<p>I used this command <code>ffmpeg -i thevideo.mp4 -c:a copy -vf 'drawbox= : x=0 : y=0 : color=invert:enable=between(t\,11\,39)' output.mp4</code> but it runs very slow.</p>

<p>So, I took another approach, I copy first and last part and run ""drawbox"" only on selected length, and then adding them all together.</p>

<pre><code>ffmpeg -i thevideo.mp4 -ss 00:00:00 -to 00:00:10 -c copy cut.mp4

ffmpeg -i thevideo.mp4 -ss 00:00:11 -to 00:00:39 -c:a copy -vf 'drawbox= : x=0 : y=0 : color=invert' cut2.mp4

ffmpeg -i thevideo.mp4 -ss 00:00:40 -c copy cut3.mp4

ffmpeg -f concat -i mylist.txt -c copy output.mp4
</code></pre>

<p>content of <code>mylist.txt:</code></p>

<pre><code>file 'cut1.mp4'
file 'cut2.mp4'
file 'cut3.mp4'
</code></pre>

<p>Can I run it in one command and have the same speed or even faster but without cutting and concatenating the parts of the video?</p>
","<p>Your current method using the <a href=""https://ffmpeg.org/ffmpeg-formats.html#concat-1"" rel=""noreferrer"">concat demuxer</a> while <a href=""https://ffmpeg.org/ffmpeg.html#Stream-copy"" rel=""noreferrer"">stream copying</a> is probably the fastest (I'm just counting the <code>ffmpeg</code> time and not any time you take to enter the additional commands). Unfortunately, you have to perform several steps, but the advantage is that your non-filtered segments are not being re-encoded which preserves quality and results in fast processing.</p>

<p>It is possible to do it all in one command with the <a href=""https://ffmpeg.org/ffmpeg-filters.html#trim"" rel=""noreferrer""><code>trim</code></a>, <a href=""https://ffmpeg.org/ffmpeg-filters.html#atrim"" rel=""noreferrer""><code>atrim</code></a>, and <a href=""https://ffmpeg.org/ffmpeg-filters.html#concat"" rel=""noreferrer""><code>concat</code></a> filters, but this will re-encode everything and possibly be slower. You will have to test. However, it may create a more ""stable"" output if you find your current method to be problematic (which would likely be fairly obvious upon playback). The filter is generally used instead of the demuxer when all streams are being filtered.</p>

<h3>Example using <code>concat</code> filter:</h3>

<pre><code>ffmpeg -i input.mp4 -filter_complex \
""[0:v]trim=end=10[v0]; \
 [0:a]atrim=end=10[a0]; \
 [0:v]trim=10:40,drawbox=color=invert[v1]; \
 [0:a]atrim=10:40[a1]; \
 [0:v]trim=start=40[v2]; \
 [0:a]atrim=start=40[a2]; \
 [v0][a0][v1][a1][v2][a2]concat=n=3:v=1:a=1[v][a]"" \
-map ""[v]"" -map ""[a]"" -movflags +faststart output.mp4
</code></pre>

<h3>Notes</h3>

<ul>
<li><p>If you get a weird output refer to the <a href=""https://ffmpeg.org/ffmpeg-filters.html#setpts"" rel=""noreferrer""><code>setpts, asetpts</code></a> filters (search for <code>PTS-STARTPTS</code>).</p></li>
<li><p>If it is still too slow use a faster x264 preset. See <a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"" rel=""noreferrer"">FFmpeg Wiki: H.264 Video Encoding Guide</a>.</p></li>
<li><p>I assume you're using OS X with the ""tessus"" build of <code>ffmpeg</code> from your previous FFmpeg question. The default AAC encoder for the tessus build is <code>libvo_aacenc</code>. The native FFmpeg AAC encoder is generally regarded as being slightly better. If you want to use it add <code>-c:a aac -strict experimental</code>. If you want the best supported AAC encoder, <code>libfdk_aac</code>, then use <a href=""https://trac.ffmpeg.org/wiki/CompilationGuide/MacOSX"" rel=""noreferrer"">homebrew to compile <code>ffmpeg</code></a>. See <a href=""https://trac.ffmpeg.org/wiki/Encode/AAC"" rel=""noreferrer"">FFmpeg Wiki: AAC Encoding Guide</a>.</p></li>
</ul>
","15479"
"How to import youtube video into premiere cs4","1297","","<p>I installed adobe premiere cs4 and was really astonished by the fact that it's not possible to import a video downloaded from YouTube, but it is needed to convert it firstly. This application is weight about 3GB on my hard disk and now I must search for a plugin to import a video from YT.</p>

<p>It would be perfectly if i could download a video directly into premiere, and also upload finished file up to youtube.</p>

<p>The only result of my search is a paid plugin named moyea-importer - USD 70<br>
Is there any free plugin for this task. Or any similar solution.<br>
If not - in which format should I download youtube video, in which format should I convert it (what is the best converter for that) to make it editable in premiere.<br>
Working on win xp sp3.<br>
Thanks.</p>
","<p>It's not really all that astonishing.  Youtube is a streaming media service and Premiere is a video editing suite designed to work with files on your local computer.  It isn't part of the function of Premiere to deal with streaming media or convert from a stream to a file or to stream content (though it can produce stream ready files).</p>

<p>To use a video from Youtube, you would first need to use a Youtube stream ripper that will save it out as an actual video file which Premiere should then have no problem opening.  When you finish your project, you would then export to a Youtube compatible format and upload the file to Youtube.  Adobe Media Encoder (which can be used for doing the rendering out of Premiere) actually has presets specifically for Youtube.</p>
","8307"
"Saving disk space, but not quality","1297","","<p>I'm pretty new at this...  I captured my analog 8mm video tapes in .avi, planning to edit them with Premier.  But I need to make space in my external hard drive.  I have the Adobe Encoder, but I'm afraid I will loose quality in my videos if I use it!  Also, I see it will change my captured videos from .avi to .mp4.
My idea is to keep these external hard drives as a ""master"".<br>
Should I use the Encoder to save space, or to get extra hard drives and leave it that way?
Thanks a lot!!!</p>
","<p>There isn't a perfect answer to this problem.  AVI and MP4 are just container's for video streams, so without knowing more about the actual streams in the containers, it is impossible to tell how much quality loss there would be, but as a general rule of thumb, it isn't all that atypical for AVI to use far less efficient video compression algorithms than MP4 on average.  That doesn't always hold true, but it often is.</p>

<p>That said, what you plan to do with the files also matters.  If these are a master intended for being a high quality copy you can come back and WATCH, then running a high quality h.264 encoding isn't such a bad thing.  You should likely be able to find a setting that will make the files quite a bit smaller and still be pretty high quality to watch.</p>

<p>The problem with this, however, is that while the compression may work well for watching the video, it doesn't work so well for editing and re-encoding the video later.  If you compress too much, the video will look great to a viewer now, but may look like crap after editing and re-encoding a final video later.  If you plan to edit the videos later, you will need to stick to a much higher file size and higher quality encoding.  This could still be h.264, or any of a number of other formats, but it would need to be a much larger file (such as all I frame h.264).</p>

<p>Personally, I've always elected to keep my masters at their highest available capture resolution and fairly minimally compressed, but I also am in a financial situation where buying large amounts of external storage is not a problem.  (I personally have around 20 TB worth of storage, of which about 14TB is full, though some of that is providing redundancy.  That's primarily for video projects, photo projects and Steam downloads.)</p>

<p>Ultimately, however, it is a personal decision about the trade offs between cost and quality.  Every option has it's costs and only you can decide which costs are best for you.  I'd encourage you to buy more storage if you want to do editing with the videos later, but if you are only worried about playback, I'd recommend encoding it down to a quality that you find high enough by trial and error.  (Also, be sure to check the videos as the level of activity in a video will greatly impact the video quality unless you are using a quality based encoder.)</p>
","12983"
"Adobe After Effects Frame-By-Frame Editing","1286","","<p>I am currently editing a clip in After Effects. We had a very limited supply of green screen so I am now having to edit some of the white wall too. The thing is, the actors move around, not allowing me to clip out these parts.</p>

<p>I was wondering if there was a thing in After Effects that allowed me to break up the clips so that I can work around the actors in certain time intervals and get the white wall to be transparent.</p>

<p>Thanks!</p>
","<p>I hope I understand this correctly, are you saying that you have actors in front of a green screen and sometimes they are off the screen? If you want to chroma key the green screen to be another surface you have to have it consistent all the way through. Green screen is hard enough to get right. (Green or blue screen backgrounds have to be evenly lit with a flat diffused light, non-reflective, and no shadows, otherwise the green screen will look blotchy,
and have noise areas that you can't just tweak out.)</p>

<p>Trying to mask out where your actors fall out of the green screen frame does not have a solution in it. If you didn't get this right in production you won't be able to fix this in post. At least not fix it in post with anything that looks close to natural. I highly recommend you shoot this over.</p>
","3218"
"In Premiere, is there an easy to way to extract sequences into new projects?","1284","","<p>I started a Premiere project that eventually spawned multiple videos.  Each of those videos is in its own sequence.  I'd like to extract them into their own projects, both to clear out the project manager and leave it more organized, but also so I can ""Clean"" the Media Cache (I don't expect to re-open these sub-projects anytime soon).</p>

<p>Is there an easy way to move the sequences (and associated assets) into new projects?</p>
","<p>There are two ways to do this; since they both have their merits, I'll include both of them.</p>

<h2>1. The Project Manager</h2>

<p>Open <strong>File</strong>  <strong>Project Manager</strong>. It's pretty straightforward, just select the sequences you want to export. If you want to clean up the preview files, uncheck <strong>Include Preview Files</strong>. Select the target directory and click ok. Repeat this for every sequence you want to export to an individual project. The problem with that approach is that <strong>this will also copy your media assets</strong> to the new project's directory, which you might not want.</p>

<h2>2. Importing sequences</h2>

<p>Another option is to import sequences from other Premiere project.</p>

<ol>
<li>Create a <strong>new project</strong> in Premiere.</li>
<li>Select <strong>File</strong>  <strong>Import...</strong>. In the pop-up window, select your old project file that has all the sequences in it.</li>
<li>Another dialogue window will pop up asking you whether you want to import the entire project or individual sequences. Select <strong>Import Selected Sequences</strong>.</li>
<li>This will open another window where you can browse your old project and <strong>select the sequence(s) you want</strong> (to select multiple sequences, hold <kbd>CTRL</kbd> / <kbd>CMD</kbd> while selecting them).</li>
</ol>

<p>This will <strong>not copy or move your media assets to a new directory</strong>. Instead it will just import the selected sequence(s) and all media assets it contains into your new project. This however will not import any assets that aren't used in your imported sequence(s). So if you're still working on one of the imported sequences in the new project and want to add clips you had already imported in the old project but hadn't used yet, you'll have to import those manually. To prevent that, import all media assets that are relevant to the sequence into your new project <em>before</em> importing the sequence and make sure <strong>Allow importing duplicate media</strong> is unchecked in the import dialogue.</p>
","16606"
"Wirecast recording lagging from a video file source","1281","","<p>I have a recorded video in HD format with the source file being about 2GB for 30 minutes of footage. I added it as a new Video Shot in WireCast intending to edit it, however, I got two problems. First, the quality drop tremendously regardless of what kind of output options I use and second, the output video is lagging and that is of course recorded in the final version.</p>

<p>I first tried to compress the video to reduce its size and I got it down to about 300MB but the results where exactly the same.</p>

<p>I then tried to stream the video from another computer on the same network using Desktop Presenter to reduce the load on the one where Wirecast was actually working, however, I got exactly the same results, if not worse.</p>

<p>Any tips on how I can deal with this issue?</p>
","<p>It sounds like your input is too compressed.  Your current quality level is already below what Netflix uses for their full quality HD streaming and is right around Youtube's recommended settings for final output of video.</p>

<p>The problem is, this is already a very highly compressed level.  Both compressing and decompressing an h.264 file take a lot of processing power.  The more compressed the video, the more processing power it takes.</p>

<p>Since the file starts highly compressed, your CPU has to work hard to decode the video and then has to try to re-compress it for the stream.  If it can't keep up with the frame rate, then you start getting quality drop and lag as it does it's best to keep up with the stream.</p>

<p>Further compressing the source video just amplified this problem, though if you have already pre-recorded the video, why not edit it offline and then export the video so that you can stream it without having to re-encode on the fly?</p>

<p>Wirecast isn't designed for editing, it is designed for streaming.</p>
","10403"
"Export high bitrate video in lightworks","1278","","<p>My goal is to get a high bitrate video to a friend of mine, so he can then use the video and edit it for another use.</p>

<p>I am to the point where i need to export the video.</p>

<p>What export option is going to keep the best quality and bitrate but still be under 2 gigs? My video is currently 12 minutes long, but i can shorten it into 4 minute sections.</p>

<p>So, what is the best export option to keep a high bitrate and quality and get 4 minutes under 2 gigs. 1080p @ 60fps.</p>

<p>Thanks</p>

<hr>

<p>Screenshots of lightworks pro export options. It is not all of them, but let me know if you would like to see more of the options.</p>

<p>Picture of the format options:</p>

<p><img src=""https://i.stack.imgur.com/P7utB.png"" alt=""enter image description here""></p>

<hr>

<p>AVI</p>

<p><img src=""https://i.stack.imgur.com/IPXdF.png"" alt=""enter image description here""></p>

<hr>

<p>Image Sequence</p>

<p><img src=""https://i.stack.imgur.com/nkf2f.png"" alt=""enter image description here""></p>

<hr>

<p>MOV</p>

<p><img src=""https://i.stack.imgur.com/pZPnc.png"" alt=""enter image description here""></p>

<hr>

<p>MXF</p>

<p><img src=""https://i.stack.imgur.com/phgvA.png"" alt=""enter image description here""></p>

<hr>

<p>WAV</p>

<p><img src=""https://i.stack.imgur.com/CVApn.png"" alt=""enter image description here""></p>
","<p>Depending on your definition of high quality, this is impossible.  For comparison sake, DV video (at standard definition) takes a gigabyte per 4.7 minutes, which is at a good, high quality level of compression.  You are trying to do 60fps 1080p video and get it under 2GB for 4 minutes.  That's over 12 times the information and you are trying to fit it in just over twice the space.</p>

<p>If you want to see how it works out when you use the highest possible quality that will fit your size requirements, you can simply calculate the data rate. Create a 2-pass VBR encoding with an average data rate determined by the size and time.  The file size is equal to the data rate times the number of seconds in the video.  2 pass VBR will ensure that the best possible use is made of that space.</p>

<p>You mention that you can break it up in to 4 minute sections to send over.  Why is it that you can't split the files (using file tools rather than video tools) and have them reassembled on the other side?  Is it a file size limitation or a file transfer limitation?  Also, is there a reason you can't send the original files along with the project files?  Each additional time that you encode, there is a slight bit of quality loss, so it is preferable to work from the original assets whenever possible.</p>

<p>Another possibility if you are working collaboratively and have issues with moving a huge amount of data.  It is possible to output low quality copies of the original video files and substitute those files for the high quality versions when working on them remotely.  Your friend would take your project files and the low quality assets, do any editing he wants, and then send the project files back to you.  You could then render the final output based on your friend's editing decisions but using the original, full quality assets.</p>

<p>The reason you don't want to go to a smaller file size in-between is that each time you encode a video, quality is lost.  The lower quality the source you are working from, the more is lost in each additional encoding, so if you move to a mid-range format early on, then the final output will suffer.  If you run the render from the originals however, you get maximum quality.</p>
","10148"
"MKVToolNix equivalent for .MP4's","1276","","<p>Good day! I'm looking for <a href=""https://mkvtoolnix.download/downloads.html"" rel=""nofollow noreferrer"">MKVToolNix</a> equivalent software for .MP4's. I will use it in a video file without subtitles, I'll find it's subtitle and merge it with the video file just like what I did in MKVToolNix. Thanks!</p>
","<p>Muxing a file simply means to extract individual streams out of existing container files or mix existing streams together into a new container file. For MP4, this can be done with ffmpeg and the <code>-c copy</code> switch. If you don't want to write command line inputs, <a href=""http://www.xmedia-recode.de/en/funktion.html"" rel=""nofollow noreferrer"">XMedia Recode</a> is a good, free GUI for ffmpeg. There's a documentation on the website, simply select <em>Direct Stream Copy</em> to avoid reencoding the individual streams.</p>
","20938"
"What happens when I upload a multi-audio video to YouTube?","1270","","<p>I livestream video game tournaments. The videos I record have two audio tracks. The first track is a low-volume game sound, combined with commentary audio. The second track is just the normalized volume game sound.</p>

<p>When I upload this video to YouTube, what is going to happen? Will YouTube combine both audio tracks into a single track? Will it throw out track 2?</p>

<p>Has anyone done this before?</p>
","<p>Youtube will encode the primary or default audio stream.</p>

<p>Youtube doesn't support multiple audio <a href=""https://productforums.google.com/forum/?hl=en#!topic/youtube/OtzjKBksQAo;context-place=topicsearchin/youtube/multiple$20audio"" rel=""nofollow"">streams</a>. Neither does <a href=""https://vimeo.com/forums/topic:107372"" rel=""nofollow"">Vimeo</a>.</p>

<p>Although, apparently, some videos like the 2012 Olympic streams did have it. Special handling by Youtube, I'm sure.</p>
","16898"
"ffprobe - why total bitrate is greater than sum of video and audio bitrates?","1269","","<p>I have been working on video conversion. So, to find out video parameters I used ffprobe. However, when I run the command on an mp4 video file, I observe this:</p>

<p><img src=""https://i.stack.imgur.com/GHt7s.jpg"" alt=""enter image description here""> </p>

<p>Total bitrate: 3547 Kb/s 
Video bitrate: 3512 Kb/s
Audio bitrate: 32 Kb/s</p>

<p>Why total bitrate is 3 Kb/s more than the sum of audio and video bitrates?</p>
","<p>There are a few possibilities.  MP4 is a container format and can contain more than just audio and video streams.  While not all necessarily supported by MP4 specifically, container formats can contain things like sub-titles, sync data, chapter markers, meta-data, etc.  They can also potentially contain more than one audio or video stream.</p>

<p>From the level of detail provided, it isn't really possible to tell exactly what is taking the extra space, it may just be overhead of the format itself or it could be some additional data stream that is stored within the container.</p>
","11958"
"Why does VLC player convert a 4-minute mp4 to a 4-hour webm?","1269","","<p>I used standard webm settings in VLC player 2.2.1 (the latest) to convert a 4-minute mp4 made in Handbrake. In the conversion panel it says ""Streaming 04:00"" but the end result is ""03:49:15"" and, on two further attempts, ""03:56:28"" and ""04:04:23"". The file size is almost the same as the mp4 (4MB).</p>

<p>The length of the webm video - i.e. more than 4 hours - is confirmed by MediaInfo and by Firefox native player, videojs player, and VLC player. In VLC player, I tried clicking on the progress bar to explore exactly what takes up 4 hours, but on-click it closes the video. In Firefox, the same test returns the video to frame 1.</p>

<p><strong>Update (30 August)</strong></p>

<p>I did a test on this ""generic mp4"": <a href=""http://download.wavetlan.com/SVV/Media/HTTP/H264/Talkinghead_Media/H264_test1_Talkinghead_mp4_480x360.mp4"" rel=""nofollow"">http://download.wavetlan.com/SVV/Media/HTTP/H264/Talkinghead_Media/H264_test1_Talkinghead_mp4_480x360.mp4</a>. Do not know if it was done with Handbrake. Then converted it to webm, using default settings, with VLC 2.2.1. </p>

<p>Results: The mp4 is 00:00:14s, the webm is 22:40:00s. But the webm is not <em>really</em> 22 hours long, it plays for 14s just like the mp4. However, in a html5 player, Firefox and Chrome native players and VLC player, the progress bar gives the length as 22:40:00. </p>
","<p>Yes, it seems to be a problem of generated timecode. See <a href=""https://trac.videolan.org/vlc/ticket/12713#no1"" rel=""nofollow noreferrer"">https://trac.videolan.org/vlc/ticket/12713#no1</a></p>

<p>Switch to <a href=""http://ffmpeg.zeranoe.com/builds/"" rel=""nofollow noreferrer"">ffmpeg</a> (with a GUI like <a href=""http://www.avanti.arrozcru.org/"" rel=""nofollow noreferrer"">Avanti</a>) to avoid this issue. My <a href=""https://video.stackexchange.com/questions/15974/how-can-i-reduce-the-size-of-a-webm-to-2mb/"">answer</a> to an earlier webm question may guide you on settings.</p>
","16282"
"convert KAZAM video file to a file, playable in windows media player","1261","","<p>I have some <code>mp4</code> video files that are created by <a href=""https://apps.ubuntu.com/cat/applications/kazam/"" rel=""nofollow noreferrer"">KAZAM</a> in <code>Linux</code>. But I need to play those files on <code>Microsoft Windows</code> only by <code>Windows Media Player</code> (for example I can not install <a href=""http://www.videolan.org/vlc/index.en_GB.html"" rel=""nofollow noreferrer"">VLC media player</a>). Also I can not use any codec. So I think <code>converting</code> propoerties of file (for example format of files) is the only solution. But I do not know which output format is best for video file </p>

<ul>
<li>without losing quality </li>
<li>ability of playing result in <code>Windows Media Player</code></li>
</ul>

<p>Also I want to know how I can convert to that format?</p>

<p>Note:</p>

<ul>
<li>I prefer to use <code>VLC media player</code> for converting video files, but <code>ffmpeg</code> is also appreciable.</li>
<li>Also do you know a simple, light software for <code>screen casting</code> and recording sounds from speaker in <code>Linux-Ubuntu 14.04</code> which it's output is playable directly in <code>Windows Media Player</code>? I can replace <code>KAZAM</code> by this.</li>
</ul>

<p>Edit:</p>

<p>I tested output by <code>WMP 12 (for windows 7)</code> and also <a href=""https://mediaarea.net/en/MediaInfo/Download/Ubuntu"" rel=""nofollow noreferrer"">MediaInfo</a> says:</p>

<pre><code>General
Complete name                            : /path/to/file/Screencast 2016-12-17 07:19:13.mp4
Format                                   : MPEG-4
Format profile                           : Base Media / Version 2
Codec ID                                 : mp42
File size                                : 314 KiB
Duration                                 : 3s 280ms
Overall bit rate                         : 783 Kbps
Encoded date                             : UTC 2016-12-17 03:48:54
Tagged date                              : UTC 2016-12-17 03:48:54
Writing application                      : x264

Video
ID                                       : 1
Format                                   : AVC
Format/Info                              : Advanced Video Codec
Format profile                           : High 4:4:4 Predictive@L3.2
Format settings, CABAC                   : No
Format settings, ReFrames                : 1 frame
Codec ID                                 : avc1
Codec ID/Info                            : Advanced Video Coding
Duration                                 : 3s 280ms
Bit rate                                 : 744 Kbps
Width                                    : 1 366 pixels
Height                                   : 768 pixels
Display aspect ratio                     : 16:9
Frame rate mode                          : Constant
Frame rate                               : 25.000 fps
Color space                              : YUV
Chroma subsampling                       : 4:4:4
Bit depth                                : 8 bits
Scan type                                : Progressive
Bits/(Pixel*Frame)                       : 0.028
Stream size                              : 298 KiB (95%)
Writing library                          : x264 core 142 r2491 24e4fed
Encoding settings                        : cabac=0 / ref=1 / deblock=0:0:0 / analyse=0:0 / me=dia / subme=0 / psy=1 / psy_rd=1.00:0.00 / mixed_ref=0 / me_range=16 / chroma_me=1 / trellis=0 / 8x8dct=0 / cqm=0 / deadzone=21,11 / fast_pskip=1 / chroma_qp_offset=6 / threads=4 / lookahead_threads=1 / sliced_threads=0 / nr=0 / decimate=1 / interlaced=0 / bluray_compat=0 / constrained_intra=0 / bframes=0 / weightp=0 / keyint=250 / keyint_min=25 / scenecut=0 / intra_refresh=0 / rc=cqp / mbtree=0 / qp=15 / ip_ratio=1.40 / aq=0
Language                                 : English
Encoded date                             : UTC 2016-12-17 03:48:54
Tagged date                              : UTC 2016-12-17 03:48:54

Audio
ID                                       : 2
Format                                   : MPEG Audio
Format version                           : Version 1
Format profile                           : Layer 3
Codec ID                                 : 6B
Duration                                 : 3s 186ms
Bit rate mode                            : Constant
Bit rate                                 : 32.0 Kbps
Channel(s)                               : 1 channel
Sampling rate                            : 44.1 KHz
Compression mode                         : Lossy
Stream size                              : 12.4 KiB (4%)
Writing library                          : LAME3.99.5
Language                                 : English
Encoded date                             : UTC 2016-12-17 03:48:54
Tagged date                              : UTC 2016-12-17 03:48:54
</code></pre>
","<p>Your KAZAM video features YUV444P pixel format which WMP may not support without extra filters. Using ffmpeg, run</p>

<pre><code>ffmpeg -i in.mp4 -pix_fmt yuv420p -c:a copy -movflags +faststart out.mp4
</code></pre>

<hr>

<p>You can also use ffmpeg itself to capture screen and sound on linux. Basic syntax would be</p>

<pre><code>ffmpeg -f v4l2 -i VIDEO -f alsa -i AUDIO -pix_fmt yuv420p -b:a 64k cap.mp4
</code></pre>

<p>where VIDEO is the name of the video device. See <a href=""http://ffmpeg.org/ffmpeg-devices.html#video4linux2_002c-v4l2"" rel=""nofollow noreferrer"">http://ffmpeg.org/ffmpeg-devices.html#video4linux2_002c-v4l2</a> for details.</p>

<p>AUDIO is the name of the audio device. See <a href=""http://ffmpeg.org/ffmpeg-devices.html#alsa"" rel=""nofollow noreferrer"">http://ffmpeg.org/ffmpeg-devices.html#alsa</a></p>

<p>You can also use x11grab to capture from the X-Window system. See <a href=""http://ffmpeg.org/ffmpeg-devices.html#x11grab"" rel=""nofollow noreferrer"">http://ffmpeg.org/ffmpeg-devices.html#x11grab</a></p>
","20164"
"How are video and audio streams placed in 'container' files and what interrelations do they have?","1255","","<p>So far I have been guessing my way through the types of audio/video streams and getting them mixed up with containers. For example, I have had a friend tell me that a video file which won't play on my TV will work if I can change the file extension to <code>.avi</code>/<code>.mpg</code> for example - which worked. However, when I have had the problem since, I have tried the same but with no luck.</p>

<p>Could someone explain which are containers and how you can have different types of audio/video files/steams within a container file?</p>
","<p>The basics of that are actually not that complicated (while there is definitely more to know about that topic than would fit in this answer).</p>

<p>Take a look at that graphic you can find on the <a href=""http://w3.org"" rel=""nofollow noreferrer"">w3c-website</a>:</p>

<p><img src=""https://i.stack.imgur.com/tAdMw.png"" alt=""Containers and file formats graphic""></p>

<p>As you can see, the container is exactly that: a container that can contain different types of media, most notably video and audio, but also captions and metadata about the file. When you speak about a ""file format"", you're normally talking about the container format, after which the file extension for normal video files is named. This is why you've probably seen many video files with file extensions such as <code>.avi</code>, <code>.mp4</code>, <code>.mkv</code> and the likes.</p>

<p>When you look at the contents of this container, you will normally have at least one video and one audio track, those are referred to as ""streams"". Streams can be encoded using different codecs. A codec is, in layman's terms, an algorithm (or rather a set of algorithms) which is used to compress a (video or audio) stream. The more efficient the codec, the better the ratio of (video/audio) quality to file size. Two very common codecs are the H.264 (MPEG-4/AVC) codec for video and the AAC codec for audio. Those are popular because they are very efficient, thus you can have very small files with (relatively) good quality.</p>

<p>So to sum this up, the container is some sort of ""wrapper"" that contains the actual media streams. Different container formats allow for different kinds of streams. For example, the <code>.avi</code>-container is very limited, you can only have one audio stream and one video stream. The <a href=""http://www.matroska.org/"" rel=""nofollow noreferrer"">matroska-container</a> (<code>.mkv</code> files) is a good alternative if you need more than that. Matroska is a very flexible container that allows for multiple audio streams (e.g. for different languages), subtitles and more. However, many devices (e.g. most TVs with a USB port) don't support that standard. This is where the <code>.mp4</code>-container (MP4 being the standard-container for the <a href=""https://de.wikipedia.org/wiki/MPEG-4"" rel=""nofollow noreferrer"">MPEG-4-standard</a>, which is a huge topic on it's own) comes in handy, because nearly all devices can playback files that use this container format along with the H264 &amp; AAC codecs. This combination is the most common approach for easily interchangable video files, which for example is used by most video hosting platforms such as Youtube.</p>

<p>Changing the file extension is generally a stupid idea. If you change an <code>.avi</code> file to something like <code>.mp4</code>, it might work on your TV in some instances, but that is just because your TV is stupid (i.e. it knows the codecs of the streams, but doesn't even try to play them if it doesn't recognise the file extension or something like that). Container format's don't behave like text-files, you can't just change the extension and expect the container format to somehow magically change as well. If you can't open a specific video file, use the <a href=""http://www.videolan.org/vlc/"" rel=""nofollow noreferrer"">VLC video player</a>, it supports most commonly used file formats and codecs. If you want/need to watch it on a specific device such as your TV which doesn't support the codec, you can re-encode it (not a good idea, because that means quality loss) or remux it, i.e. take the individual streams and put them in different containers (which is a bit more complicated but yields better results).</p>
","15824"
"How to convert a 3D movie to 2D using ffmpeg","1250","","<p>I am looking for a way to convert a 3d movie to a 2d movie.</p>

<p>I found a reference on this forum. 
<a href=""http://www.ffmpeg-archive.org/How-to-re-encode-3d-video-to-2d-video-td4676271.html"" rel=""nofollow noreferrer"">http://www.ffmpeg-archive.org/How-to-re-encode-3d-video-to-2d-video-td4676271.html</a></p>
","<p>While you might be able to achieve this by manually using ffmpeg's <a href=""https://ffmpeg.org/ffmpeg-filters.html#crop"" rel=""noreferrer"">crop</a> filter, the <a href=""https://ffmpeg.org/ffmpeg-filters.html#stereo3d"" rel=""noreferrer"">stereo3d filter</a> is designed specifically for this purpose. You didn't specify the exact 3d format you have. There are several possibilities as documented at the the stereo3d link above. Assuming that <code>input.mkv</code> is the relatively common sbsl (side-by-side left eye on left), </p>

<pre><code>ffmpeg -i input.mkv -vf stereo3d=sbsl:ml -metadata:s:v:0 stereo_mode=""mono"" output.mkv
</code></pre>

<p>will result in a 2d (left eye only) <code>output.mkv</code>. I believe the reason <code>-metadata:s:v:0 stereo_mode=""mono""</code> is required to remove the stereo tag is because the metadata isn't piped through the filter system, and ffmpeg otherwise tries to duplicate the metadata of the original. If you don't do this, some players may think your 2d result is still 3d and do weird things. </p>

<p>If you find that the result has the wrong aspect ratio, you can experiment with using <code>stereo3d=sbsl2:ml</code> instead which treats the input as half width side-by-side. If all else fails, you can set the correct aspect with <code>-aspect 16:9</code> (or whatever you know the correct aspect ratio to be).</p>
","21540"
"Is it correct to mix tripod and handheld footage?","1247","","<p>I want to shot some scenes using a tripod, and some others handheld (with an IS lens).</p>

<p>Can I mix both types of scenes in a short (4 minutes) movie, or is it unprofessional?</p>

<p>From what I've seen:</p>

<ul>
<li><p>Many movies are shot handheld: it is barely noticeable, but in most scenes when it looks like the camera is on a tripod (for example a 10 seconds scene of a woman who talks to her husband in a restaurant), the background still moves a little all the time.</p></li>
<li><p>Short movies where tripod is used use it consistently: all scenes are shot with either a tripod or a slider. An example: Philip Bloom's <a href=""http://philipbloom.net/category/films/postcard-series/"" rel=""nofollow"">Postcard series</a>.</p></li>
</ul>
","<p>You're asking for an opinion, because there's no rule book to consult here. So, IMO...</p>

<p>'Pro' movies are shot with all manner and mixtures of techniques, including dolly, mount, rig, crane, Steadicam, fixed, zoom, drone etc. I think many people would find a short film that mixed tripod with deliberate ""shaky cam"" to be a bit jarring without a good cinematic justification, but just mixing IS handheld with tripod should be no sin. Story trumps technique anyway -- if your viewers are watching how the camera moves, your story isn't compelling -- or they're film students. (-:</p>
","12921"
"Stop video (something like a screenshot) in Hitfilm 3 Express","1239","","<p>I am new in any kind of video cutting programs. I started using Hitfilm 3 Express, becuase it's free and simple to use, I think. I'm wondering if and how it is possible to make a video stop for as example 5 seconds, show up a text (to explain something) and then let the video go on. Currently im doing this by splitting the video in two parts, make a screenshot of the scene i want and then insert this screenshot between the two videoparts.</p>
","<p>The usual way to do this is with a speed effect.  Create a cut around the one frame you want to render as a still.  Your timeline should look like this:</p>

<pre><code>motion-before | single frame | motion-after
</code></pre>

<p>You can then place a speed effect on the single frame, slow it down to 0% playback speed, and it will happily play forever if you want.  Stretch single frame to 5 seconds of length (120 frames at 24fps) and then composite your text on top.</p>

<p>People do this all the time.  Some applications have a ""freeze frame effect"" which is precisely setting the speed to zero for a default number of seconds.</p>
","17593"
"Best way to dub the voice in a very low budget or no budget film","1234","","<p>I am in the middle of making of a short movie. I have tried dubbing in ordinary mike and later clearing the noise in adobe auditions. But it is not working for me. Can I know the best trick to dub clear voice without using any expensive gadgets. Your advice will help me in my low budget short. Thank you :)</p>
","<p>It sounds like you are using your laptop's microphone, and that your main problem is the lack of a quality mic. If you own an iPhone, especially iPhone 5 or later, I would recommend you use the voice recorder app, and email the audio files to yourself using the Share button. The iPhone's microphone is surprisingly good for audio. High end Android phones also tend to have good mics. Using your phone means you don't have to purchase any additional equipment.</p>

<p>If it helps, <a href=""http://blog.faberacoustical.com/2009/ios/iphone/iphone-microphone-frequency-response-comparison/"" rel=""nofollow noreferrer"">this guy</a> measured the frequency response of the iPhone 3GS back in 2009, and it's a pretty safe bet that the newer models use mics of either the same or better quality.</p>

<p><img src=""https://i.stack.imgur.com/0FPmA.png"" alt=""enter image description here""></p>

<p>Fun fact, the iPhone 5 actually has three microphones. The voice recorder app only takes input from the mic along the bottom edge.</p>

<p><img src=""https://i.stack.imgur.com/xc2mD.png"" alt=""enter image description here""></p>

<p>Also make sure you're recording in a very still, closed-off room. If you are editing in Adobe Premiere Pro, syncing the audio to the video afterwards shouldn't be a huge problem!</p>
","15786"
"Blur out an irregular shaped section of the frame using Sony Vegas Studio","1229","","<p>I have a video in which a person is speaking something. He is right in the middle of the frame. I want to blur out all of his surroundings like you have when you shoot a photograph or a video with a shallow depth of field. I am in post-production.</p>

<p>I am using Sony Vegas Movie Studio 11 HD.</p>

<p>If the shape to be blurred was rectangular, I could overlay two duplicate tracks and blur out the section from the frames in the top track, but I want to blur out everything around the person's body, which happens not to be a geometrically named shape.</p>

<p>How do I do that?</p>
","<p>Unfortunately you are asking too much from Sony Vegas my friend! In order to get a true effect you'd have to do something called ""rotoscoping"" which means you go into each frame and mask out the background.</p>

<p>You can do this in vegas by going into the pan/crop window on the video event. Look for ""mask"" and then you can key frame out the background with the pen tool there.</p>

<p>There is no way to automate this so you'll have to key frame quite a bit. Then once you have just the person visible you duplicate the track beneath it and uncheck the mask on the duplicate. You then can add lense blur or gaussian blur to create the desired effect.</p>

<p>Unless the clip is super short I wouldn't recommend doing it this way though. It takes a lot of time even for skilled maskers.</p>
","9821"
"Why does AAC have two Sampling rates?","1226","","<p>I am working on a project that reads the meta data of video files with <a href=""https://mediaarea.net/en/MediaInfo"" rel=""nofollow"">MediaInfo</a> and processes the results. While testing my code with some sample files I noticed that the audio track of a <code>.mkv</code> file has two different samling rates. The file has two different audio tracks (English and Japanese) but they are listed separately and both tracks show two different sampling rates. Here is the according output of <code>MediaInfo</code> for clarification:</p>

<pre><code>Audio #1
ID                                       : 2
Format                                   : AAC
Format/Info                              : Advanced Audio Codec
Format profile                           : HE-AAC / LC
Codec ID                                 : A_AAC
Duration                                 : 22mn 46s
Channel(s)                               : 2 channels
Channel positions                        : Front: L R
Sampling rate                            : 48.0 KHz / 24.0 KHz &lt;-- here
Compression mode                         : Lossy
Delay relative to video                  : 31ms
Language                                 : English
Default                                  : Yes
Forced                                   : No

Audio #2
ID                                       : 3
Format                                   : AAC
Format/Info                              : Advanced Audio Codec
Format profile                           : HE-AAC / LC
Codec ID                                 : A_AAC
Duration                                 : 22mn 46s
Channel(s)                               : 2 channels
Channel positions                        : Front: L R
Sampling rate                            : 48.0 KHz / 24.0 KHz &lt;-- and here
Compression mode                         : Lossy
Delay relative to video                  : 31ms
Language                                 : Japanese
Default                                  : No
Forced                                   : No
</code></pre>

<p>Can anyone explain what this means? I always thought one audio track can only have one sampling rate. Is this just a different way of describing one sampling rate? Or is it really possible to use two different sampling rates for one audio track?
At the moment, the the software just expects single values for the meta data, so storing two different values for the sampling rate would make things more complicated. Would it be possible to either convert these two values to one value or just choose one of them?</p>

<p>Thanks for your help!</p>
","<blockquote>
  <p>tracks show two different sampling rates. (...) Can anyone explain what this means? I always thought one audio track can only have one sampling rate. Is this just a different way of describing one sampling rate?</p>
</blockquote>

<p>MediaInfo reports what is played by a decoder, depending of its capabilities:<br>
- if your decoder is able to play an HE-AAC stream, the stream will be played at 48 kHz (the frequecy of the ""core"" + the <a href=""https://en.wikipedia.org/wiki/Spectral_band_replication"">SBR</a> data).<br>
- if your decoder is <strong>not</strong> able to play an HE-AAC stream, the stream will be played at 24 kHz (the frequency of the ""core"" AAC)</p>

<blockquote>
  <p>I always thought one audio track can only have one sampling rate.</p>
</blockquote>

<p>No. You can have a ""core"" stream with a specific sampling rate, decodable by old players, and an additionnal sub-stream in the stream with additionnal data for having 48 kHz for decoders capable of decoding such additional sub-stream.</p>

<p>Note that it is same for e.g. channels, with HE-AACv2 you'll get a mono ""core"" + <a href=""https://en.wikipedia.org/wiki/Parametric_Stereo"">PS</a> for upgrading to stereo. or AC-3 (and DTS) can have a 5.1 ""core"" lossy stream for old decoders + extra data for newest decoder in order to get 7.1 and/or lossless.</p>

<blockquote>
  <p>At the moment, the the software just expects single values for the meta data, so storing two different values for the sampling rate would make things more complicated.</p>
</blockquote>

<p>If you are interested only in the ""maximum"" capability, don't care of the second value.
If you use the command line or DLL, you can say to MediaInfo to not show info about legacy players with --LegacyStreamDisplay=0 option.</p>

<p>Note : this is not ""24-bit"" as I read in another comment, bit depth in AAC is ""dynamic"" so no bit depth is shown for AAC (it would be shown for e.g. PCM). This output with several values is wanted by design (showing how an old decoder would decode the stream), and you can disable such behavior with the option provided.</p>
","17681"
"GoPro Hero 3+ Microphone Mounting","1224","","<p>I own a GoPro Hero 3+ video camera which requires the following adapter to use an external microphone:</p>

<p><a href=""http://shop.gopro.com/accessories/3.5mm-mic-adapter/AMCCC-301.html"" rel=""nofollow"">http://shop.gopro.com/accessories/3.5mm-mic-adapter/AMCCC-301.html</a></p>

<p>This makes mounting the microphone onto the GoPro difficult.</p>

<p>Does anyone know of any secure mounts for this? - I can't find any.</p>

<p>What do professional filmmakers use?</p>
","<p>Professional film makers generally use audio recorded separately from the video on dedicated recorders using professional mics (using XLR cables and the like).  3.5 mm audio connectors in particular are almost exclusively a consumer level connector.  So for a professional level, the answer is use something like a Zoom h4n, h5 or h6 and microphones of your choice (either headset, lapel or most likely for action footage, shotgun mics with boom operators).</p>

<p>If you want to be able to do it in camera, then you will need to look in to rigging.  Many consumer/prosumerish mics that work via 3.5mm jacks are designed to mount on DSLR hot/cold shoes.  You can use video rigging to connect your GoPro on one mount point and a cold shoe mounted external mic on a cold shoe mount point on the same rig.  This will keep them together in an easily handleable form factor while keeping the mic securely attached to the same rig as the GoPro.  Note that building such rigs isn't particular cheap unless you build it yourself though.  <a href=""http://www.bhphotovideo.com/c/buy/Digital-SLR-Video-Stabilizers/ci/3926/N/3907816577"" rel=""nofollow"">This link</a> goes to B&amp;H's selection of DSLR rigs to get you started.</p>
","12548"
"What specs should I be looking for in a camera to record board-game sessions?","1223","","<p>I'm a computer science engineer with no experience and very little knowledge in camera and video processing.</p>

<p>I would like to record boardgame sessions (timelapse for example) and record boardgame rules description for my friends.</p>

<p>I'm therefore looking for a camera and I'm a bit lost. There are plenty of different camera on the market with lot of specs and I don't really know what I need to have to record boardgame sessions...</p>

<p>I think it has to have:</p>

<ul>
<li>HD capabilities</li>
<li>A good microphone input both to catch my voice or multiple voices during a game</li>
<li>No specific spec related to frame rate</li>
<li>No waterproof capability</li>
</ul>

<p>Could you help me with other different parameters I would need to care about when buying a camera?</p>

<p>Thanks for your help and precious advices</p>

<p>Random example : </p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/g_QRczGzXqw?start=0""></iframe>
            </div></div></p>

<p>P.s. I'm not looking for a shopping list here... </p>
","<p>There are two approaches you can take based on budget.  The first is to record audio in camera, the other is to record audio to a dedicated audio recorder and mix it with the video after.  The former is cheaper, but the latter will generally produce better quality.</p>

<p>As far as features, beyond HD, you are mostly just looking for good performance in whatever lighting conditions you will have and then looking for good color and video quality.  Best bet is to look at sample videos from them to find one you like.  I'd also recommend taking a look at the GoPro, it's one of the better, cheaper video cameras available and would probably work well for your needs.</p>

<p>If you want something more general purpose, you could also look at a DSLR with video capabilities such as a Canon T4i which would allow for high quality stills as well as high quality video.</p>

<p>Your best bet for narrowing down your choices is really going to be to think about what else you might want to do with it and how much you would like to spend, then look at what is in your price range.</p>

<p>If you do want a dedicated audio recorder, the Zoom H4n is a reasonably cheap and decent option that will allow external microphones to be used for recording audio.</p>
","8578"
"Mux mkv and dts file, change default audio stream and language","1221","","<p>I want to mux (combine) two files into one <code>mkv</code> file. The first one is an <code>mkv</code> file that contains a video and an audio stream in German. The second one is an English audio stream in <code>dts</code> format.I want one <code>mkv</code> file that contains both audio channels, the English one being the first and default audio stream.</p>

<p>Here's the ffmpeg command I'm using so far:</p>

<pre><code>ffmpeg -i movie.mkv -i audio.dts -map 0:v:0 -map 1:a:0
-map 0:a:0 -c copy movie_combined.mkv
</code></pre>

<p>This produces acceptable results. I get an <code>mkv</code> file that has two audio channels, the first one English, the second one German. However, there's two problems. The second (German) audio stream is still the default stream (VLC media player uses it by default and according to MediaInfo, there's a default flag on the stream). Also, the English stream doesn't show up as English, according to MediaInfo there's no language information for that stream.</p>

<p>How do I flag the English audio stream as the default one? And how do I add the language information for that stream (so that, for example, it shows up as English in the audio track selection of VLC media player)?</p>
","<p>For language, add <code>-metadata:s:a:0 language=eng</code>.</p>

<p>FFmpeg doesn't set or alter track dispositions in MKV. Use <a href=""https://mkvtoolnix.download/doc/mkvpropedit.html"" rel=""nofollow"">mkvpropedit</a>.</p>

<p>To set the default flag for the first audio track and remove it from the second one, use this command:</p>

<pre><code>mkvpropedit movie.mkv --edit track:a1 --set flag-default=1 --edit track:a2 --set flag-default=0
</code></pre>

<p>As mentioned in the docs, the app ""<em>does not set the 'default track flag' of other tracks to '0' if it is set to '1' for a different track automatically.</em>"" This is salient because FFmpeg sets default flags for all tracks to <code>1</code> unless the input has it different.</p>
","19644"
"Is there a way to parent an objects position to another objects position in Nuke, so they move parallel to each other?","1218","","<p>I want to move an object parallel to another object in Nuke. I managed to get the object to move to the same position as the other one. (Copy link paste absolute.) But I wasn't able to figure out how to move it relative to the other object. Copy link and paste relative did not work.</p>
","<p>You can simply edit the expression values:</p>

<ul>
<li>create a new <code>Transform Node</code> by hitting <kbd>T</kbd></li>
<li><code>drag and drop</code> the translation values by holding <kbd>CTRL</kbd> from <strong>original transform node to the new one</strong> (this should create a green line between that nodes)</li>
<li>right click the <code>translation curve icon</code> of the new transform node to open up the <code>Edit Expressions</code> Window </li>
<li>in this window you can do some <strong>math with the existing y or x values</strong> to move the the child node</li>
</ul>

<p><img src=""https://i.stack.imgur.com/elnPP.png"" alt=""enter image description here""></p>
","14521"
"Copyright of materials used in a video","1212","","<p>It is common that TV channels or internet TV channels (e.g. those regularly post their shows on Youtube) use materials from other sources. For example, displaying photos from the internet, parts of videos already posted on Youtube and other websites, etc.</p>

<p>I am curious how the copyright permission is handled for using copyrighted materials in videos.</p>

<p>In the past, major TV channels were only in the market, and they had their own reporters and photographers to make their materials. Nowadays, there are many small channels (even personal ones), and I doubt if they take of care of copyright issues at all.</p>

<p>As a more specific example, how to obtain right to use a part of a Youtube video in an online show, which will be posted on Youtube ?</p>
","<p>Very broadly speaking, you have a much easier experience obtaining permission if you are not making money from someone else's content.</p>

<p>Most copyright laws recognise 'fair use' -- a small quote or a clip that is proof of someone else having said something.  It is customary and advisable to put such clip in a box or window to show that the content is being referenced.  Fair use for a small portion of text or video is easily established than a picture, for a picture is the entire content.</p>

<ol>
<li>For images, search engines usually have a license filter in the search. Usually, it errs on the side of caution. Unfortunately, you have to contact the original agency or person and obtain permission to use.</li>
<li>Youtube now has content signature comparison and will warn you that you are using someone else's content but will leave it up to the copyright owner to enforce it.</li>
</ol>

<p>So, if it is does not fall under 'fair use' and is clearly someone else's copyright, you will have to track down the owner and obtain permission.</p>
","7298"
"What is the relationship between hardware acceleration and DXVA/QuickSync/Cuda?","1200","","<p>Many video playback applications (e.g. <em>VLC Player</em>, <em>MPC-HC</em>, etc.) are highly configurable. For example:</p>

<ul>
<li><em>VLC Player</em> allows you to enable: <em>QuickSync</em> and/or DXVA</li>
<li><em>Freemake</em> allows you to enable: CUDA and/or DXVA</li>
</ul>

<p>What I am trying to understand is... what is the relationship between hardware acceleration and these technologies?  </p>

<p>For example, can you enable DXVA to leverage GPU based hardware performance AND enable <em>QuickSync</em> to leverage CPU based hardware acceleration?</p>

<hr>

<p>It is my understanding that...</p>

<ul>
<li><strong>DirectX Video Accelerator (DXVA)</strong>

<ul>
<li>This type of hardware acceleration improves video encoding/decoding performance by routing 'work' to the GPU (integrated or discrete).</li>
</ul></li>
<li><strong>Intel QuickSync</strong>

<ul>
<li>This type of hardware acceleration improves video encoding/decoding performance by routing 'work' to the CPU - specifically a dedicated processor die that is responsible for performing video encoding/decoding operations.</li>
</ul></li>
<li><strong>Compute Unified Device Architecture (CUDA)</strong>

<ul>
<li>Is a closed API developed by NVIDIA that not only gives direct access to the the GPU's virtual instruction set, but also allows for data to be retrieved from the video card so that additional processing can be performed.</li>
</ul></li>
</ul>
","<p>They are all forms of hardware acceleration, which broadly, just means that specialized hardware is doing things faster than the basic CPU normally could.  The exact nature of what each type of acceleration does depends entirely on the software you are using and the hardware you are using.  </p>

<p>Some hardware is very purpose specific.  Something like Intel QuickSync is a specific hardware encoder and has limited functionality because it can only serve it's purpose driven role.  This is similar to any other dedicated encoder, such as the integrated encoder used for NVidia Shadowplay or the dedicated encoders on devices like the Matrox MXO2 or BlackMagic Intensity.  There are other rolls other than encoding that could be filled by a purpose driven card, but encoding is certainly one of the most common.</p>

<p>Other hardware, such as modern GPUs (CUDA or OpenCL based, I think ATI has a term too, but can't remember it) and video editing or mixing acceleration cards, such as Black Magic's ATEMs or Matrox's older RT line of real time editing cards, are designed to provide more general purpose functionality.  Depending on the hardware they may either offer a wider array of specialized functionality or simply a general purpose parallel processor that works better for video purposes.  In these cases, the functionality is made available to software, but it is up to software to utilize the processing capabilities of the hardware and the exact nature of the acceleration will depend on which algorithms and processing they choose to offload to the hardware.</p>

<p>Another option, as far as APIs are concerned, is something like DXVA, which strives to unify multiple technologies.  DirectX in general is a system for interfacing software and diverse graphics hardware.  It looks for any hardware that tells it that it can fill a role and then will utilize that hardware to do a job when software asks for the job to be done.  I'm not super familiar with DXVA, so I don't know if it puts any specific requirements on hardware (some directX stuff does), but on the hardware end, the implementation could potentially differ greatly and could use either CPU features or GPU features, depending on what's available to it.  It may or may not be directly implemented in terms of the more hardware specific feature sets (CUDA, QuickSync, etc), but it is designed to work across diverse hardware, so you know less about what is going on under the hood.</p>

<p>All of the technologies can be used to speed encoding, but the general purpose technology can also be applied to just about anything that you can process in parallel, so they can (potentially) impact render times as well (the time it takes to make the images that are going to be encoded).</p>
","16221"
"How to record a live video of yourself with a transparent background?","1198","","<p>I always watch and wonder how some youtubers have a little window on a corner of a gameplay or something...</p>

<p>My doubt isn't on how they get the image, my doubt is on how some youtubers get the video of themselves with a transparent background on top of the gameplay.</p>

<p>I know this requires a green screen, but I really want to know how this gets automatically edited. I have seen some live gameplay with the transparent effect.</p>

<p>How do the youtubers get a transparent background on a live feed?</p>

<p>What I am trying to achieve is, recording myself on a live feed, but with a transparent background... If you see what I mean...</p>

<p>Answers much appreciated,</p>

<p>Sid</p>
","<blockquote>
  <p>I really want to know how this gets automatically edited.</p>
</blockquote>

<p>The technique is called <a href=""https://en.wikipedia.org/wiki/Chroma_key"" rel=""nofollow"">chroma key</a> and consists of considering as transparent all pixels of the colors within a specified range. This is usually used with a bluescreen or a green background. The image is then simply layered over another background.</p>

<p>As you have noticed, chroma key is not limited to postprocessing: <a href=""https://www.google.fr/search?q=chroma%20key%20real-time"" rel=""nofollow"">a lot of software products</a> can do this in real-time for live streaming.</p>

<p>By the way, if you have used Google Hangouts, <a href=""http://www.googleplusdaily.com/2013/07/google-effects-for-google-hangouts.html"" rel=""nofollow"">one of the effects</a> consists of replacing the background by an image. Don't be mislead by this, since this has nothing to do with a bluescreen and doesn't require any specific color for the background; I imagine that Hangouts' algorithm rather assumes that parts that don't move are the actual background.</p>
","19292"
"Using Multicam Editing How to Color Correct an entire angle","1194","","<p>Am new to Premier CS6 - working with Multicam editing.  Having success but stuck on color correction.  I am able to create a multicam clip and do my edits using the Multicam Editing window (very slick and fast).  Also able to adjust those edits in the timeline to improve timing.  Sweet.</p>

<p><em>How do I color correct (or add an effect) to an entire angle in a multicam sequence after I've applied the edits (or even before)?</em></p>

<p>If I drag the effect into the timeline it sticks only to a particular edit and won't get to all the edits from that angle.  I can't drag it into the upper-left window (""browser""?) and don't know how else to apply the effect.  </p>

<p>I've read some online but am hitting a wall with how I get to the underlying video footage behind the multicam edit.</p>

<p>Can someone hold my hand and guide me?</p>

<h3>Detail Steps Taken in CS6</h3>

<ol>
<li>Place video files (angles) in the project browser</li>
<li>For each file, double click to preview video and then set corresponding in-points</li>
<li>Select all video angles in project browser, right-click, and choose <em>Make multicamera clip...</em> (by in-point)</li>
<li>Place new multicam clip in the sequence</li>
</ol>
","<p>If all is correct you enable the multi camera mode on a sequence containing the different angles in different tracks as shown in the image below (this is CS5.5, but CS6 is similar).
  The image shows a sequence containing 4 different clips which represent 4 different angles.
In this example these are just a collection of random footage, but it works the same if you have actual different footage from different angles.</p>

<p><img src=""https://i.stack.imgur.com/B01Li.png"" alt=""enter image description here""></p>

<p>Let's call this <code>Sequence 01</code>. Now put <code>Sequence 01</code> in another sequence,</p>

<p><img src=""https://i.stack.imgur.com/6344j.png"" alt=""enter image description here""></p>

<p>click on <code>Multi-Camera -&gt; Enable</code> and then you can switch witch camera you want in <code>Sequence 02</code>.  </p>

<p><img src=""https://i.stack.imgur.com/vaQJz.png"" alt=""enter image description here""></p>

<p><strong>According to the information in your question you've already come so far.</strong></p>

<p>Now if you want to color correct one angle (lets take the angle called <code>Leaf.MOV</code>) just open <code>Sequence 01</code>, apply the effect to the clip (drag and drop on the <code>Leaf.MOV</code> clip) and this changes should propagate into your multi-cam edits.</p>

<p>Please feel free to ask any question if this doesn't work for you. I'll try to help.</p>
","5601"
"HDMI Compressed vs Uncompressed mode","1189","","<p>What is the difference between HDMI compressed and uncompressed modes? My understanding is that in compressed mode the TV handles the decoding. Is this right?</p>
","<p>The TV still has to translate the color information and format, so there is, in effect, still some decoding on the TV.  The difference is that it is uncompressed.  The output from the device matches whatever the source captured, in whatever image format the source uses, but there is no loss of detail from applying compression.</p>

<p>Compressed requires vastly lower data rates and is easier to transfer to the TV, but the quality is substantially lower.  Uncompressed requires a high quality source and a very fast data connection, but also provides the highest possible quality as the signal is not compressed with lossy compression schemes.</p>
","13010"
"How to record live video using camcorder?","1180","","<p>I need to conduct a video conference using professional video cameras. I will be recording and streaming 1080p video <strong>[1]</strong>. To that end, I need to stream live video from a professional video camera to my PC. Here are the requirements:</p>

<ul>
<li>The camera output is HDMI or SDI</li>
<li>The computer input is USB3</li>
<li>The delay between the time an event occurs and the moment it reaches the PC must be under 300ms.</li>
<li>The camera must show up as a webcam to the PC.</li>
<li>The PC will be running Windows or Linux.</li>
</ul>

<p>Any ideas?</p>

<p><strong>[1]</strong> I am aware of how much data is involved. Please assume that I have sufficient resources to do this.</p>
","<p>Perhaps the Intensity Shuttle from Blackmagicdesign</p>

<p><a href=""http://www.blackmagicdesign.com/products/intensity/models/"" rel=""nofollow"">http://www.blackmagicdesign.com/products/intensity/models/</a></p>

<p>This link has some additional info.</p>

<p><a href=""http://help.livestream.com/customer/portal/articles/577891-what-are-your-recommended-devices-"" rel=""nofollow"">http://help.livestream.com/customer/portal/articles/577891-what-are-your-recommended-devices-</a></p>
","7409"
"What is a circular motion around a object called when filming?","1178","","<p>You know that camera technique when the camera operator usually have a steadicam and runs/walks around a person.</p>

<p>I'm trying to learn how to do that with my homemade glidecam but it is so hard. it would be nice to find a tutorial but I don't know what to search for.</p>
","<p>That's a kind of tracking shot known as an orbit. This looks cool:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/bMNnqopdUjg?start=0""></iframe>
            </div></div></p>
","5199"
"How to insert dynamic data into a produced video","1177","","<p>I was wondering if anyone knew the technique or how to place dynamic information into a produced video?</p>

<p>The idea is that you could, say connect to facebook, and then display your friends faces in a movie about missing persons or similar. It's the same principle as the <a href=""http://www.intel.com/museumofme/r/index.htm"" rel=""nofollow"">Intel Museum of Me</a> where you grant access to facebook and it creates a gallery of your facebook pictures.</p>

<p>Or similar to this video where you can upload your face into a video, <a href=""http://vimeo.com/25392805"" rel=""nofollow"">http://vimeo.com/25392805</a></p>

<p>I've been searching for ages for a method or technique for this but I can't find anything. Searching for ""how to insert pictures in video"" or ""how to load dynamic data into video"" doesn't seem to yield the right kinds of results.</p>

<p>Thoughts appreciated.</p>
","<p>You should try googling for something like <a href=""http://www.google.co.uk/search?hl=en&amp;source=hp&amp;biw=1007&amp;bih=610&amp;q=green%20screen%20software&amp;oq=green%20screen%20so&amp;aq=0&amp;aqi=g10&amp;aql=&amp;gs_sm=c&amp;gs_upl=1422l4952l0l6468l19l13l1l0l0l0l609l1860l0.2.2.0.1.1l6l0"" rel=""nofollow"">green screen software</a> or <a href=""http://www.google.co.uk/search?hl=en&amp;biw=1007&amp;bih=610&amp;q=chromakey%20software&amp;oq=chromakey%20software&amp;aq=f&amp;aqi=&amp;aql=&amp;gs_sm=e&amp;gs_upl=37225l39131l0l39428l9l8l0l0l0l3l765l1515l0.3.1.6-1l5l0"" rel=""nofollow"">chromakey software</a></p>

<p>There is a wide range of free, cheap and very expensive applications to do this. Some of the free ones actually work okay - depends on your requirements.</p>
","1979"
"Save existing project to an external drive Final Cut Pro X 10","1177","","<p>I have a project with events saved on my Mac drive. I want to save this project on my external HD drive to clean space and work on this project later. </p>

<p>I copy pasted Final Cut Events and Final Cut Projects folders to my external HD but I don't know how to restore it into project in FC.
Any help is appreciated.</p>
","<p>""I copy pasted Final Cut Events and Final Cut Projects folders to my external HD but I don't know how to restore it into project in FC. Any help is appreciated.""</p>

<p>It's better to perform this type of operation from within FCPX, instead of the Finder via copy/paste.  With the project selected in the Project Library, choose File->Move Project...  As long as you have an external drive connected, the resultant ""location"" dropdown menu will be populated (instead of saying ""no value"").  Selecting the ""Move project and referenced events"" radio button will transfer all of the necessary media to the external drive, AND remove it from the local drive, thus freeing up space.</p>

<p>When you work this way, there is no need to, as you say, ""restore it to a project in FC"".  You simply make sure the external drive which contains the project file and media is connected to your machine, and the project is available in the project library window.</p>
","7081"
"Transcription Methods/Times","1174","","<p>I figured I'd leave a question here while I did the manual labor, but what do you find is the best way to transcribe footage?</p>

<p>I'm an editor and the first thing I like to do is create a script of people said, then piece together parts of their takes, and THEN get to the actual footage editing. That way I'm only working with what I need ahead of time.</p>

<p>Basically what I do now is load up the videos in VLC, and open a TextEdit (mac), then watch it through normally or double speed until I get to a part with the interview, then I mark that timecode on the TextEdit, and go watch through the interview in triple-quadruple slowdown, so that I can keep up with what they say and not have to constantly stop and start it back up.</p>

<p>With this way it seems like I'm doing somewhere around 5 minutes real time to transcribe a minute, which comes to about 12 video mins a realtime hour. And I was just wondering if there was a better way you guys already know about. I'm not sure I trust auto-transcribe-type things.</p>

<p>Do you think this slowing the vid down and having to go back 5 seconds only a few times when I can't quite tell what he's saying because it's so slow, is saving more time than watching it in real time, pausing, writing, going back and watching the next part?</p>

<p>Or do you know better ways? Seems transcribing is the most time-consuming part of my workflow. </p>

<p>Since VLC is opensource, would it be possible to combine VLC and a Text Editor...something like VLC on the top and the text editor on the bottom, and whenever you hit a certain key, it automatically puts the timecode in the text, and you can use VLC's shortcuts while typing? Etc?</p>

<p>Again, non-automatic preferred. For one, it could be wrong, and two, I'd like to get used to the footage.</p>

<p>Thanks </p>

<p>Please help?</p>
","<h2>Speech Detection with Adobe Premiere</h2>

<p>You haven't mentioned what NLE you use to edit, however here is a <a href=""http://help.adobe.com/en_US/premierepro/cs/using/WS1c9bc5c2e465a58a91cf0b1038518aef7-7d0fa.html#WS29F692D2-7A49-44db-A109-AE016B766767"" rel=""nofollow"">method</a> you can use to transcribe if you are using Adobe CS4 or later. </p>

<p>This method uses speech detection to automatically transcribe videos - a feature brought in with CS4. It then adds the the text into the metadata of the file. </p>

<blockquote>
  <p><a href=""http://help.adobe.com/en_US/premierepro/cs/using/WS1c9bc5c2e465a58a91cf0b1038518aef7-7d0fa.html#WS29F692D2-7A49-44db-A109-AE016B766767"" rel=""nofollow""><strong>Analyze speech to create text metadata</strong></a></p>
  
  <ol>
  <li>Select a file or clip.</li>
  <li>At the bottom of the Metadata panel, click Analyze Speech, or Analyze (Adobe Premiere Pro).</li>
  <li>Set the Language and Quality options, and select Identify Speakers if you want to create separate speech metadata for each person. 
  <em>Note: Speech Search can use any of several language-specific and dialect-specific libraries, such as libraries for Spanish and UK
  English.</em></li>
  <li>Click OK.</li>
  <li>The spoken words appear in the Speech Analysis section.</li>
  </ol>
  
  <p>To retain the speech metadata, save the project.</p>
</blockquote>

<p>Obviously this depends on the quality of the audio. Good clean audio will provide the best results. Its also worth noting that Premiere uses Soundbooth to process the detection, so if your audio wasn't up to scratch you could clean it up in Soundbooth, and then perform the analysis there. </p>

<hr>

<h2>VLC Scripting</h2>

<p>I came across <a href=""http://wiki.videolan.org/How_to_use_VLC_for_transcription_in_linux"" rel=""nofollow"">this article</a> explaining how to take the current timecode from a video and add it to a text document for transcribing. The title of the article states its a method for Linux. However I'm sure it would be easy enough to take the method and apply it to Windows or OSX.</p>

<hr>

<h2>Manual</h2>

<p>In regards to your current method, it depends on how many times you fall behind. When I have transcribed video I have done it at full speed but I haven't bothered to fix errors while I'm typing. Usually I'll go back and fix it up later - and being that they are just for editing purposes they don't need to be totally correct. </p>

<h3>Manual live tagging with Adobe Prelude</h3>

<p>You might also want to have a look at <a href=""http://www.adobe.com/products/prelude.html"" rel=""nofollow"">Adobe Prelude CS6</a>. It's an ingesting and meta logging program. One of its features is that it lets you add markers (and name them) in realtime as you are watching the video.</p>
","4485"
"How to make a video from a still image and an audio file?","1168","","<p>I have an image file (png) and an audio file (aiff).  How do I combine them to make a video file that shows the still image and plays the audio file?</p>

<p>Target operating system: either OS X Lion or Ubuntu, using only free software, must be command-line driven (no GUI solutions please).  Thanks in advance!</p>
","<p>I think <a href=""http://ffmpeg.org/"" rel=""nofollow""><code>ffmpeg</code></a> is what you want, but you will need to dive into the docs to figure out how to make it do what you want.</p>
","5299"
"Issue with Opening Downloaded ASF Stream files in Adobe Premiere Pro CC","1166","","<p>I am a newbie to video editing and I've been trying to take some files I downloaded from a stream online (public government videos), to make an Adobe Premiere Pro video out of them. Here's an example link to a file I downloaded that I want to edit: </p>

<p>mms://stream.miamibeachfl.gov/tv20%20archive/design%20review%20board/drb_20130806.asf</p>

<p>The problem I am having is that the ASF files have an issue when I try to import it or just open in the Source Monitor in Adobe Premiere Pro. The ""Conforming"" notice shows at the bottom right, but it stalls there after about 1% and I have an error in the Events dialog which says the following:</p>

<p>""File importer detected an inconsistency in the file structure of XXX.avi. Reading and writing this file's metadata (XMP) has been disabled.""</p>

<p>I thought the files were possibly too big to use in Premiere (some are over 1GB), so I used some freeware programs to trim them down, thinking this would help. However, even then it doesn't work, or if it does work to trim the file and convert to AVI, I get the following error when trying to open the new file in Adobe Premiere Pro: ""Unsupported format or damaged file""</p>

<p>Here are the properties of a sample file I am having problems with, which I get from the Premiere Pro menu -> Get Properties For -> File:</p>

<p>""File Path: C:\Users\Daniel Desktop\Documents\Google Drive\Consulting\Activism\DATA SOURCES\VIDEOS\plb_20120327.asf
Type: Windows Media 
File Size: 1.4 GB
Image Size: 320 x 240
Frame Rate: 29.97
Source Audio Format: 32000 Hz - 16 bit - Stereo
Project Audio Format: 32000 Hz - 32 bit floating point - Stereo
Total Duration: 11:03:16:05
Pixel Aspect Ratio: 1.0""</p>

<p>I am not sure why this issue is happening but based on the file issues, maybe there is some setting I am missing?</p>

<p>Any tips or help would be much appreciated. I can't start video processing if it's not possible for the video to load. Thank you!</p>
","<p>The link you gave is for a streaming media file, not a download.  Most likely, whatever ripper you used to pull down the file did a poor job and produced a faulty file.  It may actually play back since most playback applications are rather permissive when it comes to errors in the file, however a professional editing package like Premiere will tend to throw errors if there are major problems in the file since it won't be able to generate suitable output from the file.  The best bet is probably to try and find a non-streaming version, particularly since the quality of the streaming version is VERY low.</p>
","9828"
"h264_omx and non-existing PPS 0 referenced","1161","","<p>Trying to stream a webcam through rtp with a raspberry pi.</p>

<p>Cating the SDP file on my desktop, and launching ffplay:</p>

<pre><code>ffplay sdp.out
</code></pre>

<p>then streaming out this way:</p>

<pre><code>ffmpeg -i /dev/video0 -c:v h264_omx -f rtp rtp://192.168.0.101:1234
</code></pre>

<p>works perfectly. But If I launch ffmpeg first THEN ffplay, ffplay stutters some error message about missing PPS and never displays any frame:</p>

<pre><code>[h264 @ 0x7fd97c0008c0] decode_slice_header error
[h264 @ 0x7fd97c0008c0] no frame!
[h264 @ 0x7fd97c0008c0] non-existing PPS 0 referenced
</code></pre>

<p>After looking for a while I understand some intermediary frame might be missing. However, using the <code>-g</code> option this way does not change anything:</p>

<pre><code>ffmpeg -i /dev/video0 -c:v h264_omx -g 30 -f rtp rtp://192.168.0.101:1234
</code></pre>

<p>It is a problem for me because I am using the raspberry as a securiy camera. My desktop can be restarted from time to time and I need ffplay to be able to catch after the streaming is started.</p>

<p>Any help is appreciated.</p>
","<p>It turns out it is impossible to live stream with the RPi hardware acceleration using ffmpeg.
For ffmpeg to configure the hardware acceleration to issue SPS/PPS frames during the stream and not only at the beginning of it, it needs to use a <a href=""https://github.com/raspberrypi/firmware/issues/242"" rel=""nofollow noreferrer"">specific API of the raspberry firmware</a> which it does not at the moment. </p>

<p><a href=""http://frozen.ca/streaming-raw-h-264-from-a-raspberry-pi/"" rel=""nofollow noreferrer"">Raspivid</a> and <a href=""http://gstreamer-devel.966125.n4.nabble.com/omxh264enc-and-stream-format-string-avc-td4671543.html"" rel=""nofollow noreferrer"">gstreamer</a> does that though. So turn to thoses solution and forget about ffmpeg.</p>
","21245"
"After Effects won't consistently stop previewing video when I ask it to","1157","","<p>I'm teaching myself After Effects CC while trying to make a music video. I use the spacebar or the 0 on the numeric keypad to preview my work. When I press one of those keys a second time to stop the preview, After Effects often ignores me and continues playback. </p>

<p>It's very frustrating to carefully time a spacebar press to place the Current Time Indicator exactly where I want it and have my input be blithely ignored by the software. What can I do to ensure that After Effects CC will always respond to keyboard commands?</p>
","<p>When you press <kbd>space bar</kbd>, After Effects <strong>tries to playback the composition out of the current playhead position instantly</strong>. Theoretically the playback depends on your machine, but an instant playback of a  composition (in real time) is a very computing-intensive process and with conventional computers not possible at the moment. This also is the simple reason why After Effects sometimes does not respond. Also see the official <a href=""https://helpx.adobe.com/after-effects/using/improve-performance.html"" rel=""nofollow noreferrer"">Improve performance</a> manual.</p>

<hr>

<p>For correct playback (25fps) use the <strong>RAM-Preview</strong> instead of the <kbd>space bar</kbd>. Press <kbd>Numpad 0</kbd> or the associated <kbd>|||></kbd> in the <strong>Preview Panel</strong> to start the preview rendering process:</p>

<p><img src=""https://i.stack.imgur.com/USoqw.png"" alt=""enter image description here""></p>

<ul>
<li>Press <kbd>Numpad 0</kbd> <strong>again</strong> to stop it</li>
<li>Press <kbd>Numpad 0</kbd> <strong>twice</strong> to stop and playback the previously rendered (green) frames</li>
</ul>

<p>In order to facilitate the work:</p>

<ul>
<li>Set the <strong>rendering resolution</strong> of the view to a lower value for e.g. to <strong>half</strong> or <strong>quarter</strong></li>
<li>Limit the <strong>work area</strong> in your timeline, press <kbd>B</kbd> to set the start render point (Begin), press <kbd>N</kbd> to set the end point (End) </li>
<li>Enable <strong>loop</strong> preview (7th button) to repeat the preview of the current work area</li>
<li>Press <kbd>Numpad , </kbd> for <strong>audio preview</strong> only</li>
</ul>
","16052"
"Techniques and technologies for synchronizing multiple audio and video streams","1157","","<p>I'm hoping to produce an amateur movie.</p>

<p>I may be renting some equipment: (professional camera, microphone and digital recorder) but I hope to primarily use my own equipment: camera phones, professional microphones, analogue audio mixer, audio digitizer and a laptop. I hope to produce the video on an open-source video editor (Linux or Windows).</p>

<p>In case it needs to be said, I'm on a near-zero budget! The biggest concern I have is how I will synchronize a phone camera with an external microphone fed into the laptop.</p>

<p>Are there any techniques to help with this? I had a go before and it's surprisingly hard getting the audio perfectly sync'd with the video. Are there any tricks?</p>

<p>Also, what industry-standard technologies are used for synchronizing professional microphone setups with video, and between multiple cameras? I assume there are at least a couple of old analogue and newer modern digital methods?</p>

<p>Thanks!</p>

<p>Edit: I looked at <a href=""https://video.stackexchange.com/questions/3646/merge-audio-and-video-in-post-production?rq=1"">this link</a> but it didn't go into detail or explain different options. </p>
","<h1>The professional way</h1>

<p>Professionally done, all cameras and audio recorders will have a running (SMPTE) time code, which can be configured in 'free run' mode - meaning the time counter runs regardless whether we record or not (ie, STOP mode).</p>

<p>At the beginning of the shooting day, all recordists (cameras, audio) will sync their clocks. This is done either to SMPTE 01:00:00:00, or to the time of the day.</p>

<p>Then all video clips and audio files (broadcast wave files - BWF, which is just a WAV file with time code imprinted) will be 'timestamped' - the time at which the clip was recorded is part of the file header.</p>

<p>In addition, these recorders allow you to choose the folder and file prefix of the recorded clips. A simple case is when all crew (camera and audio) set this prefix based on the shooting day ('Day01' for example).</p>

<p>Then when the audio and video clips are imported to the video editing software, there's alway an option to 'spot' the clips to their original time code. This way, the alignment of video and audio happens nearly automatically.</p>

<h1>The non-professional way</h1>

<p>Before the days of digital equipment, the syncing between pictures to audio was done manually with the help of a clapper (clapperboard):</p>

<p><img src=""https://i.stack.imgur.com/q3ntI.jpg"" alt=""A photo of a clapper""></p>

<p>Upon the director's instruction 'Cameras', both the camera men and audio recordists will aim at the clapper; then the clapper is clapped.</p>

<p>Later on, each clip is brought consecutively into the timeline (clips are numbered - there's a way to know which came before the other). The sync is done manually, by finding the video frame at which the clapper jaws were hitting each other; on the waveform display, you can see a transient from the sound the clapper makes. So it's all about moving the audio transient to the clapping frame.</p>

<p>For your purpose - you can just clap your hand in front of the camera (although visual and auditory labelling can be of great aid later on, but this too can be improvised).</p>

<p>Note that in the professional industry, this practice takes place until today, even if all equipment can stamp timecode.</p>
","9492"
"Will I lose quality when converting avi to mkv?","1157","","<p>I have a lot of *.avi files with either 1 audio track or several. All of them are mp3 encoded. The video codec is xvid4. </p>

<p>However, when I play these avi files I can choose between the audio tracks (if there is more than 1 audio track) but they are named ""audio track 1"", ""audio track 2"" and so on. </p>

<p>That's why I want to convert them into mkv files since they support audio track names. </p>

<p>Would I loose any quality if I convert them?</p>
","<p>AVI and MKV are both containers, so in theory it should be possible to transfer your contents with no further loss. I can't say for certain if MKV can wrap xvid4, but it's very likely. You shouldn't need to decode / re-encode to transfer the contents, just demux / remux.</p>
","12173"
"Youtube 720p video has more MB than 1080p","1155","","<p>I watched a video and downloaded with idm in 720p. I don't like the resolution and I decided to download 1080p. But I saw 720p is nearly 21 MB but 1080p is 10 MB. And I opened the video(1080p) I saw it is better than 720p. Why?</p>
","<p>I tried this with the video provided in the comment.
The reason why the 1080p video is smaller is because it has a lower bit-rate (327 vs. ~207 Kbps). In the case of this type of content its likely that the bit-rate for the 720p version is too high. Same for the audio the 1080p version has 96 Kbps and the 720p version has 144-155 Kbps.</p>

<p>A possible reason why this is the case, is how YouTube handles 720p and 1080p video. Unlike the 720p stream the 1080p stream isn't truly constant in it's bit-rate. For 1080p YouTube uses a protocol called ""<a href=""http://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP"" rel=""nofollow"">DASH</a>"" which adapts the bitrate of the video on the fly to meet the bandwidth constrains of the user which might change during streaming. You get lots of video bits from the server which might differ in bit-rate depending on your current connection. So if you download the same video on a very slow connection it will probably be even smaller.</p>

<p>In this case it saved you bandwidth while having better quality, in other cases the 1080p video could actually look worse than the 720p video, though DASH is pretty smart so usually you will probably end up with a better looking 1080p.</p>
","12490"
"best software for multicam video production","1149","","<p>We have records from 30 cameras (couple TBs of video files), which were started almost simultaneously, part of them were action cameras (so no timecode here, and there are gaps because of battery replacement), other part were usual hd cameras with time codes embeded to video files.
All video is 1080p25.</p>

<p>We need to sync all videos together, and then render all 30 sequences after color correction and cutting.I have tried adobe premiere cc, but is was real pain, especially rendering with crushes and other glitches.</p>

<p>Is there any alternative for Premiere?
We have quite recent workstations running windows 7 with consumer video cards (both ati 7970 and nvidia 680 are available), so it will be great if software can do distributed rendering and use gpu. We also have couple of 2 year mac pro, so osx only software can also be of use.</p>
","<p>Premiere is your best option unless you plan on spending a boatload (thousands and thousands) on an Avid system.  I'd suggest working on smaller sets of cameras at a time and then working off the combined feeds.  You can use multicamera editing mode with several cameras at a time, make the best choices out of those and then run each of those sequences in to another multicamera sequence to further resolve them.</p>

<p>The process is going to kind of suck no matter what system you do it on because it is a boatload of synchronous footage to go through.</p>
","9300"
"Premiere Pro CS6 Won't Install on Windows 10","1138","","<p><a href=""https://i.stack.imgur.com/4GiMt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4GiMt.png"" alt=""enter image description here""></a> I wanna to create professional videos for upload to YouTube at 1080p, but after installing Windows 10 I can't install Premiere Pro CS6.  I get warning icons and the option to install Premiere is greyed out.  How can I get around this error and get Premiere to install?</p>
","<p>Based on info revealed in <a href=""http://chat.stackexchange.com/transcript/message/29340146#29340146"">chat</a>:</p>

<blockquote>
  <p>I trying to install that on windows 10 32 bit . Is it a problem ?</p>
</blockquote>

<p>the answer is that since version CS5, Premiere Pro is 64-bit only and can't be installed on a 32-bit OS.</p>
","18282"
"How would I develop AGFA Moviechrome 40 film?","1136","","<p>I recently was crawling through old boxes and came across my grandfather's old Canon Zoom 518-2 Super 8 video camera. Inside was a cartridge of AGFA Moviechrome 40 film. I'm wondering where/how I'd go about getting this developed and was wondering if anyone had any suggestions?</p>
","<p>You could try giving <a href=""http://www.filmrescue.com/old-movie-film-developing/"" rel=""nofollow"">Film Rescue International</a> a go. Their <a href=""http://www.filmrescue.com/faq/"" rel=""nofollow"">FAQ</a> predicts the results of Moviechrome to be on ""poor"" quality level, and  people <a href=""http://www.cinematography.com/index.php?showtopic=10697"" rel=""nofollow"">seem to agree</a>.</p>

<p>There's also <a href=""http://www.rockymountainfilm.com/kiimovie.htm"" rel=""nofollow"">Rocky Mountain</a> in US; <a href=""http://www.processc22.co.uk/"" rel=""nofollow"">Process C-22</a> and <a href=""http://oldfilmprocessing.co.uk/Movie-film-processing-transfering-dvd.asp"" rel=""nofollow"">FotoStation</a> in UK.</p>
","2858"
"How do I make the DoTA 2 map work in Source Filmmaker?","1126","","<p><strong>Full Question:</strong> How do I use the DoTA 2 map in Source Filmmaker? Every time I load it up, it crashes.</p>

<hr>

<p>I've been trying to use the DoTA 2 map in Source Filmmaker, but every time I try to load it, the program crashes. I have the DoTA 2 assets extracted and mounted, but loading the map seem to be the thing that always cause it to crash.</p>

<p>I can load all the models (ie. Couriers, Heroes, Gear, map props etc.) fine. But loading the DoTA 2 playing-field map doesn't work.</p>

<p><b>These are two of the heroes/champions; Viper and Teemo* along with a bunch of other stuff, in the Counter-Strike: Global Offensive map de_nuke.</b><br>
Characters and models work. Not the map. Why? and how do I fix it?
<img src=""https://i.stack.imgur.com/0vB7y.jpg"" alt=""These are two of the heroes/champions; Viper and Teemo* along with a bunch of other stuff, in the Counter-Strike: Global Offensive map de_nuke.""></p>

<p>Note:<i>* Not from the game DoTA 2.</i></p>

<p><hr>
Also, going ""File > Load Map > [Map name].bsp"" (or Ctrl-L) does not work.</p>
","<p>Its hard to say what exactly is the issue without an error message.
Maybe you haven't properly exported the maps and or setup SFM for Dota 2 content. There is a nice guide <a href=""http://steamcommunity.com/sharedfiles/filedetails/?id=313936526&amp;insideModal=1"" rel=""nofollow"">here</a> that goes through the whole process step by step.</p>

<p>You also want to check the cache and local file for both SFM and Dota 2, if an error gets detected re-export the Dota 2 content afterwards.</p>
","12808"
"Loss of quality while adding subtitle via FFmpeg","1123","","<p>After solving on problem I cam coming to a next one. After adding subtitles, discussed <a href=""https://video.stackexchange.com/q/17543/12686"">here</a>, with the command:</p>

<pre><code>ffmpeg -i grdedFinal.mov -vf subtitles=portSbs.srt gradedFinalwithSubs.mov
</code></pre>

<p>the quality of the generated file significantly decreased. The original file was of size <strong>1,5GB</strong> whereas the resulting file was of size <strong>~300MB</strong>. The subtitles are added correctly but the compression is unnecessary. In a word I would like for the <strong>FFmpeg</strong> to add subtitless and do only that leaving the sound and picture quality untouched.</p>
","<p>As you are hardcoding subtitles, the video (with the subtitles added) <strong>will be re-encoded</strong>. </p>

<p>You can use the CRF rate control method to modulate the quality of the output.</p>

<p>So, start with</p>

<pre><code>ffmpeg -i grdedFinal.mov -vf subtitles=portSbs.srt -crf 18 -c:a copy gradedFinalwithSubs.mov
</code></pre>

<p>If the quality's not acceptable, lower that value till it is - in exchange for a larger file. Of course, don't decide based on file <em>size</em> but on visual quality. x264 is very good at compressing video while maintaining subjectively perceived quality.</p>
","17549"
"What codecs/formats are appropriate for full motion video recording for youtube?","1123","","<p>I'm recording video game footage via lossless methods, after which I need to compress them and upload them to youtube. I'm not sure which format and/or codecs are best for this. I've been experimenting with various formats and I've found MP4 (and whatever Camtasia 7 uses as a codec for that) to be mostly effective, but the quality varies by game.</p>

<p>Particularly I've noticed that lower-motion games like platforming games (Super Mario) or RPGs (Final Fantasy) seem to be fine no matter what format they're in; few ""blocky"" compression artifacts, almost no issues due to how often key-frames are used. But higher motion games, especially First Person Shooters (Halo) and scrolling shooters (Gradius) suffer relatively severe issues with ""ghost"" images between keyframes and generally blocky or choppy video.</p>

<p><strong>Is there a generally acceptable video codec for high motion gameplay footage?</strong> I want to absolutely minimize compression artifacts like ghost images between keyframes, blocks of colors, etc. To maximize quality my recordings are at 30 FPS, 32 bit RGB at naitive resolution and file size isn't a major limiting factor (but I'd like to keep files under 1 GB due to low upload speeds). So assume the source file is basically pristine. How should I compress these high quality source files of full motion gameplay <strong>to be uploaded on youtube?</strong></p>

<p>To reiterate the general requirements:</p>

<ul>
<li>Lossless or high quality source file at 30 FPS, 32 bit RGB</li>
<li>Full motion recording, minimal compression artifacts wanted</li>
<li>Resolution only limited by the game's res; generally 480p, 720p or 1080p, occasionally weird resolutions when a game requires them (but if those come out less-awesome it's not a big deal, I accept that those are problematic).</li>
<li>Youtube will receive and process the final product, so their processing determines the final quality. I want something that Youtube will butcher the least (their videos do not always match my upload file's quality and I'm not sure why)</li>
</ul>

<p>If it matters I'm using Camtasia Studio to edit/render the video before uploading, and I use a combination of Camtasia Studio and Fraps to record videos. I have the K-Lite Full codec pack on my PC, so Youtube is the major limiting factor for codecs. Fraps records lossless, and Camtasia <em>can</em> be lossless but I've been taking to DivX for encoding the initial recording in  Camtasia. My videos are at the whim of Youtube's re-encoding process for the final product as well, but I'm not sure exactly what they use for that.</p>
","<p>Personally I always choose MP4 container and the H.264 codec as this is also the codec YouTube uses in the final video stream.</p>

<p>What key-frame rates and compression you need really depends on the footage and it's unfortunately close to impossible to give as a generic answer for this reason.</p>

<p>If you have a lot of movements you will need key-frames more often as well as lower compression. This will of course result in larger file sizes as a key-frame is basically a full frame image. There are only costly compromises for this.</p>

<p>How much you need to compress depends on the frame size, the bit-rate that will be used, f.ex. there is no point in compressing at a high bit-rate if YouTube then re-compress to match their max output bit-rate, which again means even worse quality due to re-compression.</p>

<p>If I should recommend a generic compromise it would be to reduce the frame size from 1920 x 1080 to 1280 x 720 instead. A viewer won't notice small details if there are a lot going on in the movie. It of course depends on which eyes one uses, the viewer's eye or the producer's eye. What I'm trying to say is that there is the factor of ""psychology"" you can use as well. </p>

<p>You won't need 32 bits. 32-bit is just 24-bit RGB with 8-bits for an alpha channel. You should record and process in 24-bit in cases as this. This will save you some file size and processing time.</p>

<p>Camtasia uses it's own proprietary codec so you will always need to re-compress to another non-lossy or lossy format.</p>

<p>Basically it comes down to trial and error approach, and then experience from that, as each individual video may require a different approach. I saw a similar post here with a link to YouTube guide-lines. Check them out and use them as a basis for settings (especially bit-rate settings).</p>

<p>Also try not to think of the movie in terms of technical perfection but rather how a viewer will view it and for what reason (is the content interesting enough then quality matters less, think news-reports).</p>
","5320"
"After Effects render on AWS EC2","1119","","<p>I want to render my After Effects projects on an EC2 Windows AMI for better performance, but they only allow Windows Server editions which are no supported by Adobe. Will it work if I install After Effects on Windows Server 2008? Thanks!</p>
","<p>Windows Server is very much the same as the Desktop Windows regarding the core of the OS, there is just some stuff missing and other stuff added that is usefull for webservers.</p>

<p>You shouldn't have any problems installing an After Effects render node, I used Windows Server 2008 R2 myself for an After Effects CS5 network render node.
Shouldn't have changed with newer versions of After Effects.</p>
","12407"
"Cleaning tracked keyframes in After Effects","1114","","<p>I'm new to After Effects and played with Mocha for After Effects a bit (which is v.easy to pick up) and was wondering: If I have a bunch of dense set of keyframes as a result of tracking, is there an After Effects option/script that can clean the data (reduce keyframes but maintain easing/timing) ? 
Is this possible ? If so, how ? </p>
","<p>Just select all the keyframes you want reducing and use the built-in plugin called 'smother' which will convert your set of baked keyframes back into a bezier curve.</p>
","4894"
"How to Overlay Text in Premiere Pro or After Effects?","1113","","<p>I am a newbie to both Premiere Pro and After Effects.  We are making exercise instructional videos and want to overlay text instructions - very much like this video: <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/aea5BGj9a8Y?start=0""></iframe>
            </div></div>. </p>

<p>What would be the recommended way to do this. We have both Premiere Pro and After Effects. </p>

<p>Thanks for any assistance/tips. </p>
","<p><strong>Premiere Pro</strong> have more basic text tools, but it is easier to work with. Also, it is better for long files.</p>

<p><strong>After Effects</strong> allows you to manipulate with your text in much more various ways. You can use there more different text effects. But it is also more complicated.</p>

<p>So, it is up to you to decide. If you are looking for something basic, use Premiere. If you want to use some fancy effects, then use After Effects.</p>

<p>For example typewriter effect, which is used in video in your question is one of the standard AE effects. You can also <a href=""https://www.youtube.com/watch?v=s6mbBnaO-DU"" rel=""nofollow"">achieve that with Premiere Pro</a>, but you will spend more time on each text. </p>
","17574"
"How to achieve this kind of masks and transitions","1111","","<p>I've come across <a href=""http://vimeo.com/88627743"" rel=""nofollow"">this video</a> and, as a total noob in video editing, I find it stunning. After watching it a couple of questions arise regarding masks and transitions:</p>

<ul>
<li>The intro title <em>fills</em> with a smoke-like mask. Where can I find a mask like that, if possible for free?</li>
<li>The subtitle and the rest of the titles appear with each letter growing independently. I guess that could be done using a Stroke for each letter and animating it? Seems like a lot of work. Isn't there an automated process for that?</li>
<li>The transitions in 0:16, 0:49 and 1:05. How are those done?</li>
</ul>

<p>I'd also appreciate any kind of basic tutorial about color correction to achieve the same looks as that video.</p>
","<p>Let me start by saying what they did there is actually far, Far, FAR simpler than you think.  I'll try to break it down shot by shot what is going on.</p>

<p>On the title, there are three main elements.  The first element fades in on a time lapse of clouds moving across the mountain.  This appears to be the same time lapse they use a few seconds later for the first subtitle.  </p>

<p>Superimposed over that are two elements.  The first and faintest is a fairly small image of a cloud.  It appears to simply be luma keyed or possibly additively blended (bright parts make bright, dark parts don't impact) and probably has a black background  It's quite probably a CG cloud using particle simulation.  </p>

<p>The last element of this first shot is a title composed of white text.  The text is blended in using a gradient dissolve that bases the time it dissolves being driven by the luminance of an underlying image.</p>

<p>As a demonstration, lets start with <a href=""http://www.flickr.com/photos/41107941@N00/3457064913/in/photolist-6gumXt-6guohB-6guoSz-6gupCx-6gypPS-6gyuEf-6gywjA-6gyCcs-6uV2p1-6yggUd-6yp3cr-6ytc5N-6DdxdM-6DhFqC-6HkgLN-6MsbWQ-6Pt9aZ-6TZ3Mv-6VMB6T-6VRCQC-6XwcxD-77adrR-783nYh-79UJgB-7bfATD-7cS8sT-7m5PXk-7nL1wa-7weRvZ-7xYqP5-hTVxhW-e1TJrt-bCgBAv-drYF5S-bVDGTM-dPWAgT-dPWzYk-dzDbgz-aSrq4T-as7pHU-as7pNN-8UoJAp-8UoCt4-8UoCna-dn1pch-8UrGVs-bjipD6-fQwnYj-bv7uaZ-9cX4a9-ffZHWc"" rel=""noreferrer"">this</a> CC image from centophobia on flikr.  With some quick rotating and additive blending (Lighten) in Photoshop, applying a greyscale mode change and then using the curves to give us contrast more suitable to our purposes we get an image like this (after resizing and cropping to fit a 1080p video).</p>

<p><img src=""https://i.stack.imgur.com/mBD10.jpg"" alt=""Title Dissolve Image""></p>

<p>Now that we have an image to use for the base of our dissolve, we can actually do the title.  Here is a screenshot from doing this in Premiere.</p>

<p><img src=""https://i.stack.imgur.com/BjsaW.jpg"" alt=""Screen shot of building the gradient title""></p>

<p>First, I imported the image I created and then I made a basic title with plain white block letters.  I then added both of these to a sequence.  Note that the smoke layer should be in the sequence with the title, but it should not be visible.  </p>

<p>Next, I added a Gradient Dissolve to the title clip from the Transition Effects.  Set the Gradient Layer to the layer you put your gradient map on (the smoke image.)  </p>

<p>Next, adjust the settings to add softness to the transition (this is what causes it to work through the gradient rather than having a hard line being exposed).  </p>

<p>Set a keyframe on the transition completion at the beginning and end of where you want it to dissolve in (using the little stopwatch looking button) and set a keyframe at 100 for the beginning and 0 for the end.</p>

<p>When you run it you should get something like this:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/RcR3foXldOE?start=0""></iframe>
            </div></div></p>

<p>Now for the other titles, it does appear to clearly be working off the strokes of the characters, so there are a few different possibilities.  The most likely one would be to use a similar gradient dissolve, but build the mask based directly on the title.  Drawing a gradient on each character would allow for the control of the reveal to be precisely controlled and the Transition softness would be set to 0 or near 0 to keep a hard line.</p>

<p>As an example, here's doing something for a large letter K.</p>

<p><img src=""https://i.stack.imgur.com/UkEWP.jpg"" alt=""Gradient to reveal K""></p>

<p>They may have had a titling software that could generate the gradients for them.  Stroke animation is another option, but you'd have to use true vector shapes for the fonts then and a lot more manual work would have to be done to keyframe each different part of each stroke.  The gradient way isn't particularly quick, but it's probably less effort than stokes.</p>

<p>The final transition appears to just be a  highly motion blurred image flying across the frame really quickly and having the other clip after it clears the field.  Just take an image with a lot of horizontal blur, send it across within a few frames and cut to the other video the moment it completely obscures the scene.</p>
","10394"
"Using ffmpeg with avfoundation, capture_cursor and capture_mouse_clicks options are ignored","1106","","<p>Situation: cash-strapped ivory tower visionary and ffmpeg newbie manacled to a 2nd hand 2010 Macbook Pro concealing a 'Black Screen of Death' NVIDIA GeForce GT 330M graphics chip.</p>

<p>With the help of the gfxCardStatus app (option 'Integrated Only' selected), I overcame this particular Apple quality control failure to record fullscreen video and at full resolution, using the following command:</p>

<pre><code>ffmpeg -video_size 1680x1050 -framerate 30 -f avfoundation -i ""1"" -c:v libx264 -qp 0 -preset ultrafast -capture_cursor 1 -capture_mouse_clicks 1 capture.mkv
</code></pre>

<p><strong>The capture_cursor and capture_mouse_clicks options, however, are steadfastly ignored.</strong></p>

<p>Aside from these omissions, ffmpeg is recording at a quality and smoothness better than any of the screencapture/screencast apps I've tried, so for once I'm pretty sure this has nothing to do with the crap hardware, but rather with my use of ffmpeg.</p>

<p>There is nothing to suggest the installation/compilation process went wrong, for which I used:</p>

<pre><code>$ brew install ffmpeg --with-fdk-aac --with-ffplay --with-freetype --with-frei0r --with-libass --with-libvo-aacenc --with-libvorbis --with-libvpx --with-opencore-amr --with-openjpeg --with-opus --with-rtmpdump --with-schroedinger --with-speex --with-theora --with-tools --with-libavformat --with-OpenCV --with-libx264
</code></pre>

<p><strong>Is there something I'm overlooking, but which is needed for capture_cursor and capture_mouse_clicks to work? Compilation parameter? ffmpeg option order? Some OS flag setting?</strong></p>

<p><strong>Operating System</strong>
 - OS X Yosemite
 - Version 10.10.5 (14F1021)</p>

<p><strong>Hardware:</strong></p>

<ul>
<li>MacBook Pro (15-inch, Mid 2010)</li>
<li>Processor 2.8 GHz Intel Core i7</li>
<li>Memory 8 GB 1067 MHz DDR</li>
<li>Graphics Intel HD Graphics 288 MB</li>
<li>NVIDIA GeForce GT 330M</li>
</ul>

<hr>

<p><strong>Addendum</strong></p>

<p>Here the ffmpeg video capture command usage, following the guidelines provided by @Mulvya:</p>

<pre><code>$ ffmpeg -video_size 1680x1050 -framerate 30 -f avfoundation -capture_cursor 1 -capture_mouse_clicks 1 -i ""1"" -c:v libx264 -qp 0 -preset ultrafast capture.mkv
</code></pre>
","<p>Place those options <strong>before</strong> <code>-i ""1""</code></p>

<p>In ffmpeg, parameters and flags apply to the next input/output entry.</p>

<p>So,</p>

<pre><code>ffmpeg {-flags for input1} -i input1 {-flags for input2} -i input2 {-flags for output1} output1 {-flags for output2} output2
</code></pre>
","17197"
"Auto ISO for shooting videos with DSLR","1102","","<p>I'm just starting shooting videos with a DSLR (5DMIII). I already have a good background in photography. I was wondering if Auto ISO is good when shooting a video or if it is better to tune ISO manually. </p>

<p>It be nice to hear some thoughts from more experienced people. </p>
","<p>It depends on what you are doing, but generally for video you want pretty tight manual control to avoid changes during a shot.  What I often do when I'm in a hurry (and don't want to determine optimum settings manually) is to let the camera figure out what it wants for settings and then manually enter those.</p>

<p>That said, while changes in exposure during a shot can be very distracting, it's still better to get the shot than not get it, so occasionally I may shoot with the camera fully on auto and use exposure compensation as necessary to adjust the exposure.  This can be particularly useful for live events where you don't have time to adjust manually.  Note that if you are shooting with multiple identical cameras however, it is best to lock everything down (including white balance) to ensure a consistent look between cameras.  </p>

<p>Ultimately, there isn't a right answer, just be aware of how distracting changes in exposure can be during videos and balance the need to have the camera adapt vs the need to control when and how adjustments are being made for the content you are shooting.  It's best to try to avoid uncontrolled changes, but not at the expense of not getting a shot.</p>
","8238"
"Handbrake x265 Optimal settings for ripping DVD","1088","","<p>I want to rip DVDs(MPEG2) to MKV using Handbrake for viewing on PC, and currently i am not sure about the settings of the x265 encoder.
Can anyone suggest the optimal settings for:</p>

<ul>
<li>CRF : 19 ? 23 ? 25 ?</li>
<li>8-bit or 10-bit (Main or Main10) (Has 10-bit any advantages for low-res like DVD ? )</li>
<li>Encoder tune - none / SSIM / Grain ?</li>
<li><code>bframes</code> or other parameters ?</li>
<li>CFR or VFR ?</li>
</ul>

<p>Also i am open to any suggestions for tuning of an other parameteres.
I have searched by google but all guides are not specific enough for the case of DVDs.</p>

<p>The resolution is 700x574 , source MPEG2 bitrate is 3Mbps</p>

<p>Additional question - are there any good guides for general parameter tuning x265 like <code>bframes</code> ?
Thank you very much.</p>
","<p>What does optimal mean to you? Smallest size and time to finish an encode does not matter? If so then pick the H.265 MKV 576p25 profile (apparently you have PAL content) set ""optimal for source"" under dimensions to not stretch or modify the picture in any way. Set the refresh rate to ""same as source"". CRF 20 should be good enough for most DVDs, you will know when you compare a CRF 17 encode of the same source and cannot make out the difference, if you can try the numbers in between or going even 3 steps further down. 10-Bit won't hurt, but won't benefit much either regarding picture quality, if it would and you're doing lots of filtering, you could even go with 12-Bit (I have it available on Ubuntu, the choice of filters though is limited and won't improve picture quality dramatically I'd say, nor would it justify changing the subsampling if it is availabe). Change the preset from slow to slower if you have no problem waiting for the encode to finish, this is one of the more important settings as it controls all the other setting depending on the profile, level and picture size. Disabling bframes was only imporant for older devices which had trouble decoding H.264 streams with bframes, since you said you care mostly about the result when watching H.265 content on a PC this shouldn't be of any concern.</p>

<p>Simply put, if the resolution is not greater than 1080p60 and playback on current mobile devices is not considered to be important then I choose H.265 with slow or slower on Main10 and 4:2:0 subsampling. That's optimal regarding of what most desktop computers can handle today, some may require video acceleration though to display noisy 1080p 60fps content (e.g. performers dancing in front of big LED displays). That's may experience so far and I never touch the encoder tune setting, I expect the default to be good enough when I'm already throwing this much CPU time at it that the tune may be negligible.</p>
","21871"
"rendering with the video card in MeGUI","1087","","<p>I do not know this is the right forum to ask for help.</p>

<p><strong>I need help to speed up rendering with the software MeGUI 2500.</strong></p>

<p>First I render to premiere with the help of CUDA.<br/>
<em>CUDA makes the video card also yield, speeding too.</em></p>

<p>I put the second video has already ended in <strong>MeGUI 2500 with AVS scriptcreator</strong> decrease the size without affecting the quality too. Only in this software (MeGUI) did not find how to enable the video card to render.</p>

<p><em>If you have no way someone could tell me other compression software that helps with the video card? Why premiere is not very good at it.</em></p>
","<p>If I understand you correctly, it sounds like you are trying to find a piece of software that can use hardware acceleration to encode video generated by AVISynth.  If that's the case, I'd suggest looking at Handbrake, which is capable of utilizing GPU acceleration and should handle AVISynth sources just fine.</p>
","11946"
"Canon T3i with Blackmagic Design Intensity to record plays?","1086","","<p>My school's drama department wants to record their shows. As the school only has really old cameras, the recordings look like garbage. I have a Canon T3i but the recordings stop at 30 minutes. This is where the Blackmagic Design Intensity products come in. The Intensity products can record from an HDMI input. So, in theory, I can connect my T3i's HDMI out to this and record the show on the computer. My only concern is the time. Our shows are about 2 hours. Would this cause the camera to overheat? Would this even work at all?</p>
","<p>The problem is that the HDMI output on your T3i is not clean.  You would still have all the UI from the camera interface placed on it.  Higher end cameras (like the 5D mark iii) and some other makes have a clean output option, but I don't believe the T3i does.</p>

<p>You can use the third party MagicLantern firmware to force the interface clean, but at that point you can also have it automatically start a second recording after the first ends and bypass the 30 minute limit altogether.</p>

<p>As far as sensor overheating, it really depends on the camera model and the temperature in the environment you are shooting and how fast it dissipates waste heat.  MagicLantern generally displays an indicator of the temperature of the sensor with some color coding as to how hot it actually is.  You could try using ML and see how your camera handles it, but it may very well run too hot when trying to do two hours at a time.  You won't know for sure until you try though.</p>

<p>As others have noted, ML may or may not void your warantee.  It technically isn't a full third party firmware, but rather just runs on top of the underlying OS that is built in, but it can tweak settings that aren't normally tweaked internally.  There are plenty of cases of people getting warranty replacements on cameras that had had ML installed, but you should be aware that it is not a completely risk free process and in rare cases ML has caused cameras to become unresponsive (or even brick in exceptionally rare cases).</p>
","13339"
"Scroll text left to right and back","1086","","<p>I am using this line which works fine for showing text in 3 lines starting at 0.3sec, ending at 0.6 sec. in bottom left corner.</p>

<p>I would like to add scroll in and scroll out effect in ffmpeg.</p>

<p>Both in and out should be fast, lets say .05sec.</p>

<p>Scroll in should end in position like its written in code, remaining there for those 3 sec, then scrolling out.</p>

<p>ffmpeg -i scrollin.mp4 -vf ""[in]drawtext=enable='between(t,3,6)':fontsize=50:fontcolor=Green:fontfile='ariblk.ttf':text='textline1':x=10:y=h-th-130, drawtext=enable='between(t,3,6)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline2':x=10:y=h-th-75, drawtext=enable='between(t,3,6)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline3':x=10:y=h-th-20[out]"" -codec:a copy scrollout.mp4</p>
","<p>Use this filter expression:</p>

<pre><code>""drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=Green:fontfile='ariblk.ttf': \
text='textline1':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-130""
</code></pre>

<p>The above is for the first line. I've changed the values/expression for the <code>enable</code> and <code>x</code>parameters. The movement starts at 2.5 seconds, settles at 3s, stays till 6s and then disappears by 6.5</p>
","17515"
"What Happens to Frequencies Above the Nyquist Limit?","1080","","<p>If I record a sound at a rate of 8,000 samples per second, I know that the highest frequency that can be reproduced from the sound is 4000Hz. What I want to know is:</p>

<p>If the sound contains frequencies above 4000Hz, when I play the sound back what would I hear and what frequencies would be produced at the points where the frequency of the original sound is greater than 4000Hz.</p>
","<p>Sounds above the Nyquist limit (in this case 4000 Hz) will seem to fold back into the allowed range. For example, a tone 100 Hz above the limit will appear as a phantom tone 100 Hz below it. So a frequency of 4100 will seem to appear at 3900, one at 4200 will appear at 3800, and so on.</p>

<p>This is called 'aliasing' or 'folding'. The apparent 3900 tone is an alias for the 4100 tone. Since actual, complex sounds are composed of many frequencies, the audible effect is not as clean as theory describes it.</p>
","7774"
"how to convert a video to black and white color without any shades","1072","","<p>How can a video be converted to black and white color. It should not have any shades of gray color. Only two colors - black and white.</p>

<p>There are many options to convert colored video to grayscale - which will have various shades of black. But I want video to only contain two colors.</p>

<p>Is there any source where such videos can be found?</p>
","<p>I'm only familiar with Vegas Pro, but I think many NLEs will do the same.</p>

<p>Begin by applying the desaturate effect or the 100% black and white effect. Then engage the Brightness and contrast effect, adjust the contrast until you have only black and white, you may need to toy with the brightness as well but not much.</p>
","4995"
"How to create Apple Keynote style Picture in Picture video","1072","","<p>I want to take two video sources:</p>

<ul>
<li>A screencast of a PowerPoint presentation</li>
<li>A camcorder AVCHD recording of the presenter and the conference room</li>
</ul>

<p>..and create a video in the style of Apple's Keynote videos. See here for exactly what I'm looking for:</p>

<p><a href=""http://i.i.com.com/cnwk.1d/i/tim/2012/06/12/apple-wwdc-video.jpg"" rel=""nofollow"">http://i.i.com.com/cnwk.1d/i/tim/2012/06/12/apple-wwdc-video.jpg</a></p>

<p>The video feed of the PowerPoint itself would take up the majority of the screen and the video recording of the room/speaker would take up another, smaller, separate area of the screen.</p>

<p>I'd appreciate any advice on:</p>

<ul>
<li>What software can create this dual video effect?</li>
<li>What method/software would you recommend for creating the screencast?</li>
</ul>

<p>Thanks for your time.</p>
","<p>Any fairly basic video editing package should be able to closely emulate the look.  You just have to be able to distort the video to make it fit the shape that looks like it is 3d and then size and position the speaker's video appropriately.  I'd personally probably do it in After Effects with a 3d layer for simplicity, but most far cheaper consumer video editing packages should be able to do basic deformations to get the same look.</p>
","8147"
"Sony TV RCA output into Yamaha HS80M input via RCA male to XLR male: Is it safe?","1067","","<p>Title says it all really. Just want to check up before I do this.</p>

<p>I have a spare set of <a href=""http://usa.yamaha.com/products/music-production/speakers/hs_series/hs80m/?mode=model"" rel=""nofollow noreferrer"">Yamaha HS80M</a> monitors lying around. Rather than just having them sitting on the shelf, I'd like to connect them to a Sony LCD TV directly.</p>

<p>Is this safe? Is it advisable?</p>

<p>I plan to use a cable that looks something like this:</p>

<p><img src=""https://i.stack.imgur.com/EMQYo.jpg"" alt=""RCA male to XLR male""></p>

<p>I realize the signal isn't balanced. I'm really most concerned as to whether this is in some way dangerous.</p>

<p>Cheers.</p>
","<p>Not dangerous at all. You may have impedance matching issues though. And possibly lots of noise in the signal because of the unbalanced source and mis-matched impedance.</p>

<p>Start with the volume on the Yamahas all the way down and, with something playing on the television, nudge the volume up ever so slightly until you can hear the source. You want to make certain a too-hot signal from the TV doesn't rip your speakers apart.</p>

<p>For best results a DI with properly matched inputs for the TV would be my advice. Something like <a href=""http://www.radialeng.com/re-proav1.htm"" rel=""nofollow"">this</a> would do the trick.</p>
","1489"
"Each media segment file for MPEG-DASH is a full (.mp4) instead of fragment mp4 (.m4s)?","1064","","<p>I'm wondering if I can just make every media segment files as a full MP4 file instead of fragmented MP4 for a DASH264-compliant player?</p>
","<p>After studying the DASH spec, I'd say the fragmented MP4 is the must for media segment files. Therefore, it is not possible to use a full (or non-fragmented) MP4 in DASH.</p>
","22551"
"Entire Track Velocity in Sony Vegas","1057","","<p>I have created a video+audio clip in Sony Vegas. After the work was completed, I decided to increase velocity a little bit (final percentage ~95% original).</p>

<p>In the project, I have several audio and video tracks, each of which obviously has multiple events.<br>
I can <kbd>Ctrl</kbd>+<kbd>Mouse Drag</kbd> each individual <strong>event</strong>, but it seems even impossible to do the same when several events selected (only one event changes).</p>

<p>For both audio and viodeo, I need only velocity changed (<strong>time stretch</strong>, but no audio pitch shift), and all event points (transitions etc) change accordingly.</p>

<p>For audio, it seems possible to go to <em>Master FX</em> and add <em>Time Stretch</em> FX, but it only affects audio.</p>

<p><strong>How can I change velocity for each individual track (or, well, an entire project) with minimum hassle?</strong></p>

<p>P.S. Sony Vegas Pro 12.0, Windows 7.</p>
","<p>The best way to increase the playback rate for the entire project is to use nested projects: Start new project and add your original project to the timeline as an event. Then Ctrl + Mouse Drag. </p>
","5649"
"ffmpeg: white padding is light grey but not white","1057","","<p>I want to generate 16:9 thumbnails from videos with white padding but the resulting picture has grey padding instead of white. This is the command line (input here is a jpg but the effect is the same when using a video):</p>

<pre><code>ffmpeg.exe -i ""https://gooseberry.blender.org/wp-content/uploads/2014/01/franck.jpg"" -filter_complex scale=iw*min(852/iw\,480/ih):ih*min(852/iw\,480/ih),pad=852:480:(852-iw*min(852/iw\,480/ih))/2:(480-ih*min(852/iw\,480/ih))/2:white,fps=fps=50/5.000000 output.jpg
</code></pre>

<p>the resulting image (text and white box added with paint): 
<a href=""https://i.stack.imgur.com/jNMQp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jNMQp.jpg"" alt=""enter image description here""></a></p>

<p>even though the padding color is white it's light grey in reality. Is there a way to get it 'whiter'? It's especially ugly if you plan to print it on paper. </p>
","<p>The end of the filter chain should output full RGB, like this:</p>

<p>Windows batch:</p>

<pre class=""lang-bash prettyprint-override""><code>ffmpeg.exe -i ""https://gooseberry.blender.org/wp-content/uploads/2014/01/franck.jpg"" ^
  -filter_complex ^
   scale=iw*min(852/iw\,480/ih):ih*min(852/iw\,480/ih),^
pad=852:480:(852-iw*min(852/iw\,480/ih))/2:(480-ih*min(852/iw\,480/ih))/2:white,^
format=rgb24 franck-padded.jpg
</code></pre>

<p>Linux bash:</p>

<pre class=""lang-bash prettyprint-override""><code>ffmpeg -i ""https://gooseberry.blender.org/wp-content/uploads/2014/01/franck.jpg"" \
  -filter_complex \
   ""scale=iw*min(852/iw\,480/ih):ih*min(852/iw\,480/ih),\
pad=852:480:(852-iw*min(852/iw\,480/ih))/2:(480-ih*min(852/iw\,480/ih))/2:white,\
format=rgb24"" franck-padded.jpg
</code></pre>

<p>The important part is <strong>format=rgb24</strong> which keeps or converts the final filter output to the RGB pixel format, which allows full range values from 0-255. Without that final filter, ffmpeg will convert the filter output to the YUV pixel format where the range of legal values is 16 to 235 for luma (brightness) - 16 maps to black and 235 to white - and 16 to 240 for the chroma components (color). (The default YUV <a href=""https://en.wikipedia.org/wiki/Color_space"" rel=""nofollow noreferrer"">colorspace</a> range is a legacy of analog television signals and carried over into the digital realm by the MPEG committee and hence still widely adhered to by most digital video formats)</p>

<p><a href=""https://i.stack.imgur.com/d66Dd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d66Dd.jpg"" alt=""enter image description here""></a></p>
","16686"
"Issues looping a stream with FFMpeg","1056","","<p>I'm trying to overlay a video on top of an image. The problem that I'm facing is that the video duration that I want is longer than my input video's duration.</p>

<p>To solve this issue, I try to generate a streaming loop and hardcode the duration that I want using the parameter ""t"".</p>

<p>I'm using the setpts filter to make sure that my container format (.mp4) accepts past frames by rewriting their timestamps:</p>

<pre><code>ffmpeg -loop 1 -i image.jpg -stream_loop -1 -i video.mov  -filter_complex ""[1:v]setpts=N/(FRAME_RATE*TB)[1v];[0:v][1v]overlay=x=main_w*0.44:y=main_h*0.33[out]"" -map [out] -t 00:00:15  out.mp4
</code></pre>

<p>So the command reads as follows:</p>

<ol>
<li>take one infinite input where each frame is image.jpg</li>
<li>take another infinite input (video.mov) by looping (-stream_loop -1)</li>
</ol>

<p>Each frame will go through the filter:</p>

<ul>
<li>setpts will re-write the timestamps for source 1 (video.mov)</li>
<li>overlay will put each setpts-ed frame from video.mov on top of each frame from the initial input (image.jpg - 0:v)</li>
</ul>

<p>This should continue until a file with 00:00:15 seconds is created.</p>

<p>This command does generate that file but the video.mov isn't looped, once the first loop finishes the last frame is repeated until the output movie ends.</p>

<p>FYI: I made a couple more tests:</p>

<pre><code>ffmpeg -stream_loop -1 -i movie.mov -filter_complex ""[0:v]setpts=N/(FRAME_RATE*TB)[out]"" -map [out] -t 00:00:30 output.mp4
</code></pre>

<p>This works fine, it generates a movie with 30 seconds duration with the input movie looped to fill the output movie</p>

<p>on the other hard, if I add one more input file:</p>

<pre><code>ffmpeg -stream_loop -1 -i movie.mov -loop 1 -i image.jpg -filter_complex ""[0:v]setpts=N/(FRAME_RATE*TB)[out]"" -map [out] -t 00:00:30 output.mp4
</code></pre>

<p>This will not create a movie with 30 seconds, although one of the inputs is never used, ffmpeg creates a movie file with just one loop of movie.mov and the same length of movie.mov.</p>

<p>Any advices please ?</p>

<p>Thank you very much!</p>

<p>Ze</p>
","<p>Try with the movie filter instead</p>

<pre><code>ffmpeg -loop 1 -i image.jpg -filter_complex \
        ""movie=video.mov:loop=999,setpts=N/(FRAME_RATE*TB)[1v]; \
        [0:v][1v]overlay=x=main_w*0.44:y=main_h*0.33[out]"" \
       -map [out] -t 00:00:15  out.mp4
</code></pre>
","18272"
"Record raw video with ffmpeg keeping the full color range","1056","","<p>I want to do video captures with a video4linux2 device.</p>

<p>The video data contains a lot of darker shades of grey.<br>
When I capture a single PNG file, everything is fine.<br>
Capturing with ffmpeg/avconf using x264 truncates my color space to 16..235.</p>

<p>I tried to define <code>-color_range 2</code> but it didn't have any effect.<br>
I also played with the different pixel formats (yuv444p, yuv420, ..) --without success.</p>

<p>This is the command line which produces the reduce color range video:</p>

<pre><code>$avconv -f video4linux2 -i /dev/video0 -vcodec libx264 -pix_fmt yuv444p -color_range 2 -crf 14 -t 120 -y out.mp4

avconv version 0.8.17-4:0.8.17-0ubuntu0.12.04.1, Copyright (c) 2000-2014 the Libav developers
  built on Mar 16 2015 13:26:50 with gcc 4.6.3
[video4linux2 @ 0x2044b60] Estimating duration from bitrate, this may be inaccurate
Input #0, video4linux2, from '/dev/video0':
  Duration: N/A, start: 1445936359.182177, bitrate: 890634 kb/s
    Stream #0.0: Video: rawvideo, yuyv422, 1280x720, 890634 kb/s, 60.40 tbr, 1000k tbn, 60.40 tbc
[buffer @ 0x20450a0] w:1280 h:720 pixfmt:yuyv422
[format @ 0x2046d20] auto-inserting filter 'auto-inserted scaler 0' between the filter 'src' and the filter 'Parsed filter 0 format'
[scale @ 0x2047660] w:1280 h:720 fmt:yuyv422 -&gt; w:1280 h:720 fmt:yuv444p flags:0x4
[libx264 @ 0x2045fe0] using cpu capabilities: MMX2 SSE2Fast SSSE3 FastShuffle SSE4.2 AVX
[libx264 @ 0x2045fe0] profile High 4:4:4 Predictive, level 4.0, 4:4:4 8-bit
[libx264 @ 0x2045fe0] 264 - core 120 r2151 a3f4407 - H.264/MPEG-4 AVC codec - Copyleft 2003-2011 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=12 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=0 b_adapt=1 b_bias=0 direct=1 weightb=0 open_gop=1 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=14.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.25 aq=1:1.00
Output #0, mp4, to 'out.mp4':
  Metadata:
    encoder         : Lavf53.21.1
    Stream #0.0: Video: libx264, yuv444p, 1280x720, q=-1--1, 302 tbn, 60.40 tbc
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo -&gt; libx264)
</code></pre>

<p>My last tries were to define a color matrix to manually scale 0..255 to 16..235, so that I will end up with a slightly compressed, but a least complete color space.</p>

<p>But this resulted in an error from avconf</p>

<pre><code>$ avconv -f video4linux2 -i /dev/video0 -vcodec libx264 -pix_fmt yuv444p -color_range 2 -vf 'format=yuv444p,colormatrix=bt709:bt601' -crf 14 -t 120 -y out.mp4

avconv version 0.8.17-4:0.8.17-0ubuntu0.12.04.1, Copyright (c) 2000-2014 the Libav developers
  built on Mar 16 2015 13:26:50 with gcc 4.6.3
[video4linux2 @ 0xabbb60] Estimating duration from bitrate, this may be inaccurate
Input #0, video4linux2, from '/dev/video0':
  Duration: N/A, start: 1445936333.196864, bitrate: 890634 kb/s
    Stream #0.0: Video: rawvideo, yuyv422, 1280x720, 890634 kb/s, 60.40 tbr, 1000k tbn, 60.40 tbc
[buffer @ 0xac0ee0] w:1280 h:720 pixfmt:yuyv422
No such filter: 'colormatrix'
Error opening filters!
</code></pre>

<p>Do you know how to get ffmpeg to encode the full color range?</p>
","<p>In this case, the problem didn't come from ffmpeg/avconv.</p>

<p>The rawvideo source I use offers different color formats to be used:</p>

<ul>
<li>argb32</li>
<li>rgb24</li>
<li>bgr24</li>
<li>yuv444</li>
<li>yuv420</li>
<li>[...]</li>
</ul>

<p>When I extract a png file (which was fine), the bgr24 color format is used.
When I capture a video, avconv automatically chose one of the yuv formats.
I guess the clamping to (16..235) did happen in the driver itself, that's why I couldn't influence it.</p>

<p>By giving the parameter <code>-input_format bgr24</code> to the input device, I chose the unclamped value range and the capturing worked fine.</p>

<p>Final command line, with <code>yuv420p</code> as target format which produces even a Windows Media Player complatible output:</p>

<pre><code>avconv -f video4linux2 -input_format bgr24 -i /dev/video0 -vcodec libx264 -pix_fmt yuv420p -color_range 2 -crf 14 -t 120 -y out.mp4
</code></pre>
","16718"
"Windows software for recording multiple video/audio sources?","1052","","<p>I'm looking into options for recording video game sessions, including multiple different video and audio sources. I'm only at the planning stage of this, but here is basically what I want to set up:</p>

<ul>
<li>Have PC set up as recording hub for gameplay sessions either on same PC or on nearby game console</li>
<li>Capture game footage, both audio and video, either capturing PC game footage or via capture hardware for console games (PCI or USB connection)</li>
<li>Capture footage of players, probably via webcam attached to PC</li>
<li>Capture both game and players feeds simultaneously</li>
</ul>

<p>I'm fairly new to a lot of this stuff, so I'm not particularly sure what I'm looking for here. Can anyone provide some recommendations of software/hardware that would work for what I'm trying to do?</p>

<p>UPDATE: I've just discovered Telestream's <a href=""http://www.telestream.net/wirecast/overview.htm"">Wirecast</a>. It seems to be the sort of software I'm looking for, and I can use it to stream or save it to disk. It also has support for some Blackmagic Intensity capture hardware, which I was already looking at. Has anyone had experience with it or seen a similar, possibly cheaper alternative?</p>
","<p>If it were me, I would try using a video recording interface with RCA connectors, as most TVs have an RCA audio/video output. Most of these connect via USB and come with some sort of software. For the video of the players, use a second software program and webcam/etc. to capture this. If you had a Mac, this could probably be sync'd in Final Cut Pro. I'm not familiar with all the software for Windows. </p>

<p>The main part I don't think you'll be able to accomplish (without expensive software) is capturing all of this in-sync with each device. This would require some sort of video-switcher software, similar to what TV stations use for live broadcasts. The method above is probably your cheapest route, if you can handle syncing it after the fact.</p>
","7684"
"How can i achieve the effect of this video, focused in the foreground with a blurred background?","1040","","<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/KsSPodtCD7g?start=0""></iframe>
            </div></div></p>

<p>What type of camera or software, do I need to achieve that?</p>
","<p>You need a camera with a relatively large sensor and a fast lens.  What you are seeing is actually a property of optics called Depth of Field.  Only a certain distance from the camera can be in focus at any given point for a given aperture and focal length.  The degree of how out of focus it appears is also relative to the size of the image being projected (thus the distance from the point where the image inverts) and thus, larger sensors show more blur.</p>

<p>Cheap cameras often use very small sensors and thus have a hard time getting a shallow depth of field, however any DSLR will achieve it quite easily, as will any more professional video camera with a larger sensor.  Shoot with the aperture as wide open as possible and try to have the camera relatively close to the subject with the background as far away as possible.</p>

<p>Technically, increasing focal length will also decrease the depth of field, but then you have to move back from the subject which will increase it again.  You want a small depth of field close to the camera so that the background can be significantly out of focus.  If it is too out of focus, you can adjust the aperture or the ratio between the camera, the subject and the background to make it less blurry (by expanding the depth of field).</p>

<p>You can also use many online Depth of Field calculators to figure out a shooting situation with a particular camera prior to trying it in real life.  They won't visualize it for you, but they will give you an idea of what distances will be in focus and which will be out.  There are also lots of great articles on depth of field over on our <a href=""http://photo.stackexchange.com"">Photography</a> sister site, though they don't deal with video specifically, but the principal is the same.</p>

<p>In a pinch, you can fake it as well using masks and blurs, but that's really not an ideal way to do it.  In camera always looks better.</p>
","9098"
"What are some free (or cheap) options for hosting home videos that have copyright music in them?","1040","","<p>I uploaded some of my home movies to YouTube for family members to watch but there were a few songs in there which YouTube said were copyrighted. The only music in there was the theme song from the old Mission Impossible but there's not arguing with Google. So they muted the whole video.  Fine, thems the rules.</p>

<p>Are there any other options for hosting a video like that for private viewing (even though I had the Youtube video set to ""private"")  and thus allow copyright music to be in the video?</p>

<p>(I assume that putting copyright music into a home movie, not for distribution beyond my immediate family falls under Fair Use and that the issue is that YouTube is under a lot of pressure to avoid copyright issues... but I'm hot trying to broadcast this to 100's of people. Maybe 10 or so. But just trying to avoid burning DVDs for them.</p>
","<p>On youtube, I would just contest the copyright holders.</p>

<p>For it to be fair use you need to be able to ask these 4 questions (not all 4 need to be good to be fair use):</p>

<p><strong>1. What are you doing with the copyrighted content?</strong> If you are doing something highly transformative with the content then you will have more room under the fair use doctrine.  You are more likely to be covered if you are saying something quite different from what the original creator was trying to say.</p>

<p><strong>2. What is the nature of the copyrighted content you are using?</strong> Use of creative or fictional content (for example, a film or cartoon) is less frequently allowed under fair use than less creative, non-fictional material.</p>

<p><strong>3. How much of the original content are you using?</strong> You should be careful to use a reasonable amount.  Just use enough of the copyrighted content as you need to in order to get your point across.</p>

<p><strong>4. Will your work serve as a substitute for the original?</strong> If your video will take away views or sales from the original then it is less likely to be covered under fair use.  Additionally, you shouldnt create work that occupies markets that copyright owners are entitled to exploit.</p>

<p>The thing you need to realize is EVEN if you are fair use, the copyright holders can still sue you and the courts will then need to decide if your usage of the material constitute fair use. Home movies are protected by fair use. Feel free to contest them, but again depending on length and use, see court case of a 29 second home video, <a href=""http://en.wikipedia.org/wiki/Lenz_v._Universal_Music_Corp."" rel=""nofollow"">Lenz vs Universal</a></p>

<p>Now the other issue is that contentID system was abused by the rights holders and there recently has been pushback by youtube.</p>

<p>""YouTube also is noting that it had updated its ContentID tools to try to avoid some of the excessive takedowns associated with it. For one thing, it appears that ""certain rightsholders"" are being asked ""to perform in-depth audits of their references before they can make any new claims."" Reading between the lines, that sounds like YouTube is hitting back at rightsholders who have abused ContentID. That sounds good, though we'll have to see how it plays out in practice. YouTube is also making it easier to pull out incidental audio in videos that might trigger a ContentID claim, improving the way MCNs can ""fast track"" a response to claims they think are bogus, and is promising to more aggressively investigate ContentID abuse.""</p>

<p>Source: <a href=""https://www.techdirt.com/articles/20140326/08003126688/youtube-finally-admits-it-totally-screwed-up-rolling-out-contentid-to-multi-channel-networks-trying-to-improve-tools.shtml"" rel=""nofollow"">https://www.techdirt.com/articles/20140326/08003126688/youtube-finally-admits-it-totally-screwed-up-rolling-out-contentid-to-multi-channel-networks-trying-to-improve-tools.shtml</a></p>
","12143"
"4x resample videoframes using ffmpeg","1038","","<p>I need to speed up video by a factor of 4x, but do not want to drop the frames and instead average them. I.e. each 4 frames are averaged, and produce 1 output frame. Target frame-rate stays the same, so video is now 4 times faster. </p>

<p>Is there a way to do so in ffmpeg?</p>
","<p>ffmpeg with this filterchain does it:</p>

<pre><code>ffmpeg -i input -vf ""tblend=average,framestep=2,tblend=average,framestep=2,setpts=0.25*PTS"" -r srcfps -{encoding parameters} output
</code></pre>

<p><code>srcfps</code> should be replaced with your input's framerate.</p>
","17257"
"How can I generate a color-correct h264/MP4 from a DPX sequence using ffmpeg?","1038","","<p>We are converting a sequence of DPX files into an MP4. When the artist uses Adobe Media Encoder CC to generate the video, the colors match the original DPX. When we use ffmpeg, the video has a reddish hint. The command used is:</p>

<pre><code>ffmpeg -y -start_number 0101 -i \\path\to\filename.%04d.dpx -pix_fmt yuv420p sample.mp4
</code></pre>

<p>How can we generate a video that matches the color of the original DPX files?</p>

<hr>

<p>Inspecting the files with <code>ffprobe</code> revealed the following differences (correct file/bad file):</p>

<ul>
<li>major_brand: mp42 / isom</li>
<li>minor_version: 0 / 512</li>
<li>compatible_brands: mp42mp41 / isomiso2avc1mp41</li>
<li>encoder: not listed / Lavf57.34.102</li>
<li>Video: h264 (Main) / h264 (High)</li>
<li>Video: yuv420p(tv, bt709) / yuv420p</li>
<li>Video: 1920x1080 / 1920x1080 [SAR 1:1 DAR 16:9]</li>
<li>handler_name: Alias Data Handler / VideoHandler</li>
</ul>

<p>Probe references: <a href=""https://gist.github.com/kwill/a6e8df2268d6e09e20c2e31f18081e13"" rel=""nofollow"">dpx</a>, <a href=""https://gist.github.com/kwill/832da451eab85d7d55eb737840629d0f"" rel=""nofollow"">correct</a>, <a href=""https://gist.github.com/kwill/182e0f504c8049a346bae0f871b42705"" rel=""nofollow"">incorrect</a></p>
","<p>I followed up on the <code>yuv420p(tv, bt709)</code> portion of the encoding and discovered that this refers to the <a href=""https://en.wikipedia.org/wiki/Rec._709"" rel=""nofollow"">Rec. 709</a> colorspace for HDTV. In ffmpeg, colorspace is defined by the <a href=""https://ffmpeg.org/ffmpeg-filters.html#colormatrix"" rel=""nofollow"">colormatrix filter</a>. However, you must convert <em>from</em> some colorspace. According to <a href=""http://ffmpeg-users.933282.n4.nabble.com/convert-from-bt601-to-bt709-td4665658.html"" rel=""nofollow"">an ffmpeg mailing list conversation</a>, the default colorspace is assumed to be <code>bt601</code>:</p>

<blockquote>
  <p>ISTR that if you give ffmpeg RGB it assumes its full range, it also by
  default converts using 601 matrices. Although it has a number of
  colourspaces defined which can be accessed in the libs using the API,
  ffmpeg itself makes no use of them (Although I have a nagging feeling
  that that may not be completely true anymore either).</p>
</blockquote>

<p>So the final ffmpeg command that worked was:</p>

<pre><code>ffmpeg -y -start_number 0101 -i \\path\to\filename.%04d.dpx
-vf colormatrix=bt601:bt709 -pix_fmt yuv420p sample.mp4
</code></pre>

<p>The color is now correct, but there is no discernible difference in <a href=""https://gist.github.com/kwill/088afc643ea9d93fa2a0019db12487fa"" rel=""nofollow"">the output from ffprobe</a> or MediaInfo. (In other words, the colorspace of the ffmpeg-generated file does not appear in the output.)</p>
","18943"
"How to make MP4 and MPG-2 720 x 1280 videos? (aspect ratio 9:16)","1035","","<p>I need to make an video for an advertising panel. The video is nothing special ( It's just a still image for 10 seconds ), but I don't just know how to make MP4 and MPG-2 720 x 1280 videos.</p>
","<p>You can use the Free FFmpeg Mux/Demux...</p>

<p>Type onto ffmpeg/bin the following</p>

<p>ffmpeg.exe -i artwork.jpg -vcodec h264 -s 720x1280 -aspect 16:9 -ss 0 -t 10 -b 3072k outputvideo.mp4</p>
","13478"
"premiere cc: LINK audio and video tracks greyed out","1027","","<p>I have multiple audio clips and want to link them to a video clip. I first unlinked the camera audio from the video, then selected the video clips, the cam audio and the external recorded audio clips. Now the context menu entry ""LINK"" is greyed out. I can only relink the original audio clips to the video clip. This doesn't make any sense to me. To group clips doesn't help me either, because if I intercut the group, all parts are grouped together.</p>

<p><a href=""https://i.stack.imgur.com/CaAom.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CaAom.png"" alt=""enter image description here""></a></p>

<p>Any solutions for this?</p>
","<p>There are at least three ways to put things together in Premiere Pro: linking, grouping, and nesting.  Linking is the lowest-level, assigning audio tracks to a single video clip.  You cannot link two video clips together, regardless of how much audio they may want to share.</p>

<p>Grouping allows you to treat multiple clips of audio and video as part of the same group for selection, moving, trimming, etc.  If you have a bunch of related clips (video and audio) which you want to behave as a unit, but whose relationships you still want to be able to see, use grouping.</p>

<p>Nesting creates a new sequence with all the audio and video clips you want to behave as if it's just a single entity.  You can change the relationships (and of course edit) within that entity, but to do so you have to open up that sequence on its own timeline.</p>

<p>You probably want to use either grouping or nesting, not linking for this case.</p>
","17386"
"How can I move a floating text or image with the video?","1025","","<p>In this video </p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/0n9XE6MfFCw?start=0""></iframe>
            </div></div> </p>

<p>you can see a logo next to head that moves with it.</p>

<p>How can  I do that in Premiere?</p>
","<p>This is called <em>Motion Tracking</em>. You define a part on the head that will be tracked, and then have another object move relative to this position. </p>

<p>I'm not sure this can be <em>easily</em> done with Premiere, as I've never done that, but it's a fairly standard procedure in After Effects. After Effects has a built-in tracking system, but there are also plugins like <a href=""http://www.imagineersystems.com/products/mochav2"" rel=""nofollow"">Mocha</a> which offer the same features (and possibly a bit more).</p>

<p>There are hundreds of Motion Tracking tutorials online that explain how to do this, but looking for the term ""Tracking"" should get you started easily. For example, here's a tutorial for Premiere:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/DUX61MZip-A?start=0""></iframe>
            </div></div></p>
","7426"
"FFmpeg BMP to YUV x264 color shift","1015","","<p>When I encode a particular sequence of BMP image files to AVC/H.264, the colors get shifted. Why does this happen?</p>

<p>But if I convert the BMPs to PNGs before running ffmpeg, the video conversion appears correct. My source material is in BMPs, and would prefer not to manually convert everything to PNGs to perform the workaround.</p>

<hr>

<p>For the purpose of this question, I took the sequence of images and summarized them into these main colors being used:</p>

<p><a href=""https://i.stack.imgur.com/PEowN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PEowN.png"" alt=""enter image description here""></a></p>

<p>First attempt at encoding (the pure white becomes noticeably green): <code>ffmpeg -i 000.bmp -pix_fmt yuv420p -vcodec libx264 -profile:v main -crf 16 first.mp4</code></p>

<p><a href=""https://i.stack.imgur.com/w5YNB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w5YNB.png"" alt=""enter image description here""></a></p>

<p>Second attempt (even worse): <code>ffmpeg -i 000.bmp -vf ""colormatrix=bt601:bt709"" -pix_fmt yuv420p -vcodec libx264 -profile:v main -crf 16 second.mp4</code></p>

<p><a href=""https://i.stack.imgur.com/5I7L6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5I7L6.png"" alt=""enter image description here""></a></p>

<p>Third attempt (too bright): <code>ffmpeg -i 000.bmp -vf ""scale=in_range=full:in_color_matrix=bt709:out_range=full:out_color_matrix=bt709"" -pix_fmt yuv420p -vcodec libx264 -profile:v main -crf 16 third.mp4</code></p>

<p><a href=""https://i.stack.imgur.com/xKJta.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xKJta.png"" alt=""enter image description here""></a></p>

<p>However, using a PNG version of the same input image works great: <code>ffmpeg -i 000.png -pix_fmt yuv420p -vcodec libx264 -profile:v main -crf 16 fourth.mp4</code></p>

<p><a href=""https://i.stack.imgur.com/QwdPW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QwdPW.png"" alt=""enter image description here""></a></p>

<hr>

<p>I am using the latest <a href=""https://ffmpeg.zeranoe.com/builds/"" rel=""nofollow noreferrer"">FFmpeg Windows</a> x86-64 build (2016-11-22 Git d316b21):</p>

<pre><code>C:\Temp&gt; ffmpeg.exe -version
ffmpeg version N-82597-gd316b21 Copyright (c) 2000-2016 the FFmpeg developers
built with gcc 5.4.0 (GCC)
configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-dxva2
    --enable-libmfx --enable-nvenc --enable-avisynth --enable-bzlib --enable-fontconfig
    --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray
    --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme
    --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame
    --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264
    --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger
    --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora
    --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis
    --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264
    --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma
    --enable-decklink --enable-zlib
libavutil      55. 40.100 / 55. 40.100
libavcodec     57. 66.106 / 57. 66.106
libavformat    57. 58.100 / 57. 58.100
libavdevice    57.  2.100 / 57.  2.100
libavfilter     6. 67.100 /  6. 67.100
libswscale      4.  3.101 /  4.  3.101
libswresample   2.  4.100 /  2.  4.100
libpostproc    54.  2.100 / 54.  2.100
</code></pre>

<p>To rephrase my question, how can I convert the sequence of BMP images to an H.264 video without unwanted color shifts?</p>
","<p>Apparently FFmpeg doesn't set input color attributes correctly for BMP input (probably because the file doesn't contain that metadata) and the BMP decoder doesn't check for manually flagged attributes. However, we can force it using the format filter.</p>

<pre><code>ffmpeg -i 000.bmp -vf format=rgb24 -pix_fmt yuv420p -vcodec libx264 -profile:v main -crf 16 first.mp4
</code></pre>
","19958"
"Collapse Transformation not working in Adobe After Effects CC","1014","","<p>I'm following a tutorial on AE and it is completely confusing to me as I cannot get the desired result.</p>

<p>Here are the steps in the tutorial:</p>

<ul>
<li>Select a group of comps</li>
<li>Pre-compose them into new comp</li>
<li>Collapse transformations on the new pre-composed comp (press the starry button)</li>
</ul>

<p>The desired result I want is the transformation view being snapped to just around the rocket, but somehow it doesn't work as I expected. What am I missing here? Screenshots below:</p>

<p><a href=""https://i.stack.imgur.com/K4iFu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K4iFu.jpg"" alt=""Group comps and make a pre-composed comp""></a></p>

<p><a href=""https://i.stack.imgur.com/LCkvy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LCkvy.jpg"" alt=""enter image description here""></a></p>

<p>The area should fit just around the rocket but it doesn't.</p>
","<p>I get the feeling you think collapse transformation means something that it doesn't. </p>

<p>What Collapse Transforms does is to leave all the transformation properties in the subcomp intact and available to the comp that it's nested in. It's basically like you've got all the layers still in the outside comp, just grouped into one layer for ease of organisation.</p>

<p>Without collapse transforms on the subcomp is pre-rendered and treated like a single piece of footage. So, for example if any of the layers of the sub-comp extend off the edge of the sub-comp's boundaries they won't be cropped with collapse transforms on.</p>

<p>Here are two subcomps, both contain text that extends past their outside edges:</p>

<p><a href=""https://i.stack.imgur.com/ug59l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ug59l.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/Q8MGd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q8MGd.png"" alt=""enter image description here""></a></p>

<p>And here they are in the outer comp. Top one is collapsed, bottom aint.</p>

<p><a href=""https://i.stack.imgur.com/LgFAS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LgFAS.png"" alt=""enter image description here""></a></p>

<p>If you scale down a raster layer in the subcomp and then scale it up to its original size in the outer comp, without collapse transforms on it will be pixelated, because it gets pre-rendered to the resolution of the subcomp, and then scaled up in the outer comp.</p>

<p>Here they are scaled down in the subcomp:</p>

<p><a href=""https://i.stack.imgur.com/JZbMQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JZbMQ.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/V0pdt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V0pdt.png"" alt=""enter image description here""></a></p>

<p>And here's the outer comp with both layers scaled back up:</p>

<p><a href=""https://i.stack.imgur.com/KWez4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KWez4.png"" alt=""enter image description here""></a></p>

<p>Another effect of collapsing transforms is that you lose the ability to change the transfer mode of the subcomp in the outer comp. The layers in a collapsed transform-ed subcomp apply whatever transfer mode they have inside the subcomp, again, like they were actually in the outer comp, just grouped together.</p>

<p><a href=""https://i.stack.imgur.com/JsVCl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JsVCl.png"" alt=""enter image description here""></a></p>

<p>What Collapse transforms <em>doesn't</em> do is automatically crop the subcomp to fit its components. Is that what you were expecting?</p>
","19202"
"Motion graphics vs compositing","1014","","<p>I am really confused between the meaning of motion graphics and compositing. Is motion graphics mean using still image to create illusion of moving object and compositing is combining 2 or more clips together in the same frame?</p>
","<p><a href=""http://en.wikipedia.org/wiki/Motion_graphics"" rel=""nofollow""><strong>Motion graphics</strong></a> consists of <em>Graphic Design</em> and <em>Animation</em>. Main focus is to deal with <strong>images, typography, color and illustrations</strong>, also it's key to work with <strong>design assets</strong>, like flow charts, icons, images or backdrops, which created with animation in mind.</p>

<p>However <a href=""http://en.wikipedia.org/wiki/Compositing"" rel=""nofollow""><strong>Compositing</strong></a> consists of <em>Animation</em>, <em>Mathematics</em> and <em>Rating Pictures</em> (eyeballing). Main focus is <strong>digital image manipulation</strong> - it's the art and science to combine (matching) images from multiple sources like <strong>film, photographs or 3d renderings</strong> into <em>new (blended) images</em> to create a <strong>perfect illusion</strong>.</p>
","15507"
"Extract key frame from video with ffmpeg","1010","","<p>I have a need to generate a thumbnail for videos for a web application and I am currently doing that with this command:</p>

<p><code>ffmpeg -ss &lt;seconds&gt; -i &lt;input file&gt; -vframes 1 -y &lt;output file&gt;</code></p>

<p>Pretty basic, and does the job. But, of course, sometimes the frames aren't very ""interesting"".</p>

<p>For example, sometimes the video has faded to black as the scene changes, or it could be something which is somewhat boring like credits rolling or similar.</p>

<p>I need some sort of algorithmic way to extract something interesting from the video and essentially filter out the boring bits.</p>

<p>Is there anything in FFmpeg that might be able to make this easier?</p>

<p>If not, then I will need to collect a number of frames and process them outside of FFmpeg; likely using PHP. As a side note, if anyone has any existing code that does this, it would be super helpful!</p>

<p>Many thanks</p>
","<p>No ffmpeg code can identify interesting frames. At most, there are filters which detect representative frames and filters which identify frames with a scene change. So, at best, there's a thumbnail filter for this, sort of. It can't detect ""interesting"" frames but it detects representative frames. So, frames from the middle of fades should be out. </p>

<p>Syntax is</p>

<pre><code>ffmpeg -ss 50 -i in.avi -vf thumbnail=300 -vsync 0 thumbs%d.png
</code></pre>

<p>This will pick one representative frame from each batch of 300 consecutive frames.</p>
","19727"
"What is Pivot in Color Grading?","1007","","<p>Most of the controls I at least have a basic understanding of from Photography and other design applications (Affinity Photo / Photoshop / Lightroom etc). But not Pivot.</p>

<p>In the <a href=""https://helpx.adobe.com/pdf/speedgrade_reference.pdf"" rel=""nofollow noreferrer"">SpeedGrade Manual</a> the word Pivot doesn't exist at all. Pretty big omission but I'm resourceful and know other applications exist. So even though I don't use it I pulled up <a href=""http://documents.blackmagicdesign.com/DaVinciResolve/DaVinci_Resolve_12_Manual_2015-07-27.pdf"" rel=""nofollow noreferrer"">DaVinci Resolve's guide</a> which states</p>

<blockquote>
  <p><strong>Pivot:</strong> 
  Changes the center of tonality about which dark and bright parts of the image are 
  stretched or narrowed during a contrast adjustment. Darker images may require a lower 
  Pivot value to avoid crushing the shadows too much when stretching image contrast, while 
  lighter images may benefit from a higher Pivot value to increase shadow density adequately.</p>
</blockquote>

<p>Perfect cause this actually sounds like exactly what I need. Except when I use it there's absolutely no change in my video and I've got no idea why:</p>

<p>Here's the original:</p>

<p><a href=""https://i.stack.imgur.com/zTzwN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zTzwN.jpg"" alt=""original footage""></a></p>

<p>Here's the footage with the pivot greatly pushed up, yet no change:</p>

<p><a href=""https://i.stack.imgur.com/dxqWg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dxqWg.jpg"" alt=""footage with pivot increased""></a></p>

<p>I don't know if I'm not understanding the Pivot or what. I'm not seeing any difference though.</p>
","<p>As you know, the contrast function darkens dark colors further and lightens light colors further. Software without a pivot parameter assume that the point where darkening becomes lightening is exactly in the middle, at 0.5 (for a color range of 0 to 1). Those with a pivot parameter allow to move the pivot point up or down. Shifting the pivot slightly down (e.g. from 0.5 to 0.4) results in dark-medium colors (let's say 0.45) not anymore being darkened, but lightened. </p>

<p>In your image, the effect is not that visible because most of the colors are already very bright, very dark, or just in the middle.</p>

<p>If you want to lift or lower your midtone details, I would suggest that you use a gamma or midtone control instead.</p>
","16432"
"Music-sync'd Strobe effect in video","1002","","<p>What's one way to get a strobe flash effect in video (you know, virtually, in software) that's synchronized to the beat of a music track?</p>

<p>Or for that matter, how do I synchronize ANY effect with music?</p>
","<p>Trapcode's Sound Keys will do the job:<br>
<a href=""http://www.redgiantsoftware.com/products/all/trapcode-sound-keys/"" rel=""nofollow"">http://www.redgiantsoftware.com/products/all/trapcode-sound-keys/</a></p>

<p>From the promotional text:</p>

<blockquote>
  <p>Now it's easy to synchronize motion and sound in After Effects. With
  Trapcode Sound Keys, your audio-intensive animations no longer require
  extensive keyframing. Visually select parts of an audio track and
  convert them into keyframes, syncing footage with audio using
  amplitude or frequency ranges. Sound Keys is applied as a regular VFX
  effect, so you can save its settings with your project, generate
  keyframes into output parameters, and link keyframes to expressions.
  The possibilities are huge, just like the beat.</p>
</blockquote>

<p>By using the generated key frames you can hook them up to almost any effect or behavior that you want.</p>

<p>A basic example of using this plugin to drive a few ""strobes"":</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/_ptzCbhk0vE?start=0""></iframe>
            </div></div></p>
","6967"
"What is the productive way of doing multiple speed changes in a video editing project?","987","","<p>I have to stitch various pieces of footage together, and the final result will be about 11.5 hour long.</p>

<p>Because of all the needed changes (blurring faces, inserting images and icons in certain places, synchronising the footage pieces among each other and fading them out\in with right timings etc), my project map starts to look like <a href=""https://i.imgur.com/Ym2vMxV.png"" rel=""nofollow noreferrer"">this</a>. The #3 and #4 tracks are just transitions from one piece to another. In tracks #1 and #2 is how most tutorials tell to deal with blurring\changing a screen area (duplicate the track above itself and then pan\crop the duplicate).</p>

<p>The problem is that in multiple places the video also has to be sped up\slowed down, and when I do it, the footage length changes like a fractal curve, and all the alignments get messed up. And besides <em>these</em> speed changes, the whole eventual video has to be sped up again once more too (x1.5).</p>

<p>This looks like one of those things that Ill be cursing myself about later, because a simple and overlooked mistake led to losing huge amounts of time\work, so I decided to just ask about it now: am I doing it wrong, and if sowhats the more productive way of dealing with many playback speed changes in relatively big editing projects?</p>
","<p>It isn't a perfect solution, but you can use nested sequences in order to apply your edits separately from your speed changes.  This way the speed changes impact the final video instead of just one clip.</p>
","10681"
"How can I increase the resolution of a mobile clip?","985","","<p>Few days ago, at early morning I saw something falling in a slop in the sky. It wasn't a plane but something with a color of fire and so bright! I wonder what it could be, so I recorded it on my mobile. But as my mobile camera resolution is really very low, it wasn't that clear to understand. While I show it to someone, then they don't understand the exact thing that I saw. </p>

<p>So, I'd like to know that is there any way to increase the resolution of the mobile clip?</p>
","<p>You can't recover information that isn't present in the data you have.  Sometimes there's useful information hidden by noise, and filtering can help.  However, esp. in the case of downscaling/upscaling, information is just gone.</p>

<p>These wiki links are way too general and mathy to be specifically helpful, but this is the theory behind why you can't recover quality once it's gone:
<a href=""http://en.wikipedia.org/wiki/Information_theory"" rel=""nofollow"">http://en.wikipedia.org/wiki/Information_theory</a>
<a href=""http://en.wikipedia.org/wiki/Algorithmic_information_theory"" rel=""nofollow"">http://en.wikipedia.org/wiki/Algorithmic_information_theory</a></p>

<p>Think of it this way: if you could somehow increase the quality of a video, we'd just use that procedure on 100x100 pixel videos to upscale them to beautiful 4k displays, and fit HD movies on floppy disks.</p>

<p>Same deal if you compress the hell out of a JPEG image:  it'll be all blurry and blocky, and the details of the source are gone.  If they were recoverable from a tiny JPEG, then we could compress anything as much as we wanted and still have great-looking images.</p>

<p>These things are as impossible as perpetual motion machines.  Sorry to dash your hopes. :/</p>

<p>All that said, it's possible you could do some smoothing or filtering on an upscaled version to produce something that looks to a human eye more like what you saw.  With the aid of words, you might have some luck getting your point across.</p>
","15311"
"Why is compression rarely included as a builtin effect?","984","","<p>I've bought a new analogue mackie mixer that I'm very happy with, but if I need one effect it would be compression. The mixer comes with 16 builtin effects and not a compressor. Is it expensive to make or why is compression not included? Instead I have reverbs and echoes which are not very applicable for the dry sounds of a small room that I want to create. </p>
","<p>While you often see compression in the effects section of editing software, it is dynamics rather than actual effects.  In the traditional sense in the live world, compression is not considered an effect.  The software world simply does it that way because they are all software filters, but even then they are often in a dynamics category.</p>

<p>Compression, Expansion, Gating and Limiting are all dynamics.  I'm not sure if the effects hardware could do compression, but the controls for the parameters on the effects are certainly too limited.  You need attack, release, ratio, threshold at least and gain and soft/hard and auto attack/release options are common.  You can't adjust these with the very basic effects controls provided in the board.</p>

<p>There are boards that have built in compression, but they list it under dynamics normally.  There are also stand alone compression units you can get that you can use the inserts to feed the signal to.</p>
","8220"
"Cannot capture (HDMI) output of Graphics Card with BlackMagic UltraStudio","982","","<p>I am trying to convert an HDMI video signal (from my computer's graphic card) to SDI using an Ultrastudio 3D from Blackmagic.</p>

<p>I successfully converted an HDMI signal from my Canon 5D camera, but it does not work when I send a signal from my graphics card.</p>

<p>What is the difference between HDMI from a camera and HDMI from my GPU?</p>

<p><strong>Edit</strong></p>

<p>My goal is to send GPU generated content, not a film, so there should be no HDCP problems, am I right? So far I tried to sent my GPU output considering the Ultrastudio 3D as an extended desktop.</p>
","<p><strong>What solved the problem:</strong></p>

<p>Check your capture software settings. All settings (your monitor/capture device settings, your hardware settings and the settings in your capture software) should be correct and equal.</p>

<p><strong>What can help track down similar issues:</strong></p>

<p>Very likely a framerate issue. Only the 4K version of the Blackmagic Capture cards supports 60FPS and your monitor is or was probably set to 60Hz or 50Hz, though your capture card probably is only supporting 30FPS @ 1080p (assuming thats your resolution). (Edit: In this case not true, the UltraStudio (4k) does support 60fps even at 4k over SDI but if someone else has this issue, have a look at the specs of your capture card)</p>

<p>Assuming your are using Windows go to the resolution settings and click on advanced settings after selecting the capture card ""display"" and then go to the ""Monitor"" tab and select a lower refresh rate like 30FPS or 25FPS. Then set the same refresh rate in your capture settings.</p>

<p>On a Mac you can use a tool called <a href=""http://www.madrau.com/index.html"" rel=""nofollow"">SwitchResX</a> to change your refreshrate or alternatively in your case simply set your capture settings to the output settings shown in the System Informations for the Capture Card ""display"". So same resolution and refresh rate.</p>

<p>If your really unlucky your Mac doesn't recognize the UltraStudio as a display or the Ultrastudio isn't sending any EDID information, though I somewhat doubt that's the case. I had this issue all the time with the 4K PCI-E capture card from Blackmagic, it always was a settings mismatch.</p>

<p>Also make sure that if you want to capture a video that's playing back, it isn't protected by HDCP like Jim Mack suggested.</p>
","12077"
"Freeze frame effect with ffmpeg","982","","<p>I am currently fiddling with ffmpeg to achieve a <code>freeze frame effect</code> and so far have been unsuccessful. I have been able to do this with adobe after effects shown <a href=""https://www.youtube.com/watch?v=2wUZdNkiVw4"" rel=""nofollow"">here</a>. However I would like to achieve a freeze frame effect(of 5 seconds duration) at the beginning of the video. How can I achieve this in ffmpeg?</p>

<p><strong><em>EDIT</em></strong></p>

<pre><code>ffmpeg -i ""/media/test/test.mp4"" -loop 1 -i ""/media/sf_linux_sandbox/hashtag_pull/video-downloads/test/test.png"" -an \
-filter_complex ""[1:v]trim=start=0:end=5[ol];[0:v]setpts=125+PTS[nv];[nv][ol]overlay=eof_action=pass[final]"" \
-map '[final]' -c:a copy -c:v libx264 -q 1 ""/media/test/out.mp4""
</code></pre>
","<p>First you need to extract the frame that you wish to freeze. This is easy:</p>

<pre><code>avconv -i test.webm -vf ""select='eq(n,1)'"" out.jpeg
</code></pre>

<p>Then you want to put that frame for the appropriate time interval.</p>

<pre><code>avconv -i test.webm -loop 1 -i out.jpeg -an \
-filter_complex ""[1:v]trim=start=0:end=5[ol];[0:v]setpts=125+PTS[nv];[nv][ol]overlay=eof_action=pass[final]"" \
-map '[final]' -c:v libtheora -q:v 8 outtest.ogv
</code></pre>

<p>Firstly you will see that i didn't want to mess with audio, but if you'll find it too difficult, let me know and i will look into the quick way to align it as well.</p>

<p>Now what i'm doing here consists of several strange things. Firstly i am taking that frame that was generated before and i am creating a continuous loop video from it. And afterwards i cut first 5 seconds of it with the first filter <code>trim</code>. Then i do a quick and dirty move of all the timestamps of the main video by 5 seconds (125 frames). And finally comes an overlay filter, which places the grabbed frame entirely over the blank screen that was generated by shifting the video 5 seconds into the future.</p>

<p>Tested with avconv version v12_dev0-680-g3a724a7 built on Jan 22 2015 18:13:02</p>

<p>How to build <code>avconv</code>:</p>

<ul>
<li>In terminal go to any directory where you are comfortable to place libav's code</li>
<li>run <code>git clone git://git.libav.org/libav.git libav</code></li>
<li>enter into libav direcotry</li>
<li>run <code>--enable-gpl --enable-version3 --disable-encoders --enable-encoder=mjpeg --enable-encoder=flac --enable-encoder=ffvhuff --enable-encoder=libspeex --enable-encoder=libtheora --enable-encoder=libvorbis --enable-encoder=libxvid --enable-encoder=libopenjpeg --enable-libschroedinger --enable-libopenjpeg --enable-libopus --enable-libspeex --enable-libtheora --enable-libvorbis --enable-libxvid --bindir=/home/USERNAME/bin</code>
<ul>
<li>Note that here i am assuming that you have ~/bin directory, that is added to your execution PATH. If you don't just remove that option</li>
</ul></li>
<li>Run <code>make</code>
<ul>
<li>It is possible that you will not have some libraries, if you encounter such errors either install them or remove ""enable"" options for those libs. Keep in mind that removing those options can make it impossible to generate files with some codecs.</li>
</ul></li>
<li>Run <code>make install</code> (or if you chose to not place binary within your user directory, <code>sudo make install</code>).</li>
</ul>

<p>Edit: Unfortunately due to a bug, video becomes shorter than it should have been. I have reported it <a href=""https://bugzilla.libav.org/show_bug.cgi?id=858"" rel=""nofollow"">https://bugzilla.libav.org/show_bug.cgi?id=858</a></p>
","15519"
"Is there a way to directly save DNG Sequences to 16bit exr Sequences in Photoshop or After Effects?","982","","<p>I want to prepare my Magic Lantern Raw files for Nuke. I use Photoshop to convert DNG to a 16bit tiff and then re-encode them to a 16bit exr sequence. I wish I could directly save as exr.
I know i could use Davinci Resolve I just don't like the workflow with Davinci...</p>
","<p>Using After Effects this is not an issue. After Effects has CameraRAW support and by that also support importing DNG sequences.
You import them just as you would any other image sequence.</p>

<p>Then simply make the sequence into a composition and add it to the render queue.
AFAIK After Effects has the OpenEXR plugin integrated since CS5.5. In case I'm wrong you can get the great ProEXR plugin for Photoshop and After Effects from <a href=""http://www.fnordware.com/ProEXR/"" rel=""nofollow noreferrer"">fnodware</a> (I'm not affiliated). To do the same with a few more format options.</p>

<p><img src=""https://i.stack.imgur.com/j95SV.png"" alt=""render exr sequence in AE"">
<img src=""https://i.stack.imgur.com/7JHxA.png"" alt=""exr settings""></p>
","13463"
"How to open MP4 container video with mp42 codec in Vegas Pro?","975","","<p>I've been given an MP4 video with the following information, per MediaInfo</p>

<pre><code>General
Complete name                            : E:\306G5562_01.MP4
Format                                   : MPEG-4
Commercial name                          : XDCAM EX 35
Format profile                           : Base Media / Version 2
Codec ID                                 : mp42 (mp42)
File size                                : 3.45 GiB
Duration                                 : 13 min
Overall bit rate mode                    : Variable
Overall bit rate                         : 35.6 Mb/s
Encoded date                             : UTC 2016-07-16 17:57:03
Tagged date                              : UTC 2016-07-16 17:57:03

Video
ID                                       : 1
Format                                   : MPEG Video
Commercial name                          : XDCAM EX 35
Format version                           : Version 2
Format profile                           : Main@High
Format settings, BVOP                    : Yes
Format settings, Matrix                  : Custom
Format settings, GOP                     : M=3, N=12
Codec ID                                 : 61
Duration                                 : 13 min
Bit rate mode                            : Variable
Bit rate                                 : 34.0 Mb/s
Maximum bit rate                         : 35.0 Mb/s
Width                                    : 1 920 pixels
Height                                   : 1 080 pixels
Display aspect ratio                     : 16:9
Frame rate mode                          : Constant
Frame rate                               : 25.000 FPS
Color space                              : YUV
Chroma subsampling                       : 4:2:0
Bit depth                                : 8 bits
Scan type                                : Progressive
Compression mode                         : Lossy
Bits/(Pixel*Frame)                       : 0.656
Time code of first frame                 : 08:36:53:20
Time code source                         : Group of pictures header
GOP, Open/Closed                         : Open
GOP, Open/Closed of first frame          : Closed
Stream size                              : 3.30 GiB (96%)
Language                                 : English
Encoded date                             : UTC 2016-07-16 17:57:03
Tagged date                              : UTC 2016-07-16 17:57:03
Color primaries                          : BT.709
Transfer characteristics                 : BT.709
Matrix coefficients                      : BT.709

Audio
ID                                       : 2
Format                                   : PCM
Format settings, Endianness              : Big
Format settings, Sign                    : Signed
Codec ID                                 : twos
Duration                                 : 13 min
Bit rate mode                            : Constant
Bit rate                                 : 1 536 kb/s
Channel(s)                               : 2 channels
Sampling rate                            : 48.0 kHz
Bit depth                                : 16 bits
Stream size                              : 153 MiB (4%)
Language                                 : English
Encoded date                             : UTC 2016-07-16 17:57:03
Tagged date                              : UTC 2016-07-16 17:57:03
</code></pre>

<p>From what I can tell, this is XDCAM EX video, which is encoded in MP42 codec in an MP4 container. Vegas Pro won't even look at it. Says the file cannot be opened. How can I fix this? I can convert it (I've done Handbrake and VLC already to test), but I'd rather not, because I actually have tons of these videos and converting takes lots of time.</p>
","<p><code>mp42</code> is the container ID, which Mediainfo wrongly calls a <code>codec id</code>. It refers to MPEG-4 Part 14 spec for containers.</p>

<p>XDCAM is actually a Sony variant of MPEG-<strong>2</strong> video. I've worked with XDCAM natively in Vegas, which were in MOV, I believe.</p>

<p>You can use FFmpeg to rewrap these files.</p>

<pre><code>ffmpeg -i video.mp4 -c copy video.mov
</code></pre>

<hr>

<p>Looks like MOVs only work on Macs. The command below also worked.</p>

<pre><code>ffmpeg -i video.mp4 -c copy video.vob
</code></pre>

<p>(You'll have to switch to viewing <code>All Files (*.*)</code>)</p>
","19121"
"Does oval-shaped bokeh indicates that the aspect ratio of the image was modified?","965","","<p>In Mission Impossible III, I notice very often that bokeh is not circular, but has a shape of an oval. For example, here are two images where such bokeh is noticeable:</p>

<p><img src=""https://i.stack.imgur.com/NKDdG.jpg"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/TeDjS.jpg"" alt=""enter image description here""></p>

<p>Does it necessarily mean that he was filmed using chroma key and the background was filmed separately and then resized using a different aspect ratio, or this is specific to the lens (maybe anamorphic lenses)? Is this made on purpose, or is just a side effect?</p>
","<p>It's the effect of using an anamorphic lens which squashes the image horizontally to fit a wide-screen image on conventional 35mm film. The image is then un-squashed by a similar lens on the projector when the film is screened (or by computer after the film is scanned in a digital post production environment).</p>

<p>Bokeh appears elliptical because the front of the lens barrel which is circular, appears as an ellipse when viewed through the back of the lens. A regular lens with an elliptical opening at the front would generate exactly the same effect.</p>

<p>The film's director, J.J. Abrams is well known for using anamorphic lenses for the effects they generate, which also includes unique lens flare that manifests itself as strong horizontal blue lines.</p>
","12726"
"Forcing kdenlive to adapt its profile to the video clip","963","","<p><a href=""http://www.kdenlive.org/"" rel=""nofollow"">kdenlive</a> starts with a profile for instance 1080*720/etc.</p>

<p>Then I import a video clip.</p>

<p>How to tell kdenlive to modify its profile to match the video clip?</p>

<p>Modifying the profile manually by copy/pasting each of the clip's properties is very time-consuming and error-prone, I am sure there is a better way to do this.</p>
","<p>Curious... recent versions of KDEnlive practically push this option into your face once you add a clip with properties different from the project.</p>

<p>If the option got disabled in your setup you can enable it again by going to <code>Settings</code> -> <code>Configure KDEnlive</code> -> <code>Misc</code> (selected by default) -> <code>[ ] check if first added clip matches project profile</code></p>

<p>I am using this option in version 0.9.6 on Debian, but I've seen this behaviour in 0.9.4 and probably older versions as well.</p>

<p>Also if you got used at some point to dismissing this funny question window you get immediately when adding a clip, try to control the urge this time - the question is whether KDEnlive should adjust the project properties, which is what you want ;-)</p>

<p>(don't be mad at me if you think this last part is stupid, I've done user support occasionally and I have seen people do this;-)</p>
","9206"
"LifeSize DVI Input from multiple devices with in-line scaler","963","","<p>We have a number of meeting rooms that were equipped with an AV cabinet and projector each. There's a local PC and input for mobile devces at the meeting table.  All of the cable runs are HDMI (those longer than ~10m are supported by active Kramer Cat6 Baluns). We've standardised on 1080p for our displays. Locally, everything works well.  </p>

<p>We've upgraded two of these rooms for conferencing. This includes: A second, wall mounted TV and a LifeSize Express 220. Input are handled by a Kramer VS-84 (or VS-44) matrix switcher, such that the image to the Projector is mirrored to the DVI-I Input of the LifeSize unit, making it possible to present to remote participants from any of the input devices with ease. </p>

<p>Now, the LifeSize 220 series doesn't support 1080p input for presentations. This wasn't regarded as a significant issue, since a compressed, downscaled input was judged sufficient for passive observers. </p>

<p>To rectify this, we installed a Kramer VP-424 scaler in-line between the switch and the LifeSize. (HDMI in and out). This would give us a clean 720p feed to the LifeSize while all the local displays stay at 1080p.</p>

<p>Here's where the problems come in: At both sites, output from the scaler was being corrupted or not displayed. The pattern of this varied depending on the input device and the scaler used:</p>

<ul>
<li>Kramer VP-424: Macbooks and iOS devices Display, but are subject to very regular (1/s) single-frame flashes of noise with audible popping sound. Windows laptops display cleanly</li>
<li>CYP SY298H: Similar to the Kramer, with audible poppping, but the image would drop to flat blue and 'No Input', or appear squashed up into the top 1/6 of the screen. </li>
<li>Extron DCS 301: A clean display from Windows laptops, but a no output whatsoever from any Apple devices. </li>
</ul>

<p>We've also tried:</p>

<ul>
<li>Many (many) cables. </li>
<li>Direct device input to the Lifesize (Everything works, as the device will downscale to 720p on connection, but it requires manual connection wrangling prior to meetings, which isn't sustainable)</li>
<li>Discussing with our installer. (They're being brilliant about this, but they're as stumped as we are)</li>
<li>Discussing with LifeSize support (Also very helpful, but there are too many variables and 3rd party products for them to suggest anything concrete)</li>
<li>Not having the scaler: All inputs are corrupted as the switch tries to feed 1080p into the LifeSize.</li>
</ul>

<p>We can't enforce input device type, as support for everything is required in our environment. We can't really mandate 720p across the board as, although it's sufficient for passive viewing, the presenter still needs 1080p (lots of small text and Wireframe 3D in some cases).</p>

<p>I'd love a one-shot solution to this, but if anyone can give me some pointers in how to further debug the problem, I'll be in their debt. (I can provide diagrams of the setup, if it'll help)</p>

<p><strong>EDIT 1</strong> </p>

<p>At AJ's Suggestion, I removed the switch from the equation. (Mackbook 1080p@60 -> Scaler-> Lifesize). To start with, the issue was just as apparent. Then I tried something we'd eliminated previously, and switched off the <code>HDCP on Input</code> option.  Suddenly I'm looking at a clean, scaled 720p image!</p>

<p>So I put the switch back in line and now all I see is what we saw with earlier tests: A blank screen. Still, this means that without the switch in place, HDCP isn't needed. If I can disable it in the entire chain, then we might be sorted. I'll report back when I've done some further testing.</p>

<p><strong>EDIT 2</strong></p>

<p>Something I didn't point out previously: If I take the codec out of the equation (Mac -> Scaler -> TV), then I dont see any problems at all, regardless of the HDCP setting. </p>

<p>So we have HDCP being agreed from device to switch to (if it's enabled) scaler. It looks like the scaler -> Lifesize negotiation is where it's failing. Both agree on HDCP support, but then something goes wrong that causes this regular popping. </p>
","<p>I've found an answer to my specific problem that's a little ugly, but might be of some use.</p>

<p>The basic premise is to ensure that the first device in the chain does not support (or is capable of disabling support for) HDCP. Macs and iOS devices are opportunistic when it comes to HDCP; they'll always attempt to negotiate it, but will fall back to unencrypted when there's no protected content playing. </p>

<p>The Kramer VP-424 scaler has a setting ""HDCP on input"" that will, when disabled, prevent that negotiation. By putting one of these inline between the input and the switch, HDCP never starts and the whole chain is unencrypted:</p>

<p><strong>Mac</strong> -> <strong>VP-424</strong> (HDCP Disabled, 1080p output) -><strong>VS-84H</strong> Switch -> <strong>VP-424</strong> (HDCP Disabled, 720 Output) -> <strong>LifeSize Codec</strong>.</p>

<p>The scaling doesn't have any noticeable effect on image quality or unduly effect sync times, which is a bonus. I still get 1080p out to the local projector and downscaled 720p to the codec. </p>

<p>As I say, it's neither pretty, nor cheap; but it works.</p>
","10279"
"Does having hardware encoding chip help?","959","","<p>I've been googling for a while now so I finally decided to ask others as well as I'm pretty darn confused.</p>

<p>I'm looking to be able to stream at 1080p at 60fps ( from a gaming laptop, it's more than capable, running actual game at 300fps uncapped ). I know about <code>ShadowPlay</code> and it's fairly easy to use but it's annoying how it automatically changes my stream's title and the quality is pretty bad even at best settings.</p>

<p>Then, I have found the <code>Live Gamer Extreme</code> which has 1080p at 60fps recording and hdmi passthrough, but it doesn't have h.264 encoding chip so I'm receiving RAW data in <code>OBS</code> ( there are cards that have hardware encoding chips already but those don't do 60fps as far as I can see )</p>

<p>So, does having a h.264 encoding chip help in any way ? 
Because I'd like doing the whole thing on one laptop and not having to use two laptops one for gaming and one for encoding would be great.</p>

<p>In <code>OBS's Encoding</code> tab I can't find any option like <code>No encoding needed</code> since the signal is already encoded, so, should I look anymore for 60fps w/ encoding chip or should I go with this game capturing card and use the other laptop for encoding.</p>

<p>Thanks in advance !</p>
","<p>A hardware encoder will produce video with less CPU usage. However, it will require a higher bitrate to achieve the same quality image.</p>
","17412"
"What is the best workflow solution for editing a movie in FCPX and creating specific effects for it in Motion 5?","957","","<p>Since there is no way to send a clip directly to Motion 5 from Final Cut Pro X, only to open up general effects, transitions and generators; what is the best workflow for actually using Final Cut Pro X for a specific clip?</p>

<p>Let's say I have a movie that is five minutes long, and that I want to, every minute, create a floating text with some kind of motion following behavior. How am I supposed to accomplish this? Export separate clips, import them to Motion 5, do the edits, export the clip again, importing it to Fincal Cut Pro X and replace the old clip? This seems quite cumbersome.</p>
","<p>There's no really good workflow, I think, yet. The best way I found is to export the video out of FCPX and import it into Motion. Apple thinks that you don't need to work in Motion solely, you only go there to create an effect, title, whatever and then put that thing right on top of a clip <em>in Final Cut</em>. </p>

<p>The problem there is that sometimes you'd rather want to have all the editing options Motion offers and just want to see how the video's going to look like. That problem is, to my knowledge, not solved yet. I hope this is all going to change in the future.</p>
","3831"
"Are there any (visible) differences between R1 and R2 DVDs (or blu-rays, thereof)?","955","","<p>I was wondering whether there is any visible difference between Region 1 and Region 2 movies. Region 1 is encoded in 23.976 fps standard, right? And Region 2 is 25 fps.</p>

<p>But as I assume the movies are shot in 23.976 only, the R2 releases are just some convert/recode to be compatible with the region.</p>

<blockquote>
  <p>Which ultimately leads me to a question: If presented with a movie
  playing from R1 and R2 discs simultaneously, can a trained eye see any
  difference?</p>
</blockquote>

<p>I know for a fact an eye can see the difference between 23.976 fps and 25 fps. A lot of low-budget movies/web series are being released lately since the Canon dSLRs can record video and there definitely is a visible difference between the standard 24 film fps and the ""fluid, unnatural"" 25 fps.</p>
","<p>There is no visible difference, unless the DVD/BR publisher labels their discs as such.  Any region code (or even region-free data) can be burned or pressed on to any media.</p>

<p>Furthermore, region encoding (at least on DVDs, and I assume on BRs) has nothing (technically) to do with the video format.  Now, you're <em>likely</em> to find Region-1 DVDs in NTSC format, Region 1 is the U.S. and Canada, and the U.S. and Canada use NTSC, but there's no technical barrier to creating a R1 DVD in PAL format.</p>

<p>See the Wikipedia article on <a href=""http://en.wikipedia.org/wiki/DVD_region_code"" rel=""nofollow"">DVD Region codes</a>, and specifically the section on <a href=""http://en.wikipedia.org/wiki/DVD_region_code#PAL.2FSECAM_vs._NTSC"" rel=""nofollow"">NTSC vs. PAL</a>.</p>
","2469"
"Graphics Cards that have SDI outs","955","","<p>I am looking for a medium to high end graphics card that has SDI outputs. I have a home studio with a SDI matrix switch and I want to hook my Gaming Computer into this rig. I am planning on doing some gaming livestreams, and I want to use my studio m/e switch to control the stream. </p>

<p>My other idea was to use some DVI to SDI converter, but since I have multiple monitors, it would probably be cheaper to replace my graphics cards</p>
","<p>You are best off to convert from a supported format (DVI, HDMI, maybe even DisplayPort) to SDI.  SDI cards are not designed for gaming, they are designed for workstations and video.  You aren't going to get a good quality gaming graphics card with SDI outputs.</p>
","10442"
"What exactly does the video rendering quality option do in Sony Vegas Pro?","954","","<p>I have created a video with Sony Vegas Pro 8.5 with an intro that includes graphics, pan/crop events, and generated text. There is also a ""lower third"" banner right after it.</p>

<p>What is odd to me is that there are quite a few issues when I render it with ""best"" video rendering qualtiy, while ""preview"" gives me no troubles. While on ""best"" the problem is that the text, most of the images, and anything with a pan/crop event are blurred and/or wiggle. You can especially see the wiggle on the portrait images once the next image starts scrolling up and on the third slide for the duration it is showing.</p>

<p><a href=""https://youtu.be/2UlEdiXXCi8"" rel=""nofollow noreferrer"">Here is a ""best"" sample of the video on YouTube.</a></p>

<p><a href=""https://youtu.be/RiyNJiWl4eA"" rel=""nofollow noreferrer"">Here is a ""preview"" sample of the video on YouTube.</a></p>

<p>This makes no sense to me. Why is the ""preview"" setting better than the ""best"" setting? What exactly does this setting do? Vegas Pro 8.5, if it matters.</p>

<p>Sample Render settings:</p>

<p><a href=""https://i.stack.imgur.com/Hcuoc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hcuoc.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/L1h7R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L1h7R.png"" alt=""enter image description here""></a></p>

<p>Project properties:</p>

<p><a href=""https://i.stack.imgur.com/X4lHL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X4lHL.png"" alt=""enter image description here""></a></p>

<p>Here are some snap shots that show it looks crisp with ""preview"" but not ""best"".</p>

<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Crisp ""preview"" snapshot on left &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Not crisp ""best"" snapshot on right.</strong>
<a href=""https://i.stack.imgur.com/Uaf1J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uaf1J.png"" width=""320""></a>
<a href=""https://i.stack.imgur.com/0jZMK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0jZMK.jpg"" width=""320""></a>
<a href=""https://i.stack.imgur.com/45wN6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/45wN6.png"" width=""320""></a>
<a href=""https://i.stack.imgur.com/leqid.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/leqid.jpg"" width=""320""></a>
<a href=""https://i.stack.imgur.com/vnp5A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vnp5A.png"" width=""320""></a>
<a href=""https://i.stack.imgur.com/cYGCJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cYGCJ.jpg"" width=""320""></a>
<a href=""https://i.stack.imgur.com/9hns8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9hns8.jpg"" width=""320""></a>
<a href=""https://i.stack.imgur.com/HC4Rt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HC4Rt.jpg"" width=""320""></a></p>
","<p>This is the explanation for the four quality modes, as provided by Vegas tech support long ago:</p>

<blockquote>
  <p>Quality: Best</p>
  
  <p>Scaling: bi-cubic/integration</p>
  
  <p>Field Handling: on</p>
  
  <p>Field Rendering: on (setting dependent)</p>
  
  <p>Framerate Resample/IFR: on (switch dependent)</p>
</blockquote>

<hr>

<blockquote>
  <p>Quality: Good</p>
  
  <p>Scaling: bi-linear</p>
  
  <p>Field Handling: on</p>
  
  <p>Field Rendering: on (setting dependent)</p>
  
  <p>Framerate Resample/IFR: on (switch dependent)</p>
</blockquote>

<hr>

<blockquote>
  <p>Quality: Preview</p>
  
  <p>Scaling: bi-linear</p>
  
  <p>Field Handling: off</p>
  
  <p>Field Rendering: off</p>
  
  <p>Framerate Resample/IFR: always off</p>
</blockquote>

<hr>

<blockquote>
  <p>Quality: Draft</p>
  
  <p>Scaling: point sample</p>
  
  <p>Field Handling: off</p>
  
  <p>Field Rendering: off</p>
  
  <p>Framerate Resample/IFR: always off</p>
</blockquote>

<hr>

<blockquote>
  <p>Scaling:</p>
  
  <p>These methods come into play when conforming sources that differ from
  the output size. They are also used when panned, cropped or resized in
  track motion.</p>
  
  <p>Bi-Cubic/Integration - Best image resizing algorithm available in
  Vegas. Quality differences will be most noticeable when using very
  large stills or stretching small sources.</p>
  
  <p>Bi-linear - Best compromise between speed and quality. This method
  will produce good results in most cases.</p>
  
  <p>Point Sampling - Fast but produces poor results.</p>
</blockquote>

<hr>

<blockquote>
  <p>Field Handling:</p>
  
  <p>This refers to the field conformance stage of Vegas's video engine.
  This includes Interlaced to Progressive conversion, Interlaced to
  interlaced output when scaling, motion or geometric Video FX and
  Transitions are involved. Skipping this stage can sometimes result in
  bad artifacts when high motion interlaced sources are used.</p>
</blockquote>

<hr>

<blockquote>
  <p>Field Rendering:</p>
  
  <p>When the output format is interlaced, Vegas will internally render at
  the field rate (twice the frame rate) to achieve smooth motion and FX
  interpolation.</p>
</blockquote>

<hr>

<blockquote>
  <p>Frame Rate Resample / IFR (Interlace Flicker Reduction):</p>
  
  <p>Frame Rate Resample:</p>
  
  <p>This kicks in when speed changes are made through Velocity Envelopes
  and/or event stretching. In can also be used when up-converting low
  frame rate sources. This only kicks in if the resample switch is
  turned on <em>and</em> quality is set to good or best.</p>
  
  <p>Interlace Flicker Reduction:</p>
  
  <p>This kicks in if the event switch is turned on and quality is set to
  good or best. See Vegas' documentation for a description of this
  switch.</p>
</blockquote>
","17282"
"Is it possible to have stereo sound from an external mic on a Canon Vixia HF G20","953","","<p>I have a Canon Vixia HF G20 and I cannot find anywhere in the manual about an external mic in stereo mode. The ""advanced hot shoe"" does support stereo and surround sound but through the Line In/Mic port on the camera I can't seem to produce stereo sound. My source is definitely stereo. Any suggestions?</p>
","<p>According to the Using Commercially Available Microphones section on page 76 of the manual for the HF G20, it should be possible to connect a stereo microphone to the external mic connection.  There may be some problem with the kind of a source you are using or the way you are connecting it.</p>
","10160"
"How to encode video to profile high using ffmpeg?","951","","<p>This is my first post here. I am in a situation where my original file is of profile High and transcoding profile is High10. I am suspecting that this is the cause my video is not getting played in jwplayer. I have also tried to set profile to <code>-profile:v high</code>
but no help . I am using ffmpeg version </p>

<p>ffmpeg version 2.6.2</p>

<pre><code>No pixel format specified, yuv420p10le for H.264 encoding chosen.
Use -pix_fmt yuv420p for compatibility with outdated media players.
x264 [error]: high profile doesn't support a bit depth of 10
[libx264 @ 0x350d960] Error setting profile high.
[libx264 @ 0x350d960] Possible profiles: baseline main high high10 high422 high444
</code></pre>

<p>Please help me.</p>
","<p>The ffmpeg build you have uses libx264 compiled for 10-bit. Apparently one can only build with support for either 8-bit OR 10-bit. So, you'll need a build where libx264 supports 8-bit.</p>
","15957"
"How to precisely synchronise an audio and video track from separate devices?","951","","<p>Yesterday I recorded a full day event with a Panasonic HC-V700 Full HD Camcorder and I also captured the audio with a Zoom H4n Handy Recorder using the audio feed provided by the venue.  At the start I captured a ""clap"" in front of the venue microphone and the video camero to synchronise the two sources.</p>

<p>How do I precisely synchronise the audio and the video tracks using the ""clap""?  </p>

<ol>
<li>I am using Adobe Premiere CC.</li>
<li>I have got the two tracks almost synched.</li>
<li>My Adobe Premiere skill level is basic.</li>
<li>I can't work out where the nudge tool is to move the audio track via the keyboard.</li>
</ol>
","<p>First off, if you have audio with your video (that you want to replace with your high-quality audio recording), then step #1 is to nudge the Zoom audio track to line up the impulse of the clap with the corresponding impulse of the scratch audio from the video file.  To do that, you need to <a href=""https://helpx.adobe.com/premiere-pro/using/timecode.html#changehowtimecodeisdisplayed"">change the Timecode Display Format to Audio Samples</a>.  Once you do that, audio can be nudged a sample at a time, which makes it possible to get two audio files to agree, rather than a frame at a time, which makes it impossible.</p>

<p>To learn the keyboard shortcuts (which include the Nudge commands), print and study <a href=""https://helpx.adobe.com/premiere-pro/using/default-keyboard-shortcuts-cc.html"">this page</a> from the Adobe manual.  TL;DR: Nudge right is Alt+Right on Windows and Cmd+Right on OSX.  To nudge five frames at a time, add a Shift.  To nudge left instead of right, change from the Right Arrow to the Left Arrow.</p>

<p>Once you have your high quality audio aligned with your principal video, it's time to sync all the rest of your video with your principal video.  If you shoot at 24fps and capture audio at 48kHz, a single frame of video is 2000 audio samples, which is a lot of samples to sort through looking for the start of the clap!  But don't worry about that.  Once you have established principal video, all other video should be aligned based on video frames, not audio samples.  Unless you shoot with <a href=""https://en.wikipedia.org/wiki/Genlock"">genlock</a>, your video frames are not going to be perfectly in sync, which means that other video tracks could be as much as a half a frame early or late when aligned on a frame boundary.  Such is life.  Audio that's off by half a frame is not unlike sound that's coming from 21 feet away instead--about a 20ms delay.  There's nothing to do about that because (1) video needs to sync to video--you cannot slip it half a frame, and (2) you're not using audio from the other video tracks, you are using your Zoom audio, and that can only be locked to a single reference point.</p>

<p>If you are really OCD about timing, you can average your video offsets, weight them according to which is most prevalent/important, and align the audio so that lies at that weighted average point.  I don't know anybody who does this, but if you have two video streams that are 1/2 frame apart (worst case) and you split the difference with the audio, your audio/video error changes from 1/48th of a second worst case to 1/96th of a second worst (and best) case.  Only you can tell if that's preferable to you.</p>
","18167"
"h.264 Audio Out of Sync in Premiere","950","","<p>I shot some video at an event last night with an ATEM TV Studio and recorded it via USB to my computer.</p>

<p>The files themselves play fine with VLC or Windows Media, but when I import them into Premiere the audio is out of sync with the video. </p>

<p>These files were captured with Media Express on 1080i5994. Capture format says AVI 8bitYUV.</p>

<p>Any thoughts or suggestions?</p>
","<p>No that wouldnt work because the frame rate was being interpreted incorrectly causing the video to be half the length of the audio. And trying to fix the interpretation of the video only slowed down the audio. It was weird. I had to reencode the files and then edit.</p>

<p>For future reference in Media Express make sure the capture format is set to progressive. </p>
","15706"
"Add transparent background to ffmpeg with text over it","947","","<p>How to correctly add expression to transparent background like on this question <a href=""https://video.stackexchange.com/questions/15551/ffmpeg-drawtext-filter-create-transparent-background-with-text"">ffmpeg drawtext filter - create transparent background with text</a>
for my case (where there are 3 lines in bottom right corner):</p>

<pre><code>ffmpeg -i videoin.mp4 -vf ""[in]drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline1':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-130, drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline2':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-75, drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline3':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-20[out]"" -codec:a copy videoout.mp4
</code></pre>
","<p>Use the box options i.e. </p>

<pre><code>drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline1':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-130:box=1:boxborderw=12:boxcolor=black@0.45
</code></pre>

<p>This adds a box with padding of 12 pixels around the text boundary, and the color is black with 45% opacity.</p>

<p>Using <strong>drawbox</strong> (add it before the drawtexts)</p>

<pre><code>drawbox=y=ih-140:h=110:c=black@0.45:t=max:enable='between(t,2.5,6.5)'
</code></pre>

<hr>

<p>Here's the entire vf expression:</p>

<pre><code>-vf ""drawbox=y=ih-140:h=110:c=black@0.45:t=max:enable='between(t,2.5,6.5)',drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline1':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-130, drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline2':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-75, drawtext=enable='between(t,2.5,6.5)':fontsize=50:fontcolor=White:fontfile='ariblk.ttf':text='textline3':x=min(4*(tw\+10)-(abs(4-2*(t-2.5)))*(tw+10)-tw\,10):y=h-th-20"" -codec:a copy videoout.mp4
</code></pre>
","17523"
"Playing back 1080p and 720p videos (same bitrate) on a 720p display","947","","<p>Suppose a high quality, uncompressed video is re-encoded twice, with almost identical encoding settings at a relatively low bitrate of 4mbps, but at different resolutions, first at 1080p, and then at 720p. </p>

<p>So we now have two videos: One is 1080p and the other one is 720p. Both have identical bitrates. </p>

<p>I know that 4mbps is considered a low bitrate for 1080p, and the video will look bad on a full-HD display due to the compression artifacts. </p>

<p>However, during playback on a <strong>720p</strong> display, the 1080p video will obviously be downscaled to 720p. </p>

<p>My question is: On the <strong>720p display</strong>, will the 1080p video <em>(which is being downscaled to 720p in real time during playback)</em> look as good as the 720p video <em>(which has the same bitrate)</em>? Or will the 720p video <em>(which was downscaled from 1080p to 720p by the encoder)</em> look better? Or will both of them look the same?</p>

<p>I would run some tests to see this for myself, but my system isn't powerful enough for 1080p encoding. I'm looking for some expert views on this. </p>
","<p>When you ask how something will 'look' you're in the realm of the subjective. Things 'look' different to experienced professionals than they might to the average viewer.</p>

<p>But still, the 1080p video will not be as good no matter what you view it on. When you encode at a low bit rate you aren't removing pixels, you're mostly removing high frequencies. Doing either will reduce the amount of information, but in different ways and with different visible effects and artifacts.</p>

<p>The information you lose with very lossy encoding doesn't lie in the spaces that inevitably are discarded when you downscale. You'd lose in both domains. But how that 'looks' depends to some degree on who's looking.</p>
","10768"
"Update keyframe animations only every third frame in After Effects","943","","<p>I am new to After Effects and I have the following problem with an animation of mine:</p>

<p>I have a walk cycle drawn for 8 frames per second.
The composition has 24 frames per second, so each frame of the animation is shown for three frames. The background moves using keyframe animation.</p>

<p>But now the background moves every frame whereas the animation only moves every third frame, which looks odd as the feet seem to slip. If I change the frame rate to 8fps, it looks fine.</p>

<p>So is it somehow possible to change the movement of the background so that the position is only updated every third frame?</p>

<p>I would like to avoid changing the frame rate of the whole composition to 8fps, as then other parts of the animation become less smooth.</p>
","<p>I don't know if there is an easy way to do it directly.  You could do some modular math in a script for the keyframe positions, but that's kind of complicated and non-user friendly.  An alternate approach would be to do an additional nested composition.  </p>

<p>Make an animation composition for your character that is at 8 FPS, perhaps bring in the main composition as a backplate while putting it together, then remove the backplate and import the animation composition at 24fps as a stationary composition.</p>
","9090"
"After Effects - Element 3D: Text arranged in a circle/ring form","941","","<p>I need to dispose some letters extruded with Element 3D plug-in in After Effects
Like in this fast example:</p>

<p><a href=""https://i.stack.imgur.com/UnE2r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UnE2r.png"" alt=""enter image description here""></a></p>

<p>I'm going crazy! Thank you in advance!</p>
","<ol>
<li>Make some text</li>
<li>Draw a circle mask within it and set mode to 'none'</li>
<li>Go text's Path option and select mask You've created</li>
<li>Adjust mask or/and font to get desirable result</li>
</ol>

<p>Continue to work with Element 3D as in this tutorial: <a href=""https://www.youtube.com/watch?v=x0p9gwKMSqk"" rel=""nofollow"">https://www.youtube.com/watch?v=x0p9gwKMSqk</a></p>
","12053"
"I have a TS file. How do I render it?","940","","<p>(I decided to post this here because it doesn't fit very well into the Gaming category, but it is a gaming question.)</p>

<p>I had the idea to try streaming video to Twitch one on computer and to record it with VLC on another, using Livestreamer. The result is a TS file. </p>

<p>If I wanted to upload it to YouTube, how would I convert it to mp4 (Or is there a better format?) without loosing quality?</p>

<p>Also, I hear people talk about rendering their video. I understand that when a video is first recorded it is in 'raw' format. What does rendering entail?</p>
","<p>""Raw"" ususally refers to uncompressed video (though this is <em>technically</em> wrong), meaning you probably have a very large file. Rendering is the process of re-encoding the video using another codec, usually to reduce file-size while maintaining a decent enough video quality. The reason that .mp4-files with the H264-codec are widely used is that they offer a very good filesize-quality-ratio.</p>

<p>In your case, a very compressed file is <strong>not</strong> what you might want, since Youtube reencodes your video when you upload it. Every reencoding reduces the video quality, so the double reencoding will result in poor quality.</p>

<p>If you cant upload the .ts-file directly (I don't think Youtube allows this), you could reencode it to an mp4 or avi (imho, the latter is better in this case, though getting the encoding settings right is a bit tougher for beginners). The key is to chose a very <strong>high bitrate</strong> and the <strong>same framerate and resolution</strong> as the original video. Using a high bitrate, the quality-loss resulting from the reencoding can be minimized. However, this will result in very large files (espacially if you choose avi), which depending on your internet connection might take very long to upload.</p>

<p>There is a range of free encoding programs out there. I'd recommend <a href=""http://www.xmedia-recode.de/en/index.html"" rel=""nofollow"">XMedia-Recode</a>, but you could also try <a href=""http://www.xilisoft.com/"" rel=""nofollow"">Xilisoft Video Converter</a> or <a href=""http://www.pcfreetime.com/"" rel=""nofollow"">Format Factory</a>.</p>

<p><strong>Edit:</strong> If you want to <strong>cut</strong> your video (i.e. add an intro and an outro, put different videos together, remove unwanted parts, ...), you'll need a video editing program. There are some freeware programs for this (none of which are as good as professional, paid programs), you can find a <a href=""http://en.wikipedia.org/wiki/List_of_video_editing_software"" rel=""nofollow"">list of video editing software at wikipedia</a>.</p>
","15006"
"What do the colors in the timeline render bar in After Effects signfify","936","","<p>I have searched the web and PDF manual for clarification on what the colors signify on the render bar over the timeline in after effects but for some reason I cant find a resource covering them all.</p>

<p>Here's what I've found</p>

<ul>
<li>No Color - not rendered</li>
<li>Green - rendered into RAM</li>
<li>Blue - rendered into Global performance Cache</li>
</ul>

<p><strong>My questions</strong></p>

<ul>
<li>Is this above correct?</li>
<li>Are there any other colors? - <em>( I had thought there was a yellow and a red but I think these are used in Premiere Pro not After Effects )</em></li>
</ul>
","<p>All of the information is correct, to be clear:</p>

<ul>
<li>Green cached on RAM</li>
<li>Blue cached on disk</li>
</ul>

<p>Pressing 0 on the num pad you will notice that on the blue line frames will render faster then on the none line.
Of course it will skip the green line because this process is called ""Preview RAM"" and as I have just said <em>Green</em> means written on the RAM.</p>

<p>No any other colours in AE.</p>
","18978"
"What happens to frames when you speed up a video clip?","933","","<p>There is a 60 FPS video clip, with 1 Second total duration (So it has 60 frames, each 1/60s duration)</p>

<p>I make it 2 faster. So it's has a 0.5s total duration. How about frames? </p>

<p>How many frames are in my 0.5s video? <strong>60 frames each 1/120s, or 30 each 1/60s?</strong></p>

<p>In a tutorial the author says: ""I recorded those images at 60 frames a second and time remapped the video to about 2-3x getting about 120-180 frames to work with.""</p>

<p>I just wonder if speeding up (in this case time remapping) the video clip for 2x will result in removal of half of the frames, how the tutorial is saying it give you 120 frames to work with? </p>
","<p>In the simplest case, half the frames -- every second frame, or all the even (or odd) numbered original frames -- are just gone, discarded. The frame rate of the resulting video does not change, it's still 60 fps.</p>

<p>It's possible to create a video where each new frame is a blend of two original frames but this is unusual and often not effective. Or in a field-based system where each original frame is composed of two distinct half vertical resolution images, you can discard half the fields instead of half the frames. But still, the result will be 60 fps.  </p>

<p>If you were working in a 60 fps environment and imported a 30 fps clip, you could effectively double the frame rate of the original clip, but that isn't what you asked about.</p>
","15860"
"Mount a Camcorder on my Hat?","930","","<p>Aside from just gluing (or velcroing) the camcorder onto a baseball hat beak, is there something better? Would this type of thing work?</p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/B003LRWB3S"" rel=""nofollow"">http://www.amazon.com/Calumet-Quick-release-Video-Head-Adapter/dp/B003LRWB3S/ref=sr_1_102?s=photo&amp;ie=UTF8&amp;qid=1347397880&amp;sr=1-102</a></p>

<p>or is it irrelevant. If your answer is ""get an Go Pro"" or whatever, I still need to know how to mount it. </p>

<p>Thanks!</p>
","<p>The key thing to remember about mounting any camera is stability. What makes Go Pro so attractive to many cinematographers are the many mounts available for it that fit the camera snugly on a helmet, car, surf board, or what have you. There will always be some motion when attaching a camera to say a helmet, but with the Go Pro helmet mount, you have minimized any residual motion. The only motion that is left is that of the helmet moving.</p>

<p>Trying to mount a camera to a baseball hat and expecting the stability of a helmet mount Go Pro or Contour camera is not easy or trivial.</p>

<p>First of all a baseball cap is not rigid like a helmet, there is flex built into the cap even in the beak. Adding an adapter may make things worse. You will have to experiment. Consider ways to make the cap as rigid as a helmet and ways to attach the camera so it moves with the cap without any swaying or extra movement.</p>

<p>For every design you think will work you will need to make test shots that cover the conditions of the actual production.</p>

<p>I think this can be done. But without your camera or your cap in my hands I would not venture a guess on how you go about it. This is truly hands on experimental kludge-0-matic work.</p>
","4712"
"What is the difference between a fishpole and a boom pole?","925","","<p>What is the difference between a fishpole and a boom pole? I think I have heard the words interchangeably, but I really don't know the difference between the two.</p>
","<p>To be honest, man, I've never heard anyone use the term 'fishpole' in regards to video/film production. Recording audio with a microphone that is connected to an extendable rod is called booming. There are people in the film industry who make a living solely from being a boom swinger. 
I think fishpole is just another word used for the same thing, although if someone referred to a boom pole as a fishpole onset they would probably get laughed at. </p>

<p>So yeah, unless you are getting filmmaking mixed up with the honorable profession of capturing sea-dwelling animals, I'd say there is no difference between the two. </p>
","2173"
"gource and ffmpeg reduce video size","924","","<p>I try to create video with <a href=""http://code.google.com/p/gource/wiki/Videos"" rel=""nofollow"">gource</a>. I succeed with the following command line:</p>

<pre><code>gource $LOG_FILE --log-format custom --stop-at-end --caption-file ${FILENAME}_captions.log --caption-duration 3 --title $TITLE --seconds-per-day 0.7 --auto-skip-seconds 1 --date-format\
 ""%d/%m/%y"" --hide ""mouse,progress"" --user-scale 0.6 --caption-size 20 -1380x950 -o - | ffmpeg -y -r 60 -f image2pipe -vcodec ppm -i - -vcodec libx264 -preset ultrafast -pix_fmt yuv420\
p -crf 1 -threads 4 -bf 0 $FILENAME.mp4
</code></pre>

<p>But the video are too big, 9Go for 4 min. How can I reduce the size with ffmpeg?</p>

<p>I use Fedora.</p>
","<h1><code>-preset</code></h1>

<p>Use the slowest preset that is fast enough that it does not drop frames. You can see if <code>ffmpeg</code> is dropping frames in the console output (if I recall correctly). Presets are: ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow.</p>

<h1><code>-crf</code></h1>

<p>Use the highest <code>-crf</code> value that still provides an acceptable quality level. Range is 0-51. 0 is lossless, 18 is generally considered to be visually lossless or nearly so, 23 is default, and 51 is worst quality. Using a value of 1 will likely result in a huge file.</p>

<h3>Also see:</h3>

<ul>
<li><a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"">FFmpeg H.264 Video Encoding Guide</a></li>
</ul>
","12522"
"Downloading earlier segments from a live m3u8 playlist","923","","<p>I have an <code>.m3u8</code> URI of type <code>LIVE</code>. As far as I know, live playlists use a sliding window instead of containing all the segments. My questions are,</p>

<hr>

<p><strong>1)</strong> Is it possible to find out what the length of the window is (time or frame-wise)? My intention is to use the playlist I have to download a live-stream starting from an earlier time.</p>

<p><strong>2)</strong> If yes, how do I get the earlier segments, i.e., how do I specify where I want to start downloading from? I tried something like <code>ffmpeg -ss -00:00:10 -i ""in.m3u8"" out.mp4</code>, but it did not work.</p>

<p>I do not have much experience in video-encoding or live-streaming, and I would appreciate any direction!</p>
","<p>You need to download the m3u8 itself, look at the entries and see if the individual TS filenames are entirely regular i.e. no signatures which vary with each entry, and the varying elements that are present can be deduced.</p>

<p>If so, then simply work backwards i.e. if the playlist you download has entries <code>file_03532.ts</code>, <code>file_03533.ts</code>, <code>file_03534.ts</code>..., then <code>file_03529.ts</code>, <code>file_03530.ts</code>, <code>file_03531.ts</code>.. would be the earlier entries. You can then use a download manager that lets you download a list of filenames generated using a pattern. <a href=""https://www.google.co.in/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi92vny6I3OAhVEsI8KHa9uDzkQFggbMAA&amp;url=https%3A%2F%2Fwww.internetdownloadmanager.com%2Fdownload.html&amp;usg=AFQjCNHDbklZbAFiY4rOWNN9WD5bzjAblg&amp;sig2=C5g0pxZ81PzNWxev4RbLJw&amp;bvm=bv.127984354,d.c2I"" rel=""nofollow"">IDM</a> does this. There's probably others.</p>
","18989"
"What is a description along the bottom of a video generally called?","921","","<p>Sorry about the terrible title question. I'll edit it if I get an answer to be more useful to others, or you're welcome to do so.</p>

<p>I'm trying to find tutorials and even inspiration for stuff like the DKS bar in this picture:</p>

<p><img src=""https://i.stack.imgur.com/Tv9RH.jpg"" alt=""image from video""></p>

<p>I looked up ""trade show videos"" ""after effects titles"" ""after effects text"" and haven't really found much that's helping. Is there a name used in the video world for that part of a video which might help me find more examples?</p>

<p>Here's the video set to this moment so you can see it all animated:</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/ys4mj8266zA?start=24""></iframe>
            </div></div></p>
","<p>It is often called a lower third because it is a bar often appearing in the lower third of the image.  Also sometimes a name plate or title plate.</p>
","9829"
"How to remove an unwanted dynamic object from a dynamic footage","916","","<p>I need to remove an object (in my case - a newspaper) that is being hold by a man who moves it a bit from side to side and sometimes it is hidden behind some other objects. How do I do this?</p>

<p>My idea is to use rotobrush in After Effects and somehow fill it using 'content aware fill', but I cannot find anything like 'content aware fill' option. I have seen some tutorials on web about how to  remove an object from a steady background, but this case is more complicated.</p>

<p>PS I don't want to mask it frame by frame in Photoshop.</p>
","<p>If the camera and background are both stationary (or perhaps very near stationary) you should be able to extrapolate a background plate from looking for frames where the man and paper are not blocking that part of the shot.  Once you have that background plate, you can mask the newspaper and expose the appropriate portion of the background plate.</p>

<p>This works best if the camera isn't moving, though you may be able to do a camera tracking in the event of small camera movements and still get workable results by matching the position and angle of the background plate to the camera (though it will likely require far more advanced background plates to account for any perspective changes).  With anything more than minimal movement of the camera though, the perspective changes will become too severe to compensate for and you won't be able to build a background plate from previous/future frames.</p>

<p>At that point, you would actually have to build a model of the scene in order to be able to render out the background to fill in with perspective adjustments and that would be far more complicated than re-shooting.</p>

<p>Note that this also does still require frame to frame adjustment of the mask and requires building the back plate by hand as well.  It is still non-trivial, but isn't quite masking frame by frame in Photoshop since the rotoscope tool should work for it.</p>

<p>If shadows are visible on the background as well, then you may need to make some animated masks to emulate the shadows on the background plate as well.  Either way, we're talking about a pretty elaborate operation most likely depending on the quality you need.  If any shadows from the actor are visible though (particularly soft shadows against not flat objects) the quality level will drop like a rock and the complexity will go higher fast.</p>

<p>If you do get hit with shadow issues or background movement issues (and reshooting really isn't an option) you may actually be served best by replacing the background entirely).  If you build a fixed background plate that doesn't have any shadows from the actors, people are not super likely to notice the lack of shadow compared to a gap in the shadow or a reconstructed shadow.  It would involve even further rotoscoping, but might be easier than trying to build a believable shadow.</p>

<p>Reshooting really is the best option, even in the ideal case though.</p>
","12289"
"Convert high frame video to slow motion video","914","","<p>I recorded a ""slow-motion"" video with my Android phone and transferred it to my Windows computer. Now I have an MP4 file with a frame-rate of 240 fps. But when I play the video, it plays at normal speed.  I want to create a video that I can upload to YouTube or Facebook that plays at 1/8 speed (30 fps).</p>

<p>I thought I could do that with the free Movie Maker that comes with Windows. But when I select 1/8 speed it comes out chunky. It looks like the software converted the video to 30 fps when it loaded, and is now playing back that 30 fps video at 1/8 speed.  I'm guessing that Movie Maker can't do what I want.</p>

<p>How do I create a video so that when someone plays that video they see it in slow-motion?</p>
","<p>You can use <a href=""https://ffmpeg.zeranoe.com/builds/"" rel=""nofollow"">ffmpeg</a>, a free command-line tool, to do this:</p>

<pre><code>ffmpeg -i input.mp4 -vf setpts=8*PTS -r 30 -crf 18 output.mp4
</code></pre>

<p>This slows down the video 8 times and output is 30 FPS.</p>
","19325"
"FFMPEG duplicates the first frame when encoding","910","","<p>Using <em>ffprobe</em> shows that both videos have the same framecount, still if I extract the frames as images the compressed one has an extra frame at the beginning (which is just the duplicated first frame).</p>

<p>This causes me a lot of problems since I have to compute the frame differences between the two videos and having an extra frame makes everything out of sync.</p>

<p><em>Input</em></p>

<pre><code>ffmpeg -i ""720p50_mobcal_ter.avi"" -an -f mpeg2video -y ""720p50_mobcal_ter.mpg""
</code></pre>

<p><em>Output</em></p>

<pre><code>ffmpeg version N-76684-g1fe82ab Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 5.2.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55.  6.100 / 55.  6.100
  libavcodec     57. 15.100 / 57. 15.100
  libavformat    57. 14.100 / 57. 14.100
  libavdevice    57.  0.100 / 57.  0.100
  libavfilter     6. 15.100 /  6. 15.100
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, avi, from '720p50_mobcal_ter.avi':
  Metadata:
    encoder         : Lavf57.14.100
  Duration: 00:00:10.08, start: 0.000000, bitrate: 552974 kb/s
    Stream #0:0: Video: rawvideo (I420 / 0x30323449), yuv420p, 1280x720, 554059 kb/s, SAR 1:1 DAR 16:9, 50 fps, 50 tbr, 50 tbn, 50 tbc
Output #0, mpeg2video, to '720p50_mobcal_ter.mpg':
  Metadata:
    encoder         : Lavf57.14.100
    Stream #0:0: Video: mpeg2video, yuv420p, 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 50 fps, 50 tbn, 50 tbc
    Metadata:
      encoder         : Lavc57.15.100 mpeg2video
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; mpeg2video (native))
Press [q] to stop, [?] for help
frame=   41 fps=0.0 q=31.0 size=     984kB time=00:00:00.78 bitrate=10330.5kbits/frame=   80 fps= 78 q=31.0 size=    1323kB time=00:00:01.56 bitrate=6948.1kbits/frame=  124 fps= 80 q=31.0 size=    1725kB time=00:00:02.44 bitrate=5790.0kbits/frame=  168 fps= 81 q=31.0 size=    2084kB time=00:00:03.32 bitrate=5142.8kbits/frame=  212 fps= 81 q=31.0 size=    2482kB time=00:00:04.20 bitrate=4841.4kbits/frame=  255 fps= 82 q=31.0 size=    2840kB time=00:00:05.06 bitrate=4597.2kbits/frame=  296 fps= 82 q=31.0 size=    3133kB time=00:00:05.88 bitrate=4364.5kbits/frame=  338 fps= 82 q=24.8 size=    3453kB time=00:00:06.72 bitrate=4209.2kbits/frame=  382 fps= 82 q=31.0 size=    3723kB time=00:00:07.60 bitrate=4013.4kbits/frame=  426 fps= 83 q=31.0 size=    4005kB time=00:00:08.48 bitrate=3869.1kbits/frame=  470 fps= 83 q=24.8 size=    4276kB time=00:00:09.36 bitrate=3742.5kbits/frame=  504 fps= 83 q=31.0 Lsize=    4469kB time=00:00:10.06 bitrate=3639.3kbits/s
video:4469kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%
</code></pre>

<p>My current solution is to trim the compressed video before computing the differences, but this involves re-encoding and quality loss, making the results useless.</p>

<p>To make the question a bit more complete, here's the commands that I'm using to trim and compute the differences:</p>

<pre><code>ffmpeg -y -i ""720p50_mobcal_ter.mpg"" -an -f mpeg2video -vf select=gte(n\,1) ""cut-720p50_mobcal_ter.mpg""


ffmpeg -y -i ""720p50_mobcal_ter.avi"" -i ""cut-720p50_mobcal_ter.mpg"" -filter_complex ""blend=all_mode=difference,hue=s=0"" -c:v libx264 -crf 18 -c:a copy ""differences.mpg""
</code></pre>

<p>The question is: how can I avoid having the first duplicated frame? If this can't be avoided, is there a way to remove it without re-encoding the video or just skipping it while computing the differences?</p>

<p>EDIT:</p>

<p>Following @Mulvya's answer, I tried his suggestion but it doesn't work somehow. I understand that it's hard to find the problem without having the video file so, instead, I'm posting a screenshot with the first 4 frames of each video to help you visualize my issue.</p>

<p><a href=""https://i.stack.imgur.com/dErAF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dErAF.png"" alt=""enter image description here""></a></p>

<p>As you can see, the compressed video has a grey third frame that messes up the differences.</p>

<p>The reason behind this edit is that I'm sure that the given answer should work, at least from my understanding on how filter mappings work, but it probably doesn't work because the issue here is not what I thought it was.</p>

<p>I expect to have an almost black third frame on the differences video, and not the full frame from the original one.</p>

<p>EDIT 2: Tried the new solution which somehow worked the first time but now it doesn't anymore.</p>

<p>Here's the list of the commands that I'm using, the result is still the same of the screenshot but now the problem has moved to the second frame instead of the third one (basically it's like how the screenshot would be without the <em>00000.png</em> frames)</p>

<pre><code>ffmpeg  -i ""720p50_mobcal_ter.avi"" -an -f mpeg1video -y ""compressed.mpg"" 
ffmpeg  -i ""720p50_mobcal_ter.avi"" -vf fps=50 ""original\image-%05d.png""
ffmpeg  -i ""compressed.mpg"" -vf fps=50 ""compressed\image-%05d.png""
ffmpeg  -y -i ""720p50_mobcal_ter.avi"" -i ""compressed.mpg"" -filter_complex ""[1:v]trim=start_frame=1,setpts=PTS-STARTPTS[cut];[0:v][cut]blend=all_mode=difference,hue=s=0"" -c:v libx264 -crf 18 -c:a copy ""differences.mpg"" 
ffmpeg  -i ""differences.mpg"" -vf fps=50 ""differences\image-%05d.png""
</code></pre>

<p>The source video that I'm using is <a href=""https://media.xiph.org/video/derf/y4m/720p50_mobcal_ter.y4m"" rel=""nofollow noreferrer"">here</a> just in case you may want to try it yourself.</p>
","<p>You can skip that first frame during the computation process.</p>

<pre><code>ffmpeg -y -i ""720p50_mobcal_ter.avi"" -i ""720p50_mobcal_ter.mpg"" -filter_complex ""[1:v]trim=start_frame=1,setpts=PTS-STARTPTS[cut];[0:v][cut]blend=all_mode=difference,hue=s=0"" -c:v libx264 -crf 18 -c:a copy ""differences.mpg""
</code></pre>

<p><strong>Edit</strong>: as requested, added the commands I used to reproduce the behaviour but I didn't experience the problem.</p>

<pre><code>ffmpeg -i ""720p50_mobcal_ter.y4m"" -c:v copy ""720p50_mobcal_ter.avi""
ffmpeg -i ""720p50_mobcal_ter.avi"" -c:v mpeg2video ""720p50_mobcal_ter.mpg""
ffmpeg -i ""720p50_mobcal_ter.avi"" -i ""720p50_mobcal_ter.mpg"" -filter_complex blend=all_mode=difference,hue=s=0 -c:v libx264 -crf 18 ""differences.mp4""

ffmpeg -i ""720p50_mobcal_ter.avi"" avi\frames%03d.png
ffmpeg -i ""720p50_mobcal_ter.mpg"" mpg\frames%03d.png
</code></pre>

<p>My ffmpeg build, taken from Zeranoe</p>

<pre><code>ffmpeg version N-77380-g2dba040 Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 5.2.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55. 11.100 / 55. 11.100
  libavcodec     57. 18.100 / 57. 18.100
  libavformat    57. 20.100 / 57. 20.100
  libavdevice    57.  0.100 / 57.  0.100
  libavfilter     6. 21.100 /  6. 21.100
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
</code></pre>
","17262"
"Remove audio from video annotation in ActivePresenter","910","","<p>I've been creating screencasts lately (cut with real footage) with ActivePresenter.</p>

<p>ActivePresenter allows one to add ""annotations"" and they can be in various formats including both audio and video clips. I would like to overlay a video clip onto my screencast, but my issue is, I cannot mute or remove the audio from the video clip in ActivePresenter. When accessing the video's properties, there is a volume slider there but it is disabled.</p>

<p>I can remove the audio using a different tool, and then import into ActivePresenter, but I thought this would be pretty standard functionality. So, the question - is there a way to remove audio from a video annotation in ActivePresenter? The format of the video annotation is mp4 (I'll try with other various formats and see if there are formats which ActivePresenter will provide this as an option).</p>
","<p>Yes, you can remove audio from any video in ActivePresenter. To do that, please follow the <a href=""http://atomisystems.com/community/4508/how-to-i-cut-sound-out-of-a-video-i-import-into-software"" rel=""nofollow"">instruction in ActivePresenter's community</a>: </p>

<blockquote>
  <p>You just need to select the video, click Adjust Volume tool on the
  Timeline, and select Mute checkbox.</p>
</blockquote>
","14711"
"Record on laptop from streaming camera via RCA cable","908","","<p>I have a video camera that only 'streams' the images and has no way of recording them. Currently I'm plugging the RCA into a TV and then recording from that, but I need to be mobile and so need to record onto a laptop.</p>

<p>Will something like this (<a href=""http://www.amazon.co.uk/tag/av%20to%20usb%20converter/products"" rel=""nofollow"">http://www.amazon.co.uk/tag/av%20to%20usb%20converter/products</a>) work?</p>

<p>Any other ideas?</p>
","<p>EasyCap 2.0 DC60+ is what I use, and they work great. Make sure you get the DC60+ version, you don't want the older versions.</p>

<p>There are a lot of knock-offs floating around--I'd stick with <a href=""http://www.ezcap.tv/usb-video-capture/ezcap116-capture-card"" rel=""nofollow"">the official one.</a></p>

<p>There's also a good compilation of capture cards on <a href=""http://forum.telestream.net/forum/messageview.aspx?catid=44&amp;threadid=9881&amp;enterthread=y"" rel=""nofollow"">this thread</a> on the Wirecast Forum.</p>
","7319"
"Choosing the right format/codec for export","905","","<p>I have a few recordings from my <a href=""http://www.trustedreviews.com/Canon-IXUS-90-IS_Digital-Camera_review"" rel=""nofollow"">Canon IXUS 90 IS</a> that I have combined into one sequence. I am using this small project as a way to get to know <strong>Adobe Premiere Pro CS5</strong>.</p>

<p>The pieces of video from the camera are obviously already compressed, and I would like to export the finished product in an equally compressed format to reduce the file size. Which format/codec is best suited for this task? Am I fine with the ones that are already available to me in Premiere, or should I download another somewhere?</p>

<p>I've already tried the <strong>Windows Media</strong> codec, but I get the error ""<strong>The source and output audio channels are not compatible or a conversion does not exist</strong>"" and I have no idea what to do about this.</p>

<p>Video details:</p>

<ul>
<li>File type: AVI</li>
<li>Video: 640x480, 30 fps</li>
<li>Audio: 44100 Hz, 16-bit, Mono</li>
</ul>

<p>I am fine with having to fiddle with the parameters in order to get the quality just right. I just need a pointer in the right direction.</p>

<p>Please let me know if further details are needed.</p>
","<p>I use <a href=""http://vimeo.com/help/compression"" rel=""nofollow"">these</a> guidelines when uploading to the internet. I know that's not what you're trying to do but it's a starting point. I find there is very little loss of quality and massively reduced file size.</p>
","1943"
"Does the M4V file type have direct support for Audio?","903","","<p>I am working with a video vendor who just ripped a dvd and sent me BOTH m4v files AND aac files (audio and video separately),. When I asked him why he did not just send 1 m4v file with the audio and video in ONE FILE, he wrote me back this (Which I need to confirm is true or false):</p>

<blockquote>
  <p><em>.M4V is a video format that doesnt directly support audio so the computer automatically rendered the audio files out as .AAC. What you
  say is an .MP4 format is really just the ""file type"" and not the
  extension itself.</em></p>
</blockquote>

<p>Doesn't sound quite right, but I am not a video pro. But I do not want to have to splice all these files together again.</p>

<p>Please advise: Is his statement (above) correct?</p>

<p>Thanks, sleeper</p>
","<p>I find the m4v extension a bit confusing.</p>

<p>For MPEG-2, the .m2v extension is associated with a raw video stream. Those files can have only video.</p>

<p>Now for MPEG-4, Apple uses the .m4v extension as a container format, and in fact if you rename an .m4v file to .mp4 it is likely to play on players that only read .mp4 files, since the two format are essentially equivalent.</p>

<p>I think you would need to check this .m4v file and make sure it is really the container format, and not an elementary video stream similar to what you find in .m2v files for MPEG-2.</p>

<p>If you feel comfortable inspecting files with a hex editor, just open this .m4v file, skip the first four bytes and check if the next four are the letters 'ftyp'. If they are, then you have an MP4 container file that should have no trouble holding audio along with the video. If, on the other side, you've got something else, then you may have an elementary stream, in which case you may want to recommend to your vendor to use a different extension for these files. Using .avc is probably more appropriate for an H.264 elementary stream.</p>
","3186"
"After Effects on Mac: Use touchpad to scroll like in PS or Illustrator","898","","<p>In Photoshop and Illustrator I can 2-finger scroll on the x and y axis. For zooming I can pinch-to-zoom which works seamlessly.</p>

<p>However, Adobe After Effects reacts totally different to this: Pinch-to-zoom results in zooming into the top left corner so the video output ends up ""flying"" out of the lower-right corner of the screen and two-finger-up-and-down-scroll zooms at the center of the video. two-finger-left-and-right-scroll, instead of moving along the x axis, has no effect whatsoever.</p>

<p>I could get used to that if I worked on a PC but why can't I make use of the wonderful touchpad on my mac? It works in other Adobe CC products that I own, so why is After Effects behaving so weird? It's not just that it behaves <em>different</em>, I see no reason why sidewards-moving is not working and why anyone would want to zoom into the upper left corner of their screen.</p>

<p>Example:</p>

<p>This is what happens when I pinch-to-zoom. The black rectangle in the lower right corner is the video and instead of zooming into the video, pinch-to-zoom leads to putting the video out of sight because it zooms into the top left.
<a href=""https://i.stack.imgur.com/KUE9r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KUE9r.png"" alt=""""></a></p>

<p>I hope I've written this post somewhat understandable, it's pretty hard to explain but here a little summary.</p>

<p>What I want: </p>

<ul>
<li>2-finger-touchpad-move: move along x and/or y axis</li>
<li>pinch to zoom: zoom into where my mouse pointer is
(like in Photoshop or Illustrator)</li>
</ul>

<p>What I get:</p>

<ul>
<li>2-finger-touchpad-move: zoom into video</li>
<li>pinch to zoom: zoom into the wastelands of the screen, away from the video</li>
</ul>

<p>Maybe it's just some settings that I am unable to locate but I also googled for the issue and the only results are people asking about how to create zooming animations.</p>
","<p>It's so weird the way all the Adobe apps require different muscle memory to do the same thing. Even things that are the same like panning around a work area or zooming. I understand it's because of the history of the apps, but Adobe's acquisition of AE was over two decades ago, they've had time to standardise it.</p>

<p>Until they fix this (and don't hold your breath) you'll have to learn the AE way of navigating your comp window: hold (don't tap) the <kbd>spacebar</kbd> and click-drag with the left-mouse-button / single-finger-trackpad to pan and scroll, scroll-wheel / two-finger-slide to zoom. It will to the point that is in the centre of the comp window, unless you hold the alt key while you're zooming in which case it zooms to the cursor.</p>

<p>Also useful are the <kbd>,</kbd> and <kbd>.</kbd> keys which zoom in and out (it makes more sense if you think about the &lt; and > symbols which also share those keys, but you don't hold shift) and <kbd>?</kbd> which zooms to fill the comp window. You'll find that AE works best if you work two-handed. Dominant hand using the pointing device of your choice, the other working the keyboard. Efficiency in AE is <a href=""https://cdn.tutsplus.com/ae/uploads/legacy/misc/Aetuts_Cheat_Sheet_AECS6.jpg"" rel=""nofollow noreferrer"">all about keyboard shortcuts</a>.</p>
","19937"
"How to disable continuos autofocus during video shooting with Canon SX230 HS?","895","","<p>I have a Canon SX230 HS camera and want to record a movie from a train driver's perspective on a model railway.</p>

<p>Everything is working fine, except that the camera refocuses continuously during the shoot, making your eyes hurt when watching the movie.</p>

<p>Is there any way to disable the continuous autofocus when recording a movie?</p>
","<p>See ""Shooting with the AF Lock"" on p94 of the <a href=""http://gdlp01.c-wss.com/gds/4/0300005134/01/PS_SX220HS_SX230HS_CUG_EN.pdf"" rel=""nofollow"">user manual</a>.  This is linked from p120 which is for shooting movies so I would assume it works for movies as well as stills.</p>
","8565"
"Footage of different framerates in Adobe Premiere Pro","893","","<p><strong>Background information</strong> </p>

<p>I'm going to create a audio/visual production with two camera's. One camera uses 1920 x 1080 with 25fps rate. The other camera is the GoPro camera. There are several video settings for the GoPro camera, these are: </p>

<ul>
<li>3840 x 2160 with 15 or 12.5fps</li>
<li>2704 x 1520 with 30, 25, or 24fps</li>
<li>1920 x 1440 with 48, 30, 25, or 24fps</li>
<li>1920 x 1080 with 60, 50, 48, 30, 25, or 24fps</li>
<li>1280 x 960  with 100, 60, or 50fps</li>
<li>1280 x 720  with 120, 100, 60, 50, 30 or 25fps</li>
<li>848 x 480  with 240fps </li>
</ul>

<p>After recording I'm going to merge the recorded footage from both camera's together in Adobe Premiere Pro. The settings I will use for the project are 1920x1080, 25fps. </p>

<p><strong>My question</strong> </p>

<p>Which video setting would you recommend me for the GoPro? </p>

<p>I could go for the 1920x1080 with 25fps rate. But than again I have the possibility to use 50fps. But than again, would the 50fps footage give the 25fps project somehow a quality loss?</p>
","<p>It depends on the shutter speed being used on the other camera.  If the shutter speed is faster than 1/50th of a second, then shooting at 50fps and discarding every other frame would be the same as shooting at 25fps, but it means that your shutter speed must be faster than 1/50th of a second.</p>

<p>If light isn't an issue, then the faster frame rate won't have an impact, but if light may require a higher ISO and degrade the footage unnecessarily.  Similarly, if you aren't planning to need any slow motion shots, then there isn't really any great advantage to shooting at 50fps as you gain nothing.</p>

<p>Overall, say go with 1080p and 25fps on the GoPro unless you need to do slow motion and have sufficient light, in which case go 50fps on the second camera (or better yet, do 25fps for all but the slow motion shots and move to 60fps for the slow motion shots since the frame rate will be altered anyway.)</p>
","13186"
"Difference Matte Keying software","892","","<p>I would like to know if anyone out there knows of any free software, or software not as expensive as Adobe After Effects, that can do <a href=""http://help.adobe.com/en_US/aftereffects/cs/using/WS3878526689cb91655866c1103a9d3c597-7b52a.html#WS3878526689cb91655866c1103a9d3c597-7b4ba"" rel=""nofollow"">Difference Matte Keying</a>? I have searched the web a little and have not come up with any promising results. <strong>Note: Chroma Keying is not an option. I am on a Windows 7 computer.</strong></p>

<p>Thank you in advance for your time, and possible help.</p>
","<p><a href=""http://blender.org"" rel=""nofollow noreferrer"">Blender</a>, which is <strong>free and open-source</strong>, can do difference matte keying.</p>

<p>I don't know if it's better or worse than After Effects' keyer because I haven't used it before.</p>

<p><img src=""https://i.stack.imgur.com/FDTv8.png"" alt=""Difference matte keying with Blender&#39;s node-based compositor.""></p>
","4351"
"replace a clip with a different size clip but keep the scale in Premiere CC","892","","<p>I have edited a 640 x 360 clip into a sequence in Premiere CC. The sequence includes changes to the scale for different shots. Now I want to replace the 640 x 360 clip with a 1920 x 1080 version of the same clip. When I do that, the apparent scale increases by a factor of 3; a shot in the sequence that should have a scale of 100% now APPEARS to be at 300%.</p>

<p><img src=""https://i.stack.imgur.com/9MtM9.jpg"" alt=""enter image description here""></p>

<p>I'm sure it's because the new clip is so much bigger.  But is there a workaround that lets me keep the APPARENT zoom level of the original, tiny clip?  That is, can I make this new clip LOOK like its scale is 100% when the original sequence had the scale set to 100%? Can they look the same?</p>

<p>BTW, I know I can set the scale of each shot manually in the timeline.  But I may have 50 or more edits so I'm looking for a global solution.  I was hoping there was like a ""master scale"" setting for clips in the media browser, but I didn't find one.</p>
","<p>After the fact, no I don't.  If you know before the fact in the future though, you can put the source on a timeline by itself and apply the scale there and make all the clips point at that timeline.  This is the same technique I use in After Effects whenever I am working with animating a base layer that I know I am going to need to change.  That way I can make a simple change to the source clip and not have it impact the overall sequence.</p>
","12885"
"Rendering video with pixel aspect ratio 0.9091 (in premiere CS5.5)","890","","<p>I have a video (from my Android Nexus One).  Dimensions are 720 x 480.
The pixel aspect ratio is, according to data embedded in the file, 0.9091.</p>

<p>Now, editing the video and exporting it to a media that says it has square pixel obviously makes the video look 'narrow' on the X axis.</p>

<p>The only way I could edit this in Premiere and export it to MPEG (2 or 4), was to scale each source video up to 109.9% on the X axis (since 0.9091 * 1.09 is ca 1.0).  Premiere doesn't allow me to say that the generated file as a pixel aspect ratio of 0.9091.</p>

<p>This works, but feels wrong.  Especially if I have lots of source videos, to set the scaling factor on each of them.</p>

<p>Is there a better way to do this ?</p>
","<p>If you're exporting directly from Premiere, then select the MPEG2-DVD NTSC preset in File -> Export -> Media, as shown in this <a href=""http://i1.creativecow.net/u/205208/mpeg2-dvd_settings.png"" rel=""nofollow"">image</a>. </p>

<p>Also, it looks like MP4Box will allow you to change the pixel aspect ratio flag in a .mp4 without re-encode.</p>

<p>FYI: 720x480 with PAR 0.9091 @ 29.97fps is the NTSC DV standard resulting in a 4:3 display aspect ratio. Similarly, 720x576 with PAR 1.0667* @ 25fps is the PAL DV standard.</p>

<p>*<sub>The actual PAR is supposed to be 59/54, but many apps use 16/15. See <a href=""http://www.mikeafford.com/blog/2009/03/pal-d1-dv-widescreen-square-pixel-settings-in-after-effects-cs4-vs-cs3/"" rel=""nofollow"">here</a> if you're interested.</sub></p>
","3460"
"How to stop ffmpeg from interlacing when converting from m2ts to mkv","888","","<p>I have a m2ts video file that contains a h264 video stream and an ac3 audio stream. The quality is not the best, but it is okay, but converting it with ffmpeg without any additional options makes the video interlaced and very ugly with that:</p>

<pre><code>ffmpeg -i input.m2ts output.mkv
</code></pre>

<p>Output:</p>

<pre><code>ffmpeg version 3.2.2 Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 6.2.1 (GCC) 20160916 (Red Hat 6.2.1-2)
  configuration: --arch=x86_64 --bindir=/usr/bin --datadir=/usr/share/ffmpeg --disable-debug --disable-static --disable-stripping --enable-avfilter --enable-avresample --enable-bzlib --enable-cuda --enable-cuvid --enable-libnpp --enable-doc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gpl --enable-iconv --enable-libass --enable-libbluray --enable-libcdio --enable-libdc1394 --enable-libebur128 --enable-libfdk-aac --enable-libfreetype --enable-libfribidi --enable-libgsm --enable-libkvazaar --enable-libmfx --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libv4l2 --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxcb-shm --enable-libxcb-xfixes --enable-libxcb-shape --enable-libxvid --enable-libzvbi --enable-lzma --enable-nonfree --enable-openal --enable-opencl --enable-nvenc --enable-opengl --enable-postproc --enable-pthreads --enable-sdl2 --enable-shared --enable-version3 --enable-x11grab --enable-xlib --enable-zlib --extra-cflags='-I/usr/include/nvenc -I/usr/include/cuda' --incdir=/usr/include/ffmpeg --libdir=/usr/lib64 --mandir=/usr/share/man --optflags='-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic' --prefix=/usr --shlibdir=/usr/lib64 --enable-runtime-cpudetect
  libavutil      55. 34.100 / 55. 34.100
  libavcodec     57. 64.101 / 57. 64.101
  libavformat    57. 56.100 / 57. 56.100
  libavdevice    57.  1.100 / 57.  1.100
  libavfilter     6. 65.100 /  6. 65.100
  libavresample   3.  1.  0 /  3.  1.  0
  libswscale      4.  2.100 /  4.  2.100
  libswresample   2.  3.100 /  2.  3.100
  libpostproc    54.  1.100 / 54.  1.100
Input #0, mpegts, from 'input.m2ts':
  Duration: 01:15:49.44, start: 0.984822, bitrate: 19636 kb/s
  Program 1 
    Stream #0:0[0x1011]: Video: h264 (High) (HDMV / 0x564D4448), yuv420p(tv, bt709, top first), 1920x1080 [SAR 1:1 DAR 16:9], 25 fps, 50 tbr, 90k tbn, 50 tbc
Stream #0:1[0x1100]: Audio: ac3 (AC-3 / 0x332D4341), 48000 Hz, stereo, fltp, 384 kb/s
File 'output.mkv' already exists. Overwrite ? [y/N] y
[libx264 @ 0xf21240] using SAR=1/1
[libx264 @ 0xf21240] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2
[libx264 @ 0xf21240] profile High, level 4.0
[libx264 @ 0xf21240] 264 - core 148 - H.264/MPEG-4 AVC codec - Copyleft 2003-2016 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, matroska, to 'output.mkv':
  Metadata:
    encoder         : Lavf57.56.100
    Stream #0:0: Video: h264 (libx264) (H264 / 0x34363248), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 25 fps, 1k tbn, 25 tbc
    Metadata:
      encoder         : Lavc57.64.101 libx264
    Side data:
      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1
    Stream #0:1: Audio: vorbis (libvorbis) (oV[0][0] / 0x566F), 48000 Hz, stereo, fltp
    Metadata:
      encoder         : Lavc57.64.101 libvorbis
Stream mapping:
  Stream #0:0 -&gt; #0:0 (h264 (native) -&gt; h264 (libx264))
  Stream #0:1 -&gt; #0:1 (ac3 (native) -&gt; vorbis (libvorbis))
Press [q] to stop, [?] for help
</code></pre>

<p>Additional options, like presets and crf did not change that and I could not find anything on the Internet except that ffmpeg is supposed to always use progressive unless told otherwise. I also tried hardware transcoding on my GPU, which made the transcode much much faster, but the result was about the same, still heavily interlaced video.</p>

<pre><code>ffmpeg -hwaccel cuvid -c:v h264_cuvid -i input.m2ts -c:v hevc_nvenc -preset slow output.mkv
</code></pre>

<p>Any idea what might be causing this and how to fix it? Adding <code>-deinterlace</code> helps with the interlacing, but makes the quality worse.</p>
","<p>The source is interlaced. To deinterlace it, you can use the <code>yadif</code> filter.</p>

<p>To use it with the default values:</p>

<pre><code>ffmpeg -i input.m2ts -vf yadif=parity=auto output.mkv
</code></pre>
","20221"
"ffmpeg: Convert 4K 10-bit file to DNxHR 444","885","","<p>I have a 2160p29.97 AVI 10-bit RGB file recorded with a Blackmagic Intensity Pro 4K capture card.  I want to convert this file to DNxHR 444.</p>

<p>I used the following command to do so on Windows 10 with ffmpeg <code>3.3.2</code>.
ffmpeg -i ""test.avi"" -c:v dnxhd -profile:v dnxhr_444 ""test_dnxhr444.mov""</p>

<p>I received this error:</p>

<blockquote>
  <p>""[dnxhd @ 00000000026e6c60] dnxhr_444 or dnxhr_hqx profile is not
  implemented. Update your FFmpeg version to the newest one from Git. If
  the problem still occurs, it means that your file has a feature which
  has not been implemented.""</p>
</blockquote>

<p>I tried running the command again with <code>dnxhr_sd</code> instead, which works.  So, does this mean that <code>dnxhr_444</code> does not work with ffmpeg at the moment?</p>

<p>Here is the full output for reference:</p>

<pre><code>ffmpeg version N-83133-ge664730 Copyright (c) 2000-2017 the FFmpeg developers
  built with gcc 5.4.0 (GCC)
  configuration: --enable-gpl --enable-version3 --enable-cuda --enable-cuvid --enable-d3d11va --enable-dxva2 --enable-libmfx --enable-nvenc --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55. 43.100 / 55. 43.100
  libavcodec     57. 73.100 / 57. 73.100
  libavformat    57. 62.100 / 57. 62.100
  libavdevice    57.  2.100 / 57.  2.100
  libavfilter     6. 69.100 /  6. 69.100
  libswscale      4.  3.101 /  4.  3.101
  libswresample   2.  4.100 /  2.  4.100
  libpostproc    54.  2.100 / 54.  2.100
[avi @ 0000000000946500] non-interleaved AVI
[avi @ 0000000000946500] Stream #0: not enough frames to estimate rate; consider increasing probesize
Guessed Channel Layout for Input Stream #0.1 : stereo
Input #0, avi, from '20mintest.avi':
  Duration: 00:21:51.84, start: 0.000000, bitrate: 1535764 kb/s
    Stream #0:0: Video: r210 (r210 / 0x30313272), rgb48le(10 bpc), 3840x2160, 1582152 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 29.97 tbc
    Stream #0:1: Audio: pcm_s24le ([1][0][0][0] / 0x0001), 48000 Hz, stereo, s32 (24 bit), 2304 kb/s
[dnxhd @ 00000000026e6c60] dnxhr_444 or dnxhr_hqx profile is not implemented. Update your FFmpeg version to the newest one from Git. If the problem still occurs, it means that your file has a feature which has not been implemented.
Stream mapping:
  Stream #0:0 -&gt; #0:0 (r210 (native) -&gt; dnxhd (native))
  Stream #0:1 -&gt; #0:1 (pcm_s24le (native) -&gt; aac (native))
Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height
</code></pre>
","<p>As <a href=""https://video.stackexchange.com/a/20434/5349"">stated in this answer (includes an example)</a>, as of ffmpeg 3.3.6, this is now supported.</p>

<p>The error no longer appears and you can convert to 444 without trouble.</p>
","21256"
"Why do SRT subtitles fail to appear in my MP4 videos on iPhones?","883","","<p>I'm using <strong>videojs</strong> in an HTML page to publish a video with VTT captions. Problem is: iPhones do not play the mp4 in the videojs player - it renders the full videojs player, but when you click the Play button, it opens the mp4 in it's native player (QT player, presumably). This is normal behaviour for video on iPhones - but at that point I lose my captions, which are inserted in the videojs player through css.</p>

<p>So I ran the MP4s through Handbrake with imported .srt files. In my local VLC player, the captions appear. But when I put the same files (MP4 + SRT) in the same directory on my web server, the subtitles do not appear on the video in iPhones. In fact, the subtitles - or any subtitles option - also do not appear in the native players of desktop Firefox, Chrome or IE 11 (which uses a QT plugin for video). </p>

<p>It's not a problem of coding (used UTF-8) or server configuration - if I insert two <code>&lt;tracks&gt;</code> in the videojs <code>&lt;video&gt;</code> element - one the vtt file, one the srt file - both play, together, in the videojs player.</p>
","<p><strong>Update with answer</strong></p>

<p>It can be done. Run your source files through Handbrake, after importing the subtitle files you want. Then export as Mp4. I've read (a) you should put the SRT file/s in the same folder as the Mp4 on your server, and (b) the SRT should have the same name as the mp4. But (a) is probably wrong (in a text-editor, you can see the subtitles have been coded into the mp4 by Handbrake) and (b) is definitely wrong - two different language SRT files will obviously have different file names.  </p>

<p>Testing... On iPhone, a little speech bubble appeared on the right side of the QT player control bar when it loaded my MP4. Tapping that popped up the two subtitle language options. So, it worked. <strong>But it only worked because</strong> first my tester had to enable ""closed captions"" in his iPhone's:</p>

<pre><code>Settings &gt; General &gt; Accessibility &gt; Subtitles &amp; Captioning &gt; Closed Captions + SDH
</code></pre>

<p>In other words, an extra chore for users and the result is crappy-looking subtitles anyway. And all this because, unlike Samsungs and most other mobiles I've tested on, the iPhone overrides the videojs player and plays your videos in Apple's own Quicktime player. </p>
","13025"
"Metal particle Hi8 tape degradation","880","","<p>Recently I was playing a Hi8 tape from 1998, when I tried rewinding and rewatching a part (stopping the tape, before rewinding).  The part of the tape I had just watched was highly distorted and black and white.</p>

<p>So, I've learned the hard way that old metal particle tapes are prone to being erased, when watched.  Presumably, the 'MP' on the tapes stands for metal particle?  These tapes have been stored in a safe place.  How many years before this problem arises with MP tapes?</p>

<p>Is there a better alternative than playing these tapes into a capture card?  If not, I realize that I only have one chance to capture this video.  If this is the only option, I'll expect to capture until the read head becomes too affected by residue, and then stop and clean the head, and repeat.  Any advice would be appreciated.</p>

<hr>

<p>Edit</p>

<p>Unfortunately, I've found out the hard way there are consequences to over-using a cleaning tape.  The head is either dirtier than before, or damaged.</p>
","<p>You do need to play the tapes onto a capture card, yes. And the sooner the better. You don't just get one attempt, but you will get a slight degradation of quality every play, so you are best off aiming to do it in one pass if possible.</p>

<p>I recently captured about 50 tapes from the late 80's to the mid 90's and I did about half of them then cleaned the head then did the other half. Degradation wasn't noticeable, but I just thought I should do it just in case.</p>

<p>As to how long you can store them - is the safe place shielded from EMF, heat and humidity, then they can last for years, but due to the nature of magnetic tape, the magnetic field strength of particles affects adjacent particles, so quality will degrade no matter what.</p>

<p>When configuring your capture, set it for the highest quality you can afford - as there are utilities which can clean up some noise and speckle once the signal is digitised.</p>
","4482"
"Video in Sony Vegas flickers and is full of black frames","876","","<p>I'm trying to import some clips into a <code>Vegas 13</code> project, but they ""flicker"": some frames are all black (even in the preview), even though the files play perfectly in <code>mplayer</code> and <code>VLC</code>.</p>

<p>The clips are recorded from a videogame using <code>fraps</code>, and then reencoded in H.264 with <code>mencoder</code> with the settings: <code>-x264encopts preset=slow:keyint=200:crf=19:level_idc=31:colormatrix=bt709</code> (I don't have the originals anymore, I deleted them to gain disk space. Stupid mistake, I know).</p>

<p>Gspot tells me this about the files:
<img src=""https://i.stack.imgur.com/5rrVb.png"" alt=""enter image description here""></p>

<p>I have tried a lot of different options:</p>

<ul>
<li>disabling using the GPU for rendering</li>
<li>setting the switch to ""smart resample"", ""force resample"", and ""no resample""</li>
<li>changing the frame rate</li>
<li>using constant and variable bitrates</li>
<li>using the ""deinterlace"" settings ""blend"" and ""interpolate"" (pretty sure this does nothing in my case ^^)</li>
<li>checking/unchecking ""adjust source media""</li>
</ul>
","<p>While I wasn't able to fix the problem, I was able to go around it, by converting my input file into <code>xvid</code>.</p>

<p>I don't have any black frames anymore, but the video freezes for a few seconds at some point.</p>

<p>Since I was recording and playing the game on the same PC, I'm guessing that there's been a problem at some point and <code>fraps</code> had to drop the framerate.</p>

<p>In any case, I suspect some other bug in my video editing setup, since <code>Vegas</code> was adding black frames in other parts of the video, that play fine once converted to <code>xvid</code>.</p>
","15730"
"Premiere Pro: Possible to add several images on one video track?","873","","<p>I have this image
<a href=""https://i.stack.imgur.com/35RRv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/35RRv.jpg"" alt=""enter image description here""></a></p>

<p>that I want to put on top of my video, like this:</p>

<p><a href=""https://i.stack.imgur.com/ezgQN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ezgQN.jpg"" alt=""enter image description here""></a></p>

<p>And I want to fill the entire screen with multiple copies of this image. However, the only solution I've found so far is to create a new video track for each image. So if I want 100 images, I need 100 video tracks. Is there any other solution to this?</p>

<p>Premiere pro:
<a href=""https://i.stack.imgur.com/rBoZA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rBoZA.png"" alt=""enter image description here""></a></p>
","<p>If you want a hundred images at the same time, you need 100 layers. Often people will make a ""pre-comp"" which is a nested composition. You create a composition with only the images, then insert that composition into the main timeline. Any changes you make to the pre-comp will carry through even after you've laid it into your main video timeline. So your pre-comp will be the messy one, and your final comp will be nice and neat. </p>
","17117"
"Why does Adobe Premiere Pro modify original footage/asset files (e.g. mov)? (CS6)","872","","<p>Background:</p>

<ol>
<li>A colleague had given me a large 33Gb .mov for use in a project, I
put this file on a backup drive.</li>
<li>I made an identical copy of this 33Gb .mov file and placed it in a folder that I'd use to work on a Premiere Pro Project. </li>
<li>I ran Adobe Premiere Pro CS6 and dragged in the 33Gb .mov file into the Sequence (imported it) </li>
<li>Premiere Pro CS6 started conforming the file.</li>
<li>After it had finished, I noticed that
it's Modified Date was just now i.e different to the Modified Date
on the original copy of the file on the backup drive (see step 1)</li>
<li>I ran a BeyondCompare check between the .mov file on the backup drive
(see step 1) and the one that the Premiere Pro project was using
(step 2, 3) and Beyond Compare reported they were different.</li>
</ol>

<p>I had initially thought it was unrelated file corruption of some kind, but I have checked this several times and got the same outcome, so it's definitely Premiere Pro deliberately modifying the file.</p>

<p>So I am puzzled: these are supposed to be the same file.</p>

<p>Why would there be a need for Adobe Premiere Pro to modify the footage?
What does it do to the file?
Would it not be better to create a separate file if necessary?</p>

<p>I can accept this if that's how it works and it's good to know that it was Premiere Pro deliberately modifying the files and not corruption (so my hardware ought to be healthy). But it makes more sense to me if Premiere Pro made separate modified copies for its own use. Modifying the file as it does hinders my process for checking the integrity of backups; if the original asset footage files were the same as earlier copies then it would be easy to know that the backups are OK.</p>

<p>Thoughts?</p>
","<p>It's all about this setting, ""Write XMP ID To Files On Import"" - which confirms that Adobe Premiere Pro is deliberately modifying the .mov file: <a href=""http://helpx.adobe.com/premiere-pro/using/preferences.html#WSE3BD4A43-7022-4fe6-97F5-95313935347B"" rel=""nofollow"">http://helpx.adobe.com/premiere-pro/using/preferences.html#WSE3BD4A43-7022-4fe6-97F5-95313935347B</a></p>

<p>These posts give some background as to why having this setting enabled would be beneficial: one benefit being to be able to skip conforming files by matching the conformed file with the original using the embedded XMP tag :-</p>

<ul>
<li><p><a href=""http://helpx.adobe.com/premiere-pro/using/preferences.html#WSE3BD4A43-7022-4fe6-97F5-95313935347B"" rel=""nofollow"">http://helpx.adobe.com/premiere-pro/using/preferences.html#WSE3BD4A43-7022-4fe6-97F5-95313935347B</a></p></li>
<li><p><a href=""http://www.dvinfo.net/forum/adobe-creative-suite/498627-why-premiere-modifying-video-files.html"" rel=""nofollow"">http://www.dvinfo.net/forum/adobe-creative-suite/498627-why-premiere-modifying-video-files.html</a></p></li>
<li><p><a href=""https://forums.creativecow.net/thread/205/876064"" rel=""nofollow"">https://forums.creativecow.net/thread/205/876064</a></p></li>
</ul>

<p>(Found using Google search term ""adobe premiere pro date modified"")</p>

<p>(will accept this answer, when 2 days has elapsed)</p>
","13342"
"Getting audio from a Bell & Howell Filmosound 8 Projector","863","","<p>I recently acquired a Bell &amp; Howell Filmosound 8mm Projector, and several 8mm Sound Films.  To my delight, the motor and lamp work and the unit seems to work quite well.</p>

<p>I would love to figure out how to capture/play the sound from the sound films, but the unit is designed to connect to something else, presumably a preamp or stereo system of some sort.</p>

<p>Originally, this unit came with a special cable to connect to the atypical audio ports on the projector, but I don't have it.  Both ports are un-labled.  One appears to be an 1/8"" headphone type jack, and the other is smaller.  I have tried connecting the 1/8"" jack to a number of preamps/mixers, but have not been able to get any audio.  This leads me to believe the 1/8"" jack may be for recording sound to a film, and the smaller one is for playback.</p>

<p>Has anyone used this or a similar model and been able to get sound?  Any help appreciated:</p>

<p><img src=""https://i.stack.imgur.com/wJPCB.png"" alt=""enter image description here""></p>
","<p>The projector you have is for use with the Bell and Howell model 450 cassette recorder. This projector has no optical or magnetic means of playing or recording sound. IN this system, which is a ""Double Sound System"", the sound is recorded on the 450 player/recorder via a special cable that goes between the camera and the recorder. The film itself has no magnetic or optical sound track on it. The recorder records both the audio from a microphone and sync information from the camera, hence the special cable running between camera and recorder. After the film is processed and returned, it threaded into the projector and then the cassette player/recorder is connected by means of another special cable to the projector, via the 2 jacks pointed out in your photo. The jack questioned is a size 3/32.
This special cable is used to deliver sync information that was originally recorded on the tape to the projector for lip sync sound reproduction</p>
","12318"
"What does ALL-I mode mean?","861","","<p>I've started seeing this term mentioned in camera specs but have no idea what it means. It somehow relates to recording video.</p>

<p>Can somebody please explain where does it come from and what it means?</p>

<p>Here's all I found:
<a href=""http://www.dslrnewsshooter.com/2012/03/02/canon-launch-5d-mkiii-headphone-jack-all-i-recording-and-better-controls/"" rel=""nofollow"">http://www.dslrnewsshooter.com/2012/03/02/canon-launch-5d-mkiii-headphone-jack-all-i-recording-and-better-controls/</a></p>

<blockquote>
  <p>The compression system is the same as the 1D X and supports the higher
  bitrate <strong>ALL-I</strong> compression system alongside a more regular IPB option.
  The <strong>ALL-I</strong> mode is supposed to offer easier editing due to its less
  compressed nature, whether it also offers improved quality over
  standard IPB remains to be seen.</p>
</blockquote>

<p>Also, the upcoming Panasonic GH3 has </p>

<blockquote>
  <p>Video Bitrate 50Mbps (72Mbps ALL-I)</p>
</blockquote>

<p>Never seen this thing before...</p>
","<p>This will get migrated but the short answer is that ALL-I stores every frame in it's entirety, whereas other methods store a certain number of keyframes in entirety, with the other frames stored as the difference to the keyframe.</p>
","4730"
"CamTwist + Blackmagic Intensity Thunderbolt Shuttle Settings","860","","<p>I'm trying to set up CamTwist + a Blackmagic Intensity Thunderbolt Shuttle on a MacBook Pro but am getting a black screen in CamTwist.</p>

<p>What are the correct settings?</p>

<p>Thanks!</p>
","<p>Solved the problem...</p>

<p>Go to: Mac System Preferences > Blackmagic Design</p>

<p>Set default video standard as <code>HD 1080i 59.94 CamTwist</code> > <code>Blackmagic HD 1080i 59.94 - 8 Bit</code> </p>

<p>I figured out my camera settings using the Blackmagic Media Express app, which did detect the camera. Just take a look in the app's settings to find your camera's connection settings.</p>
","7792"
"Does repeatedly saving a video degrade its quality?","859","","<p>I take a video file, let's say an mp4, and use, let's say Windows Movie Maker, to repeatedly just save it, with no edits. Every time, the software has to convert the video into a format it can edit, and then convert it back to an mp4. Will the repeated conversions begin to degrade the video's quality if I do this over and over?</p>
","<p>MP4 isn't an ideal format for intermediate saves. If you know you'll be re-opening the file, save it as losslessly as practicable, and use MP4 only for the final output.</p>

<p>That said, depending on the encoder and settings you probably don't lose much if anything on subsequent saves. MP4 and similar codecs work by decimating the higher frequencies (details, resolution) in a predictable way. Once you've saved an MP4 at a particular quality setting you've already discarded the information. If you save again using the same settings, there's no information to lose in the areas that have already lost it.</p>
","18299"
"How to bring a SketchUp Animation into After Effects","859","","<p>I made a simple animation in SketchUp and am trying to bring it into After Effects. When I do the AVI is all black in Footage and Composition.</p>

<p>Reading around it sounds like the most common issue is Codec but beyond that I'm a bit clueless here. When I open it in Adobe Media Encoder it shows <code>H.264 Match Source - High bitrate</code> which from what little I understand the H.264 should be acceptable?</p>

<p>Is there something else I should be looking for? How can I get the animation from SketchUp into After Effects?</p>

<p><strong>EDIT WITH CLIP PROPERTIES:</strong></p>

<p>In Adobe Premier Pro the clip's properties show:</p>

<ul>
<li>Type: AVI Movie  </li>
<li>File Size: 508.9 MB </li>
<li>Image Size: 1280 x 720 </li>
<li>Frame Rate: 24.00 </li>
<li>Total Duration: 00:00:08:01 </li>
<li>Pixel Aspect Ratio: 1.0</li>
</ul>

<p>AVI File details:
Contains 1 video track(s) and 0 audio track(s).</p>

<p>Video track 1:</p>

<ul>
<li>Size is 508.88M bytes (average frame = 2.63M bytes) </li>
<li>There are 193 keyframes.</li>
<li>Frame rate is 24.000 fps </li>
<li>Frame size is 1280 x 4294966576</li>
<li>Depth is 24 bits. Compressor: none.</li>
</ul>

<p>Additional information:
Lavf54.34.100</p>
","<p>Not sure on the AVI but kept searching and found <a href=""http://sketchucation.com/forums/viewtopic.php?f=11&amp;t=18827"" rel=""nofollow"">http://sketchucation.com/forums/viewtopic.php?f=11&amp;t=18827</a></p>

<blockquote>
  <p>What OS are you using?</p>
  
  <p>Vista 64 bit has codec issues, so if it is indeed a Vista 64 bit you need to export as a series of images (which is best anyway) and compile it in after effects.</p>
</blockquote>

<p>I'm on Windows 7 so don't know why it was an issue to begin with but did the Export as Images instead. Then did a simple Import (Ctrl+I) which detected the jpeg sequence and that worked perfectly. Smaller file size then the AVI SketchUp exported too.</p>
","9770"
"Converting video with different resolutions slightly affects in file size, why?","858","","<p>I'm converting a video with resolution 960x720p using HandBrake on Mac. When I convert two videos with exactly the same settings but with different resolutions, the file size is almost the same. Actually, the video with larger resolution (960x720p) is slightly smaller than the video with smaller resolution (720x540p). File size of the larger resolution video is 18,7 MB and the file size of the smaller resolution video is 19,1 MB. Like I said, the conversion settings are exactly the same, only resolution is different. I'm using H.264 codec with variable bitrate. Please check the settings from the attached screenshot.</p>

<p>Why the file size isn't smaller on the lower resolution video?</p>

<p><img src=""https://i.stack.imgur.com/UGmXA.jpg"" alt=""settings""></p>
","<p>You are using the same bitrate for each video.  The bitrate determines how much data is used per second.  The resolution has nothing to do with how much data is used, it only impacts the number of points of data which are encoded (and thus determines part of the quality of the video output for a given bitrate).</p>

<p>What you end up with is a lower resolution file that more faithfully reproduces the original format since it has more data storage allocated per pixel where as the higher resolution file produces more pixels in the output, but as it has less data available per pixel, it is a less faithful reproduction.  </p>

<p>It is a fluid question as to whether you lose more quality from the downscale or bitrate being used, so there is no preferred value to change.  In some situations the best overall quality will be to reduce the resolution and bitrate and in other cases the best result will be to keep the resolution the same but only reduce the bitrate.</p>

<p>If you intend to make different resolutions for different connection speeds, you should decrease both the resolution and the bitrate.  This will result in a smaller file with similar relative quality, so long as you use similar compression ratios (the relationship between the size of the input and the file size).  </p>

<p>For example, as a rough guideline, if a video is 1080p and takes 225MB in total, then you would want the 720p version to take around 100MB because there is a little under half the pixels in a 720p video than there is in the 1080p video.  The amount of data used to store each pixel is thus the same, so the amount of artifacts should be pretty similar.  (Though the loss of detail will make it slightly unpredictable, so it doesn't hold exactly.  I'm oversimplifying a bit since compression is actually a pretty complicated field.)</p>
","12290"
"Lossless HDCP stripper?","856","","<p>Is there a product that allows you to strip HDCP from HDMI and that does not loose quality?</p>
","<p>Yes. The companies <a href=""http://www.spatz-tech.com/"" rel=""nofollow"">Spatz-Tech</a>, <a href=""http://www.hdfury.com/"" rel=""nofollow"">HDFury</a> and <a href=""http://www.faroudja.com/"" rel=""nofollow"">Faroudja</a> have products with that functionality in their portfolio. But there are selling restrictions in many countries. </p>
","4291"
"How would I remove a specific pattern (say, every n-n+59th frame) of frames from a lengthy video file?","853","","<p>I have large video files of screen recordings which I created with Camtasia Recorder. The content I am recording only updates once per minute (1/60 fps), however Camtasia appears to require at least 1 fps for recording (quite silly, really). So that's what I did, and then I tried looking in Camtasia Studio to cut out the extra frames but it seems the only way to reduce the file size without compressing to a lossy format is to manually cut by hand 1 second segments throughout the entire file, but these files are too long to do this by hand (3 files, one of which is 24 hrs and the shortest is 8 hrs).</p>

<h3>How would I cut all but 1 frame every 60 seconds from a video file?</h3>

<p>Note that while the file is currently in the CamRec format, I can convert it to other formats if need be.</p>

<h3>Attempts at using mpdecimate:</h3>

<hr>

<p>I have a 479mb ""test.avi"" video (you can download it <a href=""http://www.filedropper.com/test_71"" rel=""nofollow"">here</a>) which is 35 second uncompressed avi file of me moving some windows around on my desktop.</p>

<p>The first command I ran was:  </p>

<pre><code>ffmpeg -i test.avi -vf mpdecimate test-out.mp4
</code></pre>

<p>The <a href=""http://www.filedropper.com/test-out"" rel=""nofollow"">resulting file</a> (714kb) has 5-6 seconds of black, then the rest of the input video unaltered (but it is compressed).</p>

<hr>

<p>Then I tried:</p>

<pre><code>ffmpeg -i test.avi -vf mpdecimate -vsync 2 test-out.mp4
</code></pre>

<p>Which output <a href=""http://www.filedropper.com/test-out2"" rel=""nofollow"">a file</a> (716kb) with seemingly the same results.</p>

<pre><code>ffmpeg version N-70358-g047fd98 Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 4.9.2 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnu
tls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-
libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-l
ibrtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --
enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --ena
ble-libxvid --enable-lzma --enable-decklink --enable-zlib
  libavutil      54. 19.100 / 54. 19.100
  libavcodec     56. 26.100 / 56. 26.100
  libavformat    56. 23.106 / 56. 23.106
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 11.102 /  5. 11.102
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, avi, from 'test.avi':
  Duration: 00:00:34.87, start: 0.000000, bitrate: 115403 kb/s
    Stream #0:0: Video: rawvideo, bgr24, 572x280, 115425 kb/s, 30 fps, 30 tbr, 30 tbn, 30 tbc
File 'test-out2.mp4' already exists. Overwrite ? [y/N] y
No pixel format specified, yuv444p for H.264 encoding chosen.
Use -pix_fmt yuv420p for compatibility with outdated media players.
[libx264 @ 0000000003047200] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX
[libx264 @ 0000000003047200] profile High 4:4:4 Predictive, level 2.1, 4:4:4 8-bit
[libx264 @ 0000000003047200] 264 - core 144 r2525 40bb568 - H.264/MPEG-4 AVC codec - Copyleft 2003-2014 - http://www.videolan.org/x264.html - options:
 cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 dead
zone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_in
tra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahe
ad=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, mp4, to 'test-out2.mp4':
  Metadata:
    encoder         : Lavf56.23.106
    Stream #0:0: Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv444p, 572x280, q=-1--1, 30 fps, 10000k tbn, 30 tbc
    Metadata:
      encoder         : Lavc56.26.100 libx264
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; h264 (libx264))
Press [q] to stop, [?] for help
[swscaler @ 0000000003017fe0] Warning: data is not aligned! This can lead to a speedloss
frame=  406 fps= 50 q=-1.0 Lsize=     715kB time=00:00:34.06 bitrate= 171.9kbits/s
video:711kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.578083%
[libx264 @ 0000000003047200] frame I:4     Avg QP:20.40  size: 18550
[libx264 @ 0000000003047200] frame P:342   Avg QP:21.60  size:  1878
[libx264 @ 0000000003047200] frame B:60    Avg QP:19.97  size:   176
[libx264 @ 0000000003047200] consecutive B-frames: 75.9% 12.8%  1.5%  9.9%
[libx264 @ 0000000003047200] mb I  I16..4: 40.3% 25.6% 34.1%
[libx264 @ 0000000003047200] mb P  I16..4: 10.2%  4.6%  4.8%  P16..4:  7.6%  2.0%  0.6%  0.0%  0.0%    skip:70.1%
[libx264 @ 0000000003047200] mb B  I16..4:  0.6%  0.1%  0.3%  B16..8:  7.8%  0.4%  0.0%  direct: 0.1%  skip:90.7%  L0:42.8% L1:56.3% BI: 1.0%
[libx264 @ 0000000003047200] 8x8 transform intra:23.3% inter:53.3%
[libx264 @ 0000000003047200] coded y,u,v intra: 17.3% 7.5% 7.2% inter: 1.9% 0.4% 0.4%
[libx264 @ 0000000003047200] i16 v,h,dc,p: 20% 76%  3%  1%
[libx264 @ 0000000003047200] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 18% 57%  0%  0%  0%  1%  0%  1%
[libx264 @ 0000000003047200] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 45% 16%  1%  2%  2%  2%  2%  3%
[libx264 @ 0000000003047200] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 0000000003047200] ref P L0: 62.9% 10.7% 14.1% 12.3%
[libx264 @ 0000000003047200] ref B L0: 85.2% 14.2%  0.6%
[libx264 @ 0000000003047200] ref B L1: 99.1%  0.9%
[libx264 @ 0000000003047200] kb/s:163.69
</code></pre>

<hr>

<p>Then I tried:</p>

<pre><code>ffmpeg -i test.avi -vf mpdecimate=hi=64*12:lo=64*2:frac=0.1,showinfo -vsync 2 test-out2.mp4
</code></pre>

<p>with the same results (in terms of how the video file appeared).</p>

<pre><code>frame=  411 fps=221 q=-1.0 Lsize=     716kB time=00:00:34.06 bitrate= 172.3kbits/s
video:712kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.589478%
[libx264 @ 0000000004e66b40] frame I:4     Avg QP:20.26  size: 18595
[libx264 @ 0000000004e66b40] frame P:343   Avg QP:21.49  size:  1876
[libx264 @ 0000000004e66b40] frame B:64    Avg QP:19.52  size:   170
[libx264 @ 0000000004e66b40] consecutive B-frames: 74.7% 13.1%  1.5% 10.7%
[libx264 @ 0000000004e66b40] mb I  I16..4: 41.6% 24.6% 33.8%
[libx264 @ 0000000004e66b40] mb P  I16..4: 10.2%  4.5%  4.8%  P16..4:  7.6%  2.0%  0.6%  0.0%  0.0%    skip:70.3%
[libx264 @ 0000000004e66b40] mb B  I16..4:  0.6%  0.1%  0.3%  B16..8:  7.5%  0.4%  0.0%  direct: 0.0%  skip:91.0%  L0:43.4% L1:55.6% BI: 0.9%
[libx264 @ 0000000004e66b40] 8x8 transform intra:23.1% inter:53.7%
[libx264 @ 0000000004e66b40] coded y,u,v intra: 17.4% 7.5% 7.2% inter: 1.9% 0.4% 0.4%
[libx264 @ 0000000004e66b40] i16 v,h,dc,p: 21% 75%  3%  1%
[libx264 @ 0000000004e66b40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 17% 58%  0%  0%  0%  1%  0%  1%
[libx264 @ 0000000004e66b40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 45% 16%  1%  2%  2%  2%  2%  3%
[libx264 @ 0000000004e66b40] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 0000000004e66b40] ref P L0: 62.9% 10.6% 14.2% 12.3%
[libx264 @ 0000000004e66b40] ref B L0: 86.0% 13.3%  0.6%
[libx264 @ 0000000004e66b40] ref B L1: 99.1%  0.9%
[libx264 @ 0000000004e66b40] kb/s:164.04
</code></pre>
","<p>FFmpeg has a <a href=""http://www.ffmpeg.org/ffmpeg-all.html#framestep"" rel=""nofollow"">framestep</a> filter to take every <code>n</code>th frame.</p>

<p>There's also <code>mpdecimate</code>, which outputs a frame whenever the input frame is different enough from the previous frame.  (so if exactly every 60th frame isn't perfect, you could use <code>mpdecimate</code> instead to detect the changes, and make a potentially VFR video.)</p>

<p>I didn't test this command; post a comment if I screwed it up:</p>

<p><code>ffmpeg -i in.camrec -vf framestep=60 -r 1/60 -codec copy -map 0 -c:v libx264 -preset medium -crf 16 -movflags +faststart -vsync 2 out.mp4</code></p>

<p>Or use <code>out.mkv</code> if you like matroska better than the mp4 container.</p>

<p><code>copy</code> (stream copy) won't work while dropping frames, I think even for an intra-only codec.  So your output has to be encoded to some codec.  My example uses lossy h.264, but with the quality cranked so high it should be transparent.  You could use lossless h.264 (<code>-preset ultrafast -qp 0</code>), <code>-c:v utvideo</code>, or whatever else you like.</p>

<p><code>-vsync 2</code> turns on VFR output mode, which for some reason isn't the default in mp4.  (but it is in mkv).  You'll want that if you use <code>mpdecimate</code>, but you won't need it for <code>framestep</code>.</p>

<p>A quick google didn't tell me if camrec is already lossy or not.  If it's lossy, then there's no way to get a file as small but with the same quality as just keeping every 60th CamRec frame without decoding them.  (assuming there's an intra-only frame at the point you want one, if the format has any non-intra frames.)</p>

<p>With mjpeg, you can turn a video into a directory of images without decoding, and select whichever ones you want, and mux them up again.  Since camrec frames probably aren't a stand-alone image format, it's unlikely there's something similar, so you just have to use video tools.  Like I said, don't think ffmpeg can do clever things like frame dropping without decompressing, even for intra-only codecs.  Other tools might exist that can do that, for some codecs.</p>

<p>If it's lossless, then it just takes more CPU time to decode / re-encode to a new lossless format.</p>

<p>edit: more about mpdecimate:
As noted in the comments, I added some stats logging to mpdecimate.<br>
<code>git pull https://github.com/pcordes/FFmpeg.git mpdecimate</code><br>
and then build ffmpeg as usual.</p>

<p>Then run <code>ffmpeg -v debug ...</code>.<br>
Or <code>ffplav -v debug -vf mpdecimate  input.avi</code></p>

<p>Testing mpdecimate threshold settings with ffplay is a lot faster than making an encode.</p>
","15002"
"What is the camera technique use in desiigner - Panda vido?","852","","<p>What is the name of the technique and how do you rig a camera so that it follows the actor's face from the front, like in the two videos below. I would describe it as a ""front tracking follow"" shot.</p>

<ul>
<li><p><a href=""https://youtu.be/E5ONTXHS2mM?t=23s"" rel=""nofollow"">desiigner - Panda</a> (from 00:23)</p></li>
<li><p><a href=""https://www.youtube.com/watch?v=TrOwSEpYLKg"" rel=""nofollow"">JME - 96 Fuckries</a> (from beginning)</p></li>
</ul>

<p>Cheers</p>
","<p>It made with <a href=""https://www.google.lv/search?q=body%20camera%20rig&amp;oq=body%20camera%20rig&amp;aqs=chrome..69i57.343j0j1&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">body camera rig</a>. 
<a href=""https://i.stack.imgur.com/r44VL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r44VL.jpg"" alt=""body camera rig""></a>
Here is <a href=""https://youtu.be/hOdu1Zl1lic"" rel=""nofollow noreferrer"">tutorial</a>, hot to do it yourself, but you will need some adjustments to face it from front.</p>

<p>Or if you don't need hands in the shot, you can use your tripod, how shown in <a href=""https://youtu.be/lo4_sajFUzc"" rel=""nofollow noreferrer"">this tutorial</a> (careful, bad audio)</p>
","18753"
"How to resolve Shearing issue while rotating image in Adobe After Effects?","851","","<p>I had an image in my After Effect Composition. I want to rotate the image by time with out any disorder in its shape.</p>

<p>When I am trying to rotate image a shearing effect is happening. How to resolve this issue, so that there will not be any shearing to image while rotating.</p>

<p>Below I have attached what happening now when I am trying to rotate. </p>

<ol>
<li>Default Image : 0 degree rotation.
<img src=""https://i.stack.imgur.com/dCqdM.png"" alt=""enter image description here""></li>
<li>Transformed image : 42 degree rotation.
<img src=""https://i.stack.imgur.com/40Svu.png"" alt=""enter image description here""></li>
</ol>
","<p>Make sure <code>scale</code> values are locked and set both to <strong>100%</strong>:</p>

<p><img src=""https://i.stack.imgur.com/ZyMH4.png"" alt=""enter image description here""></p>
","14734"
"Sync up ffmpeg `rawvideo` recording and Audacity ALSA recording","851","","<p>I am attempting to transfer some Video 8 tapes to my computer with ffmpeg and Audacity.</p>

<p>ffmpeg captures a <code>rawvideo</code> stream from an EasyCap dongle, and Audacity records from the line-in audio jack.</p>

<p>With ffmpeg's frame rate parameter set to 29.97, the output video ends up being shorter than the audio recording.  At 25 frames per second, the video is significantly longer than the audio.  Audacity's settings are simply default.</p>

<p>Shouldn't the parameters match up with NTSC's; 29.97 fps?  Do you think ffmpeg or Audacity are on the wrong time scale, or both?</p>

<p>The <code>rawvideo</code> stream is sourced from the <code>somagic-capture</code> utility, shown in the script below.</p>

<p>Relevant line:</p>

<pre><code>ffmpeg -pixel_format uyvy422 -s:v 720x480 -framerate 29.97 -f rawvideo \ 
-i $PIPE  -vf scale=w=720:h=540 -vcodec libx264 -preset ultrafast \
-c:a libfdk_aac -b:a 128k $OUTFILE &gt; $FFMPEG_LOG 2&gt;&amp;1 &amp; 
</code></pre>

<p>I've also tried to record the audio stream with ffmpeg.  This solves the sync problem, but is unreliable.  The script for that attempt and the details are here: <a href=""https://stackoverflow.com/questions/28359855/forcing-ffmpeg-to-capture-unreliable-alsa-audio-stream"">https://stackoverflow.com/questions/28359855/forcing-ffmpeg-to-capture-unreliable-alsa-audio-stream</a></p>

<p>Script for recording video, without audio, in ffmpeg:</p>

<pre><code>#!/bin/sh

PIPE=/tmp/somagic-pipe
OUTFILEDIR=~/easycap/Videos/
LOGDIR=~/.somagic-log/
NOW=`date +""%m_%d_%Y_%H_%M_%S""`

OUTFILE=${OUTFILEDIR}fpv_video_${NOW}.mp4

mkdir $LOGDIR

FFMPEG_LOG=${LOGDIR}ffmpeg.log
SOMAGIC_LOG=${LOGDIR}somagic.log
MPLAYER_LOG=${LOGDIR}mplayer.log

rm $PIPE &gt;/dev/null 2&gt;&amp;1
rm $OUTFILE &gt;/dev/null 2&gt;&amp;1

rm $FFMPEG_LOG
rm $SOMAGIC_LOG
rm $MPLAYER_LOG

mkfifo $PIPE &gt;/dev/null 2&gt;&amp;1

ffmpeg -pixel_format uyvy422 -s:v 720x480 -framerate 29.97 -f rawvideo \ 
-i $PIPE  -vf scale=w=720:h=540 -vcodec libx264 -preset ultrafast \
-c:a libfdk_aac -b:a 128k $OUTFILE &gt; $FFMPEG_LOG 2&gt;&amp;1 &amp; 

somagic-capture --ntsc -c --luminance=2 --lum-aperture=3 2&gt; $SOMAGIC_LOG | \
tee $PIPE | \
mplayer -vf yadif,screenshot -demuxer rawvideo -rawvideo    \
""ntsc:format=uyvy:fps=30000/1001"" -aspect 4:3 - 2&gt; $MPLAYER_LOG

rm $PIPE &gt;/dev/null 2&gt;&amp;1 
</code></pre>

<p>Modified from this script:  <a href=""https://gist.github.com/Brick85/0b327ac2d3d45e23ed33"" rel=""nofollow noreferrer"">https://gist.github.com/Brick85/0b327ac2d3d45e23ed33</a></p>

<hr>

<p>As suggested by stib, here's the output of <code>ffprobe</code>.</p>

<p>On the audio file:</p>

<pre><code>peterbecich@Sirius:~/easycap$ ffprobe snapping.m4a 
ffprobe version 2.5.3 Copyright (c) 2007-2015 the FFmpeg developers
  built on Jan 11 2015 17:53:45 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)
  configuration: --extra-libs=-ldl --prefix=/opt/ffmpeg --enable-avresample --disable-debug --enable-nonfree --enable-gpl --enable-version3 --enable-libpulse --enable-libopencore-amrnb --enable-libopencore-amrwb --disable-decoder=amrnb --disable-decoder=amrwb --enable-libx264 --enable-libx265 --enable-libfdk-aac --enable-libvorbis --enable-libmp3lame --enable-libopus --enable-libvpx --enable-libspeex --enable-libass --enable-avisynth --enable-libsoxr --enable-libxvid --enable-libvo-aacenc --enable-libvidstab
  libavutil      54. 15.100 / 54. 15.100
  libavcodec     56. 13.100 / 56. 13.100
  libavformat    56. 15.102 / 56. 15.102
  libavdevice    56.  3.100 / 56.  3.100
  libavfilter     5.  2.103 /  5.  2.103
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'snapping.m4a':
  Metadata:
    major_brand     : M4A 
    minor_version   : 512
    compatible_brands: isomiso2
    encoder         : Lavf54.20.4
  Duration: 00:19:06.78, start: 0.000000, bitrate: 189 kb/s
    Stream #0:0(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 186 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>

<p>On the video file:</p>

<pre><code>peterbecich@Sirius:~/easycap$ ffprobe Videos/fpv_video_02_06_2015_16_34_09.mp4 
ffprobe version 2.5.3 Copyright (c) 2007-2015 the FFmpeg developers
  built on Jan 11 2015 17:53:45 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)
  configuration: --extra-libs=-ldl --prefix=/opt/ffmpeg --enable-avresample --disable-debug --enable-nonfree --enable-gpl --enable-version3 --enable-libpulse --enable-libopencore-amrnb --enable-libopencore-amrwb --disable-decoder=amrnb --disable-decoder=amrwb --enable-libx264 --enable-libx265 --enable-libfdk-aac --enable-libvorbis --enable-libmp3lame --enable-libopus --enable-libvpx --enable-libspeex --enable-libass --enable-avisynth --enable-libsoxr --enable-libxvid --enable-libvo-aacenc --enable-libvidstab
  libavutil      54. 15.100 / 54. 15.100
  libavcodec     56. 13.100 / 56. 13.100
  libavformat    56. 15.102 / 56. 15.102
  libavdevice    56.  3.100 / 56.  3.100
  libavfilter     5.  2.103 /  5.  2.103
  libavresample   2.  1.  0 /  2.  1.  0
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'Videos/fpv_video_02_06_2015_16_34_09.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf56.15.102
  Duration: 00:19:08.08, start: 0.000000, bitrate: 5352 kb/s
    Stream #0:0(und): Video: h264 (High 4:2:2) (avc1 / 0x31637661), yuv422p, 720x540, 5351 kb/s, 29.97 fps, 29.97 tbr, 11988 tbn, 59.94 tbc (default)
    Metadata:
      handler_name    : VideoHandler
</code></pre>
","<p>I can't say what's going wrong or if there's a change to the ffmpeg scripts that would help, but from a practical POV I can offer some advice on how to deal with what you have.</p>

<p>If the difference is due to drift, the fix is relatively simple. If it's due to dropped video frames or audio dropouts, it's probably not going to be fixable without a good bit of work.</p>

<p>The 'drift' scenario is this: you know they're out of sync at the end because you can see / hear the difference. Something near the end doesn't match up. So find that thing and identify its exact time relative to the start, in both the picture and sound. Reduce these times to absolute frames -- in this case numbers will be around 34000 or so at 30fps. Find the <strong><em>ratio</em></strong> of these two values and apply it in Audacity as a speed change percentage (with constant pitch, usually). Check that the audio event now occurs at the same time as the matching video event you noted. If you got it wrong, invert the ratio.</p>

<p>If after doing this you find places in the middle that are still out of sync, you're forced to apply this same technique in multiple segments, or to cut / move / blend the audio in sections. I don't envy you this task.</p>
","14817"
"I'm looking to make a Nuke expression: I want the maximum value of an animation curve of another node","849","","<p>I want to feed a custom made NoOp node with a floating point slider with the maximum value of an animation curve from another node. What would be the right expression to do this?</p>
","<p>There's not a built in method for that, but this Python expression will return the highest keyframe in an animation curve:</p>

<pre><code>max((key.y for key in nuke.toNode('Blur1')['size'].animation(0).keys()))
</code></pre>

<p>where <code>Blur1</code> is the node and <code>size</code> is the knob.</p>

<p>Paste this into your expression editor, being sure to select the <strong>Py</strong> toggle.</p>

<hr>

<p>This won't interpolate or extrapolate keys. It only iterates over every keyframe.</p>

<p>No guarantees on the speed of this either. It might bog down the node if you have a keyframe on every frame or something.</p>
","13494"
"Audio noise reduction: how does audacity compare to other options?","848","","<p>Audacity's noise reduction filter is pretty good.  Before I use it, though, I want to know if I'm missing out on any fundamentally different approaches that may work better.</p>

<p>I have some recordings of my brother singing and playing piano.  It's pretty good as-is, but there's noticeable white noise.  There's also a 60Hz signal that's audible (but not really objectionable) during quieter portions of the recording.  Audacity does a pretty good job of getting rid of them.  So I'll almost certainly just use that, esp. since very recent audacity changes apparently have improved the noise-reduction filter.  (or maybe just the config UI?  I didn't look at the diffs).</p>

<p>The comments at the <a href=""https://code.google.com/p/audacity/source/browse/audacity-src/trunk/src/effects/NoiseReduction.cpp"" rel=""nofollow"">start of the file</a> describes the design pretty well, and what the settings sliders mean (esp. the frequency smoothing was a mystery until I thought to check the source, and found it had a great comment describing the design.)</p>

<p>It gets a profile of the kind of noise you want to remove from a clip of only noise.  For the main pass, it FFTs the input, and silences every frequency bin that is below a threshold.  So it's a bandpass filter that adapts to only let through frequencies with signals above the noise level for that bin.  (Thresholds established from the noise profile.)</p>

<p>So my question is, are there other designs for noise filters that work any better?  Mostly out of curiosity, since audacity works very well for my application.  (noise energy distributed over a wide frequency range, signal I want to keep made up of fairly narrow frequencies.)</p>
","<p>After a bit of searching, I found <a href=""http://www.sonicscoop.com/2013/05/30/the-best-noise-reduction-plugins-on-the-market/"" rel=""nofollow noreferrer"">an article</a> surveying some of the commercial software noise reduction options.  For hiss/hum type noise removal (as opposed to crackles and pops), no approach other than noise-gates in multiple frequency bands was mentioned.  The differences between implementations are in the default settings, the control knobs available, and things like that.  (and whether the a noise sample is used to get a profile.  Apparently there are faster implementations that do well without needing a noise sample to build a tailored profile of the noise to remove.)</p>

<p>So AFAICT, Audacity's noise reduction plugin works the same way as commercial software implementations.</p>

<p>(For pops and crackles, Audacity allows making rectangular selections on the spectrum plot.  It might not have as sophisticated algorithms for filling in deleted glitches with similar-sounding audio from nearby, compared to some commercial software.  IDK, I haven't tried that.)</p>

<p>The other thing I was interested in was what's up with recent development activity in Audacity's Noise Reduction filter:</p>

<p>Following the svn history of <code>NoiseReduction.cpp</code>, there are really only changes to the GUI for setting parameters, from it's appearance in SVN r13591 (Nov 10, 2014) up to latest change in r13895 (Jan 24, 2015).</p>

<p>It replaces the over-optimistically-titled <code>NoiseRemoval.cpp</code>.  It appears to be a rewrite of the same basic design.  The intro comment describing the design is only slightly changed, and some of the same comments are kicking around in the code.  However, almost every line of code is different, and it's 1.9k lines of code instead of 870.</p>

<p>So it's probably not critical to get a pre-release build of Audacity 2.1 for noise removal.  If you get chimes / dropouts where you don't want them, or otherwise have problems, then go for a new version.  It looks at more data to decide if something is noise or not, as detailed below.</p>

<p>One critical part of the new filter now has several methods to choose from.  (<code>OLD_METHOD_AVAILABLE</code> is not enabled by default.  And <code>DM_DEFAULT_METHOD = DM_SECOND_GREATEST</code>)</p>

<p>(The code is well-commented.  I mostly just read the comments, on the assumption that the code matches well enough.)</p>

<pre class=""lang-cpp prettyprint-override""><code>//***** NoiseReduction.cpp:1079
// Return true iff the given band of the ""center"" window looks like noise.
// Examine the band in a few neighboring windows to decide.
inline
bool EffectNoiseReduction::Worker::Classify(const Statistics &amp;statistics, int band)
{
   switch (mMethod) {
#ifdef OLD_METHOD_AVAILABLE
   case DM_OLD_METHOD:
      {
         float min = mQueue[0]-&gt;mSpectrums[band];
         for (int ii = 1; ii &lt; mNWindowsToExamine; ++ii)
            min = std::min(min, mQueue[ii]-&gt;mSpectrums[band]);
         return min &lt;= mOldSensitivityFactor * statistics.mNoiseThreshold[band];
      }
   // New methods suppose an exponential distribution of power values
   // in the noise; new sensitivity is meant to be log of probability
   // that noise strays above the threshold.  Call that probability
   // 1 - F.  The quantile function of an exponential distribution is
   // log (1 - F) * mean.  Thus simply multiply mean by sensitivity
   // to get the threshold.
   case DM_MEDIAN:
      // This method examines the window and all windows
      // that partly overlap it, and takes a median, to
      // avoid being fooled by up and down excursions into
      // either the mistake of classifying noise as not noise
      // (leaving a musical noise chime), or the opposite
      // (distorting the signal with a drop out). 
      if (mNWindowsToExamine == 3)
         // No different from second greatest.
         goto secondGreatest;
      else if (mNWindowsToExamine == 5)
      {
         float greatest = 0.0, second = 0.0, third = 0.0;
         for (int ii = 0; ii &lt; mNWindowsToExamine; ++ii) {
            const float power = mQueue[ii]-&gt;mSpectrums[band];
            if (power &gt;= greatest)
               third = second, second = greatest, greatest = power;
            else if (power &gt;= second)
               third = second, second = power;
            else if (power &gt;= third)
               third = power;
         }
         return third &lt;= mNewSensitivity * statistics.mMeans[band];
      }
      else {
         wxASSERT(false);
         return true;
      }
   secondGreatest:
   case DM_SECOND_GREATEST:
      {
         // This method just throws out the high outlier.  It
         // should be less prone to distortions and more prone to
         // chimes.
         float greatest = 0.0, second = 0.0;
         for (int ii = 0; ii &lt; mNWindowsToExamine; ++ii) {
            const float power = mQueue[ii]-&gt;mSpectrums[band];
            if (power &gt;= greatest)
               second = greatest, greatest = power;
            else if (power &gt;= second)
               second = power;
         }
         return second &lt;= mNewSensitivity * statistics.mMeans[band];
      }
   default:
      wxASSERT(false);
      return true;
   }
}
</code></pre>

<p>(why isn't <a href=""https://meta.stackexchange.com/questions/184108/what-is-syntax-highlighting-and-how-does-it-work"">syntax highlighting</a> working?  I put <code>&lt;!-- language: lang-cpp --&gt;</code> before the code block :/)</p>

<p>I'm probably going to build audacity from this SVN checkout I already have, but I'm not planning to compare noise reduction between the latest dev build and the stable 2.0.6 release.</p>
","14998"
"How to link URL in flash media player to red5 server","848","","<p>I am trying to test a flash player for red5 live streaming video and in flash CS6 I have put a player component on the stage and not sure what url to input the live stream source?</p>

<p>I can test and see it working in mysite.com:5080/demos/publisher.html but cannot work out how to get that streaming video linked to my flash player.</p>

<p>update: it is not as simple as entering RTMP url as has failed on every attempt with all possible parameters and in any case the question now is what stops anyone from going to any given url with 5080 port and taking advantage...something to hash out in future with the help of mister AJ. </p>
","<p>I don't know this particular product, but based on what you are describing, I think this may help.  The player in flash probably wants the actual streaming media link.  When you go to the html page, you are accessing an HTML page that probably has an embedded player.  Flash streaming actually uses RTMP rather than HTTP, so you would need to find out what the RTMP URL is and provide that to the Flash video player.</p>

<p>Update: So to do a live stream, you need to use the rtmp link to the publishing point on your server.  You then need to use the publisher to send a feed to that publishing point.  Whatever name you provide in the publisher will be the name of the file that you need to feed to your viewer.  Everything gets relayed through the server.</p>
","9788"
"Corrupt video when playing through JW Player","846","","<p>I have some files that when played in jwplayer the playback is corrupt.
The files are encoded to h.264 using FFMpeg and there is other files encoded in the same way that works.</p>

<p>At the moment we only use the flash version of the player.</p>

<p>The corrupt playback looks like this:</p>

<p><a href=""http://adam.ingmansson.com/public/jwplayer-corrupt-video.png"" rel=""nofollow"">http://adam.ingmansson.com/public/jwplayer-corrupt-video.png</a></p>

<p>This problem started showing up after we did an upgrade to FFMpeg, so I haven't ruled out that it could be an encoding error.</p>

<p>the command used to run FFMpeg is:</p>

<pre><code>ffmpeg 
    -i /home/ftp/1c8f08b7d0d9e7fa4b24066156ad50bc981497a0.mov 
    -vcodec libx264 
    -preset ultrafast 
    -profile baseline 
    -acodec libfaac 
    -ab 96k 
    -crf 19 
    -vf movie=""/home/adam/logo.png [watermark]; [in][watermark] overlay=main_w-overlay_w-10:main_h-overlay_h-10 [out]"" 
    -y /home/ftp/1c8f08b7d0d9e7fa4b24066156ad50bc981497a0.flv
</code></pre>

<p>I am in no way an expert in FFMpeg commandline, so feel free to point out any mistakes made.</p>

<p>FFMpeg info:</p>

<pre><code>ffmpeg version git-2012-05-02-2330eb1 Copyright (c) 2000-2012 the FFmpeg developers
  built on May  3 2012 08:51:25 with gcc 4.4.3
  configuration: --enable-gpl --enable-libfaac --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libtheora --enable-libvorbis --enable-libvpx --enable-libx264 --enable-nonfree --enable-version3 --enable-x11grab
  libavutil      51. 49.100 / 51. 49.100
  libavcodec     54. 17.101 / 54. 17.101
  libavformat    54.  3.100 / 54.  3.100
  libavdevice    53.  4.100 / 53.  4.100
  libavfilter     2. 72.103 /  2. 72.103
  libswscale      2.  1.100 /  2.  1.100
  libswresample   0. 11.100 /  0. 11.100
  libpostproc    52.  0.100 / 52.  0.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/home/ftp/javarecorder/aeb1038bd3e380782e2293cfdeb56ba8fab0d167.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 537199360
    compatible_brands: qt
    creation_time   : 2012-05-15 16:33:35
  Duration: 00:25:01.00, start: 0.000000, bitrate: 4299 kb/s
    Stream #0:0(eng): Video: qtrle (rle  / 0x20656C72), rgb24, 1920x1080, 3593 kb/s, 9.24 fps, 1k tbr, 1k tbn, 1k tbc
    Metadata:
      creation_time   : 2012-05-15 16:33:35
      handler_name    : Apple Alias Data Handler
    Stream #0:1(eng): Audio: pcm_s16be (twos / 0x736F7774), 44100 Hz, 1 channels, s16, 705 kb/s
    Metadata:
      creation_time   : 2012-05-15 16:33:35
      handler_name    : Apple Alias Data Handler
</code></pre>
","<p>Seems like the framerate was the problem. JW Player seems to have problems playing videos with high framerates (like 1k).</p>

<p>A simple <code>-r 30</code> in the FFMpeg command seems to have solved it</p>
","4036"
"Photoshop Layer Blending Goes Away When Imported into After Effects","846","","<p>When I import my Photoshop file into After Effects, the blending options I used on the layers (like drop shadow, stroke, etc) disappear, but they do show up when I view the composition file. I know I could apply each blending option permanently in Photoshop, but is there another way to just make it show up in After Effects? Thanks!</p>
","<p>If you want to be able to edit your layer styles from Photoshop inside After Effects, you need to do the following:</p>

<ol>
<li>Save your photoshop project as a .PSD file. </li>
<li>Import it into After Effects with the following options- </li>
<li>Choose ""Import Kind: Composition (retain layer sizes if you wish) </li>
<li>Set Layer Options - Editable Layer Styles</li>
</ol>

<p><img src=""https://i.stack.imgur.com/NTXXw.png"" alt=""enter image description here""></p>

<p>Double click on the comp that is created.  You'll see a series of layers in your timeline, which should correspond to your layers in Photoshop.</p>

<p>If you pop open the triangles next to the relevant layer, you should see the layer styles, as shown below:</p>

<p><img src=""https://i.stack.imgur.com/I7dKM.png"" alt=""enter image description here""></p>
","14795"
"ffmpeg and decklink- how to specify audio_input for hdmi video?","840","","<p>I'm using an Intensity Pro 4K to capture with <code>ffmpeg</code> via the <code>decklink</code> commands.</p>

<p>According to <a href=""https://www.ffmpeg.org/ffmpeg-devices.html#decklink"" rel=""nofollow noreferrer"">the documentation</a>:</p>

<blockquote>
  <p><strong>video_input</strong></p>
  
  <p>Sets the video input source. Must be unset, sdi, hdmi, optical_sdi, component, composite or s_video. Defaults to
  unset.</p>
  
  <p><strong>audio_input</strong></p>
  
  <p>Sets the audio input source. Must be unset, embedded, aes_ebu, analog, analog_xlr, analog_rca or microphone.
  Defaults to unset.</p>
</blockquote>

<p>So I set <code>-video_input hdmi</code>, which guarantees I will get the video via HDMI even if the Blackmagic driver is set to something else (e.g., component, which for some reason every time I reboot it gets reset to component).</p>

<p>But, while I get video, I don't get audio.  I tried setting <code>-audio_input</code> to <code>embedded</code> (no effect) <code>aes_ebu</code>, and <code>microphone</code> (not supported by device error).</p>

<p>So, I still have to go to the Blackmagic video drivers and switch the input signal to HDMI.</p>

<p>Is there a way to specify the hdmi audio feed together with the hdmi video feed on the ffmpeg command line?</p>

<p><strong>Reference</strong></p>

<p>Full command I am using:</p>

<pre><code>ffmpeg -rtbufsize 1500M -f decklink -video_input hdmi -i ""Intensity Pro 4K@20"" -pix_fmt yuv420p -c:a copy -c:v v210 abc.mov
</code></pre>

<p><code>list_formats</code> output:</p>

<pre><code>E:\&gt;ffmpeg -f decklink -list_formats 1 -i ""Intensity Pro 4K""
ffmpeg version 3.2 Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 5.4.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-dxva2 --enable-libmfx --enable-nvenc --enable-avisynth --enable-bzlib --enable-libebur128 --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55. 34.100 / 55. 34.100
  libavcodec     57. 64.100 / 57. 64.100
  libavformat    57. 56.100 / 57. 56.100
  libavdevice    57.  1.100 / 57.  1.100
  libavfilter     6. 65.100 /  6. 65.100
  libswscale      4.  2.100 /  4.  2.100
  libswresample   2.  3.100 /  2.  3.100
  libpostproc    54.  1.100 / 54.  1.100
[decklink @ 0000000000f66460] Supported formats for 'Intensity Pro 4K':
[decklink @ 0000000000f66460]   1       720x486 at 30000/1001 fps (interlaced, lower field first)
[decklink @ 0000000000f66460]   2       720x576 at 25000/1000 fps (interlaced, upper field first)
[decklink @ 0000000000f66460]   3       1920x1080 at 24000/1001 fps
[decklink @ 0000000000f66460]   4       1920x1080 at 24000/1000 fps
[decklink @ 0000000000f66460]   5       1920x1080 at 25000/1000 fps
[decklink @ 0000000000f66460]   6       1920x1080 at 30000/1001 fps
[decklink @ 0000000000f66460]   7       1920x1080 at 30000/1000 fps
[decklink @ 0000000000f66460]   8       1920x1080 at 25000/1000 fps (interlaced, upper field first)
[decklink @ 0000000000f66460]   9       1920x1080 at 30000/1001 fps (interlaced, upper field first)
[decklink @ 0000000000f66460]   10      1920x1080 at 30000/1000 fps (interlaced, upper field first)
[decklink @ 0000000000f66460]   11      1920x1080 at 50000/1000 fps
[decklink @ 0000000000f66460]   12      1920x1080 at 60000/1001 fps
[decklink @ 0000000000f66460]   13      1920x1080 at 60000/1000 fps
[decklink @ 0000000000f66460]   14      1280x720 at 50000/1000 fps
[decklink @ 0000000000f66460]   15      1280x720 at 60000/1001 fps
[decklink @ 0000000000f66460]   16      1280x720 at 60000/1000 fps
[decklink @ 0000000000f66460]   17      3840x2160 at 24000/1001 fps
[decklink @ 0000000000f66460]   18      3840x2160 at 24000/1000 fps
[decklink @ 0000000000f66460]   19      3840x2160 at 25000/1000 fps
[decklink @ 0000000000f66460]   20      3840x2160 at 30000/1001 fps
[decklink @ 0000000000f66460]   21      3840x2160 at 30000/1000 fps
Intensity Pro 4K: Immediate exit requested
</code></pre>

<p>Output of <code>E:\&gt;ffmpeg -rtbufsize 1500M -f decklink -video_input hdmi -audio_input embedded -i ""Intensity Pro 4K@20"" -pix_fmt yuv420p -c:a copy -c:v v210 abcde.mov</code></p>

<pre><code>ffmpeg version 3.2 Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 5.4.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-dxva2 --enable-libmfx --enable-nvenc --enable-avisynth --enable-bzlib --enable-libebur128 --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
  libavutil      55. 34.100 / 55. 34.100
  libavcodec     57. 64.100 / 57. 64.100
  libavformat    57. 56.100 / 57. 56.100
  libavdevice    57.  1.100 / 57.  1.100
  libavfilter     6. 65.100 /  6. 65.100
  libswscale      4.  2.100 /  4.  2.100
  libswresample   2.  3.100 /  2.  3.100
  libpostproc    54.  1.100 / 54.  1.100
[decklink @ 0000000002872cc0] Found Decklink mode 3840 x 2160 with rate 29.97
[decklink @ 0000000002872cc0] Frame received (#1) - Input returned - Frames dropped 1
Guessed Channel Layout for Input Stream #0.0 : stereo
Input #0, decklink, from 'Intensity Pro 4K@20':
  Duration: N/A, start: 0.000000, bitrate: 3978870 kb/s
    Stream #0:0: Audio: pcm_s16le, 48000 Hz, stereo, s16, 1536 kb/s
    Stream #0:1: Video: rawvideo (UYVY / 0x59565955), uyvy422, 3840x2160, 3977334 kb/s, 29.97 tbr, 1000k tbn, 1000k tbc
Incompatible pixel format 'yuv420p' for codec 'v210', auto-selecting format 'yuv422p'
Output #0, mov, to 'abcde.mov':
  Metadata:
    encoder         : Lavf57.56.100
    Stream #0:0: Video: v210 (v210 / 0x30313276), yuv422p, 3840x2160, q=2-31, 200 kb/s, 29.97 fps, 30k tbn, 29.97 tbc
    Metadata:
      encoder         : Lavc57.64.100 v210
    Stream #0:1: Audio: pcm_s16le (sowt / 0x74776F73), 48000 Hz, stereo, 1536 kb/s
Stream mapping:
  Stream #0:1 -&gt; #0:0 (rawvideo (native) -&gt; v210 (native))
  Stream #0:0 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
</code></pre>
","<p>Using <code>embedded</code> (or omitting <code>-audio_input</code>) is the intended way to get the audio of HDMI or SDI signals.</p>

<p>Please make sure that there is a 2ch-stereo audio signal on HDMI. If it is 5.1 or 7.1 you might have to use <code>-channels 8</code> in the commandline before the <code>-i</code>.</p>

<p>Note: please pay attention to the <code>pix_fmt</code> warning. Leave that option out of your commandline.</p>
","20744"
"ffmpeg overlay x expression with logical condition","840","","<p>I am trying to create a simple overlay graphic where the overlay moves and screen and changes speed according to time. So I tried a simple </p>

<p>ffmpeg -i input.mp4 -vcodec qtrle -i overlay.mov -filter_complex '[0:0][1:0]overlay=x=if(t&lt;2\,t*3\,t*5)[out]' -map '[out]' -y output.mp4</p>

<p>and got an error code of :</p>

<p>[Parsed_overlay_0 @ 0000003f4e1e2ea0] [Eval @ 0000003f4c0fe5b0] Missing ')' or too many args in 'if(t&lt;2,t*3,t*5)'</p>

<p>[Parsed_overlay_0 @ 0000003f4e1e2ea0] Error when evaluating the expression 'if(t&lt;2,t*3,t*5)' for x</p>

<p>[Parsed_overlay_0 @ 0000003f4e1e2ea0] Failed to configure input pad on Parsed_overlay_0</p>

<p>Error configuring complex filters.</p>

<p>Invalid argument</p>

<p>if I remove the ""if"" part and just leave a formula it works :</p>

<p>-filter_complex '[0:0][1:0]overlay=x=12+t*3[out]'</p>

<p>I tried playing with escaping of the different characters but it doesn't look like its a parsing error. Any ideas of how to use this correctly? The ffmpeg documentation has a sample for doing font color expression with a logical value <a href=""https://ffmpeg.org/ffmpeg-all.html#Examples-80"" rel=""nofollow"">here</a> , but any attempt at </p>

<p>overlay=x_expr</p>

<p>doesn't really work as well</p>

<p>please help</p>

<p>Thanks in advance</p>
","<p>FFmpeg uses functions, rather than operators, for conditional execution and comparisons, so <code>t&lt;2</code> is not a valid syntax. </p>

<p>The syntax for your given command would be</p>

<pre><code>ffmpeg -i input.mp4 -i overlay.mov -filter_complex \
       ""[0][1]overlay=x=if(lt(t\,2)\,t*3\,t*5)[out]"" \
       -map '[out]' -c:v qtrle -y output.mp4
</code></pre>

<p>where <code>lt(var,value)</code> is the <code>less than</code> function. </p>
","18529"
"What exactly do the sequence settings do?","837","","<p>Sooo, this may sound fairly stupid, especially for professional editors, but I'd like to know the role of the sequence settings in Premiere Pro. I know that the format of the preview files depends on the sequence settings, but apart from that, do they serve any purpose? I mean, regardless of the sequence settings, I can export my video to any format I want. So I'd e.g. like to know if it makes a great difference if I choose AVCHD 1080p25 or DSLR 1080p25 ...</p>
","<p>The entries for the creation of a new sequence are only presets.  They load things like pixel pitch, resolution, color depth, etc.  Many of these can be changed after creation, particularly if you haven't added anything to the timeline yet.  You can also make additional presets if you have frequent use of settings that there isn't already a preset for (or you can even manually select all the settings and not use a preset.)</p>

<p>This is the actual sequence settings:</p>

<p><img src=""https://i.stack.imgur.com/UlueS.jpg"" alt=""Sequence Settings""></p>

<p>As you can see, the framerate(timebase), frame size, pixel pitch (aspect ratio), fields, display format, audio sample rate and audio display format make up the primary settings.  </p>

<p>There are additionally settings for how previews are rendered, which particularly matters if you intend to use previews in the rendering of final output, otherwise it just impacts what you see during editing (after preview renders have finished.)</p>

<p>While you can export in different formats, that will result in scaling if the resolution differs.  The sequence settings determine how the video renders, the export settings determine how the rendered frames are saved in to a final output.</p>
","13060"
"File compatibility for Final Cut Pro 7 project files with Adobe Premiere Pro CS4","836","","<p>The video editing labs in my university have Final Cut Pro 7 installed on them, and most projects that we work on are done in FCP. I don't have a Macbook so I personally use Adobe Premiere Pro CS4 instead for my projects. Is there any way to transfer project files back and forth between the two software?</p>
","<p>Yes, in Premiere Pro, you can export to Final Cut XML. In Final Cut 7, you can export into an XML as well. Then you can import the XML files into each program.</p>

<p>The only problem is that Premiere on windows uses the AVI wrapper, and Final Cut 7 uses Quicktime. It might work going from Final Cut to Premiere. But going back into Final Cut probably will not work, as Final Cut does not like AVI. Also, the file paths will be different (but you can reconnect the media, but you are looking for an easy way).</p>

<p>Unless there is a software that does this, switching in between Programs and OS is going to take time.</p>
","1720"
"Handbrake :: Why is an RF value of 20 to 23 recommended for HD while a lower value of 18 to 20 for SD?","836","","<p><em>This question is based on the software Handbrake.</em></p>

<p>Being a newbie to video editing/conversion and Handbrake, I understand that the RF values are used to select quality.</p>

<p>Lower the RF value, higher is the quality.</p>

<p>RF = 51 means fast encoding while very low quality.
RF = 0 means very very slow but lossless.</p>

<p>So from this logic it would mean that if I want to encode a 360p video for YouTube I would have to select <em>higher</em> RF value because it is lower quality and conversely, it I want a 1080p video then I must go for a <em>lower</em> RF value.</p>

<p>But Handbrake itself recommends RF=23 for HD and RF=18 for SD. How is this possible? I'm confused. </p>
","<p>Resolution != quality, at least, not in the sense referred to by RF. What the RF primarily modulates is the quantization parameter (QP).</p>

<hr>

<p>Let's say you have a 2x2 array of pixels and each pixel is represented by one floating-point value e.g. 3.98, 2.10, 1.05, 7.88. If your goal is to compress the video, then storing these values with full precision and fidelity will be more expensive than if you could store 4,2,1,8. Given the limitations of human vision, it turns out this is an acceptable compromise in the service of video compression. The greater the quantization parameter, the greater the 'smoothing out' of values. Such smoothing out reduces spatial detail i.e. sharpness and subtle color variation in the picture. Of course, this is just one strategy among the many used.</p>

<p>(The above is just a thematic illustration and not an example of the result of an actual quantization) </p>

<hr>

<p>Before RF, video encoders would apply a single QP across the entire video, so a scene with a lot of diversity of visual forms would have the same smoothing applied as a simple scene. If the degree of smoothing is low, the bitrate will shoot up in complex scenes. If it's high, it will degrade simpler scenes more than desired. What RF does is analyze the video content and modulate QP so that visual details appreciated by the human eye are not highly quantized but neglected details are. The RF value determines how much latitude the encoder has to vary the QP.</p>

<p>Now, to your question. A group of pixels, say a 16x16 block, forms a larger part of an SD image than of a HD image, so it will play a larger role in the subjective visual perception of the image (and have greater variation within), so you ought to smooth it out less.</p>
","20389"
"When nesting clip, how to make new sequence same resolution as clip instead of original sequence?","832","","<p>The scenario is this: I have a standard HD resolution sequence, which I drop a 4k clip into. Later I decide I want to use Warp Stabilizer on that clip in the sequence, but I get an error message saying I need to nest the 4k clip in order to use stabilizer.  The easiest thing to do then is right click the clip and choose Nest...</p>

<p>The problem is that the newly created nest sequence uses the resolution of the original sequence, instead of matching itself to the 4k clip resolution. This means the Warp Stabilizer still won't work.</p>

<p>I can get a nested 4k sequence manually with a lot of steps, but is there a better way to achieve what I want?</p>
","<p>You can automatically create a sequence from a clip by right-clicking on it in the project panel and selecting ""New Sequence From Clip"". This will create a sequence matching the resolution and framerate of the clip and put the clip in it. You can drop the newly created sequence in your original sequence like any other asset in the project panel.</p>
","15212"
"Encore CS6 ""DualLayer: This disc requires a layer break""","826","","<p>I am trying to create a dual-layer 8.5 GB DVD image.</p>

<p>It consists of an MPEG multiplexed video with audio and Encore chapter markers created in Premiere Pro CS6 and does not require transcoding in Encore, occupying about 1.69 GB of space.</p>

<p>I also have ROM content where I have two HD video files, with one of them 5.63 GB in size.</p>

<p>Now when I run <code>Check Project</code>, Encore does not show any warnings, however when I try to build the DVD image, it fails with the message</p>

<pre><code>DualLayer: This disc requires a layer break
</code></pre>

<p>I have tried both the <code>Automatic</code> and the <code>Manual</code> layer break setting, and I do not see where and how I could set a manual break in Encore.</p>

<p>So, my question is, how do I make this work, what are my options ?<br>
Is this issue because the 5.63 GB file cannot fit onto one layer (why would this even matter) ?</p>
","<p>It looks like it is not possible to create a DVD DL with a ROM part that exceeds a layer's size.</p>

<p>So I created a 4 GB HD video (which also fits on a FAT32 file system) and created a DL disc with some GBs wasted space.</p>

<p>I tried to put another file onto the ROM part but this always fails as soon as the ROM part exceeds the layer limit, which is strange because DVD-ROM-only DL discs seem to work perfectly.</p>
","13006"
"Screen capture AND audio recording in ffmpeg","825","","<p>I am using ffmpeg built from source at <a href=""https://github.com/FFmpeg/FFmpeg/commit/a66dcfeedc68c080965cf78e1e0694967acef5af"" rel=""nofollow"">this revision</a> in Fedora 20.</p>

<p>I am able to record audio <strong>perfectly fine</strong> with the command:</p>

<pre><code>FFmpeg/ffmpeg -f alsa -ac 2 -i pulse -- output.wav
</code></pre>

<p>However, when I try both screen capture and audio, like this:</p>

<pre><code>FFmpeg/ffmpeg -video_size 800x600 -framerate 25 -ac 2 -f x11grab -i :0.0+0,0 -ac 2 -f alsa -i pulse -ac 2 -acodec copy output.mpeg -ac 2
</code></pre>

<p>I'm getting a video with 0 audio channels, which I can check running <code>FFmpeg/ffplay output.mpeg</code></p>

<pre><code>Input #0, mpeg, from 'output.mpeg':
  Duration: 00:00:09.44, start: 0.540000, bitrate: 2743 kb/s
    Stream #0:0[0x1e0]: Video: mpeg1video, yuv420p(tv), 800x600 [SAR 1:1 DAR 4:3], 104857 kb/s, 25 fps, 25 tbr, 90k tbn, 25 tbc
    Stream #0:1[0x1c0]: Audio: mp2, 0 channels, s16p
   7.26 M-V:  0.000 fd=   0 aq=    0KB vq=   33KB sq=    0B f=0/0   
</code></pre>

<p>and no audible sound is played. The position of <code>-ac 2</code> doesn't change the behaviour, nor does repeating it only once. This is despite ffmpeg claims to record the audio with 2 channels, here is the input:</p>

<pre><code>ffmpeg version N-71312-ga66dcfe Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 4.8.3 (GCC) 20140911 (Red Hat 4.8.3-7)
  configuration: 
  libavutil      54. 22.100 / 54. 22.100
  libavcodec     56. 34.100 / 56. 34.100
  libavformat    56. 29.100 / 56. 29.100
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 13.101 /  5. 13.101
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
Trailing options were found on the commandline.
Input #0, x11grab, from ':0.0+0,0':
  Duration: N/A, start: 1428348285.201679, bitrate: N/A
    Stream #0:0: Video: rawvideo (BGR[0] / 0x524742), bgr0, 800x600, 25 fps, 25 tbr, 1000k tbn, 25 tbc
Guessed Channel Layout for  Input Stream #1.0 : stereo
Input #1, alsa, from 'pulse':
  Duration: N/A, start: 1428348285.225901, bitrate: 1536 kb/s
    Stream #1:0: Audio: pcm_s16le, 48000 Hz, 2 channels, s16, 1536 kb/s
 File 'output.mpeg' already exists. Overwrite ? [y/N] y
[mpeg @ 0x2d079a0] VBV buffer size not set, using default size of 130KB
If you want the mpeg file to be compliant to some specification
Like DVD, VCD or others, make sure you set the correct buffer size
Output #0, mpeg, to 'output.mpeg':
  Metadata:
    encoder         : Lavf56.29.100
    Stream #0:0: Video: mpeg1video, yuv420p, 800x600, q=2-31, 200     kb/s, 25 fps, 90k tbn, 25 tbc
    Metadata:
      encoder         : Lavc56.34.100 mpeg1video
    Stream #0:1: Audio: pcm_s16le, 48000 Hz, stereo, 1536 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; mpeg1video (native))
  Stream #1:0 -&gt; #0:1 (copy)
</code></pre>

<p>I'm clueless. If anybody can tell me how to get this to work by either:</p>

<ul>
<li>fixing my mistakes in the command line flags</li>
<li>if it's a regression, which revision to build from.</li>
<li>any other way</li>
</ul>

<p>I'll buy him/her a pint.</p>
","<p>The solution seems to be to use both a different video and audio encoder. This line works:</p>

<pre><code>FFmpeg/ffmpeg -video_size 1024x768 -framerate 25 -f x11grab -i :0.0+0,0 -f alsa -ac 2 -i pulse -acodec aac -strict experimental output.flv
</code></pre>

<p>I couldn't get it to work without <code>-acodec aac -strict experimental</code>, contrary to the HOWTO on <a href=""https://trac.ffmpeg.org/wiki/Capture/Desktop"" rel=""nofollow"">ffmpeg wiki</a>.</p>

<p>The error message I'm getting is:</p>

<pre><code>Output #0, flv, to 'output.flv':
    Stream #0:0: Video: flv1 (flv), yuv420p, 1024x768, q=2-31, 200 kb/s, 25 fps, 25 tbn, 25 tbc
    Metadata:
      encoder         : Lavc56.34.100 flv
    Stream #0:1: Audio: adpcm_swf, 0 channels
    Metadata:
      encoder         : Lavc56.34.100 adpcm_swf
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; flv1 (flv))
  Stream #1:0 -&gt; #0:1 (pcm_s16le (native) -&gt; adpcm_swf (native))
Error while opening encoder for output stream #0:1 - maybe incorrect parameters such as bit_rate, rate, width or height
</code></pre>

<p>Neither could I get this to work using:</p>

<pre><code>FFmpeg/ffmpeg -video_size 1024x768 -framerate 25 -f x11grab -i :0.0+0,0 -f alsa -ac 2 -i pulse -acodec copy output.mpeg
</code></pre>

<p>which contains <code>-ac 2</code> probably in the right place. The error I'm getting repeatedly is:</p>

<p><code>[mp2 @ 0x7faffc007da0] Header missing</code></p>

<p>This most likely is a bug/regression.</p>

<p>To sum up, I've found a probable bug in both ffmpeg and on ffmpeg's wiki. And it looks like I'll have to buy the promised beer for myself ;)</p>
","15251"
"Command-line tool to split video at timepoints?","819","","<p>I have an hour-long video that I want to split into individual clips at arbitrary timepoints. I could do this by hand in iMovie, or even borrow a machine with Final Cut, but there are upwards of thirty separate clips, so doing it manually strikes me as time-consuming and error-prone.</p>

<p>(It's a sequence of recordings for a language textbook, but it's all one file, and it's unwieldy not to be able to separately view, repeat, etc. the chapter-by-chapter clips. Currently it's an H.264 MPEG-4 but I'm not adverse to converting it.)</p>

<p>I know all the timepoints, and I'd like to feed a list of them to something, or build a script around the list, to produce the split clips without further human intervention. Linux would be okay but Mac OS would be ideal; Windows isn't really an option.</p>

<p>Suggestions?</p>
","<p><a href=""http://howto-pages.org/ffmpeg/#basicvideo"" rel=""nofollow"">ffmpeg</a> is the obvious choice. </p>

<pre><code>ffmpeg -i x.mov -ss 00:05:00 -t 00:01:00 y.mov
</code></pre>

<p>will split out a segment of the movie 1 minute long from minute 5.</p>

<p>I've not had much luck with lip-sync when doing this. I think there a problems with both decoding and re-encoding the GOP structure and the equivalent AAC or MPEG1 audio structures; but the underlying issue must be that the audio and video frame rates are not linked, e.g. 48 kHz for the audio and 29.97 fps.</p>
","4049"
"Transitions in Premiere Pro CC 2015 with After Effects Sequence","814","","<p>I have two clips in Premiere, to which I want to add a black fade transition. So far that works without a problem.</p>

<p>But when I replace the clips with After Effects compositions (because I want to add some effects on the clips, like Twixtor) and then try to add a black fade transition between those clips (now AE compositions), it doesn't work.</p>

<p>Premiere tells me there are not enough frames for the transition, altough the clips are still the exact length, but only AE compositions...</p>

<p>What is the right workflow for doing stuff like this, so I will still be able to add transitions over the clips when they come back from After Effects?</p>
","<p>Often (and perhaps in your case) what is happening is the following.  Initially you have a clip that has plenty of content before or after your transition point.  You trim that clip to represent the content you actually care about.  If you do a fade at this point, the fade is going to use frames from the trimmed part of the clip that you don't necessarily see, but which are there (and which are used to complete the fade).  Now, if you send your trimmed clip through some external process, it doesn't send the /whole/ clip to the external process, just the frames you ""see"".  That clip that seems properly trimmed doesn't actually have the frames to allow the fade to complete the way you want it to.</p>

<p>To fix this, send a longer trim of the clip to AE, then, when it comes back, re-trim it to what you want.  You should find the fade working as well as it did for you initially.</p>
","16593"
"Why does my Class 10 memory card run in to buffering problems?","814","","<p>My Canon EOS 600D's class 10 SD card does not write out data very quickly and this results in buffering issues which can be seen when shooting 1080p@30fps video.  Why can't the card keep up and what can I do to fix the problem?</p>
","<p>The class of a memory card often deals with the reading speed of the card, even if it should deal with the write speed.  It is relatively common for a card to have much quicker read times than write times, but since the read time is fast enough, they slap a faster class label on it than it can actually write.</p>

<p>You will need to get a class 10 card that actually has high speed writing in addition to high read speeds.  The best bet for checking this kind of thing is to read reviews of the cards and buy from reliable manufacturers that put the write speed in their spec.  The spec can be important since SD card makers will often change the actual storage chips used without changing the part number of the card.</p>
","9919"
"1080p video partly pixled after rendering","808","","<p>So I'm pretty new to video production and I'm struggling with properly rendering 1080p game footage. My main problem is that multiple frames of the output video are pretty pixled, looking like this 
<img src=""https://i.stack.imgur.com/rUhS5.jpg"" alt=""enter image description here""></p>

<p>I'm working with footage recorded using fraps and I'm editing with Sony Vegas pro 12 atm. 
My project settings are matched to the footage I'm using in this particular project:</p>

<p><img src=""https://i.stack.imgur.com/VSKEn.jpg"" alt=""enter image description here""></p>

<p>No doubt I messed up the rendering options somehow, since the raw footage looks okay. Here are the render settings I used for the shown output:</p>

<p><img src=""https://i.stack.imgur.com/TK7ZI.jpg"" alt=""enter image description here""></p>

<p>Can someone help me get those pixled scenes out of the video? Desired output format is MP4 since it seems easier to handle for youtube uploads.</p>

<p>Thanks in advance, I hope someone can help me with this.</p>

<p>Best regards,
Rickyfox</p>

<hr>

<p><strong>Edit:</strong></p>

<p>I just discovered that about a minute of the output video just turned out black with the scene settings being exactly the same as the rest - anyone have a clue what that could be?</p>
","<p>It honestly looks like something got corrupted with your export.  Have you tried encoding it more than once?  12 megabits per second with a 24mpbs peak should be more than sufficient for high quality 1080p video.  If anything, it may be excessive.  The quality you are getting looks more like what I'd expect at sub-1 mbps video for this quality level.  As an aside, it is worth noting that 2 pass will produce better results, thought it will extend encoding time.</p>

<p>One other thought is to try watching it in more than one viewer.  It is possible that the viewer you are using is struggling to decode the high data rate fast enough which could also result in the block issues you are seeing.  Basically, it looks like some of the blocks in the H.264 file are not encoding or decoding properly and that either means file corruption or insufficient decoding speed.</p>

<p>I guess to rule out data rates, you could try increasing it to 30mbps with 45mpbs peaks.  It shouldn't have to be that high (that's the high end of bluray data rates), but it should rule out the data rate entirely.</p>

<p>Update: From some additional reading, I would try turning on 2-pass and turning on the deblocking filter.  It supposedly should help.</p>
","8556"
"Can I use unlicensed music as soundtrack for a video?","808","","<p>I'm shooting a 1 min. promo video for an iPhone app startup. Can I use unlicensed music as long as I mention the artists on the closing credits? Are there any other resources available to help understand how copyright applies here?</p>
","<p>The short answer? <strong>No</strong>.</p>

<p>The long answer: No, copyright law requires you to have an appropriate licence from the copyright holders (usually the record label - not the artist - if they're in a record deal!)</p>

<p>There are three distinct rights for every piece of music:</p>

<ul>
<li>Phonogram copyright (the sound recording) (symbol: , ""P"" in a circle)</li>
<li>Composition, score, artwork, cover design, lyrics, which can all be included in one overarching copyright registration per musical work (symbol: , ""C"" in a circle)</li>
<li>The Publishing right, considered separately by some</li>
</ul>

<p>Each of these rights may be 'assigned' to a third party (e.g., a publisher or a record label) and this is what usually what happens when a track is released by a label. However you may be able to 'encourage' the artist to grant a licence for the music's use if the project is cool or they like it. Ultimately the person who has control of the sound recording copyright is the person to request a licence from.</p>

<p>You are in breach of copyright law if you use a copyrighted piece of music without a licence - moreso if you exploit that copyrighted composition for commercial gain (use it to represent the product or service, even if it's just background music). Whilst I think the current system of licensing music is crazy, that's just how it works for the time being.</p>

<p>Occasionally artists are willing to 'look the other way' if you agree a licence arrangement after the finished work using their copyrighted music is released -- this may not even be a cash payment, it may just be promotion with the band, song name and iTunes link at the bottom of the screen. However, arranging a license 'after the fact' still compromises you from a legal perspective, and you could be forced to stop using that music in your own work (and, worst case, be taken to court for copyright infringement) if the artist or label doesn't like it.</p>

<p>I concur with the other answerer's suggestion to seek Creative Commons licensed music; Jamendo is a good site to start on, and even if the initial license is CC-NC you can communicate with the artist to request a commercial use licence (or to be granted an exemption). You may also wish to try sites like YouLicense.com to seek appropriate sync licenses for music for a reasonable cost.</p>
","2277"
"How to create TS files from pictures and videos?","808","","<p>I have many videos and pictures. I'd like to burn DVD with pictures and videos and play in DVD Player.
And I want to make cml tool to prepare pictures and videos for creating DVD. I know how I burn dvd from video_ts folder, but not how I create video_ts folder with pictures and videos.</p>

<p>1.Can I write pictures and videos to Video-DVD?<br>
2. How to create video_ts files with pictures and videos?<br>
3. To prepare pictures and videos for writing dvd, how should I do?<br></p>
","<p>You can use the free <a href=""http://www.dvdstyler.org/"" rel=""nofollow"">DVDStyler</a> to create DVDs. </p>

<p>It should also be possible to include photo slideshows, although I didn't test that.</p>
","16126"
"How to change the start time of track in Final Cut Pro X?","807","","<p>I'm exporting a project from iMovie to Final Cut Pro X.</p>

<p>It's really a mess, but working patiently on the project on FCPX can achieve some better results... in any case, during the exporting many videos and sounds chenged their start and end time.</p>

<p>So, to give an example, if I had an audio of me reading the sentences ""Hi all, how are you? Today I'm not fine because I'm getting crazy"", and in iMovie I had cut the 
""how are you"" sentence, then in FCPX it gives me the ""I'm getting crazy"" part of the track.</p>

<p>So I have two question:</p>

<ol>
<li>do you know how to fix this? Is there a known issue? I searched a lot but I couln't find anything!</li>
<li>in case there is not a solution to this problem, is there a FAST way to change the start time and the end time of a track? </li>
</ol>

<p>I tought it was simple to do with the ""open in timeline"" menu, but it is not possible.</p>

<p>When I open the audio track in the timeline, the active part is eveidenced, but appartenly there is no way to MOVE the part I want to use...</p>

<p>I hope I made myself understand.</p>
","<p>Try using the trim tool (keyboard shortcut ""t"").</p>
","7153"
"How to make Premiere Pro Connect Hindi Characters?","805","","<p>I am using Premiere Pro CC 2017.</p>

<p>Recently when I try to type ""Hindi""  Language viaGoogle Input tools, </p>

<p>Typed text not showing ""Joint word"" correctly.</p>

<p>But my language working completely at everywhere except Premiere Pro.</p>

<p><strong>Already Changed text engine to South Asian</strong></p>

<p>If not understanding my question please visit thisquestion,this problem is for android and my question for Premiere Pro. </p>
","<p>Solved.. </p>

<p>Turned On Ligatures in Preference-Titler</p>

<p>Changed font to Adobe Devnagari</p>

<p>Restarted Premiere </p>

<p>Thanks to @user1118321 for Ligatures Meaning</p>
","20629"
"How can I make an actor cry?","803","","<p>In a film I am making I need and actor to start crying, but I cant think of how I am going to do that without actually making them sad. </p>

<p>One thing I have heard is that there are eye drops you can get. </p>

<p>Are there any other options?</p>
","<p>Eye drops or saline are certainly simple. Alternatives include blowing dry air at the face just before action, or rubbing the eyes. </p>

<p>Can your actor not use the old tried and tested method of remembering the death of a childhood pet?</p>
","1797"
"How do I edit video footage from an interview?","800","","<p>I was asked to produce a set of video interviews for a friend's wedding consisting of individual single-subject interviews (the interviewer is not on camera or mic) with the bride, groom, close family, and wedding party.</p>

<p>The questions were prepared, the lighting and scenery set up, the subjects miked for sound, the cameras positioned, the interviews conducted, and now I have a bunch of video files sitting on my hard drive.</p>

<p><strong>In short my question is, ""How do I edit together interview footage?""</strong> </p>

<p>To word it in more detail: My experience and background is weighted towards shooting video rather than editing it. I have access to all the software I might need and I know the basics of how to use it (Premier Pro, Sony Vegas), but I don't really know what I am doing. <strong>Is there a crash course in video editing, or possibly interview editing, that I could study to get started?</strong> Or quintessential interviews to watch? Or books to read?</p>
","<p>There is no one right way to edit anything. I've never watched a video of this kind so I probably can't give the best advice. But if I was in your position there are a few ways I'd consider cutting it together:</p>

<ol>
<li>Edit it a bit like a 'behind the scenes' video. Begin with footage of someone talking and then use appropriate cutaways of the wedding. Cut back to different people whenever it is appropriate and put some music underneath, something that fits the mood.
This would be my preferred way but you'd need a fair amount of footage of the actual event and even some archival footage or photos would be very handy. Most movies have a behind the scenes on the DVD so I'd suggest watching one or two of those if this method sounds good to you. </li>
<li>The simplest way would be to show each interview one by one. So you'd show the first person's interview in its entirety followed by the next person's, and so on. Crossfade between them and have music underneath. This would probably be a boring video to watch in the end, but it'd do the job.</li>
<li>Similar to my previous suggestion, this would require only the interview footage (no event or archival footage). However, you'd intercut the interviewees based on topic. For example, if the bride talks about how she met the groom you'd intercut that along side the groom talking about how they met (assuming he does). This way you would be able to cut out a lot of the fluff that you get when interviewing people.</li>
</ol>

<p>That's how I'd go about it anyway, mate. Hope that helps you out a little. </p>
","2944"
"after effects how to Zoom-in but keep same size (zooming and cropping)","799","","<p>I am using After Effects, i am zooming an image using two keyframes and the scale setting, but as i zoom in, the image gets bigger. </p>

<p>I want the image to have the zoom-in but keep its assigned size. So i want something like ""crop"" on top of the zoom.</p>

<p>How to do this?</p>
","<p>There are a couple of ways to achieve this:</p>

<ul>
<li><p>Use a <strong>track matte</strong>. Create a layer with the desired size / shape and put it above the zooming layer. You can use masks to create any shape you want, or use an image that has an alpha channel, or even just a copy of the original layer without the animated scale. On the zooming layer go to the Tack matte / mode panel (you may have to hit <kbd>f4</kbd> to toggle between switches and modes), and choose <strong>Alpha Matte ""layer name""</strong>. This will turn off visibility for the top layer, and apply its transparency to the layer below.</p>

<p><img src=""https://i.stack.imgur.com/20E2J.png"" alt=""enter image description here""></p></li>
<li><p>Use a layer with the <em>stencil alpha</em> transfer mode above your original layer. This does a similar thing to a track matte, but it's not just for one layer, it's for every layer below the matte layer. And the visibility for the matte layer has to be on for it to work.</p></li>
<li><p>Use a preComp. Pre-comping replaces the original layer (or layers) with a composition containing just that layer(s). So you want to pre-comp the layer with the animated <strong>Scale</strong> property  select it and hit <kbd>shift</kbd>-<kbd>cmd/ctrl</kbd>-<kbd>c</kbd> or choose <strong>Layer>Pre-Compose</strong> from the menu. In the dialogue that appears choose <em>move all attributes into the new composition</em>. Now you can add a mask to the pre-comp to crop off the scaled edges of the layer. A pre-comp is similar to what you'd get if you rendered the layer as a movie and then brought that back into your comp (but with a few advantages). As the pre-comp is not zooming, just its contents, the mask will crop the edges as the contents get bigger.
<img src=""https://i.stack.imgur.com/cGasI.png"" alt=""enter image description here""></p></li>
</ul>
","14639"
"Cropping a Video based on a motion track","795","","<p>I have some raw footage from a lacrosse game that I filmed and the coach told me to make a video for a collage about a player.  Is there a way to crop a video relative to a motion track.</p>

<p>I would want to track the player and crop the video so the player stays in the center of the frame rather than giving the coach general footage with the player in it. I understand that I will be giving up a lot of quality cropping the video to a small size but just having the ability of doing such a thing would be nice.</p>

<p>Programs to use:
- Blender
- After Effects CC
- Premiere Pro CC
- Any of the programs in Adobe CC</p>
","<p>Here's one way to do it. I'll assume you have basic motion tracking knowledge already for the sake of brevity. </p>

<h2>AfterEffects</h2>

<ul>
<li><p>Turn on 3D space for your footage layer. </p></li>
<li><p>Track the player in your footage. I'm going to guess that you'll have to do a lot of this by hand, but there's not really a faster way to track a player like this.</p></li>
<li><p>Apply the track to a Null, including both X + Y values of course. Turn on 3D space for the null as well. </p></li>
<li><p>Create a Camera. Default values are fine. They will match your footage, so you initially won't see any change.</p></li>
<li><p>Select the Null and press P to open the position parameter.</p></li>
<li><p>Here's the tricky part. Alt+Click on the Position stopwatch for your Camera. This will turn the position numbers red, open up an Expression text box, and reveal some new buttons. Click and drag the Pickwhip onto the Null's position parameter. </p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/UsMFB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UsMFB.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>Your comp should go black. Good, it's working!</p></li>
<li><p>Open up the Position twirly for your footage layer (in my comp it's A037C006_160309CZ.MXF). The third value is the Z-depth. Adjust this to taste - the larger the number, the larger the resulting ""crop"" of your footage. Here I've set it to 1700 for a pretty extreme crop (and this is also 4K footage) - a more reasonable look, in my case, might be more like 2500. For you it will vary depending on the footage. </p></li>
<li><p>As you scrub through the timeline you should be perfectly locked onto the player. Unfortunately, depending on how close they get to the edge of the shot, you may see black appearing at the edge. This means you've ""cropped"" in far enough that you've hit the edge of the frame. You can reduce the crop (move the footage layer closer to the camera, a smaller number) as necessary to fix this. </p>

<ul>
<li>You may find that your track is a bit choppy. You can smooth it out a bit non-destructively using some of the techniques <a href=""https://forums.adobe.com/thread/465641"" rel=""nofollow noreferrer"">detailed here</a>.</li>
</ul></li>
</ul>
","18072"
"After Effects: Shortcut for showing all transforming properties of a layer","794","","<p>just wondering if there is a shortcut in after effects to reveal all transforming properties (scale, position, opacity, etc.) of a selected layer at once?</p>
","<p>Some shortcuts depend on the <strong>language</strong> and your <strong>keyboard layout</strong>. </p>

<p>For a german layout it's: <kbd>Ctrl</kbd>+<kbd></kbd>. </p>

<hr>

<p>However <strong>concatenating</strong> the shortcuts always works.</p>

<p>For <em>Scale</em>, <em>Position</em> and <em>Opacity</em> press <kbd>S</kbd>, <kbd>Shift</kbd>+<kbd>P</kbd> and <kbd>Shift</kbd>+<kbd>T</kbd>.</p>
","17342"
"D7000 HD video format - can each frame contain metadata?","792","","<p>In fact the question is the other way round:</p>

<p>I shoot a HD video with my Nikon-D7000, and along the way things happen, such as I change zoom, focal length, light levels change such that the D7000 adjusts its sensitivity during the recording...</p>

<p>Is all this data available for reading somewhere, and if yes how?</p>

<p>I could not find the information on regular websites (Nikon's, www.imaging-resource.com, general google search).</p>

<p>I tried exiftool, but it seems to only show general metadata that is true for the overall video recording.</p>

<p>So is this data recorded somewhere, and can I read it on my Linux system?</p>

<p>Any answer or hint appreciated.</p>
","<p><strong>No.</strong> Standard video formats do not work like that and the D7000 is no exception to this.  Conceivably it can be done but I am sure it is not on anyone's priority list. I worked for 9 years on video processing and rendering software and there are more important problems to solve. Indexing and managing that data would also become a problem. When working on functions that process video from a series of still images, it makes it vastly simpler to assume the metadata of the first frame applies to all subsequent ones.</p>

<p>Obviously I understand why it would be desirable but this is the same on all cameras. GPS is another perfect example. On cameras with built-in GPS, only the location of at the time video recording starts is recorded while you may be shooting from a moving vehicle and cover some ground while recording.</p>
","2983"
"h264 and h265 keyint params settings","792","","<p>Fiddling with the h264 and h265 params I found both <strong>keyint</strong> and <strong>min-keyint</strong> settings.</p>

<p>Setting keyint shouldn't force the codec library to use exactly that frame interval?</p>

<p>Do min-keyint have something to do with scene detection and if set it take priority over keyint plain setting ?</p>

<p>TL;DR: I found that h265 default settings are keyint=250 and min-keyint=25 ... what I should expect from that? A fixed length GOP or a variable length one?</p>
","<p>See these two links:</p>

<p>1) <a href=""https://en.wikibooks.org/wiki/MeGUI/x264_Settings"" rel=""nofollow"">MeGUI/x264 Settings</a> for an explanation of the parameters like keyint.</p>

<p>2) <a href=""http://forum.doom9.org/showthread.php?t=165197"" rel=""nofollow"">About --keyint --min-keyint and --scenecut</a> for a discussion at Doom9 about how they interact, specifically</p>

<blockquote>
  <p>--keyint is the maximum interval. x264 will insert key frames earlier if it deems them beneficial, e.g. due to a scene change. From my
  experience the majority of key frames are in place because of the
  properties of the video, not because keyint was reached. The larger
  --keyint is the smaller the impact on overall key frame placement should be.</p>
</blockquote>
","16961"
"Chromakey (Green Screen) Best Practices in Adobe After Effects","789","","<p>I've been doing green screen in iMovie (and it works surprisingly well!), but I'm downloading Adobe Creative Cloud tonight for my new PC. I'd appreciate any ""best practice"" suggestions for using the After Effects chromakey features, particularly with green-screen (rather than blue).</p>

<p>BTW, I'm trying After Effects because iMovie was having a problem with my actress's blue eyes and blonde hair, both of which became partially transparent.  (I think the scene was lit fine, so her hair shouldn't have had green reflecting off of it.)</p>
","<p>Assuming the lighting is solid, you should be in good shape. I haven't used the keying features of iMovie, but I can give you a few tips for After Effects. I'll defer to any power users of AE's keying functionality to chime in with more specific advice.</p>

<p>First, you'll probably want to start with the Keylight plugin (from The Foundry). It should come bundled with After Effects Creative Cloud. There are tons of tutorials, guides, etc. for Keylight on the web.</p>

<p>Second, know that you can create masks with After Effects. Not sure if you're already familiar with them, but masks are custom shapes that allow for specific areas of footage to be visible or modified. Masks allow you to create a ""junk matte"" around your subject, which can be helpful if lighting gets a little uneven or equipment is visible on the edge of the shot. Masks could also allow you to preserve the actor's eyes so that they are not being keyed (especially if the shot is relatively static). For example, you might want to duplicate the main video layer, mask around the actor's eyes, then pull the key on the duplicated background layer.</p>

<p>Third, if you're still having problems with the hair after firing up Keylight, you'll want to look into the ""Despill Bias"" option. With despill bias, you'll basically select a skin color to reduce the blue / green bleeding. It might take a little trial and error.</p>
","10705"
"Is it possible to merge two or more videos in real-time like this?","788","","<p>Is it possible to play video online that's made of two or more video files?</p>

<p>Here's what I mean.</p>

<p>My site is hosted on Linux/Apache/PHP server. I have video files in FLV/F4V format. I can also convert them to other available formats if necessary. All videos have same aspect ratio and other video parameters.</p>

<p>What I want is to have online video player that plays video composed of multiple video files concatenated together in real-time, i.e. when user clicks to see a video.</p>

<p>For example, user sees video titled ""Welcome"" available to play. When he/she clicks to play that video, I take video files ""Opening.f4v"", ""Welcome.f4v"" and ""Ending.f4v"" and join/merge/concatenate them one after another to create one continuous video on the fly.</p>

<p>Resulting video looks like one video, with no visual clues, lags or even smallest observable delay between video parts. Basically what is done is some form of on-the-fly editing or pre-editing, and user sees the result. This resulting video is not saved on the server, it's just composed and played real-time.</p>

<p>Also, if possible, user shouldn't be made to wait for this merging to be over before he/she sees resulting video, but to be able to get first part of the video playing immediately, while merging is done simultaneously.</p>

<p>Is this possible with flash/actionscript, ffmpeg, html5 or some other online technology? I don't need explanation how it's possible, just a nod that it's possible and some links to further investigate.</p>

<p>Also, if one option is to use flash, what are alternatives for making this work when site is visited from iphone/ipad?</p>
","<p>DASH HTML5 video streaming already works by getting the player to load video in chunks that are usually about 10 seconds long.  You should be able to cook something up that gets the player to display your segment seamlessly, by giving it a DASH-like list of videos.</p>

<p>You might need to remux your video segments into .mp4, if some web browsers choke on the flv container format.  (assuming that's the container for f4v, which I've never seen before.)</p>
","14970"
"Crop then center and zoom","787","","<p>My video is 1080p HD, shot with GoPro. Using GoPro Studio I have removed the fisheye effect and exported it in AVI format.</p>

<p>Using Pinnacle 17 (or another tool, but this one has to be free), I want to crop some part of the entire image, then zoom it (a little), then center it above another clip. </p>

<p>I don't know how to do this. I've tried effects in ""media editor"" but so far I only ended with standalone pictures.</p>
","<p>I used Picture in Picture to display my orginal video, cropped, zoomed and where I want above another picture or video.</p>
","10662"
"How to add meta data to a video file","784","","<p>I tried to change the Meta data of a video file. 
My system runs on Windows. So I tried to change meta data through rightclicking on the file and going to the properties > details. After chaning some information and saving this error occured: </p>

<p><img src=""https://i.stack.imgur.com/04t8I.jpg"" alt=""enter image description here""></p>

<p>What happend next is that the video file couldn't be played in a videoplayer anymore. This is the error message in Windows Media Player: </p>

<p><img src=""https://i.stack.imgur.com/VsoO0.jpg"" alt=""enter image description here""></p>

<p>I have to re-render my video file. But my question is: what is the best method for adding meta data to a video file? What is the best Tag editor application on Windows?</p>
","<p>Googling for ""mp4 metadata editor"" brings up lots of candidates. Many of these will search online databases for tagging DVD movie rips, a feature I don't think you're looking for, but they also let you add and edit tags in most common container formats like MP4. Two you might look at are <strong>MP3Tag</strong> (yes, MP3) and <strong>MetaX/MetaZ</strong>.</p>
","15425"
"How to record a video of computer screen movements","783","","<p>Can someone suggest good softwares for making a video tutorial of computer screen, like is done on most computer tutoring sites?</p>

<p>I am looking a software that can capture a video of my blackberry simulator run so that the client can preview the look and feel of an app on the device.</p>

<p>Please suggest software that is freely available and which doesn't add any watermarks.</p>
","<p>There are a large number of these, mostly advertised on gaming websites, but I think this question may be too localised. </p>

<p>Google will find you many, 2 seconds found me this:</p>

<p><a href=""http://www.anewmorning.com/2009/01/06/5-free-alternatives-to-fraps/"" rel=""nofollow"">http://www.anewmorning.com/2009/01/06/5-free-alternatives-to-fraps/</a></p>

<p>And if you want to find a good one, that link gives some information about each one.</p>
","4097"
"How do I correctly use an ACES workflow with CinemaDNG footage in Davinci Resolve?","782","","<p>I'm just now discovering ACES with Resolve version 11. I can get Sony S-Log2 footage to look good, but I'm having trouble with CinemaDNG from my 2.5kBMCC. I've seen <a href=""https://www.youtube.com/watch?v=B7DqtW7LB7g"" rel=""nofollow noreferrer"">this</a> tutorial on how to do it, but when I get to the step where you select an IDT, CinemaDNG is not available. Any thoughts on why this might be so? Is there something different I have to do in version 11 than what this guy's doing in 9.x? Here's the IDTs from which I have to choose, in case I'm just crazy.  <img src=""https://i.stack.imgur.com/0oCxc.png"" alt=""available formats""></p>
","<p>You should use BMD Film (Black Magic Design) This is the colorspace used by the Black Magic Cameras.</p>
","14524"
"How to create exact copy of protected DVD?","779","","<p>First the explanation: I just bought some workout DVDs. My wife and I have a room in our house where we exercise, but my wife also works out with friends she dances with at her studio. I'd like to work out at home, and she'd like to be able to work out at her studio. </p>

<p>We could swap DVDs and try not to do the same workouts on the same days, but it would be easier if I could just copy the DVDs and she could just have those to herself. So my question is, how can I create an exact copy of the DVDs, complete with title menus, etc? I want the experience to be the exact same from the originals to the copies.</p>

<p>I've found a bunch of articles on this through Google, but they all seem to be centered on ripping. Furthermore, the few articles I've found about actually copying never mention whether the entire DVD will be copied (title menus, etc.. everything the same) or just the primary title (like the movie, but not extra features, etc.). I've messed around some with MakeMKV, and it seems to have ripped all the titles ok, but I don't know how to reconstruct them back into the original DVD.</p>

<p>I'm willing to pay for software that can do it. I really just need to know if this is possible, particularly with encrypted, protected DVDs.</p>

<p>Thanks.</p>
","<p>Since you don't seem to want to make any changes, just rip the whole DVD to an ISO image and then burn that to blank media. I recommend <a href=""http://www.imgburn.com/"" rel=""nofollow"">Imgburn</a>.</p>
","13264"
"How can I color correct a footage from a 7D using the CineStyle Picture Style","778","","<p>I want to use the CineStyle color style for my 7D to get the most out of the dynamic range.  Saw some tutorials on how to use a LUT file to color correct in Final Cut Pro or Premiere, but only on the Mac version with the help of a plugin.  How can I do this on the PC?</p>

<p>Technicolor Cinestyle provide a .mga file to color correct the picture.  But I can't seem to find how to use that.</p>
","<p>The CineStyle picture style provides you with a flat, desaturated image: providing the best dynamic range of any of the picture styles for Canon DSLRs. Technicolor provide you with a file that you can apparently import into your editing software to help you grade your footage. However, I never figured out how to use it and frankly, I don't think one needs to. I think your time would be better spent learning how to color grade rather than learning how to import an .mga file. You don't need the file to grade your footage. I think it is more of a preset kind of thing used to return the flat, desaturated image into a contrasty, attractive one albeit with more detail in the shadows and highlights than if you'd used any of the inbuilt Canon picture styles. However, this is something you can do yourself, and if done right, you will achieve better results than any preset can give you. </p>

<p>Most editing programs will have a color correction filter. Some programs have better than others but most should satisfy your needs. However, I will recommend a good piece of free software called <a href=""http://www.blackmagic-design.com/support/detail/?os=win&amp;sid=3948&amp;pid=11735&amp;leg=false"" rel=""nofollow"">DaVinci Resolve Lite</a>. Like I said, it's free, and it's awesome. </p>

<p>Sorry I couldn't give you any help with the .mga file. </p>
","3173"
"How To Get The Webcam To Record From Guitar Rig Directly","778","","<p>i am trying to figure out how to get my webcam to record directly from Guiat Rig. Currently i have to manualy sync the audio and video.
i have the focusrite scarlet 2i2 audio interface. If possible i don't want to use a mic. 
When i see the recording sources in webcam I see either the laptop mic or the line in from focusrite audio interface. I choose line in from focusrite. The guitar recording happens but the effects from guitar rig don't get recorded.</p>

<p>Please suggest.</p>
","<p>The focusrite scarlet 2i2 has no inbuilt mic, you need to connect an ""external mic"" to it, or connect the guitar lineout to it. If you have effects pedals, just connect the last 1/4"" connector n the chain to the input of the Focusrite.</p>

<p>Please update why you are having sync problems. Anyway don't not rely on a webcam to give synced audio/video. I would say that if you record video &amp; audio from different sources, almost always you have to sync audio with video. Professional gear have a ""sync clock"" that makes it automatic but this is expensive. At less expensive level you have to work ""hard sometimes"" with sync. Having a good program like Final Cut Pro or Adobe Premiere helps.</p>

<p>Apart from the discussion of how to fix <strong><a href=""http://www.zoom.co.jp/products/q3hd/"" rel=""nofollow"">this</a></strong> (Zoom Q3HD Video Recorder) to the guitar, this is a very practical audio/video recorder, you can even connect external input to it. </p>
","8539"
"What software to use for lighting and rendering 3d animation shorts?","778","","<p>I am new to 3d animation. I have always wanted to make 3d animation shorts (cartoons) and have now made the plunge. Over the past few months I have learnt to model, rig and animate characters (Maya/zbrush). I want to take it a step further and learn lighting and rendering. After doing research for the past week, I have realised I have no choice but to ask here as several forum posts just mention that big studios use their own proprietary pipelines and softwares, while many indie-animators use all kinds of softwares from NUKE to after effects, V-ray to mental-ray. The thing is that people are using such software for all kind of VFX (like NUKE for live-action and v-ray for photo-realism). Can someone please guide me on the following questions:</p>

<p>For 3d/CGI animation shorts, what it the right lighting and rendering software to learn? </p>

<p>Do you recommend any tutorials out there that I can look at for those specific softwares? I do not want to be looking at a 10 hours video tutorial series on v-ray if they are teaching interior design lighting examples instead of how to light up a cartoon cgi animation. So unfortunately google has been unable to help me find specific tutorials on this.</p>

<p>I do not want to compromise on the quality so difficulty level and price of the software doesnt matter (not yet anyways).</p>
","<p>There seem to be a few basic misconceptions here.  For cartoons you don't need anything fancy as long as cell-shading is supported, which is typically a camera filter.</p>

<p>Lighting is actually done in your 3d animation software.  The job of a render engine like Mental Ray or V-ray is simply to process the scene you build to generate the final image, but it takes the geometry, light sources, materials, and cameras that you have defined in your animation software.</p>

<p>I'm personally a big fan of Mental Ray in general, but that's just because of the fact it is included with my 3d animation package of choice (SoftImage).  I thought it was included in most editions of Maya as well, but it might not still be.  I don't remember which render engines are included with Maya, but they should be more than sufficient for your purposes.  For cartoon rendering, you really don't need a hyper-accurate ray tracing engine that can do good physical simulation (which is the point of most of the third party render engines).</p>

<p>Lighting and rendering is also a complex and broad topic that can't really be covered in a couple of simple tutorials.  The basics are very similar to building your model, you just choose the types of lights you want and place them throughout the scene and then place one or more cameras to allow you to render viewpoints.  Lights and cameras can also be animated, just like your characters and scene.</p>

<p>Do your lighting in Maya and try rendering it out in Maya.  If you run in to any specific issues while working on trying to get the look you are going for, you can always post a screen shot here with a specific question and we should be able to offer more directed feedback about how to accomplish what you are trying to do.</p>
","10783"
"What's the best before editing format to convert to?","777","","<p>I am making a production for my companies trade show booth.  We're going to be running 2 40"" inch TVs one cycling a few videos of our products in production and one cycling through a bunch of product pictures.  Anyway...</p>

<p>I have a bunch of videos we want to use but they're all in different formats and resolutions (Taken with different cameras over the years).  Some 720p, some 640x480, some MOV, some MPG, mp4, avi, etc, etc...  I am trying to get away with free on the video editing (VirtualDUB, Handbrake, etc...) so I am forced into using different tools to do different things.  Well in my experimenting I am finding that some video tools open mp4 and some don't.  Some open mov but most not.  I'd like to keep the videos as ""RAW"" as possible because they're all going to end up on a 40"" screen eventually.  So you can see where I want to lose as little resolution as possible in the conversions.  I like mp4 (H.264) because it's the format that my player (Roku 2 XS) requires, but there isn't much as far as free video editing tools that will open those video files.  What format should I convert everything into before I start doing my editing?</p>

<p>Thanks
-Craig </p>
","<p>I would recommend downloading the trial version of <a href=""http://www.adobe.com/cfusion/tdrc/index.cfm?product=premiere_pro"" rel=""nofollow"">Adobe Premiere Pro</a>, and cutting the videos together in there. The trial lasts 30 days which should be enough to get the job done..</p>

<p>Premiere Pro can take almost any file you throw at it (so you can leave all your videos in their original format). This way you wont be loosing any quality by converting your videos to one uniform codec before you edit. </p>

<p><strong>Otherwise</strong> as a completely free solution Id suggest converting all your videos to 1920x1080p h.264 files. Then you can bring them into iMovie or Windows Movie Maker to order and export one final file. </p>

<p>If you dont need the all the videos combined into one file (ie. if your player can play all the files on its drive continuously) then all you need to do is convert them to 1920x1080 h.264 files and load them on the drive. </p>
","3914"
"Software to create nice presentations/infographics","774","","<p>I need to find some software that will help me create a nice and user friendly presentation, like this one : <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/XU0llRltyFM?start=0""></iframe>
            </div></div></p>

<p>I prefere to use better effects and transition between objects but powerpoint needs lot of work to do that, and it is very hard to get it done with good precision.</p>
","<p>I think you would like to take a look at this free online presentation editor:</p>

<p><a href=""http://prezi.com"" rel=""nofollow"">Prezi.com</a></p>

<p>It might not give you results as ""fancy"" as in your provided video sample but you might be satisfied by it's simplicity of constructing your presentations in a dynamic way. Here is some presentation <a href=""http://prezi.com/sx2j5xrnniam/3d-backgrounds-freshwater-shortage/"" rel=""nofollow"">sample</a> to check it out if still wondering before signing up.</p>
","4431"
"Creating effects/transitions in AviSynth or Lightworks","773","","<p>I'm very much an amateur video maker, but I am trying to broaden my skill set a bit and start creating my own effects and short video clips to supplement the recordings I make in a more substantial way.  As an example, one of the first things I'd like to do is to make a small clip that functions a little bit like Megaman boss select screen of yore; moving a selector around some pictures and scrolling in a text label to briefly explain it.</p>

<p>I can handle the resources that go into it to a large extent; the portraits, the actual selector graphic, the text, all no problem.  However, I have no idea how to properly overlay and tie all this stuff together!  I'm most familiar with AviSynth and Lightworks, and I've pored over the documentation for both the best I know how.  It seems like I'm just missing out on a structure for the basic work flow.  Can anyone help me out with an approach to making basic animations and movies when you have a large amount of static content to go into it?</p>
","<p>You will need so-called video composition software such as <a href=""http://www.adobe.com/products/aftereffects.html"" rel=""nofollow"">Adobe After Affects</a> or even <a href=""http://en.wikipedia.org/wiki/Blender_%28software%29"" rel=""nofollow"">Blender</a>.</p>

<p>A few years ago Combustion was available which was very good for things like this, but Autodesk came along and purchased it, sucked it dry of blood, and put it in a drawer never to be seen again.</p>

<p>AE comes with a price tag, while Blender is free (site <a href=""http://www.blender.org"" rel=""nofollow"">http://www.blender.org</a> appear to be down when I tested it now - see Wiki link above for information on it). Both have a pretty steep learning curve though if you never used composition software before. But there exist a lot of free learning material on the web to help you up and going.</p>

<p>Update: Adding some links to resources that hopefully can be of use:</p>

<p>After effects, layers, compositions - This will show how you can first create elements, then how to place them with different position and angle onto another video. The more relevant part starts from about 8:30 or so:<br />
<a href=""http://library.creativecow.net/articles/rabinowitz_aharon/Jumbotron/video-tutorial"" rel=""nofollow"">http://library.creativecow.net/articles/rabinowitz_aharon/Jumbotron/video-tutorial</a></p>

<p>After Effects introduction:<br /></p>

<p>This site gives you bucket loads of video tutorials on AE and composition effects, layers and so forth:  <br />
<a href=""http://library.creativecow.net/"" rel=""nofollow"">http://library.creativecow.net/</a></p>

<p>Video editing with Blender (part 1):<br />
<a href=""https://www.youtube.com/watch?v=1sODml0PBlo"" rel=""nofollow"">https://www.youtube.com/watch?v=1sODml0PBlo</a></p>

<p>Special video effects with Blender:<br />
<a href=""https://www.youtube.com/watch?v=x3ytZrGJ1ZQ"" rel=""nofollow"">https://www.youtube.com/watch?v=x3ytZrGJ1ZQ</a></p>

<p>Hope these will be of help!</p>
","5267"
"ffmpeg bgr vs rgb and other similiar pixel formats","772","","<p>I was wondering about the difference between <strong>rgb and bgr pixel formats</strong> available in many codecs.</p>

<p>It reminds me in some way the Big Endian and Little Endian flavours of computer processors.</p>

<p>While I always guessed that the big / little endian was more a matter of patents rather than performance, why we have both rgb and bgr?</p>

<ul>
<li>It's still a matter of patents?  </li>
<li>Have something to do with <a href=""https://en.wikipedia.org/wiki/BGR_%28subpixels%29"" rel=""nofollow"">Subpixels Rendering</a> ?  </li>
<li>Why some codecs has alternate support of them like this huffyuv example here:</li>
</ul>

<blockquote>
  <p>Encoder huffyuv [Huffyuv / HuffYUV]:<br>
  Threading capabilities: no<br>
  Supported pixel formats: yuv422p <strong>rgb24 bgra</strong></p>
</blockquote>

<p>It has <strong>rgb24</strong> but then not <strong>rgba</strong> as I could expect. It jumps directly to <strong>bgra</strong>!  </p>

<p>Could be again a matter of patents that the codec author could not break ?</p>

<hr>

<p>Please feed my curiosity with some extended explaination here if possible, i want to know something more about this various pixel formats!</p>
","<p>The order of the components in RGB32 seems to do with <a href=""https://www.ffmpeg.org/doxygen/0.5/pixfmt_8h.html"" rel=""nofollow"">endianness</a>:</p>

<blockquote>
  <p>PIX_FMT_RGB32 is handled in an endian-specific manner. An RGBA color
  is put together as: (A &lt;&lt; 24) | (R &lt;&lt; 16) | (G &lt;&lt; 8) | B This is
  stored as BGRA on little-endian CPU architectures and ARGB on
  big-endian CPUs.</p>
</blockquote>

<p>The descriptions of the various related formats enumerated on that page provide more details.</p>
","16826"
"How to handle extreme dynamic range in a vocalist?","772","","<p>Just finished a tracking session with a singer who has a <em>huge</em> dynamic range. This musician is otherwise very professional, and is a joy to have in the studio, and I'm not in any way criticizing her. However, there are some variables I'd like to know how to handle should they recur (with either her or another musician): </p>

<p>The was musician playing guitar while singing live in the studio. By the end of the session, we got the takes we needed, and their quality was good. She never needed more than a few takes, but we had to do a few retakes on top of that because of a little clipping. (But hey, six songs in three hours...not bad.) But this is making me wonder: </p>

<p>The extreme dynamic range makes using the pad setting problematic, coupled with a slightly noisy environment. (Read: lotsa planes and trucks today in my normally quiet neighborhood.) </p>

<p>In general, what can one do to cope with a situation like this? Should the tracking engineer ride the trim pot in a situation like this, or would that make the recording choppy? Suck it up and use automation and light compression on the audio in post? Run a second mic for the vocal?</p>

<p>Here's my rig. (Today was the first non-test session with a new computer and interface, after a recent rig meltdown.)</p>

<ul>
<li>Macbook Pro with Garageband, wille export to Logic for mixdown</li>
<li><a href=""http://www.motu.com/products/motuaudio/audio-express"">MOTU Audio Express</a> interface</li>
<li>SM58 on her vocal and a Studio Projects C1 on her guitar, with a bit of bleed between the two (as anticipated)</li>
</ul>
","<p>What Kim Burgaard said is very true. The vocalist has a lot of control over how their voice is recorded. As I'm a much inferior vocalist to those I record, it can be difficult teaching them this technique, as you just want to get a good take.</p>

<p>So what I end up doing is have the vocalist scream. Well, I ask them to warm up and hit the loudest levels they can. I then adjust via that. I try not affect overall sound with compression in the analog domain. With the takes in 24bit, I can use envelopes or a compressor in the mixing stage.</p>

<p>I would stay away from ridding the level during recording. It's more difficult to get the levels right in the mixing stage.</p>

<p>You mention running a second mic for the vocal. I would hope that is what you are doing anyway. I don't think recording the mic and guitar with one mic is a good idea at all. You'll not be able to mix anything.</p>
","1327"
"Quality of VHS converted to DVD","772","","<p>I work for a small library. We have a collection of VHS tapes that we have recorded to DVD using a Magnavox VHS HQ DVD recorder. <del>They came in as .ogg</del>, They came in a a set of files including some .VOB which I can play by changing the file extension to .mp4. How much quality have I lost in this conversion? Is there a way to convert VHS to digital with higher fidelity?</p>

<p>The collection has historical value, so there is some debate on whether we need to save the original VHS tapes as opposed to just keeping the videos in electronic format (on a server that is backed up nightly). </p>

<p><strong>Edit:</strong>
A bit of new information:</p>

<p>I used <a href=""http://www.headbands.com/gspot/"" rel=""nofollow"">GSpot</a> to determine the codec (I don't have admin/install rights on my computer, so that is the only program I could use), and it says mpeg2. Does any of this information help with the original question?</p>

<p><strong>Edit2:</strong>
I'm such a dumb dumb. I had all these people trying to figure out why my files were .OGG. They weren't. They were .VOB. What was on the DVDs is exactly what Perter Cordes described below. Now that I've sent people on a wild goose chase...</p>
","<p>The best way to conserve the VHS's would be to scan each frame in at the highest quality possible and export to a video file, this would be a very long process and would require some expensive kit or a professional service. If they are historical value and you want the best it would be worth spending the money and getting it done by a pro but that cost is your choice in the end but it can be expensive as I looked into it myself.</p>

<p>VHS quality is the best it would ever be without spending huge amounts of time and money as above. what your getting on your DVD is going to be as the content going in.</p>

<p>I have done this myself a few times from VHS and no matter what settings I chose and no matter how big the bit rate, file size, up-scaling options etc the quality was always the same, the phrase ""rubbish in, rubbish out"" comes to mind.</p>
","15281"
"synchronizing video and audio in a remuxed MKV","769","","<p>I am quite new in the area of audio/video manipulation so apologies if the question is obvious. I did not manage to find an answer online, though.</p>

<p>I have a MKV file with several tracks: video (MPEG4), audio (DTS EN), audio (AC3 FR) and subtitles. I wanted to extract the video and French audio, then merge them back into an MKV file.
I did this via mkvextract and mkvmerge (without any specific options). The MKV file I got is playable but the audio is shifted (by 10 or so seconds). The audio in the original file is fine for both languages (ie. it is correctly synchronized with the picture).</p>

<p>I guess that there should be a way to align the video and audio, though I do not really understand why they are shifted since I did not do any manipulations on the extracted files.</p>

<p>I would be grateful for any hints.
Thank you </p>
","<p>Use mkvmerge with
  <strong>-y</strong> or <strong>--sync </strong></p>

<pre><code>                       Synchronize, adjust the track's timecodes with

                       the id TID by 'd' ms.

                       'o/p': Adjust the timecodes by multiplying with

                       'o/p' to fix linear drifts. 'p' defaults to

                       1 if omitted. Both 'o' and 'p' can be

                       floating point numbers.
</code></pre>
","3197"
"Change codec so seek bar works in Windows Media Player","766","","<p>The library I work at has been converting VHS tapes to DVDs. They used a <a href=""http://www.magnavox.com/product/product.php?id=118"" rel=""nofollow"">Magnavox ZV427MG9</a>. The DVDs play in Windows Media Player, but the seek bar doesn't work. It works fine in VLC Player. It is essential that they play in WMP, because that is the supported institutional standard here. </p>

<p>What would it take to re-encode the DVDs so that the seek bar works? Can it be done in a lossless way, or would the VHS tapes need to be transferred again using a different method?</p>
","<p>The problem is probably not WMP but the Microsoft MPEG-1 codec. The straight forward solution other than using a decent player is to use a better codec pack - <a href=""http://ffdshow-tryout.sourceforge.net/"" rel=""nofollow"">ffdshow</a>, it feeds WMP through the operating system level. Besides the seek issue you'll probably get better playback performance, and it doesn't require any end-user involvement - they continue to use WMP as usual.</p>

<p>If touching the end-user's machine is entirely out of the question then you'll have to transcode the video to something else. You'll probably get the best results (file size, quality, compatibility) by transcoding to H.264/AAC (mp4), although the encoding process is heavier/longer than alternatives such as MPEG2. You'll need for that <a href=""http://ffmpeg.org/"" rel=""nofollow"">ffmpeg</a> and start with:</p>

<pre><code>ffmpeg -i mpeg1filename outputfilename.mp4
</code></pre>

<p>There are <em>tons</em> of options that will affect the transcoding quality and speed, and much of your success will depend on trial and error with your specific footage. All that said, your first choice should be to install ffdshow on the end-user machine.</p>
","15632"
"Can FFmpeg output time duration since the start of a job?","765","","<p>Is there a way for <code>FFmpeg</code> to output or log time duration since a particular job/process was started?</p>

<p>I know that there are more or less complicated ways to display a <a href=""https://stackoverflow.com/questions/747982/can-ffmpeg-show-a-progress-bar"">progress bar</a> which really seems more like a programmatical problem.</p>

<p>The reasoning is that I want to measure how long time my encoding jobs take (without having to set up a separate timer).</p>
","<p>In Linux you can use the <code>time</code> command:</p>

<pre><code>$ time ffmpeg -i input.avi output.mp4
  [...]
  real  0m1.927s
  user  0m7.067s
  sys   0m0.243s
</code></pre>

<p>This example took 1.927 seconds to complete.</p>
","17233"
"How do I resize a image preserving aspect ratio in After Effects?","765","","<p>In my compossition I imported an image, I would like to scale the image in real-time (I mean using the mouse to select the image, center it in the composition and resize it to see the results in real-time because for me is more productive to do it in this way, instead of manually specifying width and height wherever) preserving the aspect ratio of the image.</p>

<p>In <strong>Photoshop</strong>, this can be done by clicking in a corner of the image element then holding the <strong>CTRL</strong> key while using the mouse to resize the image, so the image is scaled preserving aspect ratio, however, I can't reproduce this behavior in <strong>After Effects</strong>, if I do click on a corner of the image, nothing happens.</p>

<p>If this is not possible to do in the same productive and easy way than <strong>Photoshop</strong>, then how I could do it (in the less tedious way)?.</p>
","<p>Click on a layer name. You should see the control points on its border. Click on one of the border points but don't drag it yet. Hold down Shift key and then drag it. Release the mouse button when done and then release the Shift key.</p>
","20220"
"Simple way to downscale from 1080p@60 to 1080i@60","765","","<p>I have a <a href=""http://www.canon.co.uk/For_Home/Product_Finder/Camcorders/High_Definition_HD/LEGRIA_HF_R56/index.aspx"" rel=""nofollow"">Canon LEGRIA HF R56</a> which only outputs <code>1080p@60</code> from its HDMI port. Are there any methods to downscale this to <code>1080p@30</code> or lower?</p>

<p>Googling this doesn't seem to throw up light. I guess I could use a HDMI capture device to downscale the signal, but then that would defeat the object of downscaling it, as I could just use that device to perform my capture.</p>

<p>I am doing this because my capture device, an <a href=""http://www.blackmagicdesign.com/products/intensity/"" rel=""nofollow"">Intensity Shuttle</a>, has a maximum input rate of <code>1080p@30</code>.</p>

<p>EDIT:</p>

<p>From @ProfessorSparkles answer below, the HDMI output could be 1080p@50. I realize that my TV simply says 1080p as the input signal, rather than the framerate.</p>
","<p>Speaking to the Canon technical team, it seems that at the time of this writing, there is no way on this particular model of camera to change the resolution of the HDMI output.</p>
","12979"
"How to live stream 4 cameras to a flat screen TV","760","","<p>I wanted to live stream different views from 4 cameras at the same time on a flat screen TV or monitor. What materials would I need for such a project. I have seen home security surveillance setups in which 4 cameras are streamed live to a monitor, however two things that are very important to this project are:</p>

<p>1.) live stream quality</p>

<p>2.) monitor size (need ~50inch display)</p>

<p>3.) placement of stream views on display</p>

<p>If there are surveillance packages that accomplish these goals (especially flexible positioning of the 4 live stream views), feel free to point me to them. Also, I am new to this website so If this question is not a good fit for this thread, then where would it be a good fit? Thanks in advance.</p>
","<p>You need something like the Blackmagic Multiview: <a href=""https://www.blackmagicdesign.com/products/multiview"" rel=""nofollow"">https://www.blackmagicdesign.com/products/multiview</a> or Matrox Quad-Split <a href=""http://www.matrox.com/video/en/products/microquad/"" rel=""nofollow"">http://www.matrox.com/video/en/products/microquad/</a> or Decimator MD-QUAD <a href=""http://decimator.com/Products/MultiViewers/MD-QUAD%20Quad-Split%20MultiViewer/MD_Quad.html"" rel=""nofollow"">http://decimator.com/Products/MultiViewers/MD-QUAD%20Quad-Split%20MultiViewer/MD_Quad.html</a></p>

<p>These units take HD-SDI input and can then display it on a HDMI (or SDI) display however you like.</p>

<p>So grab as many cameras as you need - you can decide what quality camera suits your needs. Get cameras with with SDI output for long cable runs. If they don't have SDI, you can get HDMI to SDI converters. If an SDI cable isn't long enough, you can get SDI/HDMI to Fibre converters that let you run fibre optic cable between your cameras and a multiview. </p>

<p>Any half decent multiview will let you use a computer to re-arrange the placement of the images on the screen.</p>

<p>Good luck!</p>
","15975"
"What determines video orientation (landscape or portrait) during playback?","759","","<p>I captured a short video on my Android phone in portrait mode (i.e. to be viewed vertically).  When I played it on Windows 7 using WMP (or VLC), it displayed incorrectly in landscape orientation.</p>

<p>Thinking that I had to rotate it, I transferred the video to a Windows 10 laptop to process in After Effects.  To my surprise, I found out that it plays in the correct portrait mode on Windows 10 (using whatever is the default player that comes with Windows 10), and also plays correctly when imported into After Effects, requiring no rotation.</p>

<p>What determines how a video is oriented during playback?</p>
","<p>If the video is meant to be played in a different orientation than its stored representation, then a rotation flag is set in the stream metadata. A compliant player uses that tag and rotates the video during playback.</p>

<p>A tool like <a href=""https://mediaarea.net/en/MediaInfo"" rel=""noreferrer"">Mediainfo</a> will display that data (if specified) like here:</p>

<p><a href=""https://i.stack.imgur.com/vJDI8.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vJDI8.jpg"" alt=""Mediainfo rotate tag""></a></p>

<p>A tool like ffmpeg can reset the tag. Of course, you'll also want to rotate the video stream as currently stored so it plays in the correct orientation after the tag has been reset. ffmpeg's transpose filter can do that.</p>
","16902"
"Do Premiere, After Effects and Adobe Media Encoder support huffyuv lossless codec?","758","","<p>Can the lossless <a href=""http://wiki.multimedia.cx/index.php?title=HuffYUV"" rel=""nofollow"">HuffYUV</a> codec be incorporated into the workflow of a VFX pipeline as an intermediate codec, given ample hard disk space?</p>

<p>I read that since Premiere and After Effects are now 64-bit and commonly HuffYUV is just 32-bit, that might pose problems. However, a x64 version is now <a href=""http://www.digitalfaq.com/go/tool-huffyuv/"" rel=""nofollow"">available</a>.</p>

<p>I would like to get opinions on a HuffYUV workflow, possible drawbacks and a complete installation guide with suggested settings to use, thanks!</p>
","<p>They do on Windows, you need to use the AVI container as HuffyUV isn't available as a quicktime codec, afaik.</p>

<p>I used huffyUV for a while, but have switched to <a href=""http://www.magicyuv.com/index.php"" rel=""nofollow"">magicYUV</a> which has a better range of colour space support (including 10-bit in beta).  <a href=""http://www.videohelp.com/software/Ut-Video-Codec-Suite"" rel=""nofollow"">UTVideo</a> is another good option and is available for mac.</p>

<p>With the Premiere - After Effects - Media Encoder workflow I find that exporting to a flattened movie only happens when I am making the final master, I use Dynamic Link for the intermediate transfers from say AE to Premiere. As well as saving disk space it's faster and easier because you can make changes and have them update on the fly.</p>

<p>So for final masters these lossless codecs are great, with the one caveatthey may not be supported in the long term. Mostly they are developed by small teams or even a single person, so if they get a full-time job or have twins the project might tank. If you're making masters that need to be available for the long term  as in decades, then FFV1 is a good option, though for everyday use as an intermediate codec it's a bit slow.</p>
","16771"
"Adobe After Effects vs. Final Cut Express for chroma keying","756","","<p>Which is better for chroma keying, Adobe After Effects or Final Cut Express and why?</p>
","<p>I haven't tried chroma keying in Final Cut Express, but I'm pretty sure it depends on your needs and workflow. If you use Premiere for editing movies than AE should be more integrated. </p>

<p>Yet, if you really want to get awesome results and have some spare bucks, I'd say you look at some plugins. After all neither AE nor FCE is specialized in chroma keying. There are plenty of companies out there that build specialized plugins that might give you better results, and they usually offer them for all video solutions (AE, Premiere, FC, etc)</p>
","2185"
"Split video screen into 3 equal parts","754","","<p>I'm working on a project that will be presented on 3 screens. The idea is that each screen in showing a part of the screening video, which means the upper display shows the upper 1/3 of the video screen, the middle display shows the middle part of the video, and the lower display the lower part of the video.</p>

<p>So my question is how can I cut the video screen in 3 equal parts, and with which program?</p>

<p>Thanks for each reply!</p>
","<p>Edit your footage in one timeline, with the sequence setting set to the combined dimensions of all the screens (so the width of one screen across  three times the height). So if they're 1080p screens you'll be looking at 1920  3240.</p>

<p>Now make three sequences, each with the resolution of one playback screen, e.g 1920  1080. Take your original sequence and lay it into these timelines (as a nested sequence).</p>

<p>Adjust the motion properties of the clip in the first sequence so that the top of the clip lines up with the top of the frame, then in the next sequence align the middle of the clip with the middle of the frame and in the last sequence align the bottom of the clip with the bottom of the frame.</p>

<p>Now you have three sequences that align as you wish. The beauty of nesting the combined sequence is that you can make changes without having to synchronise three separate sequences.</p>
","16181"
"Is there a low-cost automated pan/tilt/zoom system?","752","","<p>Our church wants to install a video camera to record services and sermons, nothing fancy - here's what I'm thinking:</p>

<ol>
<li>Mount a remote-control pan/tilt head on the wall or railing of the balcony</li>
<li>During the service, attach a video camera that has remote-control zoom/start/stop control</li>
<li>Have software that lets the operator hit buttons that say ""pan/tilt/zoom so the camera is pointing at the pulpit"" or ""pan/tilt/zoom so the camera shows the entire altar area"".</li>
</ol>

<p>The challenge, of course, is that we don't have $5000 to spend on one of the fancy commercial systems (nor do we need something that sophisticated).</p>

<p>We have a Canon VIXIA HF-R52 that you can control remotely (zoom/start/stop), and I'm considering the Panasonic CTR-1 Remote Pan Tilt Cradle, what I can't tell is whether the controls for either let you say ""go to zoom level 8.3, pan 20 degrees from center and 30 degrees down from horizontal"" (which is what I would need, in order to give the operator a button for ""pan/tilt/zoom to the altar"".</p>

<p>BTW, I don't want the system that follows motion, or requires everyone to carry a special tag that the PT head detects and follows.</p>

<p>Is there a low-cost solution that gives me what I need?</p>

<p>I saw this question: <a href=""https://video.stackexchange.com/questions/9835/remote-pan-tilt-zoom-cameras-that-arent-expensive"">Remote pan/tilt/zoom cameras that aren't expensive</a> which is similar, but didn't go far enough.  As far as I can tell, the low-end pan/tilt heads let you say ""pan to the left"" but they don't let you say ""pan to 30 degrees left"" which is what I am looking for.</p>
","<p>What you look for is probably a combination of the Bescor / Hague MP360 motorized pan tilt head with the CamRanger controller. CamRanger are selling the combo under their label, you can see some nice video explanations at <a href=""http://camranger.com/camranger-pan-tilt/"" rel=""nofollow"">their web site</a>. If you don't need the extra pizzaz then you can look at just the MP360, or even the lower-end <a href=""http://www.bhphotovideo.com/c/product/64399-REG/Bescor_MP101_MP_101_Motorized_Pan_Head.html"" rel=""nofollow"">MP101</a> which is a great product too.</p>

<p>Note that these heads have a tilt limit of 30deg, so, depending on your setting, you may have to throw in a correction plate.</p>
","15614"
"Freezing/repeating the last frame of a video clip in Apple Motion 5","742","","<p>I have two clips running at the same time. One is a computer and the other is a clip that I've made to look like its running on the computer screen. I've used four corner motion tracker to keep the two in sync as the computer clip zooms in.</p>

<p>At a certain point i'd like to freeze the computer clip while the clip on its screen plays out. I can crop the computer clip up to the point I want it frozen but I'm unsure of how to extend/repeat that last frame so it stays visible until the other clip finishes. At the moment it just dissapears when it hits the crop.</p>
","<p>Okay, figured out how to do this. Make sure you location is at the frame you want to freeze, hide all the other layers in Motion to just show the video that you want to freeze.</p>

<p>Then go Share menu > Save Current Frame. Then save that image somewhere.</p>

<p>All you have to do then is import that image back in and place it again at the end of the clip it came from. Ensure it's lower in the layer stack so that other clips appear on top of it until they disappear - or just crop it.</p>
","8389"
"How do I record video for more than 30 minutes at a time with a Canon A1400 PowerShot?","742","","<p>I'm planning to send a Canon A1400 PowerShot to the edge of space with a weather balloon (we will figure out how to keep the temperature in our payload box suitable, so that is not a pertinent issue to the question). Unfortunately I believe it can only record 30 mins/4GB of 720p at a time. Our flight's duration is 3 hours and we want to capture all of that.</p>

<p>Can I program the A1400 with CHDK to recursively capture video till the memory card fills up? That is, start recording another video (almost) right after the 30 mins/4GB design limit is reached. If so, I would appreciate a script for the same!</p>
","<p>Yes, CHDK should be able to do this.  There are actually scripts available on their <a href=""http://chdk.wikia.com/wiki/Continuous_Video_Scripts"" rel=""nofollow"">wiki</a> for this.  Basically, you have to tell it to press the video start button, wait a second or two for it to start, then begin polling to see if the get_movie_status has changed to the status for being done recording.  You then wait a second and repeat.</p>

<p>You'll also want to make sure you remove the 30 minute time limit with CHDK itself if available on your camera (under the video menu).  If it is, then you should only be limited by the 4gb file size limit and the script should restart the video.</p>
","12847"
"Power for lav mic plugged into a Zoom H5?","740","","<p>I have a handheld Zoom H5 recorder and just ordered a lav mic (<a href=""http://www.vt-switzerland.com/en/vt401-omni-lavalier-microphone"" rel=""nofollow"">Voice Technologies VT4001</a>).</p>

<p>The <a href=""http://www.zoom.co.jp/products/h5/spec/"" rel=""nofollow"">Zoom's spec sheet</a> lists its inputs as 2 ""XLR/TRS combo jacks"" capable of providing phantom power at +12V, +24V and +48V.</p>

<p>I will be using the lav wired.</p>

<p>Question: which is the simplest way to connect and power this mic from this recorder? Do I need some sort of adapter or will a simple mic jack &lt;=> TRS cable do?</p>
","<p>You will need an electret power supply that accepts phantom power and then will hook it up via a standard XLR cable to the H5.  If you have the bare wire version, you will also need a connector to put on the bare wire so that it can tie in to the electret power supply.  You will also need to make sure that +48v phantom power is turned on on the H5.  (Alternately you could use a battery powered electret power supply, but I'd recommend sticking to using phantom since it is available.)</p>
","14633"
"Are the properties (bitrate, fps) of my h.264 video OK for the web?","737","","<p>I'm converting a Flash vector animation to an mp4 video for the web. After trying and failing with <a href=""https://www.google.com/doubleclick/studio/swiffy/"" rel=""nofollow"">Swiffy</a>, it's the only way I have of preserving the work ""for posterity"", once Flash is gone.</p>

<p>When exporting a FLA to MOV, the frame rate changes. For example, an 18 fps FLA comes out as 9.6 fps in the MOV (according to the QT player properties).</p>

<p>Because the animation is jittery, I used Handbrake to export the MOVs as h.264, with framerates of 24, 30 and 60. They look smoother. (I was surprised the time-length of the video doesn't change, I thought at higher framerates it'd speed up). </p>

<p>Anyway, an mp4 produced like this has the following properties (according to the QT player):</p>

<p>Format: H.264, 550 x 250, millions, AAC, Mono, 48 kHz</p>

<ul>
<li>FPS: 24.00</li>
<li>File size: 7.41 MB</li>
<li>Data rate: 202.25 kbit/sec</li>
<li>Length: 00:05:07.09</li>
</ul>

<p>My newbie question is: can anyway tell me, at a glance, whether those properties are within the normal range for a video that is meant to be served from a dedicated web server or CDN? </p>
","<p>Sure - it seems ok. However, it is also a good idea to offer other types of video for browsers that do not support mp4 / h264. The usual best practice involves supplying mp4, webm and ogg as containers. <a href=""http://www.html5rocks.com/en/tutorials/video/basics/"" rel=""nofollow"">This site is a good reference.</a></p>

<p>A back of the napkin calculation:</p>

<p>1000 viewers / month @ 7.4 MB = 7.4 GB / Month.</p>

<p>Which is probably within the acceptable range for a generic server, just make sure the traffic allowed by your server provider can deal with it...</p>

<p>Assuming your server has a gigabit uplink (which is generally a bit more expensive), you could ""theoretically"" serve this video to 5000 users concurrently, however this won't work with normal apache servers - and it is probably a good idea to host the media files at amazon AWS (or whatever) if you are expecting that kind of traffic.</p>
","12509"
"How to apply an offset to one axis only in 3D space for linked expression position?","735","","<p>I have a 3D position property linked with another composition. I want the z-axis to have an offset in the linked expression. But how can I apply an offset just to one axis?</p>

<p>The current expression of the 3D position:</p>

<pre><code>comp(""Main COMP"").layer(""text"").transform.position
</code></pre>
","<p>The position property is an array, which is a container with a number of components in it, in this case the three position values: x, y and z. To specify an array you use this notation <code>foo = [A,B,C]</code> where A, B and C are the individual values  constants, variables, whatevs. <strong>TL;DR</strong> to create an array you put a number of comma-separated values in square brackets.</p>

<p>Conversely you can access any of the components of an array using this notation: <code>foo[n]</code> where foo is the name of the variable, and <code>n</code> is the <em>index</em> of the component you want. <strong>Note that the index starts at 0</strong>, so to get the first component of the array you would use <code>foo[0]</code>, the second <code>foo[1]</code> and so on. Arrays can also be multi-dimensional, so the address could be <code>foo[0][5][99]</code> for a three dimensional array. But you probably don't need to worry about that at this point.</p>

<p>In your case you want just the third component to have the offset. So you need to unpick the original array using the <code>foo[0]</code>, <code>foo[1]</code> notation and then put it back together as an array using the <code>[A,B,C]</code> notation. Thus:</p>

<pre><code>thePos = comp(""Main COMP"").layer(""text"").transform.position;
[thePos[0], thePos[1], thePos[2]+offset]
</code></pre>

<p>where <code>offset</code> is whatever your offset is, obviously.</p>

<p>The first line is just for convenience - it assigns the position property as the value of the variable myPos. You could do it all on one line, but it gets a bit hard to read:</p>

<pre><code>[comp(""Main COMP"").layer(""text"").transform.position[0], comp(""Main COMP"").layer(""text"").transform.position[1], comp(""Main COMP"").layer(""text"").transform.position[2]+offset]
</code></pre>
","13437"
"Interlaced Video Format 1080i (59.94)","735","","<p>I have a video project and the format is for Broadcast TV and needs to be interlaced NOT progressive video.</p>

<p>High Definition File</p>

<ul>
<li>All High Definition content MUST be <strong>1080i (59.94) format</strong></li>
<li>All High Definition content should be 4:3 center-cut safe.</li>
<li>Video Format: <strong>1080i, 29.9 frames/sec (59.94 fields/sec)</strong> HD File
Format(s)</li>
<li>File Format: XDCAM HD422: 4:2:2, at 50 Mbps MXF (OP1A, self
contained)</li>
<li>Video Codec: MPEG-2 Long GOP, Sony XDCAM compliant</li>
<li>Field Order: Upper Field first</li>
<li>Audio Format: 48 KHz sample rate, uncompressed</li>
<li>Operating Level: Peak program levels at -12 to -8dBfs, nominal
-10dBfs</li>
</ul>

<p>I have never received / edited OR built / rendered out a 1080i (59.94) video before, always digital / progressive(.mp4 ). </p>

<p>I plan on using Adobe Premiere for any editing of the uncompressed footage and then for motion graphics / animation using After Effects. </p>

<p>My question is, how can I keep the format consistent, interlaced fields, across different applications and also what output format should be used for the final video / deliverables.</p>

<p>Can I apply my editing and motion graphics like normal with progressive, but @ 60 frames/sec, then at the very end export a rendered video that is 1080i?</p>
","<p>You're not going to be able to replicate the precise ""look"" of 59.94, even with special plugins. This, due to the fact that interlaced is in many ways like:</p>

<p>Double Frame Rate - Half Resolution. </p>

<p>Think of 59.94i at 1080 really as 119.88 at 540p. </p>

<p>Interlaced lines are scanned odd lines first, then even. So you technically have a first set of lines which are actually one frame, with the other set of lines missing. These lines are then displayed back to the viewer odd/even//odd/even//odd/even etc. </p>

<p>That said, you get smoother motion with i vs p. You are able to achieve better slow motion in post production with i vs p, however, p, is of course the preferred format to most. A single frame being a single frame. </p>

<p><em>NOW THAT SAID...</em></p>

<p><strong>Looks aside,</strong> in order to output your video from PPro, all you will simply want to do is import the footage material in (original); right mouse click; new sequence from clip; make any edits/trims, etc. </p>

<p>It is really unnecessary to make any additional adjustments from the timeline in Premiere, because what you ultimately want to do is use <strong>Frame Blending</strong> when you export.</p>

<p>After you've made any adjustments, go to export, choose the format you need; 59.94i, whatever codecs they are requiring, many are already built in; others may need to be downloaded. </p>

<p>Ensure you turn on Frame Blending, IF your source material is progressive. </p>

<p>Frame Blending will force interpolation of the frames during the rendering of your new video outputs, using two side by side discreet progressive frames to create a new ""quasi"" interlaced frames, twice as many frames as your progressive source at the correct frame rate. </p>

<p>It isn't quite the same ""look"" wise as actual Interlaced source material; however, specification wise it should meet their needs; and to the average viewer most will not notice. </p>

<p>Do not however use Frame Blending if you are using Interlaced material. And do not double the effect by applying it to the clip in the timeline (which you can do by right clicking the clip; Under Frame Blending/Field Options); and then stack it with another Blend on export. Only one is necessary, and my recommendation is simply allow Adobe Media Encoder handle the p to i conversion. </p>

<p>Lastly, if you have material at lower resolution, AME will be upscaling it to get to 1080. If so, you can use the Use Maximum Render Quality Checkbox at the bottom of the export panel window. </p>

<p>Be advised this will likely increase your rendering time by multiple fold. </p>

<p>In regards to the audio; they have peaking requirements. You will need to apply a Dynamics and/or Limiter filter in the Audio Mixer Window Filter Dropdown. Limit to the specifications they have stated. </p>

<p>In regards to center safe, if you're given material that isn't center safe, you're going to need to talk with them about that, as you'll have to do a push in and do manual pan and scan using the motion filter, adjusting the frame position on the canvas to get it to 4:3 safe. </p>
","19419"
"Using h264 in loseless mode brings small unexpected results","735","","<p>I got curious on ffmpeg screen capture capabilities and started messing around with a simple realtime capture test in h264.</p>

<pre><code>ffmpeg -f dshow -i video=""screen-capture-recorder"" -video_size 1920x1080 -framerate 30 -c:v libx264 -crf 0 -preset ultrafast capture.mkv
</code></pre>

<p>Based on what is said on ffmpeg h264 documentation with the <code>-qp 0</code> or <code>-crf 0</code> options libx264 should work in lossless mode.</p>

<blockquote>
  <p><em>You can use -qp 0 or -crf 0 to encode a lossless output. Use of -qp is recommended over -crf for lossless because 8-bit and 10-bit x264 use different -crf values for lossless.</em></p>
</blockquote>

<p>This is also repeated in the realtime capture section help when talking about an optional re-encoding with a slower preset to try saving some size:</p>

<blockquote>
  <p><em>Note that since the initial recording was lossless, and the re-encode is lossless too, no quality loss is introduced in this process in any way.</em></p>
</blockquote>

<p>Based on this I trusted the guide and assumed using -qp 0 I would achieve a fully lossless workflow ;)</p>

<p>However I found it introduce some loss in particular situations.</p>

<p>So I made another test with the huffyuv codec with this code:</p>

<pre><code>ffmpeg -f dshow -i video=""screen-capture-recorder"" -video_size 1920x1080 -framerate 30 -c:v huffyuv capture.mkv
</code></pre>

<p>the results:</p>

<p><a href=""https://i.stack.imgur.com/CqIw5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CqIw5.png"" alt=""Screen Capture encoded in h264""></a> <a href=""https://i.stack.imgur.com/6Bn3e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Bn3e.png"" alt=""Screen Capture encoded in huffyuv""></a></p>

<p>Screen 1: <strong>h264 in lossless mode</strong><br>
Screen 2: <strong>huffyuv</strong></p>

<p>Based on screens <code>huffyuv</code> is perfect, a true lossless codec while <code>h264</code> instead compress something here and I'm unable to understand why if it should be setup in a lossless mode.</p>

<p>(huffyuv is identical to a bitmap screenshot of the desktop, I would achieve the same with h264)</p>

<p>Can someone help me figuring it out?</p>

<hr>

<p>Edit: Adding some ffmpeg dumps as required in comments ;)</p>

<p>h264 run:</p>

<pre><code>ffmpeg -f dshow -i video=""screen-capture-recorder"" -video_size 1920x1080 -framerate 30 -c:v libx264 -qp 0 -preset ultrafast capture.mkv  
ffmpeg version N-73411-g5233f25 Copyright (c) 2000-2015 the FFmpeg developers  
  built with gcc 4.9.2 (GCC)  
  configuration: --arch=x86_64 --target-os=mingw32 --cross-prefix=/home/user/san  
dbox/mingw-w64-x86_64/bin/x86_64-w64-mingw32- --pkg-config=pkg-config --enable-g  
pl --enable-libsoxr --enable-fontconfig --enable-libass --enable-libutvideo --en  
able-libbluray --enable-iconv --enable-libtwolame --extra-cflags=-DLIBTWOLAME_ST  
ATIC --enable-libzvbi --enable-libcaca --enable-libmodplug --extra-libs=-lstdc++  
 --extra-libs=-lpng --enable-libvidstab --enable-libx265 --enable-decklink --ext  
ra-libs=-loleaut32 --enable-libx264 --enable-libxvid --enable-libmp3lame --enabl  
e-version3 --enable-zlib --enable-librtmp --enable-libvorbis --enable-libtheora  
--enable-libspeex --enable-libopenjpeg --enable-gnutls --enable-libgsm --enable-  
libfreetype --enable-libopus --disable-w32threads --enable-frei0r --enable-filte  
r=frei0r --enable-libvo-aacenc --enable-bzlib --enable-libxavs --enable-libopenc  
ore-amrnb --enable-libopencore-amrwb --enable-libvo-amrwbenc --enable-libschroed  
inger --enable-libvpx --enable-libilbc --enable-libwavpack --enable-libwebp --en  
able-libgme --enable-dxva2 --enable-libdcadec --enable-avisynth --enable-static  
--disable-shared --extra-cflags= --prefix=/home/user/sandbox/mingw-w64-x86_64/x8  
6_64-w64-mingw32 --extra-libs=-lpsapi --enable-nonfree --enable-libfdk-aac --dis  
able-libfaac --enable-nvenc --enable-runtime-cpudetect  
  libavutil      54. 28.100 / 54. 28.100  
  libavcodec     56. 46.101 / 56. 46.101  
  libavformat    56. 40.100 / 56. 40.100  
  libavdevice    56.  4.100 / 56.  4.100  
  libavfilter     5. 20.100 /  5. 20.100  
  libswscale      3.  1.101 /  3.  1.101  
  libswresample   1.  2.100 /  1.  2.100  
  libpostproc    53.  3.100 / 53.  3.100  
leaving aero onInput #0, dshow, from 'video=screen-capture-recorder':  
  Duration: N/A, start: 362931.503000, bitrate: N/A  
    Stream #0:0: Video: rawvideo, bgr0, 1920x1080, 30 tbr, 10000k tbn, 30 tbc  
No pixel format specified, yuv444p for H.264 encoding chosen.  
Use -pix_fmt yuv420p for compatibility with outdated media players.  
[libx264 @ 00000000004c7e00] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2  
AVX FMA3 AVX2 LZCNT BMI2  
[libx264 @ 00000000004c7e00] profile High 4:4:4 Predictive, level 4.0, 4:4:4 8-b  
it  
[libx264 @ 00000000004c7e00] 264 - core 144 r2533 c8a773e - H.264/MPEG-4 AVC cod  
ec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=0 r  
ef=1 deblock=0:0:0 analyse=0:0 me=dia subme=0 psy=0 mixed_ref=0 me_range=16 chro  
ma_me=1 trellis=0 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=0 chroma_qp_offset=0  
threads=6 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 blur  
ay_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=25 sce  
necut=0 intra_refresh=0 rc=cqp mbtree=0 qp=0  
Output #0, matroska, to 'capture.mkv':  
  Metadata:  
    encoder         : Lavf56.40.100  
    Stream #0:0: Video: h264 (libx264) (H264 / 0x34363248), yuv444p, 1920x1080,  
q=-1--1, 30 fps, 1k tbn, 30 tbc  
    Metadata:  
      encoder         : Lavc56.46.101 libx264  
Stream mapping:  
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; h264 (libx264))  
Press [q] to stop, [?] for help  
frame=   11 fps=0.0 q=0.0 size=    2421kB time=00:00:00.20 bitrate=99163.6kbits/  
frame=   22 fps= 22 q=0.0 size=    2538kB time=00:00:00.73 bitrate=28368.6kbits/  
frame=   33 fps= 22 q=0.0 size=    2647kB time=00:00:01.26 bitrate=17116.4kbits/  
frame=   46 fps= 23 q=0.0 size=    2770kB time=00:00:01.80 bitrate=12608.4kbits/  
frame=   58 fps= 23 q=0.0 size=    2842kB time=00:00:02.23 bitrate=10427.1kbits/  
frame=   71 fps= 23 q=0.0 size=    2908kB time=00:00:02.80 bitrate=8508.6kbits/s  
frame=   83 fps= 23 q=0.0 size=    2977kB time=00:00:03.26 bitrate=7467.0kbits/s  
frame=   96 fps= 24 q=0.0 size=    3085kB time=00:00:03.80 bitrate=6649.8kbits/s  
frame=  108 fps= 24 q=0.0 size=    3195kB time=00:00:04.30 bitrate=6084.5kbits/s  
frame=  120 fps= 24 q=0.0 size=    3309kB time=00:00:04.80 bitrate=5645.8kbits/s  
frame=  133 fps= 24 q=0.0 size=    3398kB time=00:00:05.33 bitrate=5219.0kbits/s  
frame=  147 fps= 24 q=0.0 size=    3492kB time=00:00:05.86 bitrate=4876.1kbits/s  
frame=  160 fps= 24 q=0.0 size=    3568kB time=00:00:06.36 bitrate=4591.4kbits/s  
frame=  173 fps= 24 q=0.0 size=    3660kB time=00:00:06.86 bitrate=4366.2kbits/s  
frame=  186 fps= 25 q=0.0 size=    3720kB time=00:00:07.36 bitrate=4136.5kbits/s  
frame=  187 fps= 24 q=-1.0 Lsize=    3737kB time=00:00:07.63 bitrate=4010.9kbits  
/s  
video:3735kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing  
overhead: 0.052628%  
[libx264 @ 00000000004c7e00] frame I:1     Avg QP: 0.00  size:1345491  
[libx264 @ 00000000004c7e00] frame P:186   Avg QP: 0.00  size: 13327  
[libx264 @ 00000000004c7e00] mb I  I16..4: 100.0%  0.0%  0.0%  
[libx264 @ 00000000004c7e00] mb P  I16..4: 16.5%  0.0%  0.0%  P16..4:  0.3%  0.0  
%  0.0%  0.0%  0.0%    skip:83.3%  
[libx264 @ 00000000004c7e00] coded y,u,v intra: 6.1% 3.8% 3.9% inter: 0.2% 0.1%  
0.1%  
[libx264 @ 00000000004c7e00] i16 v,h,dc,p: 96%  4%  0%  0%  
[libx264 @ 00000000004c7e00] kb/s:4008.11  
[dshow @ 00000000004bf760] real-time buffer [screen-capture-recorder] [video inp  
ut] too full or near too full (545% of size: 3041280 [rtbufsize parameter])! fra  
me dropped!  
Exiting normally, received signal 2.  
Terminate batch job (Y/N)?
</code></pre>

<p>huffyuv run:</p>

<pre><code>ffmpeg -f dshow -i video=""screen-capture-recorder"" -video_size 1920x1080 -framerate 30 -c:v huffyuv capture.mkv  
ffmpeg version N-73411-g5233f25 Copyright (c) 2000-2015 the FFmpeg developers  
  built with gcc 4.9.2 (GCC)  
  configuration: --arch=x86_64 --target-os=mingw32 --cross-prefix=/home/user/san  
dbox/mingw-w64-x86_64/bin/x86_64-w64-mingw32- --pkg-config=pkg-config --enable-g  
pl --enable-libsoxr --enable-fontconfig --enable-libass --enable-libutvideo --en  
able-libbluray --enable-iconv --enable-libtwolame --extra-cflags=-DLIBTWOLAME_ST  
ATIC --enable-libzvbi --enable-libcaca --enable-libmodplug --extra-libs=-lstdc++  
 --extra-libs=-lpng --enable-libvidstab --enable-libx265 --enable-decklink --ext  
ra-libs=-loleaut32 --enable-libx264 --enable-libxvid --enable-libmp3lame --enabl  
e-version3 --enable-zlib --enable-librtmp --enable-libvorbis --enable-libtheora  
--enable-libspeex --enable-libopenjpeg --enable-gnutls --enable-libgsm --enable-  
libfreetype --enable-libopus --disable-w32threads --enable-frei0r --enable-filte  
r=frei0r --enable-libvo-aacenc --enable-bzlib --enable-libxavs --enable-libopenc  
ore-amrnb --enable-libopencore-amrwb --enable-libvo-amrwbenc --enable-libschroed  
inger --enable-libvpx --enable-libilbc --enable-libwavpack --enable-libwebp --en  
able-libgme --enable-dxva2 --enable-libdcadec --enable-avisynth --enable-static  
--disable-shared --extra-cflags= --prefix=/home/user/sandbox/mingw-w64-x86_64/x8  
6_64-w64-mingw32 --extra-libs=-lpsapi --enable-nonfree --enable-libfdk-aac --dis  
able-libfaac --enable-nvenc --enable-runtime-cpudetect  
  libavutil      54. 28.100 / 54. 28.100  
  libavcodec     56. 46.101 / 56. 46.101  
  libavformat    56. 40.100 / 56. 40.100  
  libavdevice    56.  4.100 / 56.  4.100  
  libavfilter     5. 20.100 /  5. 20.100  
  libswscale      3.  1.101 /  3.  1.101  
  libswresample   1.  2.100 /  1.  2.100  
  libpostproc    53.  3.100 / 53.  3.100  
leaving aero onInput #0, dshow, from 'video=screen-capture-recorder':  
  Duration: N/A, start: 362514.497000, bitrate: N/A  
    Stream #0:0: Video: rawvideo, bgr0, 1920x1080, 30 tbr, 10000k tbn, 30 tbc  
[huffyuv @ 0000000000380ae0] using huffyuv 2.2.0 or newer interlacing flag  
[huffyuv @ 0000000000377280] using huffyuv 2.2.0 or newer interlacing flag  
[huffyuv @ 00000000003b0fc0] using huffyuv 2.2.0 or newer interlacing flag  
[huffyuv @ 00000000003b1700] using huffyuv 2.2.0 or newer interlacing flag  
[huffyuv @ 0000000000357a00] using huffyuv 2.2.0 or newer interlacing flag  
Output #0, matroska, to 'capture.mkv':  
  Metadata:  
    encoder         : Lavf56.40.100  
    Stream #0:0: Video: huffyuv (HFYU / 0x55594648), rgb24, 1920x1080, q=2-31, 2  
00 kb/s, 30 fps, 1k tbn, 30 tbc  
    Metadata:  
      encoder         : Lavc56.46.101 huffyuv  
Stream mapping:  
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; huffyuv (native))  
Press [q] to stop, [?] for help  
frame=   12 fps=0.0 q=0.0 size=   23668kB time=00:00:00.50 bitrate=386999.4kbits  
frame=   22 fps= 22 q=0.0 size=   44696kB time=00:00:00.96 bitrate=379033.1kbits  
frame=   35 fps= 23 q=0.0 size=   72074kB time=00:00:01.46 bitrate=402750.2kbits  
frame=   49 fps= 24 q=0.0 size=  101620kB time=00:00:02.00 bitrate=416236.4kbits  
frame=   63 fps= 25 q=0.0 size=  131190kB time=00:00:02.50 bitrate=429712.0kbits  
frame=   78 fps= 26 q=0.0 size=  162896kB time=00:00:03.03 bitrate=439829.0kbits  
frame=   83 fps= 26 q=0.0 Lsize=  175587kB time=00:00:03.23 bitrate=444776.1kbit  
s/s  
video:175582kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxin  
g overhead: 0.002479%  
[dshow @ 000000000034f420] real-time buffer [screen-capture-recorder] [video inp  
ut] too full or near too full (545% of size: 3041280 [rtbufsize parameter])! fra  
me dropped!  
Exiting normally, received signal 2.  
Terminate batch job (Y/N)?  
</code></pre>

<p>note: even if I have accepted the answer for general purposes I tend to use the soluction provided by @paul-b-mahol with the <strong>libx264rgb</strong> encoder.</p>

<hr>
","<p>It's an artifact of the player, not the encoder.</p>

<p>I used the command below to generate a RGB format capture in HuffYUV</p>

<pre><code>ffmpeg -f dshow -video_size 1920x1080 -framerate 30 -i video=""screen-capture-recorder"" -c:v huffyuv -t 5 cap.mkv
</code></pre>

<p>Then transcoded the file to</p>

<p>x264 lossless RGB</p>

<pre><code>ffmpeg -i cap.mkv -c:v libx264rgb -crf 0 -preset ultrafast h264rgb.mkv
</code></pre>

<p>HuffYUV YUV 4:2:2</p>

<pre><code>ffmpeg -i cap.mkv -c:v huffyuv -pix_fmt yuv422p huffyuv.mkv
</code></pre>

<p>x264 lossless YUV 4:2:2</p>

<pre><code>ffmpeg -i cap.mkv -c:v libx264 -crf 0 -preset ultrafast -pix_fmt yuv422p h264yuv.mkv
</code></pre>

<hr>

<p>Then, using the SSIM metric, compared the HuffYUV and x264 RGB files</p>

<pre><code>ffmpeg -i h264rgb.mkv -i cap.mkv -filter_complex ssim -f null -
</code></pre>

<p>which resulted in</p>

<pre><code>SSIM R:1.000000 (inf) G:1.000000 (inf) B:1.000000 (inf) All:1.000000 (inf)
</code></pre>

<p>And the HuffYUV and x264 YUV files</p>

<pre><code>ffmpeg -i h264yuv.mkv -i huffyuv.mkv -filter_complex ssim -f null -
</code></pre>

<p>Result</p>

<pre><code>SSIM Y:1.000000 (inf) U:1.000000 (inf) V:1.000000 (inf) All:1.000000 (inf)
</code></pre>

<p>So, x264 does produce lossless output.</p>
","18385"
"What setup do I need to record and render at the same time?","728","","<p>I need help figuring out how to record while my computer is rendering.</p>

<p>I am very new to video editing, so this is my current workflow.  First, I record footage using Fraps (30 fps, 1080p, .avi). I usually get several hours worth.  Then I use Adobe Premiere CC and Media Encoder to cut the video into 20 minutes chunks. I queue all the videos up in Media Encoder (h.264, mp4, 1080p, 30fps).</p>

<p>Each video takes about 30 minutes to render (intel 2600k. dual amd 6950s, 16 gb ram). My source video is on a 2 TB HDD and the destination is a separate 1 TB HDD.</p>

<p>Here is my problem.  While rendering, the computer can't be used for much else since my CPU usage is capped.  My GPU doesn't seem to be used at all (as I understand, h.264 encoding is almost all CPU based).</p>

<p>How can I change my setup so that I can record more video while the machine is rendering away? In addition to my PC, I have a NAS available, a Macbook Retina Pro (2012, 16gb ram), and two external HDD cages. I think I need to come up with a setup where I can record on one machine and render with the other, but I don't know if this is viable or which HDD to record video to in that situation (is recording to NAS common? or do I make the other computer's HDD a share drive? record to external drive and move it around?) </p>

<p>There are a lot of unknowns for me here and I would just like to know what a ""normal"" setup is for using one computer to record and another to render.  I am on a gigabit LAN with my NAS and laptop, but the laptop only has wireless connectivity.</p>

<p>I would just render my videos over-night, but that is when I upload to Youtube, as that takes many hours. </p>
","<p>Don't, just don't.  File access is your primary problem.  You can offset this by using SSDs, but the much more practical solution is to either upgrade to gigabit connectivity between two systems or use high speed external drives to move content captured on one system to the other system and then run the encoding completely distinctly.</p>

<p>Capturing footage on a system running a game is already a very difficult thing to do as both the file capture and the game engine put a lot of demand on the system.  Adding a third major demand of system resources is asking for trouble.  The main reason that files are so large from video game screen captures is that very little compression is applied.  Video games don't use much disk resources (outside of loading levels) but they do use a lot of CPU and GPU.  So screen capture tools for games put off CPU and GPU intensive tasks at a cost of disk utilization.</p>

<p>The problem is that encoding is going to make use of both disk utilization and CPU and GPU resources.  The CPU and GPU utilization will interfere with the game engine and the disk utilization will interfere with capture unless they are on a completely separate pool of resources on a different machine.  It is possible to segment resources off in some cases, but you'd still be substantially impacting the capability of your system and reducing the quality of capture and gameplay it could achieve.</p>
","13101"
"How to Record a Dark scene without any Noise?","726","","<p>I am using Nikon D3200, I am trying to shoot a dark scene. The scene is so dark and there is minimal amount of light. I am shooting in manual mode and these are my settings. ISO 200, f/5.2, Shutter Speed 1/50.</p>

<p>With all these settings i am getting so much noise in the scene. Kindly tell me how to avoid noise in the Dark Scene.</p>

<p>Thanks</p>

<p>Taha<img src=""https://i.stack.imgur.com/yGvJo.jpg"" alt=""enter image description here""></p>
","<p>With low light levels your brightest signal will be close to the noise floor, so you only really have three options:</p>

<ul>
<li>a camera with better low light performance (although this can only take you so far)</li>
</ul>

<p>More expensive sensors can give a lower noise floor, allowing you to resolve more detail</p>

<ul>
<li>a faster lens</li>
</ul>

<p>As Jason commented:</p>

<blockquote>
  <p>If the widest aperture on your kit lens is 4.0, then you could double the amount of light your camera sees by purchasing an f2.8. Every additional ""stop"" doubles the light, so if you could find an f1.0, you'd be getting 16 times the light that f4.0 allows.</p>
</blockquote>

<ul>
<li>more light</li>
</ul>

<p>Your easiest option may be just to increase the illumination and then reduce brightness in post. Remember that if you do this, you also need to watch for shadows that look too sharp.</p>
","12617"
"Blur a face using only one video layer with Premiere CS6?","717","","<p>so with the new built-in masking option of the gaussian blur and similar effects in the CC version of Premiere Pro it's pretty easy to limit effect to a specific area of the video in order to e.g. blur a face. However, I'm searching for an easy and <strong>easily comprehensible</strong> way to achieve this effect in CS6 without duplicating the clip to a second video layer (video lane?) and using the gaussian blur in combination with a track matte key (which is what I used to do in Premiere Pro CS6).
So, is there a way to blur a face without duplicating the clip to a second video layer in the CS6 version?
(Unfortunately, I only have the current CC version at my disposal right now, so I can't try it out myself at the moment.)
Thanks a lot!</p>
","<p>Yes and no, you can do it without duplicating the clip, but not without a second layer.  Instead of using a second copy of the clip, you can use an adjustment layer.  You make a new adjustment layer by creating a New Item in the Project window.  You add that adjustment layer over top of the video you want to adjust and then you apply your masking to the adjustment layer and apply your effects to the adjustment layer.</p>

<p>The effects applied to the adjustment layer are then applied to the sum total of all the layers below the adjustment layer.  Effectively, the adjustment layer is a layer that is composed of a sub-render of everything below it and then has the set of effects applied to it applied to that sub-render.</p>

<p>It technically isn't adding two copies of the sequence, but practically it is still doing the same thing.  I'm not personally aware of any more straightforward technique than that for doing what you describe in CS6 though.  That doesn't necessarily mean much though as I normally do this kind of thing in AE anyway, so my experience doing it in Premiere is limited at best (bordering on non-existent.)</p>
","13034"
"Is there a way to do both Extruded Text and Layer Modes natively in After Effects?","717","","<p>I followed <a href=""http://www.videocopilot.net/tutorials/3d_shadows/"" rel=""nofollow noreferrer"" title=""Video Copilot 3D Shadows"">Video Copilot 3D Shadows</a> tutorial to create a nice shadow on my text. Because of the angle I'm doing though which is much more ... perspective I suppose you could say it could really use a little depth to it. I looked up if AE could do extrusion (also since I've been told some basic 3D is built-in to AE by members of this exchange) and found <a href=""http://www.youtube.com/watch?v=4mZsANPiv-Y"" rel=""nofollow noreferrer"" title=""How to Extrude Text in AE"">How to Extrude Text in AE</a>. The problem is this tutorial tells me to change my Renderer to Ray-Traced 3D as a prerequisite for extrusion but if I do that it tells me Blending Modes don't work and clicking okay anyways seems to agree as my ground on Multiply goes back to Normal with no option to change.</p>

<p><strong>Extruded Text</strong></p>

<p><img src=""https://i.stack.imgur.com/wrAyI.jpg"" alt=""enter image description here""></p>

<p><strong>Extruded Text moved into scene</strong></p>

<p><img src=""https://i.stack.imgur.com/ECxCm.jpg"" alt=""enter image description here""></p>

<p><strong>Extruded text when I try to make it 3D for shadows, light and placement in scene</strong></p>

<p><img src=""https://i.stack.imgur.com/o4fyW.jpg"" alt=""enter image description here""></p>

<p>Is there a native way in After Effects to combine both Layer Modes and Extruded Text?</p>
","<p>You can work around it using multiple compositions.  Do one composition to generate the 3d layer and then use that composition within another to handle the blending (or vice versa depending on your needs).</p>
","9945"
"What are some things to know and learn about the Canon 550D (T2i) when shooting movies?","717","","<p>I am working on using our 550D for shooting movies. What are some things I need to know and learn about the 550D before shooting with it?</p>
","<p>Some things to be aware of when shooting video (this applies to most DSLRs):</p>

<h1><a href=""http://en.wikipedia.org/wiki/Rolling_shutter"" rel=""nofollow noreferrer"">Rolling Shutter</a></h1>

<p>Because of the way the 550d records each frame you can end up with <a href=""http://youtu.be/GMENDJdzlf0"" rel=""nofollow noreferrer"">slanted video</a> when combined with fast motion. This is because the camera recordes each frame line by line, so each line is recorded at a slightly different time causing a warped effect. Its not a problem for normal motion, but if you are doing moves like panning the camera quickly you will see the effect.</p>

<p><img src=""https://i.stack.imgur.com/2z5Rb.jpg"" alt=""Example of Rolling Shutter""></p>

<h1>Recording Sound</h1>

<p>The Canon 550d (and again most other DSLRs) dont have the best quality audio recording. This leads to two methods you can use to record audio for film work</p>

<h3>1. External Recording</h3>

<p>Most commonly, people will shoot using an external recorder like a <a href=""http://en.wikipedia.org/wiki/Zoom_H4n"" rel=""nofollow noreferrer"">Zoom H4n</a> and then <a href=""https://vimeo.com/5774898"" rel=""nofollow noreferrer"">sync the audio to the video in post</a> using software like <a href=""http://www.singularsoftware.com/pluraleyes.html"" rel=""nofollow noreferrer"">Plural Eyes</a>. The important thing to remember if you choose this method is to make sure you still have internal recording enabled on the 550d because you will need it as reference audio (there is never really any good reason to NOT record the internal audio, its always useful as a reference even if you dont intend on using the audio)</p>

<h3>2.  Built-in Recording</h3>

<p>The camera has a compressor built in so anything you plug into the line in jack will be running through the cameras compressor. The issue that can come from this is clipped audio if your audio suddenly jumps up in volume. <a href=""http://mattthomasfilm.blogspot.com.au/2011/02/recording-sound-with-your-550d.html"" rel=""nofollow noreferrer"">Here is a good article</a> going over recording sound specifically with a 550d and ways to avoid having your audio clipped. </p>

<h1>Memory &amp; Recording Time</h1>

<p>A common problem with almost all the Canon DSLRs (not sure about the new ones coming out) is that they can only record around 12 minutes at a time. In my experience the amount is never exact. Its not a problem for any fictional work because its rare you would be shooting for longer than a few minutes at a time, but for documentary work (eg interviews) this can be a problem!</p>

<p>Its worth testing your memory cards before you shoot with them, I've had many experiences where some cards will record fine and others will stop recording after 5-15 seconds. Its always worth getting higher class memory cards which allow for <a href=""http://en.wikipedia.org/wiki/Secure_Digital#Speed_Class_Rating"" rel=""nofollow noreferrer"">faster transfer rates</a>. Class 6 or Class 10 is often the standard choice for video recording (though the higher the class, the higher the price!)</p>

<h1>Batteries</h1>

<p>Shooting video chews through the battery life. For a single days shoot you would be very lucky if you didn't use more 3-4 batteries. Its always a good idea to have at least two batteries (ideally 3). That way when your battery dies you can use the spare battery while the other charges. But I wouldnt rely on this, sometime it takes longer for the battery to charge and you may not always have access to power depending on your location.</p>

<h1>Good Resources</h1>

<p>A good resource for DSLR film making is <a href=""http://philipbloom.net/category/dslrposts/"" rel=""nofollow noreferrer"">Phillip Bloom</a>'s website and also <a href=""https://vimeo.com/videoschool/archive/dslr"" rel=""nofollow noreferrer"">Vimeo's DSLR video school</a>.</p>

<p>Good luck!</p>
","3879"
"DaVinci Resolve: hide timeline viewer","713","","<p>I was playing around with Resolve, and managed to show a second window thingy (the black one in the screenshot) via View > Source/Timeline Viewer. Now I can't seem to make it disappear. I have tried Workspace > Layout > Reset UI Layout, but it doesn't remove the second window. How can I remove it?</p>

<p><a href=""https://i.stack.imgur.com/vaaIS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vaaIS.png"" alt=""Resolve""></a></p>
","<p>I stumbled upon one way of doing this, and that is to click ""Inspector"" at the top right of the UI:</p>

<p><a href=""https://i.stack.imgur.com/Cu88z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cu88z.png"" alt=""Resolve screenshot - Inspector""></a></p>
","20185"
"Amateur orchestra recording/post-processing","712","","<p>I play in an amateur university orchestra, and I just recorded our last concert this weekend. This was a very simple setup: just a Sony PCM-D50 recorder, set to record at 96/24 through its internal mics, on a tripod a couple of meters away from the orchestra.</p>

<p>I set the recording level low enough that the internal limiter never triggered, while getting peaks at around -3 dB when the orchestra was really loud (and 0 dB during applause, of course).</p>

<p>Now with an orchestral program (especially in this case where we played movie soundtracks, so you get a solo violin in Schindler's List, a small woodwind ensemble in Harry Potter (Nimbus 2000 theme), and full-tilt brass/percussion in Star Wars) there is an enormous dynamic range to cover. It seems that modern productions use heavy dynamic range compression to ""lift"" the soft passages, and while I'm quite happy with the sound quality of the recording as-is, I'd still like to try and ""equalize"" it a bit so it's more in line with other CDs.</p>

<p>My problem is that when I use the pre-set Dynamic range compression settings in Sound Forge Audio Studio (for example 2:1 compression from -18 dB, or 3:1 from -15 dB) and then add gain (say, +6 dB), I find that the recording starts sounding quite muddy.</p>

<p>I don't think that that's because of a bad algorithm (I find the same problem in Audacity) but probably because I'm using a sledgehammer instead of a fine screwdriver to adjust the sound. Do you have any tips on how to lift low volumes without muddying the middle dynamic and high peaks?</p>
","<p>The real answer: make sure your input level is OK every time you record something. Make sure that the loudest passages are just hitting the head (I try getting to > -1dB FS while not clipping.) , so the soft passages have enough detail. Also remember that dynamic range is a musical ingredient in classical music, even between different pieces. If things are soft in the hall, you don't want them as loud in your recording as things that are loud in the hall, you will lose a bit of musicality.</p>

<p>So: never ever compress classical music to solve your dynamic range problems, unless you use it for professional broadcasting purposes only. (And even then, don't compress the master, but use compression in the broadcasting chain.)</p>

<p>Instead, if you still want to fix things because you are turning the volume up and down between the pieces: slightly adjusting the overall levels of the pieces a bit is in many occasions the right thing to do. To do this, edit all pieces so that they fade in and out on the beginning and the end. (With a piece, I mean, if you record a Beethoven symphony, those 4 movements are related, so only fade the beginning of the first movement in, and the applause at the end of the last movement out. Don't fade in between the movements and don't adjust the volume for the individual movements!) If you do this, you can make slight adjustments to the overall level of the pieces. Don't normalize to 0dB FS though, and don't make the changes too big. A few (e.g. 3) dB could just be enough to solve your problem.</p>

<p>If you want to go this path, make sure you always record in 24 bits so you have more detail in your audio if you increase the volume in the master. (Sample rate is of less importance, but to record in 96KHz or in 88.2KHz is always good.)</p>

<p>My personal tips:</p>

<ul>
<li>Adjusting the volume of a classical mix changes the accoustics of the hall; all sustaining noise from e.g. the audience is louder as well as the reverb it creates.</li>
<li>Adjusting the volume of a classical mix/master is almost always an excuse for having too much headroom on your recording and will result in dynamic detail getting lost. Therefore, always record in 24 bits if you decide to increase it.</li>
<li>As mentioned: dynamic range in classical music is a piece of the music itself, even between the different pieces. Getting rid of it is destroying part of the musical contents.</li>
<li>As a rule of thumb: never touch the volume of a part of a classical recording if the difference in dynamics is so bad that you can't listen to the recording in the living room without touching the volume button.</li>
</ul>
","733"
"Green screen without green screen","707","","<p>Is it possible to, rather than a green screen, take a static shot of a background image (e.g. for a Skype call with a fixed-position webcam), and use this static image as the green screen?</p>

<p>So, I would take a photo of my sitting room through the cam, and then be able to use that image as my ""green screen"".</p>
","<p>Yes, that is possible. What you're searching for is a <strong>difference matte</strong> / <strong>difference key</strong>. For that technique, you supply a base image and the program keys everything out that equals that image. So you could key out your static background so that only you remain in the picture.</p>

<p>However, there are a few downsides:</p>

<ul>
<li>This technique doesn't work nearly as good as a greenscreen. It will probably require a lot of work and there will still be some artifacts left in the video.</li>
<li>If you're using a bad camera (such as a laptop webcam), the result will be even worse.</li>
<li>If you move your camera even slightly, you'll have a problem. So if you do that with a webcam, don't touch it at all, if possible.</li>
<li>There probably aren't many applications that can do this effect live (as opposed to post-production). IIRC, there is some video chat app for Mac-Computers that can do this. I'm not sure if it is possible with Skype, you'll have to research that. However, keep in mind that it probably won't look as good as you imagine it.</li>
</ul>
","16229"
"How to film a strobe light","705","","<p><a href=""https://i.stack.imgur.com/KQcly.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KQcly.jpg"" alt=""Strobe light with half the screen lit and half not""></a></p>

<p>The above picture sums up my problem. I'm struggling to work out what settings to put my camera on to work with my strobe light. I vaguely understand the problem; that the camera takes pictures row by row, so the lighting has changed by the time it gets to the bottom half. But I can't work out how to fix it. I would have thought that a very fast shutter speed would help, but apparently not.</p>

<p>The camera is a Fujifilm X-E1 and the strobe light is a cheap model with a rotary dial to change the speed. It doesn't have DMX control.</p>
","<p>What you're seeing is an inherent problem with sensors that use ""rolling shutter"". There are tradeoffs in sensor technology among speed, sensitivity, and size, and using a sequential line readout with continuous exposure is a popular choice that's a good compromise for many situations. Unfortunately yours isn't one of those.</p>

<p>What you need is what's known as ""total"" or ""global"" shuttering, where the entire frame is exposed at once and read out as a single image. Instead you have a sensor that essentially exposes each line of the array at different times.</p>

<p>So I'm afraid the answer is: use a camera / sensor that employs a different technology. I hope for your sake someone has a solution that allows you to use your existing setup, but I don't expect to see one.</p>
","17784"
"What is the correct MPEG-2 FourCC?","705","","<p>In my attempts to encode a video to MPEG-2 in an MOV (which seems to be a valid combination, at least <a href=""https://en.wikipedia.org/wiki/Comparison_of_video_container_formats"" rel=""nofollow"">according to Wikipedia</a>), using ffmpeg (<code>-vcodec mpeg2video</code>) and other tools, I have become extremely confused about the FourCCs as well as seemingly related limitations on container formats. I have found:</p>

<ul>
<li><code>m2v1</code>: This is the default FourCC used by ffmpeg when encoding MPEG-2, but it is unplayable in all media players I have tried (VLC, Windows Media Player, mplayer, and QuickTime Player). It also does not seem to exist in any FourCC databases I have found. Forum posts are rare and are related to old Mac software and compatibility problems regarding this code.</li>
<li><code>mp2v</code>: I can force this code with ffmpeg. It is then playable in all common media players. However, MPEG-2 videos encoded with other encoders (such as Handbrake or Prism), to AVIs, do not use this code, they use <code>mpgv</code>, which makes me suspicious.</li>
<li><code>mpgv</code>: I can force this code with ffmpeg as well, but only when outputting to an AVI (and Handbrake uses it too but only outputs Matroska and MP4). Ffmpeg fails when I attempt to output MOV. Forum posts also indicate trouble with the <code>mpgv</code> + QuickTime container combination.</li>
</ul>

<p>I'm very confused. Why are there different FourCC's for this, what is the correct one to use, how is it related to the container and, at the end of the day, if I want an MPEG-2 in an MOV what do I do?</p>

<p>I guess by ""correct"" in the title I mean compatible with the most common players; least likely to cause problems for normal people with stock software.</p>

<p>As a sub-question, VLC describes <code>mpgv</code> and <code>mp2v</code> as ""MPEG-1/2 Video"". So, if I see a video with FourCC <code>mpgv</code>, is it MPEG-1, or is it MPEG-2? I suspect my lack of understanding of how all the MPEG versions are related to each other is coming into play here.</p>
","<p>Strictly speaking, <a href=""https://msdn.microsoft.com/en-in/library/ms922669.aspx"" rel=""nofollow"">fourcc</a> is the codec ID used by Microsoft. It has been adapted for use with many other formats, thus making it seem like a standardized ID format, but it's not.</p>

<p>ffmpeg, in particular, seems to only consider XDCAM standard MPEG2 for inclusion in MOV.</p>

<p>From the <a href=""https://github.com/FFmpeg/FFmpeg/blob/6e249466cc6bd5b17d6e8cbd9a84a636cc92fd60/libavformat/movenc.c"" rel=""nofollow"">source code</a>:</p>

<pre><code>else if (track-&gt;enc-&gt;codec_id == AV_CODEC_ID_MPEG2VIDEO)
    tag = mov_get_mpeg2_xdcam_codec_tag(s, track);
</code></pre>

<p>where if the xdcam tag function doesn't find a matching XDCAM stream, it runs</p>

<pre><code>if (!tag)
    tag = MKTAG('m', '2', 'v', '1'); //fallback tag
</code></pre>

<p>which ffmpeg, interestingly, <a href=""https://github.com/FFmpeg/FFmpeg/blob/979572365f2133f969f3f49ec6a99cc8739d2eba/libavformat/isom.c"" rel=""nofollow"">thinks</a> is tha tag that should be used for recording MPEG-2 in a MOV by a camera.</p>

<pre><code>{ AV_CODEC_ID_MPEG2VIDEO, MKTAG('m', '2', 'v', '1') }, /* Apple MPEG-2 Camcorder */ 
</code></pre>

<p>and <code>mp2v</code> is signalled as required for (an old version of) Final Cut Pro.</p>

<pre><code>{ AV_CODEC_ID_MPEG2VIDEO, MKTAG('m', 'p', '2', 'v') }, /* FCP5 */
</code></pre>

<p><code>mpgv</code> is not on that (ISO media) list that the MOV muxer can be forced to work with, but it is on the <a href=""https://github.com/FFmpeg/FFmpeg/blob/11bc4fd653fab05a9e24f7aca22c913ffb238b5a/libavformat/riff.c"" rel=""nofollow"">RIFF list</a> (for AVIs) where it's only declared for MPEG-2, not MPEG-1</p>

<pre><code>{ AV_CODEC_ID_MPEG2VIDEO, MKTAG('m', 'p', 'g', 'v') }
</code></pre>

<hr>

<p>All the tag variants exist because each vendor or program decides to mark their streams a certain way and there's no single body policing their use. Stick with <code>mp2v</code> since that works for you.</p>

<p><strong>Update</strong>: I just realized that any stream can be tagged with any tag if <code>-strict -1</code> or <code>-2</code> is applied. I was able to tag an AVC stream in a MOV with <code>mpgv</code> :) </p>
","17527"
"why is a video file with .mts extension so huge?","700","","<p>I just recorded a 2 minute long video clip with my Sony camera. The output is an .MTS file, which is ~450 MB. What is the reason for that? Could you please recommend a (preferably free) app that I could use to convert it to another file format resulting a much more reasonable file size?</p>
","<p>HD video is very, very large at high quality.  450MB for two minutes isn't actually that bad.  To put it in perspective, assuming you were shooting 1080P and 24fps, if there was no compression applied, that same 2 minutes of video would be 6 gigabytes of information.</p>

<p>The reason that videos you download or watch on bluray disks are so much smaller is that they are not source video, but rather highly compressed final output video.  When you compress video, a lot of quality is lost.  It isn't quality that the human eye can easily distinguish, but it is quality that impacts the quality of future encodings and the quality of edited results.</p>

<p>For best quality, you want to take those high quality, large files you captured, edit them to whatever the final output should be and then do a one time compression down to a final output format.  That final output won't be suitable for editing, but it will be perfectly suitable for viewing at a much smaller size.</p>

<p>If you have no desire to edit it and simply want the basic video to be smaller, using a free tool like Handbrake or FFMPEG will easily allow you to transcode to h.264 and a much, much smaller file size.  I recommend using two-pass VBR to maximize the quality you get for the size.</p>
","10492"
"How can I convert large jpgs (15000 x 10840px) to video?","699","","<p>Whenever I try the command</p>

<pre><code>ffmpeg -y -r 1 -i File01.jpg -c:v libx264 -tune stillimage -pix_fmt yuv420p out.mp4
</code></pre>

<p>I get the failing result:</p>

<pre><code>[swcaler @ 000000000347fe0] deprecated pixel format used, make sure you did set range correctly
[libx264 @ 0000000002da3c0] using SAR=1/1
[libx264 @ 0000000002da3c0] frame MP size (938x678) &gt; level limit (36864)
</code></pre>

<p>and a corrupt file which cant be read.
Is there a maximum size of the resolution that can be created?</p>
","<p>That's way more than h.264 can handle. The maximum resolution for h.264 is 40962304 with Level 5.1 or 5.2.
<a href=""http://en.wikipedia.org/wiki/H.264#Levels"" rel=""nofollow"">http://en.wikipedia.org/wiki/H.264#Levels</a></p>

<p>There are lossless codecs who can theoretically handle this video resolution but it would make no sense to do this. There is no hardware capable of playing videos in this resolution at any acceptable frame rate.
Atleast none that is in any way affordable by a normal human being.</p>
","11944"
"Small Kitchen studio setup","698","","<p>I have a small apartment kitchen that I'd like to be able to convert (at will) into a small studio.  The idea is to capture myself cooking.  I'd like your thoughts on the type of cameras to purchase, unique mounting options and lighting.</p>

<p>Ideally I'd like to be able to take all video sources and bring them into my computer (Mac) and edit via Final Cut.</p>

<p>Space is at a premium and with that, what would be my best option for digital cameras for the following purposes:</p>

<ol>
<li>Overhead shot of stove </li>
<li>Active camera I can look at to speak and give direction  </li>
<li>Overhead shot of cutting board</li>
</ol>

<p>Secondly are Mounts. I'd need something that could 1) support the camera in question 2) be long and flexible so I can position a camera above the stove</p>

<p>Do I have to be concerned with heat/steam interfering?  Any tips on thwarting such problems?</p>

<p>Lastly, lighting.  I have no clue, I guess I'm looking for affordable and not too bulky.</p>
","<p>I will answer this in a few parts</p>

<p>Cameras:
I assume you are looking for some kind of a web release for what ever media you are recording and I assume you want to go high def. For this most if not all of the current DSLR's from the big makers (cannon, nikon, etc) will shoot in both 720 and 1080 (i and p depend on the camera but for what you are looking to do I would say either will suffice). When it comes to video my only requirement is that the camera has an external mic input (I know the Nikon D series has them). The on board mics on DSLR's are not that great and an external one is a must. This would be your main camera.</p>

<p>Go Pros are GREAT and come with a wide variety of mounting options. They are also in waterproof cases (I have used mine in the pool a few times with good success) so I would think they could stand up to some puffs of steam although continuous steam will be an issue for any camera.</p>

<p>You can also troll ebay or tag sales etc. for used DSLR's for your overhead shots. You can usually find them cheap and be less worried about them breaking form steam/heat</p>

<p>Mounting:
Good tripods area easy to find. In this case I would look for something hefty but on the smaller side. I have a small Slik 504 that is small but really steady and has a smooth head. I also have a MeFoto for when Im on the road. These are much smaller than the Slik's but REALLY well made (all aluminum) and will not have a problem holding your DSLR.</p>

<p>Lights:
Theater/Video lights can be pricy as can their mounting hardware. For this I will offer a simple home brew hack. Go to home depot and pick up the double halogen with stand work light. They are constantly on sale and fit the bill really well. they are bright, cheap, and come mounted to a stand. They usually are capable of pivoting in all directions as well.</p>

<p>Steam solution:
I dont know much about this issue but one possibility is to mount a mirror over the stove and shoot the mirror. This puts the camera out of the line of steam and out of harms way. You can also get one of those shower mirrors that does not fog up in high steam environments.</p>

<p>Hope This Helps...</p>
","12237"
"Video metadata: What is ""Video delay"" used for?","694","","<p>Recently, I'm forced to fiddle with a nasty little problem with videos that have some special metadata attributes, namely ""delay"" or ""delay_relative_to_video"".</p>

<p>There's a software videoconferencing solution wich produces these special files, and when our videoprocessing software examines these files, it gets longer duration values than the actual length of the content, thus storing wrong data into the database which causes more trouble in the postprocessing phase.</p>

<p>For example, we recorded a 46 second long video, which was detected as 104.7 seconds long.</p>

<p>After inspecting the file with FFmpeg and Mediainfo, it turned out that the original file had a so called ""delay"" value which can be seen as <code>start</code> time in FFmpeg's output:</p>

<pre><code>Input #0, flv, from '/path/to/the/file/converter/master/194/194_video.flv':
  Metadata:
    metadatacreator : Yet Another Metadata Injector for FLV - Version 1.4
    hasKeyframes    : true
    hasVideo        : true
    hasAudio        : true
    hasMetadata     : true
    canSeekToEnd    : false
    datasize        : 12892571
    videosize       : 12198311
    audiosize       : 680584
    lasttimestamp   : 105
    lastkeyframetimestamp: 104
    lastkeyframelocation: 12646873
  Duration: 00:01:44.77, start: 58.033000, bitrate: 984 kb/s
    Stream #0:0: Video: h264 (Constrained Baseline), yuv420p, 1280x720, 30 fps, 30 tbr, 1k tbn
    Stream #0:1: Audio: aac (LC), 44100 Hz, stereo, fltp
</code></pre>

<p>...and <code>Delay</code> or <code>Delay_relative_to_video</code> in Mediainfo's:</p>

<pre><code>mediainfo --full --output=XML /path/to/the/file/converter/master/194/194_video.flv

&lt;track type=""Video""&gt;
(...)
&lt;Delay&gt;0&lt;/Delay&gt;
&lt;Delay&gt;00:00:00.000&lt;/Delay&gt;
&lt;Delay__origin&gt;Container&lt;/Delay__origin&gt;
&lt;Delay__origin&gt;Container&lt;/Delay__origin&gt;
&lt;/track&gt;

&lt;track type=""Audio""&gt;
(...)
&lt;Delay&gt;58036&lt;/Delay&gt;
&lt;Delay&gt;58s 36ms&lt;/Delay&gt;
&lt;Delay&gt;58s 36ms&lt;/Delay&gt;
&lt;Delay&gt;58s 36ms&lt;/Delay&gt;
&lt;Delay&gt;00:00:58.036&lt;/Delay&gt;
&lt;Delay__origin&gt;Container&lt;/Delay__origin&gt;
&lt;Delay__origin&gt;Container&lt;/Delay__origin&gt;
&lt;Delay_relative_to_video&gt;58036&lt;/Delay_relative_to_video&gt;
&lt;Delay_relative_to_video&gt;58s 36ms&lt;/Delay_relative_to_video&gt;
&lt;Delay_relative_to_video&gt;58s 36ms&lt;/Delay_relative_to_video&gt;
&lt;Delay_relative_to_video&gt;58s 36ms&lt;/Delay_relative_to_video&gt;
&lt;Delay_relative_to_video&gt;00:00:58.036&lt;/Delay_relative_to_video&gt;
&lt;/track&gt;
</code></pre>

<p>""Surprisingly"", substracting the length and the delay gives me the correct duration...</p>

<p>Also, it is really embarrassing that some encoder libraries count these delays towards the total length, but some of them doesn't! Seemingly there's no way to distinguish which length values are correct and which were incremented accordingly!</p>

<p>So long story short, I'd like to know what are these metadata used for?
What's the reason of their existence and why does some encoders/video editing software injecting these metadata into the file?</p>

<p>Thanks for your answers in advance!</p>

<p><strong>Edit:</strong></p>

<p><strong>According to Mulvya's answer, running a simple stream-copy on the file can reset the timestamps!</strong></p>

<p>However, there was a .mov file on which it didn't worked.
Related console output:</p>

<pre><code>ffmpeg -i 22_video.mov -c copy _22_video.mov
ffmpeg version N-79632-g3ce1988-static Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 4.7 (Debian 4.7.2-5)
  configuration: --prefix=/home/gergo/buildscript/ffmpeg-static-master-customVSQ/target --extra-cflags='-I/home/gergo/buildscript/ffmpeg-static-master-customVSQ/target/include -static' --extra-cflags=--static --extra-ldflags='-L/home/gergo/buildscript/ffmpeg-static-master-customVSQ/target/lib -lm -static' --extra-libs=-ldl --extra-version=static --disable-debug --disable-shared --enable-static --extra-cflags=--static --disable-ffplay --disable-ffserver --disable-doc --enable-gpl --enable-pthreads --enable-postproc --enable-gray --enable-runtime-cpudetect --enable-libfaac --enable-libmp3lame --enable-libopus --enable-libtheora --enable-libvorbis --enable-libx264 --enable-libxvid --enable-bzlib --enable-zlib --enable-nonfree --enable-version3 --enable-libwavpack --enable-libvpx --enable-librtmp
  libavutil      55. 22.101 / 55. 22.101
  libavcodec     57. 38.100 / 57. 38.100
  libavformat    57. 34.103 / 57. 34.103
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 44.100 /  6. 44.100
  libswscale      4.  1.100 /  4.  1.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '22_video.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 537199360
    compatible_brands: qt
    creation_time   : 2013-10-29 21:33:33
    com.apple.finalcutstudio.media.uuid: 3B712819-1D1D-4F9B-8593-01656870A04C
    timecode        : 01:00:00:00
  Duration: 00:03:25.00, start: 0.000000, bitrate: 9332 kb/s
    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, 9019 kb/s, 25 fps, 25 tbr, 2500 tbn (default)
    Metadata:
      creation_time   : 2013-10-29 21:33:33
      handler_name    : Apple Video Media Handler
      encoder         : H.264
    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 307 kb/s (default)
    Metadata:
      creation_time   : 2013-10-29 21:33:33
      handler_name    : Apple Sound Media Handler
    Stream #0:2(eng): Data: none (tmcd / 0x64636D74) (default)
    Metadata:
      creation_time   : 2013-10-29 21:33:33
      handler_name    : Time Code Media Handler
      timecode        : 01:00:00:00
File '_22_video.mov' already exists. Overwrite ? [y/N] y
[mov @ 0x46c6660] Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead.
    Last message repeated 1 times
Output #0, mov, to '_22_video.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 537199360
    compatible_brands: qt
    timecode        : 01:00:00:00
    com.apple.finalcutstudio.media.uuid: 3B712819-1D1D-4F9B-8593-01656870A04C
    encoder         : Lavf57.34.103
    Stream #0:0(eng): Video: h264 (avc1 / 0x31637661), yuv420p, 1920x1080, q=2-31, 9019 kb/s, 0.04 fps, 25 tbr, 10k tbn (default)
    Metadata:
      creation_time   : 2013-10-29 21:33:33
      handler_name    : Apple Video Media Handler
      encoder         : H.264
    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, 307 kb/s (default)
    Metadata:
      creation_time   : 2013-10-29 21:33:33
      handler_name    : Apple Sound Media Handler
Stream mapping:
  Stream #0:0 -&gt; #0:0 (copy)
  Stream #0:1 -&gt; #0:1 (copy)
Press [q] to stop, [?] for help
frame= 5125 fps=0.0 q=-1.0 Lsize=  233585kB time=00:03:25.00 bitrate=9333.9kbits/s speed= 547x
video:225709kB audio:7706kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.072599%
</code></pre>
","<p>Streaming formats maintain timestamps for each frame, whether audio or video, which govern when the player ought to present them. Those non-zero big start times usually occur when a snippet is cut out from a longer video and the tool used does not reset the timestamps. Although if this FLV was recorded on its own, then it's strange.</p>

<p>In any case, running the command below should fix the problem.</p>

<pre><code>ffmpeg -i input.flv -c copy output.flv 
</code></pre>
","18446"
"Nikon D3300 Headphone output?","691","","<p>How can I from my Nikon d3300 get a headphone output? Im using rode micro I would like it not to be a huge box or super long cable. </p>

<p>This is my first preference. Start in the upper left. (mic)
<a href=""https://i.stack.imgur.com/6CPyH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6CPyH.png"" alt=""This is my first preference. Start in the upper left. (mic)""></a></p>

<p>This is my second preference. Start in the upper left. (mic)
<a href=""https://i.stack.imgur.com/Nx4Xe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nx4Xe.png"" alt=""This is my second preference. Start in the upper left. (mic)""></a></p>

<p>REG/rode_videomicro_compact_on_camera.html</p>
","<p>Microphone outputs are typically very low level and have only enough output to drive a sensitive preamp, not headphones.  To drive headphones you need an amplified output.  According to a variety of sources, the D3300 does not have a headphone jack, which means you have to provide a separate amplifier, which means you have to provide one, and the power to drive it, and the cable to connect to it.  I suspect the best solution would be a battery-powered preamp that receives your mic output and then delivers two amplified outputs: line level for your D3300 input jack and phones-level for your headphones.</p>
","20768"
"Blackmagic setup: How do I record all cameras in a multicam broadcast?","690","","<p>I've been tasked with acquiring equipment to broadcast a festival through online stream, and have decided that blackmagic may be a good direction to go, so I'm going to strictly reference their products.</p>

<p>Currently I'm planning on purchasing:</p>

<ul>
<li>1 x Blackmagic ATEM Production Studio 4K</li>
<li>4 x Blackmagic Micro Studio Camera 4K</li>
<li>1 x Blackmagic UltraStudio Mini Recorder (thunderbolt)</li>
<li>Some very long SDI cable runs</li>
</ul>

<p><br><strong>Problem:</strong>
<br>The cameras do not have internal storage, which means I can only record what's going to the stream through the UltraStudio mini recorder. Is there some kind of device I can set up between the cameras and the ATEM production studio to record my video feeds?</p>

<p>I don't think the ATEM has enough video outs to send to each feed to a capture card like the DeckLink SDI 4K. If I went this route, would I need something like the Smart Videohub?</p>

<p>Lastly, the cameras claim to have built in talkback and tally. Can I use this feature with my current equipment list? Or do I need the ATEM Talkback Converter?</p>

<p><br><strong>Update:</strong>
<br>I've decided that 3 blackmagic pocket cinema cameras (BPCCs) and a GoPro may actually be a better option for this. The BPCCs have a built in display (useful for my camera operators) and have internal storage via SD card. They also seem to perform well in low lighting. The downside is the cameras only have HDMI out, so I would require HDMI to SDI converters to run long lengths of cable (probably around 150-200ft per cable).</p>

<p>Am I going to have issues trying to use BPCCs with converters for live streaming? Are there other solutions for long cable runs that I may have missed?</p>
","<p>You can use an SDI Distribution Amplifier (""DA"") (one SDI input, and two or more SDI outputs) for each camera so you can ""duplicate"" the camera video to send to both the production switcher and to your camera iso recorders. I use DAs I buy on Ebay.  My current favorite is ""AVUE 3G-SDI/HD-SDI/SDI 1 to 2 Repeater &amp; distribution extender with re-clocking"" at around $100</p>

<p>Or you could use something like the Blackmagic Smart VideoHub 12x12 if you have the budget for it.  Of course, you will need a separate recorder for each video stream. It seems unlikely that you can successfully use more than one UltraStudio Mini Recorder per computer.</p>

<p>The camera has a built-in tally light and the switcher sends the tally signals back to the cameras (assuming you use TWO SDI cables per camera!)  The tally light appears to be quite small and hidden just above the lens mount, so it seems of limited visibility unless you have a very small lens and are very close to the camera. There does not appear to be any kind of output for an external tally that can be seen at a distance one would expect in the Real World, or any tally provision for a camera operator. Are you planning on using these as human-operated cameras?  Or remote-controlled?  Or fixed POV cameras?</p>

<p>The Micro Studio Camera 4K has intercom built-in, with a 3.5mm TRRS socket for a headset as one would use with an iPhone, etc.  However you will need an ATEM Talkback Converter to access the intercom/talkback back at the switcher end.</p>

<p>Remember also that those cameras do not come with lenses, and they use a rather premium lens mount, so you may end up spending 2x or 3x for each camera by the time you hang some glass on the front of them.  The Blackmagic stuff seems like great performance for the money, but not necessarily when you add up all the required ""accessories"".  Especially if you want to equip the camera(s) with operator-controlled lenses. You could end up with 4x-5x the base price for each camera.</p>

<p>Remember also that SDI is quite high frequency and doesn't tolerate ""very long SDI cable runs"".  My working rule is that even with large RG-6 cable, 300ft/100m is the practical length limit. If you want to go longer than that, those fiber optic boxes from Blackmagic are one solution if you have the budget for that.</p>
","19801"
"Deinterlace by dropping even lines from command line","690","","<p>I want to make an interlaced video with 50fps into a 25fps progressive, in the process i'm also reducing the size of the frame. Currently i am using avconv and there are deinterlacing options, however, they are not documented. So what i am afraid of is that they will actually start bleeding information i am trying to drop into the frames that i am trying to keep.</p>

<p>The only tool that i have found, that does a documented drop of even/odd frames is a plugin for AVIDemux, which unfortunately does not export into Ogg or even encode into Theora. When i attempt to dump into Huff codec (to not introduce quality loss due to reencoding) it demands that i split files at 4 GiB. So i can only use it for small videos.</p>

<p>Is there another tool that can help me?</p>
","<p><code>ffmpeg -i input -vf field,scale=123:456 output</code> will do the trick, where 123:456 is your x:y frame dimensions for the output. The field filter just chucks away the fields it doesn't use. Documention on the field filter <a href=""http://ffmpeg.org/ffmpeg-all.html#field"" rel=""nofollow"">here</a>. Note that you'll need extra stuff in the command to get good results - e.g. you'll probably want to specify codec details and so on.</p>
","9971"
"Best render options for custom size movie in after effects","689","","<p>I have a composition of 1500 x 643. I need this format for a moviebanner in a website. My assets are in 4K, very good quality.</p>

<p>I can render it as an AVI, best quality. Then it's almost 4 GB for 30 seconds.</p>

<p>When I choose H264, it has problems with framerate and the size (it resizes, ...). What do I have to choose as render settings to have a good quality movie (+30mb? instead of 4Gb), for web with my custom size?</p>

<p>I use After Effects CS5.</p>

<p>EDIT: After changing the settings to an odd resolution, I can export as H.264.. 
But I don't like the quality. The size is 3.3Mb, it could be a little bit more.
How can I get better quality for this export?</p>

<p>My screens:
<a href=""https://i.stack.imgur.com/nWHUL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nWHUL.png"" alt=""No Adobe Media Encoder Queue""></a>
<a href=""https://i.stack.imgur.com/dhegd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dhegd.png"" alt=""Composition settings""></a>
<a href=""https://i.stack.imgur.com/BGzrU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BGzrU.png"" alt=""Export settings""></a>
<a href=""https://i.stack.imgur.com/bkvZc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bkvZc.png"" alt=""Export settings 2""></a>
<a href=""https://i.stack.imgur.com/7He34.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7He34.png"" alt=""Bitrate""></a></p>

<p>The results can you find here: <a href=""https://wetransfer.com/downloads/a15013dad2f4bc5c0b4ce0aa92cb821c20170508063926/347bd4"" rel=""nofollow noreferrer"">https://wetransfer.com/downloads/a15013dad2f4bc5c0b4ce0aa92cb821c20170508063926/347bd4</a></p>
","<p>Are you using the inbuilt After Effects render queue? It's a bit easier to use custom settings when you use the Adobe Media Encoder instead. AME comes with After Effects and is installed automatically when you install AE. Simply open the composition you want to export in After Effects and select <em>Composition</em>  <em>Add to Adobe Media Encoder Queue</em>. This will open AME and add the current state of your composition to the render queue (this might take some time). Once you see the item in the render queue, press the highlighted format designation to open the Export Settings dialogue. </p>

<p><a href=""https://i.stack.imgur.com/e6NGz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e6NGz.png"" alt=""AME render queue""></a></p>

<p><a href=""https://i.stack.imgur.com/gDGWj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gDGWj.png"" alt=""AME export settings""></a></p>

<p>In the Export Settings, you can change the format (codec/container) as well as the other export parameters. Make sure to set the dimensions (Width/Height) to whatever you need them to be. To bring down the size, the most important option is the bitrate setting (<em>Target</em> / <em>Maximum Bitrate</em> for H.264).</p>

<p>Read <a href=""https://video.stackexchange.com/questions/16051/rendered-two-videos-from-adobe-premiere-with-the-same-settings-but-different-res/16056#16056"">this</a> for an explanation of bitrate and it's relation to the resolution of your video and <a href=""https://video.stackexchange.com/questions/18621/how-to-export-maximum-quality-in-minimum-size/18623#18623"">this</a> for the math you need to figure out the appopriate bitrate dependent of your desired output size. Since you mentioned you need the video for your website, you might also be interested in <a href=""https://video.stackexchange.com/questions/14728/encode-settings-for-html5-background-video/14730#14730"">my post here</a> regarding recommended encoding settings for web video. </p>
","21344"
"How is possible for YouTube and Vimeo transcode so fast?","687","","<p>For the bit I read YouTube and Vimeo rely on servers powered by some modified libs i guess like libavcodec and tools like ffmpeg in a serious custom build.</p>

<p>Let's assume a transcode to h264 but the same could be applied to other codecs, I think VP8 and VP9 works in the same way.</p>

<p>If I upload 1h video file on their servers, right after the upload i have all the versions ready... pratically like 1 seconds after.</p>

<p>But, if codecs doesn't scale horizontally in a good way unless you want to trade time vs compression (see h264) and I'm pretty sure this websites care about the size of the videos..</p>

<p>Given also that single threads speed are capped by CPU frequency and actual frequency isn't up to 3.5 ghz generally unless they use overclocked servers under liquid nitrogen :D</p>

<p>How the transcode is done so fast?</p>

<p>They use still multithread with a mix of settings, trading on initial file size, custom builds and everything else to have just the files insta-ready to the user and then continue a second transcode with a slow preset that will be swapped as soon this file will be ready? They start to transcode as soon they receive the file stream (aka: during the upload ?)</p>
","<p>A common method is split-and-stitch where the file is cut into pieces and sent to multiple servers for transcode. That way you can transcode a file of any length in a fixed amount of time.</p>

<p>Telestreams Episode Engine can do this, but I'm sure Google uses something custom coded. </p>
","16935"
"Re-encode with ffmpeg using libx265 - rawvideo","686","","<p>I have a script that is going through a media library and re-encoding files to HEVC with ffmpeg and using libx265. The default command I use is:</p>

<pre><code>ffmpeg -i ""$origFile"" -map 0 -c copy -c:v libx265 -preset slow -crf 28 ""$newFile""
</code></pre>

<p>However, for some files this process fails, it outputs the following information about the video:</p>

<pre><code>Output #0, matroska, to 'test.mkv':
  Metadata:
    encoder         : Lavf56.15.105
    Stream #0:0: Video: hevc, none, q=2-31, 128 kb/s, SAR 1:1 DAR 0:0, 23.98 fps
    Metadata:
      encoder         : Lavc57.14.100 libx265
    Stream #0:1: Audio: mp3 (U[0][0][0] / 0x0055), 44100 Hz, stereo, 128 kb/s
Stream mapping:
  Stream #0:0 -&gt; #0:0 (rawvideo (native) -&gt; hevc (libx265))
  Stream #0:1 -&gt; #0:1 (copy)
</code></pre>

<p>And then, I just get the wall of errors that are printed below here: </p>

<pre><code>[rawvideo @ 000000c56052c920] Invalid buffer size, packet size 295 &lt; expected frame_size 1
769472
Error while decoding stream #0:0: Invalid argument
[rawvideo @ 000000c56052c920] Invalid buffer size, packet size 362 &lt; expected frame_size 1
769472
Error while decoding stream #0:0: Invalid argument
[rawvideo @ 000000c56052c920] Invalid buffer size, packet size 5926 &lt; expected frame_size
1769472
Error while decoding stream #0:0: Invalid argument
[rawvideo @ 000000c56052c920] Invalid buffer size, packet size 406 &lt; expected frame_size 1
769472
Error while decoding stream #0:0: Invalid argument
</code></pre>

<p>I've tried following some other advice, since when I try and read the codec, it says ""rawvideo"", and using commands like <code>ffmpeg -f rawvideo -pix_fmt yuv420p....</code>, but that also does not work. If anyone has any insight or ability to help explain how to overcome this issue, it would be really appreciated. I've included the ffprobe output for this particular file as well if that is helpful:</p>

<pre><code>ffprobe version N-76479-gc878082 Copyright (c) 2007-2015 the FFmpeg developers
  built with gcc 5.2.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --e
nable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-li
bass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libf
reetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libm
p3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable
-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --en
able-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libv
o-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enabl
e-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-deckli
nk --enable-zlib
  libavutil      55.  5.100 / 55.  5.100
  libavcodec     57. 14.100 / 57. 14.100
  libavformat    57. 14.100 / 57. 14.100
  libavdevice    57.  0.100 / 57.  0.100
  libavfilter     6. 14.101 /  6. 14.101
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.100 /  2.  0.100
  libpostproc    54.  0.100 / 54.  0.100
Input #0, avi, from '109 - The Voices of the Dead.avi':
  Metadata:
    encoder         : Lavf56.15.105
  Duration: 00:58:35.47, start: 0.000000, bitrate: 359 kb/s
    Stream #0:0: Video: rawvideo, bgr24, 1024x576, 219 kb/s, SAR 1:1 DAR 16:9, 23.98 fps,
23.98 tbr, 23.98 tbn, 23.98 tbc
    Stream #0:1: Audio: mp3 (U[0][0][0] / 0x0055), 44100 Hz, stereo, s16p, 128 kb/s
</code></pre>
","<p>For files already encoded to AVI, you'll have to extract the video to a raw HEVC stream first:</p>

<pre><code>ffmpeg -i in.avi -c copy -map 0:v -f hevc in.265
</code></pre>

<p>and then</p>

<pre><code>ffmpeg -framerate 30 -i in.265 -i in.avi -c copy -map 0:v -map 1:a -video_track_timescale 1k fixed.mp4
</code></pre>

<p>Replace the framerate value with the true rate of the video.</p>

<hr>

<p>Encode new files to MP4 or MKV.</p>

<pre><code>ffmpeg -i ""$origFile"" -map 0 -c copy -c:v libx265 -preset slow -crf 28 ""newFile.mp4""
</code></pre>
","20639"
"Spec for GPAC's DashCast configuration?","684","","<p>According to <a href=""http://gpac.wp.mines-telecom.fr/dashcast/"" rel=""nofollow"">DashCast</a>, we should provide a configuration file of several entries. The document demos some options:</p>

<pre><code>[v1] 
type=video 
width=1280 
height=720 
bitrate=800000 
[v2]
type=video 
width=640 
height=360 
bitrate=400000 
[a1]
type=audio
bitrate=128000
[a2]
type=audio
bitrate=256000
</code></pre>

<p>For my first run of DashCash, the configuration file would be automatically generated if it is not existing. Here is the autogenerated one:</p>

<pre><code>[v1]
type=video
bitrate=2000000
framerate=24
width=1920
height=1080
crop_x=0
crop_y=0
codec=libx264

[a1]
type=audio
bitrate=192000
samplerate=44100
channels=2
codec=aac
</code></pre>

<p>I'm wondering where I can get a spec of options for this configuration file?</p>
","<p>This is available on the page you linked to directly.  Look under the Configuration Files section.</p>

<p>Relevant details from the page:</p>

<p>A file ""-conf"" file.</p>

<pre><code>[ID]        # The ID of the representation
type=T      # T can be ""audio"" or ""video""
width=W     # if type is ""video"", 
            # W is the width of the representation
height=H    # if type is ""video"", 
            # H is the height of the representation
bitrate=B   # B is the bitrate (bps) of ""video"" or ""audio""
</code></pre>

<p>And a ""-switch-source"" file.</p>

<pre><code>[ID]         # The ID of the source identification.
type=T       # T can be ""video""
start=ST     # ST is the start time YYYY-MM-DD hh:mm:ss
end=ET       # ET is the end time YYYY-MM-DD hh:mm:ss
source=S     # S is the address of the source
</code></pre>

<p>Additionally, from looking in the source itself, the configuration file is read in the cmd_data.c file starting on line 160 or so (depending on rev) in the dc_read_configuration method.</p>

<p>For the conf file, video type can have codec, framerate, crop_x, crop_y and custom.  Audio types can have codec, samplerate, channels and custom.</p>

<p>Custom appears to push the value assigned directly in to the libav codec.</p>

<p>For the Switch source file, the type appears to actually be able to be video or audio.  Both only have the listed 4 options.</p>
","12079"
"How do I rescue a Final Cut 7 project that elicits ""File Error: Wrong Type"" when I try to open it in FCP?","683","","<p>When I try to open this project, Final Cut Pro 7.0.3 running on Mavericks pops up an error box that says ""File Error: Wrong Type.""</p>

<p>I've fixed this in the past using the Project Repair tool in Digital Rebellion's great <a href=""http://www.digitalrebellion.com/promaintenance/"" rel=""nofollow"">Pro Maintenance</a> set of tools but I'd like to find a free option if possible.</p>

<p>I've seen people discuss setting the file type and creator, possibly using a command line tool, SetFile, that is part of the OS X developer tools but none of them seemed to have complete instructions and there were a number of claims of it not working anyway.</p>
","<p>After installing SetFile, go to terminal and execute this command:</p>

<pre><code>setfile -c ""KeyG"" -t ""FCPF"" INSERT_FILENAME_HERE
</code></pre>

<p>According to SetFile's man page, ""-c"" sets the creator and ""-t"" sets the type.</p>

<p>I found what the type and creator should be set to in Peppy's second post on this thread: <a href=""http://hintsforums.macworld.com/archive/index.php/t-16983.html"" rel=""nofollow"">http://hintsforums.macworld.com/archive/index.php/t-16983.html</a></p>

<p>SetFile can be installed as part of the Xcode Command Line Tools by opening Terminal and typing:</p>

<pre><code>xcode-select --install
</code></pre>

<p>But if I recall that sometimes fails so you might also try downloading it from the <a href=""https://developer.apple.com/downloads/index.action"" rel=""nofollow"">Apple Developer</a> site.</p>
","10548"
"Will Canon 600D be able to do these?","683","","<p>A few days ago I watched Chris Nolan's Following and thought to myself ""I can do something like this"".</p>

<p>I have found out that Nolan used Bolex for the film. He still uses film cameras and is against digital cameras.</p>

<p>I would like to create shots that are reminiscent of La Haine , Following and the older films like Death on the Nile etc.</p>

<p>I found Canon 600D for about 550-600 dollars ( and various versions with different lenses) .</p>

<p>Is it good for a filmmaking beginner? I want to create old-film style setups. Maybe some car interior shots. Timelapses. 4:3 videos.</p>

<p>Which lens should I use for old film looks? Do you recommend cheap tripod and external mics? I'm open to any help.</p>

<p>Also: Whats the difference between IS and DC lens. DS is much cheaper.</p>
","<p>Yes, DSLRs are a good starting point for doing movie type projects.  The use of interchangeable lenses and the shallower depth of field offered by larger sensors than found in most consumer video cameras allows for far more artistic options.  This is why DSLRs are very popular with the amateur film making community.</p>

<p>It should be noted, however, that they are not without their drawbacks.  All but the most recent DSLRs (including the 600d) lack auto-focus while shooting video, so you will have to either focus before shots or manually adjust your focus during the shot.  (If you move up to the 650d instead and use STM lenses, it IS able to auto focus when shooting video.)  Focus peaking can help with this by providing highlighting of the most in-focus points in the image, but it requires the use of custom firmware software like Magic Lantern.  (I've personally used ML, but it isn't without some small degree of risk to your camera.)</p>

<p>Additionally, DSLRs suffer from something known as a rolling shutter.  On a high end video camera, the entire image for each frame is captured at the same time, thus quick pans look crisp.  On a DSLR however, the sensor is scanned line by line when shooting video, so fast pans result in a ""jello"" effect (which is well documented with numerous videos demonstrating the problem on Youtube).</p>

<p>It is also worth noting that you do still get what you pay for in quality.  A near bottom of the line consumer DSLR, particularly with a dirt cheap kit lens that comes with it is going to have much more marginal quality than anything professional.  Even a higher end entry level model like a t4i (650d) or better yet, a mid-range model like a 70d will have far superior quality.  Similarly, replacing the kit lens with something like a 50mm f/1.8 prime or other better quality lens will result in a substantial improvement in video quality.</p>

<p>It still won't be anything near Hollywood level though.  About the closest you can get to Hollywood quality for under $7,000 is a 5D Mark iii running ML and shooting RAW footage on high end L optics, but even at $3,500 for the camera body and $1,500 to $2,500 for the lens, the quality still ends up being noticeably lower (to the professional eye) than a professional video camera like an ARRI Alexa (which costs $50,000+), but it is close enough to raise eyebrows in the professional community.</p>

<p>That all said though, DSLRs (even the basic models) still offer more bang for the buck in terms of artistic control than almost any consumer level camera, just so long as you also realize they are harder to use for video than a consumer camera, which can handle more for you, but gives you less options.</p>
","12416"
"Ronin M vs. Helix Jr","682","","<p>Has anybody used the new DJI Ronin-M and compared it to the Letus Helix Jr.?
I'm considering a gimbal purchase for my DSLR videography and haven't seen anything that tells me why the Helix is twice the price of the Ronin-M.</p>
","<p>For half the price and DSLRs only, the Ronin M probably makes more sense. The Helix Jr. makes sense for more. It can hold up to 12 lb of stuff vs the M's 8 lb (which is kind of crazy, considering that it's even lighter), and the fact that it's naturally at heart-ish level as opposed to belt level means a lot for good framing and the ability to hold it at a good frame for an extended period of time. You can get it to eye level for more easily than the Ronin, even in Upright mode (or whatever it's called), because it flips the camera upside down. (They both have briefcase mode for very low shots.)</p>

<p>If you want a second person with total control, the Ronin is absolutely the way to go. But if you're a one-man band and can afford the premium, the Helix is probably the better option.</p>

<p>Source: I have demoed both.</p>
","15504"
"Console Audio and Commentary Capture - Is it possible to capture both?","681","","<p>I've been scouring google to see if it's possible to record gameplay audio for consoles as well as live commentary from a microphone or headset. Most of what I come across, from either forums or product FAQs say that it's possible to do one but not both. On PCs, it's easy, but on consoles you can only do one. Or so I'm told.</p>

<p>However, I see quite a few videos on Youtube from consoles with both game audio and live commentary. So it must be possible.</p>

<p>Can anyone tell me how how it's possible to do both on a console? I know nothing about video editing and production so I'm curious what devices and programs are needed to accomplish this. Also, is it possible to record the console audio and live commentary as two separate tracks?</p>

<p>Thank you for your time! I appreciate any help.</p>
","<p>As I understand it you want to record and not stream your gameplay live via twitch or other live streaming services?</p>

<p>In that case you just record your game video and audio with your desired method (f.e. a PC capture card) and then you just record you voice at the same time with your PC/Laptop/Smartphones microphone input. When you are done recording you just sync your commentary audio file with your game footage later on using a video editor.</p>

<p>Something that could help to sync game footage and commentary would be to record a small bit of the game audio with your microphone over your speakers before starting your commentary, then you can easily sync your commentary by alligning it with the game audio manually. Thats how you usually sync multiple audio sources the cheap way when filming, another method would be to use a mixer like AJ Henderson suggested which will keep everything in sync for you but also costs money.</p>

<p>Popular video editors are Adobe Premiere and Apple Final Cut. A good free editor would be Lightworks.</p>
","12530"
"fix a few videos with bad white balance?","681","","<p>I have a few video that i made with my olympus though and i made the mistake of not setting my white balance to sunny (or auto)</p>

<p>it was set to incandescent while being outdoor scenes.</p>

<p>now I got a few blueish videos</p>

<p>what is the best way to fix this? </p>

<p>can it be done with a free / opensource product?</p>
","<p>I have used virtualdub with avisynth <code>ColorYUV(autowhite=true)</code></p>

<p>it seem to have done an ""ok"" job</p>
","15964"
"Smooth horizontal and vertical camera movement","677","","<p>I have a Canon EOS 60D camera. I want to use this to take some video by moving the camera smoothly in horizontal and vertical axis. For example, if I want to take video of the bezel of a TV, I'd like to be able to smoothly have the camera move down on the vertical axis with the lens facing the bezel in the same angle the whole time.</p>

<p>Something like this, but not this expensive</p>

<p><img src=""https://i.stack.imgur.com/QW1Ny.gif"" alt=""enter image description here""></p>

<p>Here is an example at 3:40 when the camera man slides through the keyboard horizontally.</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/gFWR8r5sdWY?start=0""></iframe>
            </div></div></p>
","<p>Unfortunately, there aren't really cheap options to do this.  You can approximate it using a crane or a stabilization rig (like a SteadiCam Jr), but both of those is still going to cost you hundreds of dollars.  </p>

<p>For horizontal movement, the cheapest option is probably to use a more basic track and roll it by hand.  You can use a motion control unit to get smoother motion, but a motion control unit itself is going to be over a grand normally.  </p>

<p>For pure vertical, you need something called a camera pedestal.  Unfortunately, this is probably the most complex movement to perform in a controlled manner, so there simply are no cheap options.  There are some DIY approaches out there, but you're looking at expensive hardware if you want to do a pure vertical movement with good control.  (The cheapest commercial pedestal I know of is over a grand.)</p>

<p>That is just the way the market is for being able to do advanced moves.  They are all specialized, limited use devices and the cost rapidly multiplies with the weight of the camera.  That's why rigging costs so much, because the market is so limited and specialized.  It requires careful design, tight tolerances and a very limited market, that all adds up to big costs to the consumer.</p>
","9789"
"Are ""mp2v"" and ""mpgv"" valid MPEG-2 codec tags in an MOV?","674","","<p>This question is related to <a href=""https://video.stackexchange.com/questions/17526/what-is-the-correct-mpeg-2-fourcc"">What is the correct MPEG-2 FourCC?</a> and to an open discussion <a href=""http://ffmpeg-users.933282.n4.nabble.com/MPEG-2-video-m2v1-vs-mp2v-tp4674329p4674338.html"" rel=""nofollow noreferrer"">currently on the ffmpeg-user mailing list</a>:</p>

<blockquote>
  <p>I am wondering if I should file a bug report about the MOV output. Open 
  questions in that vein, though, are: </p>
  
  <p>...</p>
  
  <ul>
  <li>Is it a bug that mpg2 and mpgv cause MOV output to fail or is that 
  correct behavior? </li>
  </ul>
</blockquote>

<p>There seem to be numerous possible tags in use for MPEG-2 video, in particular <code>m2v1</code>, <code>mp2v</code>, <code>mpg2</code>, and <code>mpgv</code>.</p>

<p>Now, ffmpeg fails with e.g. ""Tag mpgv/0x7667706d incompatible with 
output codec id '2' (m2v1)"" when the output is MOV and either <code>mpg2</code> or <code>mpgv</code> is specified:</p>

<pre><code>ffmpeg ... -vcodec mpeg2video -f mov ...             # succeeds (m1v1)
ffmpeg ... -vcodec mpeg2video -vtag mp2v -f mov ...  # succeeds
ffmpeg ... -vcodec mpeg2video -vtag mpg2 -f mov ...  # fails
ffmpeg ... -vcodec mpeg2video -vtag mpgv -f mov ...  # fails
</code></pre>

<p>All of those succeed for AVI. Ffmpeg's default tag for MPEG-2 in AVIs is <code>mpg2</code>.</p>

<p>Part of a bug report that I am writing for ffmpeg's MOV output handling may include the <code>mpg2</code> and <code>mpgv</code> failure behavior (with the desired behavior being success), but before I do that I need to be certain that this behavior is a bug rather than appropriate behavior.</p>

<p>So my question boils down to this: Are <code>mpg2</code> and <code>mpgv</code> valid, allowable alternative tags for MPEG-2 video <strong>in an MOV file</strong>, and are there others? I am looking for official Apple sources and MOV specifications rather than anecdotal evidence or lists of generally observed FourCC's / codec IDs.</p>

<p>I know that, at least <a href=""https://en.wikipedia.org/wiki/Comparison_of_video_container_formats"" rel=""nofollow noreferrer"">according to Wikipedia</a>, MPEG-2 video in an MOV is an acceptable combination, I'm just not sure what the acceptable set of codec tags are in MOVs. This question is not about the ""correct"" MPEG-2 ID in general cases, it is about the allowable variants specifically in MOVs.</p>
","<p>MPEG-2 doesn't seem to be an officially sanctioned codec for the Quicktime container. The official specification has a section related to <a href=""https://developer.apple.com/library/mac/documentation/QuickTime/QTFF/QTFFChap3/qtff3.html#//apple_ref/doc/uid/TP40000939-CH205-BBCGIJJI"">MPEG-1</a> in MOV but not MPEG-2. The US Library of Congress also does not have an entry for the <a href=""http://www.digitalpreservation.gov/formats/fdd/fdd000052.shtml"">subtype</a> MPEG-2 in MOV, like it does for MPEG-1 (just called QTV_MPEG).</p>

<p>So, there's no official word on ""allowable alternative tags"". One can only examine MPEG-2 MOVs and see whether apps, specifically, Apple apps, play them or not and prepare a survey report. Since each app has its own fiat on whether it plays a file with a certain codec ID or not, that report whould be categorized by app i.e. VLC, Quicktime player. FCP..etc.</p>

<p>As an example, VLC recognizes the <a href=""https://github.com/videolan/vlc/blob/9053b0919f00d88f16e1ef3e3f79b1cced74f742/src/misc/fourcc_list.h"">following tags</a> for generic MPEG-2 i.e. not IMX or XDCAM:</p>

<pre><code>    A(""mpeg""),
    A(""mp2v""),
    A(""MPEG""),
    A(""mpg2""),
    A(""MPG2""),
    A(""H262""),
</code></pre>
","17533"
"Using a parameter file for filter option of libav (or ffmpeg)","674","","<p>Good time of the day,</p>

<p>I have heard that such a feature exists, but i've lost the reference and have been unable to find it now. I am looking for a way to use the file as the input for all the fileter options passed to libav's avconv (or ffmpeg).</p>

<p>I know i can do that via a command line with -f or -vf, but there are two problems: i often reuse the same settings and sometimes i need to reuse the particular approach after bash has already lost that particular command line from history.</p>

<p>If i'm mistaken and there's no way to do that, also kindly let me know.</p>
","<p>You can use the <code>-filter_script</code> option:</p>

<pre><code>$ ffmpeg -i input -filter_script filtering -codec:a copy output.mkv
</code></pre>

<p>In this example, <code>filtering</code> is a file with the filtergraph.</p>

<pre><code>$ cat filtering 
scale=640:-1,negate
</code></pre>

<p>I believe this only works for <a href=""http://ffmpeg.org/ffmpeg.html#Simple-filtergraphs"" rel=""nofollow"">simple filtergrahps</a> that contain exactly one input and one output.</p>

<p>Libav stuff (avconv and their fake, so-called ""ffmpeg"" version) is buggy. You can simply download a recent <a href=""http://ffmpeg.org/download.html#LinuxBuilds"" rel=""nofollow"">Linux build of ffmpeg from FFmpeg</a> or follow a <a href=""https://ffmpeg.org/trac/ffmpeg/wiki/CompilationGuide"" rel=""nofollow"">step-by-step guide to compile ffmpeg</a>.</p>
","9545"
"Interactive YouTube video navigation","670","","<p>I want to create a youtube video based on the technique used in this video: <a href=""https://www.youtube.com/watch?v=m97O1dQAyhc"" rel=""nofollow"">Interactive Trailer</a>. How can I make annotation(spotlight) on youtube that will link to a video and start at various times.  (Like the buttons on the bottom of the video.)</p>

<p>Example: </p>

<p>If I click the spotlight 5 sec, then step the slider to 15 sec. </p>

<p>If I click the spotlight 6 sec, then step the slider to 16 sec.</p>
","<p>You'll need to make a separate spotlight for each different link.  So one spotlight goes from 5 seconds to 6 seconds, and another spotlight goes from 6 seconds to 7 seconds.  Each of them starts your linked video at a different point (15 seconds, 16 seconds, or whatever).</p>

<p>It's easy to set the starting time within an annotation link.  When you create the annotation, and add the link URL, it will ask you about the starting point.  (The default is 0.00, the beginning of the video.) You just set it there before you publish your annotations.</p>

<p>Note that annotations don't yet work on mobile devices (as far as I know).</p>

<p><a href=""https://support.google.com/youtube/answer/92710?hl=en"" rel=""nofollow"">Here's a useful page</a> that instructs you how to create and use annotations, in case you need some more info.</p>
","10657"
"After Effects: controlling slider in ExtendScript","668","","<p>I'm newbie in After Effects. I've learned how to control slider from expressions. Now I need to get value of slider in script (jsx/extendscript). How can I achieve that?</p>

<p>I've tried (in *.jsx): <code>var a = comp.layer(""Slider 1"").effect(""Slider Control"")(""Slider"")[0];</code></p>

<p>Result: </p>

<blockquote>
  <p>After Effects error: Unable to call ""setValue"" because of parameter 1.
  Value ""NaN"" of element 0 in the array is not a valid float.</p>
</blockquote>

<p>It means that value of slider is not set yet?</p>

<p>Thanks</p>

<p><strong>EDIT:</strong></p>

<p>Here is a example:</p>

<pre><code>var slider = comp.layers.addNull();
slider.name = ""Slider"";
slider.effect.addProperty(""ADBE Slider Control"")(""Slider"");
slider.effect(""Slider Control"").property(""Slider"").setValue(100);

var a = slider.effect(""Slider Control"").property(""Slider"");
var titleTriangle = comp.layers.addShape();
titleTriangle.name = (""Title Triangle"");
var titleTriangleGroup = titleTriangle.property(""Contents"").addProperty(""ADBE Vector Group"");
var titleTriangleFill= titleTriangleGroup.property(""Contents"").addProperty(""ADBE Vector Graphic - Fill"");
titleTriangleFill.property(""Color"").setValue([0.68,0.29,0.29,1]);

// using 'a' here
titleTriangle.property(""Position"").setValue([a,188]);
</code></pre>
","<p>I found out - <strong>.value</strong> after slider.effect(""Slider Control"").property(""Slider"") and it works.</p>

<pre><code>var a = slider.effect(""Slider Control"").property(""Slider"").value
</code></pre>
","18242"
"Common newbie mistakes for cinematography?","667","","<p>I'm going to make a serious movie, what are the BIG mistakes newbies often do, both in the clipping and the shooting stage?</p>
","<p>Common oversights of beginners in pre-production:</p>

<p>1) No script or plan of action. Even if you are doing something very abstract or experimental, have a plan of how things begin, develop and end. You can always change this as you proceed, but it's most important to have a baseline. This should include your budget of money and time for all concerned.</p>

<p>2) No research. If you are shooting a documentary or a narrative be sure to do all the fact checking or if this is fiction, make it plausible or not if you desire the comical.</p>

<p>3) No storyboard. The storyboard is a series of panels like a comic book that describes each scene and the action contained within. This is how the camera will set up for each shot. From the storyboard you can derive a shot list.</p>

<p>4) No idea of screen direction. Right from the storyboard you should have a good handle on the direction the action moves within the frame as well as taking into account how the camera may move to cover such action whether that is a pan, tilt, crane, handheld, etc. shot. The key here is continuity of movement from shot to shot. Learn about screen direction and cover the 180 degree rule as this also falls under continuity.</p>

<p>5) No production schedule. Most movies do no shoot each scene in the scripted sequence because you can save a lot of time and money by shooting by location, time of day, or other logistical considerations. I once worked as an Hollywood extra in ""Indiana Jones and the Temple of Doom"". The last shoot or wrap night as they called it was actually a sequence near the beginning of the film. Plan your shooting schedule to optimize each location as appropriate.</p>

<p>Common oversights of beginners in production:</p>

<p>1) Not locking down exposure, focus, and white balance.</p>

<p>2) Terrible lighting. Learn about shooting with existing light, and how to make it work with reflectors if need be. If you are shooting indoors, learn about basic 3 point lighting to start. The most important thing is to make sure your subject is lit well and defined.</p>

<p>3) Terrible sound recording. Learn the basics of sound recording, like setting the level so it does not exceed peak load and not changing the recording level while recording. Also, get creative with sound too, learn about Foley and explore ways to create your own effects.</p>

<p>4) Camera shake: it's one thing to shoot hand held but if you are trying for a steady tripod shot, make sure you don't touch the camera when its recording. For instance one of my cameras has a remote so I can start and stop it with out even getting near the camera. Also, if you are using a tripod be sure that it has enough weight to handle the wind. For hand held shots shoot wide angle as vibration is proportionately magnified by increasing focal length (zooming in).</p>

<p>5) Over zealous use of zooming. Learn to use the zoom feature when it is transparent and does not call attention to itself. For instance a more effective use of zooming in or out is done simultaneously with a pan or tilt (assuming you have a tripod with a fluid head) while following an action. A good example of this is how a sports cameramen will follow a play such as someone trying to steal 2nd base.</p>

<p>6) Learn to compose a shot. Use the law of thirds in framing a subject, and when to make exceptions. I highly recommend art history 101 for beginners to learn about composition. Study the greats from Leonardo da Vinci to Pablo Picasso to understand how to frame your subject and study film history 101 to learn how great directors worked with framing in motion, e.g., Orson Welles' work in ""Citizen Kane"". Others: Billy Wilder, Alfred Hitchcock, Akira Kurosawa to name a few.</p>

<p>7) Didn't get enough takes to cover the action. Make sure that you have enough good takes of each shot on the shot list. Be sure to allow some flexibility with adding long shots, medium shots, and close ups all with different angles to optimize the light and action. Shooting more is better than less from an editing point of view. Editors love to have tons of stuff to choose from as long as these extra shots highlight the intent and the director is behind it.</p>

<p>Common oversights of beginners in post-production:</p>

<p>1) Wrong format. Be sure that your camera files are compatible with your video and music/sound editing suite.</p>

<p>2) Too much time on one clip. Unless you are using a steady moving crane shot don't expect to keep anyone's attention if your shots are too long, and don't have any sense of movement or action. Having a wide variety of long, medium, and close up shots with varying angles to choose from that cut swiftly on action and appear transparent will be more engaging to your audience.</p>

<p>3) Jump cuts: this is all about continuity, learn when and when not to use a jump cut.
One acceptable form of the jump cut is the 'magic cut' such as some one suddenly appears in the frame like in Star Trek when they beam up. A bad jump cut is when something that may have been in the previous scene is misplaced in the following shot or the reverse is true too. There are a zillion examples of jump cuts. One famous one is Dustin Hoffman driving the wrong way on the SF Bay Bridge in ""The Graduate"".</p>

<p>4) Over use of effects. There is a huge temptation to over use an effect you may find interesting, even experienced cinematographers can easily over use something new to them and unfortunately your audience finds this boring and distracting. The golden rule is to use effects sparingly but appropriately when used. The best way to learn this is to see how this effect has been successfully used before by professionals in great movies.</p>

<p>5) Non-original music. Too many beginners use popular music for their productions. Even if it appears to fit, we've heard it already. Does ""Also sprach Zarathustra"" have to always play for a sun rise or moon rise? The question you should ask yourself is, how many buddies of mine play music, how many are good and want to work on it with me? Got a friend that can compose? Original music makes any production refreshing and takes your film from the ordinary to the unique as long as you make the music interesting and supports the film action.</p>

<p>6) Pay attention to the basic rules but don't be afraid to take some risks. My number one rule is that if I break one I better have a darn good reason and it really has to work within the context of the film I am working on.</p>
","3267"
"How to cutting a video and keep all format and metadata","666","","<p>I want to cut long videos into several smaller parts without changing any format and metadata from the original video.</p>

<p><a href=""https://i.stack.imgur.com/0y23G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0y23G.png"" alt=""Original""></a></p>

<p>I used these commands</p>

<blockquote>
  <p>ffmpeg -i input -ss 00:00:05 -t 100 -c:a copy -c:v copy output</p>
  
  <p>ffmpeg -i input -ss 00:00:05 -t 100 -metadata ""major_brand=mp42, minor_version=0"" -c:v copy -c:a copy output</p>
  
  <p>ffmpeg -i input -ss 00:00:05 -t 100 -map_metadata 0 -c:v copy -c:a  copy output</p>
  
  <p>ffmpeg -i input -ss 00:00:05 -t 100 -map_metadata 0 -b:v 460k -c:a  copy output</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/PKJmd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PKJmd.png"" alt=""Result""></a></p>

<p>but still get different results from the original video.</p>

<p>Is there another command that I can use for cutting the video with metadata results identical to the original one ?</p>

<p>Thanks</p>
","<p>FFmpeg only allows you to alter/override the major brand, not the minor.</p>

<p>Command is </p>

<pre><code>ffmpeg -i input -ss 00:00:05 -t 100 -brand mp42 -c:v copy -c:a copy output
</code></pre>

<p><a href=""https://gpac.wp.mines-telecom.fr/downloads/gpac-nightly-builds/"" rel=""nofollow"">MP4box</a> will allow you to override both.</p>

<pre><code>mp4box -brand mp42:0 file.mp4
</code></pre>

<p>If you want to change the handler name as well, use</p>

<pre><code>mp4box -brand mp42:0 -name 2=""IsoMedia File Produce by Google"" file.mp4
</code></pre>

<p>(2 is the track index i.e. the 2nd track)</p>
","19185"
"Moving keyframes with duration change","666","","<p>I have a still image with four opacity keyframes which look like this:
<a href=""https://i.stack.imgur.com/jsTQ2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jsTQ2.png"" alt=""first""></a></p>

<p>When I change the duration my keyframes stay at the same place (which is normal).
<a href=""https://i.stack.imgur.com/IlSyK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IlSyK.png"" alt=""second""></a></p>

<p>Is there a way to move the keyframes automatically with duration change?</p>
","<p>Since this is a still image, what you can do is use the ""Rate Stretch"" tool (keyboard shortcut: <kbd>R</kbd>) and stretch the image shorter. If the keyframes don't move with the Rate Stretch tool, you can always create a ""Nested Sequence"" of just the image (Premiere Pro's rough equivalent to an After Effects precomp) and then rate stretch your nested sequence. This will stretch both the image and the keyframes together.</p>
","16148"
"mp4 to avi/mpeg conversion","666","","<p>I'm trying to convert an mp4 video to play on a digital photo frame. The photo frame (Living Image Li1210) manual states: </p>

<p>The digital photo frame can playback MPEG1, MPEG2, MPEG4, and AVI (dependant upon codec) video formats.</p>

<p>And in the FAQs: The device only works with the avi/mpeg4 files taken by most digital cameras. It is also capable of playing some compressed AVI files, such as DivX and XviD videos... The best supported resolution of a photo or video is 480x234.</p>

<p>I've tried various options with ffmpeg such as</p>

<pre><code>ffmpeg -i in.mp4 -vcodec mpeg4 -vf scale=480x234 out.avi
</code></pre>

<p>but can't seem to get anything but a blank screen on the photo frame.</p>

<p>Any help or advice would be greatly appreciated.</p>

<p>Thanks</p>
","<p>Apologies I'd being faffing with different scaling settings and must have got my files mixed up. The following (quoted in the question) does in fact work on this product - Living Images Li1210.</p>

<pre><code>ffmpeg -i in.mp4 -vcodec mpeg4 -vf scale=480x234 out.avi
</code></pre>

<p>However the following scaling seemed to produce better results when converting from 1280x720 (youtube standard download)</p>

<pre><code>ffmpeg -i in.mp4 -vcodec mpeg4 -vf scale=416x234 out.avi
</code></pre>

<p>And I think this addition from <a href=""https://video.stackexchange.com/questions/20399/whats-the-best-way-to-convert-mp4-to-avi"">this question</a> produced a slight improvement</p>

<pre><code>ffmpeg -i in.mp4 -q:v 6 -vcodec mpeg4 -vf scale=416x234 out.avi
</code></pre>
","20645"
"3D layers not affected by camera zoom in After Effects","662","","<p>The composition I'm working with has a 3D object I created in Cinema 4D Lite, as well as another layer which is controlled in After Effects (the content on the screen):</p>

<p><img src=""https://i.stack.imgur.com/leQM1.png"" alt=""Layers""></p>

<p>When I zoom in using the Camera layer the only thing affected by the zoom is the layer in After Effects, and the 3D layers stay in place.</p>

<p>Normal zoom:</p>

<p><img src=""https://i.stack.imgur.com/Eg0T6.png"" alt=""Normal zoom""></p>

<p>Increased zoom:</p>

<p><img src=""https://i.stack.imgur.com/UmxcM.png"" alt=""Increased zoom""></p>

<p>Is there a way that I can zoom in/out of the entire composition, including the 3D layer? I'd like to begin zoomed in entirely on the screen, and at one point zoom out and make it apparent that it's all taking place on the computer screen, which is 3D because it eventually rotates. </p>

<p>Thanks for any help.</p>
","<p>In After Effects, make sure your Cineware effect on the Cinema4D track has the Project Settings --> Camera set to Comp Camera, as shown below:</p>

<p><img src=""https://i.stack.imgur.com/tL67o.png"" alt=""screenshot""></p>

<p>This setting means that the camera in your After Effects comp will control the viewpoint of the object, instead of any camera in your Cinema 4D project (I think C4D creates a camera by default, even though you can't see it in the Objects panel, otherwise nothing would render).</p>
","15036"
"How to set pts time format when using ffmpeg filter to add timestamp","660","","<p>I want to add timestamp of current played time to a video, so I use this:
<code>
ffmpeg -i video.mkv -filter_complex ""drawtext=fontfile=/Library/Fonts/Arial.ttf: 
       text='timestamp: %{pts \: hms}': x=5: y=5: fontsize=16:
       fontcolor=yellow@0.9: box=1: boxcolor=blue@0.6"" -c:a copy -c:v libx264 -map 0 output.mkv
</code></p>

<p>This comes out that the timestamp was in hh:mm:ss:mm format, but I want it in hh:mm:ss format. So how can I achieve this goal?</p>
","<p>Use</p>

<pre><code>text='timestamp \: %{pts\:gmtime\:0\:%H\\\:%M\\\:%S}'
</code></pre>
","21907"
"How do I rig a Kino Flo to a gobo arm like this?","659","","<p>I want to know how to rig a 4-bank Kino to a gobo arm like this. I can tell they're using two arms, but I don't understand how it is attached to the first, nor do I know what mounting plate they have on the Kino. </p>

<p><a href=""http://provideocoalition.com/images/uploads/fbm-makeoutpict.jpg"" rel=""nofollow noreferrer"">film set with Kino on C-stand http://provideocoalition.com/images/uploads/fbm-makeoutpict.jpg</a></p>

<p>Most photos I've seen just have the Kino with its own lollipop, right into a knuckle. </p>

<p><a href=""http://tanklightsyouup.com/wp-content/uploads/2011/11/two-kinos.jpg"" rel=""nofollow noreferrer"">Back of Kino Flo on C-Stand http://tanklightsyouup.com/wp-content/uploads/2011/11/two-kinos.jpg</a></p>
","<p>The photo shows two grip heads, two arms and a c-stand. The kino would have its normal mounting plate (the one in the second photo).</p>

<p>The two grip arms are interlaced together; this makes them trombone-able (adjustable in length), stronger, and gives the gag a longer reach. One grip head attaches the arms to the c-stand, another attaches the kino to the arms.</p>

<p>I use this rig all the time; it works better if the stand is a junior stand and the grip arms are attached to the junior stand with a junior lolly-pop (much stronger).</p>

<p>Cardellini clamps can indeed clamp a stud to many things, but they aren't the best tool for this rig: you want a 5/8"" female receiver for the Kino, not a 5/8"" stud.</p>
","12149"
"Nuke particles escaping bounce node with geometry input","659","","<p>I created a sphere emitting particles in Nuke. Around that sphere is another sphere connected to a particle bounce which is set to kill the particles. Some particles seem to slip through though. Why is this, and how can I fix it?</p>

<p><img src=""https://i.stack.imgur.com/SodjL.png"" alt=""Nodes"">
<img src=""https://i.stack.imgur.com/RMLUT.jpg"" alt=""Viewer with particle escaping""></p>

<pre><code>    #! /Applications/Nuke9.0v3/Nuke9.0v3.app/Contents/MacOS//libnuke-9.0.3.dylib -nx
version 9.0 v3
define_window_layout_xml {&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;layout version=""1.0""&gt;
    &lt;window x=""0"" y=""23"" w=""1440"" h=""873"" screen=""0""&gt;
        &lt;splitter orientation=""1""&gt;
            &lt;split size=""910""/&gt;
            &lt;splitter orientation=""1""&gt;
                &lt;split size=""34""/&gt;
                &lt;dock id="""" hideTitles=""1"" activePageId=""Toolbar.1""&gt;
                    &lt;page id=""Toolbar.1""/&gt;
                &lt;/dock&gt;
                &lt;split size=""872""/&gt;
                &lt;splitter orientation=""2""&gt;
                    &lt;split size=""497""/&gt;
                    &lt;dock id="""" activePageId=""Viewer.1"" focus=""true""&gt;
                        &lt;page id=""Viewer.1""/&gt;
                    &lt;/dock&gt;
                    &lt;split size=""350""/&gt;
                    &lt;dock id="""" activePageId=""DAG.1""&gt;
                        &lt;page id=""DAG.1""/&gt;
                        &lt;page id=""Curve Editor.1""/&gt;
                        &lt;page id=""DopeSheet.1""/&gt;
                    &lt;/dock&gt;
                &lt;/splitter&gt;
            &lt;/splitter&gt;
            &lt;split size=""526""/&gt;
            &lt;dock id="""" activePageId=""Properties.1""&gt;
                &lt;page id=""Properties.1""/&gt;
            &lt;/dock&gt;
        &lt;/splitter&gt;
    &lt;/window&gt;
&lt;/layout&gt;
}
Root {
 inputs 0
 name /particle.nk
 frame 47
 format ""2048 1556 0 0 2048 1556 1 2K_Super_35(full-ap)""
 proxy_type scale
 proxy_format ""1024 778 0 0 1024 778 1 1K_Super_35(full-ap)""
}
Sphere {
 inputs 0
 display wireframe
 render_mode off
 radius 3
 name Sphere1
 xpos -417
 ypos 81
}
Sphere {
 inputs 0
 name Sphere2
 xpos -410
 ypos -50
}
Sphere {
 inputs 0
 display wireframe
 render_mode off
 name Sphere3
 xpos -266
 ypos -144
}
push 0
ParticleEmitter {
 inputs 3
 name ParticleEmitter1
 xpos -261
 ypos -50
}
ParticleBounce {
 inputs 2
 out_bounce_mode kill
 in_bounce_mode kill
 object input
 name ParticleBounce1
 selected true
 xpos -261
 ypos 77
}
Viewer {
 frame 47
 frame_range 1-100
 name Viewer1
 xpos -261
 ypos 176
}
</code></pre>
","<p>This looks like a bug in Nuke's <code>ParticleBounce</code> node. The <code>geometry</code> input is new to Nuke 9, which was released pretty recently.</p>

<p>That said, the bug seems to manifest only in this idealized environment of two spheres. The first sphere (radius 1) emits the particles, and the second sphere (radius 3) kills them. It is likely because of the exact alignment between vertices that causes it to penetrate.</p>

<p><strong>Rotating one of the two spheres 0.1 degrees on each axis solves the problem</strong></p>

<p>In theory, the ""real world"" wouldn't see this bug as frequently, but I've submitted this bug report to The Foundry.</p>

<hr>

<h2>Response from The Foundry</h2>

<blockquote>
  <p>Thanks very much for providing reproduction script, we can see the
  problem easily. Your comments and script are logged:</p>
  
  <p>Bug 47417 - ParticleBounce - Leaks with static primitive geometry
  input in some cases</p>
  
  <p>We recommend checking upcoming release notes for updates on this
  issue.</p>
</blockquote>
","14571"
"How to not import/duplicate media in FCPX?","656","","<p>When I create a new Event and Project within it, I don't see a way to avoid copying imported media to the FCPX folder (apparently there used to be an option when creating a new Event to not copy imported media, but it doesn't appear to be there any longer).</p>

<p>How can I do that? By the way, I'm importing from the Photos library browser within FCPX if that makes any difference.</p>
","<p>For FCPX 10.1 and later:</p>

<p>Open preferences
Select the import tab
Choose to either leave Files in Place or Copy to Storage Library Location.</p>

<p>To set the Storage Library Location:</p>

<p>Select the library. 
Open the inspector.
Select Modify Settings in the inspector to choose where your generated files go.</p>
","15822"
"Wavy video export in Premiere Pro","656","","<p>I've exported a video within Premiere Pro &amp; certain areas of the video produce wavy lines or it comes out distorted. What could be the cause of this and how can this be fixed?</p>

<p>See below for an example.</p>

<p><strong>Exported Video:</strong> <img src=""https://i.stack.imgur.com/yD5Sh.jpg"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/ryB4n.png"" alt=""enter image description here""></p>

<p><strong>Same clip within Premiere Pro:</strong></p>

<p><img src=""https://i.stack.imgur.com/KmjvT.png"" alt=""enter image description here""></p>
","<p>Those are interlacing problems it looks like. In the editor, it displays 'deinterlaced' previews so you won't notice this in the editor. In the clip, select ""Field options"" and then ""Always deinterlace"" then render your video again, it should be fine then.</p>
","13308"
"What are the audio production capabilities in Sony Vegas Pro because I see mostly plug-ins only?","656","","<p>I have read that from the NLE video editors Sony Vegas Pro is better than the competitors in terms of the audio editing capabilities. But using it there still seem to be a limited set. Mostly the editing is the addition of plugin effects or to open the audio in another editor. Am I missing something which the program allows?</p>
","<p>If you are judging Vegas Pro audio editing on native effects than I don't think it has any edge over others.</p>

<p>I have been using the Pro version for about 18 months now and I find it pretty darn good overall. I am also learning Adobe Premiere and After Effects CS5.5, so far I think Vegas has the edge on the audio for sure.</p>

<p>However, are you aware that Vegas Pro audio editing has the ability to add as many audio tracks as your system can handle (stereo or mono), treat each track with a pan position feature, set individual volume (amplitude) envelopes for each clip, boost or buck the audio overall output of each track, name each track, size the vertical and horizontal view of the audio track as you see fit, record directly to any track, cut any audio clip any way you want, reverse an audio clip with two mouse clicks, and edit Dolby 5.1? And this is only a partial list of some of the built in controls available.</p>

<p>UPDATE: I omitted one feature that I really love, both audio and video tracks can have their order rearranged very easily. With a simple mouse click and drag, you can put that bass line near the drum track or move the voice-over above the music track. I like to move audio tracks as I am building them to keep it tiddy.
:></p>

<p>This list is not complete and does not include the native sound effects which as mentioned earlier are rather standard with any NLE: reverb, EQ, pitch shift, delay, etc. And yes, AFAIK, it can accept plugins but I have never needed any at this point so I am not aware what plugins are compatible.</p>

<p>I have both Vegas Pro 10 and the home version of Sony Sound Forge. I can export an audio clip to Sound Forge for further effects, it's sort of like Audacity on steroids. As far as I know you can render your audio portion to several formats and export to anything that has the same format. I haven't tried this, but I would think you could easily export your audio as a .wav to Audacity but I have never needed to export the audio for further processing except to test it with Sound Forge, and in that exercise, I just had to copy paste the clip--easy.</p>

<p>I have made video content with Vegas Pro that has had as many as 18 stereo tracks with no issues.</p>

<p>Remember Vegas was originally a multi track audio editor.</p>

<p>I think the best answer is for you to try the trail version of Vegas Pro 11 for free for 30 days and kick the tires and then take a spin:</p>

<p><a href=""http://www.sonycreativesoftware.com/download/trials/vegaspro?keycode=64506"" rel=""nofollow"">http://www.sonycreativesoftware.com/download/trials/vegaspro?keycode=64506</a></p>

<p>Disclaimer:
I should mention that I sort of have one hand tied behind my back on this answer as it would be so much more useful to launch Vegas when answering this to review the features. My version of Vegas lives on a machine that is not accessible from the network for security reasons and that location is about 8 miles from where I am typing this.</p>
","4137"
"How can I rename a footage .ai file for AE without breaking the references?","655","","<p>I've got a composition in After Effects CS 6 on Windows that contains several .ai files. One of them has over 30 layers, which I imported and animated in AE. Unfortunately, I gave the .ai file the not so clever name <code>foo.ai</code> because it started out as a test but worked, and I'd like to go ahead with it. When I rename the file to something more sensible, like <code>Scene 1 - Spaceship.ai</code>, AE complains about a missing footage file. Now it wants me to give it the new path, which I do. I have to select the layer from the .ai as well. And it looks like it wants me to do it for every layer from the same file.</p>

<p>I've already tried making a copy of the AE project file, opening it with a text editor and replacing all occurrences of <code>foo.ai</code> with <code>Scene 1 - Spaceship.ai</code>. After that, AE told me it <em>tried to read after the end of file</em>. Looks like there's a file length in there. Damn.</p>

<p>So how can I do this without having to change the source file and resellecting the layer for each individual layer in AE?</p>
","<p>Save the project in XML file format, do as you already did, open it in a text editor, find-replace the name. Open the modified file in AE and re-save it in its normal binary format (aep).</p>

<p>Size shouldn't be an issue since renaming a file doesn't change it size. It seems like you have edited the aep file? Hope this helps!</p>

<p>More on the topic here:<br>
<a href=""http://help.adobe.com/en_US/aftereffects/cs/using/WS3878526689cb91655866c1103906c6dea-7fa0a.html"" rel=""nofollow"">http://help.adobe.com/en_US/aftereffects/cs/using/WS3878526689cb91655866c1103906c6dea-7fa0a.html</a></p>
","8026"
"How to position spheres that are created with the CC Ball action?","653","","<p>I'm using the CC Ball action to make some spheres to illustrate a chemical reaction. </p>

<p>When I make the balls, the circle to move it is thrown to the left, and the actual sphere is to the right. I tried changing the anchor point, however I can't move the shapes wherever I want. </p>

<p>How do I change the position of the created shapes?</p>
","<p>Try using CC Sphere on a large texture instead.  It will wrap the texture into a sphere, which you can then move around in two dimensions.  If you need to pass the spheres behind or in front of each other, you could place the layers in 3D space, even though the spheres will still really be flat layers.</p>

<p>The other option would be to use some particle generators.  Make a single particle that doesn't move in each particle generator you need, then manipulate them within the generator.</p>

<p>Finally if you really need true 3D, use something like Cinena4D, then import the rendered animation into After Effects.</p>
","9549"
"Video files compatibility for Adobe Premiere pro CS4","650","","<p>I have Adobe Premiere Pro CS4 in its basic settings. I am trying to merge some .AVI files that a friend shot on his digital camera. The videos play perfect on Windows Media Players and on other players. But in Premiere it looks as though it's stuck. Even after I render.
This does not happen when I work with video captures from DV camera. </p>

<p>Why is that? Is there a workaround?</p>
","<p>You need to convert the files into a format that Premiere can understand. You should try converting the files into an AVCHD format using a converter. </p>

<p>A free conveter I like is Media Coder</p>
","1862"
"Gstreamer 1.0 rtspsrc to rtp (audio and video)","648","","<p>Hi I'm currently trying to use gstreamer-1.0 to consume rtsp source and provide RTP streams for audio and video in the streams (for Janus Gateway). I have a working solution with ffmpeg so basically would need help to translate this to working gstreamer pipeline.</p>

<pre><code>ffmpeg -rtsp_transport tcp -i rtsp://ip:port/h264.sdp \
-an -c:v h264 -profile:v baseline -preset ultrafast -tune zerolatency -vf ""fps=20"" -bsf:v h264_mp4toannexb  -f rtp rtp://localhost:8004 \
-vn -acodec copy -f rtp rtp://localhost:8005
</code></pre>

<p>Currenlty I have only this:</p>

<pre><code>gst-launch-1.0 rtspsrc location=rtsp://ip:port/h264.sdp latency=0 protocols=tcp ! rtph264depay ! rtph264pay name=pay0 pt=96 ! udpsink port=8004 host=localhost
</code></pre>

<p>but this is not transcoding, and by itself the stream is not playable (strangely it works if I start the ffmpeg example to see video and then run gst but this is beyond the point here), so I would like to have x264 transcoding (re-encoding?) to baseline profile with minimum processing power required.</p>
","<p>Ok I've found the solution that works, it was <a href=""https://stackoverflow.com/a/8197837/592005"">here</a> all along but needed some changes (link only since it's link to the stackoverflow)</p>
","19564"
"Remove light reflection from eyes","647","","<p>I have a video with a man speaking (very stable, motion tracking on his eyes works perfectly)</p>

<p>The problem is each eye has reflections of 4 light sources, so each eye looks like it has 4 white pupils inside the iris. The black pupil suffers from these reflections too. <s>I hope you can understand what i mean.</s> See the following photo.</p>

<p>What effect i need to apply to remove these light reflections and make his iris look natural brown (his original color)? Tried lowering brightness, but his eyes became like big black eyes of a shark. </p>

<p>What to do? Do i need some sort of effect and an artificial black pupil on top?</p>

<p>I'm asking this question hoping this is a somewhat common problem so maybe there is a solution already for that.</p>

<p>Here is what it looks like</p>

<p><img src=""https://i.stack.imgur.com/lWP1F.png"" alt=""enter image description here""></p>
","<p>I solved my problem using 1) motion tracking and 2) a good picture of an eye.</p>

<ol>
<li><p>With adobe after effects i managed to track the motion of the problematic iris. The result was a very precise motion path of the iris for the whole video. Great.</p></li>
<li><p>Then i used a good picture of brown eyes (google images) from which i cropped a circular selection which had ""some"" iris and the pupil. I made that circular selection a little transparent around the edges and finally had this result:</p></li>
</ol>

<p><img src=""https://i.stack.imgur.com/kogz1.png"" alt=""enter image description here""></p>

<ol start=""3"">
<li>Using that image (2) on the motion path (1), the result was excellent. I had to play a bit with the colors to make the new iris blend with the original iris which ""embraced"" the new one.</li>
</ol>

<p>(Fortunately, person wasn't blinking.)</p>

<p>p.s. More answers are welcome!</p>
","14789"
"Camera shake produces ripple-like effect","644","","<p>I'm using a Kodak Zi8 at the moment, but I've noticed this effect on other cameras. When a camera is moving it occasionally produces fun house mirror-style effects. It makes things appear to stretch and shrink. Is this a software issue or a problem with cheap components? Are there any cameras that don't produce this effect?</p>

<p>See example video:
 <p><a href=""http://vimeo.com/44503828"">Kodak Zi8 shake issue</a> from <a href=""http://vimeo.com/reedlaw"">Reed Law</a> on <a href=""http://vimeo.com"">Vimeo</a>.</p></p>
","<p>This ""effect"" is called <a href=""http://en.wikipedia.org/wiki/Rolling_shutter"">Rolling Shutter</a>. It is common in most CMOS camera sensors because they don't capture an instantaneous image, instead they do it progressively over a period of time. CCD sensors do not have this problem as they do have instantaneous capture.</p>
","4249"
"Optimizing MPEG2 Encoder in Adobe Media Encoder","643","","<p>We're looking at using MPEG2 as an alternative to ProRes 422 for delivery &amp; archiving.</p>

<p>There are many options in there which are beyond my knowledge; I would like to understand them and maybe get even better results. Is there a guide to these terms?</p>

<ul>
<li>Macroblock quantization</li>
<li>VBV buffer size</li>
<li>Noise Control (Sensitivity/Reduction)</li>
<li>Write SDE</li>
<li>Intra DC Precision (8/9/10 bits) </li>
<li>Ignore Frame Interval</li>
</ul>
","<p>MPEG-2 works by compressing in two different ways.  Within a frame, it looks at groups of pixels (blocks) and compresses them based on patterns it finds within them.  This is kind of similar to how JPEG compression work.  the Macroblock quantization level describes how loosely the encoded block can represent the original block.  The lower the number, the higher the quality but the more data required to represent the block since less error is accepted when trying to compress.</p>

<p>VBV Buffer Size is the size of the buffer required in order to maintain a constant frame rate.  Each frame of video in MPEG-2 requires different amounts of data from other frames.  Assuming the stream arrives at a constant data rate, a certain amount of buffer is needed to ensure that data continues to arrive in time to present a frame since the rate of arrival may sometimes be lower than the rate of data being consumed.  This value allows the player to judge how much data it needs to buffer in order to avoid running out of information to display to the viewer.</p>

<p>Noise Control is a means of trying to avoid encoding inefficiencies from noise present in the video.  MPEG-2 and other predictive encoding schemes rely on the fact that video tends to not change from frame to frame.  Noise, however, is random and not able to be predicted.  This is a significant problem for high efficiency compression since it can't be predicted.  Reducing the noise before encoding results in better compression rates as the video is more predictable, but also results in some loss of detail as it is not possible to distinguish between noise and fine detail in many cases.</p>

<p>Write SDE writes out information about the vertical and horizontal size of the frame.</p>

<p>Intra DC Precision is the number of bits used (thus the accuracy) of the DC coefficients.  It also impacts how accurately blocks are quantized.  Higher values mean more precision is used, but also that higher data rates are required.</p>

<p>Ignore Frame Interval I'm not 100% sure on for that encoder, but generally it allows the encoder to adjust the keyframe interval as it deems necessary.  It may use more or less data as needed, low motion video will likely benefit as the keyframes can be further spread out, however high motion video is more likely to suffer if the interval becomes too long.</p>
","12506"
"Shapes overlapped in pre-comp, transparent fade in visibility","643","","<p>I have two white shapes that overlap on a purple background, which are in a pre-comp. One in animated (an arm) and the other is not (the rest of the body).</p>

<p>When I fade the pre-comp in, from 0 to 100 opacity, I can see the parts where the shape overlaps. This is made worse by having a motion blur, which seems to multiply the blur in that area.</p>

<p><img src=""https://i.stack.imgur.com/Ce4sR.png"" alt=""enter image description here""></p>

<p>Do you know how to avoid this situation, and to have the two shapes behave as one.</p>
","<p>Pretty sure this is due to the little sun icon - the ""collapse transformations"" icon.. 
See more here: <a href=""http://helpx.adobe.com/after-effects/using/3d-layers.html"" rel=""nofollow"">http://helpx.adobe.com/after-effects/using/3d-layers.html</a></p>

<p>It's to do with what order 3D comp layers are processed when they're precomped.</p>
","12833"
"After effects Timeline display: frames into milliseconds","641","","<p>I'd like to change frames into milliseconds in the timeline.</p>

<p>Right one, it displays seconds, and frames in between. But since, I have to work on a music video, i'd like it to be milliseconds inbetween (base 100).</p>

<p>How to do it ?</p>
","<p>Here's a workaround:</p>

<p>Create a left aligned text layer - type a zero. Format to the size and colour of your choice.
Twirl down this layer, twirl down text, ALT click the stopwatch on the Source text property. In the expression field that opens in the timeline, replace the text with:</p>

<p>Math.round(time*1000)+"" ms""</p>

<p>click out of the text field into a blank area of the timeline.</p>

<p>The text will now show the time of the current frame in milliseconds.</p>
","18759"
"How is each type of video camera stabilizer different?","641","","<p>What are the kinds of video camera stabilizers I could buy in the market? How are they different? What are the pros and cons of each?</p>
","<p>There are a very wide and diverse range of different techniques for stabilization.</p>

<p>At the most basic, you have pure software stabilization.  This uses no specialized hardware, but rather tracks objects within a scene and then clips the video such that it maintains a frame which it can keep more stable.  This has an apparent impact of subjects moving less, but often produces an awkward feeling wavy effect as motion blur is still present in the source footage and occurs while the subject doesn't visibly appear to be moving.  It can also be used in support of other physical techniques.  It can handle minor corrections, but isn't advisable for large scale correction if it can be avoided.  This can be applied either in post or directly during capture on the camera (electronic image stabilization.)</p>

<p>At the next level, we move up to optical image stabilization systems.  There are two main techniques here.  The first utilizes a motion correction element in the lens along with small gyroscopes in the lens to keep the orientation of the corrective element relatively static.  This results in a correction for shake in the lens' orientation, but doesn't correct for lateral movement.  A similar system can be accomplished in camera by moving the sensor itself to correct for movement of the camera.  Gyroscopes feed the data on how to move the sensor.</p>

<p>Moving past that, the next big step up is a hand held gimbal stabilizer.  Gimbal stablizers work using inertia to offset movement rather than gyroscopic forces.  A typical handheld gimbal stabilizer consists of a rod with a free floating gimbal about 1/3 of the way along the length.  The camera is mounted on the top and a series of balancing weights are added to the bottom.  The inertia of the combined and properly balanced system results in orientation changes being absorbed at the gimbal as the inertia keeps the camera rig pointed in the same direction.  These provide far better stabilization for orientation and also provide a fair bit of stabilization for lateral movements, however they are considerably more expensive and more difficult to use well than an optical image stabilization system is.  It can also still be used with the previous mechanisms.</p>

<p>The next step up from this is a steadicam like rig.  Steadicam utilizes a gimbal stabilizer, however, rather than hand hold the gimbal, it is mounted on a spring arm attached to the operator's chest.  The spring arm allows for much better weight support as well as better absorption of lateral movement, thus it is able to deal with situations such as an operator running while operating the camera.</p>

<p>The final upgrade is to move back in to active systems again.  The inertial systems are all passive and thus don't require independent power (the OIS systems are active, but the inertial ones are passive.)  By adding high powered gyroscopes to an inertial system, additional stabilization can be achieved by providing active counter-action to changes in orientation.  The upside is that these hybrids provide the absolute best possible stabilization, but the downside is they are generally heavy, expensive (thousands to tens of thousands of dollars), power hungry and complex.</p>
","13099"
"ffmpeg buffered recording","639","","<p>need some advice to solve the problem </p>

<p>ffmpeg is receiving the nonstop live stream (rtmp source) from camera monitoring entry gates </p>

<p>after trigger is fired (gate/door is open) I need to create the 30sec videofile saved to HDD</p>

<p>I mean this 30 sec as 25 seconds of video before trigger was fired and some 5 seconds after trigger</p>

<p>the similar idea is covered by ffmpeg wiki (<a href=""https://trac.ffmpeg.org/wiki/Capture/Lightning"" rel=""nofollow"">https://trac.ffmpeg.org/wiki/Capture/Lightning</a>) but I can't get this working in any command configuration</p>

<p>I was playing with filters, buffering etc too but still no luck ;-((</p>

<p>any ideas would be appreciated</p>
","<p>What you can do is add a blank 30 second segment in front of the camera stream, and then use the method in the ffmpeg wiki.</p>

<p>Basic template is</p>

<pre><code>ffmpeg -f lavfi -i color=black:WxH:r=FPS:d=30 -i camera_input \
       -filter_complex ""[0][1]concat[v]"" -map ""[v]"" StreamingOutput
</code></pre>

<p><code>WxH</code> should be set to the camera's resolution and <code>FPS</code> to its framerate.</p>

<p>And the 2nd command which is run 5 seconds after the trigger.</p>

<pre><code>ffmpeg -i Streamingoutput -t 30 snippet.mp4
</code></pre>
","18522"
"Crossfading audio at beginning and ending of video","638","","<p>The html5 video tag has a loop attribute. When playing a video in a continuous loop, it doesn't do this seamlessly. I hear cracks and pops at the beginning. With ffmpeg I am able to trim <code>.05</code> seconds from the start and end to make audio and video uniform. That helped a lot but still not fully smooth when played in a loop. Then crossfade came into question. How can I gradually lower the volume at the end of video and gradually raise the volume at the beginning?</p>

<p>ffmpeg <code>.05</code> seconds from the start and end</p>

<pre><code>for f in *.mp4; do
    duration=$(ffmpeg -i ""$f"" 2&gt;&amp;1 | grep ""Duration""| cut -d ' ' -f 4 | sed s/,//)
    length=$(echo ""$duration"" | awk '{ split($1, A, "":""); print 3600*A[1] + 60*A[2] + A[3] }' )
    trim_start=.05
    trim_end=$(echo ""$length"" - .05 - ""$trim_start"" | bc)
    echo ffmpeg -ss ""$trim_start"" -i ""$f"" -c copy -map 0 -t ""$trim_end"" ""${f%.mp4}-trimmed.mp4""
done
</code></pre>
","<p>For purposes of this answer I'll assume the audio is encoded with an aac codec.</p>

<p>For each video:</p>

<p>Use <em>ffmpeg</em> to extract the sound track to its own file.</p>

<pre><code>ffmpeg -i ""${f%.mp4}"" -vn -acodec copy ""${f%-sound.aac}""
</code></pre>

<p>If not yet known, determine the duration of the audio file.  In some cases ffmpeg is known to give an incorrect duration (<a href=""https://stackoverflow.com/questions/10437750/how-to-get-the-real-actual-duration-of-an-mp3-file-vbr-or-cbr-server-side"">https://stackoverflow.com/questions/10437750/how-to-get-the-real-actual-duration-of-an-mp3-file-vbr-or-cbr-server-side</a>), so I'd use <em>sox</em>.</p>

<pre><code>duration=$(sox soundtrack_before_processing.mp3 -n stat 2&gt;&amp;1 | sed -n 's#^Length (seconds):[^0-9]*\([0-9.]*\)$#\1#p')
</code></pre>

<p>Use <em>sox</em> to apply fades in and out.</p>

<pre><code>sox ""${f%-sound.aac}"" ""${f%-sound-with-fades.aac}"" fade t $fade_in_seconds $duration $fade_out_seconds
</code></pre>

<p>Replace the audio track in the original video file with the faded track.</p>

<pre><code>ffmpeg -y -i ""${f%.mp4}"" -i ""${f%-sound-with-fades.aac}"" -map 0:v -map 1 -vcodec copy -acodec aac -strict experimental -shortest ""${%f-new.mp4}""
</code></pre>
","16618"
"Auto generate video clips using a CSV of times as markers from a single MPG video","637","","<p>I have an MPG video and a CSV of times. I am searching for a way to automatically generate clips from the video using those times.</p>

<p>So at each time position a clip is generated with 10 seconds either side of the time resulting in a 20 second clip.</p>

<p>I have investigated scene detectors and trying to import markers into Adobe Premier etc, but with no luck.</p>

<p>Any ideas?</p>

<p>Thanks. </p>
","<p>You can use an EDL (Edit Decision List) in the CMX3600 format. An EDL is a very simple import/export format from the 1970s, when data was exchanged using floppy disks and videos were stored on tape. It's just a text file and most modern NLEs (including Premiere Pro) can import/export it. (Today there are some variations between NLEs, but that doesn't have to bother you.)</p>

<p>To understand EDLs, it's probably best that you create a new timeline and put your MPG video in it. Then export the timeline into an EDL (File > Export > EDL) and open the exported file with a simple text editor. Analyse the content. Then do some cut in your timeline and export it again.</p>

<p>Basically, an EDL consists of several text lines like this:</p>

<pre><code>VJ0K006W V    C         00:00:00:00 00:00:18:19 00:00:22:02 00:00:40:21
</code></pre>

<p>The first column is an ID of your video clip, followed by V for a video track and C for a cut. An A instead of V indicates an audio track. The last 4 columns are timecodes which define the in and out point of the source and the in and out point in the timeline. You see that this clip is the first 18 seconds and 19 frames of the input video starting in the timeline at 22 seconds and 2 frames. It's only logical that the clip must end 18 seconds and 19 frames later, at the timeline position 40 seconds and 21 frames.</p>

<p>Now, to answer your question: You have to write a small program (or ask a friend to write) which takes your CSV file as input, extracts the time position, calculates the +/- 10 seconds, and writes an EDL file in the way that Premiere Pro exported. Premiere Pro should import it without problems and assemble the timeline.</p>

<p>Here are some footnotes:</p>

<ul>
<li><p><a href=""https://en.wikipedia.org/wiki/SMPTE_timecode"" rel=""nofollow"">Wikipedia</a> explains how timecodes are constructed. If you have simple frame rates like 24, 25, or 30 fps, calculating your +/- 10 seconds is not so difficult. But if your clip has 29.97 fps, then you have to consider drop frames. (You know that leap years have an additional day - February 29, but only occur in distinct years. Clips with 29.97 fps have normally 30 frames per second, but at distinct times have 2 frames less.)</p></li>
<li><p>I didn't cover the problem of media file names here. When the EDL/CMX3600 format was defined, media only had tape numbers, which were written in the first column of the EDL. When file-based media came into existence, NLEs had to add the file names somehow to the EDL. They did it, but not in a consistent way. But since you only work with Premiere Pro, just see how your video file name appears in the EDL file and mimic the same when writing your file. </p></li>
</ul>
","16191"
"QoS for video streaming","636","","<p>What are the most used metrics to evaluate the QoS of a video streaming session? I am looking for tutorials, papers, lecture notes, ppt, anything of that kind, or your own answer.</p>
","<p><strong>From the perceptual front (in the user point of view):</strong>  </p>

<ol>
<li><p><strong>Quality:</strong> 
Here we are evaluating quality of video and audio being streamed. Usually, higher the bit rate better the quality - but it also means more <em>cost</em>. The basic quality measure for Video is <a href=""http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"" rel=""nofollow"">PSNR</a>. However, more relevant matrices from the perceptual quality is <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.7246&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">blockiness</a> and there are many other approaches like <a href=""http://www.cic.unb.br/~mylene/PI/WinklerMohandas2008.pdf"" rel=""nofollow"">this</a>.
Usually, choosing the codec and bit rate fixes the quality primarily, so network doesn't effect that much in that case.</p></li>
<li><p><strong>Consistency (or lack of jerkiness):</strong>
This is a rough measure of times when the continually running stream got interrupted. Usually this is a measure of length and pauses that are caused due to unavailability or reduction of bandwidth during some time and player needs to wait for the data to arrive. 
Usually, buffering is done to the optimal level in the receiver to minimize the effect on this. </p></li>
<li><p><strong>Errors in pictures:</strong> 
This is the indication of errors occurred during the transmission due to which patchy or blank screens. There are cases, where transmission could be error free, in which case, this won't be a problem. </p></li>
<li><p><strong>Latency:</strong> 
This refer to amount of time the service needs to begin the deliver and time the system gets reflected to screen. There are various elements of the system that contributes to this. Usually, for VoD type systems, some reasonable but it is a reasonable concern for the two systems.</p></li>
<li><p><strong>End-to-end delay or so called guarantee:</strong>
The end-to-end delay is essentially same as round trip delay between two systems. For two way communications like telephony, end-to-end delay is of great importance. </p></li>
<li><p><strong>Service guarantee:</strong> 
Usually, this refers to probability with which the given service will be surely available to the end users. This is equivalent to probability of call drops or network blocking of calls. </p></li>
</ol>

<p><strong>Intrinsic parameters:</strong><br>
Here we can think of QoS metrics from the point of view of basic measures of the network which is reflects the QoS as perceived by user. </p>

<ol>
<li><p><strong>Bit rate guarantees:</strong> 
This includes the maximum and minimum of available bandwidth during the entire session. Guarantees available for the network bandwidth is usually the single most </p></li>
<li><p><strong>Error rate or Loss runs:</strong> 
Usually, unlike satellite paths, in the internet there are no bit errors (above transport layer) - the errors get reflected in the form of packet losses. Basic metrics is hence packet error rate on the networks. More research has concluded that these can be modeled as loss lengths where typically, errors occur in some bursts. The packet loss and the length of the losses has a significant effect in terms of perceptions. <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.7087&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">This paper</a> describes this metric in depth. </p></li>
<li><p><strong>Latencies or Various delays:</strong> 
This involves the delay between the path, setup delay, resource reservation delays and buffer delay that all contributes to initialization of display and latency. Many a times, efficient codecs collects various pictures together before encoding to optimize bit rates this allows more efficient compression (higher quality) but increases the latency. </p></li>
<li><p><strong>System uptime and usage factor:</strong>
If either the systems is dysfunctional (down) or it is fully utilized (blocked) it will have </p></li>
</ol>

<p>The paper below shows good relationship between various taxonomies. 
Survey of QoS specifications : <a href=""http://eia.udg.es/~marzo/doctorat/aur98.pdf"" rel=""nofollow"">http://eia.udg.es/~marzo/doctorat/aur98.pdf</a></p>
","2689"
"Mp4 Video Container and the Codecs use with it","634","","<p>I have started to learn about mp4 file format, and I just learned that mp4 is nothing but just a container that defines the structure of the video file, and that it has different codecs inside it for audio and video, as well as other data like metadata and subtitles.</p>

<p>I have been searching around for a while now but couldn't find so much on the basics of mp4 containers and the working and complete structure explanation of it.</p>

<p>So far I have read <a href=""http://www.videomaker.com/article/15362-video-formats-explained"" rel=""nofollow"">this</a> and <a href=""http://www.dr-lex.be/info-stuff/mediaformats.html"" rel=""nofollow"">this</a>, but there is not enough information specifically about mp4 container.</p>

<p>My question is,</p>

<p>There are so many Audio/Video Codecs available which are used, but which Codecs are used with the mp4 container ?</p>

<p>Can we use any Codec with the mp4 container ? because there are like dozen different Codecs and how one would know which can be used with the mp4 container ?, </p>

<p>and is there a book or a paper on Mp4 container structure and explanation ?</p>
","<p>The <a href=""http://en.wikipedia.org/wiki/MPEG-4"" rel=""nofollow"">Wikipedia article</a> on MPEG-4 is a great start as the MP4 file specification is part of the MPEG-4 spec.  Specifically version 2 of MP4 is <a href=""http://en.wikipedia.org/wiki/MPEG-4_Part_14"" rel=""nofollow"">MPEG-4 Part 14</a>. While not free, you can purchase copies of the ISO spec under <a href=""http://www.iso.org/iso/catalogue_detail.htm?csnumber=38538"" rel=""nofollow"">ISO# 14496-14:2003</a>.  A preview with some detail is available from the ISO <a href=""http://webstore.iec.ch/preview/info_isoiec14496-14%7Bed1.0%7Den.pdf"" rel=""nofollow"">here</a>.  </p>

<p>It is designed to contain any of the various MPEG video formats, including MPEG, MPEG-2, MPEG-4 Part 2 (generic MPEG-4) and, probably most commonly, MPEG-4 part 10 (H.264).  It can theoretically support almost any format of video stream, however those 4 are the most widely supported for the file type.</p>

<p>The full list of officially registered codec types is available from the MP4 Registration Authority <a href=""http://mp4ra.org/codecs.html"" rel=""nofollow"">here</a>.</p>
","12562"
"Canon Vixia HF R600 - MP4 vs AVCHD","633","","<p>I just bought a Canon Vixia HF R600. It's giving me the option to save files as AVCHD or MP4. I will be importing them to my PC and editing them in Adobe Premiere CC.  I'd heard that AVCHD has a better picture quality, but I also heard that AVCHD requires multiple files (???). My questions are:</p>

<ul>
<li>Can Adobe Premiere CC work equally well with both file types?</li>
<li>Which file type will give better image quality?</li>
<li>Does a single AVCHD clip really comprise multiple files that I'll have to import?</li>
</ul>

<p>Thanks!</p>
","<p>Generally, MP4 is better for compatibility, whereas AVCHD provides the better quality. MP4 (which by the way can mean a million different things, in this case it probably refers to <a href=""https://en.wikipedia.org/wiki/MPEG-4_Part_14"" rel=""nofollow"">MPEG-4 Part 14</a>) files will be compatible with most devices and media players as is, and will be smaller in size compared to AVCHD. This is the option I'd recommend if you wanted to film your vacation and show the recordings as is, without further editing.</p>

<p>If you want the best quality, go with AVCHD. Files will be bigger in size and won't play back on most devices, but you can import them into Premiere Pro and export your edited video to every format you want anyway. Chosing the AVCHD will also open up other recording options on your camera. According to the manual, the highest bitrate avaiable for MP4 is 35Mbps. For AVCHD, the highest possible bitrate should be even higher, though I couldn't find an exact value.</p>

<blockquote>
  <p>Can Adobe Premiere CC work equally well with both file types?</p>
</blockquote>

<p>Yes. MP4 will record, well <code>.mp4</code> files. AVHCD will probably come as <code>.mts</code> or <code>.m2t</code>/<code>.m2ts</code> files, both of which Premiere Pro can handle.</p>

<blockquote>
  <p>Which file type will give better image quality?</p>
</blockquote>

<p>AVCHD with the highest recording quality settings. Keep in mind that this will fill up your SD card pretty quickly, according to the manual at about 4 GB per 20 minutes of recording. If this is a problem for you, MP4 might be the better solution, as it offers the better quality/filesize-ratio.</p>

<blockquote>
  <p>Does a single AVCHD clip really comprise multiple files that I'll have to import?</p>
</blockquote>

<p>Not really sure about that. Normally, it should produce one container file every time you press record. Many cameras will split the file at 4 GB or stop the recording when a continuous recording reaches that size. Since you already bought the camera, just try out both options and see what files you get. But Premiere pro should be able to handle whatever you throw at it.</p>
","18115"
"Is there a standard resolution for Video Presentations?","632","","<p>A friend asked me to put together a video presentation for his companies national sales meeting.  I put it together in Premiere.  All of that went fine, but I'm not sure what resolution to make it.  They're obviously going to be using a projector to present it, but I'm not sure of the specs.  Is there a resolution you recommend using that would probably work best?  Would it probably be a widescreen format?  What questions should I be asking of them about the equipment? </p>
","<p>If it's NTSC/standard def/4x3, 640x480 will be good. If it's HD/widescreen/16x9, 1920x1080 or 1280x720 (1080p and 720p, respectively). Premiere will likely have presets for all of these aspect ratios. </p>

<p>I would ask the projector owner for the pixel dimensions or pixel resolution. Likely, it'll match one of the above pixel dimensions. If the owner's not very projector savvy, ask for the model number and manufacturer, and look up the info you need on their website's support section or downloadable manuals.</p>

<p>The thing to keeop in mind is that the projection hardware is a fixed size...usually one of the pixel dimensions noted above, or something CLOSE to it for computer monitor resolution. Even if you feed a 1280x960 image to an NTSC display, it's going to interpolate it into a 640x480 display. On a projector, what makes it bigger is the lens projecting that tiny 640x480 pixel image to a 10' screen. My point is that if you know the maximum pixel resolution on the projector, you don't need to bother rendering anything MORE than that; it won't affect image quality.</p>

<p>good luck! </p>
","4460"
"Changing the input frame rate of a series of images in FFmpeg","631","","<p>I am attempting to animate a series of PNG images at 30 frames per second (and add an audio track) with FFmpeg.</p>

<p>On the command line, I specify the input and output frame rates as 30 fps.</p>

<blockquote>
  <p>ffmpeg.exe <strong>-r 30</strong> -i %d.png -itsoffset 0.770 -i soundtrack.wav -r 30 -vcodec mpeg4 -y ""ffmpegtest.mp4""</p>
</blockquote>

<p>The output shows that it is reading the input images at the default <strong>25 frames per second</strong>.</p>

<pre><code>    ffmpeg version N-82225-gb4e9252 Copyright (c) 2000-2016 the FFmpeg developers
      built with gcc 5.4.0 (GCC)
      configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-dxva2 --enable-libmfx --enable-nvenc --enable-avisynth --enable-bzlib --enable-libebur128 --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenh264 --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib
      libavutil      55. 35.100 / 55. 35.100
      libavcodec     57. 66.101 / 57. 66.101
      libavformat    57. 57.100 / 57. 57.100
      libavdevice    57.  2.100 / 57.  2.100
      libavfilter     6. 66.100 /  6. 66.100
      libswscale      4.  3.100 /  4.  3.100
      libswresample   2.  4.100 /  2.  4.100
      libpostproc    54.  2.100 / 54.  2.100
    Input #0, image2, from '%d.png':
      Duration: 00:02:24.00, start: 0.000000, bitrate: N/A
        Stream #0:0: Video: png, rgba(pc), 480x320, 25 fps, 25 tbr, 25 tbn, 25 tbc
</code></pre>

<p>[I'll interrupt here to say it is that last line that confuses me]</p>

<pre><code>    Guessed Channel Layout for Input Stream #1.0 : stereo
    Input #1, wav, from 'soundtrack.wav':
      Duration: 00:06:51.26, bitrate: 1411 kb/s
        Stream #1:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s
    Output #0, mp4, to 'ffmpegtest.mp4':
      Metadata:
        encoder         : Lavf57.57.100
        Stream #0:0: Video: mpeg4 ( [0][0][0] / 0x0020), yuv420p, 480x320, q=2-31, 200 kb/s, 30 fps, 15360 tbn, 30 tbc
        Metadata:
          encoder         : Lavc57.66.101 mpeg4
        Side data:
          cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1
        Stream #0:1: Audio: aac (LC) ([64][0][0][0] / 0x0040), 44100 Hz, stereo, fltp, 128 kb/s
        Metadata:
          encoder         : Lavc57.66.101 aac
    Stream mapping:
      Stream #0:0 -&gt; #0:0 (png (native) -&gt; mpeg4 (native))
      Stream #1:0 -&gt; #0:1 (pcm_s16le (native) -&gt; aac (native))
    Press [q] to stop, [?] for help
    [image2 @ 0000000000622cc0] Thread message queue blocking; consider raising the thread_queue_size option (current value: 8)
    frame= 3600 fps=242 q=19.1 Lsize=    9908kB time=00:06:52.04 bitrate= 197.0kbits/s speed=27.7x
    video:3263kB audio:6494kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.540179%
    [aac @ 000000000064e600] Qavg: 448.922
</code></pre>

<p>I am fairly new to FFmpeg. What am I missing? How do I tell it the input stream is 30 fps?</p>
","<p>With image sequences, you have to use the framerate option. So,</p>

<pre><code>ffmpeg -framerate 30 -i %d.png -itsoffset 0.770 -i soundtrack.wav -vcodec mpeg4 -y ""ffmpegtest.mp4""
</code></pre>

<p>With <code>-r</code> specified before the sequence name, FFmpeg assumes 25 fps and duplicates/drop frames to meet the set rate.</p>
","19755"
"Empty library in DaVinci Resolve","629","","<p>I just wanted to work through the DaVinci Lite's manual. It says, the library browser should display all available volumes (Fixed, network and removable).</p>

<p>It displays, however, only one empty, cryptic ""c:\vol0""</p>

<p>Any idea how I can access my footage?</p>

<p>Re-Installing the package didn't help.</p>

<p>I'm using version 11.3.0lite004 on Windows 8.1.</p>

<p><img src=""https://i.stack.imgur.com/LIBsS.jpg"" alt=""Empty Library Browser""></p>
","<p>I've just received an answer on another forum:</p>

<ol>
<li>Select ""DaVinci Resolve"" from the main menu, select ""Preferences""</li>
<li>In the media storage tab press ""+"" and select the desired folder / drive.</li>
<li>Press OK and restart DaVinci.</li>
</ol>

<p>OK, this software is definitely not intuitive, and one should not read the first 100 pages of the manual too fast!</p>

<p><img src=""https://i.stack.imgur.com/XbbgD.jpg"" alt=""Select desired folder in DaVinci""></p>
","15464"
"How do I find out about the shutter speed of frames in a video?","629","","<p>I shot a video at 24FPS, and I want to find out what the shutter speed of each frame was. Quicktime Player and VLC's Info panels don't give me this information.</p>

<p>Neither does exiftool. Here's exiftool's output:</p>

<pre><code>ExifTool Version Number         : 10.36
File Name                       : 24 FPS at 480p.mov
Directory                       : /Users/kartick/Desktop/Google Drive/Photos/Comparisons/Night Video Comparison
File Size                       : 7.7 MB
File Modification Date/Time     : 2017:04:18 19:55:15+05:30
File Access Date/Time           : 2017:05:05 06:57:31+05:30
File Inode Change Date/Time     : 2017:04:24 11:25:17+05:30
File Permissions                : rwxr-xr-x
File Type                       : MOV
File Type Extension             : mov
MIME Type                       : video/quicktime
Major Brand                     : Apple QuickTime (.MOV/QT)
Minor Version                   : 0.0.0
Compatible Brands               : qt
Movie Data Size                 : 8037178
Movie Data Offset               : 36
Movie Header Version            : 0
Create Date                     : 2017:04:18 14:25:15
Modify Date                     : 2017:04:18 14:25:38
Time Scale                      : 600
Duration                        : 22.75 s
Preferred Rate                  : 1
Preferred Volume                : 100.00%
Preview Time                    : 0 s
Preview Duration                : 0 s
Poster Time                     : 0 s
Selection Time                  : 0 s
Selection Duration              : 0 s
Current Time                    : 0 s
Next Track ID                   : 3
Track Header Version            : 0
Track Create Date               : 2017:04:18 14:25:15
Track Modify Date               : 2017:04:18 14:25:38
Track ID                        : 1
Track Duration                  : 22.75 s
Track Layer                     : 0
Track Volume                    : 100.00%
Image Width                     : 640
Image Height                    : 480
Clean Aperture Dimensions       : 640x480
Production Aperture Dimensions  : 640x480
Encoded Pixels Dimensions       : 640x480
Graphics Mode                   : ditherCopy
Op Color                        : 32768 32768 32768
Compressor ID                   : avc1
Source Image Width              : 640
Source Image Height             : 480
X Resolution                    : 72
Y Resolution                    : 72
Compressor Name                 : H.264
Bit Depth                       : 24
Video Frame Rate                : 24.004
Matrix Structure                : 1 0 0 0 1 0 0 0 1
Media Header Version            : 0
Media Create Date               : 2017:04:18 14:25:15
Media Modify Date               : 2017:04:18 14:25:38
Media Time Scale                : 44100
Media Duration                  : 22.80 s
Media Language Code             : und
Balance                         : 0
Handler Class                   : Data Handler
Handler Vendor ID               : Apple
Handler Description             : Core Media Data Handler
Audio Format                    : mp4a
Audio Channels                  : 1
Audio Bits Per Sample           : 16
Audio Sample Rate               : 44100
Purchase File Format            : mp4a
Handler Type                    : Metadata Tags
Software                        : ProCamera 10.2
Make                            : Apple
Model                           : iPhone 7 Plus
Avg Bitrate                     : 2.83 Mbps
Image Size                      : 640x480
Megapixels                      : 0.307
Rotation                        : 0
</code></pre>

<p>I tried saving a frame to an image file in VLC, but it saved without metadata, so I couldn't figure the shutter speed out.</p>

<p>I know the shutter speed is usually 1/48s (half of 1/frame rate), but this video was shot at night, so I want to check if the camera app used a longer shutter speed.</p>
","<p>If the data was recorded by the camera as EXIF MetaData, Adobe Bridge would reveal the camera settings under the Metadata Panel. You may have to customize within Bridge WHICH metadata you want to look at; because there are literally hundreds, thousands, of types of EXIF data; which could or could not have been embedded; AND at times; you will find what you need hidden in a strange EXIF container. </p>

<p>Simply put; if you don't see a shutter speed setting under MAIN, or CAMERA metadata; the brand camera/type, could have written it elsewhere; so show all; expand all, and scroll down to see if the data is possibly there; hidden in a collapsed field. </p>

<p>If you shot the video at 24fps; you are right in thinking that it likely was recorded at 1/48 / 180 shutter. However; I would think; just based off the engineering / camera / lens type which is built onto the iPhone 7; and how I would engineer the camera to work using, which it does by default-  auto-exposure; that it would rely on aperture priority for exposure; and would thus constantly adjust your shutter speed on the fly based on it's exposure method. So if you did a constant shot from a dark area to bright area; you'd literally see an on the fly adjustment from 1/48 all the way up to 1/4000 or higher. Then aperture might kick in, as the phone as no ND filters to fall back on. </p>

<p>I only add that because usually iPhone footage shot outdoors in bright sunlight looks like it was recorded at HIGH shutter (strobe effect) rather than iris down. The diameter on the lens of an iPhone 7 also would make me think they would use Aperture Priority (constant) <em>as it's practically a pin-hole camera to begin with.</em> </p>

<p>Cheers!</p>
","21367"
"What do you call the video effect where water looks like it's standing still when filmed","629","","<p>I found a few interesting effects, which was labelled under the rolling shutter effect, however I was told that this was NOT an example of rolling shutter effect, what would you call this effect?</p>

<p>The question is 3 fold: I heard they used this effect in movies, which ones, an example clip would be nice, what do they officially call it, and why does this happen? </p>

<p>These are the examples I am referring to:<br>
<strong>REALLY interesting</strong> effects:<br>
For example, the water looks like its static. [PetaPixel][1] explains it; ""by pressing a water tube against a speaker, bibio was able to control the vibration frequency of the water flowing through the tube. He then adjusted the pulses of the water to match up with the frame rate of his Canon 5D Mark II.""<br>
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/_PkgQQqpH2M?start=13""></iframe>
            </div></div><br>
<br>
Another example was the static helicopter:<br>
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/cxddi8m_mzk?start=0""></iframe>
            </div></div></p>
","<p>This is called <a href=""http://en.wikipedia.org/wiki/Stroboscopic_effect"">stroboscopic effect</a>.  It refers to when the sample rate is synchronized or very slightly out of sync resulting in a much slower representation of the motion that results from taking a picture that is at the same point or slightly advanced point in a subsequent cycle of a higher frequency cyclic motion.</p>

<p>You can actually do the same exact thing in real life for your own eyes by using a strobe light, which is the more common demonstration, though frame rate can be used too if you can adjust it closely enough.</p>
","12210"
"320 kbps MP3 -> laptop sound card -> studio headphones = what is the bottleneck?","628","","<p>I am usually listening to </p>

<ul>
<li>a) 320 kbps MP3</li>
<li>b) Spotify over WiFi</li>
</ul>

<p>on my </p>

<ul>
<li>c) Samsung Galaxy S4</li>
<li>d) HP laptop with internal sound card (I can't even google what card is there)</li>
</ul>

<p>over </p>

<ul>
<li>Sony Mdr-V6 closed headphones.</li>
</ul>

<p>What is the bottle neck in my sound quality? What should be my upgrade path?</p>

<ol>
<li>upgrade headphones?</li>
<li>buy external sound card for the laptop?</li>
<li>buy headphones amplifier?</li>
<li>improve the audio source?*</li>
</ol>

<p>*<em>I can/will not use CDs as audio source as this is not compatible with my lifestyle.</em></p>
","<h2>File format</h2>

<p>Bit rate tells surprisingly little about quality in the case of mp3 files. There are old encoders where no matter how low you set the compression, there will always be audible artifacts. But LAME and the like have long gotten over this, and <a href=""https://video.stackexchange.com/questions/8382/is-there-a-losslessly-compressed-audio-file/8383#8383"">properly done 320 kBit mp3 is for listening purposes lossless</a>, just like <a href=""http://xiph.org/~xiphmont/demo/neil-young.html"" rel=""nofollow noreferrer"">CDs are</a>. Note anyway that mp3 is an outdated standard; I'd like to recommend switching to OGG Vorbis but practically this wouldn't gain you anything.</p>

<h2>Network</h2>

<p>What digital connection you use to transfer the file from the source to the DA unit has no influence on the sound whatsoever<sup>1</sup>. WiFi is fine as long as its avarage transmission rate can keep up, which should never be a problem nowadays (apart from complete dropouts if the connection breaks down somehow, of course).</p>

<h2>Sound card</h2>

<p>Laptop on-board sound cards are horrible, so off the top I'd say that's your bottleneck. <em>However</em>, the one thing they usually manage quite well is driving headphones. The main problems that such sound cards have are</p>

<ul>
<li>Awful mic preamps.</li>
<li>Bad AD/DA units.</li>
<li>Insufficient (or no) shielding against interference from power supply bursts etc.</li>
</ul>

<p>You don't need the mic inputs. Even low-quality DAs of today can actually reproduce properly bandlimited audio quite faithfully. And the interference issues mostly turn up when you connect to some other audio device (with ground connection), but not so much with headphones. The headphone amplifiers themselves aren't great of course, but since that's quite an easy job electronically they still don't have easily audible influence on the sound. One remaing problem is high volume levels, especially in mobile devices, for power-supply (as well as, possibly, heat dissipation) reasons.</p>

<p>You may try an external sound card, it <em>might</em> improve you listening experience; but perhaps not significantly.</p>

<h2>Headphones</h2>

<p>Like any kind of speaker device, those actually have more influence on the sound than any properly designed purely electronic component<sup>2</sup>. That's often not so much considered a flaw as a characteristic; HiFi speakers aren't really designed to transmit with flat response at all, but to transmit ""nicely sounding"", which obviously isn't quite well-defined.</p>

<p>Studio monitors and -headphones are, in principle, designed to operate as linearly as possible. Incidentally, this means they <em>don't</em> really sound ""good"": they're most of all supposed to <em>reveal</em> any issues in the material. Sound engineers value this, obviously. But again, <em>mastering engineers</em> anticipate that most listeners will use technically inferior speakers, and set up the mastering accordingly.</p>

<h2>Music source</h2>

<p>This, in a way, means the actual bottleneck is probably the orginal source; not the file format but the mastering. Indeed many of today's records are mastered quite horribly IMO, <a href=""http://en.wikipedia.org/wiki/Loudness_war"" rel=""nofollow noreferrer"">grossly overcompressed</a> etc..</p>

<p>Of course this depends on what style of music you hear. Classical music and Jazz are least affected, Pop through Electronic and Metal are most. What can you do about it? It's nothing you could fix by buying better listening equipment; if anything you could get <em>worse</em> equipment to better match the master engineers' expectations.</p>

<p>Just try and be selective about what tracks you listen to, that's the advice I'd give you. A great recording<sup>3</sup> over 30$ equipment is much more enjoyable than a bad recording over 1000$ equipment.</p>

<p><hr>
<sup>1</sup>Actually, that's only true for asynchronous transport, as in your case. If you need realtime performance, then there are sure enough a couple of issues in the digital transmission.</p>

<p><sup>2</sup>Of course, I'm leaving out deviced <em>designed to alter the sound</em> here, like EQs etc..</p>

<p><sup>3</sup>Sure enough, in the <em>recording studio</em> they will have needed expensive equipment. Recording has much higher demands (e.g. low-noise mic preamps) than listening.</p>
","9589"
"Rough Idea of Time Spent Editing Videos","627","","<p>I currently am the Video guy for our company. We do video training for installs for automotive parts. I do the planning for the videos. I do the shooting of the videos. I do the editing of the videos. I do the final release of the videos.</p>

<p>My boss wants a report about the amount of time it should be taking me to do this. Its a fair question and I'm all about self improvement and figuring out where I can do my job more effectively, especially if it will help me get a raise or move up in the company. My question comes from the amount of time I should be allotting for the start to completion for a video. Its not my only job, but it is about 30-35 hours a week, so a huge portion.</p>

<p>So how much time should I be allotting for start to finish of a video. I have put together several of the parameters bellow.</p>

<p>SO lets assume I was going to release a 10 minute video on YouTube. </p>

<ol>
<li>I do my planning for the different shots I'm going to need.</li>
<li>I work with our other guy who is the ""Host"" or ""Talent"" on screen to get together what he needs to cover.</li>
<li>We start shooting, We do about 3 hours of shooting/installing, giving me what I need to work from.</li>
<li>I start editing the video, I join the audio, I sort through the footage, Pick all the different pieces I'm going to use, Cut in each scene (Which is rather easy because we shoot in consecutive order of the build), Lay in the intro &amp; outro graphics, Lay the sound effects/background music (Which is typically most of the video and the same), Export the video, Make any suggestions/changes from the installers, Build the video thumbnails.</li>
<li>I upload the completed video to YouTube, add to other website locations, send an email its been release, write any type of descriptions/keywords.</li>
</ol>

<p>Now I know how long I'm spending on each video roughly, its about 2-3 weeks time per video, and sometimes even more than that. I have about 4 years of experience doing this. I also know that it varies from person to person based on skill. I do have plenty of room for improvement, I know that. How long <strong>SHOULD</strong> it take me to complete a project like this? Am I in the ball park? Am I running really slow? Or am I doing good for my time frame?</p>
","<p>This is a tough question because there are a ton of different factors involved here, from complexity of the video to the equipment you're working with to the NLE you use. However, the default formula we use to give time estimates to clients breaks down like this (I've run a small video production company for a few years working for a large variety and diversity of clients):</p>

<h2>Pre-Production</h2>

<p><strong>2 hours per minute of final product</strong>:
This is a number that can swing wildly in each direction, mostly depending on how much pre-production we have to do. For instance, if a client already has a script prepared, we typically don't have to do much besides potentially locking down a location, prepping a shot list, and lining up a crew. However, if a client comes to us and just says ""I want a video to express such and such"" (without a concept), we'll spend a large amount of time brainstorming, writing, etc. Typically we'll even send the client 3 fairly well-developed concepts to see which one they like best.</p>

<h2>Production</h2>

<p><strong>1 hour per minute of final product</strong>: This is the number that stays the most consistent across all our projects, and really boils down to only a few factors. The biggest factor is who we're working with - if we get to use our own (experienced) actors in the video, everything goes smoothly. If a company or organization wants to put their own people into the video, it takes longer because (most of the time) these people don't have acting experience and thus haven't memorized the script, aren't strong in staying consistent between takes, etc.</p>

<p>The other main factor in production time is the equipment you're using and how complex the shoot is. More cameras cuts down on production time because you can grab multiple angles at once (we typically shoot with 2 cameras at a time). A complicated shoot involving a lot of equipment set-up/breakdown will take much longer than a shoot with just a camera on a tripod. Set-up and breakdown of equipment (especially large equipment like a jib or dolly) can take a ton of time.</p>

<h2>Post-Production</h2>

<p><strong>1.5 hours per minute of final product</strong>: Like pre-production, this one can swing pretty wildly in either direction, but I almost never give an estimate of less than 1.5 hours/minute of final product. This one really depends on what exactly you're doing in your post-production workflow - are you just throwing clips together on a timeline and then exporting it? Or are you going through all the extra steps, like color correcting/grading, audio cleaning/mixing, adding titles, VFX, etc.? Typically, if we're on a deadline, we'll cut unnecessary steps from that workflow to get a video out on time (most clients may not care if you don't color grade a project simply because most people won't notice it if it's not horrible).</p>

<h2>Total</h2>

<p>This works out to around 4.5 hours of work per minute of ""final product"" (a 10-minute video would take 45 hours, for instance). Like I said, there's a ton of volatility to that number - we've knocked out a 4-minute video in 10 hours, but we've also spent 20 hours on a 1 minute video. The biggest thing I've learned while in this industry is that people who are unfamiliar with the process of video production <em>always</em> underestimate how long a video will take from start to finish. To do it right, it takes time. </p>

<p>In your specific case, it looks like your video has a decent number of angles, cuts, etc., and your production value is fairly high. Your videos are long, too, so I'd say 2-3 weeks of working 30-35 hours/week is definitely within the acceptable range.</p>
","19043"
"How do I create this slide/bouncy effect in After Effects?","626","","<p>The news channel in my country is using a nice bouncy/slide effect. Now I'm trying to recreate that in After Effects. I tried many effects and expressions, Googled a lot and watched many tutorials. But none fit to my expecting result.</p>

<p>Does someone know how to recreate those effects? With expressions or maybe a nice plugin?</p>

<p><a href=""https://i.stack.imgur.com/RF2rW.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RF2rW.gif"" alt=""Animation""></a></p>

<p>Look very close to the end of the bar. Its bouncing a little bit.</p>
","<p>Key is creating 2 additional keyframes after the last one (one copy of the last one 5-10 frames ahead and another one in between). The keyframe in between is for shifting the position slightly into the opposite direction. In order to get a non linear animation ease all the keyframes at the end.        </p>

<p><a href=""https://i.stack.imgur.com/gf5Ha.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gf5Ha.jpg"" alt=""enter image description here""></a></p>

<hr>

<h3>Instructions</h3>

<ol>
<li>Create a new composition and a new solid layer with the dimensions of the gauge</li>
<li>Click the solid layer and create a mask (double click the rectangular mask path icon to create new mask with the dimensions of the solid layer)</li>
<li>Open up the mask properties of the solid / gauge layer <kbd>M</kbd><kbd>M</kbd></li>
<li>Go to the end of the composition and add a keyframe by clicking on the stop watch of the mask path property</li>
<li>Go to the first frame of the composition and drag both right mask bezier points to the left (this will automatically insert a new keyframe)</li>
<li>Copy the last keyframe 5-10 frames ahead and another one in between</li>
<li>With the cursor on the keyframe in between, drag both bezier points on the right side of the rectangle slightly to the left</li>
<li>Select the last 3 keyframes</li>
<li><em>Right Click > Keyframe Assistant > Easy Ease</em> or simply hit <kbd>F9</kbd></li>
</ol>

<h3>Examples</h3>

<p><em>Bounce to the right</em></p>

<p><a href=""https://i.stack.imgur.com/eydZS.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eydZS.gif"" alt=""enter image description here""></a></p>

<p><em>Bounce to the left</em> </p>

<p><a href=""https://i.stack.imgur.com/I94Cq.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I94Cq.gif"" alt=""enter image description here""></a></p>

<hr>

<p><a href=""https://www.dropbox.com/s/gvocdxhqcxi5xsp/gauge.aep.zip?dl=0"" rel=""nofollow noreferrer""><strong>Project File Download</strong></a></p>
","16048"
"ffmpeg streaming to rtmp and extracting image periodically","625","","<p>I'm using this command to receive a input stream and transcode it to a different resolution and stream it to ustream. </p>

<pre><code>ffmpeg -hide_banner -loglevel info -progress /tmp/ffmpeg.log -i udp://10.0.0.150:8181?listen \
-framerate 30 -video_size 1080x720 -vcodec libx264 -b:v 768k -crf 23 -preset medium -maxrate 800k -bufsize 800k \
-vf ""scale=640:-1,format=yuv420p"" -g 60 -c:a aac -strict -3 -ar 44100 -ab 32k -f flv rtmp://&lt;ustream url&gt;/&lt;ustream key&gt;
</code></pre>

<p>I also want to extract a full quality snapshot every 1 minute. I read <a href=""https://trac.ffmpeg.org/wiki/Create%20a%20thumbnail%20image%20every%20X%20seconds%20of%20the%20video"" rel=""nofollow"">here</a> how I can extract a thumbnail from a video periodically and I read <a href=""https://trac.ffmpeg.org/wiki/Creating%20multiple%20outputs"" rel=""nofollow"">this</a> explaining how to send one input stream to different output streams with different video filters.</p>

<p>When I combined the two commands I ended up with this:</p>

<pre><code>ffmpeg -hide_banner -loglevel info -progress /tmp/ffmpeg.log -i udp://10.0.0.150:8181?listen -filter_complex '[0]split=2[in1][in2];[in1]scale=640:-1,format=yuv420p[out1];[in2]fps=1/60[out2]' \
-map '[out1]' -framerate 30 -video_size 1080x720 -vcodec libx264 -b:v 768k -crf 23 -preset medium -maxrate 800k -bufsize 800k -g 60 -c:a aac -strict -3 -ar 44100 -ab 32k -f flv rtmp://&lt;ustream url&gt;/&lt;ustream key&gt; \
-map '[out2]' img%03d.jpg
</code></pre>

<p>The command almost works, but the audio is missing on the livestream. The images are created correctly and the video is fine but there is no audio. How can I also get audio in my livestream?</p>
","<p>Try </p>

<pre><code>ffmpeg -hide_banner -loglevel info -progress /tmp/ffmpeg.log -i udp://10.0.0.150:8181?listen -filter_complex '[0:v]split=2[in1][in2];[in1]scale=640:-1,format=yuv420p[out1];[in2]fps=1/60[out2]' \
-map '[out1]' -map 0:a -framerate 30 -video_size 1080x720 -vcodec libx264 -b:v 768k -crf 23 -preset medium -maxrate 800k -bufsize 800k -g 60 -c:a aac -strict -3 -ar 44100 -ab 32k -f flv rtmp://&lt;ustream url&gt;/&lt;ustream key&gt; \
-map '[out2]' img%03d.jpg
</code></pre>
","17375"
"iMovie 11 starts with a black screen. How can we make this a white screen?","623","","<p>I want to make a movie that is a continuous loop. All images have white background.  When I make the movie with iMovie it starts and ends with a black background image.  This does not look good when the video loops.</p>

<p>How do we control the basic background in iMovie?</p>
","<p>If there is an image with a white background at the start and end of the video, it should never export a black frame at either end.  (I've never seen it do that, and I've used iMovie '09 to edit hundreds of videos!)</p>
","12219"
"Camcorder pass-through or stand alone capture box for VHS source","619","","<p>What would yield better quality: camcorder pass-through or buying a more recent dedicated capture box?</p>

<p>I have a Canon Elura 65 that I've used in the past to capture video from VHS sources. It produces 480i AVI files. However, I lost the AV cable.</p>

<p>So, should I get a new cable from EBay or buy a dedicated capture box?</p>
","<p>This is really going to depend on the quality of your camera and the quality of your VCR.  In general, the circuitry that is used to digitize an analog signal has grown by leaps and bounds in the past 10 to 15 years, but also VHS tapes and VCRs produce limited quality to begin with.   For maximum quality you will certainly want something with an S-Video input and a VCR deck with an S-video output to maximize quality.</p>

<p>As far as the line input goes, I'd probably favor the line input on a high quality camera from the end of the DV era over a cheap USB capture device, but a modern moderate or high quality capture device is going to do much better than even a high end in-camera analog to digital converter.  Even cheap USB devices now are most likely better than cheap camcorders or early camcorders in the DV era, but millage will vary depending on the specific device.</p>
","10688"
"Split PGS subtitles (.sup file)","619","","<p>I'd like to know if there is any method to split a PGS format subtitle pulled from a bluray as a .sup file.</p>
","<p>Here is the structure of a .sup subtitle file and a link to the splitter I created. Information is either from linked reference or my own research.</p>

<p>The .sup subtitle splitter I created. <a href=""http://forum.doom9.org/showthread.php?p=1737122"" rel=""nofollow"">http://forum.doom9.org/showthread.php?p=1737122</a></p>

<p>SupRip github page with sup related code. You can read more specifics about each section here that I don't discuss.
<a href=""https://github.com/peterdk/SupRip/blob/master/Bluray%20Sup.txt"" rel=""nofollow"">https://github.com/peterdk/SupRip/blob/master/Bluray%20Sup.txt</a></p>

<p>The hex 0x50 0x47 are headers for sections in the .sup file, however there are multiple sections per subtitle line. There is always (in my experience) a 0x16, 0x17, 0x14, 0x15, and 0x80 flagged section for displayed subtitles. There can be multiple 0x15 sections if the bitmap data is large. There are also ""empty"" subtitles which are used to end a subtitle which only have 0x16, 0x17, and 0x80 flags. Subtitles only have a start time and no end time or duration, so a subtitle is ended by the next subtitle starting. So if two subtitle lines are separated by a time period, a blank subtitle section is used to end the first subtitle.</p>

<p>Referencing the linked page these are the flags each header can have.</p>

<pre><code>TIMES = 0x16
SIZE = 0x17
PALETTE = 0x14
BITMAP = 0x15
END = 0x80
</code></pre>

<p>Hopefully explaining everything with hex code of a full subtitle should help. I separated each of the sections and truncated the palette and bitmap sections.</p>

<p>Header structure, always 13 bytes long</p>

<pre><code>50 47        start of header
00 01 E3 E0  time code
00 00 00 00  second time code (normally zero)
16           flag
00 13        data size
</code></pre>

<p>To get the time code to a usable form take the hex and convert to decimal, then multiple by 100,000 and divide by 9 to get nanoseconds.</p>

<p>0001e3e0 -> 123872 *100,000/9 = 137635555 ns = 1.37635555 sec</p>

<pre><code>start of .sup file
1st subtitle

50 47 00 01 E3 E0 00 00 00 00 16 00 13 07 80 04 38 10 00 02 80 00 00 01 00 00 00 00 05 41 02 6F   Times section of subtitle, gives the start time of the subtitle

50 47 00 01 E3 34 00 00 00 00 17 00 13 02 00 05 41 02 6F 00 81 00 37 01 02 40 03 CB 03 04 00 46   Size section of subtitle

50 47 00 01 CD 04 00 00 00 00 14 01 47 00 00 00 ...       Palette section of subtitle

50 47 00 01 CD 2C 00 00 00 00 15 0C 68 00 00 00 ...      Bitmap section of subtitle

50 47 00 01 CD 2C 00 00 00 00 80 00 00     End section of subtitle, denotes the end of one subtitle

2nd subtitle (actually blank which ends 1st subtitle)

50 47 00 07 E7 80 00 00 00 00 16 00 0B 07 80 04 38 10 00 05 00 00 00 00   Times section of subtitle, gives the start time of the subtitle. Previous subtitle ends when this ""subtitle"" begins

50 47 00 07 E6 D4 00 00 00 00 17 00 13 02 00 05 41 02 6F 00 81 00 37 01 02 40 03 CB 03 04 00 46   Size section of subtitle

50 47 00 07 E6 D3 00 00 00 00 80 00 00      End section of subtitle, denotes the end of one subtitle
</code></pre>
","17134"
"What is a projector's ""pixel frequency""?","618","","<p>I am looking at the specs of a BenQ projector and trying to understand it's output frequency. </p>

<p>On page 74 of <a href=""http://www.projectorcentral.com/pdf/projector_manual_5848.pdf"" rel=""nofollow"">the manual</a>, there are specs for vertical, horizontal and ""pixel"" frequency. What does pixel frequency mean?</p>
","<p>Pixel frequency, also known as <em>dot clock</em> is referring to the bandwidth of the projector's controller chip.</p>

<p>For a resolution a certain <em>data rate</em> is required to display all the data (not pixel frequency, see later) - it can be for example:</p>

<p>A resolution of 800x600 at 60 Hz would require:</p>

<pre><code>800 x 600 x 3 (RGB) x 60 Hz/2 (interlaced frame rate) = 43 200 000 bytes per second
</code></pre>

<p>or divided on 1024^2 = 41.2 Mb/s, as data rate with full 8-bit color-depth.</p>

<p>To calculate the projector/monitor's <em>bandwidth</em> you will need accurate timing measurement available, but you can get close by using the vertical frequency and compensate for VBLANK (vertical blanking which is a sync feature) with about 5% and HBLANK with about 30%:</p>

<p>So with 800x600 resolution at 60 hz:  </p>

<pre><code>800x1.3 x 600x1.05 x 60 = approx. 39 Mhz
</code></pre>

<p>The closer these two results get the better it is as that will tell that the projector/monitor is capable to handle the needed data rate to get full resolution, color depth as so forth.</p>

<p>The bandwidth becomes a factor when showing video in particular.</p>
","5617"
"ffmpeg - mapping an audio file with 2 channels to a video file?","612","","<p>I have a video file and an audio file (that contains 2 channels) and i want to map the video to stream 0 of the resulting file and the audio file to the next stream/s.</p>

<p>Since the audio file has 2 channels, would i have to map it to 2 separate streams? (1 and 2 of the resulting file)
Or just one stream? </p>

<p>Finally, how should i go about doing that?
I've only tried mapping with video files so i'm not sure how to do it with audio ones.</p>

<p>Thanks!</p>
","<p>Channels are conjoint tracks within a single stream, so a stereo audio WAV has one stream which contains two channels. If this is what you have, use</p>

<pre><code>ffmpeg -i video -i audio -map 0:v -map 1:a output
</code></pre>

<p>If they are distinct streams, then the above command will still map all the streams, but for explicit specification, use</p>

<pre><code>ffmpeg -i video -i audio -map 0:v -map 1:a:0 -map 1:a:1 output
</code></pre>
","18739"
"How does HDR relate to color space and bit depth?","611","","<p>I'm not a photographer, but I'm struggling to understand HDR technology as it relates to TVs and monitors, so I thought this would be a good place to ask.</p>

<p>First, I got a good understanding of the terms ""color space"" and ""bit depth"" by reading this <a href=""https://photo.stackexchange.com/a/50551/63891"">excellent answer</a>.</p>

<p>How does HDR technology relate to these terms? Is HDR simply a combination of a 10-bit color depth and really wide color space, or is there something more to it? </p>

<p>In the marketing language, certain areas of the screen (such as the rising sun) are said to be ""brighter"" than other areas of the screen on a HDR display. What does ""brighter"" actually mean here  does it just mean ""farther to the edge of the color space""?</p>

<p>I currently have a DELL U2410 monitor, which, as I understand, has a 12-bit internal processor and an 8-bit panel with FRC dithering, and supports an Adobe RGB color space. Even though this is not true 10-bit, will it produce a result similar to HDR if I use a device which can output HDR? I currently set the color space to ""sRGB"" since Adobe RGB looks too oversaturated. Could this be because my graphics card does not support a 10-bit output?</p>

<p>EDIT: I am talking about ""HDR"" specifically as it relates to how the term is used in modern 4K TVs. </p>
","<p>TV technology and video encoding is in principle outside the scope of this site. 
Still, as photographers we are presumably interested in new display standards that promise both more dynamic range and wider color gamut. HDR computer monitors <a href=""http://www.pcworld.com/article/3154466/displays/jaw-droppingly-gorgeous-hdr-explodes-onto-pc-monitors-at-ces-2017.html"" rel=""nofollow noreferrer"">are coming</a>, so it probably won't be long before photographers too will be interested in HDR displays.</p>

<p>HDR by itself just means High Dynamic Range. Dynamic range is the brightness ratio between the brightest and the darkest parts. ""High"" is relative to what the sensor can capture or to what a standard display can show.</p>

<p><a href=""https://en.wikipedia.org/wiki/High-dynamic-range_video"" rel=""nofollow noreferrer"">HDR video</a> is one of several standards for video encoding. They have names like HDR10, Dolby Vision, and Hybrid Log-Gamma. They all support higher bit depth (10-12 bits), wider color gamut (<a href=""https://en.wikipedia.org/wiki/Rec._2020"" rel=""nofollow noreferrer"">Rec. 2020</a> and/or <a href=""https://en.wikipedia.org/wiki/DCI-P3"" rel=""nofollow noreferrer"">DCI-P3</a>), higher dynamic range, and 4k resolution. They are not compatible with each other, but some devices support more than one HDR standard.</p>

<p><a href=""http://www.trustedreviews.com/opinions/hdr-tv-high-dynamic-television-explained"" rel=""nofollow noreferrer"">""HDR TV""</a> doesn't necessarily mean much, except that it will accept a HDR video signal and display an image. It does <em>not</em> necessarily mean that it can <em>display</em> the wider gamut and higher dynamic range, just that it can read the signals and show something, even if what it shows is plain sRGB with a mundane dynamic range.</p>

<p><a href=""https://www.cnet.com/news/what-is-uhd-alliance-premium-certified/"" rel=""nofollow noreferrer"">""UHD Premium""</a> is a certification standard for displays that can actually <em>show</em> HDR. Requirements include:</p>

<ul>
<li>minimum 10 bit color</li>
<li>minimum 90% of the DCI-P3 color gamut</li>
<li>use a HDR transfer function <em>(The transfer function for sRGB is called 'gamma', it translates bit value to brightness value. HDR uses a different function so it can cover a larger dynamic range with a small increase in the number of bits)</em></li>
<li>either: Minimum 1,000 nits brightness and max 0.05 nits black level, contrast ratio 20,000:1 or better (for LCD displays)</li>
<li>or: Minimum 540 nits brightness and max 0.0005 nits black level, contrast ratio 1,080,000:1 or better (for OLED displays, they can't go as bright but have better black levels)</li>
</ul>

<p>Standard displays are 200-400 nits and rarely go beyond 1,000:1 contrast ratio. So a ""Premium"" HDR display has significantly higher peak brightness as well as an order of magnitude better contrast. </p>

<p>Brighter highlights, higher contrast ratios, and a different ""bits to brightness"" encoding/decoding all combine to enable the ""high dynamic range"" part of a HDR TV.</p>

<p>Color space and bit depth is only related to HDR in the sense that they are part of the same standards. Yes, you need more bits to represent a higher dynamic range without banding. But the larger color space is a different decision, it's more a matter of including enough improvements to make a new video standard worth the trouble. (4k is also part of the standard, and 4k is not related to HDR either.)</p>

<p>""The sun looks brighter on a HDR display"" means that a) the display emits more light in the brightest parts of the scene, and b) it may look even brighter because the rest of the scene can be dimmer without losing detail. </p>

<p>So it's a real improvement; the wider color gamut and higher dynamic range can  contribute to more vivid photos and video. </p>

<p>The caveat is that TVs/monitors may be sold as ""HDR compatible"" without any actual HDR capability in the display. The content needs to be made specifically for HDR to make use of the new capabilities. And for computer monitors, you need a graphics card that supports HDR output. </p>
","21678"
"Project Organization, searchable keywords and tags in Adobe Premiere","611","","<p>In Final Cut Pro X you are able to add ""tags"" or ""keywords"" as a form of labeling clips. This way when you are editing you can search to bring up all the clips relating to those tags or keywords. </p>

<pre><code>//////////////////////////////////////////
Example:
Name --- Tags
Clip1: profile, interview
Clip2: audio, interview
Clip3: B-roll
Clip4: B-roll

Search = ""interview""
Results = Clip1, Clip2
//////////////////////////////////////////
</code></pre>

<p>Adobe Premiere uses ""bins"" ""metadata"" and ""labels"" to organize media. Labels however are limited to about 6 I believe. Sure you can create a bunch of bins and copy and paste clips to use bins like keywords in a sense, however one of the great features of editing software is that clips show which parts of them are used in a sequence, when you copy and paste clips in the project panel you now have two separate entities, so if you where to use one and not the other, even though they are the same clip premiere will only show the one you used was used in a sequence. (Which kinda defeats the purpose).</p>

<p>How do you use keywords and or tags just like you can in Final Cut Pro X?</p>
","<p>Keywords are an optional field of the project panel. In order to view or add keywords, you need to set the project panel to <em>List View</em> (you can switch between <em>List View</em> and <em>Icon View</em> with the buttons in the lower left corner).</p>

<p>In the flyout menu of the project panel, select <em>Metadata Display</em> to customize the columns of the list view to show whatever information you want it to. You will find the keywords in the category <em>Dublin Core</em>. Check keywords and click ok. You might have to resize the project panel (or remove the columns you don't need from the Metadata Display menu) in order to see the keyword column. You can rearrange the columns using drag-and-drop. Once you have added some tags to your clips, you can search for clips with specific tags using the search function of the project panel.</p>

<p>There's also some more useful metadata columns hidden in that menu that are not shown by default that you might find useful. For example, the category <em>Premiere Pro Project Metadata</em> has the entries <em>Video Usage</em> and <em>Audio Usage</em> that show you how many instances of a clip exist in your sequences in your current project. There are also fields for <em>Comment</em>, <em>Description</em>, <em>Scene</em>, <em>Shot</em> and much more. Using the Metadata Display menu, you can customize the metadata columns to only show those that you need for your workflow. </p>

<p><a href=""https://i.stack.imgur.com/ZNxCU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZNxCU.png"" alt=""project panel""></a></p>
","20416"
"motion effects missing from adobe premiere elements 10","610","","<p>I'm a new user to Premiere Elements 10.</p>

<p>I want to rotate a video.</p>

<p>However, <strong>the motion effect is not present in the effects panel</strong>.</p>

<p>I have only recently installed Elements.</p>
","<p>It doesn't appear in your panel as rotate, but if you go to your transform effect you will see roll and rotate options in there. </p>
","7190"
"Does ffmpeg have a configuration file?","607","","<p>Using a lot the ffmpeg program, I want to configure ffmpeg on linux so that it always hides banner (option -hide_banner) to reduce text output. I cannot seem to find anything that would tell me where this file should be and how it should be named so that ffmpeg will look at it every time it is run.</p>
","<p>No, ffmpeg does not use a configuration file post-install.</p>

<p>lots of ways to build ffmpeg, but generally you have to configure everything <strong>before</strong> install. </p>

<p>for example, with <code>brew</code> (macOS package manager) you have to edit the following file to supply your <code>--enable or --disable</code> args BEFORE INSTALLING. </p>

<p><code>/usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/ffmpeg.rb</code></p>

<p>if you didn't get your args in on your original install, then you need to uninstall and re-install again with your desired options and args.</p>

<p>also: instead of supplying your options (<code>--with-webp</code> for example) from the CLI during install, you can write them in <code>ffmpeg.rb</code>. </p>
","23118"
"Find/replace duplicate comps?","606","","<p>I'm working on a giant project file that was obviously created by importing lots of other projects into one big one.</p>

<p>Many of the comps use many of the same precomps ... from projects that were copies of each other ... so there are many precomps that are probably identical, but each is used in a different master comp.</p>

<p>Is there any way to (a) check if 2 comps are truly identical?</p>

<p>And (b) automatically replace all references to identical comps with references to just one copy, so that the project can then be properly reduced?</p>
","<p>I do not know of and couldn't find a script that does this already but it would be certainly possible to write one. After Effects doesn't have a functionality built-in for that.</p>

<p>With the help of ExtendScript you can write very powerful extensions for After Effects and its very easy to get going with it. Just open up the ""Adobe ExtenScript Toolkit"" that gets installed with every CS suite and load up an example for After Effects and have a look at the JavaScript documentation. While you can also script with AppleScript and Batch Script the documentation for the JavaScript version is a lot better.
You can access it under the ""Help"" menu. Most importantly look at the After Effects Scripting guide. It can be found for different CS/CC versions <a href=""http://www.adobe.com/devnet/aftereffects.html"" rel=""nofollow"">here</a>.</p>

<p>What I would do is start to iterate through all your compositions, grab the current composition in your iteration, get an array of all the layer names in that composition and then iterate with that array through all compositions and compare the array with the array of all the other compositions. Best to not sort the arrays in case you used generic names for layers and unique layer ordering is important.</p>

<p>Unless you have micro changes in your duplicate comps like changed property values and keyframes this approach would be effective.
If you do have micro changes you would, after having fetched all comps with duplicate layers, also have to access every layer in your array and compare every property with the corresponding layer in the other comp.</p>

<p>You then delete all true duplicates of a comp and while doing that replace them with a link of your now unique comp in all occurrences in other comps.</p>

<p>Shouldn't be much code but it could take some time to write if you have never used Adobe ExtendScript.</p>

<p>On another note, a very helpful plugin to keep the composition import madness at a minimum is <a href=""http://aescripts.com/bao-dynamic-comp/"" rel=""nofollow"">BAO Dynamic Comp</a>.</p>
","12432"
"video letterboxing in premiere pro with the same image as background","603","","<p>I am creating a slide show in premiere pro and the slideshow contains few portrait images and few box images. which give a black letterbox. I want the letterbox to show the same image blurred as background.</p>

<p>example:</p>

<p><img src=""https://i.imgur.com/Tivu37P.jpg"" alt=""example""></p>

<p>or this youtube video: (I dont want to add links)</p>

<pre><code>https://www.youtube.com/watch?v=0Qm07MUkgiA
</code></pre>
","<p>Copy the picture into a second video track. Then, zoom in the picture on the lower track (i.e. increase it's size in the effect settings panel). Then, apply a gaussian blur from the effects panel and raise the blur factor in the effect settings panel until you are satisfied with the result.</p>

<p>If you need more detailed instructions, please provide some additional info on how you set up your project in Premiere Pro, what you have tried to achieve this and why it didn't work.</p>
","15475"
"How to create screen capture video without losing it's quality?","602","","<p>I want to create screen capture tutorials such as <a href=""https://laracasts.com/series/php7-up-and-running/episodes/1"" rel=""nofollow noreferrer"">https://laracasts.com/series/php7-up-and-running/episodes/1</a> (with the HD option) for YouTube. On laracasts' videos, words are excellently readable, however words on my captured video are spreads. </p>

<p>How can I fix it (I'm using Movavi Screen Capture Studio)? 
What program is the best solution for creating tutorials (like laracasts) on MacOS?</p>
","<p><a href=""https://support.apple.com/kb/ph5882?locale=en_CA"" rel=""nofollow noreferrer"">QuickTime</a> is included with macOS. This app has a built in screen capture tool. You can create a new screen recording by pressing Command-N to create a new screen recording after opening QuickTime, then set settings to Maximum Quality by clicking the drop down beside the record button. The video will save after recording to any location on your machine. See more <a href=""https://support.apple.com/kb/ph5882?locale=en_CA"" rel=""nofollow noreferrer"">Here on Apple's Webiste</a>.</p>

<p>After this, you can ingest the footage onto your video editor and send it to someone.</p>

<p>Hope this helps.</p>
","19795"
"Is there a video player where I can draw on top of video?","600","","<p>I wonder if there is a video player (similar to VLC) where I could stop the video, and draw annotations on top of the video (lika on a transparent layer). Then I could choose to erase it when continuing to play the video, or leave it.</p>

<p>The purpose would be showing recordings of football/hockey matches. When stopping the video I could highlight players and draw tactical instructions(using the mouse).</p>

<p>Does such a video player exist?</p>
","<p>Why not just use an app that annotates the screen? Depending on whether you are on Windows, OS X, or Linux there should be plenty of apps when you search for ""screen annotate"" or ""annotation"". Maybe you've searched for the wrong thing until now. </p>
","7416"
"Renaming source files without breaking link in Premiere?","596","","<p>So, I messed up and rushed into a project for work. Instead of giving my files good naming conventions, I worked off a generic file naming convention off the camera. Now I have many edits on premiere but if I want to rename my source files outside of premiere, I find I have to go one at a time. </p>

<p>Otherwise, if I batch rename them all at once I will have no idea what to relink to what in my project. I've searched and haven't found a solution other than renaming the files inside premiere and consolidating the project to copy to new location and selecting to rename media files to match clip names. </p>

<p>I don't like this solution because it means I'll have to use the new premiere files. Much appreciated.</p>
","<p>I figured it out! It was super easy.</p>

<p>I used Adobe Bridge to batch rename. Very importantly, I left the preserve current filename in XMP metadata option checked. <a href=""https://i.stack.imgur.com/CwIhj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CwIhj.png"" alt=""Adobe Batch Rename""></a></p>

<p><strong>BUT</strong> before clicking rename, I first clicked preview and exported the CSV file and saved it somewhere safe. This file will help you remember the original file name. In this instance I will know that file 001.png was renamed to 120.png. This is very important.<a href=""https://i.stack.imgur.com/yoxuF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yoxuF.png"" alt=""enter image description here""></a></p>

<p>Now in Premiere, all your renamed files will go offline. If all your files in one location on your drive, this next step is very easy. Just link one file and if you can't remember what you renamed it to, the beauty is that you can quickly search your CSV file in many ways like excel. Then just relink your file to the renamed file that way. Premiere will recognize all your other renamed files because you preserved the XMP meta data. And that's it! </p>
","21435"
"In search of a Helmet-based steadicam solution for GoPro Hero 3","595","","<p>I recently tested my GoPro Hero 3 for the first time during a run in the park using a helmet mounting. It's not bad footage but it's very 'bouncy'. I've been looking at a few options to stabilize the camera. There are some interesting steadicam options out there and some very fine DIY videos on YouTube. However most helmet-based steadicam solutions involve mounting the camera on a long pole which includes a counter weight.</p>

<p>I'm going to be running the Tough Mudder (<a href=""http://www.toughmudder.com"" rel=""nofollow"">http://www.toughmudder.com</a>) soon and I don't think sticking the rig of a long pole will be particularly convienent, least of all for the other runners whom I might injure.</p>

<p>Is there an alternative steadicam-style helmet mounting that I could buy or make?</p>

<p>Thank you.</p>
","<p>Unfortunately, the physics of a Steadycam require a counterweight (sometimes provided by springs) and distance to minimize the impact of movement by using the inertia of the system.  The best you might be able to do in a small size would be a shock mount.  If you had a power source, you could presumably put a small gyroscope in to try and further stabilize the rotation, but that would become pretty (very) expensive really fast.</p>

<p>A shock mount is probably your best option within a semi-affordable range and will deal with the actual movements forward and backward, up and down, etc, but not rotation.  It does appear that people have made multi-axis gyro mounts that would work, but they've all been custom builds and the power and weight is still probably prohibitive to wearing it in a run.  It would also still have to be shock mounted to account for lateral movements as the gyro would only deal with rotation.</p>
","8421"
"Overloading in a Mackie profx8","595","","<p>On one channel of my Mackie mixer, the OL light is on whatever I do. The channel sounds like it's working but the OL light is lit. What is the reason?</p>
","<p>Try turning the gain all the way down and unplugging the channel.  If the overload light is still lit, then there is a fault with your board.</p>
","8427"
"Editing 1440p @ 47 FPS in Premiere Pro CS6","594","","<p>I am attempting to edit the videos I've recorded with a GoPro Hero 3 in Premiere Pro CS6. However, the preview window jumps, is slow and not smooth at all. As this pretty much makes video editing impossible, I was looking for a solution.</p>

<p>Are there any settings in Premiere Pro CS6 which would fix this or are you guys able to recommend me another program for Windows 7 which has a comparable features?</p>

<p>My computer specs:</p>

<ul>
<li><strong>OS:</strong> Windows 7 Professional 64-Bit</li>
<li><strong>RAM:</strong> 32 GB</li>
<li><strong>Processor:</strong> Intel i7-3820 @ 3.60Ghz (8 CPUS)</li>
<li><strong>Graphic Card:</strong> NVIDIA GeForce GTX 670</li>
</ul>
","<p>This sounds like a hardware problem.  The playback engine in Premiere has been very, very fast since CS5, so it is unlikely to be a software problem.  The data rate for 1440p video at 47fps is going to be astonishingly high (170MB/sec once decoded, for comparison, 1080p 24fps is just under 50MB/sec) and extremely demanding on your system.  Based on your system specs, it probably isn't a processing or memory problem, particularly if you have CUDA enabled, however you may be running in to a bandwidth problem loading off your hard-disk or loading in to memory (depending on your memory speed).</p>

<p>It also may be an issue with the format of the video you are using.  Not all video formats are designed for random access.  Even with a very powerful computer, if the file format you are working with requires rendering out multiple frames from the latest key frame every time that you move your preview, you will hammer your computer far harder.  (In the worst case, you end up having to load over a dozen frames of video to get one frame.)</p>

<p>It also wouldn't hurt to double check if anything is bottle necking by looking at Task Manager when you are previewing video.  Does memory utilization or CPU get high?  I expect it will be hard disk access (which won't peak the CPU or memory utilization) but it doesn't hurt to double check.</p>
","9978"
"Correcting video flicker problem caused by camera's auto white balance","592","","<p>I have shot a video with a handheld camera that uses auto white balance. Unfortunately, the auto white balance seems to have done more harm than good. The camera was readjusting the white balance every time when it was moved. The video ends up having many transitions from dark to bright then back to dark every few seconds throughout it. The video looks as though it was flickering in slow motion. </p>

<p>I have tried adjusting the colour balance of the video in After Effects but because the white balance by the camera was on every frame, a linear colour balance correction doesn't seem to help.</p>

<p>Is there any way to correct this problem without having to retake all my shots?</p>
","<p>Probably not easily.  You might be able to try applying a better auto-white balance and hope for the best.  You could also manually adjust it repeatedly and adjust the interpolation between values (on the animation curves) but this is highly time consuming and still won't get as good of a result as reshooting.</p>

<p>If reshooting is an option, it's going to probably be the quickest, easiest and best looking solution.  If not, then your stuck trying some very painful and time consuming adjustments.</p>
","9357"
"Encode video for html5 and javascript controlled playback","592","","<p>I want to create something like Apples Mac Pro product page <a href=""http://www.apple.com/de/mac-pro/"" rel=""nofollow"">http://www.apple.com/de/mac-pro/</a> where html5 video playback is being controlled via javascript (by manually setting the currentTime attribute of the video). It works perfectly when I use their video but not at all with my own videos. How can I convert my videos so that they play back nicely?</p>

<p>Using ffmpeg I tried everything I could think of like</p>

<ul>
<li>removing the audio track</li>
<li>decreasing the videos dimensions</li>
<li>decreasing the framerate</li>
<li>setting the framerate to 30fps (like in apples video)</li>
<li>converting their video to jpgs and create a video from those frames (i thought maybe the color-spectrum made a difference (mainly black))</li>
<li>different options for quality (-crf [18-30] -preset [veryslow-veryfast])</li>
<li>decreasing the length of the video (apples video is 47s long)</li>
</ul>

<p>I also tried some different options of handbrake but nothing helped.</p>

<p><a href=""http://movies.apple.com/media/us/mac-pro/2013/16C1b6b5-1d91-4fef-891e-ff2fc1c1bb58/videos/macpro_main_desktop.mp4"" rel=""nofollow"">http://movies.apple.com/media/us/mac-pro/2013/16C1b6b5-1d91-4fef-891e-ff2fc1c1bb58/videos/macpro_main_desktop.mp4</a></p>

<pre><code>ffmpeg -i macpro_main_desktop.mp4 
</code></pre>

<p>shows:</p>

<pre><code>Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'macpro_main_desktop.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp42mp41
    creation_time   : 2013-10-16 22:55:31
  Duration: 00:00:47.03, start: 0.000000, bitrate: d kb/s
    Stream #0:0(eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1120x840, 2797 kb/s, 30 fps, 30 tbr, 30 tbn, 60 tbc (default)
    Metadata:
      creation_time   : 2013-10-16 22:55:31
      handler_name    : Apple Video Media Handler
    Stream #0:1(eng): Data: none (rtp  / 0x20707472), 139 kb/s
    Metadata:
      creation_time   : 2013-10-16 22:55:31
      handler_name    : hint media handler
</code></pre>

<p>here is an example of the videos I encoded via ffmpeg:</p>

<pre><code>Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video-720-main-30fps.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf56.15.102
  Duration: 00:05:00.49, start: 0.033333, bitrate: 420 kb/s
    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 284 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 128 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>
","<p>The Apple video uses a profile with fewer reference frames and also carries a streaming hint track.</p>

<p>The following ffmpeg command template should create a quick seeking MP4 file:</p>

<pre><code>ffmpeg -i input -c:v libx264 -profile:v baseline -x264opts keyint=3:min-keyint=2 -{other video encoding parameters} -{audio encoding parameters} -movflags +faststart+rtphint output.mp4
</code></pre>

<p>Given the profile and GOP sizes set, compression effiency won't be great.</p>
","16838"
"Halogen brightness for video","592","","<p>I'm trying to get an idea of the brightness of halogen lights before I go and buy any. I'm going to be producing a music video and for some indoor scenes I'll need lighting. I'm on a small budget unfortunately so hiring hundreds of dollars of lighting is not on the agenda.</p>

<p>I was planning on getting some cheap work lights of 250W or 500W but I've heard people say that 500W lights are actually very bright and get rather hot.</p>

<p>If I was looking for an exposure at 1/50th of 1/100th (for 50fps shots) at f/2.8-4 at around ISO 100 or 160, will a 500W light be too bright? I obviously want the ISO to be as low as possible since I'm working with an APS-C DSLR, not a dedicated cinema camera.</p>

<p>I do have an ND4 filter on hand. The only remaining issue would be controlling the light from the work lights.</p>
","<p>If anyone was curious, I managed to pick up a 400W halogen work light (supposedly delivering equivalent to 500W) and the exposure within a few metres (3-4) was spot on. I also bought a 250W halogen that I would probably use more as a fill light as it's a stop or so darker.</p>

<p>At f/2.8, ISO 100-320, 1/100th at 50 frames per second, the exposure on the histogram was pretty well on the money.</p>

<p>And the light cost me all of $20. I filmed it at 3000K (approximate temperature of halogens) using the CINEMA pf2 profile on my Canon EOS 60D. The profile produced an almost completed colour grade.</p>
","3504"
"After reading separate video and audio tracks (in Shotcut), just export a portion of it as one video","591","","<p>I am using Shotcut for my video editing (I am a beginner). I record one big video file, and one audio file separately, but when I load them into Shotcut actually I just need some portion of it. Is there a way then when I have read in the video and the audio into separate tracks to just export a portion of it (like from 1min - 2min or so)?</p>

<p>The way I do it now goes like this: I add them both to Shotcut, export everything, so that I end up with one big mp4 file. Then I reread this into Shotcut and just add the portion (with the blue/red-slider beneath the preview) I actually want (as a track), and then I export this again. But this additional step of exporting (and thereby rendering) everything I wish to skip, as it takes some time. So is there a way to do this?</p>
","<p>I am using Shotcut in version 17.03.02. There are probably more ways how you can accomplish this.</p>

<p>The simplest is to mark <strong>in</strong> and <strong>out</strong> points in you video after adding it to the playlist. You can do this using keyboard (I - for in) and (O - for out) also while playing the clip. </p>

<p>Then you can right click the file name in playlist and either <em>Replace</em> the clip with the new cut points or <em>Insert cut</em>, basically inserting a second clip with the cut points marked. Then you can append the file into the Video Track. Do the same with your audio file. Create cut points and append it to a new audio track. </p>

<p>You can do some additional trimming in the track editor. For example using the <em>Split At Playhead</em> and delete the parts you don't want. That way you can trim both files to the same length. Now you have to export the project. </p>
","20974"
"How to deal with this heavily underexposed footage","588","","<p>I recently was at friends birthday with a few musical performances and this friend asked me to film the event. Unfortunately, it was really badly lit (I did notice it, but I couldn't do anything about it). So I had a bad feeling, but I shot as much footage as I could anyway. So, this is how it turned out:</p>

<p><img src=""https://i.stack.imgur.com/Ilghj.jpg"" alt=""original still""></p>

<p>With a luma curve adjustment, raised saturation and an orange-ish color-grading, it looks like this:</p>

<p><img src=""https://i.stack.imgur.com/60f7U.jpg"" alt=""still with corrections""></p>

<p>If I lighten it up even more, it becomes even more grainy. Also, the missing color informations become more obvious: </p>

<p><img src=""https://i.stack.imgur.com/xPqev.jpg"" alt=""still with corrections, lighter""></p>

<p>So, my question; should I rather sacrifice some visibility so that it looks not totally grainy (2nd picture) or lighten it up even more at the cost of strange-looking colors and very grainy scenes (3rd picture)? And, using either variant as a starting point, how could I further increase the quality of the result? Especially bring more color into it while maintaining a natural look and reduce the noise. I'm working with Premiere Pro CC. I've tried the <em>remove grain</em> effect in After Effects, but that didn't yield great results (I'm not sure if I got the settings right though, I'll try it again later). I also found the <a href=""http://www.neatvideo.com/"" rel=""nofollow noreferrer"">Neat Video</a> Plugin for PP and AE, but I'm not making any money from this, so I'm not so keen on spending money on it ...</p>

<p><strong>Edit:</strong> As requested, here are some <a href=""https://www.dropbox.com/sh/7q73qgz5e013r0u/AAApYTMX0Kc7FAzMoN_SK4Yka?dl=1"" rel=""nofollow noreferrer"">uncompressed stills</a> (directly exported frames from Premiere). I've decided to go with the 'middle ground' option (uncompressed_graded.bmp).</p>
","<p>If this is no raw material, there is no additional pixel information to balance your colors and achieve a natural look. Only chance you have is to denoise your video and desaturate the colors. Try to find a good relationship between brightness and noise to get an acceptable result.</p>

<p>Unfortunately there is no good, free, build-in or open source solution at the moment. Denoising a video requires very complex algorithms, not only because every camera produces different noise types. Currently best approach are <a href=""http://cs.haifa.ac.il/hagit/courses/seminars/wavelets/Presentations/Lecture09_Denoising.pdf"" rel=""nofollow"">wavelet algorithms</a>, but there are only 2 tools which use it:</p>

<ul>
<li><a href=""http://www.neatvideo.com/"" rel=""nofollow"">Neat Video</a></li>
<li><a href=""https://vimeo.com/26571727"" rel=""nofollow"">Nuke's Denoiser</a></li>
</ul>

<p>Note: In my experience Neat Video does a very good job and in some cases you are able to denoise the video twice without getting an <em>over blurred image</em>. </p>
","15285"
"how to do an amplitude modulation in Ableton Live","585","","<p>What I want is simple: I want to modulate the volume (amplitude) of one channel, with the amplitude of another channel.</p>

<p>There's the vocoder effect which does something vaguely similar to what I'm looking... but it splits the audio in bands to (obviously) create the vocoder effect.</p>

<p>A sidechain compression/gating won't do it either.</p>

<p>So how can I do this?</p>
","<p>If you have Max4Live (included in Live9 Suite), it should be easy enough to build a Live device that does the amplitude modulation exactly the way you want.</p>

<p>Otherwise, look for Ring Modulator and Amplitude Modulation VST plugins. The easy way to write a Ringmod plugin is to just do amplitude modulation. So the free plugins are likely to do what you want. Actually modeling an analog ring modulator takes more effort, but maybe <em>that</em> is the sound you are really looking for.</p>

<p>If you happen to have Absynth, I'm pretty sure you can accomplish AM or ring modulation on arbitrary incoming signals with that.</p>
","7818"
"Video noise and harsh shadows on green screen caused by feet","585","","<p>We have a recurring shoot where we continue to run into the same two issues. Our set consists of a medical exam table sitting on top of a green screen. The talent sits on the exam table and we record 'Doctor/Patient interviews'. </p>

<p>ISSUE 1: When the talent is on the taller side, their feet hang down past the bottom of the exam table and create really harsh shadows on the green screen, making the footage very difficult to key.</p>

<p>ISSUE 2: The dark area on the front of the exam table, behind the talent's legs, is very noisy. </p>

<p>I believe these two issues are linked and both have to do with lighting. However, I have yet to find a permanent solution to either one of them. I am currently setting a small LED light in front of the camera that helps with the shadows, but it still isn't solving the problem. </p>

<p>Our camera is a Panasonic HMC150 and I use the 'Keylight' effect in After Effects to key the footage. Keying is very difficult around the feet shadows and some of the green reflection shows on the exam table, which in turn, ends up being keyed slightly creating a lot of noise. I use the 'Remove Grain' effect to help remove the noise.  </p>

<p>Any advice on a different lighting setup, keying solution, or anything else that could help this situation is greatly appreciated. I've attached images demonstrating the issues.</p>

<p><img src=""https://i.stack.imgur.com/TWQZp.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/IVXw3.png"" alt=""enter image description here""></p>
","<p>To reduce or eliminate the greenish reflection on the front of the table, you can buy some dark gray or black construction paper, or something equally non-reflective, cut it to size, and tape it over the 2 dark rectangles on the front of the table.</p>

<p>I have 3 different solutions for the foot-shadows on the floor.</p>

<ol>
<li><p>You could point a semi-dim spotlight directly on that section of the green screen. Make sure to mask it so it doesn't hit the dark rectangles on the front of the table too much.</p></li>
<li><p>If the camera is always stationary, you could lower the camera slightly so the talent's feet never extend past the dark rectangle on the front of the table and into the green (as the tip of her left shoe does in the pictures you included). Then you can mask out that part of the shot instead of relying on the Keylight effect.</p></li>
<li><p>You could just move the camera about a foot closer to the talent so you crop out the bottom of the table (and the green in front of it). You can also do this by cropping the shot during post production.</p></li>
</ol>

<p>(Or, obviously, just get shorter talent.)</p>
","14625"
"Using mediafilesegmenter on a video with edit lists","583","","<p>I'm trying to use mediafilesegmenter on a video I transcoded with ffmpeg to package it for HLS. However, it complains about ""edit lists"" and refuses to do anything. Does anyone know how to either remove edit lists or to get mediafilesegmenter to work with them?</p>

<pre><code>$ mediafilesegmenter -I -f 180 -t 9 -s bbb_180p_30fps_264k.mp4
Jun 29 2016 17:12:33.676: Using floating point is not backward compatible to iOS 4.1 or earlier devices
Jun 29 2016 17:12:33.677: Single file output is not backward compatible to earlier than iOS 5.0
Jun 29 2016 17:12:33.677: Processing file /Users/aspera/git/dash-demo/video-transcoding/bbb-transcoded-hls/bbb_180p_30fps_264k.mp4
Jun 29 2016 17:12:33.689: track 1 of /Users/aspera/git/dash-demo/video-transcoding/bbb-transcoded-hls/bbb_180p_30fps_264k.mp4 contains edit lists; these tracks cannot be used for segmentation
Jun 29 2016 17:12:33.689: no tracks found!
Jun 29 2016 17:12:33.689: average bit rate is  0.00 bits/sec - max file bit rate is  0.00 bits/sec
</code></pre>

<p>The input video:</p>

<pre><code>$ ffprobe bbb_180p_30fps_264k.mp4 
ffprobe version 3.0.2 Copyright (c) 2007-2016 the FFmpeg developers
  built with Apple LLVM version 7.3.0 (clang-703.0.31)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/3.0.2 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-libfdk-aac --enable-nonfree --enable-vda
  libavutil      55. 17.103 / 55. 17.103
  libavcodec     57. 24.102 / 57. 24.102
  libavformat    57. 25.100 / 57. 25.100
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 31.100 /  6. 31.100
  libavresample   3.  0.  0 /  3.  0.  0
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'bbb_180p_30fps_264k.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    title           : Big Buck Bunny, Sunflower version
    artist          : Blender Foundation 2008, Janus Bager Kristensen 2013
    composer        : Sacha Goedegebure
    encoder         : Lavf57.25.100
    comment         : Creative Commons Attribution 3.0 - http://bbb3d.renderfarming.net
    genre           : Animation
  Duration: 00:10:34.53, start: 0.021333, bitrate: 184 kb/s
    Stream #0:0(und): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 320x180 [SAR 1:1 DAR 16:9], 113 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 64 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>

<p>EDIT:
I've discovered that ffmpeg seems to add edit lists when transcoding.</p>

<p>Original file:</p>

<pre><code>$ MP4Box -info originals/bbb_sunflower_2160p_60fps_normal.mp4 
* Movie Info *
    Timescale 600 - 3 tracks
    Computed Duration 00:10:34.533 - Indicated Duration 00:10:34.533
    Fragmented File: no
    File suitable for progressive download (moov before mdat)
    File Brand isom - version 1
    Created: GMT Tue Dec 17 16:40:26 2013
    Modified: GMT Tue Dec 17 16:40:26 2013

File has root IOD (9 bytes)
Scene PL 0xff - Graphics PL 0xff - OD PL 0xff
Visual PL: ISO Reserved Profile (0x15)
Audio PL: Not part of MPEG-4 audio profiles (0xfe)
No streams included in root OD

iTunes Info:
    Name: Big Buck Bunny, Sunflower version
    Artist: Blender Foundation 2008, Janus Bager Kristensen 2013
    Comment: Creative Commons Attribution 3.0 - http://bbb3d.renderfarming.net
    Composer: Jan Morgenstern
    Writer: Sacha Goedegebure
    Genre: Animation
1 UDTA types: meta (1) 

Track # 1 Info - TrackID 1 - TimeScale 60000
Media Duration 00:10:34.533 - Indicated Duration 00:10:34.533
Media Info: Language ""und (und)"" - Type ""vide:avc1"" - 38072 samples
Visual Track layout: x=0 y=0 width=3840 height=2160
MPEG-4 Config: Visual Stream - ObjectTypeIndication 0x21
AVC/H264 Video - Visual Size 3840 x 2160
    AVC Info: 1 SPS - 1 PPS - Profile High @ Level 5.1
    NAL Unit length bits: 32
    Pixel Aspect Ratio 1:1 - Indicated track size 3840 x 2160
    SPS#1 hash: 73C3B5E51841EEDF280132A1F7C72DD97D9FDE86
    PPS#1 hash: EB72205FBA0E2A677BE65E145507E2E05750EB4F
Self-synchronized
    RFC6381 Codec Parameters: avc1.640033
    Average GOP length: 184 samples

Track # 2 Info - TrackID 2 - TimeScale 48000
Media Duration 00:10:34.200 - Indicated Duration 00:10:34.200
Media Info: Language ""und (und)"" - Type ""soun:mp4a"" - 26425 samples
MPEG-4 Config: Audio Stream - ObjectTypeIndication 0x6b
MPEG-1 Audio - 2 Channel(s) - SampleRate 48000 - Layer 3
Synchronized on stream 1
    RFC6381 Codec Parameters: mp4a.6b
    All samples are sync

Track # 3 Info - TrackID 3 - TimeScale 48000
Media Duration 00:10:34.144 - Indicated Duration 00:10:34.144
Media Info: Language ""und (und)"" - Type ""soun:ac-3"" - 19817 samples
    AC-3 stream - Sample Rate 48000 - 5.1 channel(s) - bitrate 32000
    RFC6381 Codec Parameters: ac-3
    All samples are sync
</code></pre>

<p>After FFMPEG:</p>

<pre><code>$ MP4Box -info bbb_180p_30fps_264k.mp4 
* Movie Info *
    Timescale 1000 - 2 tracks
    Computed Duration 00:10:34.600 - Indicated Duration 00:10:34.534
    Fragmented File: no
    File Brand isom - version 512
    Created: UNKNOWN DATE   Modified: UNKNOWN DATE
File has no MPEG4 IOD/OD

iTunes Info:
    Name: Big Buck Bunny, Sunflower version
    Artist: Blender Foundation 2008, Janus Bager Kristensen 2013
    Comment: Creative Commons Attribution 3.0 - http://bbb3d.renderfarming.net
    Writer: Sacha Goedegebure
    Genre: Animation
    Encoder Software: Lavf57.25.100
1 UDTA types: meta (1) 

Track # 1 Info - TrackID 1 - TimeScale 15360
Media Duration 00:10:34.533 - Indicated Duration 00:10:34.533
Track has 2 edit lists: track duration is 00:10:34.600
Media Info: Language ""und (und)"" - Type ""vide:avc1"" - 19036 samples
Visual Track layout: x=0 y=0 width=320 height=180
MPEG-4 Config: Visual Stream - ObjectTypeIndication 0x21
AVC/H264 Video - Visual Size 320 x 180
    AVC Info: 1 SPS - 1 PPS - Profile Baseline @ Level 3
    NAL Unit length bits: 32
    Pixel Aspect Ratio 1:1 - Indicated track size 320 x 180
    SPS#1 hash: 4FD0A6F8F73D905B5A9973F02BF9E2D401820F71
    PPS#1 hash: E79F98332B320CB0B190539E0D4A262181656B90
Self-synchronized
    RFC6381 Codec Parameters: avc1.42c01e
    Average GOP length: 90 samples

Track # 2 Info - TrackID 2 - TimeScale 48000
Media Duration 00:10:34.221 - Indicated Duration 00:10:34.221
Track has 1 edit lists: track duration is 00:10:34.200
Media Info: Language ""und (und)"" - Type ""soun:mp4a"" - 29730 samples
MPEG-4 Config: Audio Stream - ObjectTypeIndication 0x40
MPEG-4 Audio AAC LC - 2 Channel(s) - SampleRate 48000
Synchronized on stream 1
    RFC6381 Codec Parameters: mp4a.40.2
Alternate Group ID 1
    All samples are sync
</code></pre>
","<p>Looks like MP4 won't skip the <code>edts</code> boxes when creating MP4s. You should transcode to MPEG-TS and feed that into mediafilesegmenter</p>

<pre><code>ffmpeg -i in.mp4 -&lt;transcoding options&gt; -vbsf h264_mp4toannexb out.ts
</code></pre>
","18800"
"What to do when camcorder angle does not cover entire area to record?","583","","<p>I'm trying to record an indoor soccer game from the middle of the bleachers. However, I cannot backup enough to get the entire field in the recording area.</p>

<p>I thought about fish-eye lenses but I'm guessing the video will be distorted near the edges where the goals are. I would like to avoid that if possible.</p>

<p>Another option is to try placing the camera near one of the edges of the field and backing up until the entire field is in the recording area.</p>

<p>Any ideas? I have no experience with videography so simpler solutions are preferred.</p>

<p>Thanks.</p>

<p><img src=""https://i.stack.imgur.com/xqxSZ.jpg"" alt=""The field""></p>
","<p>You're correct that you either need to place the camera farther away until the entire field is in view, or use a wide-angle lens adapter (not necessarily ""fisheye"" but wider). For indoor soccer, clearly you're limited by the walls. </p>

<p>Presumably, you want to hit ""record"" and enjoy watching the game, rather than having to actively pan the camera back &amp; forth. Also, I'm presuming you only have 1 camera &amp; 1 operator, and you don't really intend to edit the footage together later... If that's the case, look for a wide-angle adapter &amp; put the camera as far away &amp; as high up as possible (a GoPro or similar as suggested previously might be easier to stick high up on the wall :))</p>

<p>For most basic consumer camcorders, you can get a wide-angle adapter to screw onto the front for about ~$40 (make sure the lens &amp; screw threads are the same size or find one specifically compatible for your camera). That should give you ~20-30% wider field of view. It won't be perfect &amp; might be slightly distorted, but it will capture more of the field than you would otherwise. </p>

<p>That still probably won't be enough to capture the entire game from mid-field, but it will get more of it. To fit the whole thing you'd likely need to be up on the ceiling or on a ladder at the top of the bleachers. Not safe, or easy to maneuver. </p>

<p>If you watch professional soccer matches on TV, you'll notice that they don't capture the entire field either, but instead follow the action of the ball. This is because 1) that's the focal point of the game and 2) In order to really see the whole field the camera would have to be so far away you wouldn't really be able to tell what's going on anymore.</p>

<p>There's little/no zooming (instead they cut between cameras). They tend to keep the player w/the ball just off-center, so there's room for them to run &amp; not go out of frame. Mid-range or close-up shots are usually done w/separate cameras, often at field level along the sidelines. As you'd suggested, you could put a camera near the goal line &amp; see the whole field, but then you're down low &amp; will only capture what happens nearby in that half of the field.</p>
","12699"
"ffmpeg concatenation introduces video stutter but does not affect audio","581","","<p>I've gone through several different variants of this process and it always causes me problems.  I've gotten it working sometimes but it doesn't seem to be consistent.</p>

<p>I have a video camera which records AVCHD to MTS files, it splits files at 3.89GB.  Since its microphone is crap, I also record audio separately and have to sync them back up before editing.  The problem I have is that the audio recorder doesn't break files at the same times so I'll usually end up with 3 video files and 1 audio file.</p>

<p>So I'm wondering where things are going wrong in this process:</p>

<ol>
<li>Converted all the H264 MTS files to ProRes MOV files.  The editor handles ProRes better than the compressed files so I convert. <em>ffmpeg -i 00000.MTS -vf ""yadif=3:-1:1,mcdeint"" -vcodec prores -vprofile 2 -ar 48000 1.mov</em></li>
<li>Concatenated the 3 .mov files together.  At this point I have a file with video stutters every ~32 minutes, but the audio stays in sync with the video. <em>ffmpeg -f concat -i mylist.txt -c copy full.mov</em></li>
<li>Extracted the audio to FLAC.  Imported into audacity, lined up the external audio with a clap and trimmed to length.  At this point both audio tracks line up, the external track is still in sync with the camera audio at the end.  Export the new external audio matched to the start and the length of the original audio. <em>ffmpeg -i full.mov -aq 256k full.flac</em></li>
<li>Recombine the large .mov file with the new audio.  At this point the audio that was just in sync with the original audio from the video files now goes out of sync when the video stutters. <em>ffmpeg -i full.mov -i full-tascam.flac -map 0:0 -map 1 -vcodec copy -ar 48000 full-tascam.mov</em>  </li>
</ol>

<p>What do?  I've had the same problem before introducing ProRes, just concatenating the MTS files into MP4 copying the video stream and adding the new audio.  Is there anything I can set while concatenating to remove the stutters, or to prevent them from throwing the new audio out of sync when I'm adding that track?</p>
","<p>For some reason I think this is avoided when using ffmpeg's concat PROTOCOL (<em>ffmpeg -i ""concat:...""</em>) rather than its concat DEMUXER (<em>ffmpeg -f concat -i mylist.txt</em>).  Oddly, it's also slightly more convenient since using the demuxer requires me to create the text file with the file names in it while with the protocol I can just specify them on the command line, eg: <em>ffmpeg -i ""concat:00001.MTS|00002.MTS|00003.MTS"" -c copy output.mts</em></p>

<p>I'll need to confirm this later, I'm at the wrong computer but something brought the question to my attention so I decided to update it with new information.</p>
","16800"
"How to lower the volume of the second audio channel with FFMPEG","577","","<p>It's easy to mute one of the audio channels of a video, but I have to lower the second one with some dB.</p>

<p>I have searched in documentation about mapping the channels but I am still confised how to lower the volume of the specific channel. I have only two audio channels in my videos.</p>

<p>Thank you for the help!</p>
","<p>After a few hours of searching, I was only able to find <a href=""https://superuser.com/questions/769168/ffmpeg-mixing-and-setting-each-volume-of-a-multi-audio-track-file"">this.</a></p>

<p>I don't know if that's kind of what you are looking for, or at least gives you the info you need to modify the channel volume.</p>

<p>I hope it helps!</p>
","13253"
"Organizing a Multicam Clip in Premiere","576","","<p>In Adobe Premiere Pro 2015, you cannot sub-clip a multi-camera source clip, which is a shame, because sub-clipping is the most useful way I've found in Premiere for organizing segments of an extended interview. </p>

<p>I have a 2-hour multi-camera clip from an interview for a mini-documentary. My normal mini-doc/corporate workflow is:</p>

<ul>
<li>import footage</li>
<li>watch and tag (sub-clip) footage</li>
<li>create rough cut by adding sub-clips to a sequence and fine-tuning</li>
</ul>

<p>Now that I've got a multicam interview, I want to preserve this workflow or find an alternative. I've considered creating a sequence for each interview segment, but that requires a lot more interaction with the UI, slowing down the workflow. Yet it's important to organize these clips, because this interview may get used in multiple productions over the next several months, and I only want to watch and tag it once.</p>

<p><strong>Question:</strong> Is there a way to tag multicam clips in Premiere that achieves the same organizational goal as sub-clipping?</p>
","<p>I was recently editing a 5-camera short film with Premiere. Over 7 hrs worth of footage. Lots of going back and forth over the clips. Lots of trying different angles. All that over many weeks. Yet, we never got lost or confused about what was happening or where things were.</p>

<ul>
<li>Create your multi-cam sequence.</li>
<li>Log all camera angles individually by using a lot of markers and/or extended markers. Label those markers clearly and consistently: mention the subject discussed and whatever interesting thing the interviewee does (facial expression, laughter, hand movement, etc.)</li>
<li>Maybe use a marker-color per subject-matter.</li>
<li>I would make sure to keep the original multi-cam sequence cuts-free. Just duplicate it as many times as you need to: that way you can always come back to it, a clean slate, if and when the cutting gets messy.</li>
<li>Edit the interview by copying and pasting bits that deal with similar issues to as many sequences as you have subject matters. Duplicate those initial ""sub-sequences"" and edit their duplicates down, as the director and you see fit.</li>
<li>Make sure that you name those ""subject-sequences"" clearly. Let's say one is about ""Boats"". Call the first one ""151031_Boats_E01"" for the date, the subject matter and the edit number. Then the duplicate would be called ""151031_Boats_E02"", etc. Duplication is you friend. Use bins, etc. I'm sure you have your workflow already anyway.</li>
</ul>

<p>That way, you should be able to get back to your edit and the various subject matters over the course of a few months without finding yourself lost.</p>
","16755"
"Old 8mm video recording tapes and standards","575","","<p>I want to transfer a set of old home video tapes recorded on a Sanyo VM-D66P. I do not have access to the camcorder anymore need to buy a camcorder that will play them so I can capture it via USB. The manual unfortunately does not make it clear precisely the type of tapes that it used.</p>

<p>The instruction manual gives the following hints:</p>

<ul>
<li>Television System: PAL, CCIR</li>
<li>Video recording system: Rotating 2-head helical-scan FM, color under system</li>
<li>Audio recording system: Rotating head FM system</li>
<li>Cassette type: 8mm video tape cassette</li>
<li>Tape speed: SP: 20.051 mm/sec</li>
<li>Tape speed: LP: 10.025 mm/sec</li>
<li>Recording/playback time: 180 minutes (P5-90 cassette)</li>
</ul>

<p>After looking through the <a href=""http://en.wikipedia.org/wiki/8_mm_video_format"" rel=""nofollow"">8mm video format wikipedia page</a>, I have come to the weary conclusion that the VM-D66P used the standard <strong>""original Video8 (analog recording) format""</strong>. Does this sound correct? And does this mean I can use any Video8-compatible camcorder to play back the VM-D66P tapes?</p>

<p>Thanks!</p>
","<p>The logo on the manual I could find indicates that it was standard 8mm, not Hi8 or Digital8.  If you are in a PAL region, then most likely it is the PAL version of 8mm.  You should be able to use any PAL based 8mm playback deck or camcorder to play the tape back.  You then be able to use a standard video capture device to digitize the video that you playback.</p>
","9804"
"Convert 1920x1080 Video to 720x480 With Letterboxing","574","","<p>I'm trying to find the best application on windows to do this HD to SD conversion for a video server which requires files to be exactly 720x480/29.97fps MP4 H.264 to function. I can't find any options for leterboxing in Handbrake so I suppose I'll need an alternative.</p>
","<p>You can use <a href=""http://ffmpeg.org/download.html"" rel=""nofollow"">ffmpeg</a>, a free command-line tool, to do this:</p>

<h3>Without letterboxing, NTSC wide pixel-aspect</h3>

<pre><code>ffmpeg -i input.mp4 -vf ""scale=720x480,setdar=16/9"" -r 30000/1001 -c:v libx264 -crf 20 -c:a aac -movflags +faststart output.mp4 
</code></pre>

<h3>With letterboxing, NTSC 4:3 pixel-aspect</h3>

<pre><code>ffmpeg -i input.mp4 -vf ""pad=1920:1440:0:180,scale=720x480,setdar=4/3"" -r 30000/1001 -c:v libx264 -crf 20 -c:a aac -movflags +faststart output.mp4
</code></pre>

<h3>With letterboxing, square pixels</h3>

<pre><code>ffmpeg -i input.mp4 -vf ""scale=720x404,pad=720:480:0:38,setsar=1"" -r 30000/1001 -c:v libx264 -crf 20 -c:a aac -movflags +faststart output.mp4
</code></pre>

<p>Read up on sample or <a href=""https://en.wikipedia.org/wiki/Pixel_aspect_ratio"" rel=""nofollow"">pixel aspect ratios</a>, if you're unfamiliar with them. Your requirements make it sound like NTSC-spec files are expected, so try the commands in the order listed.</p>

<p>If your source audio is MP4-ready i.e. of codec AAC or MP3, then you can replace <code>-c:a aac</code> with <code>-c:a copy</code></p>
","18200"
"1080p60 recording has very strong compression artifacts, default Neat Video not reducing them much","573","","<p>In attempting to record in the highest quality on a JVC GC-PX100 camcorder, I've been recording 1080p video.  Unfortunately, viewing the video upon edit shows very strong compression artifacts, which affect quality to the point that burning to DVD and burning to Blu-Ray end up with roughly the same results.  The video was taken on a reasonable photo tripod, so the camera was definitely steady, and the bulk of the video was unmoving walls with detailed paintings.</p>

<p>Adobe Premiere Elements 11 is what I'm doing for editing, and I've just tried the Neat Video plugin.  Unfortunately, while the default settings reduce the artifacts/noise in flat areas (cheeks, walls), but not along all edges (faces).</p>

<p>Essentially, the video is ""crunchy"", and that's visible on playback on a large TV.</p>

<p>Adobe Elements shows the .mts file from the camcorder is MPEG Movie, 1920x1080, frame rate 59.94, 48kHz audio, average data rate 3.3 MB/s (~34Mbit/s), pixel aspect ration 1.0.</p>

<p>VLC shows the .mts file from the camcorder is Codec H264 - MPEG-R AVC (part 10)(h264), 1920x1080, frame rate 59.940060, decoded format Planar 4:2:0 YUV, with A52 (aka AC3) 48kHz audio.</p>

<p>My goal is the highest quality result video, targeting Blu-Ray output.  My budget can handle a plugin fee, but not a new camcorder (spent too much on the add-on microphone).</p>

<p>So my questions in particular are:</p>

<ul>
<li>Are there better settings to increase overall quality during recording?</li>
<li>Are there better settings for Neat Video</li>
<li>Is there another way to reduce compression artifacts in HD video taken by a good consumer camcorder?</li>
</ul>

<p>EDITED TO ADD:
AJ, Jim, thank you for your answers; they led me to the regrettable conclusion that my expectations are too high for any 1080p60 mode at 35Mbit, and no 1080p30/25/24 mode is available on my model.</p>

<p>Even after the usual firmware update, and a factory reset as recommended by JVC Customer Support, the video is ""crunchier"" than I like.  There is no 1080p30 mode, unfortunately, and lesser modes have lesser bitrates as well.</p>

<p>After some experimentation, it appears that the AVCHD and MP4 1080p60 modes are more or less equivalent, with a possible slight edge to the AVCHD variant.</p>

<p>NeatVideo needs to have the plugin expanded and the Temporal Radius increased, as well as going into the settings to profile the video clip in question.  Edges remain crunchy, and dedicated ""reference"" profile video appears necessary to get good results.</p>
","<p>There is no way around needing more data rate to capture that much video information.  It sounds like the camera probably isn't actually capable of delivering 1080P/60 but that they use a very low quality version so they can slap the label on the box.</p>

<p>Data rates are generally measured in Mb(bits)/sec rather than MB/s, but even if your suggested rate actually is in MB/sec, 3.3MB/s for the video you are describing is near uselessly small and strong artifacting would be expected (particularly for a real time encoding) as the compression algorithm breaks down.</p>

<p>You need to use a mode that supports either a higher data rate or supplies less data to be encoded at that data rate.  There is no other real solution since there is no information there to actually use to fix the problem.</p>
","10503"
"Proper way to play 50 fps video as 25 fps in FCPX","573","","<p>I just started using Final Cut Pro X and I have footage in 50 fps. I want to play this back in 50% speed in a project that outputs 25 fps. What is the proper way do achieve this? I can start a project with 50fps, import the media an choose ""50%"" speed reduction, but will this really intepret the media as 25 fps and output 25fps from the project? I suspect that it won't that it infact just will ""make up"" frames and ouput 50fps, and I don't want that. Any clue on how to fix this would be appreciated. </p>
","<p>Your project's frame rate should be the playback frame rate.  So if you want to output 25fps, make it a 25 fps project.  When you import other frame rates, many NLEs will attempt to automatically adapt the frame rate to your project rate.  If FCPX does not do this for you automatically, then you can manually set the timing of the clip to play back at 50%.</p>

<p>But when you make a 50fps project, you are basically making it impossible for FCPX to do the right thing.  Setting a clip to play back at 25fps in a 50fps project is meaningless.</p>
","20318"
"Apply custom lut via ffmpeg?","572","","<p>I'd like to batch convert some <em>log footage</em> as well as transform each clip into <em>rec709</em> colorspace. Is there any way to apply a <em>custom lut</em> to the footage? If so, what's the correct way to do that?</p>
","<p>You may want to use FFMPEG's <a href=""https://ffmpeg.org/ffmpeg-filters.html#lut3d-1"" rel=""nofollow noreferrer"">lut3d</a> filter. It requires you to provide a look-up table (a *.cube file). For example, if you have an ARRI camera you can generate these files using <a href=""http://www.arri.com/camera/amira/downloads/"" rel=""nofollow noreferrer"">ARRI Color Tool</a> or simply download a package with them from the linked page.</p>

<p>Once you have the files, use FFMPEG like this:</p>

<blockquote>
  <p>ffmpeg -i ""Input.mov"" -vf lut3d=""ARRIP3D65PQ108-33.cube"" -s 1920x1080
  -c:v dnxhd -pix_fmt yuv422p -b:v 120M DNxHD_for_Editing.mxf</p>
</blockquote>

<p>The important part here is the use of <strong>lut3d</strong> filter:</p>

<blockquote>
  <p>-vf lut3d=""ARRIP3D65PQ108-33.cube""</p>
</blockquote>

<p>where ARRIP3D65PQ108-33.cube is the LUT file to use with the filter.</p>
","21653"
"What needs to happen before we expect x265 to be in wide use, along with encoders something like Handbrake available for it?","571","","<p>I know that the project is already out and <a href=""https://bitbucket.org/multicoreware/x265/downloads"" rel=""nofollow"">work has began on it</a>.  However, realistically speaking, a large number of things have to happen before it is ready for prime time. (For example, given the encoding/decoding <a href=""http://www.cnet.com/news/what-is-hevc-high-efficiency-video-coding-h-265-and-4k-compression-explained/"" rel=""nofollow"">challenges to overcome</a>, decoders need to ""smarten up"" to the new encoding process.) What other things have to be accomplished before we can reasonably expect to see x265 in wide use?</p>

<p>Are there any additional barriers to something like Handbrake being released?</p>
","<p>If you want to know when x265 is going to be in a usable state you will have to ask the developers about that. Though I'm pretty sure they don't know either.
If you want to know when h265/HEVC will be starting to get actually used I'd say earliest end of 2014 though more realistically in mid 2015. </p>

<p>Why? At the moment the devices most profiting from this new codec are mobile handsets that operate on limited data plans and 4K TVs.
As Smart TVs often use the same SoCs as smartphones and tablets both will happen pretty much at the same time.</p>

<p>Qualcomm already released a preview of its Snapdragon 805 which will be a SoC with h265 hardware decoding support. Thats the most important step towards the widespread adoption of a new codec, without hardware support most smartphones will have a hard time with playing back h265 content. SoCs that also support hardware encoding will come out in 2015 (f.e. Snapdragon 810). At that point all important factors on the consumer sides are ready for a widespread adoption. Now we just need to wait until even the low-end SoCs have hardware playback support.</p>

<p>Though that might go faster than expected with ARM v8 being around the corner at the same time. There might be some more rapid changes again in the Smartphone market.</p>

<p>Something like Handbrake will come around pretty quickly, its not all to hard. In the end its just a good user interface and some good presets. You just have to come up with it.</p>
","10857"
"Recommendations for HD camcorder for all-day static recording?","570","","<p>I am working on a computer vision project where I will setup a camcorder in a room at around 6am and record people as they walk around and interact with things. The recording ends at 11pm. The room itself is huge (~50 ft tall) and I need to mount the camera either on the ceiling or on one of the rafters, so I'd prefer not to have to fetch an SD card at the end of each day if at all avoidable. The people are not moving super fast, but I will be doing motion analysis based on an image of the empty room, so it's important that the video be uncompressed (compression changes pixels and messes up the before/after diff for the motion tracking). Also, the people will be interested in viewing their actions later, so it needs to be in color and preferably HD. There is likely to be minor vibrations, so it also needs to be able to stabilize the video, though I imagine that is standard nowadays.</p>

<p>Given those requirements, does anyone have any recommendations for a camcorder? I'm looking in the sub-$400 range if at all possible.</p>

<p><strong>Edit</strong>: Compressed video is fine after all. I was misunderstanding what kind of video my test data was using, but it seems to be compressed, so I can handle that. Also, if possible I'd like the camera to be able to just stream the video to a server on a local network so I can store it offline without having to retrieve the SD card.</p>

<p><strong>Edit 2</strong>: After some more looking, it does seem like there are 30M (~64ft) active USB extension cables. So it's possible that I could run a cable to a laptop on the ground and not need to worry about power or SD storage. As a result, I'm currently leaning towards an HD webcam like AJ suggested.</p>
","<p>Ok, so now that I got that comment out of my system, let me start by saying your expectation is 100% completely, totally and insanely unrealistic.  HD + uncompressed = a data rate measured in gigabytes (11.2 gigabytes to be specific) per minute (16.1 terabytes per 24 hour day) and is only available on very high end cameras (>$3000, without optics) price range.  Additionally, to not have to get the card every 30 minutes, let alone every day, you would need to have it wired in to a recorder elsewhere which means getting a record deck alone that will be over $600.  There is simply no possible way to accomplish all of your goals within an order of magnitude of your stated target cost.</p>

<p>For $6,000 or so, I could probably recommend a setup that would work nicely for capture and then you'd need another $4,000 or so just for the disk drives that could store it, but unless you can bear a reasonable level of compression, then it isn't going to be any cheaper.  Even SD would have problems running fully uncompressed for lengths of time without a dedicated professional deck, though SD might possibly drop the cost in to the 1 to 2k range.</p>

<p>While off topic for AVP, I would suggest that what you need to do is update your analysis to support compression.  Add a buffer that will recognize pixels that are close in color as the same and adjust as necessary until compression artifacts aren't recognized.  Unless people are all the same color as the floor, it should probably still be able to track pretty well.  </p>

<p>You'll need this anyway as from frame to frame, there are going to be variations in what hits the sensor even if you have a fully uncompressed signal.  Noise happens in the real world.  You have to deal with it.  The noise in the signal is going to be at least as much of a problem as most good compression, probably much more.</p>
","8339"
"best alternative to QuickTime .mov","569","","<p>My company is stripping away QuickTime enterprise-wide, due to Apple dropping support for the format.  I use After Effects and Premiere, and others in my group are on Avid Workstations.  Occasionally when I create a video, I'm handed .mov files exported from Avid Media Composer and Sorensen Squeeze to work with in Adobe software, usually as QuickTime uncompressed.  Now that we are unable to work with QuickTime, going forward, what's the best codec alternative to import clean footage to work with?</p>
","<p>Apple is dropping support for the player, so no new releases, but the format remains and can be decoded by recent versions of CC without needing Quicktime to be installed. See</p>

<p><a href=""http://blogs.adobe.com/creativecloud/quicktime-on-windows/"" rel=""nofollow"">http://blogs.adobe.com/creativecloud/quicktime-on-windows/</a></p>

<blockquote>
  <p>Adobe has worked extensively on removing dependencies on QuickTime in
  its professional video, audio and digital imaging applications and
  native decoding of many .mov formats is available today (including
  uncompressed, DV, IMX, MPEG2, XDCAM, h264, JPEG, DNxHD, DNxHR, AVCI
  and Cineform). Native export support is also possible for DV and
  Cineform in .mov wrappers.</p>
</blockquote>

<p><a href=""http://blogs.adobe.com/creativecloud/apple-quicktime-on-windows-update/"" rel=""nofollow"">http://blogs.adobe.com/creativecloud/apple-quicktime-on-windows-update/</a></p>

<blockquote>
  <p>Today were pleased to announce that Adobe has been able to accelerate
  work that was already in progress to support native reading of ProRes.
  This new capability is fully licensed and certified by Apple, and
  barring any unforeseen issues during pre-release, these fixes will be
  included into an update to the relevant products in Creative Cloud
  shortly.</p>
</blockquote>
","18785"
"Post processing - image stabilisation","569","","<p>I own a <a href=""http://goo.gl/mCkbQA"" rel=""nofollow"">Panasonic HC-V130</a>. Recently I've read an article about image stabilisation in a computing magazine. Can this be achieved without a special camera? </p>

<p>I am using too much zoom to hold the camera still - causing the image to bounce up and down. Can I correct some of this shaking using software after? If possible can anyone suggest a few pieces of software that can achieve this - if possible a free one. </p>

<p><em>I use windows 8.1 and Ubuntu 14.04 (Linux).</em></p>
","<p>Adobe premiere and After effects have a stabilizer plugin with that does what you want. These are commercial applications.</p>

<p>The free Da vinci resolve lite also has a ""stabilize option"" (but I have never tried personally)</p>
","15948"
"Can I shoot in 16:9 then change it to 4:3?","567","","<p>I'm trying to get the old film look with Canon 600D, so I will be shooting in 640*480 25 fps. But what if I shoot in 16:9 at 24 fps, will I be able to get the same 4:3 after editing? (The reason I'm asking this is films are 24 fps, 600D doesn't have 4:3 at 24 fps)</p>
","<p>Of course you can set your project up so that it will output a 4:3 video. However, keep in mind that there are only two ways to achieve this:</p>

<ul>
<li>Nr. 1 You crop the video, which means you will lose stripes of the video at both the left and the right side.</li>
</ul>

<p>Here's an example:</p>

<p><img src=""https://i.stack.imgur.com/jcBNn.jpg"" alt=""enter image description here""></p>

<ul>
<li>Nr. 2: You resize the 16:9 input video so that it fits in the 4:3 output video. I wouldn't recommend this though, since the video and everything in it will be deformed, which looks completely aweful.</li>
</ul>

<p>So if this is your input:</p>

<p><img src=""https://i.stack.imgur.com/UZWrF.jpg"" alt=""enter image description here""></p>

<p>It will result in this output:</p>

<p><img src=""https://i.stack.imgur.com/4Rz1Q.jpg"" alt=""enter image description here""></p>

<p>Both options result in a quality / information loss. How about shooting in 4:3 at a higher framerate? You could easily lower the framerate in editing to achieve your desired effect ...</p>
","12463"
"cloud backup routines","563","","<p>I am looking for cloud backup or remote backup routines and practices that may be used for storing video.</p>

<p>Specifically if services or practices that offer versioning are relevant. If so, why or why not? Any examples of services you use will also be appreciated</p>
","<p>I've used <a href=""https://www.backblaze.com"" rel=""nofollow"">Backblaze</a> for this. I understand that <a href=""http://www.code42.com/business/"" rel=""nofollow"">CrashPlan Pro</a> and other similar services work in the same way. Basically, you install the software (free download), do some setup, and then it just runs in the background. Every time you create a new file or modify an existing one, it will (at some later point when there's less activity on your system) just back it up to the cloud. You can usually configure it to constrain how much bandwidth it uses, which folders you do and don't want backed up, and maybe even when backups occur. And if you have a very large initial backup, they'll even send you a hard drive to back it up to. You send it back once the backup has occurred. From then on, it's incremental over the internet.</p>
","17383"
"Why is ffmpeg outputting a 600fps video from a 120fps video input?","563","","<p>I have <a href=""https://worm-bucket-prod.s3-eu-west-1.amazonaws.com/videos/56bb088118db47954f00006e/cd92b51a-51c6-4580-b86c-e2604cba41f2-720p.mp4"" rel=""nofollow"">a very short 120fps video</a>.</p>

<p><code>ffprobe input.mp4</code> shows <code>118.99fps</code>:</p>

<pre><code>ffprobe version 3.0 Copyright (c) 2007-2016 the FFmpeg developers
  built with Apple LLVM version 7.3.0 (clang-703.0.29)
  configuration: --prefix=/usr/local/Cellar/ffmpeg/3.0 --enable-shared --enable-pthreads --enable-gpl --enable-version3 --enable-hardcoded-tables --enable-avresample --cc=clang --host-cflags= --host-ldflags= --enable-opencl --enable-libx264 --enable-libmp3lame --enable-libxvid --enable-ffplay --enable-vda
  libavutil      55. 17.103 / 55. 17.103
  libavcodec     57. 24.102 / 57. 24.102
  libavformat    57. 25.100 / 57. 25.100
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 31.100 /  6. 31.100
  libavresample   3.  0.  0 /  3.  0.  0
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.101 /  2.  0.101
  libpostproc    54.  0.100 / 54.  0.100
[h264 @ 0x7f8262009600] Increasing reorder buffer to 1
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp41mp42isom
    creation_time   : 2016-04-27 10:43:14
  Duration: 00:00:02.76, start: 0.000000, bitrate: 10318 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 720x1280, 10272 kb/s, 118.99 fps, 600 tbr, 600 tbn, 1200 tbc (default)
    Metadata:
      creation_time   : 2016-04-27 10:43:14
      handler_name    : Core Media Video
</code></pre>

<p>I'm running a square crop and resize operation:</p>

<pre><code>ffmpeg -i input.mp4 -vf ""crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)', scale=640x640"" -an -c:v libx264 -profile:v high -level 4.1 -preset veryslow -crf 20 output.mp4
</code></pre>

<p>This gives:</p>

<pre><code>....
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp41mp42isom
    creation_time   : 2016-04-27 10:43:14
  Duration: 00:00:02.76, start: 0.000000, bitrate: 10318 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 720x1280, 10272 kb/s, 118.99 fps, 600 tbr, 600 tbn, 1200 tbc (default)
....
Output #0, mp4, to 'output.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 1
    compatible_brands: mp41mp42isom
    encoder         : Lavf57.25.100
    Stream #0:0(und): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 640x640, q=-1--1, 600 fps, 19200 tbn, 600 tbc (default)
....
</code></pre>

<p>And sure enough, <code>ffprobe output.mp4</code> confirms that <code>output.mp4</code> is at 600fps, as does VLC.</p>

<p>How can I ensure that ffmpeg will encode my video using the same framerate as the input (be that 30, 60, 120, or 240fps)?</p>
","<p><strong>Update</strong>: The PTS entries of the input and output for the command below are identical in version N-79630-g9ac154d. Looks like the file is falsely flagged as VFR. But this single-step command below works for the sample video.</p>

<p>Use</p>

<pre><code>ffmpeg -i input.mp4 -vf ""crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)', scale=640x640"" \
   -an -c:v libx264 -profile:v high -level 4.1 -preset veryslow -crf 20 -vsync 0 output.mp4
</code></pre>

<hr>

<p>The framerate stored in the container (<code>tbr</code>) is 600, whereas the reported fps of <code>118.99</code> is arrived at by counting the number of video packets and dividing by the total duration. If this is not a variable frame rate stream, then add <code>-r 118.99</code> as an output option.</p>

<hr>

<p>For older versions,
Alternatively, output to MKV i.e.</p>

<pre><code>ffmpeg -i input.mp4 -vf ""crop='if(gte(iw,ih),ih,iw):if(gte(ih,iw),iw,ih)', scale=640x640"" \
   -vsync 0 -an -c:v libx264 -profile:v high -level 4.1 -preset veryslow -crf 20 output.mkv
</code></pre>

<p>If you then mux the MKV to MP4,</p>

<pre><code>ffmpeg -i output.mkv -c copy output.mp4
</code></pre>

<p>The resulting MP4 will show (close to) the source framerate.</p>

<pre><code>Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'no-rc.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf57.34.103
  Duration: 00:00:02.75, start: 0.000000, bitrate: 2462 kb/s
    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x640, 2444 kb/s, 119.88 fps, 600 tbr, 16k tbn (default)
</code></pre>
","18268"
"Codec genealogy / kinship: DivX, MPEG-4 Part 2, H.263","561","","<p>Yes, I know, H.264/AVC is here for everyone, and we're moving (slowly) towards H.265/HEVC, so why bother with these historic formats? Well, just because it's history. I admit my research has been limited to Google/forums/Wikipedia, but I haven't managed to get the relationship between these codecs straight.</p>

<p><a href=""http://en.wikipedia.org/wiki/DivX"" rel=""nofollow"">Wikipedia DivX</a></p>

<blockquote>
  <p>There are two DivX codecs; the regular MPEG-4 Part 2 DivX codec and
  the H.264/MPEG-4 AVC DivX Plus HD codec.</p>
</blockquote>

<p>Okay, the latter doesn't matter here, that's just their H.264 implementation. But this does seem to state that the first and famous DivX <em>is</em> an MPEG-4 Part 2 implementation.</p>

<p><a href=""http://en.wikipedia.org/wiki/MPEG-4_Part_2"" rel=""nofollow"">Wikipedia MPEG-4 Part 2</a></p>

<blockquote>
  <p>Several popular codecs including DivX, Xvid and Nero Digital implement this standard.</p>
</blockquote>

<p>Okay, but wasn't DivX some kind of rogue development, decompiled Microsoft code, then modified, and even the Microsoft code being a modification of MPEG-4 Part 2?</p>

<p><a href=""http://en.wikipedia.org/wiki/H.263"" rel=""nofollow"">Wikipedia H.263</a></p>

<blockquote>
  <p>MPEG-4 Part 2 is H.263 compatible in the sense that a basic H.263
  bitstream is correctly decoded by an MPEG-4 Video decoder.</p>
</blockquote>

<p>So there's overlap between the two, but one is for movies and such (MPEG), the other for teleconferencing and such (ITU), so their specced out for different contexts.</p>

<p>Sorry for the confused question  guess what I'm looking for is a mental map of these codecs with their relationships and how they came about and in how far they are the same and/or different.</p>
","<p>H.263 was a sole development of the ITU but I wouldn't bother all too much with the specified use case of video conferencing. It's a codec with the main purpose of improving the compression compared to older codecs which of course is beneficial for video conferencing where bandwidth is a very limiting factor, especially at the time the codec got developed.</p>

<p>The MPEG then developed MPEG4-Part 2 which is based upon H.263 and offers bit-stream compatibility to the base-line level of h.263, meaning there is a h.263 compatible bit-stream in an MPEG4-Part 2 encoded video that you could call the basis for the actual MPEG4-Part 2 video stream that extended the h.263 compatible stream. So MPEG4-Part 2 offers a few more features than h.263 but essentially they are very closely related.</p>

<p>With MPEG4-Part 2 started this weird age of many companies making their own implementation of these two codecs resulting in a few new container formats that were only used by these specific implementations f.e. RealMedia's .rmf/.rmvb or DivX's altered version of AVI, .divx.
RealMedia even made their own codec again based on h.263. A horrible age of a ton of different formats that needed their own browser plugins and players because they weren't fully compatible to each other, either because of their container or slight alterations to either h263 or MPEG-4 Part 2.</p>

<p>Which made this codec generation a bit special, while .MP4 was already present at the time in an early form it was barley used as the container format for MPEG4-Part 2. So h.263 and MPEG-4 Part 2 was found in a ton of different container formats unlike h.264 which has is also found in several container formats but with the intended container format MP4 being the defacto standard that is found everywhere. The most popular alternative being the open source Matroska container format (.mkv/.mka). Apple's QuickTime .mov also supports h.264 but even in the Apple Ecosystem MP4 is dominating. In my opinion the biggest reason why h.264 is such a huge success today.</p>

<p>DivX began as a reversed engineered version of Microsoft's MPEG4-Part 2 implementation. At that point there was no company behind DivX, it gained popularity because of its good compression at that time and also being free to use.
After DivX made it into a company they revamped the code for the codec as it was still based on some Microsoft code.</p>

<p>Though it still was an MPEG4-Part 2 implementation after the rewrite and became a pretty popular codec that was supported by a lot of DVD players at the time. Some developers of DivX went on and made the open source version called Xivd (that used AVI instead of .divx) that gained a lot of popularity in the movie ""pirating"" scene and was their standard for encoding videos for several years until h.264 came along. Though even after h.264 was readily in use by consumers Xvid was still used for a few years because of it's good understanding in the scene. It became somewhat of an art to encode movies in Xvid because of all those sophisticated encoding options available in the Xvid encoder.
Xvid had a few advanced features that weren't implemented in DivX which made videos encoded with those feature in-compatible with DivX certified devices.</p>

<p>Since MPEG4-Part 2 the MPEG and ITU-T are jointly developing codecs, H.264 being their first co-operation. This is the reason why the codec is often referred to as h.264/MPEG4-4 AVC. MPEG-4 AVC being the MPEG's/ISO name for the standard and h.264 that of the ITU-T. Different names but identical standards. The same goes for h.265/HEVC.</p>

<p>I hope that helped a bit clearing things up, though a lot of this is also knowledge taken from Wikipedia, so it might not be new to you but I tried to put it an order that made sense.</p>
","10893"
"How do I add text like this (picture) to a video? Can it be added post or is it an old camera function?","561","","<p><a href=""https://i.stack.imgur.com/szXiM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szXiM.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/cxDNd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cxDNd.png"" alt=""enter image description here""></a></p>

<p>I've read around and a lot has been said about transferring video through VHS and back again but I was wondering about adding this specific kind of text? 
Is it a function that exists on old cameras or can the same be achieved with editing software? 
Many thanks!!</p>
","<p>It was in camera function. But there is no such thing in video editing, which can not be accomplished with correct software.</p>

<p>You can add this text in Adobe Premiere, or Adobe After Effects with some noise etc... You can add it in almost any video editing software. Main thing you need  correct font. It is pixeelated all caps text. You can find one here: <a href=""http://www.fontspace.com/category/pixelated"" rel=""nofollow"">http://www.fontspace.com/category/pixelated</a></p>

<p>Next: Install that font in your system and add text. Make it all caps and with chosen font. Select color and for the second example you also will need black stroke.</p>

<p>Good luck!</p>
","16294"
"Extract specific video frames","559","","<p>I am looking for a Windows based open source method to extract specific frames from video files (*.avi). Ideally the method will be suited to R.</p>

<p>I need to do this as I have annotated (many) video files using Solomon Coder. This is an open source program which, quickly and easily, allows for user specified frame by frame annotation.</p>

<p>I have then summarized the data using R. In my summary I have a behaviour event and the start and end frame numbers. I also have the start and end time in seconds, epoch time and as a POSIXct object.</p>

<p>I'd like to extract the specific frames in some instances.</p>

<p>I am aware of the following options:</p>

<ol>
<li><p>ffmpeg in Matlab. However, I do not have access to Matlab nor I am that competent using it.</p></li>
<li><p>Using the command prompt and VLC. However, many images seem to get skipped and one ends up with lots of corrupt files. (e.g: <a href=""https://www.raymond.cc/blog/extract-video-frames-to-images-using-vlc-media-player/"" rel=""nofollow noreferrer"">https://www.raymond.cc/blog/extract-video-frames-to-images-using-vlc-media-player/</a>)</p></li>
<li><p>Potentially calling Octave through R and using similar approach of ffmpeg. Again, this is a bit beyond my ability for the moment.</p></li>
</ol>

<p>If anyone has some quick guidance that would be greatly appreciated.</p>
","<p>To extract frame # <code>x</code> through <code>y</code>, use</p>

<pre><code>ffmpeg -i in.mp4 -vf select='between(n,x,y)' -vsync 0 frames%d.png
</code></pre>
","19883"
"Does rackmount hardware attach only on the faceplate?","556","","<p>I've just purchased a few simple hardware rackmount devices, and a rack case.  I've never set anything like this up before.  I've been told that the devices' faceplates are screwed to the rack, and no other support is required.</p>

<p>Is this true?  Are rack devices only attached by the faceplates, and there's no other support?  Some of these devices are kind of heavy and it doesn't seem like enough support to handle the weight, particularly during transport where they bump around.  </p>

<p>I want to make sure I'm not missing something.</p>
","<p>Yes, almost all rackmount devices are designed to bear their loads on the front plate. Only in very rare circumstances have I seen a piece of rack gear that has not remained rigid while mounted in a rack, and that was only after <em>years</em> of being in one. There are also some rack pieces that have a second set of mounts in the back, but I haven't seen that in a long time. </p>

<p>My Crown power amp, which has got to be 50+ pounds, is only front mounted, as is my Akai S5000, which is about as unwieldy as it gets. You should be fine. Make sure you use four rack screws, and be careful not to overtighten them. You may also want to use rubber or nylon washers, but that's a whole other topic in itself.</p>
","1210"
"FFmpeg pan filter and clipping","554","","<p>I have a file that has one video stream and one audio stream with 8 channels. I want to create a new file that will only contain the first 2 channels of the audio stream (no video). I can do this using the <a href=""https://ffmpeg.org/ffmpeg-filters.html#pan"" rel=""nofollow"">pan filter</a>. I also want to clip the file. I can do this using the <a href=""https://www.ffmpeg.org/ffmpeg.html#toc-Main-options"" rel=""nofollow"">-ss and -t options</a>. However, when I try to do both together, I get an error.</p>

<p>So, if I run:</p>

<pre><code>ffmpeg -i a.mov -filter_complex ""[0:1]pan=2c|c0=c0|c1=c1[stereo]"" -map ""[stereo]"" ""a.audio.mov""
</code></pre>

<p>It works fine - I get the audio file with the correct channels.</p>

<p>But when I run:</p>

<pre><code>ffmpeg -i a.mov -filter_complex ""[0:1]pan=2c|c0=c0|c1=c1[stereo]"" -map ""[stereo]"" -ss 00:00:01 ""a.audio.mov""
</code></pre>

<p>I get the following error</p>

<blockquote>
  <p>[trim for output stream 0:0 @ 0000000002145720] Cannot select channel
  layout for the link between filters trim for output stream 0:0 and
  output stream 0:0.<br>
  [trim for output stream 0:0 @ 0000000002145720] 
  Unknown channel layouts not supported, try specifying a channel layout
  using 'aformat=channel_layouts=something'.</p>
</blockquote>

<p>Anyone have any ideas what I'm doing wrong and how to fix it?</p>
","<p>This was bug in atrim filter, not supporting unknown channel layouts, it should be fixed in latest ffmpeg.</p>
","17628"
"Premiere Pro CS6 preview problem: other application shows through","553","","<p>Sorry if this is the wrong place to ask this question, but, I'm having a issue with Premiere Pro CS6 from Creative Cloud.  Whenever I put footage in the timeline this happens: <img src=""https://i.stack.imgur.com/k3mYm.jpg"" alt=""problem image"">.  The file's encoding is fine and it happens with every file.  I'm running it on a MacBook Pro Retina.  Does anyone have any ideas on how I could try and get Premiere working?</p>
","<p>It looks like a problem with the playback engine.  I would try updating your video card drivers and/or making sure the playback engine (in the project/ project settings/general menu) is set to Mercury Playback Engine Software Only.  If that doesn't work, I'd try uninstalling and re-installing Premiere.</p>

<p>I don't know what can cause it on Mac OS, but on Windows seeing something from another application through a portion of a window is generally a sign of a graphics driver being misconfigured or misused by the software in question.</p>
","8031"
"Detect GoPro camera model from MP4","550","","<p>I have raw video file named GP010661.MP4, which seems to be taken with GoPro. Is there a way to detect the exact GoPro camera model and mode used for this video?</p>

<p>This is the the output of ExifTool:</p>

<pre><code>ExifTool Version Number         : 9.46
File Name                       : GP010661.MP4
Directory                       : .
File Size                       : 3753 MB
File Modification Date/Time     : 2014:11:27 09:37:20+03:00
File Access Date/Time           : 2015:02:19 09:22:46+03:00
File Inode Change Date/Time     : 2015:02:19 09:22:46+03:00
File Permissions                : rw-r--r--
File Type                       : MP4
MIME Type                       : video/mp4
Major Brand                     : MP4 Base w/ AVC ext [ISO 14496-12:2005]
Minor Version                   : 0.0.0
Compatible Brands               : avc1, isom
Movie Header Version            : 0
Create Date                     : 2014:11:27 11:19:55
Modify Date                     : 2014:11:27 11:19:55
Time Scale                      : 90000
Duration                        : 0:17:25
Preferred Rate                  : 1
Preferred Volume                : 100.00%
Preview Time                    : 0 s
Preview Duration                : 0 s
Poster Time                     : 0 s
Selection Time                  : 0 s
Selection Duration              : 0 s
Current Time                    : 0 s
Next Track ID                   : 3
Track Header Version            : 0
Track Create Date               : 2014:11:27 11:19:55
Track Modify Date               : 2014:11:27 11:19:55
Track ID                        : 1
Track Duration                  : 0:17:25
Track Layer                     : 0
Track Volume                    : 0.00%
Image Width                     : 1920
Image Height                    : 1080
Graphics Mode                   : srcCopy
Op Color                        : 0 0 0
Compressor ID                   : avc1
Source Image Width              : 1920
Source Image Height             : 1080
X Resolution                    : 72
Y Resolution                    : 72
Compressor Name                 : .GoPro AVC encoder
Bit Depth                       : 24
Color Representation            : nclc 1 1 1
Gamma                           : 2.2
Video Frame Rate                : 59.94
Matrix Structure                : 1 0 0 0 1 0 0 0 1
Media Header Version            : 0
Media Create Date               : 2014:11:27 11:19:55
Media Modify Date               : 2014:11:27 11:19:55
Media Time Scale                : 48000
Media Duration                  : 0:17:25
Handler Type                    : Audio Track
Handler Description             : .GoPro AAC
Balance                         : 0
Audio Format                    : mp4a
Audio Channels                  : 2
Audio Bits Per Sample           : 16
Audio Sample Rate               : 48000
Movie Data Size                 : 3933436091
Movie Data Offset               : 1474560
Avg Bitrate                     : 30.1 Mbps
Image Size                      : 1920x1080
Rotation                        : 0
</code></pre>

<p>The reason I need it, is that file chokes on my Ubuntu 14.04 with VLC and I couldn't even watch it. It took few days to recode the videos, and I do not want to run into the same problem again. If I know the camcoder model and mode, I can to ask people to use different camera or settings.</p>
","<p>MP4 files don't have as much data as .JPEG or .CR2 files so there is probally no way to know. Your cpu is that fast and GoPro footage is hard for the computer to play. </p>
","15950"
"GoPro Footage with Adobe Premiere is stuttering","549","","<p>Okay.</p>

<p>I searched many internet sites but i can't handle this problem.</p>

<p>My PC:</p>

<ul>
<li>6 GB RAM</li>
<li>4* 2.4 GHz CPU</li>
<li>1 GB Radeon GPU</li>
<li>SSD for System and Program</li>
<li>HDD for Video footage</li>
<li>Software: Adobe Premiere Pro CS6 with TotalCode Package.</li>
</ul>

<p>Video footage: </p>

<ul>
<li>Captured with GoPro</li>
<li>1920*1080</li>
<li>29,97 fps</li>
<li>MP4 Format</li>
<li>data-rate 35140 kBit/s</li>
<li>Clip 28s, 123 MB.</li>
</ul>

<p>When i open this Clip within Adobe Premiere, create a sequence from the clip and try the preview the first 2 sec will play smooth and after that the video is jumping and stuttering or after running 2 sec the preview freezes. No effects applied. (CPU Usage ~50%, all cores)</p>

<p>So my Sequence settings are: Rovi TotalCode AVC-Intra 100, 1920*1080 30 fps Non-Drop-Frame-Timecode. No matter if i choose the TotalCode renderer or the Mecury Renderer. Video preview is set to AVC Intra-Class 100 1080 24p (or 30p or 60i - no difference). 
Video preview format makes no difference between full, 1/2 or 1/4.</p>

<p>I think because i created the sequence from the clip, there is no red/yellow/gren bar over the clip in the cutting window. </p>

<p>I get better results if i create a 720p sequence (for example: For editing 1280x720 AVCHD type video. Widescreen NTSC video (16:9 progressive). 29.97 fps 48kHz (16 bit) audio.). When i insert a clip and preview it the video stutters and jumps. But now i have the yellow bar. If i render the whole working area the preview plays smooth.</p>

<p>Okay - now i would say: its my fault, my PC has not enough power.</p>

<p>BUT:</p>

<p>if i convert the MP4 via GoPro Studio to avi (Medium resolution) with the same specs (1920*1080*29,97) i got a 520 MB file (so roughly 4x the size of the original video).</p>

<p>When i use this clip instead of the MP4 the preview will play smooth (or nearly smooth) (CPU usage ~50%, all cores). </p>

<p>So Adobe Premiere has a problem with a data-rate of 35140 kBit/s but not with 151182 kBit/s on my PC? Hard to understand that this should be a performance issue...</p>

<p>I also tried to copy the clip to my SSD and start playing it, but nothing happens. Also tried to safe my project files on my SSD, not working.</p>

<p>I also tried to change the optimize rendern setting between Memory and Performance - nothing changed.</p>

<p>I saw this question:
<a href=""https://video.stackexchange.com/questions/10361/adobe-premiere-is-lagging-in-preview-window"">Adobe premiere is lagging in preview window</a> -> Because of the mentioned AVI i don't think it has something to do with the data size. </p>

<p><a href=""https://video.stackexchange.com/questions/9976/editing-1440p-47-fps-in-premiere-pro-cs6"">Editing 1440p @ 47 FPS in Premiere Pro CS6</a> -> Much higher data-rate and video quality.</p>

<p><a href=""https://video.stackexchange.com/questions/8133/adobe-premiere-preview-video-lagging"">Adobe Premiere preview video lagging</a> -> also talks about data rate (think this doesn't apply because of described avi/mp4 behavior)</p>

<p>Does anyone have some more ideas? Of course i could just convert 20 GB from mp4 to avi, but i'd like to safe this additional space on my hdd.</p>
","<p>The reason Premiere runs just fine with a massive file but struggles with a smaller file is the issue of data compression. Many people (logically) believe that the two tradeoffs between high and low compression codecs is information vs. space, and that codecs with more information will take more processing power to process. This is actually not the case. While more compression will cause a loss of visual information, it also makes the data more difficult to process because the CPU (or the GPU in the case of accelerated hardware) has to decompress the codec information before it can be displayed. This means that codecs like h.264 and VP9 are more difficult to process than DNxHR and Prores.</p>

<p>If you can give more information about your CPU and GPU, I can adjust this to be more useful to you. The main issue I see in your specs is a mediocre CPU and GPU. The all-intra bit encoding (all-intra isn't technically a codec, just a means of storing data within h.264) uses single-frame compression. This helps retain more information and improves scrubbing performance within a video editor, but it makes playback more difficult than a normal h.264 encoding. While I'm no expert on the subject, I imagine that any hardware h.264 acceleration your computer has may not be able to process all-intra.</p>

<p>My personal recommendation would be to upgrade your GPU to something like a GTX 950 or a GTX 960, or simply convert your files to a visually lossless format such as DNxHR before you begin editing.</p>
","16494"
"What can cause poor quality video in Canon point&shoot camera?","546","","<p>I recorded some videos from my superzoom compact camera Canon Powershot SX150 IS. When I run these videos on camera itself, they run fine. But when I move them on PC and run it, I can see it being pixelate and gray framing many times.</p>

<p>I have tried it with Auto mode as well as M mode (where I can do all settings myself).</p>

<p>Am I missing some setting?
Or if I can repair these videos...</p>
","<p>On camera viewing is limited by the low resolution of the lcd-display of the camera. Maybe there is the same issues but you just don't notice them on the small display?</p>

<p>As for what might be causing this, without seeing a sample video, I'd suggest a slow memory card. Would you get these quality issues if you shot a video with one step lower quality? I would buy/borrow a faster or at least better quality memory card and see if the problem is solved.</p>
","8449"
"FFmpeg. CUDA decode. Past duration too large. Variable FPS","545","","<p>Please see my input udp stream information: </p>

<blockquote>
  <p>ffprobe udp://IP:PORT</p>
</blockquote>

<pre><code>Input #0, mpegts, from 'udp://IP:PORT':
  Duration: N/A, start: 42243.737022, bitrate: N/A
  Program 17
  . . .
    Stream #0:0[0x781]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, bt470bg, top first), 720x576 [SAR 12:11 DAR 15:11], 24.67 fps, 25 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x782](Tam): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, mono, s16p, 64 kb/s
    Stream #0:2[0x783](Kan): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, mono, s16p, 64 kb/s
</code></pre>

<p>And run this command again:</p>

<blockquote>
  <p>ffprobe udp://IP:PORT</p>
</blockquote>

<pre><code>Input #0, mpegts, from 'IP:PORT':
  Duration: N/A, start: 42589.601022, bitrate: N/A
  . . . 
  Program 17
    Stream #0:0[0x781]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, bt470bg, top first), 720x576 [SAR 12:11 DAR 15:11], 24.58 fps, 25 tbr, 90k tbn, 50 tbc
    Stream #0:1[0x782](Tam): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, mono, s16p, 64 kb/s
    Stream #0:2[0x783](Kan): Audio: mp2 ([3][0][0][0] / 0x0003), 48000 Hz, mono, s16p, 64 kb/s
</code></pre>

<p>Please pay attention to FPS value. In the first execution it <strong>24.67</strong> and in the second it <strong>24.58</strong></p>

<p>I want to transcode this stream with the following command:</p>

<blockquote>
  <p>ffmpeg -y -hwaccel cuvid -c:v h264_cuvid -deint 2 -i
  ""udp://IP:PORT"" -map 0:#0x0781 -map 
  0:#0x0782 -vcodec h264_nvenc -acodec libfdk_aac -f flv rtmp://IP/test/name</p>
</blockquote>

<p>Please see the log:</p>

<pre><code>Past duration 0.981102 too large
frame=83 fps=0.0 q=18.0 size=502kB time=00:00:04.05 bitrate=1015.4kbits/s dup=0 drop=80 speed=7.42x  
frame=96 fps=90 q=18.0 size=643kB time=00:00:04.69 bitrate=1122.1kbits/s dup=0 drop=93 speed=4.38x  
frame=111 fps=70 q=18.0 size=802kB time=00:00:05.14 bitrate=1278.1kbits/s dup=0 drop=108 speed=3.26x 
frame=121 fps=57 q=19.0 size=921kB time=00:00:05.61 bitrate=1344.7kbits/s dup=0 drop=118 speed=2.64x 
frame=137 fps=49 q=19.0 size=1096kB time=00:00:06.25 bitrate=1436.4kbits/s dup=0 drop=134 speed=2.26x
</code></pre>

<p>I also can add the framerate parameter ""-r"". As follow:</p>

<blockquote>
  <p>ffmpeg -y -hwaccel cuvid -c:v h264_cuvid -deint 2 -r 25 -i
  ""udp://IP:PORT"" -map 0:#0x0781 -map 
  0:#0x0782 -vcodec h264_nvenc -r 25 -acodec libfdk_aac -f flv rtmp://IP/test/name</p>
</blockquote>

<p>In this case this warning is not shows. But the transcoding FFmpeg process failed after 10-15 seconds, without any errors in the log. <br><br>
Everything works fine if I do it via software decoding:</p>

<blockquote>
  <p>ffmpeg -y -i
  ""udp://IP:PORT"" -map 0:#0x0781 -map 
  0:#0x0782 -vcodec h264_nvenc -acodec libfdk_aac -f flv rtmp://IP/test/name</p>
</blockquote>

<p>Also everything works fine if I transcoding it to the local file and specify framerate:</p>

<blockquote>
  <p>ffmpeg -y -hwaccel cuvid -c:v h264_cuvid -deint 2 -r 25 -i
  ""udp://IP:PORT"" -map 0:#0x0781 -map 
  0:#0x0782 -vcodec h264_nvenc -r 25 -acodec libfdk_aac -f flv result.flv</p>
</blockquote>

<p><strong>This issue affects only for some input streams.</strong><br>
How can I transcode this UDP stream to FLV stream with CUDA decoding? </p>
","<p>The key <strong>-drop_second_field 1</strong> fixed this issue. <br> And result command looks as follow: <br></p>

<blockquote>
  <p>ffmpeg -y -hwaccel cuvid -c:v h264_cuvid -deint 2 -drop_second_field 1
  -i ""udp://IP:PORT"" <br> -vcodec h264_nvenc -acodec libfdk_aac -f flv rtmp://IP/test/name</p>
</blockquote>

<p><br>
What I think about this problem: at the moment in cuda decoding we can use only 2 deinterlace variants: </p>

<ul>
<li>-deint 1 it's bob deinterlacing,</li>
<li>-deint 2 it's adaptive deinterlacing.</li>
</ul>

<p>Both of these variants are increase bitrate, because after deinterlacing each field(half of frame) displaying as frame (framerate is doubled) Bitrate is grow and if fps is variable we got timestamps issue. And when we enable drop second field then everything looks fine with bitrate and timestamps.</p>
","21508"
"Does Sony PMW-EX1R HDMI output contain audio?","544","","<p>I'm trying to get a Sony PMW-EX1R XDCAM to output HDMI for live streaming (using Teradek Vidiu). The picture looks fine, but it seems to lack audio. Is there a setting that I'm missing, or is it impossible to use HDMI in this way? </p>
","<p>HDMI does support audio, but it is possible that either the PMW-EX1R doesn't output audio over HDMI (which would seem odd to me, but is possible).  It is also possible that the settings are wrong on the Vidiu.  The Vidiu appears to support audio input from either the HDMI port itself or a line input.  If it is expecting signal on the line input, it might ignore the HDMI audio.</p>

<p>I'd check the documentation for the Vidiu to see if you can find anything about that since I don't see anything in the PMW-EX1R documentation to answer it.  You could also try plugging the PMW-EX1R in to another HDMI device to see if you get audio.  If not, you can also try running the audio output of the camera in to the line input of the Vidiu and monitor off of the headphone jack of the Vidiu.</p>
","9107"
"Do you have to build ffmpeg from source to use 10-bit codecs in Windows?","541","","<p>I'm trying to find somewhere I can download pre-built binaries of ffmpeg for Windows with 10-bit codec support.  Apparently, from what I've read, ffmpeg has to be either 8-bit or 10-bit, and the builds on www.ffmpeg.org for Windows are all 8-bit.</p>

<p><a href=""https://ffmpeg.zeranoe.com/forum/viewtopic.php?f=10&amp;t=2270&amp;sid=5b85890be38d279b4bbe57652105fad9&amp;start=10"" rel=""nofollow noreferrer"">This thread</a> on the ffmpeg forums suggests that there were 10-bit releases for a time, but no longer.</p>

<p>My google searching has turned up several sites with 10-bit builds from 2013, 2016, and so on, but nothing consistent over time.</p>

<p>Before I go to the trouble of trying to set up a way to build this from source, I just want to confirm-- there's no site regularly providing 10-bit Windows builds of ffmpeg, right?</p>
","<p>Not quite. The x264 encoder, specifically, can be compiled with either 8-bit or 10-bit support, but not both.That restriction doesn't apply to x265 or any other encoders that I can think of. So, unless you need to encode to H.264 10-bit using x264, the regular builds of ffmpeg are fine.</p>

<p>You can fetch standalone binaries for 10-bit x264 from the official site at <a href=""https://download.videolan.org/x264/binaries/"" rel=""nofollow noreferrer"">https://download.videolan.org/x264/binaries/</a> Too much hassle to compile ffmpeg with all other dependencies if you just need that one 10-bit encoder.</p>

<p><strong>Edit</strong>: This is no longer true as of late Dec 2017. x264 now compiles and links as a multilib with both 8-bit and 10-bit support.</p>
","21228"
"Shoot moving clouds","538","","<p>I need to shoot the clouds and how they are moving during the time. I saw a lot of time similar clips on youtube and always shot it is just a set of animated photos. Am I right? If yes, is there any special name for this approach? Or is it just an accelerated video?</p>
","<p>I think what you're thinking of is time-lapse photography. In a sense it <em>is</em> animated photos, except that the ""photos"" are captured with a video camera. </p>

<p>You can certainly Google time lapse and get hundreds of good ideas for creating time lapse photography, there are two methods for doing it--in-camera, and post-production. </p>

<p>For in-camera, you're generally going to be shooting 1 frame every x seconds. The larger ""x"" is, the longer the time lapse will last, and the more time between each frame. I usually run some tests starting at 1 frame every 30 seconds and adjust from there. It take a lot of time to test, since you have to go through about 200 frames to get a true sense of what the motion looks like.</p>

<p>In post-production, you would just take the 1hour clip you photographed in real time, tell the software to make it, say 30 seconds, and the software will automatically pick out the frames it'll show you during playback. Again, it's like animated stills, one frame at a time. </p>

<p>The principle is the same: normal film/video is 24/25/30 (depending on your standard) frames every second, you compress the time by INCREASING the time interval between frames. Instead of 24 frames per second (fps), 8 fps will give you fast motion and 1 frame every 8 seconds will move so fast it's a time lapse clip. </p>

<p>Whether that technique happens in-camera or in the post production proceess is up to you. </p>

<p>d  </p>
","4474"
"How to avoid sluggish UI in FCPX - more CPU, GPU, RAM or what?","537","","<p>My question is about what aspects of hardware performance would best prevent sluggish UI and 'beachballing' in FCPX.</p>

<p>Currently I am running on a Macbook Air Core i7 2.0ghz, 8GB RAM and Intel HD4000 graphics.</p>

<p>I am editing 1080p only. The project files are on a Drobo 5D raid-style Thunderbolt drive connected via Caldigit Thunderbolt dock.</p>

<p>Generally the edits are not complicated, just one layer of video with some colour correction basic filter and transitions.</p>

<p>I realise my current system is underpowered, and at times I suffer from unresponsive UI. My question is what upgrade would give the most benefit:</p>

<ul>
<li>quad core i7?  (currently dual core)</li>
<li>16GB RAM or more?</li>
<li>discrete GPU?  or more recent Intel Iris would be enough?</li>
<li>if GPU, is available video RAM more important than processing power?</li>
<li>the current HDD should be fast enough?</li>
</ul>

<p>I want to stress that, for the purposes of this question, I do not care about rendering times - only to avoid unresponsive UI.</p>

<p>I have proxy and optimised media for all my projects, and I have the proxy media playback option selected:</p>

<p><img src=""https://i.stack.imgur.com/rzYUv.png"" alt=""enter image description here""></p>
","<p>Your hardware is NOT underpowered. I can run FCPX on a old MacBook Air using proxy media, with only 4GB if memory. That machine is much slower than yours. So it's not the MacBook that's the problem.</p>

<p>Have you tried to copy your media to a local drive just to see if you have issues with your raid system?</p>

<p>Another thing to check is running the Activity Monitor utility and see if your CPU is busy when you get the beach ball. </p>

<p>Before we know why you are having these issues it's hard to give a recommendation on what parts to upgrade.</p>
","15250"
"Vegas pro will not show video for avi with mp4v fourcc codec?","534","","<p>I have a few avi videos that are encoded with the mp4v fourcc codec. I have Sony Vegas Pro 8 and it will not open the video part. It will bring the files into the file manager, but when I drag them into the project timeline they only show the audio track as if there is no video track at all. There is no error message at all.</p>

<p>I would like to be able to resolve this without converting the files.</p>
","<p>I resolved this issue by installing <a href=""http://www.videolan.org/vlc/"" rel=""nofollow"">VLC</a>. Apparently VLC has some necessary codecs that Vegas Pro uses.</p>

<p>After installing VLC I was able to import the video and use it normally with no hindrances. </p>

<hr>

<p>After some time, this issue cropped up on a different computer. Installing/reinstalling VLC did not work.</p>

<p>I installed <a href=""http://www.softpedia.com/get/Multimedia/Video/Codec-Packs-Video-Codecs/D-i-v-X-AV-Codec-Pack-Pro.shtml"" rel=""nofollow"">AVI Codec Pack Pro v2.4.0 from Softpedia</a> and the issue was resolved. </p>

<p>If Softpedia no longer has the file available by the time you are reading this, the file was titled ""avi.codec.pack.pro.v2.4.0.setup.exe"" and was 16.7MB (17,538,685 bytes). A google search at the time of update yields other sources for download, but some are concerned with malware imbedded in it. I haven't noticed any malware with the softpedia download.</p>
","10011"
"ffmpeg default output frame rate","533","","<p>What is the default output frame rate chosen by <code>ffmpeg</code> to encode MP4s?<br>
Where is this specified on the <code>man</code> page?</p>

<p>My input video stats are:</p>

<ul>
<li><code>r_frame_rate=120/1</code></li>
<li><code>avg_frame_rate=31230000/1042111</code></li>
<li><code>duration=11.579011</code></li>
<li><code>nb_frames=347</code></li>
</ul>

<p>After (demuxing, decoding) splitting (rescaling, encoding and muxing) the video stream I end up with has the following stats:</p>

<ul>
<li><code>r_frame_rate=120/1</code></li>
<li><code>avg_frame_rate=120/1</code></li>
<li><code>duration=9.575000</code></li>
<li><code>nb_frames=1149</code></li>
</ul>

<p>I started with <code>347</code> frames and ended up with <code>1149</code> (should have been <code>287</code>).
The input average fps was <code>29.97</code> and now I end up with <code>120</code>... which was the max fps of the input video.
So, I imagine <code>ffmpeg</code> picks the highest one from the input... but is this specified anywhere?
How does one go simply matching the input fps?
And why does I have a variable fps to begin with?</p>

<p>Hmm... too may questions I guess.
Still, is this simply a consequence of this paragraph?</p>

<pre><code>STREAM SELECTION                                                 
       By default, ffmpeg includes only one stream of each type  
       (video, audio, subtitle) present in the input files and   
       adds them to each output file.  It picks the ""best"" of    
       each based upon the following criteria: for video, it is  
       the stream with the highest resolution, for audio, it is  
       the stream with the most channels, for subtitles, it is   
       the first subtitle stream. In the case where several      
       streams of the same type rate equally, the stream with the
       lowest index is chosen.                                   
</code></pre>
","<p><code>r_frame_rate</code> is ""<em>the lowest framerate with which all timestamps can be represented accurately (it is the least common multiple of all framerates in the stream).</em>""</p>

<p><code>avg_frame_rate</code> is just that: <code>total duration / total # of frames</code></p>

<p>You can just specify <code>-r 30000/1001</code> to maintain the average rate (near-abouts). You don't specify which format you're outputting to, but for MP4, ffmpeg defaults to constant-frame rate, where it picks <code>r_frame_rate</code> as the value. It will then duplicate or drop frames to keep that rate. Use <code>-vsync vfr</code> to keep the variable rate.</p>
","20790"
"How do i put the image behind video by using FFmpeg?","533","","<p>I have one video (400x300) and an image (1280x300).
pls show me how can i put this image behind the video like that:
i've try, but not working T-T</p>

<p><a href=""https://i.stack.imgur.com/w7PB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w7PB5.png"" alt=""enter image description here""></a></p>
","<h1>Use the <a href=""https://ffmpeg.org/ffmpeg-filters.html#overlay"" rel=""nofollow"">overlay filter</a></h1>

<p>This will overlay the video on top of the image:</p>

<pre><code>ffmpeg -loop 1 -i image.png -i video.mp4 -filter_complex ""overlay=(W-w)/2:shortest=1"" output.mp4
</code></pre>

<p>If you need to resize the video then add the <a href=""http://ffmpeg.org/ffmpeg-filters.html#scale"" rel=""nofollow"">scale filter</a>:</p>

<pre><code>ffmpeg -loop 1 -i image.png -i video.mp4 -filter_complex ""[1:v]scale=400:-1[fg];[0:v][fg]overlay=(W-w)/2:(H-h)/2:shortest=1"" output.mp4
</code></pre>

<h1>You can use pad filter instead of an image</h1>

<p>If you just want to add black padding you don't need an image and can use the <a href=""https://ffmpeg.org/ffmpeg-filters.html#pad"" rel=""nofollow"">pad filter</a>.</p>

<pre><code>ffmpeg -i video.mp4 -filter_complex ""pad=1280:0:(ow-iw)/2"" output.mp4
</code></pre>

<h1>Using the player to do it instead</h1>

<p>You could add the padding upon playback so you don't even need to re-encode:</p>

<pre><code>ffplay -vf ""pad=1280:0:(ow-iw)/2"" input.mp4
</code></pre>
","16976"
"Do internal SSDs challenge the conventional wisdom of storing digital assets on external ""scratch"" disks?","530","","<p>I've heard that all project assets should be stored on a separate ""scratch"" disk for the sake of increased performance. I'm wondering if that conventional wisdom holds even when your ""scratch"" disk is HDD and your internal disk is SSD. (I'm aware that the cost/capacity ratio is lower for HDDs. My question concerns performance issues, not capacity.)</p>

<p>Here's how my question originated: I am experiencing lag using my mid-2012 MacBook Air (i5, 8GB RAM) to edit Final Cut Pro X. I'm wondering if re-arranging my assets would stand a change of improving performance. Here's where things are right now:</p>

<ul>
<li>The application (the *.app file for Final Cut Pro X) is on my internal SSD.</li>
<li>The library (the *.fcplibrary file) is on my thunderbolt-attached HDD.</li>
<li>The digital assets (stills and video) are on the same external HDD.</li>
</ul>

<p>My library file merely references the digital assets; I am not copying them into the resource fork. For that reason, my library file is small enough that I could store it on my local drive. The new arrangement would be:</p>

<ul>
<li>The application (the *.app file for Final Cut Pro X) is on my internal SSD.</li>
<li>The library (the *.fcplibrary file) is <em>on my internal SSD.</em></li>
<li>The digital assets (stills and video) are on the same external HDD.</li>
</ul>

<p>So, would I be breaking some law of the universe with this configuration? And where did the conventional wisdom on ""scratch"" disks come from, anyway? Does it still apply to SSDs? (Again, in terms of performance only please, not capacity.)</p>
","<p>Scratch disks should never be external.  I'm not sure where you heard that.  External drives can be used for long term asset storage, but it is best to move assets local to work on them.</p>

<p>The point of scratch disks is to be the highest speed work area you have available to work from.  Scratch disks are used for data that needs to be moved in and out of memory quickly, so internal SSDs are the best bet.</p>

<p>The one thing you don't want to do is you don't want assets and scratch disks on the same magnetic drive because of seek time.  This isn't a concern with SSDs though, so you can generally have them on the same drive.  If you are working with very large files, then there may be a bandwidth advantage to having them on different SSDs, but in order of importance you want memory to be the fastest possible, then scratch disk, then working asset storage, then long term asset storage.</p>

<p>The point is to keep data supplied to the CPU and GPU as much as possible.</p>
","10430"
"Usage of Target and Maximum Bitrate with the H264 Codec in Premiere Pro","529","","<p>When exporting a video using the H264 Codec with a variable bitrate in Premiere Pro, you can set the target bitrate and the maximum bitrate. I would like to know how those work together and if there is any rule of thumb on what to set them to.</p>

<p>So AFAIK, the target bitrate is the average bitrate that the encoder will try to achieve by balancing out parts of the video where more or less information is required respectively. In that case, what is the purpose of the maximum bitrate setting? Is it the highest possible bitrate that won't be ecelled at any point of the video? Or can the encoder actually exceed the average bitrate over the amount speficied as target bitrate and the maximum bitrate is the highest possible <em>average</em> bitrate that the resulting video can have?</p>

<p>Also, how can I determine a good value for the maximum bitrate depending on the target bitrate? Let's say I export with 1, 10 or 20 MB/s (as my target bitrate), what should the maximum bitrate setting depend on?</p>
","<p>Sondell spelled it out correctly. </p>

<p>You use the <strong>Target</strong> to size your final output file, as VBR will be used. AME will then try to average as close as possible to ""average out"" to your target, yet the actual bitrate could be much lower, or up to your Maximum Setting. </p>

<p>The <strong>Maximum Setting</strong> is in place because of bottlenecks which exist for playback; for instance, many Blu-Ray players cannot read anything higher than 30 Mbps. DVD Caps around 9 mbps I believe. </p>

<p>The one part that I would like to point out is I recommend always using 2-Pass VBR, never one pass; unless your content is very consistent frame for frame. </p>

<p>AME/PPro <strong>lack</strong> a <strong>""Minimum BitRate""</strong>; which is a shame. Using VBR 1 Pass can cause pixelation issues; such as when a camera flash goes off - blowing out to near all white a single discreet frame. As a 1 Pass encode <strong>does not</strong> pre-analyze the program stream prior to setting the bitrate for a given frame set; a one pass VBR can cause artifacting after a camera flash; or a white or black hard edit to picture. </p>

<p>Also, any flickering type of video as well, such as strobe lights, can also cause this. Basically; if your image goes from flat color (any color) to complex picture/moving image in a 1-2 frame span... there can be trouble. </p>

<p>This occurs because it sees the flash as a blown out white image... thus AME drops the bitrate down very low (think under 0.5 Mbps); because after all; its just a frame of white... high bit rates are not needed. </p>

<p>The issue is that AME, its scheme for how it ramps up and down the bit rate is not fast enough; when using 1 Pass VBR. So after a camera flash goes off; your next frame, or even next two frames; could appear blocky, as the VBR bitrate has not ramped back up fast enough, and the flash lasted for only a single discreet frame. </p>

<p>2 Pass; while it takes longer; does a much better job handling this. </p>

<p>To be perfectly honest; I master nearly all of my content using CBR. While this goes against the mainstream norm; I know what bitrates are needed for what quality I desire; I'm not happy with the 1 Pass Encoder Issues; and 2 Pass is a time consuming render. </p>

<p>There are cases where 2 Pass is necessary; such as long form online content delivery, but for most outputs CBR is fast, clean, and consistent. </p>
","21377"
"VHS to digital, flickering, lack of video and good sound","529","","<p>I'm trying to convert some VHS videos into digital.
To do so, I got the VGB100 from August and connected it to the VHS player and to my computer.</p>

<p>I've used two applications to capture video:</p>

<ul>
<li>Arcsoft Showbiz</li>
<li>VirtualDub</li>
</ul>

<p>Both of them get the same input. </p>

<p>Most of the time the image doesn't even show but when it does, it is a very flickery image in general and from time to time you can see the image more or less good.</p>

<p><a href=""https://i.stack.imgur.com/ObylJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ObylJ.png"" alt=""enter image description here""></a></p>

<p>I've tried it with the most recent VHS cassettes I have (spiderman 1, Toy Story 1...) and I get the same result with all of them.</p>

<ul>
<li>Do you think the problem is in the VHS cassettes? </li>
<li>Or in the VHS player? </li>
<li>Or in the SCART cable / output / cable? </li>
<li>Or even on the VGB100 device?</li>
</ul>
","<p>If you are using old VHS player, then its head can be dirty</p>

<p>Also VHS cassettes' tape can be dirty, but how I understand you using new cassettes. </p>

<p>So, try to connect your VHS player to TV. If signal will be the same, you need to locate that thing:
<a href=""https://i.stack.imgur.com/DfhJN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DfhJN.jpg"" alt=""VHS player head""></a>
And clean it and head (black thing on background to the left)</p>

<p>You can do it <a href=""http://www.hardwaresecrets.com/how-to-manually-clean-your-vcr-heads/"" rel=""nofollow noreferrer"">by yourself</a> or maybe there still exist place in the world, where professionals can do it for you.</p>
","17297"
"final cut pro x - 1440 x 1080 prevent video stretched to fill in timeline","526","","<p>I have created a FCPx project that is by default 1920 x 1080 however i have some video sources that are 1440 x 1080 the problem becomes when i import the video into the timeline and it stretches the 1440 to 1920 distorting it.</p>

<p>it's similar to this example <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/l8q8cnnUeY0?start=0""></iframe>
            </div></div> but even in this video it doesn't seem to fix the issue of the video being stretched.</p>

<p>how do i prevent such a thing in FCPx it does seem to not stretch it if the video isn't 1080..</p>

<p>thanks.</p>
","<p>I did a test for you and Larry is right ... Normally if the footage or file was anything other than 1440x1080, all you would need to do it set your Spatial Conform to none. However, in this instance FCPX is forcing a size based on the magic 1440 number. To correct this, select your clip and click the drill down triangle under scale in transform. Change just the X: scale to 75% because 1440 is 75% of 1920. That should do it!</p>
","18244"
"Video editor that lets me code my own effects from scratch?","526","","<p>I'm the type of guy who likes to produce beauty from code - I mostly experiment with shaders.</p>

<p>Up to this day, I've only been doing so for the sake of games, but an idea popped up in my mind that I could decorate an actual film footage using handwritten postprocessing.</p>

<p>Is there any software that, apart from facilitating basic video editing tasks, lets me code (and later apply) my own effect completely from scratch, that is, lets me manipulate each input frame pixel by pixel? Either in some shader language or maybe CUDA or even C++?</p>
","<p>I second the vote for Processing, but if C++ is more your thing than Java you could also look at <a href=""http://openframeworks.cc/"" rel=""nofollow"">openFrameworks</a>. It allows for more processor-intensive computation and as such is good at real-time synthesis and manipulation of video.</p>

<p>Another tool worth checking out is <a href=""https://www.nodebox.net/"" rel=""nofollow"">NodeBox</a>. It allows for coding, but wraps it all in a node-based GUI, the aim being to simplify the boring stuff.</p>

<p>Final Cut Pro  7 used to have a built in language called <a href=""http://www.joemaller.com/fcp/fxscript_reference.shtml"" rel=""nofollow"">fxcript</a> (based on c, but weird and buggy) that gave you pixel level access to the video stream and integrated into the editor seamlessly. I wrote a few plugins for that <a href=""http://web.archive.org/web/20091023111748/http://www.pureandapplied.com.au/plugins.html"" rel=""nofollow"">back in the day</a>, but I jumped ship after the initial FCPX fiasco so I can't comment on the new language for visual effects <a href=""https://developer.apple.com/library/mac/documentation/AppleApplications/Conceptual/FXPlug_overview/FXPlugSDKOverview/FXPlugSDKOverview.html"" rel=""nofollow"">FXPlug</a>. I believe it's quite powerful, and, like processing, leverages openGL to do the heavy lifting.</p>
","19085"
"How should I go about making 80's style graphics?","524","","<p>I want to make a title screen thing that looks like this:
<a href=""http://www.madman.com.au/images/screenshots/screenshot_10_18762.jpg"" rel=""nofollow noreferrer"">screen http://www.madman.com.au/images/screenshots/screenshot_10_18762.jpg</a></p>

<p>See how it's all anti-aliased and bloomy, and the font is sort of computery?</p>

<p>Also, I want to make it slightly waver here and there.</p>

<p>I can use After Effects, Premiere, and Sony Vegas</p>

<p>Thanks!</p>
","<p>There are a few things you can do to get this look, first, use an artificially low resolution.  A lot of early graphics generators were done on things like the Amiga Video Toaster at 320 by 240 resolution.</p>

<p>Additionally, you can get the background look by making a normal gradient in Photoshop and then reducing the color depth substantially.  You can either reduce to 8 bit color or simply use a GIF with dithering turned on and lower the number of colors until you get the desired result.</p>

<p>Use classic pixel fonts for the lettering.  There are literally hundreds of possibilities out there to choose from, so you best bet is to look through something like Google fonts until you see something you like.</p>

<p>Finally, use interlaced video and run a couple interlaced to non-interlaced and back conversions to add the scan line related issues typical of old footage.  If you have an old VCR, try recording the video through a couple generations of VCR recording too and it will give it some nice analog degradation to really maximize the feel.</p>
","10800"
"See layers beneath the solid I'm editing in After Effects","524","","<p>I have imported a photo into one layer and I created an opaque solid on top of it.
I want to animate the solid with the Brush tool (Write on).
When I enter layer edit mode for the solid I can't see the photo layer underneath it, and hence can't use the photo as a guide for the Brush tool.</p>

<p>Is there a way to increase layer transparency so I can temporarily see a guiding layer beneath it?</p>
","<p>Don't apply the brush tool to the solid - apply it to your photo instead.  There is an option in the tool to choose whether to apply the brush over the image, or to reveal the image.  When you're drawing your brush strokes, set it to apply over the image.  When you're ready to animate the strokes, set it to reveal the image.</p>
","12679"
"How to create 3D video from two separate stereo images?","523","","<p>I have two images that form a stereo pair. I'd like to view this in 3D on a <a href=""http://www.samsung.com/hu/consumer/televisions/televisions/plazma-tv/PS51D8000FSXXH"">Samsung 3D television</a>. I was looking for such a software but did not find anything exactly for this task. My question is: how could I put this togedher? What if I make a video that changes the two images frame-by-frame in 25 fps? Is 3d avi supposed to work that way?</p>
","<p>Use Sony Vegas. It can merge the stereo inputs and renders the output file as the format you need.</p>
","14864"
"Multiple Videos On Screen","523","","<p>I'm trying to get a whole bunch of videos on screen, playing at one time. The idea would be to slowly pan into the central video. Any ideas how this can be achieved? I'm using Sony Vegas Pro 9. I've attached these two pictures to illustrate what I'm trying to achieve.</p>

<p><img src=""https://i.stack.imgur.com/ey1bX.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/WKyfT.png"" alt=""enter image description here""></p>

<p>Tim</p>
","<p>Use Track Motion to create video wall - 9 tracks, each clip on its own track. There are tools that can help you to do it (I use Pan/Crop Assistant from <a href=""http://vegasaur.com/Vegasaur"" rel=""nofollow"">Vegasaur toolkit</a>).
Add another track (parent track) and make these 9 tracks composing childs of this parent track.
Use Parent Motion to create pan/zoom animation.</p>

<p>Read in the Help about Track Motion...</p>
","13319"
"Recording a gaming console onto my computer from my TV","523","","<p>So I have a TV with two types of inputs. Composite (just mono), and coaxial - yes it's a pretty old TV. I play a video game that I want to record some matches of onto my laptop so I can edit the clips and stuff.</p>

<p>My gaming system is hooked up to my TV via composite (just mono). What type of parts do I need to buy to be able to record from the TV onto my laptop from the gaming console?</p>
","<p>You will need a <a href=""http://rads.stackoverflow.com/amzn/click/B00392K4LQ"" rel=""nofollow"">USB Video Capture Device</a> to connect the game to your laptop.  </p>

<p>In order to also be able to view the game on the TV, you will need 2 <a href=""http://rads.stackoverflow.com/amzn/click/B000M52X62"" rel=""nofollow"">splitters</a> for the video and audio and an extra <a href=""http://rads.stackoverflow.com/amzn/click/B000V0DY7U"" rel=""nofollow"">composite cable</a>.</p>

<p>You would run the yellow and white game console outputs to the splitters and attach the splitters directly to the Video Capture device.  The red cable goes into the Video Capture device directly.  Connect the yellow and white connectors of the extra cable between the splitters and the TV.</p>

<p>I'm not sure how much the video and sound quality will be affected by splitting the signal, so you may need to buy a better <a href=""http://rads.stackoverflow.com/amzn/click/B003C2T03G"" rel=""nofollow"">splitter</a> to get it to work without any loss.</p>
","2369"
"Can QuickTime Pro Edit mp4 files?","522","","<p>I just started trying out Snagit 11 on Windows. It now records screencasts in .mp4 instead of .avi. I thought this would be great but there's a problem. I typically record a screencast and then open it in QuickTime Pro 7.x and edit out the bits I don't want. When I'm done I export to .mp4.  When I tried to do this with the mp4 that Snagit11 created, deleting a few seconds of video would actually delete a couple of minutes. I tried it multiple times and it was very repeatable. It got the starting point of the cut correct but it didn't get the stopping point even close. I'm not sure if this is a problem on the Snagit11 encoding side or on the QT pro side. I've never tried to edit an MP4 before. I don't see why this wouldn't be possible and I can see how I might lose <em>some</em> precision in adjusting playheads on an already compressed format but to miss by a couple of minutes seems a wee bit extreme. Are there any tricks I need to know to edit a video in QTPro that is already in mp4 format? </p>
","<p>I think the short answer (one you may have already found) is no! I'm in a similar situation: I write music to picture and used to receive clips as .movs. I could then add final audio myself for showreel/web purposes. Now those clips arrive as mp4 and are useless and unchangeable. I can merge files in QT pro for viewing, but no saves are allowed.</p>

<p>Editing software usually prefers 'linear' formats (aiff or wav for audio, .mov or similar for vids) whereas mp4 is heavily compressed and 'lossy'. Also, the key frame rate is very poor (300 I think) so that might explain some of your edit problems. The only workaround I can find is to open in QT and re-export, but the results are pretty ugly! Saving to mp4 for final product? Why not - it looks pretty and the file sizes are good. Working with mp4 in the edit? Hell no :)</p>
","4141"
"ffmpeg encoded videos are not playing on mobile browser","522","","<p>I have set up a server to upload and play videos. I used django/python as the scripting language, and nginx and gunicorn as the webserver. I am using <a href=""http://www.videojs.com/"" rel=""nofollow"">videojs</a> as the video player.</p>

<p>For h264 mp4 I tried this command:</p>

<pre><code>ffmpeg -i inputfile.avi -codec:v libx264 -profile:v baseline -preset slow -b:v 250k -maxrate 250k -bufsize 500k -vf scale=-1:360 -threads 0 -codec:a libfdk_aac -movflags +faststart output.mp4
</code></pre>

<p>But its only playing on desktop browsers and not on mobile browsers (except only on firefox android browser).</p>

<p>Here are some points that I gathered along the way:</p>

<ol>
<li>Some of the videos which I downloaded from youtube and uploaded it on the server without encoding are playing nicely on all the browsers.</li>
<li>However if I encode the same video (youtube video) and upload it on the server, it does not play on the mobile devices but only on desktop browsers.</li>
<li>Videos which I took from my mobile (samsung s4 and iphone 6), and encode it with ffmpeg are not playing on the mobile browsers, only on desktop browsers.</li>
<li>But, the url of the same videos (which I took from mobile) which are hosted on amazon s3 are playing nicely on all the browsers (even the non encoded videos).</li>
</ol>

<p>What am I missing? I was really hoping if you could guide me and help me to solve the problem. Thank you.</p>
","<p>Unfortunately due to patent/licensing issues<sup>1</sup>, MP4 playback is not supported natively in all browsers. That's where file format redundancy comes into play; if one file format is not able to be natively loaded by a system a backup version may be able to work.</p>

<p>I have found that certain browsers will be able to run files encoded with the .webm container using VP8 plus Vorbis for audio and video, respectively, out of the box while using HTML5 players.<sup>2</sup></p>

<p>Citations</p>

<hr />

<p><sup>1</sup> <a href=""https://developer.mozilla.org/en-US/docs/Web/HTML/Supported_media_formats#Browser_compatibility"" rel=""nofollow"">https://developer.mozilla.org/en-US/docs/Web/HTML/Supported_media_formats#Browser_compatibility</a></p>

<p><sup>2</sup> <a href=""https://en.wikipedia.org/wiki/HTML5_video#Multiple_sources"" rel=""nofollow"">https://en.wikipedia.org/wiki/HTML5_video#Multiple_sources</a></p>
","16040"
"Video recording using external microphone and smartphone","519","","<p>My plan is to record gameplay (using fraps) and myself using a Galaxy S + microphone. I then want to glue them together into one video - picture in picture kind of thing. </p>

<p>My concern is for the part where I use my phone (filming myself): do I have to put the audio and video together in an editing software? Or is the phone smart enough to match the sound that the external mic captures with the image?</p>

<p>Thanks a lot in advance. Also, feel free to recommend a good (~40$) microphone.</p>

<p>Cheers,</p>

<p>Adrian</p>
","<p>If the microphone is hooked in to the phone, then it should be an input to the camera app and it should record both at the same time.  If the microphone was not linked to the camera app for some reason but was recording separately, it isn't that hard to sync, just clap on camera and look for the spike on the waveform and align it with the clap.  I wouldn't expect it to be a problem though.</p>
","8219"
"Output format and quality to be same as input","518","","<p>Have many videos different videos of different format and quality in which i need to add small amount of text.</p>

<p>What command should i use to get same file format/quality for output as it was for input ?</p>
","<p>Since these are for broadcast play-out over local storage, these commands are biased towards quality and away from filesize.</p>

<p>For MP4s:</p>

<pre><code>ffmpeg -i input.mp4 -vf ""text filters here"" -c:v libx264 -crf 8 -c:a copy -map 0 -movflags +faststart output.mp4
</code></pre>

<p>For MPGs:</p>

<pre><code>ffmpeg -i input.mpg -vf ""text filters here"" -c:v mpeg2video -q:v 2 -c:a copy -map 0 output.mpg
</code></pre>
","17521"
"how to measure bitrate of video over time","516","","<p>Given an encoded video file with non-constant bitrate, how would one analyze the bitrate over time? So far I found one old program that produces a bitrate plot, but none that can give the raw data of the encoded frames.</p>

<p>I think bitrate data should be generated by averaging the size of a set of frames spanning 1 second--i.e. frames with a DTS belonging in that interval. So a 10 second video would have 10 data points describing the encoder input bitrate. Though I am curious to hear alternative ways of calculating bitrate.</p>

<p>Is there a way to get the size of each (encoded) frame? preferably with windows software. I know it is trivially easy for an uncompressed source (resolution * bitdepth * time).</p>
","<p>It does not make sense to speak of the size of an encoded frame in an inter-coded video stream, which is typical of MPEG-<em><code>X</code></em> videos. I-frames are self-contained but P- and B- frames are predicted frames and the final decoded result uses data from various inputs. On the other hand, the size of a single Group of Pictures (GOP) may be a meaningful measure.</p>

<p>The FFmpeg segmenter in combination with a compatible demuxer can break apart the video stream at GOP boundaries. By specifying an ultra-low time for segment size, you can ensure that each segment contains 1 GOP. Running a ffprobe on each segment will then provide the bitrate for that GOP.</p>

<p>I tested the command below on MP4s which had 1) a variable-sized GOP, 2) an intra-coded scheme and 3) fixed GOP of unusual length (11 frames), and each output had the expected result.</p>

<pre><code>ffmpeg -i input.mp4 -an -c:v copy -segment_time 0.00001 -f segment i%0d.mp4
</code></pre>

<p>Each segment contained exactly 1 I-frame (each segment of the intra input contained exactly 1 frame in all).</p>

<p>Running ffprobe on each segment gives a bitrate reading:</p>

<pre><code>Duration: 00:00:00.37, start: 4.466016, bitrate: 1539 kb/s
Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p,
1280x720 [SAR 1:1 DAR 16:9], 1520 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)
</code></pre>

<p>The above one is from the 11-frame GOP, and has a bitrate of 1520 kbps.</p>
","17193"
"Scaling videos of variable size to a fixed aspect ratio","514","","<p>I have 45 videos of varying aspect ratios and resolutions.
I need to scale all of them so that they fit neatly inside a 640x360 (16x9) container with the following specs:</p>

<p>16x9, 640x360, 1600kbps, H.264, MP4, progressive, 29.97</p>

<p>For example, some videos might be 512x12, others might be 480x320, but they all have to fit, scaled and centred, inside a new 640x360 output video.
If a video is not wide enough to fill that space, black bars should be added on the left and right.</p>

<p>Is this possible with ffmpeg?</p>

<p>Here's the code I've been toying with so far:</p>

<p><code>
ffmpeg -i ""input.mov""  -b:v 1600k -bufsize 1600k -r 29.97 -vf scale=640:360 -vcodec libx264 output.mp4
</code></p>

<p>This seems to do everything I need, except for dynamically scaling to that fixed resolution.</p>

<p>Does anyone know how to achieve the result I'm looking for?</p>

<p>Thanks!</p>
","<p>Use</p>

<pre><code>ffmpeg -i ""input.mov"" -vf 
          ""scale='if(gt(a*sar,16/9),640,360*iw*sar/ih)':'if(gt(a*sar,16/9),640*ih/iw/sar,360)',
          pad=640:360:(ow-iw)/2:(oh-ih)/2,setsar=1""
       -vcodec libx264 -b:v 1600k -bufsize:v 1600k -r 30000/1001 output.mp4
</code></pre>

<p>The scale filters use the conditional <code>if(a,b,c)</code> expression i.e. <code>if a then b else c</code>. I check if the video is wider than 16:9. If yes, I scale the width to 640 and scale height to preserve proportion. If not, I scale height to 360, and keep width proportional. Then the result is padded out to 640x360. The setsar is used to make sure ffmpeg registers the video as square-pixel.</p>
","19853"
"Is 360 video editing the same as traditional video editing?","513","","<p>With the rise and price drop in 360 video I'm wondering what the format is. Looking at the <a href=""http://centrcam.com/#technology"">CENTR Camera</a> doesn't even mention editing or file format. <a href=""https://theta360.com/en/about/theta/"">Ricoh Theta</a> doesn't mention file format or serious editing but does say they offer their own software for some editing.</p>

<p>What file type do 360 degree cameras use? Can it be edited in regular video production suites like SpeedGrade, DaVinci Resolve, Premier, FinalCut, etc.. or do you need some special software?</p>
","<p>Usually 360 videos became 360 only in realtime during playback. You can see it:</p>

<ol>
<li>Go to to <a href=""https://www.youtube.com/watch?v=S3xOB-Bigc8"" rel=""nofollow noreferrer"">this video</a> (youtube now supports 360 videos)</li>
<li>Now, using Chrome install <a href=""https://chrome.google.com/webstore/detail/disable-youtube-html5-pla/enmofgaijnbjpblfljopnpdogpldapoc"" rel=""nofollow noreferrer"">this extension</a> (it will disable HTML5 player)</li>
<li>Try to see video from the point one again.</li>
</ol>

<p>You will see something like that:
<a href=""https://i.stack.imgur.com/Xxk0V.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xxk0V.jpg"" alt=""Video from Vi Hart channel""></a></p>

<p>That happens because Flash player don't know how to interpret 360 video and showing original state of it.</p>

<p>So, during editing your 360 video can be displayed as normal video with huge distortion. If you can handle that, than you can edit it in any program of your chose. </p>

<p>Good luck!</p>
","16400"
"How is the default video player in Windows 10 achieving the low CPU utilization while playing FHD videos","512","","<p>I am trying to understand how the default Windows 10 player (called ""Films and TV"") is able to play FHD (1920 x 1080 x 60fps) videos (shot with GoPro 4) and utilize only around 10% of the CPU, while other players namely VLC and GOM will utilize around 60-70% CPU.</p>

<p>I tried looking at GPU utilization, but even there while using the default Windows 10 player utilization is around 15% vs around 30% when using VLC and GOM.</p>

<p>Right now this looks like magic to me as FHD playback is taxing on the hardware and I can't figure out what is going on. I am thinking the difference will be in the decoder used, but can't find more information. </p>

<p>The CPU / GPU used are Core i5 5200U / Intel HD Graphics 5500</p>

<p>VLC Player CPU utilization:
<a href=""https://i.stack.imgur.com/XGSQE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XGSQE.png"" alt=""VLC Player CPU utilization""></a></p>

<p>Films and TV CPU utilization
<a href=""https://i.stack.imgur.com/agPr7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/agPr7.png"" alt=""Films and TV CPU utilization""></a></p>

<p>This is the info for a sample video file.</p>

<pre><code>General
Format                                   : MPEG-4
Format profile                           : Base Media / Version 1
Codec ID                                 : mp41
File size                                : 2.61 GiB
Duration                                 : 12mn 23s
Overall bit rate mode                    : Variable
Overall bit rate                         : 30.1 Mbps

Video
ID                                       : 1
Format                                   : AVC
Format/Info                              : Advanced Video Codec
Format profile                           : High@L4.2
Format settings, CABAC                   : Yes
Format settings, ReFrames                : 1 frame
Format settings, GOP                     : M=1, N=30
Codec ID                                 : avc1
Codec ID/Info                            : Advanced Video Coding
Duration                                 : 12mn 23s
Bit rate mode                            : Variable
Bit rate                                 : 30.0 Mbps
Width                                    : 1 920 pixels
Height                                   : 1 080 pixels
Display aspect ratio                     : 16:9
Frame rate mode                          : Constant
Frame rate                               : 59.940 fps
Color space                              : YUV
Chroma subsampling                       : 4:2:0
Bit depth                                : 8 bits
Scan type                                : Progressive
Bits/(Pixel*Frame)                       : 0.241
Stream size                              : 2.60 GiB (99%)
Title                                    : GoPro AVC
Color primaries                          : BT.709
Transfer characteristics                 : BT.709
Matrix coefficients                      : BT.709

Audio
ID                                       : 2
Format                                   : AAC
Format/Info                              : Advanced Audio Codec
Format profile                           : LC
Codec ID                                 : 40
Duration                                 : 12mn 23s
Bit rate mode                            : Constant
Bit rate                                 : 128 Kbps
Channel(s)                               : 2 channels
Channel positions                        : Front: L R
Sampling rate                            : 48.0 KHz
Compression mode                         : Lossy
Stream size                              : 11.4 MiB (0%)
Title                                    : GoPro AAC
</code></pre>

<p>VLC reports the following video output in: Tools -> Messages -> Module Tree -> playlist -> video output</p>

<pre><code>window ""qt4"" (0x5a738fc)
vout display ""direct3d"" (0x5a57a34)

subpicture (0xf456b4)
spu text ""freetype"" (0x5a24264)
scale ""yuvp"" (0x5a70c1c)
scale ""swscale"" (0x55a1eac)
</code></pre>
","<p>Quite possible that the Windows player can make use of some HW acceleration the others can not / are not using. </p>

<p>Check this how to enable HW video decoding on VLC: </p>

<ul>
<li><a href=""https://wiki.videolan.org/VLC_GPU_Decoding"" rel=""nofollow noreferrer"">https://wiki.videolan.org/VLC_GPU_Decoding</a></li>
<li><a href=""https://superuser.com/q/128478/228841"">https://superuser.com/q/128478/228841</a></li>
</ul>

<p>If you are using VLC 2.2.2 check this
<a href=""https://forum.videolan.org/viewtopic.php?t=130590"" rel=""nofollow noreferrer"">https://forum.videolan.org/viewtopic.php?t=130590</a></p>

<p><strong>UPDATE</strong> from the author of the question:
Adding screenshot from VLC v. 2.2.1 to compliment this answer.</p>

<p>VLC 2.2.1 with Hardware Acceleration enabled.
<a href=""https://i.stack.imgur.com/fqQTz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fqQTz.png"" alt=""enter image description here""></a></p>
","17824"
"Nikon D3300 video auto-focus?","512","","<p>Could someone please explain to me how the auto focus works whilst videoing. Does it continuously auto focus or should i half press the shutter?</p>
","<p>Nikon D3300 supports AF-S and AF-F modes for video recording. AF-F helps you to keep focus without touching the shutter release button. <a href=""http://www.dummies.com/photography/cameras/nikon-camera/nikon-d3300-focus-modes-for-live-view/"" rel=""nofollow noreferrer"">This page</a> says that you never touch the shutter release button in AF-F mode. Just turn live view on, switch your focusing mode to AF-F, and wait until the camera focuses by itself (unlike what you do with a normal autofocus system while taking stills). Once the camera has focused, you can press the record button. Focus will be updated automatically.</p>

<p>I usually use manual focus, and tried AF-F to answer your question. It works well, except under low-light conditions (that's the reason for some issues, I think).</p>
","21774"
"Would rendering After Effects project in Premiere Pro decrease quality?","512","","<p>I have created a motion logo using After Effects. Now, I want to render it in <strong>H.264</strong> or <strong>MP4</strong> format. Now, there are <strong>3</strong> ways for it:
1. Using Media Encoder
2. Export as Premiere Pro Project in After Effects and Use it in Premiere Pro
3. Save the Project as After Effects project and Import it in Premeire Pro</p>

<p>Now, what I want to ask is about quality of the video. Would all ways yield same quality for same format? Or There's difference between them in terms of quality? If yes, what would be best?</p>

<p>My <strong>2</strong>nd question is: I'm making a short video like a Movie trailer. I would create the scenes in <strong>Pr</strong> and the Motion Logo (which I've already created in After effects) in <strong>Ae</strong>. In what way should I render them (I need <strong>MP4</strong> video)? Import the Motion Logo project in Premiere Pro and render both Logo and Video <strong>OR</strong> both Video scenes and Logo in Media encoder?</p>

<p>NOTE: I'm asking these questions in terms of quality of Motion Logo and Video scenes, not in terms of ease of usage.</p>

<p>Thank you.</p>
","<p>Importing from AE using dynamic linking and rendering in Premiere or Media Encoder will yield the same quality, but it does give you a little less control over the render settings. </p>

<p>While in AE you can choose whether to render with frame blending , motion blur, draft settings and so on, Premiere will always render your AE comp with the current settings. So when you're setting up your final render in Premiere, make sure you jump over to AE and turn on motion blur and frame blending and check that all your layers are rendering at full quality, not draft. Most of the time you <em>won't</em> want these on while you're editing because they slow down the preview so much.</p>

<p>As far as quality goes I believe that it uses the same render engine, so don't obsess about it. In fact re-encoding from an After Effects render is probably more lossy than importing using dynamic linking, because of the two compression stages. <strong>But you will never <em>ever</em> notice the difference one way or the other</strong>, trust me. Any miniscule losses due to things like quantisation errors and so on are going to pale into insignificance with quality lost in the compression to MP4. </p>

<p>Use the way that makes it much easier for you. For me that's importing via dynamic linking, allowing me to change things in the AE project without re-rendering all the time and having great big render files all over the place.</p>
","19261"
"FFmpeg encod set of time stamped images as Iframes","510","","<p>I have a set of images that are in time stamped order ( the unit is seconds ) but not in a set interval </p>

<pre><code>012.png
017.png
024.png
</code></pre>

<p>etc.</p>

<p>How can I encode them so that each image is an Iframe at that given interval ?</p>

<p>Also possibly this is not what I want to do . I'm not sure if its mandatory to have videos have their Iframes at a fixed interval ( forgive my naivety about the subject )</p>

<p>Hopefully the images would interpolate between each other smoothly ( with a feel of motion ) . I can imagine a way to do it by just creating my own in-between images by interpolating in some way between two frames ( no chance I do as good a job as encoders already do ). </p>

<p>I feel like though that this is exactly what an encoder does and am trying to figure out if I can leverage it to do this frame generation work for me by handing it a list of Iframes with timestamps . </p>

<p>If this is something that needs to be done at a lower level c++ or by working directly with an encoder ( if thats the right term ) I would certainly be willing to work on that level . I am really unsure of the approach to take . </p>

<p>-------- Additional Information </p>

<p>I took a look at</p>

<p><a href=""http://theora.org/doc/libtheora-1.0/group__encfuncs.html#gdbe7dd66b411c2d61ab8153c15308750"" rel=""nofollow"">http://theora.org/doc/libtheora-1.0/group__encfuncs.html#gdbe7dd66b411c2d61ab8153c15308750</a>
But there doesn't seem to be a way to insert a frame at a target position .</p>
","<p>There are three options here.  One is to use a variable framerate format that can play back each frame for different lengths of time, however this would result in each frame jumping from one to the next which might not keep the illusion of motion if the frame rate is too low.  This would also be the smallest file size, but I'm not aware of any easily available encoders that would let you specify the length each frame is exposed.  (Granted, I haven't looked seriously for one.)</p>

<p>The second option is to simply place each frame on a timeline at the appropriate time in an editor and leave it up until the next frame is reached.  This can then use a traditional encoder, however it still requires a fair bit of manual work to layout the frames and results in a larger file than the first option.</p>

<p>The final option is to use interpolation software to interpolate objects out of the frame and generate between frames to smooth the motion.  This is by far the most complicated, but would also produce the smoothest results if done well.  If the gap is too wide, you will probably need to manually define objects in the scene with advanced interpolation software and have it generate the in between frames.  </p>

<p>Note that, as Jim Mack said, this is NOT something an encoder does.  An encoder only encodes the frames it is given, it makes no attempt to interpolate.  The closest to interpolation that an encoder might come is adjusting framerate on the fly (typically using a fixed conversion) or applying a de-interlace filter, but they do not offer a complex or particularly high quality interpolation solution.  Certainly not enough to handle what you are talking about.</p>
","12035"
"Convert mkv to mp4 (or webM) with the output file viewable in the process","508","","<p>I'm trying to find a way to convert a MKV file into MP4 and actually already start to view it, while it's still converting.
The output file would be viewed via Chrome or Firefox and I would like to use ffmpeg or avconv (or similar available on a headless server) to convert.</p>

<p>Any advice? Maybe even on recommended settings?</p>

<p>*edit: if it is quality-wise possible, webM could also be an option</p>
","<p>This is possible for MP4, with a caveat. The command below will generate a fragmented MP4, which you can view in a browser while the conversion is taking place. However, only the fragments completely encoded at the time of launching the file, will be viewable. To view fragments encoded after that point, you'll have to reload the file/page.</p>

<pre><code>ffmpeg -i input &lt;encoding parameters&gt; -movflags +frag_keyframe+separate_moof+omit_tfhd_offset+empty_moov out.mp4
</code></pre>
","18179"
"Intersecting animated masks","506","","<p>As a quick demonstration, here's an illustration;</p>

<p><img src=""https://i.stack.imgur.com/cCGWr.jpg"" alt=""Intersecting masks""></p>

<p>So I have two shapes (2 and 3), which intersect (4). I want to animate the position (possibly also scale) of these two shapes, while having footage masked inside each numbered area. The shapes should move independently of the footage.</p>

<p>I've gotten as far as masking the footage for 2 and 3, but the intersection is what makes me scratch my head. Is there a way to do this in After Effects CC without any hand-tooling for a mask for the intersecting area?</p>
","<p>Simply copy the two animated masks to the 4th footage file and set them to ""Intersect"" instead of ""Add"". Then they will only reveal the footage that they are applied to when they intersect with another mask. Then put this footage files layer ontop of the others.</p>

<p>Here a screenshot as a reference.
<img src=""https://i.stack.imgur.com/4FXJe.png"" alt=""enter image description here""></p>
","10628"
"Methods of Amplifying Poor Audio from Camcorder Recording?","505","","<p>I have a recording from a camcorder (with a broken mic) where the audio is 1) largely white noise and 2) almost in-audible sound from people chatting in a room.</p>

<p>What methods could be used to amplify the ""almost in-audible"" parts (and remove the white noise)?</p>

<p>I guess what consumer level audio tools could do this?</p>
","<p>From your description of the audio, it is going to be hard to fix, as white noise tends to cover a very broad frequency spectrum and so is hard to get rid of without losing what you want to keep.
A decent starting point for you would be to use a freeware audio editing program like <a href=""http://audacity.sourceforge.net/"" rel=""noreferrer"">Audacity</a>. Here is a <a href=""http://wiki.audacityteam.org/index.php?title=Noise_Removal"" rel=""noreferrer"">noise removal tutorial using Audacity</a>.</p>

<p>You can also use audacity to cut out sections where there is no background talking. Then you can use an effect called a <strong>compressor</strong> to attempt to raise the level of the speaking to a better level, although you may still find you are amplifying some background noise. Here's a description of the <a href=""http://wiki.audacityteam.org/wiki/Compressor"" rel=""noreferrer"">audacity compressor</a>.</p>
","1284"
"When is it necessary to enable ""Time Remap"" in After Effects?","505","","<p>In After Effects to speed up and slow down footage <a href=""http://www.youtube.com/watch?v=4feQ1HylGC0&amp;feature=share"" rel=""nofollow"" title=""Slow Motion in After Effects"">this tutorial</a> shows that you first enable <code>Time Remap</code> then use <code>Timewarp</code>. He doesn't appear to do anything with the Time Remap though.</p>

<p><a href=""http://www.lynda.com/After-Effects-tutorials/Slowing-accelerating-video-speed/59957/64216-4.html"" rel=""nofollow"" title=""Slowing and accelerating video speed in After Effects"">This other tutorial</a> shows doing slow motion either with the <code>Stretch</code> or with <code>Timewarp</code></p>

<p>I get the <code>Stretch</code> vs <code>Timewarp</code> a bit but don't understand the <code>Time Remap</code>. When, if ever, should I be toggling that? From this <a href=""http://library.creativecow.net/articles/preston_bryan/time_remapping.php"" rel=""nofollow"" title=""Time Remapping in After Effects"">nice written tutorial on time remapping</a> it seems that <code>Time Remapping</code> is more useful if you need to reverse playback or add still frames and such. So was there a reason the <code>Time Remap</code> was toggled before the first guy did <code>Timewarp</code>? Does it make it render faster or work better or was it completely unnecessary?</p>
","<p>If you are using Time Remap, a Timewarp should not be needed and vice versa.  Timewarp is designed to allow adjusting the playback rate, where as Time Remap is designed to allow you to tell the playback to be at a particular point in the original timecode when it reaches a particular time in your composition.  It mostly just matters how you want to control the adjustments and what level of control you want over the processing, but only one is needed.</p>
","9865"
"What are some good accessories for a DSLR video recording setup?","504","","<p>I have a DSLR that I've started using for video recording and I'd like to buy some accessories for shooting video. I know this could be subjective, but it doesn't have to be. I am looking for accessories for a DSLR that help with video recording, such as a follow focus, or shotgun mic. What are some important accessories for recording with a DSLR?</p>
","<p>I'm biased, but before anything else, invest in AUDIO GEAR. Far too often, filmmakers forget to keep audio on an equal par with picture. Invest in some good, quality mics. Shotgun? Meh...a good, all purpose cardioid like the reasonably-priced <a href=""http://www.coutant.org/evre10/index.html"">ElectroVoice RE-10</a> can hold you over for a while until you get some audio recording practice under your belt. </p>

<p>Essential to MY toolkit is an <a href=""http://www.zoom.co.jp/english/products/h4n/"">H4N</a>...an excellent digital audio recorder. It's small enough to hide on set, and I'll be honest...I've used it hidden behind a centerpiece to record a conversation at a table with no problems. They're stupid cheap, too...like...um...$299? I forget offhand. </p>

<p>The microphones in DSLRs are pretty much all crap from my experience; I use them more as a reference track than anything else. </p>

<p>Good audio will make your images sparkle even more!  </p>
","3559"
"Premiere Pro CC 2017 - Displacement map","503","","<p>Can't find it:<br>
<a href=""https://i.stack.imgur.com/10PO5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/10PO5.png"" alt=""enter image description here""></a></p>

<p>Is it a plugin I have to import?<br>
Thanks for your time.</p>
","<p>The displacement map effect relies on another layer as a source for the map. Since Premiere doesn't have unique layers like AE does it doesn't support this plugin. If you have After Effects then replace the shot with an after effects comp (right-click then choose Replace With After Effects Composition) and apply the effect in AE.</p>
","21020"
"Which different EDL types exist?","503","","<p>Edit decision lists are integrated in every pipeline nowadays. I've seen different types of it, but <a href=""http://en.wikipedia.org/wiki/Edit_decision_list"" rel=""nofollow"">wikipedia</a> only knows CMX 3600 EDL and a xml modification. </p>

<p>Which other formats exist and what are the differences of it? Is there a standard type which supported by all editing systems? Which open type has most features?</p>
","<p>CMX 3600 (or earlier 340) is very common and understood by many systems. GVG (Grass Valley Group) format is almost as common. For simple cuts, dissolves and keys the two are highly compatible, but for more complex edits, including speed changes, there are significant differences. Neither deals well with multiple layers or multiple sources in one event.</p>

<p>Many other companies use proprietary formats that are often only slightly different from basic CMX/GVG. Things like the size and contents of the source ID, the use of notes for special effects, and so on, may change from version to version as well.</p>

<p>To list the specifications would take a very long post. If you have specific questions about what each can handle, try asking here. But in general, for interchange you'll probably find XML-based EDLs to be most flexible, and CMX/GVG will be understood by the most different systems, within their limitations.</p>

<p>Google ""EDL XML AAF"" for a lot more information on the topic.</p>
","14761"
"Corrupt images, when capturing from multiple cameras using the V4L2 API","501","","<p>I would like to grab images from multiple cameras under linux. I followed <a href=""http://linuxtv.org/downloads/presentations/summit_jun_2010/20100206-fosdem.pdf"" rel=""nofollow noreferrer"">this presentation</a> used the code and enhanced it for my purpose. It works very well for 1 camera.</p>

<p><img src=""https://i.stack.imgur.com/ORkXX.png"" alt=""Saved image from the carema using v4l2 and libpng"">
Once I begin to grab images from multiple cameras (successively) I get corrupt images.
<img src=""https://i.stack.imgur.com/u4DCQ.png"" alt=""Corrupt image when grabbing picture from multiple cameras"">
The more cameras I sue the more artefacts/stripes in the images appear. It does not make any difference if I save the images as BMP using another code. So I assume the problem has nothing to do with the storing routine. The resolution is also right (744 * 480).</p>

<p>The result is the same on two different computers running Fedora and Debian. I am absolutely baffled and can not find any clue what is going wrong. Could me please someone give some hints ?</p>

<p>Here is my code</p>

<pre><code>int main()
{
    /* #################### INIT #################### */

    int numOfCameras = 1;
    int xRes = 744;
    int yRes = 480;
    int exposure = 2000;
    unsigned int timeBetweenSnapshots = 2; // in sec
    char fileName[sizeof ""./output/image 000 from camera 0.PNG""];

    static const char *devices[] = { ""/dev/video0"", ""/dev/video1"", ""/dev/video2"", ""/dev/video3"", ""/dev/video4"", ""/dev/video5"", ""/dev/video6"", ""/dev/video7"" };

    struct v4l2_capability cap[8];
    struct v4l2_control control[8];
    struct v4l2_format format[8];
    struct v4l2_requestbuffers req[8];
    struct v4l2_buffer buffer[8];

    int type = V4L2_BUF_TYPE_VIDEO_CAPTURE; // had to declare the type here because of the loop

    unsigned int i;
    unsigned int j;
    unsigned int k;

    int fd[8];
    void **mem[8];

    /* #################### OPEN DEVICE #################### */

    for (j = 0; j &lt; numOfCameras; ++j) {

        fd[j] = open(devices[j], O_RDWR);
        ioctl(fd[j], VIDIOC_QUERYCAP, &amp;cap[j]);


        /* #################### CAM CONTROLL #################### */

        control[j].id = V4L2_CID_EXPOSURE_AUTO;
        control[j].value = V4L2_EXPOSURE_SHUTTER_PRIORITY;
        ioctl(fd[j], VIDIOC_S_CTRL, &amp;control[j]);

        control[j].id = V4L2_CID_EXPOSURE_ABSOLUTE;
        control[j].value = exposure;
        ioctl(fd[j], VIDIOC_S_CTRL, &amp;control[j]);

        /* #################### FORMAT #################### */

        ioctl(fd[j], VIDIOC_G_FMT, &amp;format[j]);
        format[j].type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
        format[j].fmt.pix.width = xRes;
        format[j].fmt.pix.height = yRes;
        //format.fmt.pix.pixelformat = V4L2_PIX_FMT_YUYV;
        format[j].fmt.pix.pixelformat = V4L2_PIX_FMT_GREY;
        ioctl(fd[j], VIDIOC_S_FMT, &amp;format[j]);

        /* #################### REQ BUF #################### */

        req[j].type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
        req[j].count = 4;
        req[j].memory = V4L2_MEMORY_MMAP;
        ioctl(fd[j], VIDIOC_REQBUFS, &amp;req[j]);
        mem[j] = malloc(req[j].count * sizeof(*mem));

        /* #################### MMAP #################### */

        for (i = 0; i &lt; req[j].count; ++i) {
            buffer[j].type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
            buffer[j].memory = V4L2_MEMORY_MMAP;
            buffer[j].index = i;
            ioctl(fd[j], VIDIOC_QUERYBUF, &amp;buffer[j]);
            mem[j][i] = mmap(0, buffer[j].length,
                    PROT_READ|PROT_WRITE,
                    MAP_SHARED, fd[j], buffer[j].m.offset);
        }

        /* #################### CREATE QUEUE #################### */

        for (i = 0; i &lt; req[j].count; ++i) {
            buffer[j].type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
            buffer[j].memory = V4L2_MEMORY_MMAP;
            buffer[j].index = i;
            ioctl(fd[j], VIDIOC_QBUF, &amp;buffer[j]);
        }

    } /* ### ### end of camera init ### ### */

    /* ##################### STREAM ON #################### */
    for (j = 0; j &lt; numOfCameras; ++j) {

        ioctl(fd[j], VIDIOC_STREAMON, &amp;type);
    }


    /* ##################### GET FRAME ##################### */

    k = 0;
    while (!kbhit()){
        k ++;

        for (j = 0; j &lt; numOfCameras; j++) {

            buffer[j].type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
            buffer[j].memory = V4L2_MEMORY_MMAP;
            usleep(100000);
            ioctl(fd[j], VIDIOC_DQBUF, &amp;buffer[j]);

            // create filename
            sprintf(fileName, ""./output/image %03d from camera %d.PNG"", k, j);
            // save as PNG file
            saveToPng(mem[j][buffer[j].index], fileName, xRes, yRes);

            ioctl(fd[j], VIDIOC_QBUF, &amp;buffer[j]);

            sleep(timeBetweenSnapshots);
        }
    }

    /* ##################### STREAM OFF ##################### */
    for (j = 0; j &lt; numOfCameras; ++j) {

        ioctl(fd[j], VIDIOC_STREAMOFF, &amp;type);
    }

    /* ##################### CLEANUP ##################### */

    for (j = 0; j &lt; numOfCameras; ++j) {

        close(fd[j]);
        free(mem[j]);
    }

    return (0);
}
</code></pre>
","<p>Your issue is probably that you don't have enough USB bandwidth available, if your webcams support it switch to MJPEG instead of uncompressed frames. Usually any webcam supports MJPEG encoding to deliver frames to your PC.</p>

<p>Here a similar question on SO <a href=""https://stackoverflow.com/questions/9781770/capturing-multiple-webcams-uvcvideo-with-opencv-on-linux"">https://stackoverflow.com/questions/9781770/capturing-multiple-webcams-uvcvideo-with-opencv-on-linux</a></p>
","12534"
"Is Open-Source -film production possible with open-source tools?","501","","<p>Samuli -- of the Iron-Sky -film -- mentions (3.40 <a href=""http://www.youtube.com/watch?v=czpYwqV22p4"" rel=""nofollow"">here</a>) their used software:</p>

<ol>
<li>adobe.com, Adobe Creative Suite 5</li>
<li>autodesk.com, Maya</li>
<li>newtek.com, Lightwave 3D 10</li>
<li>renderpal.com, RenderPal</li>
<li>thefoundry.co.uk, Nuke</li>
</ol>

<p>Now I am total newbie here but I would like to know whether it is possible to replace the software with some open-source alternatives? Look they cost a lot of capital to even trial so trying to find some good open-source alternatives. It would be cool to engage into project which does not require massive capital boundary to start from the participants. Suggestions highly appreciated!</p>
","<p>To make your vector artwork into a playable movie file, it will have to be rendered, and another term for rendering is rasterizing.  For example, in printing, a single image is rasterized when a vector image of a page is converted to the tiny dots the printer produces on the page. </p>

<p>Digital movies exist in pixel-based raster formats, so each frame of the vector animation must be converted into a pixel-based representation of that frame, for each frame in the movie. Most software today uses the word render. Rendering will also apply some form of video compression to the resulting movie file, you'll want to experiment with different <em>codecs</em> to see what kind of render quality they offer.</p>

<p>You are trying to identify a 'workflow', a path you can take to make your project, that uses open source software.  The programs that have been brought up are very good, and could be the tools for the whole workflow, for many projects. You may also need some kind of video editing software. I'm not sure if there's a good open source video editor, or if Blender allows you to edit. </p>

<p>You should proceed by figuring out your production workflow, and looking for tools that can fulfill that workflow. </p>

<p>I would like to suggest Celtx as an excellent open source script writing tool.</p>
","7844"
"Color correction workflow","500","","<p>I completed my first short film in December. One of the most time consuming aspects of the project was making the colors look nice. I used Adobe Premiere Pro CS5.5 and the free Magic Bullet plugin. I learned a lot. I now realise that color correction and grading is more of an art than an established set of rules.</p>

<p><strong>Could an expereinced colorist provide some practical tips about dealing with color correction and grading over the scope of the entire project.</strong></p>

<ol>
<li>Should I wait until I have a rough cut until I start correcting?</li>
<li>Should I correct individual clips on the timeline, or correct the original source?</li>
<li>Should correction (of exposure etc.) and grading (achieving a ""look"") be done at separate stages?</li>
<li>What kind of automated colour correction tools are there?</li>
<li>Are there things I can do during production that would help with the color correction (i.e. using a target with color squares printed on it <a href=""http://rads.stackoverflow.com/amzn/click/B0000ALKEJ"">like this</a>)?</li>
</ol>
","<p>I am not an experienced colorist but I believe I can provide you with some answers. At least from an indie filmmaker's point of view.</p>

<ol>
<li><p>Wait until you have the final cut (or lock-off cut) before you grade. Grading usually happens at the same time as the sound design as both require a locked-off cut. It is best to wait until this stage so that you don't have to go back and grade new shots if the director decides he/she wants to make a change.</p></li>
<li><p>Correct individual clips on the timeline. You said you grade in Premiere using a plugin. That's cool. But there are other options. I always went from Final Cut to Apple Color but have recently discovered <a href=""http://www.blackmagic-design.com/"">DaVinci Resolve</a>. The Lite version is free. <a href=""http://www.taoofcolor.com/1066/davinci-resolve-8-1-1-lite-not/"">Here</a> is a bit of info on it.
Grading clip by clip is the best way to go because you can see what the clip before looked like and so you know what the next one needs to look like, i.e. it's easier to reference. 
You don't want to color grade the original video incase you decide you want to change something later. I believe doing something like that is called destructive editing. You want to keep it raw so that you can always come back to it. </p></li>
<li><p>In my experience, no, it happens at the same time. </p></li>
<li><p>Not too sure about this one. Avid Media Composer has pretty good color grading capabilities, I never use the auto correct on it, but it's there if you need it. Check out <a href=""http://www.youtube.com/watch?v=bIb8QtZ0FfU"">this</a> tutorial on grading in Avid. Other editing programs may have similar features, but I personally don't use them. And if you're editing a film, I don't see why you would want to.</p></li>
<li><p>Yes there are things you can do to help with grading in post. That color chart you linked to wouldn't really help you. I think that's more for when people shoot on film. They shoot 10 seconds or so of color chart on every new reel of film to make sure that the people who are processing their film are consistent. So, they have the same color chart and the same film. Provided they shoot it with the same lighting and exposure, the color charts should all look exactly the same when they come back from processing and telecine. If they don't the film processing company can get into trouble. 
During production you pretty much want to make sure you get the flattest image possible and correct exposure. If you under or overexpose a part of the scene, there is no detail recorded and there is no way to get it back. If you shoot on Canon DSLR there is a great (free) picture profile which helps to obtain a flat image optimized for color grading. It's called <a href=""http://www.technicolor.com/en/hi/cinema/filmmaking/digital-printer-lights/cinestyle"">CineStyle</a> by Technicolor. Otherwise you can shoot with the neutral picture profile with the contrast turned all the way down. 
I only have experience with Canon DSLRs so if you are using something else you would need to do some research on how to achieve a flat image. </p></li>
</ol>
","2807"
"JVC Everio GZ-MS90E aspect ratio is 5:4 instead of 16:9","499","","<p>I am working with an JVC Everio GZ-MS90E which is configured to produce a 16:9 video. But my aspect ratio is 5:4 instead of 16:9.</p>

<p>Width: 720
Height: 576</p>

<p>Width/Height = aspect ratio
720/576 = 1.25 = 5:4</p>

<p>Does anybody have similar problems or probably a solution how to fix that?</p>

<p>I am really annoyed because I don't know the right preset for my Adobe Premiere Pro CS5.5 sequence. I have already some clips and I would like to use them without transcoding.</p>
","<p>Now I found the right way for my footage. It was simple: Just right click on the footage, and select <strong>ndern  Filmmaterial interpretieren</strong>. This is probably <strong>Change  Footage</strong> or something like that.</p>

<p>Then I fored premiere to use the aspect ratio <strong>D1/DV PAL Widescreen 16:9</strong> and everything is fine.</p>
","4448"
"Add black (or other color) padding when doing vstack (or hstack)","497","","<p>I am using the following <code>filter_complex</code> to put 2 videos of resolution <code>320 x 240</code>, one right next to the other:</p>

<pre><code>-filter_complex ""[v1][v2]hstack=inputs=2[videoout]""
</code></pre>

<p>This works great, but now I was wondering how could I do it, so that I could add a small padding between them. Let's say for instance 5 black pixels (even though I would love to be able to customize the color or at least the distance).</p>

<p>How could I do that?</p>
","<p>First, setup a color filter of the same height, and then add it to the hstack</p>

<pre><code>-filter_complex ""color=black:5x240[c];[v1][c][v2]hstack=inputs=3:shortest=1[videoout]""
</code></pre>

<p>See the options for the color filter <a href=""https://ffmpeg.org/ffmpeg-utils.html#Color"" rel=""nofollow"">here</a>.</p>

<hr>

<p>Alternate method is</p>

<pre><code>-filter_complex ""[0]pad=iw+5:ih:color=black[0v];[0v][v2]hstack[videoout]""
</code></pre>
","18420"
"Can an expression parse a text file or the expression of another property in After Effects?","496","","<p>I am going to have tons of layers use the same expression, but I need to be able to edit the expression later without having to edit it for every layer. I also can't simply have each layer's property point to the original, because the expression needs to be evaluated for each layer relative to itself.</p>

<p>Is there a way I can have each layer expression simply point to and parse a text file? I could edit the text file so all layers expressions update.
Or, if somehow I could put the expression on a null object and have each layer parse that expression for itself.
Thanks for any help!</p>

<p>My workaround solution is to make a null object with a slider control for every single value that I may edit later, updating all layers, but this doesn't allow me to change the non-numerical information in the expression, such as adding parenthesis or other functions.</p>
","<p>You can't put the expression on a null object, but you can use a text layer.</p>

<p>Following @bobtiki's <code>eval()</code> tip I tried using the source text of a text layer and running <code>eval()</code> on it. Initially I had no success, I was getting an <strong>""object of type TextProperty found where a Number, Array or Property is needed""</strong> error. But if I put something like a number on its own in the text layer and treated the source text like an array I <em>could</em> get it, I just wasn't able to access a whole string.</p>

<p>I was about to give up, then I remembered from doing extendscript scripting that you get the raw value of an AE property object using the javascript property <code>.value</code>*.</p>

<p>So after a bit of fiddling around with quotes and escaping (neither are needed, thank heavens) I got it to work. </p>

<p><strong>TL;DR: Here is how you reference an expression that is written in a text layer's source</strong>, without having to save external files:</p>

<pre><code>var theExpression = thisComp.layer(""your text layer"").text.sourceText.value;
eval(theExpression);
</code></pre>

<p>And then you write the expression into the text layer, in this case called ""your text layer"". This is really cool! It even updates in real time <strong>as you're typing the expression</strong>!</p>

<p>Here's a comp with the position of the cyan layers being driven by the expression you see on the text layer. If I duplicate one of the cyan layers its position is calculated individually based on the random seed <code>index</code>, and if I change the expression, they all update!</p>

<p><a href=""https://i.stack.imgur.com/MiUZY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MiUZY.png"" alt=""enter image description here""></a>
I'm stoked, this was the best question ever!</p>

<p>* <em>this sentence sounds like gibberish I know. The problem is that AE layers' property's values are JavaScript objects, and as such they have JavaScript properties, and one of the AE property's value's JavaScript properties is the <code>value</code> property which represents the raw value of the AE property. And the value of the AE source text property is an object which must include formatting and whatnot, and so it is different to the text.sourceText.value value. Confused?</em></p>
","18949"
"Loss of quality in after effects 3D layer","493","","<p>I have a 2D image layer that looks great as a 2D layer. Both <code>.png</code> and <code>.eps</code> files are (near) pixel perfect. However when I convert the layer to 3D, there is a massive loss in quality. Is there a setting somewhere that can improve the quality of the video?</p>

<p><img src=""https://i.stack.imgur.com/T9olF.png"" height=""500""/></p>

<p>2D</p>

<p><img src=""https://i.stack.imgur.com/Hxr8Y.png"" height=""500""/></p>

<p>3D</p>
","<p>Unless you need it make sure you are using the Classic 3D rendering mode and not the ""Ray-Traced 3D"" renderer. You can see and change the renderer in the top right corner of your comp view or just go into the composition settings menu through the top menu bar.</p>

<p>If you are in need of using the RayTraced 3D renderer click on the options menu and increase the quality from 3 (standard) to a higher value like 6 or 8 and see if that fixes your issue.
You may also change your AntiAliasing mdoe to cubic instead of box.</p>

<p>Also make sure your preview quality is not set to fast draft but to adaptive or final quality.</p>

<p><img src=""https://i.stack.imgur.com/vQ2AU.png"" alt=""enter image description here""></p>
","12453"
"How to extract frames and saving frame type to filenames with ffmpeg?","492","","<p>I have a research project where I need to apply very, very computationally heavy image filters on a h264 video. Due some restrictions outside my control I have to use proprietary filter that works only on images and not video. Filter has two modes, A mode is very computationally heavy but produces production ready quality and B mode is fast but produces less than stellar quality.</p>

<p>My theory is that given how h264 uses I-frames to get the reference and rest of the frames are ""non-essential"", if I can find apply the high quality A mode on I-frames and B mode on other frames, I should be able to leverage h264's own compression to get the result without having to use filter on A mode to every frame. Another avenue would be to try with h265 because it's more robust frame prediction capabilities.</p>

<p>What I've figured out so far is that I need to find a way to extract all frames from a video while knowing which ones are I-frames, then do a filter on all the frames, and then re-encode the video back together with minimum quality loss.</p>

<p><strong>So first question is that how can I extract frames with ffmpeg so that it somehow identifies each frame type on the filename?</strong></p>

<pre><code>ffmpeg -i C:\test.mp4 -vf select='eq(pict_type\,I)',setpts='N/(25*TB)' C:\testTemp\%09d.jpg
</code></pre>

<p>Above gives you i-frames but I haven't figured out how I can get this to export all the frames and to put the type on the filename?</p>

<p><strong>Second question is that how can I re-encode the video so that it keeps the I-frame positions?</strong></p>

<pre><code>ffmpeg -framerate 24 -i frame-%03d.png output.mp4
</code></pre>

<p>Above would make the video out of frames but the I-frame positioning eludes me.</p>

<p>I know this is a complex problem so all help is truly appreciated!</p>
","<p>Can't comment on the soundness of your theory, but here's a way to do it. Not directly, but in a roundabout way.</p>

<p>FFmpeg's segment muxer can break up a file at GOP boundaries, so running the command below generates a set of videos which each start with a keyframe but contain no other keyframes.</p>

<pre><code>ffmpeg -i test.mp4 -c copy -f segment -segment_time 0.001 -segment_list list.ffcat -reset_timestamps 1 segs%d.mp4
</code></pre>

<p>This will create</p>

<pre><code>segs0.mp4
segs1.mp4
...
segs101.mp4
..etc
</code></pre>

<p>Extract each segment to images</p>

<pre><code>ffmpeg -i segN.mp4 outN-%d.png
</code></pre>

<p>Apply A on first frame of each image series and B on the rest.</p>

<p>Then for each processed series, run</p>

<pre><code>ffmpeg -framerate X -i procN-%d.png -c:v libx264 -keyint_min 1000 -x264opts stitchable ""result\segsN.mp4""
</code></pre>

<p><code>-keyint_min</code> value should be greater than no. of frames in each series.</p>

<p>Hopefully the names of the resulting MP4s are the same as the original split segments, which makes the next step more convenient.</p>

<p>Copy the <code>list.ffcat</code> created in the initial segmenting command to the results folder and run</p>

<pre><code>ffmpeg -f concat -i list.ffcat -c copy processed.mp4
</code></pre>
","21301"
"Compressor 4.1: Why can't I select the group I made for distributed processing?","490","","<p>I'm currently trying to get distributed processing to work using two different computers on the same network running the same versions of osx, fcpx, and compressor. When I go to choose the group I made from the ""process on:"" dropdown, it will not let me choose the group that I made. I'm trying to export to compressor directly from FCPX, and that works, but it would be much faster if I could farm out the videos from this computer to my other mac. Does anyone have any idea why I can't select my group?</p>
","<p>Compressor 4.1 does not support distributed renderings for other programs (such as FCPX).  This is documented on page 11 of the <a href=""http://images.apple.com/final-cut-pro/docs/Transitioning_to_Compressor.pdf"" rel=""nofollow"">Transitioning To Compressor document</a>.  You can only do distributed encoding for transcodes defined entirely within Compressor, not renders.</p>
","10162"
"What is the best way to plan and sync voiceover?","490","","<p>I'm working on an informational video which has some live action product shots, animation created in flash, a backgroud music track, and a voiceover track.</p>

<p>I've got total control over everything so I can edit the animation or re-record the voiceover (I do the voiceover myself). I just read <a href=""http://www.directvoices.com/plaza/syncing-voice-over-to-video/"">http://www.directvoices.com/plaza/syncing-voice-over-to-video/</a> for example but it doesn't really go into specifics of how to do the planning or the fine tuning.</p>

<p>I'm trying to figure out what the best methodology is for planning and executing this. This is what I currently do but it feels very clunky to say the least:</p>

<ul>
<li>Record Audio Track</li>
<li>Make animation in Flash with no Audio</li>
<li>Bring into compositing software (I use Premier CC2015) and do my best to trim and move tracks around. At times adding still frames to extend parts of the animation.</li>
<li>If I have to then I go back into Flash to make more animation to fill empty areas. I try not to have to redo the audio as much since its much more time consuming and would then have to start the entire syncing process over as well since talking isn't as easy as controlling a timeline.</li>
</ul>

<p>There's got to be a better way to do this though, I'm just not sure what it is.</p>

<p>Should I just do the entire video and then play it while I record the voiceover? That would work for parts where the audio is short and the animation long. But is more difficult on parts where the audio is long and the animation is short.</p>
","<p>In addition to @Mulvya answer I will suggest to use not Flash, but After Effects for animation, if possible. Then you will be able to make <a href=""https://helpx.adobe.com/premiere-pro/how-to/integration-after-effects.html"" rel=""nofollow"">Dynamic Link</a> to your AE project. Than will be easier to jump between applications.</p>

<p>But when you work with animation, you deferentially need to have voiceover first. Like in cartoon development.</p>
","16429"
"Convert resolution 4096x2304 to 3840x2160 keeping the quality the same","488","","<p>I have recently come into possession of a couple of 4k movies with a resolution of 4096x2304. Unfortunately my Samsung 4k TV wont play anything over 3840x2160.</p>

<p>I have managed to re-encode the videos to 3840x2160, unfortunately the bitrate fell to 1/10th of what it was before.</p>

<p>I used:</p>

<pre><code>for %%a in (""*.mkv"") do ffmpeg -i ""%%a"" -s 3840x2610 -q:v 0 -c:v libx264 -acodec copy D:\%%~na.mp4
</code></pre>

<p>Original file size was 27.8GB, the re-encoded one was under 8GB.</p>

<p>Is there any way I can re-encode them so I can keep the original bitrate/quality?</p>

<blockquote>
  <p>Rant: Even my 4 year old computer can play the movie, but that fcking
  new Samsung TV of course can't /rant-end</p>
</blockquote>
","<p>Judge based on the quality rather than the bitrate value. Use CRF mode encoding and if the quality isn't what you can accept, decrease the CRF value.</p>

<pre><code>ffmpeg -i ""%%a"" -s 3840x2610 -c:v libx264 -crf 18 -acodec copy D:\%%~na.mp4
</code></pre>
","16752"
"How can I create Motion titles with fixed-length transitions?","488","","<p>I'm using Motion to create text overlays for a video that I'm editing in Final Cut Pro X. Each time an overlay appears on the video, it uses a transition to slide into view and to slide out again when finished. I've created a custom title in Motion that behaves as follows: the text is visible for 7 seconds, with a transition lasting 10 frames at the start and end.</p>

<p>Here's my problem: sometimes I want the text to be visible for a different duration. If I add my custom title to the FCPX timeline, then change the duration of the title from 7 seconds to 14 seconds, the transitions take twice as long to complete. It looks silly! Is there any way that I can make a title whose start/end animation takes place over a fixed duration, but where the total duration of the title can be customized freely?</p>
","<p>You can use <a href=""https://help.apple.com/motion/mac/5.0/en/motion/usermanual/index.html#chapter=11%26section=17%26tasks=true"" rel=""nofollow""><em>Build-In</em> and <em>Build-Out</em> markers</a>. If you right click (or control-click) on the top of the timeline, you'll get a pop-up menu with an option to add a marker. When you add it, you'll get a green timeline marker. Double-click it to edit it and choose whether it's a mandatory or optional build-in marker. Everything before that should play at the same speed in FCPX as it does in Motion. The area between the build-in and build-out markers will stretch to fit, and everything after the build-out marker will play as it does in Motion.</p>
","14648"
"What equipment do I need to be able to produce 'HD quality' videos?","483","","<p>For starters, please forgive me if this question is very amateurish - if they sound that way, that's because I am an amateur.</p>

<p>I would like to start shooting some webisodes in my garage and want to know what equipment I will need.</p>

<p>The quality would be the equivalent to that of any of Revision3's shows - or more specifically <a href=""http://www.totallyradshow.com"">TRS</a>. So nothing TOO high quality, but decent-ish quality.</p>

<p>I have a Canon 7D, and would like another camera for a 2nd shot. My budget is about $4Kish for everything. The 2nd camera, lighting, audio, etc.</p>

<p>Someone suggested a t3i, or t4i for my second camera - what do you guys think about that?</p>

<p>Also, about 60% of the shots I do will be in this garage, but I also want to be mobile and be able to setup shots elsewhere - so keep that in mind.</p>

<p>Can you recommend lighting (both individual pieces and entire kits) and audio solutions that I might want to look at?</p>

<p>I was thinking of getting stuff on B&amp;H, but the choices just seem overwhelming and I know it's easy to spend a ton of money on stuff that I don't ""need"".</p>

<p>So I would love any feedback on how I can get a ""decent-ish"" studio setup for a few $K.</p>
","<p>With your budget, I would suggest investing in a few basic lights and decent audio recording equipment. </p>

<p>While a second camera is nice to have, creating production value with the non-picture parts of your films will make them a lot better. If you need to get second or third angle, just move the camera for a closeup and re-run the scene. Sure, it may take longer to shoot, but from my experience, you'll have better selection in the edit suite than if you're just cutting a bunch of suimultaneously rolling cameras together. </p>

<p>There's also the issue of matching. The 7D is so spectacular, that I'd fear mixing in something that's not comparable (and likely over your budget) will be very visible to the viewer. </p>

<p>Lighting: Learn basic three point lighting. Rather than buying a kit new, check eBay for Lowel lights. They're relatively well made (using metal!) and still very economical. The ProLight, VLight and Rifa lights are good to start a digital video lighting package with. I've put together kits designed to my specs buying individual pieces off of eBay, and spent half the cost of a comparable kit new. I would say a good place to start is 2-3 ProLights and a Rifa Light, <em>maybe</em> a vLight. The latter is good for just makign something bright, but isn't terribly good at adding modeling to a scene. It may not be appropriate for your garage shoot. I usually use it to uplight a tree in night shots jsut to add some texture.</p>

<p>Accessories for lighting: stands, dimmer switches and a roll of blackwrap. Gels are useful once you start shooting outside...you'll need to balance the lights to match the sun. Read about Color Balance and Color temperature if you're unfamiliar with the concept. </p>

<p>Audio: Invest in a good shotgun mic and a boom pole. I <strong>cannot</strong> stress enough the importance of good audio. If your picture is mediocre, awesome sound can save it, but no matter how awesome your picture is, bad sound makes it unwatchable. Play around with the mic to learn how its pickup pattern works and to find the sweet spot for placement. If you need to record sounds effects, ambient sounds, or record dual system, pick of an H4N. It's one of the best investments I made into my audio kit, and I use it on <strong>every</strong> shoot.</p>

<p>A solid, economical mic is the Audio Technica AT-435B. Sennheiser is also a good brand, but more expensive. I forget my model numbers off the top of my head; perhaps someone can fill in? </p>

<p>In short, a second camera is not as necessary as lighting and audio. The production value that you can add with those things far outweighs the convenience of adding a second camera. If you <strong>do</strong> insist on a second camera, look at the GoPro. I work on a lot of network shows that just place one of these to get b-roll...for cooking shows, it'll just monitor the cooktop while the rest of the crew runs around the restaurant. Quality is good enough to cut ionto broadcast shows, but the size of the chip limits its contrast and color reproduction...so there is a marked quality difference between that and the 7D. That being said, perhaps killer audio and lighting will compensate. Good luck! </p>
","4401"
"In what aspect ratio shoud one record with a DSLR?","481","","<p>This Question is not about what aspect ratio should be chosen for the final film, it's rather about how to get the biggest ""resolution"" capable of the cammera.</p>

<p>When you take a photo with a Modern camera ""usually"" it's 4/3, I know you can change the aspect ratio, but that's not my point.
I assume, that the sensors of DSLR cameras are the same ratio, because they are created for the purpose of creating photos, aren't they?</p>

<p>I guess focal lenght can be taken in account as well.</p>

<p>Now when you shoot Video in 16:9, I assume, that the image is ""croped"", because you can't change the size and ratio of your sensor.</p>

<p>So the only way, to have that aspect ratio, is to leave it blank or record on the top/bottom of the image (or both) a black bar.</p>

<p>Wouldn't it be smarter, to record with the fullsensor, to get ""more"" of the image""?</p>

<p>This might sound redicolous, because you'd have a bigge file size and have to put black bars over it in post. But like this you'd have some extra ""room"", for example someone dips the Microphone or the boom in the picture, you could just crop the clip a bit further down and still be able to use it.</p>

<p>More footage = better to edit, right?</p>

<p>Because you still have the same width of the sensor that you use, you just use all of the avaivable height as well.</p>

<p>Now here is my question:
What aspect ratio, should you put your cammera into, to get the highest resolution. </p>
","<blockquote>
  <p>When you take a photo with a Modern camera ""usually"" it's 4/3, I know you can change the aspect ratio, but that's not my point. I assume, that the sensors of DSLR cameras are the same ratio, because they are created for the purpose of creating photos, aren't they?</p>
</blockquote>

<p>Actually, no. Only low-end digital cameras (mostly point-and-shoot) use a 4/3 aspect ratio, because they were designed to match the aspect ratio of computer displays (which used to be 4/3). DSLRs use the traditional 3/2 aspect ratio of 35mm film. So if you set your DSLR to 4/3 (for photos), a part of the sides gets cropped.</p>

<blockquote>
  <p>Wouldn't it be smarter, to record with the fullsensor, to get ""more"" of the image""?</p>
</blockquote>

<p>Most DSLRs don't use their 'full' resolution for video because of technical limitations (mostly memory buffer, but also memory card transfer speed). For example: My Canon EOS 80D has a maximum resolution of 6000x4000 pixels for photos. However, the highest video resolution it can record in is 1920x1080p @ 50fps (IPB). IPB means every frame is recorded as a full frame, no interframe compression is applied. Recording 25 or even 50 frames per second at a resolution of 6000x4000 pixels would result in an absurdly high data rate that DLSRs simply aren't built to process and transfer. </p>

<blockquote>
  <p>What aspect ratio, should you put your cammera into, to get the highest resolution. </p>
</blockquote>

<p>Depends on your camera. Either the camera menu or manual will specify the available resolutions for each aspect ratio, possibly it's a seperate setting as well. This way you can tell what combination gives you the highest resolution. If you can't find the resolution specified in the menu or the manual, take a test recording with every setting the camera has, copy the files to your computer and check each file's resolution with <a href=""https://mediaarea.net/en/MediaInfo"" rel=""nofollow noreferrer"">MediaInfo</a> or a similar program.</p>

<hr>

<p>Note that apart from the resolution, the bitrate will have a much higher impact on your video quality than some pixels more or less. Make sure to get a fast SD card with a lot of space so you can always record at the highest bitrate (and I-frames only if that's an option with your camera). Also, it's good practice to film in the resolution that you want your final video to be in for more reasons than video quality. So if you're creating a video that is intended to be viewed on normal video screens, film in 16/9 from the start.</p>
","20737"
"Video editing objects from black and white to colour","478","","<p>For school we came up with a cool concept, the video starts in black and white and when a person in the video touches an object it turns to colour.</p>

<p>Could you guys help us out here? Do you have any tutorials for us? Which programs do we have to use (the price of the software is not a problem).</p>

<p>Thanks!</p>

<p>PS.</p>

<p>I now understand that rotoscoping can be used for this. But how bout a gradual change from black and white to colour if a building has been touched for example? For it to change in one frame will look kinda terrible...</p>
","<p>This will probably require a technique called rotoscoping, which involves painting a mask on each frame.  You would need to have two layers of video, one which is desaturated (to make the image black and white) and one which is full color.  You would then need to make a mask (a layer which effectively makes a hole in the black and white layer so that the color layer can be seen).  Since the mask may change based on camera movement and actors moving around the scene, you have to update the mask periodically (possibly every frame).</p>

<p>You can simplify the process quite a bit if the objects are stationary in the scene and the camera is stationary on a tripod.  You could also try using object tracking to maintain the placement of the mask roughly, but to get a satisfactory result, you will probably still need some rotoscoping.</p>

<p>After Effects can accomplish what you are looking for.  Any tutorials on rotoscoping and masking in AE would work to demonstrate the finer points of what tools to use to accomplish it.</p>
","7386"
"x265: Is it possible to re-compress HEVC video without quality loss?","478","","<p>My situation:</p>

<ol>
<li>Transcoding 1080p bluray videos to HEVC. Audio untouched;</li>
<li>My rig is old (pre-2012) and to speed up the process I'm using <code>--preset ultrafast</code>;</li>
<li>From x265 documentation, I understand that video quality is independent of the preset used. And the slower the preset, the better the compression ratio it seems;</li>
<li>I'm using FFmpeg.</li>
</ol>

<p>My Reasoning:</p>

<ol>
<li>I want to start converting to HEVC now because the codec is good enough for me and the space saving is phenomenal;</li>
<li>I've seen people reporting a 35GB video compressed to 1GB or so with <code>crf=18</code> and <code>--preset veryslow</code>. It sounds ludicrous but an experiment of my own shows about 40% additional space saving between presets <code>ultrafast</code> and <code>veryslow</code>.</li>
</ol>

<p>The Question:</p>

<p>When I get a new rig with a more capable CPU (Skylake or later, I hear), is it possible for me, to ""re-compress"" my HEVC videos using eg. <code>--preset veryslow</code>, without quality loss due to encoding a second time?</p>
","<p>In strict mathematical terms, no.</p>

<p>In terms of maintaining an acceptable image quality, yes. If you use CRF <code>18</code> and a preset like <code>ultrafast</code> now, you should be able to get a smaller acceptable file with preset <code>veryslow</code> later on.</p>
","17121"
"Tips to make a stopmotion walk","476","","<p>I'm making a game using photographs for the animation. I've asked a friend to walk in place, and taking pictures of each step, using a 6-steps cycle.</p>

<p>It ended ""cool"", but as the photo session was made in like 20 minutes, it needs to be improved.</p>

<p>I myself added some tips:</p>

<ul>
<li>Use a better camera (right know I'm using a standard digital camera).</li>
<li>Place the camera on a tripod.</li>
<li>Use a greenscreen to better separate the subject from the background.</li>
<li>Don't use auto, because the lightning can't vary from picture to picture.</li>
<li>Trace a line on the floor to indicate where my friend has to stand.</li>
</ul>

<p>Do you have any more ideas to help me with this? What I don't really know is how to make the walking to appear as ""natural"" as possible.</p>

<p>You can play what I've done so far <a href=""http://jorjon.com.ar/knock"" rel=""nofollow noreferrer"">here</a>. (Or just watch a <a href=""http://www.youtube.com/watch?v=zMjx78l4nn0"" rel=""nofollow noreferrer"">video</a> if you don't have the patience).<br/>
You can also see a making-of in this <a href=""http://www.youtube.com/watch?v=PdP-13IPlvg"" rel=""nofollow noreferrer"">video</a>.<br/></p>

<p>And finally, these are the pictures I'm currently using (don't mind the posterize, that's just a filter for the game):</p>

<p><img src=""https://i.stack.imgur.com/YnUzp.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/zT9Zk.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/gYNJP.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/gefi8.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/bUXep.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/e5mPc.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/CU15p.gif"" alt=""enter image description here""><img src=""https://i.stack.imgur.com/p4WSW.gif"" alt=""enter image description here""></p>
","<p>I'm not sure you need a better camera for this. since you are downgrading the images to a pretty low quality anyway. The secret to good green screen photography is in the lighting more than in the camera. You need to light the background as evenly as possible, and use a background color that is not present in the subject's clothing. In your example blue would not work due to the shirt, but green might. You can also use a white sheet if you make sure no part of your subject gets blown specular highlights. You need to make sure your subject is not too close to the background, as it may catch reflected light from it. This is particularly important if you go with blue or green background, maybe less with white.</p>

<p>The camera on a tripod is a good idea, but don't expect you'll get images that will be perfectly registered. I expect you will still need to align them, but the job will be easier when the camera was fixed on a tripod.</p>

<p>The camera should be in manual. If you don't want to mess much with the settings just take a few test pictures in auto mode (no on-camera flash, try to use other light sources from the sides, to not create shadows on the background) and once you find one that you like switch to manual and copy the exposure settings.</p>

<p>I think you want to mark a spot on the floor that your subject can use to position himself for the pictures. A line does not seem too useful, since your friend isn't really walking.</p>

<p>My final advice (the one that might be off-topic for this site) is that you grab a good book on animation and learn the animated walk cycle. You just need to have your friend pose in those proven positions that make up natural walks in cartoons. My recommendation is the Preston Blair book, <a href=""http://rads.stackoverflow.com/amzn/click/1560100842"" rel=""nofollow"">Cartoon Animation</a>.</p>

<p>Good luck with your game.</p>
","2987"
"Any nice tool for replacing all black frames in a video with another frame","475","","<p>My video file has some blackish frames (it's almost black, but not exactly black, i.e. #000000), and I want to filter out them, replacing them with neighboring frames.</p>

<p>However, I can't find a simple way to do that.</p>

<p>Of course, I can extract all the frame images from the video file with FFmpeg:</p>

<p><code>ffmpeg -i foo.mp4 -an -framerate 30 -s 1280x720 -f image2 foo-%05d.jpeg</code>  </p>

<p>and then list all the blackish images with some image tools, replace them with neighboring-numbered files, and merge them:</p>

<p><code>ffmpeg -i foo.mp4 -vn -acodec copy foo.m4a</code><br>
<code>ffmpeg -i foo.m4a -f image2 -framerate 30 -i foo-%05d.jpeg -r 30 -s 1280x720 output.mp4</code></p>

<p>but this will create over 10,000 intermediate image files and it is also time-consuming.</p>

<p>Is there any software or programming language that helps this work?</p>

<p>I am using Windows x64.</p>
","<p>You could do this in After Effects.  There are lots of examples online where people use Expressions to examine the average brightness of each frame, then act upon it.  For example, you could add a marker to each frame that fell below a certain threshold level. Or make the frame's opacity go to 0%, to allow another track to show through from behind.</p>

<p>Tutorial examples here:
<a href=""http://markos.co.nz/2012/05/after-effects-expression-use-the-luminance-of-one-layer-to-drive-another-layer/"" rel=""nofollow noreferrer"">http://markos.co.nz/2012/05/after-effects-expression-use-the-luminance-of-one-layer-to-drive-another-layer/</a></p>

<p><a href=""http://blogs.adobe.com/aftereffects/2009/07/color-sampler-using-sampleimag.html"" rel=""nofollow noreferrer"">http://blogs.adobe.com/aftereffects/2009/07/color-sampler-using-sampleimag.html</a></p>

<p>Here's an example expression I wrote based on the blog posts above -</p>

<pre><code>threshold = 0.01;  // this is the threshold below which the track becomes transparent - black = 0.
driverLayer = thisComp.layer(""Pre-comp 1"");
samplePoint = [0,0]; // examine from the top left pixel
sampleSize = [640,360]; // examine the full comp size (in this case 640x360)
lightnessSample = driverLayer.sampleImage(samplePoint,sampleSize);  // this samples the image as rgb
driverLightness = rgbToHsl(lightnessSample)[2]; // converts to hue, saturation and lightness, then only takes lightness
if (driverLightness &gt;=threshold) transform.opacity = 100 else 0; // tell opacity to be either 100 or zero depending upon the threshold point chosen in line 1
</code></pre>

<p>I applied this to the opacity layer.  You would need to adjust the threshold value, and the size of your comp ( in my case 640x360). (the red solid below was to see whether the opacity change was working.  I guess you would put a copy of your video showing the previous frame here?  Will look as though the video got stuck though...</p>

<p><img src=""https://i.stack.imgur.com/wLUD5.png"" alt=""enter image description here""></p>
","13424"
"What is the best way to combine two audio sources into a single audio output?","475","","<p>A friend and I are looking to start a podcast, yea, just like everyone else on the Internet. </p>

<p>We are trying to determine the most practical way to combine two audio sources into a single audio output of not necessarily production quality. The combined audio will not be recorded, and used only for communication purposes. It is our intention to record our own independent tracks at different locations, and later combine them in post.</p>

<p>Our setup is to include a PC at each end with either skype/hangouts, audacity, a USB microphones, and a set of earphones each. The PCs will be used to record audio from our USB microphones, and to establish a line of communication between sites. How can we reasonably combine the audio out of a microphone (with a 3.5 mm and/or XLR) with the audio out of a PC (3.5 mm) so that we can hear both ourselves and the other person(s) in the skype/hangout meeting?</p>
","<blockquote>
  <p>I misspoke in my original question, I've fixed the sentence so
  hopefully it should be clearer. I'm just wanting to listen to the PC
  out, while simultaneously listing to the Mic out as I record it.</p>
</blockquote>

<p>If you using Windows, then just go to the</p>

<p><kbd> Sound </kbd><sub>(Right click in notification area)</sub> > <kbd>Recording devices</kbd> > Chose you device > <kbd>Properties</kbd> > <kbd>Listen</kbd> > <kbd>Listen to your device</kbd></p>

<p>That would allow you to hear your recording device input in your playback device(speaker).</p>
","16301"
"How to loop outer glow layer style in After Effects?","473","","<p>I have a layer in After Effects to which I have applied an outer glow as a layer style. Ideally I would like the glow to turn off and on, in a loop, starting at a point in time and then stopping. Is this possibleperhaps with a script?</p>
","<p>I'd suggest animate the 'Opacity' property of the <em>outer glow</em>:</p>

<p><a href=""https://i.stack.imgur.com/ERB3i.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ERB3i.jpg"" alt=""enter image description here""></a>
<em>Click to enlarge</em></p>

<ol>
<li>Create a keyframe by clicking on the <em>stopwatch of outer glow</em> 'opacity' and set its value to <code>0%</code></li>
<li>Create a second keyframe and set its value to <code>100%</code></li>
<li><kbd>Alt</kbd><em>-Click</em> the stopwatch to create an expression and insert <code>loopOut(type = ""pingpong"")</code> to get an endless loop of <em>both</em> values</li>
</ol>

<p><a href=""https://i.stack.imgur.com/pi9NM.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pi9NM.gif"" alt=""enter image description here""></a><br>
<em>Composition length: 49 Frames, <br>
2 Keyframes: frame <code>0</code> > opacity <code>0%</code>, frame <code>25</code> > opacity <code>100%</code></em></p>

<p><strong>Note:</strong> In order to create a less generic animation, you can also create 3 or more keyframes by using <code>loopOut(type=""cycle"")</code> expression instead.</p>

<hr>

<p>To start the animation at a predefined time as well as stop the animation as you like, you can either <em>duplicate the layer twice</em> and <em>remove the layer styles of both duplicates</em> in order to concatenate all 3 layers by hand or write a <em>simple condition</em> like this:</p>

<pre class=""lang-js prettyprint-override""><code>// get the current frame of the composition
currentFrame = timeToFrames(time, 1 / thisComp.frameDuration, false);

// check if the current frame is in the predetermined range
if (currentFrame &gt;= 20 &amp;&amp; currentFrame &lt;= 60) {
  // loop all keyframes
  loopOut(type = ""cycle"");
}
else{
  // if the current frame is not in predetermined range set glow opacity to 0
  thisLayer.layerStyle.outerGlow.opacity = 0;
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/Sa4Lx.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sa4Lx.gif"" alt=""enter image description here""></a><br></p>

<p><em>Composition length: 80 Frames, <br>
Loop animation range: Frame 20 - 60, <br>
3 Keyframes: frame <code>0</code> > opacity <code>0%</code> , frame <code>10</code> > opacity <code>100%</code>, frame <code>20</code>> opacity <code>0%</code></em></p>

<p>Official <a href=""https://helpx.adobe.com/after-effects/using/expression-language-reference.html"" rel=""nofollow noreferrer"">expression language reference</a></p>
","19544"
"How were copies of movies meant to be distributed to theaters made?","473","","<p>In the ""olden days"" before digital movie projection and the Digital Cinema Package were around, movies were recorded onto film (usually 35mm, I have heard of 70mm instances). The ""master film"" was the 35mm roll, and somehow copies were made to be distributed to theaters, which could also be in the 35mm format but 16mm could be used as well. Just how was this done effectively?</p>
","<p>Regardless of format; 16, 35, 65, etc... The studios would distrubute release prints. The prints were shipped (speaking about 35mm here) on 2000' reels; 5 reels to a can. </p>

<p>In the olden day's they would use two projectors and the projectionist would have to literally load reel one, and two to the other, and they would switch over. Back and forth. A typical movie in today's world (2 hours) would run about 5 reels. </p>

<p>So let's talk the 1990s. The theater receives a can of 5 reels. The reels would be sent up to the projectionist booth. The projectionist would then assemble the reels into one giant reel; on a platter; basically a large spinning lazy-susan. The platter can feed either from the inside; called the core (the core is roughly 10 inches; and has a center mechanism so with rollers in an assembly so the film never has to be ""rewound"". </p>

<p>The projectionist would first add blank leader, just black, typically 30 seconds; then they would use a film splicer and film tape to add the theater opening title, and each trailer which would play before the film. Some films; shipped with specific trailers on them. But most times; there would be racks and racks of trailers (small film reels about 6 inches (3 minutes) of film; and the projectionist would; whatever his preference or theater policy; add the trailers he wanted (typically matching action with action films, etc). Then any other small film reels would be added; ""Turn off your cell phones"". Etc. </p>

<p>Lastly; the projectionist would add reel 1, then 2, and so on; and the platter is motorized so it literally pulls the film off the reel which is setup on a rolling stand. </p>

<p>Each reel would need to be checked at the tails; and spliced and glued together. </p>

<p>The final reel would be a very large reel of film; roughly 5 feet in diameter, with a core about 10 inches in the middle. </p>

<p>Because this would be the ""first run"", the start of the movie would be on the inside, in the core; so the film would be pulled out of the core; through multiple rollers; and into the projector; and then fed to a take up platter either below or above the feed platter. (Typically there would be 3 platters so you could have more than one movie available on any given screen). </p>

<p>This setup would yield after the movie with the film left on a new platter, ready to be ""re-threaded"" through the projector.  Every showing required the projectionist to check the platter; and reload the projector threading the film over rollers with sprockets; through the gate, and then to the take up reel. </p>

<p>Release Prints were VERY expensive for the studios to produce. So many theaters had rollers for ""Big Releases"" - think ""Titanic""; where one platter would run through the projector, then instead of going straight to a take up platter; it would be fed through a series of rollers down a long wall; to a 2nd or even 3rd projector; and they would have 3 screens starting all at the same time. The final take up reel would be at the last projector. </p>

<p>After the film was finished being run. The projectionist would then literally do the opposite of putting it together. Trailers typically discarded; and the film put back onto the 2000' reels. The film reels would be put back into the can; labeled for FedEx, and sent to a B-Theater; a less popular/lower income movie house. </p>

<p>This is why when you saw movies in the pre-2000s at say an old mall movie theater; there would be lots of scratches and dust, because the film had been played so many times and had been out for so long. </p>

<p>Now; you can find release prints on ebay; I almost bought a ""Terminator 2"" release print just to have... they were owned by the studio, and destroyed after use... so they are fairly scarce. </p>

<p>The projectionist would have to pay close attention during assembly and tear down; because you cannot rewind the platter. </p>

<p>I speak from being a projectionist for a year after highschool. One time I assembled a movie in the order of reel 1, 2, 4, 3, 5. So half way through the movie people got confused and came out and complained. With no way to rewind or fixed, the rest of the days shows had to be canceled, and the film reels taken back apart and put back into the same order. </p>

<p>Last note; you can typically see the change from reel to reel when watching a movie; either by seeing what's called a cigarette burn (small circle top right of frame), or simply, you notice a weird cut/shift in color/sound where the splice was... Because the release prints were made on multiple printers, color always didn't match reel to reel. Most times, you could at the least (if the film had an optical audio track); a small pop. Towards the later 90s... films went to digital DTS, and THX went digital as well. But before that, the film was a waveform optically printed on the left side of the frame. The audio, was optical. </p>

<p>In terms of how they did it ""effectively"". It's just how it was done for almost a 100 years. They were very very happy when things went digital. And theaters had to literally fight the studios, because most projectors in film houses were decades old... mechanical... and the studios wanted them all to upgrade to 20K Lumen 4K Dual Projectors (Digital); which run about $200,000 each. </p>

<p>Cheers!</p>
","21177"
"Web Videos via Watch Folders Transcode","472","","<p>What I am looking to do is fairly simple, however seems complicated in theory.</p>

<p>What I need to do is be able to have a folder where all video files placed in the folder, are transcoded to all applicable web video formats (H264, OGG, WEBM, FLV) Then placed into another folder...</p>

<p>I can set this up to a certain extent using Adobe Media Encoder however I need to use source Frame Rate, and Frame Size, as these clips will vary in range from PAL to NTSC to HD...</p>

<p>I am open to using FFmpeg but cant seem to get it work in the Mac Environment correctly doing what I want.  Any software as long as it is mac based would work, but must be able to create all 4 of the above mentioned formats.</p>
","<p>...FFMPEG worked, it was a steep learning curve in terms of getting it to work in my environment and, took me a few days to learn how to code in Bash Scripts Shell based, but in the end very happy with the results and it's integration with both Macs and my Unix server.</p>
","7195"
"How to recover a video that has no time index?","470","","<p>I have a wmv video file that was recorded using widows media encoder. However, while recording, the computer lost power and the video is was not ""completed"". The video will play in most player, however, it reads that it is 0:00:00 minutes long. So when I try to seek the video the player either freaks out and crashes or it won't do anything.</p>

<p>I tried loading the video in Vegas Pro, but it didn't work. Vegas Pro sees it as an infinitely small clip, rather than the whole thing. So I can't edit it either.</p>

<p>I was able to convert it to an Mp4 in handbrake, but now that I have to do some editing on <em>that</em>, I'm going to lose significant quality. I would rather not have to do this and just be able to find the index file.</p>

<p>What can I do about this? Is there a program or feature in some program that can reanalyze the video and give it a proper time index (if that's what this is called)?</p>
","<p>I've had some success with <a href=""http://www.videohelp.com/software/ASFTools"" rel=""nofollow"">ASFTools</a> and <a href=""http://www.videohelp.com/software/Steeper"" rel=""nofollow"">Steeper WMV Repair</a>. Both of these are older tools (Steeper is from 2005) so may not be able to handle later versions of WMV, but they're worth a try.</p>
","15664"
"Resynchronize Audio to Video","469","","<p>When I have audio and video tracks de-synchronized, is there a way to make them synchronous?</p>

<p>A workflow:</p>

<ul>
<li>Add a video+audio track to a project;</li>
<li>Crop a small <em>Event</em> in the middle and remove the rest;</li>
<li>Uncheck <code>Ignore event grouping</code> option;</li>
<li><p>Drag one of the tracks sideways;<br>
<strong>Note</strong> the audio track gets reddish background to indicate it is not synced to Video.</p></li>
<li><p>Resize Video and/or Audio <em>Events</em> so that original start/end timestamps get lost;</p></li>
</ul>

<p>Now I need to get Audio synced back to Video.<br>
Obviously, dragging Video or Audio will not work as I have resized both <em>Events</em> individually.</p>

<p>How to get Audio back in sync, probably by losing cropping?<br>
<em>I'm aware about opening the Event in the Trimmer and re-dragging the Audio track from there.</em></p>
","<p>This does it for me (see the screenshots):</p>

<ol>
<li>Set the cursor to the beginning of the audio event.</li>
<li>Drag the beginning of the audio event with the mouse to the beginning of the original clip (note: the cursor is still positioned at the cropped beginning) (screenshot 2)</li>
<li>Cut the audio event at cursor position (press S)</li>
<li>Repeat step 1 to 3 with the video event (screenshot 3)</li>
<li>Now you can easily synchronize video and audio by aligning the events (screenshot 4)</li>
<li>Delete the beginnings. (screenshot 5)<img src=""https://i.stack.imgur.com/OU6lv.jpg"" alt=""synchronizing audio and video""></li>
</ol>
","10792"
"What VFX and video production softwares are used in movies like ""Captain America: Civil War"" and ""Doctor Strange""?","469","","<p>I was thinking that for VFX, they used Maya or some other animation software (especially for Dr.Strange) and for the video production, they used Sony Vegas or After Affects. And of course, they might have also made some of the movie visual effects directly from the video production software itself (for example, in AfterEffects you can make VFX in the software itself). </p>

<p>Those were just my guesses, but if you guys have any ideas or links to what VFX and video production softwares were used in Dr.Strange and Civil War, please state them.</p>

<p>Thanks!</p>
","<p>This depends from Company to company, there are multiple companies working on multiple shots, or sometime even on the same shots. Because this (sometimes) differs a lot from eachother, it's really hard to give you an answer to this question.</p>

<p>As mentioned, this heavily depends on effects you want to acchieve and the company you work with.</p>

<p>These are some commonly used programms:</p>

<ul>
<li>Cinema 4D </li>
<li>BodyPaint </li>
<li>AutoCAD </li>
<li>3DS Max </li>
<li>InfraWorks </li>
<li>Maya </li>
<li>Mudbox </li>
<li>Flame </li>
<li>Smoke </li>
<li>Softimage </li>
<li>Mental Ray </li>
<li>Combustion </li>
<li>FLIX </li>
<li>MODO </li>
<li>MARI </li>
<li>NUKE </li>
<li>OCULA </li>
<li>HIERO </li>
<li>Katana </li>
<li>keylight </li>
<li>Massive Prime </li>
<li>Massive Jet </li>
<li>Massiv for Maya </li>
<li>Ready to run Agents </li>
<li>Matchmover </li>
<li>Z-Brush </li>
<li>Adobe CS6 </li>
<li>Illustrator </li>
<li>Photoshop </li>
<li>Flash </li>
<li>After effects </li>
<li>Lightroom </li>
<li>v-Ray for 3DS Max </li>
<li>Rendermann  </li>
<li>Avid Media Composer </li>
<li>Avid effects </li>
<li>davinciresolve</li>
<li>Houdini</li>
<li>3D coat</li>
<li>topgun  </li>
<li>shotgun Studio  </li>
<li>PF Track </li>
<li>NURBS </li>
<li>Filemaker Pro </li>
<li>vue </li>
<li>Plant factory </li>
<li>Ozone </li>
<li>Carbon scatter </li>
<li>Lumen RT </li>
<li>Primatte </li>
<li>ultimate </li>
<li>Final Cut Pro </li>
<li>Qube </li>
<li>Synth-Eyes </li>
<li>boujou </li>
<li>Fusion </li>
<li>3D equalizer </li>
<li>Mantra </li>
<li>Real Flow </li>
<li>Fume FX  </li>
<li>BlueArc</li>
</ul>

<p>I can narrow them down, if you can tell on which effect you are specifically interessted.</p>
","20316"
"Why filmmaking with DSLR?","469","","<p>Adding video feature to digital camera was for their popular applications, as unprofessional users wanted to have both photography and videography at the same time. Professional cameras were exclusively divided to photography or videography. However, this situation is changing, and videography features are added to DSLR cameras too.</p>

<p>The interesting point is that the level professionality of videography on photography cameras are reaching the highest level, as many filmmakers use DSLR camera for professional projects. I understand that technology is advancing with better DSLR cameras, but video cameras were also advanced. Why there is such tendency to use photography camera for filmmaking?</p>

<p>What are the technical advantages (or even better pros and cons) of using photography DSLR camera for filmmaking?</p>
","<p>The main advantage is cost.  Historically, there were two main types of imaging sensors, CMOS and CCD.  CMOS was historically used in digital cameras (because of the higher quality images it can capture, the lower cost, and the fact that digital cameras only have to capture a single moment).  </p>

<p>CCDs on the other hand were historically used in video cameras, with one CCD per color (3CCD) to increase the quality.  They have been significantly more expensive and generally capable of lower resolutions, but had the advantage of capturing the entire frame at the same time.</p>

<p>Most CMOS sensors scan line by line rather than capturing a fixed image at a time, they exhibit an artifact known as a rolling shutter.  This appears as a distortion when the camera is moving (particularly panning) due to the change of orientation mid-frame.  Previously, the sample rates for CMOS sensors were too slow to allow their use for video, but as the technology has improved, it is now possible to sample quickly enough that, while still present, rolling shutter is not nearly the problem it was even a few years ago.  Now there are even CMOS sensors capable of offering a global shutter and they have all but supplanted the old CCD based cameras, with even major motion pictures being shot on CMOS sensors with global shutters.</p>

<p>The advances in CMOS technology have allowed for CMOS sensors to be used in professional level video, bringing with them the low cost, high resolution and high quality that had previously not been possible.  Global shutter CMOS still costs more than rolling shutter relative to other features of the sensor, so rolling shutter is still present in many cameras (even fairly high end cinema cameras) due to the cost savings and the fact that higher sampling speed has greatly reduced the impact of rolling shutters.</p>

<p>Another large advantage is size.  DSLRs tend to have less processing capability to work with the large amounts of data involved in things like 4k video, so they are more limited in capability, but they also are much smaller and more portable.  This is probably most visible in comparing the Black Magic Production Camera and the Black Magic Ursa Mini 4k.  Both use the same 4k sensor, however the size of the BMPC limits it's ability to work with high frame rate video as well as some other features of the camera.  The Ursa Mini is a much larger form factor that currently costs the exact same amount, but is able to fit more features in due to the extra size and hardware that can be included in the larger form factor.</p>
","7881"
"Control key frame value with slider control","468","","<p>I have an object in After Effects comp and I need its scale to change from <em>keyframe A</em> to <em>keyframe B</em> and I need <em>keyframe A</em> to have value of 95% and <em>keyframe B</em> value of whatever is set via slider control. Now I know how to set overall scale by slider, but I seem to have problem finding a way to link just that one keyframe to my slider control.</p>

<p>I am fairly new to expressions and I tried to re-purpose following one I found for the same thing but with opacity, but it doesn't work. I think it's because it supplies just one value and scale needs two values, but I honestly have very little idea whats going on in the expression and how to adapt it.</p>

<pre><code>if (numKeys &gt; 1){
  t1 = key(1).time;
  t2 = key(2).time;
  v1 = 0;
  v2 = comp(""INPUT"").layer(""CONTROL"").effect(""Zoom intensity"")(""Slider"");
  linear(time,t1,t2,v1,v2);
}else
  value
</code></pre>
","<p>You were close.</p>

<pre><code>if (numKeys &gt; 1){
  t1 = key(1).time;
  t2 = key(2).time;
  v1 = [95, 95]; //or if it's 3d [95, 95, 95]
  slider = comp(""INPUT"").layer(""CONTROL"").effect(""Zoom intensity"")(""Slider"");
  v2 = [slider, slider]; // or [slider,slider,slider] if 3D
  linear((time, t1, t2, v1, v2);
} else {
  value
}
</code></pre>

<p>You were right that the scale property requires 2 (or 3) dimensions. So you need to use arrays, which look like <code>[value1, value2]</code>, or <code>[value1, value2, value3, valueN]</code> for as many members you want the array to have.</p>

<ul>
<li>On line 4 I assume you want it to be <code>95%</code> not <code>0</code>. I also changed it to an array.</li>
<li>on line 5 I assign the slider value to a temporary variable and then put that in an array. You could do it in one line by repeating the <code>comp(""INPUT""  (Slider)</code> phrase inside an array, but it would be really long and awkward. Alternatively you could use a point control if you wanted to be able to set the dimensions separately.</li>
<li>on line 8 the <code>else</code> without curly braces works, but it makes me nervous.</li>
</ul>
","21065"
"What does the value in parentheses for ffmpeg ssim log denote?","468","","<p><a href=""https://ffmpeg.org/ffmpeg-all.html#ssim"" rel=""nofollow"">https://ffmpeg.org/ffmpeg-all.html#ssim</a></p>

<p>I ran a command like this:</p>

<pre><code>.\..\ffmpeg -i [Input] -i [Output] -lavfi  ssim=""stats_file=ssim.log"" -f null -
</code></pre>

<p>and saw this in the ssim log outputs:</p>

<pre><code>n:70 Y:0.947271 U:0.985112 V:0.985611 All:0.959968 (13.975937)
</code></pre>

<p>I understand the values for everything except for the value in the parentheses, after All (13.975937). Is this the db representation of the YUV,All values, or is it something different? How is it calculated? Thank you.</p>
","<p>It is dB representation of All value, calculated with following formula:</p>

<p><code>10 * log10(1 / (1 - ssim))</code></p>
","17853"
"Traditional animation techniques and 30fps software animation?","466","","<p>I'm putting together a very basic software to animate a few pixels frame-by-frame. I've skimmed through Richard William's Animator's Survival Kit in the past and noticed most traditional animation uses 24fps and he used to ""animate on twos"" for most motion and on ""ones"" for fast motion. </p>

<p>Since my tool will deal with frame by frame animation I'd like to make use of those principles, but I am constraint to 30 fps. Because of this, I'm wondering how would I workout at what frame interval(every frame, every two frames, every three frames, etc.) a 'keyframe'/changed frame should be placed to look good ? Is there a way to work that out arithmetically ? Does it make a bit difference/ is it safe two stick to twos even for 30fps ?</p>
","<p>Yes, repeating every other frame works the same way at 30fps as it does at 24fps.</p>

<p>Animating on twos is done not because it looks better, but because it means less drawing and you can 'get away' with it. Nothing stops you from drawing every frame if you want the smoothest possible motion, and nothing prevents you from repeating a frame indefinitely when there is no motion.</p>

<p>If you really want to work at 24fps, you could always animate 'as though' you were working at 24fps -- your work will look sped up when viewed at 30fps -- and applying a cine expansion (3:2 pulldown) as a final step.</p>
","6998"
"FFmpeg script to clean up and compress iPhone video recordings from university lectures","462","","<p>I video record my university lectures using an iPhone 6 and then export them to OS X. The files can often become up to 4 GB/hour in size.</p>

<p>I'm looking to create or use a pre-existing solution that can do the following for my video files:</p>

<ul>
<li>Amplify the audio</li>
<li>Normalize or level the audio</li>
<li>Audio click removal (optional)</li>
<li>Compress video to save disk space</li>
<li>Do the above by running a script or shell command (for batches of files if possible)</li>
</ul>

<p>I'm assuming that FFmpeg can be helpful for running existing scripts, or creating a new one.</p>

<p>How should I go about to meet these objectives?</p>

<p>I have also looked at other alternatives like Handbrake but didn't find that it can do the audio adjustments adequately  let me know if I'm wrong.</p>
","<p>This is what I have come up with so far. Feel free to improve this answer liberally, with the objective of cleaning up and encoding lecture notes, within a streamlined process.</p>

<p>The command is run in the working directory, e.g. <code>cd /Users/me/Downloads</code>, and will encode all .MOV files present.</p>

<pre><code>for i in *.MOV; do ffmpeg -i ""$i"" -c:v libx265 -preset veryslow -crf 23 -af ""volume=25dB, highpass=f=200, equalizer=f=50:width_type=h:width=100:g=-15"" -c:a aac -strict experimental -b:a 192k ""${i%.MOV}-ENCODED.MOV""; done
</code></pre>

<p>The individual settings:</p>

<ul>
<li>Video codec: <code>libx265</code> (HEVC). This codec is unorthodox because it's still in development, but it's many times better than its predecessor in keeping file sizes low. <a href=""https://www.videolan.org/vlc/index.html"" rel=""nofollow"">VLC</a> supports this codec for viewing the video.</li>
<li>Encoding speed vs. compression density: <code>-preset veryslow</code>. Converting videos with this preset takes <em>very</em> long time, but may reduce file size with around 20 % compared to the fastest setting. If you want quicker encoding, use <code>ultrafast</code>, <code>fast</code>, <code>medium</code> or <code>slow</code>.</li>
<li>Video quality: <code>-crf 23</code>. Quite good quality. Decreasing this number will increase quality but also logarithmically increase file size. This setting strikes an okay balance between file size and quality. <code>18</code> is visually lossless but takes up much space.</li>
<li>Volume gain: <code>volume=25dB</code></li>
<li>High pass filter >200 Hz: <code>highpass=f=200</code></li>
<li>EQ notch filter @ 50Hz100Hz (this should be mitigated by the high pass filter already, but for some reason this seems to remove background noise: <code>equalizer=f=50:width_type=h:width=100:g=-15</code></li>
<li>Audio codec: <code>aac</code></li>
<li>Audio quality: <code>-b:a 192k</code></li>
</ul>

<p><strong>Optional</strong>:</p>

<ul>
<li>Lighten video curves: <code>-vf ""curves=preset=lighter""</code></li>
</ul>

<p><strong>Ideas for improvement:</strong></p>

<ul>
<li>A good way to gate audio to filter out background noise and keep only the talky bits.</li>
</ul>
","16957"
"Is it possible 3D Transform a video clip in Final Cut Pro X?","462","","<p>All i've ever seen in Final Cut Pro is 2D Transform on a video clip. I know you can do that in After Effects but I want to stay within Final Cut Pro X to do the 3D transform.</p>

<p>Is there any plugins that could allow me to set a video clip in 3D Space ?</p>
","<p>Apple Motion will let you create ""motion templates"" which are usable in FCPX.  You can apply 3D transforms with Motion, and publish to Final Cut.  This approach is easy to learn, but limited in scope.</p>
","10695"
"False copyright claims by ""Orchard Music""","460","","<p>I've created a video with background music. The background music is licensed under Creative Commons. Link (contains a download button): <div class=""soundcloud-embed"">
            <iframe src=""https://w.soundcloud.com/player/?url=http://api.soundcloud.com/tracks/243351088?show_artwork=false"" width=""640"" height=""116"" scrolling=""no"" frameborder=""no""></iframe>
            </div>
Please click on the 'Awareness (Ambient Lounge Mix)' text to be directed to the license description.</p>

<p>When I uploaded the video to Youtube it had a copyright notice from ""Orchard Music"". So, I disputed the copyright infringement claim. My argumentation was that the music was a Creative Commons license. Orchard Music rejected my dispute. </p>

<p>I can dispute the rejected decision again. Two things can happen then. </p>

<ol>
<li>The Copyright claim will be taken down</li>
<li>My uploaded video will be taken down</li>
</ol>

<p>I fear that option 2 will be applied. I did some research and found valuable sources. </p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/SYXbK4OKxq0?start=0""></iframe>
            </div></div>
<a href=""https://productforums.google.com/forum/#!topic/youtube/aVZ_dp7Ikt4"" rel=""nofollow noreferrer"">https://productforums.google.com/forum/#!topic/youtube/aVZ_dp7Ikt4</a></p>

<p>I have three questions actually: </p>

<ol>
<li>How is it possible that Orchard Music can claim ownership of a Creative Commons music production?</li>
<li>Why do I need a publishing license to a Creative Commons music production?</li>
<li>Should I dispute the claim for the second time?</li>
</ol>
","<p>Ok, this is quite a complicated problem actually, and depending on the specific situation, the answer might vary. So I'll break this down in a couple of subsections. </p>

<h2>CC licences and unauthorized distribution</h2>

<p>Creative Commons are a set of licences that allow creators to distribute their original content (e.g. music) using liberal licences under easy to understand terms. However, in order to do so, the uploader needs to hold the rights to the music (usually this means they are the creator). If they don't, they have no right to grant any permissions regarding the use of that music. It's your responsibility to check if they do have those rights. </p>

<p>Excerpt from <a href=""https://video.stackexchange.com/questions/19952/where-to-get-good-music-for-corporate-videos/19964#19964"">my answer here</a>:</p>

<blockquote>
  <p>Remember that just because someone uploads music to a website and claims it's free-to-use, it doesn't mean he actually has the right to do so or the authority to grant you any rights to it. And if you use a copyrighted piece of music in your project without knowing any better, you can stil be held responsible even if you downloaded it from a website that claimed it was free. Always check if the website you are downloading from is legit and if the uploaders actually have to rights to the music (in most cases, that means they made it themselves). </p>
</blockquote>

<p>In short, just because someone uploads a Hannah Montana album to some website and puts a CC icon next to it, you still can't use Best of Both Worlds in your video. And if you do, you can't rely on your claim that you got it from that site, because the uploader didn't hold the rights to that music in the first place and it would've been your responsibility to check if they do. </p>

<p>If it's music from a rather unknown author and there were no signs that the original source isn't legit, you might not be held responsible for the 'damage' (claimed by the copyright holder, whether that damage actually exists is a rather philosophical question) caused by your usage of the music. However, even in that case, you'd have to take your video down. So there's no winning in a situation like that.</p>

<p><strong>tl;dr:</strong> Always make sure that uploader of whatever music you want to use actually made that music.</p>

<p>In your case, the Soundcloud account looks to be legit, so let's proceed. </p>

<h2>CC licenses and retractability</h2>

<p>This is only tangetially related to your specific problem, but it's actually something that I think is quite relevant to false copyright claims regarding CC licenced work. All CC licences contain this condition: </p>

<blockquote>
  <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>
</blockquote>

<p>What this means is as long as you uphold the terms of the licence, the uploader can't retract the permissions granted by the CC licence. Of course that doesn't mean he has to continue distribution his works indefinetely. But if a music creator at some point decides to stop distributing his work under a CC licence, this doesn't affect any work using his music (e.g. your YouTube video) that was created before that time. </p>

<p>So, assuming you made sure the uploader of the music holds the right to it and follow the licence terms (e.g. proper attribution), your video doesn't infringe any copyright and you are right to dispute such a claim. </p>

<p>(Note: If an artist at some point stops distributing his music under a CC licence, from that point onwards you can't rightfully publish any <em>new</em> work using his music, even if you downloaded it while it was still CC licenced. The non-retractibility only affects works published while the music was being distributed under a CC licence.)</p>

<p>However, being right isn't all there is to it. There's a saying in German:</p>

<blockquote>
  <p>Recht haben und Recht bekommen sind zwei Paar Schuhe.</p>
</blockquote>

<p>Roughly translated: Being right and being proven right are two different things. That's where YouTube's ID system and false copyright claims come into play.</p>

<h2>YouTube's ID system</h2>

<blockquote>
  <p>""In the beginning the YouTube ID system was created. This has made a lot of people very angry and been widely regarded as a bad move""</p>
</blockquote>

<p>YouTube has a system in place that automatically checks for copyrighted music in uploaded videos and places copyright claims accordingly. The copyright holder of that music (according to YouTube's database) then gets notified and can decide what to do with it; for example, they can have the video taken down, or they can decide to leave it up but receive a portion of this video's ad proceeds. They can also decide on a default action, and that's a big part of the problem that made many people very angry.</p>

<p>In 2013 especially there was much trouble regarding this system. The problem was, the automated ID system was terrible to begin with and would often attribute music wrongly. There were also many scammers that would claim copyright for music that wasn't theirs. The problem is that in cases like this, YouTube favors the copyright holder. So instead of having the person who claims a copyright infringement provide them with proof, the video creator's have to dispute a claim and provide prove that they didn't infringe any copyrights. This usually took a couple of weeks and in the meantime, the person to claim the copyright infringement got all the proceeds from that video.</p>

<p>This system has since been <a href=""http://kotaku.com/youtubes-content-id-system-gets-one-much-needed-fix-1773643254"" rel=""nofollow noreferrer"">refined</a> and the uproar died down, but you still hear stories like yours occassionaly. When in doubt, YouTube still favors the copyright claimer over the video creator. </p>

<h2>The Orchard Group</h2>

<p>I hadn't heard of the Orchard Group before, but I've looked through the links you included in your question and did some additional research. As you already said, you are not the first creator to have problems with false copyright claims. On the forums, many people are quick to call them scam artists. But I'm not sure about that. There are also some stories about people that contacted them and got their false copyright claim resolved quickly. I believe the root of the problem might be this:</p>

<blockquote>
  <p>The Orchard's YouTube multi-channel network has more than 1,000 channels across the globe and uses technology, built in-house, called B.A.C.O.N. (Bulk Automated Claiming on The Orchard Network) to crawl, claim and track YouTube videos to monetize for their clients. It was ranked 7th in the U.S. in July 2014.</p>
</blockquote>

<p><sub><a href=""https://en.wikipedia.org/wiki/The_Orchard_(company)#cite_ref-47"" rel=""nofollow noreferrer"">Source: Wikipedia</a></sub></p>

<p>However, according to this source, the Orchard Music isn't unwilling to forfeit wrongly submitted copyright claims: </p>

<blockquote>
  <p>YouTube users have expressed concerns about The Orchard claiming copyright ownership of music used in user-generated content that may or may not belong to them. According to an article by The Orchard on their Daily Rind blog, if audio is matched to a particular copyright owner via YouTube's content identification system, then one or more links will be placed under the video in order to help promote the copyright owner's music, with the video remaining available. The article states that erroneous claims are removed by The Orchard Team after review upon email communication with their dispute department.</p>
  
  <p>In this context, The Orchard has been criticized by YouTube users for claiming content that is not theirs or in some cases, content that did not exist within the video at all. In a blog post on The Daily Rind, The Orchard explains that this is due to YouTube's automated Content ID matching and outlines steps for resolving this scenario.</p>
</blockquote>

<p>So, you got two automated systems that can both screw up and have been shown to do so often. Not an ideal situation for content creators. If you get caught in such an automated system, the solution is almost always to contact the people responsible for it. Contact the Orchard Group and possibly also contact the original artist of the music. If they are indeed not trying to scam anyone, they will be willing to help you. If the Orchard Group doesn't reply, public shaming on Twitter oftentimes helps you get in touch with an actual person, not just with an email autoresponder.</p>

<p>If they actually <em>are</em> scammers or plainly refuse to forfeit their copyright claim, you will have to contact YouTube to get it resolved. So ...</p>

<h2>Your questions</h2>

<blockquote>
  <ol>
  <li>How is it possible that Orchard Music can claim ownership of a Creative Commons music production?</li>
  </ol>
</blockquote>

<p>As mentioned, this is mostly done by automated systems that are prone to misfires, especially if larger distribution/collection companies are involved. Also, you can claim anything, that doesn't mean you actually have any lawful grounds on which your claim is based. I can claim that I be crowned High King of the Universe, but nobody will have to call me Your Majesty. And if I sue them, I'm unlikely to win that case.</p>

<blockquote>
  <ol start=""2"">
  <li>Why do I need a publishing license to a Creative Commons music production?</li>
  </ol>
</blockquote>

<p>You don't. The CC licence gives you the right to publish your video if you follow the terms laid out in the licence, you don't need to pay any additional licence fees or royalties. However, a human error is also possible.</p>

<p>For example, maybe the artist is represented by the Orchard Group and never intended to publish their work under a CC licence. Maybe they just forgot to change that setting while uploading their music to Soundcloud. I'm not sure about Soundcloud, but if you for example upload something to Vimeo, a CC-BY licence is applied <em>by default</em>, and you have to go into a submenu of the settings to change that. I think that is a terrible implementation, but that's how it is. </p>

<p>Even if that case, you would probably be within your rights to invoke the licence terms the author published his music under, but it might not be feasible or realistic to do so and see your claim through. </p>

<blockquote>
  <ol start=""3"">
  <li>Should I dispute the claim for the second time?</li>
  </ol>
</blockquote>

<p>That is your decision. Before you do anything, check if you followed all the licence terms, the legitimacy of your music source and everything else mentioned in this post. It might also be better to contact the Orchard Group or the artist directly to see if you can't get the issue resolved without involving YouTube as a third party. Though a lawyer would probably tell you that contacting them is the worst thing to do if you plan to follow this up with a lawsuit.</p>

<p>Just remember, being right doesn't mean YouTube will take your side. And if they don't, there's little you can do besides suing, which might not be feasible for you.</p>

<hr>

<p><strong>Disclaimer:</strong> I'm not actually sure I answered your question after all that text. I'm sure I forgot something, but there's so many important factors involved, it's hard to put everything into one answer. Feel free to ask if I forgot anything, I'll be happy to edit my answer if I can. Also note that <strong>none of this constitutes legal advice</strong>. It's rather a description of the current situation regarding YouTube's copyright claims systes and some pointers as to what you can reasonably do to get this problem resolved. Your theoretical (lawful and moral) rights and responsibilites might actually differ from what you can reasonably enforce.</p>
","20092"
"How to convert MP3 file to MIDI on WIndows?","460","","<p>How to convert MP3 file to MIDI on WIndows?  MP3 in general is <a href=""http://en.wikipedia.org/wiki/Lossless_compression#Audio"" rel=""nofollow"">not losssless</a>, is it possible to get MIDI out of a MP3 file?</p>
","<p>There are several tools out there. The free but rather dated <a href=""http://www.pluto.dti.ne.jp/~araki/amazingmidi/"" rel=""nofollow"">AmazingMIDI</a> which takes in <code>.wav</code> files and generates the corresponding MIDI. You can convert to <code>.wav</code> from <code>.mp3</code> using <a href=""http://web.audacityteam.org/"" rel=""nofollow"">Audacity</a> or ffmpeg.</p>

<p>Then there are a few commercial products such as <a href=""http://www.athtek.com/digiband.html"" rel=""nofollow"">AthTek DigiBand</a>, <a href=""http://www.widisoft.com"" rel=""nofollow"">WidiSoft</a>, and others.</p>

<p>Note that figuring the tune from the recording can be done only as an approximation, and the resulting accuracy depends on the sound profile of the audio content. So don't expect a 100% accuracy, you'll have to try several tools and find the one that yields the better result for your specific case.</p>
","15617"
"Why does variable bitrate still have a bitrate specified?","457","","<p>My understanding is that the vorbis algorithm uses a variable rate of encoding.</p>

<p>So, when encoding videos with audio in vorbis format, why does Handbrake ask me for a bitrate? What does Handbrake do when I choose a bitrate of 160, say, in the audio section?</p>
","<p>You can see the bitrate in this case as a quality target. It will not encode the audio strictly at that bit-rate but try to encode the audio in a way that it will always be ""near"" your specified bit-rate.</p>

<p>Usually you can translate certain bit-rates to a specific quality level which can be compared to lames ""V0"", ""V2"" etc. VBR settings, these presets don't define a specific bit-rate but files encoded with these settings will always end up at certain ""bit-rate range"". The encoder will encode the audio around a specific bit-rate but will go higher or lower within a certain threshold.</p>

<p>It doesn't make all that much sense to have bit-rats present instead of the Vorbis quality targets but I assume handbrake does this because all other codecs it offers use bit-rates. Most users are used to bit-rates and can associate it to a rough quality level. Unlike Vorbis own ""q5"" or ""q9"" which you can only translate to a relative quality difference between the different levels (if you are not used to using this codec) but you wont know what you actually end up with.</p>

<p>Wikipedia has a nice table for Vorbis quality levels/settings: <a href=""http://en.wikipedia.org/wiki/Vorbis#Technical_details"" rel=""nofollow"">http://en.wikipedia.org/wiki/Vorbis#Technical_details</a>
(on the right)</p>

<blockquote>
  <p>Quality   Nominal Bitrate     Official Xiph.Org Foundation Vorbis     aoTuV<br>
  beta 3 and later</p>
  
  <ul>
  <li>-q-2   not available   32 kbit/s</li>
  <li>-q-1   45 kbit/s   48 kbit/s</li>
  <li>-q0    64 kbit/s</li>
  <li>-q1    80 kbit/s</li>
  <li>-q2   96 kbit/s</li>
  <li>-q3   112 kbit/s</li>
  <li>-q4   128 kbit/s</li>
  <li>-q5   160 kbit/s</li>
  <li>-q6    192 kbit/s</li>
  <li>-q7   224 kbit/s</li>
  <li>-q8   256 kbit/s</li>
  <li>-q9   320 kbit/s</li>
  <li>-q10  500 kbit/s</li>
  </ul>
</blockquote>
","10650"
"Best way of presenting both 720p and 1080p screencasts","456","","<p>I record screencasts in 720p resolution, but would like to put up 1080p as well. This brings a problem: how can I record videos in 1080p such that they still look OK when scaled down to 720p? I've been thinking about DPI scaling (on Windows), but maybe it's possible to go the other way, i.e. record at 720p and then use some sort of smart scaling to produce a (relatively speaking) high-quality 1080p render?</p>
","<p>Generally speaking shooting in 1080p and scaling it down to 720p is not a problem with cameras as I do this all the time and it looks just fine. When I do this I may also be mixing 1080p and 720p raw images, and I always scale to the lowest resolution. When I render a 1080p I will generally render a 720p version as an answer print to view before committing the final cut to a 1080p render. I say this with caution because I have not worked with screencasts or using screen motion capture software.</p>

<p>However, shooting in 720p and trying to make it into 1080p is a losing battle because then you are asking software to make up for pixels that do not exist. You can try it, but I don't think you will like it unless someone makes a software product something like Twixtor that adds frames needed for extra slow motion shots.</p>
","4619"
"How do I move (rearrange) the orientation of a video? Using ffmpeg","456","","<p>I want to crop the video at some position and move the part next to the part that wasn't cropped. (See image, it explains better)</p>

<p><a href=""https://i.stack.imgur.com/k2UeY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k2UeY.png"" alt=""enter image description here""></a></p>
","<p>This should do it:</p>

<pre><code>ffmpeg -i input.mp4 \
-filter_complex ""[0:v] crop=iw/3:ih:0:0,pad=iw:2*ih [left]; \
 [0:v] crop=iw/3:ih:(iw/3)+1:0 [middle]; \
 [0:v] crop=iw/3:ih:(2*iw/3)+1:0 [right]; \
 [left][middle] overlay=0:main_h/2,pad=iw:(3*ih/2) [out1]; \
 [out1][right] overlay=0:(2*main_h/3) [fout] "" \
  -c:v libx264 -an -map ""[fout]"" trisected-vertical-overlay.mp4
</code></pre>

<p>I haven't supplied any specific video or audio encoding parameters. This just shows the overlay filter to use. You'll have to encode and map the audio, as needed.</p>
","16557"
"Transcoding 100+ files from H.264 to H.265","455","","<p>I am in a process of transcoding 100+ High <strike>Quality</strike> Bitrate 720p <a href=""https://en.wikipedia.org/wiki/Matroska"" rel=""nofollow noreferrer"">MKV</a>s containing <a href=""https://en.wikipedia.org/wiki/X264"" rel=""nofollow noreferrer"">H.264 (x264)</a> to <a href=""https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding"" rel=""nofollow noreferrer"">H.265 (x265 / HEVC)</a>.</p>

<p>I do this with <a href=""https://en.wikipedia.org/wiki/HandBrake"" rel=""nofollow noreferrer"">HandBrake</a> software under Linux <a href=""https://en.wikipedia.org/wiki/Debian"" rel=""nofollow noreferrer"">Debian</a> 9 on <a href=""https://ark.intel.com/products/75461/Intel-Xeon-Processor-E3-1225-v3-8M-Cache-3_20-GHz"" rel=""nofollow noreferrer"">Intel Xeon E3-1225 v3 3.2GHz 4-core</a>.</p>

<p>Various versions follow.</p>

<p>Kernel:</p>

<pre><code>4.8.0-1-amd64
</code></pre>

<p><a href=""https://en.wikipedia.org/wiki/HandBrake"" rel=""nofollow noreferrer"">HandBrake</a>:</p>

<pre><code>0.10.5 (x86_64)
</code></pre>

<p><a href=""https://en.wikipedia.org/wiki/High_Efficiency_Video_Coding"" rel=""nofollow noreferrer"">x265</a>:</p>

<pre><code>2.1-2
using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX AVX2 FMA3 LZCNT BMI2
</code></pre>

<p>I have these settings applied, according to <a href=""https://en.wikipedia.org/wiki/MediaInfo"" rel=""nofollow noreferrer"">mediainfo</a>:</p>

<pre><code>Encoding settings: wpp / ctu=64 / min-cu-size=8 / max-tu-size=32 / tu-intra-depth=3 / tu-inter-depth=3 / me=3 / subme=4 / merange=57 / rect / amp / max-merge=4 / temporal-mvp / no-early-skip / rskip / rdpenalty=0 / no-tskip / no-tskip-fast / strong-intra-smoothing / no-lossless / no-cu-lossless / no-constrained-intra / no-fast-intra / open-gop / no-temporal-layers / interlace=0 / keyint=240 / min-keyint=24 / scenecut=40 / rc-lookahead=40 / lookahead-slices=0 / bframes=8 / bframe-bias=0 / b-adapt=2 / ref=5 / limit-refs=1 / limit-modes / weightp / weightb / aq-mode=1 / qg-size=32 / aq-strength=1.00 / cbqpoffs=0 / crqpoffs=0 / rd=6 / psy-rd=2.00 / rdoq-level=2 / psy-rdoq=1.00 / log2-max-poc-lsb=8 / no-rd-refine / signhide / deblock=0:0 / sao / no-sao-non-deblock / b-pyramid / cutree / no-intra-refresh / rc=crf / crf=21.0 / qcomp=0.60 / qpmin=0 / qpmax=69 / qpstep=4 / ipratio=1.40 / pbratio=1.30
</code></pre>

<p>In short I have applied Constant Quality RF21 and at <a href=""https://x265.readthedocs.io/en/default/presets.html#presets"" rel=""nofollow noreferrer"">veryslow preset</a>.</p>

<p>Originals are all at fixed 2 GiB size. The sizes my encodes differ (you can deduce how much time it took too from the file list):</p>

<pre><code>494M Dec  7 07:02 S05E16.mp4
551M Dec  7 00:14 S05E17.mp4
654M Dec  6 16:11 S05E18.mp4
668M Dec  6 08:10 S05E19.mp4
</code></pre>

<p>The original files have these settings applied:</p>

<pre><code>Encoding settings: cabac=1 / ref=5 / deblock=1:0:0 / analyse=0x3:0x133 / me=umh / subme=7 / psy=1 / psy_rd=1.00:0.00 / mixed_ref=1 / me_range=16 / chroma_me=1 / trellis=1 / 8x8dct=1 / cqm=0 / deadzone=21,11 / fast_pskip=0 / chroma_qp_offset=-2 / threads=12 / sliced_threads=0 / nr=0 / decimate=1 / interlaced=0 / bluray_compat=0 / constrained_intra=0 / bframes=3 / b_pyramid=2 / b_adapt=1 / b_bias=0 / direct=1 / weightb=1 / open_gop=0 / weightp=2 / keyint=250 / keyint_min=23 / scenecut=40 / intra_refresh=0 / rc_lookahead=40 / rc=2pass / mbtree=1 / bitrate=5670 / ratetol=1.0 / qcomp=0.60 / qpmin=0 / qpmax=69 / qpstep=4 / cplxblur=20.0 / qblur=0.5 / ip_ratio=1.40 / aq=1:1.00
</code></pre>

<p>The question is, is the CPU time <em>well spent</em>? I mean, can I do any obvious optimizations to speed things up without having lower quality and / or larger files as a result?</p>

<p><strong>EDIT1</strong>:</p>

<p>Citation from <a href=""https://x265.readthedocs.io/en/default/presets.html#presets"" rel=""nofollow noreferrer"">the official source</a>:</p>

<blockquote>
  <p>x265 has ten predefined --preset options that optimize the trade-off between encoding speed (encoded frames per second) and compression efficiency (quality per bit in the bitstream). The default preset is medium. It does a reasonably good job of finding the best possible quality without spending excessive CPU cycles looking for the absolute most efficient way to achieve that quality. When you use faster presets, the encoder takes shortcuts to improve performance at the expense of quality and compression efficiency. When you use slower presets, x265 tests more encoding options, using more computations to achieve the best quality at your selected bit rate (or in the case of crf rate control, the lowest bit rate at the selected quality).  </p>
</blockquote>

<p><strong>EDIT2</strong>:  </p>

<p>Hardware configuration</p>

<hr>

<p>Type: Dedicated server of mine. Currently used only to transcode videos.</p>

<p>CPU: <a href=""https://ark.intel.com/products/75461/Intel-Xeon-Processor-E3-1225-v3-8M-Cache-3_20-GHz"" rel=""nofollow noreferrer"">Intel Xeon E3-1225 v3 3.2GHz 4-core</a></p>

<p>GPU: No dedicated card, only integrated</p>

<p>RAM: 32GB ECC 1600MHz</p>

<p>Disks:</p>

<ol>
<li><p>SSD; used for system</p></li>
<li><p>HDDs in RAID6; used for writing the output videos</p></li>
<li><p>RAMDisk 20GB; used for reading the source videos</p></li>
</ol>

<p><strong>EDIT3</strong>:</p>

<p>With regard to the comment about swapping the memory, I have had set <a href=""https://en.wikipedia.org/wiki/Swappiness"" rel=""nofollow noreferrer""><code>vm.swappiness</code></a> = 1.</p>
","<p>The whole one day I dedicated to testing various scenarios. The following applies to this particular series in this particular quality, so <strong>I don't say it applies generally</strong>. So in this case the following applies:</p>

<ul>
<li><p><strong>file sizes</strong> at the very same settings with only <code>medium</code> and <code>veryslow</code> preset changed vary negligibly: ~ -10 MiB on average in favor of <code>veryslow</code> preset</p></li>
<li><p><strong>visual quality</strong> at a very close look is rather the same in either <code>medium</code> or <code>veryslow</code> preset</p></li>
</ul>

<p>One thing solved. I have changed the preset to <code>medium</code>.</p>

<hr>

<ul>
<li>at a very close look, <strong>the source videos are not in such a high quality as I thought</strong>, that explains why x265 compresses to almost 1/4 of the original size</li>
</ul>

<p>Another mystery solved.</p>

<p>Provided I now transcode at 25 fps, I have raised the RF to 20.</p>

<hr>

<ul>
<li><strong>RAMDisk</strong> does not provide benefit in this case, both for <code>medium</code> and <code>veryslow</code> preset; I ended up with RAID6 as the source and SSD as the output drive</li>
</ul>

<p>The inconvenience of moving files to RAMDisk solved.</p>

<hr>

<p>To answer the question, <strong>the CPU time was not spent well at all</strong>.</p>
","20078"
"Why do higher frame rates require lower resolution?","455","","<p>I've noticed that when comparing video camera specifications, higher frame rates usually require a lowered resolution.  For example, the GoPro Hero4 Silver can get 15fps at 4k, 60fps at 1080p, 120fps at 720p, and 240fps at vga.  I would think that the resolution of the individual frames would be independent of the rate of capture.  Is this because of bitrate capabilities of internal cables, or an encoding limit on the internal circuitry?  Thanks for your help.</p>
","<p>I am not an engineer, but I think you've answered your own question. </p>

<p>This is a bit of an oversimplification, but if <strong>t</strong> is the time it takes to record, process and store a pixel from a frame consisting of <strong>p</strong> pixels, then 1/(<strong>t</strong> * <strong>p</strong>) is your maximum frame rate. So if you increase the framerate above the maximum you need to decrease either <strong>t</strong> or <strong>p</strong>. And since <strong>t</strong> is fixed you have to decrease <strong>p</strong> - the number of pixels.</p>
","15926"
"FFMPEG - Image before video","451","","<p>I need to put an image before a video with FFMPEG. The image should not be instead of the video but before. The best is if I can choose a number of frames to show the image, instead of seconds. I'm going to run the command via PHP exec.</p>

<p>I tried some commands but could not get them to properly work via exec or directly via CMD.</p>

<p>So tips on how this could be achieved is appreciated.</p>

<p><strong>How can I make the output an .mxf file? When I only change the extension of output file I get:</strong></p>

<pre><code>    Could not write header for output file #0 &lt;incorrect code parameters ?&gt;: Error number -1 occurred
</code></pre>

<p>Command used:</p>

<pre><code>    ffmpeg -r 30 -loop 1 -t 3 -i image.jpg -i video.mxf -f lavfi -t 3 -i aevalsrc=0 -filter_complex ""[0:v] [2:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]"" -c:v libx264 -c:a aac -strict -2 -map ""[v]"" -map ""[a]"" out.mxf
</code></pre>

<p>FFprobe output:</p>

<pre><code>    ffprobe version N-74751-gb54e03c Copyright (c) 2007-2015 the FFmpeg developers
  built with gcc 4.9.3 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-decklink --enable-zlib
  libavutil      54. 31.100 / 54. 31.100
  libavcodec     56. 59.100 / 56. 59.100
  libavformat    56. 40.101 / 56. 40.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5. 40.100 /  5. 40.100
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  2.101 /  1.  2.101
  libpostproc    53.  3.100 / 53.  3.100
[mxf @ 000000000310b8a0] broken or empty index
Input #0, mxf, from 'C:/video.mxf':
  Metadata:
    uid             : 8206ba85-3954-2f4a-9d3c-c4e8d5ba793a
    generation_uid  : de90d87a-f4a7-944c-b6c1-368903cb7a3f
    company_name    : Rhozet
    product_name    : Carbon
    product_version : 1.00
    product_uid     : 060e2b34-0401-0101-0e00-000000000000
    modification_date: 2015-10-30 15:45:05
    material_package_umid: 0x060A2B340101010501010D33130000009BC9C7D36B3F43438EAECC12C87EB630
    timecode        : 00:00:00:00
  Duration: 00:00:42.04, start: 0.000000, bitrate: 36660 kb/s
    Stream #0:0: Video: mpeg2video (4:2:2), yuv422p(tv), 1920x1080 [SAR 1:1 DAR 16:9], 50000 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 59.94 tbc
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:1: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:2: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:3: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:4: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:5: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:6: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:7: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:8: Audio: pcm_s24le, 48000 Hz, 1 channels, s32 (24 bit), 1152 kb/s
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
    Stream #0:9: Data: none
    Metadata:
      file_package_umid: 0x060A2B340101010501010D3313000000B70F9172F67EE340AE91E11FC1BA4B0B
      data_type       : vbi_vanc_smpte_436M
Unsupported codec with id 0 for input stream 9
</code></pre>
","<p>Use this:</p>

<pre><code>ffmpeg -r 25 -loop 1 -t 3 -i ""image"" -i ""video"" -f lavfi -t 3 -i aevalsrc=0 -filter_complex ""[0:v] [2:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]"" -encoding_parameters -map ""[v]"" -map ""[a]"" output.mp4
</code></pre>

<p>Ideally set 'r' of image to frame rate of video. The 'r' will be the framerate of the output as well.</p>

<p>The 't' before the image input and the dummy audio input should be the same and represents the image hold in seconds.</p>

<p>The image should be the same resolution as the video. Else, scale (and pad) one or the other in the filter chain before the concat operation.</p>

<p><strong>Edit</strong>: Here's a commandline with some common encoding parameters set</p>

<pre><code>ffmpeg -r 25 -loop 1 -t 3 -i ""image"" -i ""video"" -f lavfi -t 3 -i aevalsrc=0 -filter_complex ""[0:v] [2:a] [1:v] [1:a] concat=n=2:v=1:a=1 [v] [a]"" -c:v libx264 -c:a aac -strict -2 -map ""[v]"" -map ""[a]"" output.mp4 
</code></pre>
","16824"
"Stack images with ""hstack""-filter from video or image sequence in ffmpeg","451","","<p>Is it possible to input a video and extract images every second while simultaneously stack them horizontally or vertically with the <strong>""hstack""</strong> filter to create one jpg image?</p>

<p>The only way I can figure out how to do it is by extractning the still-frames with this command:</p>

<pre><code>ffmpeg -i INPUT.mov -vf fps=1 -s 192x108 thumbnails%03d.jpg
</code></pre>

<p>And then inputting them one by one and then specifying then number of inputs like this.</p>

<pre><code>ffmpeg -y -i thumbnails001.jpg -i thumbnails002.jpg -i thumbnails003.jpg... filter_complex vstack=inputs=3 output.jpg
</code></pre>

<p>It would be nice if it was possible to do in one more dynamic chunk from the original video or at least from one inputted Video-Sequence and not manually inserting the number of INPUTS needed to create the Stack. </p>
","<p>You're better off using the <code>tile</code> filter for this:</p>

<pre><code>ffmpeg -i INPUT.mov -vf fps=1,scale=192:108,tile=54x1 output.jpg
</code></pre>

<p>The tile argument is the size of a rectangle. So for a 54 second video, <code>54x1</code> produces a horizontal stack of 54 frames. Use <code>1x54</code> for a vertical stack.</p>
","19837"
"Premiere Pro CC: How to reset control panel group settings","449","","<p>I accidentally clicked on ""loose panel group"" in the mirror for creating a new title, but there is no button to fix them together again. I tried resetting the setting, but it doesn't work.</p>
","<p>Is 'loose panel group' a free translation of whatever the option is labeled in whatever language you're using Premiere Pro in? Because I can't find an option or button that says exactly this ...</p>

<p>Anyway, you can change workspaces in the menu, click <strong>Window</strong>  <strong>Workspaces</strong> and select a workspace preset. If you want the default workspace back, make sure to select the <strong>Assembly</strong> preset (in Premiere Pro CC). If you want to revert any changes you made to the saved layout, select <strong>Reset to Saved Layout</strong> from the same submenu or press <kbd>ALT</kbd> + <kbd>SHIFT</kbd> + <kbd>0</kbd>.</p>
","18625"
"What format should I digitize mini DV tapes with HDV footage in?","449","","<p>I want to digitize a lot of mini dv tape footage, shot on a Sony a1(HDV).  What format should I digitize the footage in? (and on a side note can you point me in the direction of what hardware I will need for this)?</p>
","<p>You have a few options for capturing Mini-DV tape.  The first and highest quality, but largest size, is to capture it in HDV format (HDV2 is used by most Sony cameras).  It is a form of MPEG-2 Part 2 video.  This is the same as the format actually stored on the tape.  Storing your files as HDV will preserve the quality of the recorded footage, but it will take quite a lot of space.  This is ideal for high quality archives and any footage that will be used as a source for editing.  It only requires a firewire port and DV capture software (which is also built in to most NLEs.)</p>

<p>If you only care about viewing, then your best option is probably to initially capture to HDV and then transcode to h.264.  h.264 is a high quality compression that you can tweak for the quality/size trade off you desire.  I'd personally recommend either a constant quality or a 2 pass VBR encoding for maximum quality to size.  This doesn't require any specialized hardware beyond the initial HDV capture and can be done entirely in software with free tools (which may even be able to leverage GPU acceleration depending on your video card.)</p>
","13059"
"Best way to increase the display size of a web video","446","","<p>I have a H.264 (ex-MOV, ex-Flash) video of 550px x 250 px. In the HTML player (videojs), I can easily set its size to 640px x 290px and it still looks OK (only a 16.363636% increase in display size). </p>

<p>Or should I increase it's size in ""real pixels"" by exporting it as H.264 from, say, Avanti or Handbrake at a bigger size? In Avanti, under ""Frame size"" I specified 640 x 290 and the file size increased by 8% and data rate from 200 kbps to 215.</p>

<p>What are the trade-offs - would enlarging via HTML be stressful for low-end CPUs? Is the 8% increase in file size and kbps worth it for the extra quality, if any?</p>
","<p>If it is a flash source, then there is a good chance it has vector elements.  You should export at a higher resolution from the source rather than trying to upscale the mov file.  </p>

<p>Working from the original will ensure accurate pixel data for any vector elements and produce a much higher quality upscale.  (This is actually one of the major reasons for Flash's early popularity.)  If that isn't an option or it was a raster format asset in Flash, then it is really a toss up between the two options depending on the quality of the original encoding.</p>

<p>Encoding to a new size can do slower and more accurate upscaling that will produce fewer artifacts, but if the original source you are upscaling from is already pretty low quality, a second generation of encoding loss may very well exceed the amount of quality improvement you get from the rendered out upscale.</p>

<p>So in summary, the best option is to export from Flash at a higher resolution.  If you can't do that, re-encode if you can do it without another generation of loss or have a high enough quality source (200kbps is not high enough for a re-encode most likely).  Finally, if you don't have that, then stick with the re-size in the player on the fly.</p>
","12565"
"Can video have multiple streams (like it has 2 audio streams)","446","","<p>Is that possible, that a video (.mp4 or etc) had two video streams and we could to do work with each stream individually ? (like we do with 2 audio streams and we can remove any of them)..</p>

<p>if so:</p>

<p>1)  do you know which video editors can handle such videos? Adobe Premiere , Sony Vegas, DaVinci or ... ?</p>

<p>2) At this moment, my typical MP4 files are converted by youtube (after upload) to i.e. <code>mpeg-4 10 AVC</code>. And will those 2-stream videos will be converted to 1-stream video anyway? (maybe I have to ask that to YT forum, right?)</p>
","<p>Yes. MPEG-4 Part 14 (the MP4 container format) supports any number of objects (e.g. video, audio and subtitle streams) and isn't limited to only one stream per content type.</p>

<p>In practice however, you seldom see a video file that has multiple video streams, because the range of applications for this is very limited. I would also advise you not to put multiple video streams into one file, as (a) users don't expect it, many won't know how to switch the video track or not notice there is a second video track at all and (b) many players (soft- and hardware) aren't equipped to play files with multiple video streams and either won't have any option to switch between streams or fail to play the file completely.</p>

<p>It is also more common to use Matroska (.mkv) containers for video files with multiple audio/subtitle streams, but even with those, you rarely see files with more than one video stream.</p>
","20937"
"ffmpeg audio to visualisation","446","","<p>I would like to ""convert"" audio to video visualisation (oscilloscope or waveform) sort of like some media players have. </p>

<p>I read this thread on how to do so with ffmpeg: <a href=""https://video.stackexchange.com/q/9644"">How do I turn audio into video (that is, show the waveforms in a video)?</a></p>

<p>I have never used ffmpeg before and having read the previous thread I am still very confused with how to do it. As far as I know, ffmpeg does not have a GUI, so its a bit hard to understand. </p>

<p>I tried the following code:</p>

<pre><code>ffmpeg -i audio.mp3 -filter_complex ""[0:a]avectorscope=s=1080x1080,format=yuv420p[vid]"" -map ""[vid]"" -map 0:a -codec:v libx264 -crf 18 -preset fast -codec:a aac -strict -2 -b:a 192k output.mp4
</code></pre>

<p>But the result i get is just a vertical line (the avectorscope does not spread out).</p>

<p><a href=""https://i.stack.imgur.com/eaO6O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eaO6O.png"" alt=""enter image description here""></a></p>

<p>I have made sure that the audio is in stereo and has a high amplitude. I guess there is something I am missing in the code...</p>

<p>Also I would really like to know how I can change the background color from black to something custom.</p>
","<p>The following command shows a video with a waveform of the corresponding audio:</p>

<pre><code>ffmpeg -i input.mp4 -filter_complex ""[0:a]showwaves=s=1280x720,format=yuv420p[vid]"" -map ""[vid]"" -map 0:a -codec:v libx264 -crf 18 -preset fast -codec:a aac -strict -2 -b:a 192k output.mp4
</code></pre>

<p>I replaced avectorscope with showwaves: avectorscope is for showing the difference between stereo pairs.
Try showcqt,showfreqs,showspectrum for other visualization effects.</p>
","17398"
"Tessellation-like animation in after effects","446","","<p>I am trying to make something I've seen on the local new in the after effects. Now, let's say there's a large picture that has text on it ""WBC"" or something like that and those letters, when zoomed in are made of smaller pictures. I made the large image in Photoshop and it's pretty decent looking but I can't seem to figure out a way to create this tessellation animation that is probably simple since it revolves mainly around zooming in and moving around but I am a novice in After Effects so i need your help. Thanks ;)</p>
","<p>The easiest way to do this is to sync the scaling and make a cross dissolve between them as you do the scaling.  You want it to fade fairly quickly as you work your way in.  One layer would be the many images in whatever arrangement you want, the other would be the overall logo.  This is also often the technique used for doing a smooth zoom in on a satellite image from a very high level to a lower level.</p>

<p>If you time the fades right, it isn't that noticeable as you move to progressively more detailed images because the more detailed image is starting in while it is still small enough to not be distinguishable.</p>

<p>To keep the sizes manageable, you may also have to do multiple images to transition through.  Don't bring one in to the timeline until the previous image is big enough that details of it will start being noticeable.  Also, be sure to remove the larger previous image from the timeline once the newer image completely covers it.</p>
","12654"
"Scroll text up (like in credits) then reverse and scroll text down?","445","","<p>Unsure if this is the right ""sub-Stack Exchange"" for this question, however I've been unable to find a way to scroll a block of text up then reverse it and scroll down after similar to ""The Future of Publishing - by DK (UK)""</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/Weq_sHxghcg?start=0""></iframe>
            </div></div></p>

<p>Or ""Lost Generation""</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/42E2fAWM6rA?start=0""></iframe>
            </div></div></p>

<p>I've tried with Movie Maker (I have a Windows 8 laptop) but if there's some other program that I can get this done with then I'm happy to try it out.</p>
","<p>After Effects or just about any titling software should allow this easily.  You just need to make a layer with all the text on it (or even an image) then you simply need to apply a motion effect to scroll from the top to bottom, put a keyframe in the middle and then the keyframe at the end should bring it back.  This should produce exactly what you are looking for.</p>
","9394"
"Add border to video, maintaing aspect ratio, and insert subtitles in the border","443","","<p>I have an MKV wrapped 1920x800 video and a sidecar .srt subtitle file. I want to convert the video to 1920 x 1080 and add black border on top and bottom while keeping the original aspect ratio. I also want to insert the subtitles on the black border.</p>

<p>I have an idea on how to insert the subtitles, but I'm not sure how to add the border. I'd appreciate any suggestions on how to do this all in one pass so that I lose at little quality as possible.</p>
","<p>You can use <a href=""http://ffmpeg.org/download.html"" rel=""nofollow"">FFmpeg</a>, a free command line tool to do this:</p>

<pre><code>ffmpeg -i input.mkv -vf pad=1920:1080:0:140,subtitles=filename.srt -crf 20 -c:a copy out.mkv
</code></pre>

<p>The <a href=""https://ffmpeg.org/ffmpeg-all.html#subtitles-1"" rel=""nofollow"">subtitles</a> filter accepts ASS styling <a href=""http://fileformats.wikia.com/wiki/SubStation_Alpha"" rel=""nofollow"">parameters</a>, such as font size and margins.</p>
","18867"
"Automated rendering of After Effects or similar","442","","<p>Initially posted on <a href=""https://graphicdesign.stackexchange.com/questions/70707/automated-rendering-of-after-effects-or-similar"">graphicdesign.stackexchange.com</a>:</p>

<p>I'm looking for a way to integrate animations into our existing product. The animations would essentially be 30 second commercials containing some variable data (names, dates, numbers). The system needs to automatically render the animation when the user pushes a button. These are non-technical users, so it needs to be all very black-box.</p>

<p>It looks to me like we could use After Effects for this, but I am not a designer so I'm looking for confirmation and suggestions from the experts. This is how I see it working:</p>

<ul>
<li>We have a designer create an AE project for each desired animation. </li>
<li>The AE project is set up to pull the variable data from a file at render time. </li>
<li>When our user pushes the button, the system generates the data file and puts it into a folder known to the AE project. </li>
<li>The system then kicks off the render using <a href=""https://helpx.adobe.com/after-effects/using/automated-rendering-network-rendering.html"" rel=""nofollow noreferrer"">aerender.exe</a>.</li>
<li>When rendering is complete, the system shows the animation.</li>
</ul>

<p>Does this sound like a reasonable way to proceed? Any alternative suggestions?</p>

<p>One area I'm concerned about is the data merge part. Can an AE project be set up to automatically do that at the time of rendering (i.e. initiated by <a href=""https://helpx.adobe.com/after-effects/using/automated-rendering-network-rendering.html"" rel=""nofollow noreferrer"">aerender.exe</a>)?</p>
","<p>After Effects can be automated using the Adobe extendscript scripting language,  and can be run from the command line, meaning that it can be integrated into other processes.</p>

<p><a href=""http://www.adobe.com/devnet/aftereffects.html"" rel=""nofollow"">Extendscript</a> is JavaScript and it works across Adobe's product line (to varying degrees, you can use it to script Photoshop for instance). If you have a JavaScript developer available then this would be a relatively straightforward task.</p>

<p>There are also third party frameworks such as <a href=""http://dataclay.com/"" rel=""nofollow"">Dataclay</a> that make the process easier for non-technical users to avoid scripting, but they are relatively pricey, though they might work out cheaper than hiring a developer and rolling your own.</p>
","18327"
"Is ok to post on youtube or vimeo custom resolution videos?","442","","<p>I'm having the need of post some screen capture content where some cropping is applied (eg: 850x620) and/or the original monitor resolution is not full hd 1920x1080 but could happen i have to use directly a 1600x900, 1024x768, 1600x1200 and so on.</p>

<p>Uploading videos in h264 but with a video resolution like that will end in a serious quality loss?</p>

<p>What could be a best practice? Give to youtube, vimeo and other services a classic full hd video adding a black frame to match the size or is possible upload a file in native res?</p>

<p>Main goal is keep everything as sharp as possible.</p>
","<p>A custom video resolution has no bearing on quality here; only the choice of encoder, bitrate/rate factor and other parameters will impact the final result.</p>

<p>Youtube's player frame on its site is always 16:9, so if your video has an aspect ratio other than 16:9, YT will automatically add black bars as necessary. No need for the uploader to do so. However, if you decide to embed videos hosted on Youtube elsewhere, you can specify a custom frame size in your embed code which matches the aspect ratio of your uploaded video. In which case, your video will be displayed with no black bars. So, best practice is to never manually append black bars.</p>
","16705"
"Is there a program that can change monitor color temperature for lighting?","442","","<p>I'm shooting a scene for a film where the actor will be placed in front of a computer monitor. I want to use the monitor to light the actor, but I want to be able to adjust the color temperature to get the look I want. I have F.lux (<a href=""https://justgetflux.com/"" rel=""nofollow"">https://justgetflux.com/</a>), so I know changing monitor color temperature is possible, but I want to be able to control it. Preferably the program would be free, and I would be able to adjust the temperature in real time.</p>
","<p>Thanks for the other good answers. Here is what I ended up doing.</p>

<p>I created a blank white html page and made it fullscreen, and then used f.lux to adjust the color temperature. It worked for me, but I should point out to anyone looking to use this solution in the future that it isn't perfect. F.lux doesn't give a very broad range of color temperatures. They were enough for my purposes, but someone looking for a very warm or very cool look might not be satisfied. In addition, f.lux changes the color temperature for the entire desktop on all monitors, so if you have something color-sensitive running during the shoot, like using the computer as a monitor, your colors will be off on that as well.</p>
","15339"
"Cropping VR video afterwards","440","","<p>I have mp4 SBS 180 VR videos made for Oculus Rift. I'd like to crop the videos a little (simulate the zoom) and then downscale to ""cardboard"" resolution so it would be smoothly playable on mobile devices.</p>

<p>Some mobile players have already zoom function built in, but this should work as a workaround for keeping the picture sharp (working down from the big resolution) at the expanse of FOV.</p>

<p>Pretty soon I realized I can't simply crop the whole video. I have to crop both sides separately and then join them together as they were.</p>

<p>Could you point me to video-editing software (preferably free and fast to learn) that would allow me to do such thing?</p>

<p><a href=""https://i.stack.imgur.com/t3m7u.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t3m7u.jpg"" alt=""enter image description here""></a></p>
","<p>Using the mask image below, you can use <a href=""https://ffmpeg.org/download.html"" rel=""nofollow noreferrer"">ffmpeg</a> to crop and output a MP4 with the visual content bounded in the same elliptical shape as the mask</p>

<pre><code>ffmpeg -i input.mp4 -loop 1 -i 360mask.png \
       -filter_complex ""[0]split[a][b]; \
        [a]crop=iw/2:1522:0:300,crop=iw*0.8:ih*0.8,scale=960:-1,pad=960:960:0:150[l]; \
        [b]crop=iw/2:1522:iw/2:300,crop=iw*0.8:ih*0.8,scale=960:-1,pad=960:960:0:150[r]; \
        [l][r]hstack[c]; [c][1]alphamerge[ca]; [1][ca]overlay=shortest=1""    vr.mp4
</code></pre>

<p>The video is split into two cropped halves to a rectangle which exactly bounds the visual area, then cropped to 80% along each of its dimensions, and then scaled and padded to a 960x960 canvas, rejoined to form a 1920x960 frame. Then the mask is applied to form the elliptical cut-out.</p>

<h3>Mask</h3>

<p><a href=""https://i.stack.imgur.com/kgCmO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kgCmO.png"" alt=""enter image description here""></a></p>

<hr>

<p>This is the basic technique. The visuals in your supplied image aren't centered, so I customized the crop expressions based on measurements within an image editor.</p>
","18425"
"How to convert HDV to dvd full resolution 640 x 480?","439","","<p>I have some content recorded in hdv, which is anamorphic. I only recently came to understand what that means exactly. Concerning the video size, it is 1440 x 1080, but the pixels are not square; they are wide. The effect is that the video displays the same as full HD 1920 x 1080.</p>

<p>I currently don't have a need for these particular videos to be in HD. Standard DVD definition is suitable. I also prefer that they are cropped to full size frame, instead of widescreen. So I would like to convert them to 640 x 480.</p>

<p>I have used handbrake to try and accomplish this but I am not sure how to make the best settings for this. Here's what I have come up with, but I am not sure what exactly I am getting out of it:</p>

<p><img src=""https://i.stack.imgur.com/4VwEm.png"" alt=""enter image description here""></p>

<p>I don't necessarily mind if the video is anamorphic or not as long as it consistently displays 640 x 480. I don't know if it makes a difference, but this will be web videos in an HTML5 player. Currently, the player is designed to declare only the video width, so the height is whatever the ratio-correct number should be. I honestly don't know if this works for anamorphic videos.</p>

<p>All I really want here is the best settings to convert this HDV to an Mp4 that will be shown in an HTML5 player with the full screen aspect ratio at a maximum resolution of 640 x 480.</p>
","<p>Whenever you publish something to end-users you ALWAYS want square pixels, everything else WILL make problems.</p>

<p>So just convert your anamorphic video to a square pixel aspect ratio. Handbrake should automatically handle that, the settings you have right now should output a video with square pixel aspect ratio. Just add the vertical resolution.</p>

<p>Is there any reason why you would want to crop your video to 4:3? A down-scaled 16:9 square pixel resolution would equal to 640x360 for a ""SD"" resolution (bare in mind there are several different DVD standards) and I would suggest you to use that resolution and don't just crop your video unless you have an artistic reason for that.</p>

<p>If handbrake is unable to handle the non-square pixels you will have to reside to use either a full fledged video editor or alternatively FFmpeg.</p>
","12016"
"Best HD camcorder for recording sports","438","","<p>Im looking to start recording sporting events, specifically professional wrestling and mixed martial arts (MMA) events. Initially these will be recorded for DVD, but one of the events will be self-produced and Id like it to be of good quality that I could potentially sell it to a local TV station.</p>

<p>What should I look for in a HD camcorder for recording such fast-paced events?</p>
","<p>Nothing really. All recent camcorders are more than enough for what you want.</p>

<p>Two things to look: </p>

<ol>
<li>That shutter speed can change so that you can define how smooth motion you want</li>
<li>That they offer fast zoom/focus capabilities for sports.</li>
</ol>

<p>Any recent cam-corder from major companies already covers this.</p>

<p>If you want quality sound you should also look for a camcorder that has an external mic jack so that you can connect your own microphone. This raises the budget a lot.</p>

<p>Unless you are a professional I would stay away from using a DSLR for sports filming.</p>

<p>I personally own a panasonic-v250. I also bought it for sports filming.</p>
","15898"
"Transfer project from one application to another","432","","<p>As a beginner, I would like to know how important it is which software I decide to start out with. (Currently eyeing Adobe Premiere Elements and Sony Movie Studio 13).</p>

<p>I realize that whichever one I'll use, if I decide to switch application, I'll need to learn the other's interface and terminology. However, I was wondering about something else - Can I transfer a project from one to the other. Say I want to make a change to a video I created in one application, and obviously I don't want to input the created video (both because I lose resolution, and because I want each sound track separate etc.) - is it possible to transfer the video to other video editors, or will this project be ""stuck"" to this editor?</p>
","<p>There are ways to transfer projects between different video editing programs, but they are <strong>very</strong> limited. Take a look at <a href=""https://video.stackexchange.com/questions/14885/how-can-i-save-the-project-for-premiere-cs-5-5/14887#14887"">my answer here</a>; this question is about transfering projects between different versions of Premiere Pro, but the answer is basically the same. You can export a project as an <a href=""http://en.wikipedia.org/wiki/Edit_decision_list"" rel=""nofollow noreferrer"">Edit Decision List (EDL)</a>, however this will only contain the arrangement of the clips in the timeline (namely your <em>edit decisions</em>, hence the name), any effects, e.g. color-grading, will be lost. Another format that does basically the same is <a href=""http://en.wikipedia.org/wiki/Advanced_Authoring_Format"" rel=""nofollow noreferrer"">AAF</a>. I'm not sure about the differences between both formats, but either way, most of your post-production efforts will be lost when transfering the project to a different program.</p>

<p>Also, this applies to prosumer-/production-level software. I'm not sure whether ""low-end"" software supports importing and exporting EDL-files. If I remember correctly, it's not possible with Premiere Elements; don't know about the Sony programs (you should be able to find that out over the developer's websites and/or Google).</p>
","15158"
"Is there a way to make Premiere reuse nested previews?","431","","<p>I have a rather effect-heavy clip in my project. I made it a separate sequence using <em>nest</em> from the context menu and rendered its preview to see it's ok.</p>

<p>Now that it's rendered(shows green on top of the timeline), I go to the main sequence and notice that Premiere has to render the nested sequence again.</p>

<p>My question is, is there any way to make Premiere reuse these already rendered previews in a nested sequence either through native settings or a plugin?</p>

<p>I'm using Premiere Pro CC on Win 8.1</p>
","<p>I don't think there is a way to accomplish this, since you can apply effects on nested sequences as well as to single clips, which means the nested sequence might look different in the main sequence than it does in the nested one. My humble guess is that that is also the reason why Premiere does not use the existing preview files.</p>

<p>A workaround would be to export the nested sequence and re-import the rendered file in your project, replacing the nested sequence. This of course means you won't be able to make changes to that part of your video anymore, as well as a slight loss in quality (unless you export the sequence using a visually lossless codec such as ProRes or DNxHD) ...</p>
","12293"
"Which technology do YouTube and Vimeo use for processing 1080 HD Video?","430","","<p>For me personally seems youtube videos crisper in 1080 or ""original size"".</p>

<p>Vimeo seems a little fuzzy when HD is on.</p>

<p>What technology does Youtube use, and why is it apparently better? What is the difference between the two technologies?</p>
","<p>They both are powered by flash, which can read most file types, so I think it really has to do with the way and speed it is buffered.</p>
","5824"
"Creating a ""finger using device"" effect like in iPhone app ads","428","","<p>I have been searching for quite a while on a tutorial of how to create the rotating phone ad with the finger coming in to interact with the app once in a while.</p>

<p>There is an example of this effect in <a href=""http://www.youtube.com/watch?v=_ejvLxKBuYA"" rel=""nofollow"">this ad</a> starting around 8 seconds in...</p>

<p>Can anyone please point me to a tutorial of how to achieve this effect or perhaps just explain how if they can't find one?</p>
","<p>ok.
lets start:</p>

<p>we need a 3d model of a phone, lets say, an iphone (properly modeled, textured), then we need a shot of a moving finger which we will use later on. lets say we have everything.</p>

<p>after we have the idea, what we should do with the finger(adequately having this shots on green screen so we can remove the background) we animate the phone in a 3d package, after we are satisfied with the result, we render the phone sequence out from the package (properly lighten and rendered with alpha channel - ready for compositing)
Open after effects or any other compositing software, we put the a white background as solid, then we put the phone sequence on top of the background, the sequence goes, we put the finger where we want him to interact with the phone, since our phone has no animations on it, we can put them in 3d or we can add them in after effects, a little bit of tweaking and there you are, you got your 3d rotating phone with a finger and a hand interacting with it.</p>

<p>if we want to comp anything with the phone in aftereffects, we need the rotation and camera information from the 3d package, lets say cinema4d, we can export ae proj file out, with all the info wee need, and later use that data in after effects to have the perfect sync. Other way around is to have a properly UV for the phone, and put in animation sequence on the screen material, the render it out, then we have just to sync the figer with the animation.
hope this reveals some magic ;)</p>
","2493"
"I want to have 2.7K footage stabilised in Full HD","428","","<p>I'm solving following situation. I have plenty of footages from my GoPro recorded in 2.7K. 
As I don't need such a resolution, I will be happy to have some from my shoots stabilised and as I need ""only"" Full HD resolution so I can crop a lot, I would like to have it stable as much as possible.</p>

<p>I've tried Warp stabiliser in Priemere, but the software throws me an error ""Warp Stabilizer requires clip dimension to match sequence"" but as I don't want to nest the video and loose pixels...</p>

<p>Can you please help me how to stabilize properly ? Thank you a lot! </p>
","<p>Well you won't loose pixels if you nest it.  Nest it. Then change the sequence settings of your nest to match your clip settings. Then run warp on the clip in the nest. </p>

<p>Back in your main comp you can set the sequence to full HD and just scale it down. You will get the extra sharpness of 2.7k as it will be in the HD timeline.</p>

<p>If you have a majority of the clips in 2.7k then it might be best to edit in 2.7k and drag that composition into a new composition with full HD settings and scale that instead of doing it on a clip by clip basis. Which can get tedious. </p>

<p>Or you can just edit in 2.7k and export in HD.</p>

<p>Side note: you always loose a little bit of pixels if you stabilize (this can be avoided to a certain extent if you adjust the settings albeit at the detriment to the actual stabilization). But downscaling to 1080p it won't be noticeable. Especially with gopro footage. </p>

<p>I've also have mercalli does better with some clips than warp. It's available as a plug in for premiere. </p>
","20156"
"Add path operation to multiple shape layers","427","","<p>I have multiple shape layers, and I want to add <code>Wiggle Paths</code> to all of them.<br>
One way of course is to manually add to each of them.<br>
Is there a way to add it to all of them at once? Specially is there a way to control them all in one place? so if I wanted to change the Wiggle parameters (like size, speed, correlation ...) I can change them all at once?</p>
","<p>Use Expression Slider Controls to drive <strong>multiple Shape Layer Wiggle Path Values</strong>.</p>

<p>Here is an example of driving <code>Size</code> and <code>Detail</code> values of the <code>Wiggle Path Property</code> on <strong>6 Shape Layers</strong> with 2 created <strong>Slider Controls</strong> on a seperate <code>Null Layer</code>:</p>

<p><img src=""https://i.stack.imgur.com/iPOCJ.gif"" alt=""enter image description here""></p>

<p>Note: To save time, it's a good idea to create <strong>one shape layer first</strong>, set it up as described below and <strong>duplicate it afterwards</strong> to maintain the expression links.</p>

<p><img src=""https://i.stack.imgur.com/DPZKs.png"" alt=""enter image description here""></p>

<ol>
<li>Create a <code>Shape Layer</code> and a <code>Null Object</code></li>
<li>Add <code>Wiggle Path Property</code> to the <code>Shape Layer</code> </li>
<li>Add a <code>Slider Control</code> to the <code>Null</code> <strong>for every value</strong> you need for animation <em>(size, speed, correlation ...)</em> and name it like these for a better organization</li>
<li>Select the <code>Null</code> and go to <em>Effect > Expression Control > Slider Control</em></li>
<li>Open up the <strong>properties of the both layers</strong> by clicking on the small arrow <kbd> > </kbd> right beside the layer's name to see the values of the <code>Wiggle Path</code> and the newly created <code>slider control</code> at the same time</li>
<li>Holding down <kbd>Alt</kbd> while clicking on the <strong>stop watch</strong> of a <code>Wiggle Path Value</code> like the <code>Size</code> or <code>Detail</code>, this automatically will insert an <strong>expression</strong></li>
<li>Use the <strong>expression pick whip</strong> and drag it onto the predefined Slider Control of the <code>Null</code> before to link/parent it, for e.g. the <strong>Size of Wiggle Paths</strong>, this should create something like: <code>thisComp.layer(""Name of the Null"").effect(""Size"")(""Name of Slider Control"")</code></li>
<li><strong>Repeat the process of parenting</strong> the <code>Wiggle Path Values</code> to the <code>Sliders</code> as long as all needed values are parented to slider controls correctly </li>
<li>If you duplicate the <code>Shape Layer</code> with <kbd>CTRL</kbd>+<kbd>D</kbd> now, <strong>all expressions will retained</strong> for the duplicated layers</li>
</ol>
","15232"
"Canon DSLR that changes aperture in Video","427","","<p>I purchased a 1100D/Rebel T3 as a starting DSLR.
I like the video aspect the most, but its quite limited.</p>

<p>I have some low aperture lenses and I like them very much, but they make it difficult to focus  when something is moving around.</p>

<p>The only option I have with this camera to change the aperture setting on video is... buying a different lens with a different minimum aperture.</p>

<p>I was wondering what Canon DSLR allows me to change aperture in video mode?</p>
","<p>With canon the first DSLR that can do that is the 550d (you need to set the movie mode to manual in the settings otherwise it will adjust automatically).</p>

<p>However i would suggest something with a swivel screen (600d and up) because it's very useful while shooting video. The 60d and 7d have more options for video (frame rate for one) but it depends on your needs and budget if those are an option.</p>

<p>Hope this is helpful.</p>
","5285"
"How did cameramen manually focus in the film days?","427","","<p>In the days before LCDs and digital cameras, manually focusing an interchangeable lens still camera wasn't a whole lot different than it is today. In most designs, you had a mirror that gave the photographer a through-the-lens view, allowing them to accurately focus. When the shutter button was pressed, the mirror flipped out of the way, exposing the film.</p>

<p>However, this method is obviously not usable for the continuous focusing required by video camera operators. Modern cameras solve this problem by using Live View. How was this problem solved in the film days?</p>
","<p>There are two main ways this was solved:</p>

<h1>1. Measuring Focus</h1>

<p>When using this method, the focus puller measures the distances at every point during the shot, and then pulls focus to these marks, at times making adjustments when an actor or grip misses their mark.</p>

<p>This still occurs today, however is becoming less prevalent due to the filmmaking styles being used (handheld, where the same marks are not hit each time).</p>

<h1>2. Video Assist</h1>

<p>Using one of several methods, including half-silvered mirrors, CCD sensors behind the film negative, and telecine transfers, a video split is made. On modern productions shooting on film, this split is what is sent to 'Video Village', where the director sits.</p>

<h2><strike>Telecine Transfer</strike> (Not the correct term)</h2>

<p>I've never really understood how it worked, until now, so I present this lovely diagram from Wikipedia:</p>

<p><img src=""https://i.stack.imgur.com/FqiaC.png"" alt=""enter image description here""></p>

<p>In essence, the light shines in and exposes the negative. After that, it passes through a series of half-silvered mirrors, each time being split into separate light components and recorded onto an imaging chip. These are them combined, and displayed in the viewfinder, or as a video-out.</p>

<p>One side effect of this method, is the mechanical shutter is visible, as a black screen lasting 1/50th of a second, 25 times a second (whenever the negative itself is not exposed).</p>

<h1>3. Parallel Sight</h1>

<p>Common throughout the 1950s and 60s, having a parallel sight (separate lens, like what you had on a cheap film point-and-shoot camera) allowed the operator to see approximately what was being exposed onto the negative.</p>

<h1>4. Half-Silvered Mirror</h1>

<p><strong>Update: Added from AJ Henderson's comment below.</strong></p>

<p>As mentioned in the comment, a very cheap way of getting an image to a viewfinder was to simply use a half-silvered mirror, somewhere between the lens and the negative. In this way, half the light transmitted through the lens exposes the negative, and half is seen through the operator's viewfinder.</p>

<p>This method does not allow for multiple viewfinders, monitors, or screens, unless another split is created.</p>
","10334"
"Letterbox when converting 4k to 1080p","426","","<p>Got handed a pile of stock footage downloaded from various sites. And found one clip is 4k ( 4096x2160)</p>

<p>I put it into AME to reexport it as DNx115 but when I set it to 1920x1080 23.976 I see letterboxing top and bottom. It says the source clip pixel ratio is 1 and mine is 1.. tried the others for kicks but none get rid of it. </p>

<p>Why am I getting letterbox for this conversion?</p>
","<p>Because 4096 / 1920 = 2.13333 and 2160/1080 = 2.  They are different aspect ratios, so yes, they end up letter boxing when you convert between them.  That is the expected result.</p>

<p>The 4K format you are using is a multiple of 2K and is not the same widescreen aspect ratio as 1080p.  You can't translate between them without letterboxing, though there are several different competing ""4k"" resolutions, so it won't hold true for all ""4k"" formats, but it does for the one you are working with.</p>
","12694"
"Why reencode MP4 to MOV for ""easy post processing""?","426","","<p>On <a href=""https://gist.github.com/KonradIT/ee685aee15ba1c3c44b4#file-convert-multiple-files-md"" rel=""nofollow"">this page</a> author reencodes MP4 from GoPro to MOV for ""easy post processing"". Why the processing becomes easier after that? The author also uses <code>-sameq</code> flag which is gone from <code>ffmpeg</code> without the explanation about what it did for H.264 blocks.</p>
","<p>LordNeckbeard explained in his answer, why the given ffmpeg command line does not provide any known advantage.</p>

<p>But what - in general - could be reasons to transcode?</p>

<ol>
<li><p>Available input codecs/containers of your NLE (video editing software)</p>

<ul>
<li>There are hundreds of codec/container combinations out there, and maybe your NLE has just problems with the one that you have on your disk. Ex- and importing on different computers/NLEs/operating systems may introduce more unsupported combinations. </li>
</ul></li>
<li><p>Playback speed on the timeline
<ul>
<li>This is about inter-frame vs. intra-frame codecs. The article <a href=""http://www.batchframe.com/quick-tips/tip.php?t=3"" rel=""nofollow"" title=""Inter-frame vs Intra-frame | BatchFrame.com"">""Inter-frame vs Intra-frame""</a> has a short explanation:</p>

<blockquote>
  <p>Basically, inter-frame encoding [...] means that a video file contains sets of grouped frames that reference each other in order to produce an image. [...]
  Intra-frame compression means that each frame is individual and contains all of its own information. The file size will be bigger, but the computer doesn't have to look around to find what it needs.</li>
  </ul>
  Examples of inter-frame codecs are the H.264/mpeg family, while Apple ProRes and Avid DNxHD are intra-frame. GoPro Cineform may also be an intra-frame codec, but I couldn't find enough info to verify it.</p>
</blockquote>

<p>But to make it short, you only have to consider transcoding, if your playback speed on the timeline is not smooth.</p></li>
<li><p>Special filters that should be applied before working with the NLE</p>

<ul>
<li>This may be the case, if you have some filters/effects that your transcoding software can apply, but not your NLE. Or your NLE sometimes forgets these effects.</li>
</ul></li>
</ol>

<p>In conclusion, there are some good reasons to transcode, but you should know, if they apply in your case and are worth the effort. Then choose a high quality intra-frame codec, but not mpeg.</p>
","15208"
"How to put this video effect of switching from one scene to another very rapidly?","425","","<p>In many music videos, for example in <a href=""https://www.youtube.com/watch?v=WaV4Bc31OPA"" rel=""nofollow"">T.I. feat. Lil Wayne - Wit Me music video</a> from 1:14 - 1:17, u can see the video effect where the video switches from one scene to another very, very rapidly, syncing with that instrument/beat, what is this video effect called and how to put this video effect? </p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/WaV4Bc31OPA?start=0""></iframe>
            </div></div></p>
","<p>It's not an effect per se, it's plain old fast editing cuts. Some scenes have 1 frame, other 2 or 3 or more:</p>

<p><a href=""https://i.stack.imgur.com/bJUEl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bJUEl.png"" alt=""timeline cuts""></a></p>

<p>Just put two scenes on the timeline and go cutting from one to another, and aligning them with the sound waveform as you feel like. Beware that this should be used in really small doses of few seconds, otherwise it becomes an unpleasant stroboscopic effect.</p>
","16583"
"What equipment should I get for starting out making professional quality films?","425","","<p>I want to start two short film projects but I don't know what equipment I need to get a good professional quality result. I've been saving some money so I hope budget is not a problem. Also, what's most commonly used to record audio in this kind of project? I want to get the sound from two people talking with the camera placed far away from them. I'm trying to make an independent production company so I'm seeing this as an investment. The cheaper the better though.</p>
","<p>For videography, the best bang for your buck is, hands down, going to be an entry level DSLR and a decent lens.  For audio, if your budget can allow for it, I suggest getting a stand alone multi-track recorder.  The Zoom H4n is a particularly popular model with videographers for its low price and decent (though not superb) audio quality.  (Update: The H5 and H6 are now available offering significant upgrades over the H4n, but also at a slightly higher price point.)</p>

<p>There are far better options and values as you get more skill in the field, however for starting out, you probably don't want to invest too much until you get some experience under your belt.  It's easy to spend upwards of $10 to $15 grand on a nice videography setup as you get more and more professional, but a $1500-$1700 setup with something like a Canon T4i or Lumix G2 with a moderate level lens and an H4n or similar for recording audio (with some basic shotgun or lavaliere mics) is a decent start.</p>

<p>Also, be sure not to skimp on a tripod.  You'll want a solid, sturdy tripod (probably aluminum) with a good fluid tripod head to make sure you can get steady shots.  Just a solid tripod alone can run you in to the $350 to $500 range.</p>

<p>If you'll be shooting indoors, you may also need a light kit.  A cheap 3 point lighting kit can be had for a couple hundred dollars, though depending on what you intend to shoot, you may be able to get by without this initially.</p>

<p>So overall breakdown I'd recommend starting out is around $500 to $600 on an entry level DSLR, $150 to $300 on a lens (if your DSLR body supports video auto focus, such as the Canon T4i, a stepper motor lens is needed), about $280 for an audio recorder, about $100 to $200 on mics, about $400 on a tripod and optionally about $150-$250 on lights.  Altogether, that's about $1600 or $1850 and you should be able to make some pretty solid professional looking stuff.  </p>

<p>It does add up quickly, but the good news is that you can improve on the gear incrementally over time once you have the basics.  If you do have more than 2 grand to start out with, I'd spend a little bit extra on the lens or the camera body to get better low light performance and better background blur.  Better mics or mic boom arms also couldn't hurt and if you do go with lights, light modifiers or more powerful lights is also never a bad investment, though the best place to spend any extra money is going to depend on the kind of shooting you are planning on doing.</p>
","10069"
"Do I need tripods etc. for filming?","424","","<p>I'm going to make a movie now with a D7000 I'm going to buy. I want it to look as professional as possible, but I don't want to blow the budget. (I'm a 16 years old student..)</p>

<p>What equipment do I really need to make a normal movie with a few action scenes? Is it good enough with a monopod, or do I need a tripod? In that case, do you know a concrete product I can buy? Is a DIY Manfrotto Fig-Rig better than completely hand-held? My budget is around $200, but if it is like as important to spend a bit on this as a good camera, then of course I'll put some more money into it.</p>

<p>Do I need something like this to get smooth shots? <a href=""http://viewitem.eim.ebay.no/NEW-MANFROTTO-701HDV-701-HDV-Pro-Fluid-Video-Head/280535333409/item"" rel=""nofollow"">http://viewitem.eim.ebay.no/NEW-MANFROTTO-701HDV-701-HDV-Pro-Fluid-Video-Head/280535333409/item</a></p>

<p>Thanks for your answer and time!</p>
","<p>A fig rig would be an excellent choice for an action piece. If the fig is out of your budget range look at the Opteka X-Grip. Cheap ($30) and pretty flexible, especially if you mount it sideways instead of with the handle parallel to the lens barrel when the shot needs it.</p>

<p>Also if you have access to After Effects spend some time learning the stabilization and tracking capabilities. That will probably improve your shots, helps with rolling shutter issues too that you may experience on a DSLR.</p>

<p>Borrow a tripod. Lots of tripods sitting around in closets in this world.</p>

<p>Edit: Audio is the achilles heal of DSLR video. Make sure you take that into consideration. You can pick up some decent but cheap shotgun microphones that will plug into the D7000, that will help. Also consider grabbing an H4, or several H1s.</p>
","3264"
"Is there a way to apply an Animation Preset - Text Effect onto a shape?","424","","<p>I have a few bullet points as so:</p>

<p><img src=""https://i.stack.imgur.com/4fWj4.jpg"" alt=""Screenshot of Bullet Point""></p>

<p>I would like to apply an <code>Animation Preset &gt; Text &gt; Blurs &gt; Bullet Train</code> to it... But I want the effect to include the blue circle. I've tried applying directly, parenting, and pre-composing. None had the desired result. Is there a way to get the <code>Text Effect</code> onto the shape layer?</p>
","<p>There sadly does not appear to be a direct way to do it, however you can approximate it by using a Directional Blur on your shape layer with a 90 degree orientation and then applying a fade in.  If you mess with the keyframes a bit, you should be able to get it to look like a constant animation.</p>
","10070"
"Converting videos taken by D5100 for a Youtube upload","423","","<p>What is the best way to compress a video shot by my Nikon D5100 so it can be easily uploaded to Youtube? 
I have a video of around 600MB, I tried converting using Xilisoft video converter but without any success.</p>
","<p>Amazingly, the best place to start with a question like this is Youtube. Believe it or not they have help file that explain the process with quite a bit of detail:
<a href=""https://support.google.com/youtube/bin/answer.py?hl=en&amp;answer=1722171&amp;topic=2888648&amp;ctx=topic"" rel=""nofollow"">https://support.google.com/youtube/bin/answer.py?hl=en&amp;answer=1722171&amp;topic=2888648&amp;ctx=topic</a></p>

<p>On Youtube itself many others have offered up video tutorials on how to do it:
<a href=""https://www.youtube.com/results?search_query=how%20to%20convert%20video%20for%20youtube&amp;oq=how%20to%20convert%20video%20for%20youtube&amp;gs_l=youtube.3..0.638.6986.0.7066.32.16.0.14.14.0.132.1067.13j3.16.0...0.0...1ac.1.11.youtube.RZDyIjFJGEg"" rel=""nofollow"">Youtube.com search</a></p>

<p>I would take the recommendations that YouTube has on the help pages, and simply apply those settings to your favorite video software, it could be something like Adobe Premiere Elements which is a very common video editor, but many others exist.</p>
","7797"
"inserting video clip into another video but keep original audio","423","","<p>I need to replace video in a larger mp4 file with a smaller mp4 clip - but leaving the original (larger clip) audio intact.</p>

<p>So for example I might want the first 2 seconds of SmallClip.mp4 to replace the video in LargeClip.mp4 starting at (say) 10 seconds in to LargeClip.mp4.</p>

<p>The format of both clips is likely to be identical, so I'm hoping I can copy instead of decoding for speed etc (I'll have to do it a lot).</p>

<p>Finally, I might want to do exactly the same thing but in several places in LargeClip.mp4 - all places in the output getting the video replaced with the same input clip's video.  I'd be happy to do this in multiple oerations if neccessary but if it were possible on one command so much the better :)</p>

<p>I'm reading up on the options for ffmpeg but I'm new to it and haven't figured it out yet.  Any help much appreciated!</p>
","<p>You can use the concat demuxer in ffmpeg to do this. For accurate splicing, re-encoding is needed which is what the commands below do.</p>

<p><strong>Step 1</strong> Prepare the concat text file</p>

<p>You must first create a plaintext file, say, <code>concat.txt</code>, like this</p>

<pre><code>file 'LargeClip.mp4'
inpoint 0.0
duration 10.0
file 'SmallClip.mp4'
inpoint 0.0
outpoint 2.0
file 'LargeClip.mp4'
inpoint 12.0
duration 5.0
file 'SmallClip.mp4'
inpoint 23.0
duration 4.0
file 'LargeClip.mp4'
inpoint 21.0
</code></pre>

<p>This shows the first 10 seconds of LargeClip, then the first two seconds of smallclip, then resumes the Largeclip at 12:00, shows it for 5 seconds, then splices in 23:00 to 27:00 of small clip, switches back to LargeClip at its 21:00 and keeps it till its end. These timecodes, in seconds, refer to the source file timecode, not of the assembled output.</p>

<p>Make sure that the sync of LargeClip isn't broken, i.e. after switching to SmallClip for 12 seconds, you want to resume LargeClip 12 seconds later from where you left it off.</p>

<p><strong>Step 2</strong> Concat the video streams</p>

<p>This command will skip the audio since we don't to splice the audio streams.</p>

<pre><code>ffmpeg -f concat -i concat.txt -an -c:v libx264 -crf 23 -fflags +genpts spliced-vid.mp4
</code></pre>

<p><strong>Step 3</strong> Insert the audio from LargeClip</p>

<pre><code>ffmpeg -i spliced-vid.mp4 -i LargeClip.mp4 -c copy -map 0:v -map 1:a Final.mp4 
</code></pre>
","17653"
"Benro and Manfrotto Monopods - QR System","422","","<p>I was looking into some monopods to improve my on-the-run potential for some upcoming projects and I'd like to unify all my gear with one QR system so I can switch from a slider to tripod to monopod with relative ease (if needed). I'm interested in these monopods:</p>

<ol>
<li><p><a href=""http://www.bhphotovideo.com/c/product/945109-REG/manfrotto_mvm500a_pro_fluid_monopod_with.html"" rel=""nofollow"">http://www.bhphotovideo.com/c/product/945109-REG/manfrotto_mvm500a_pro_fluid_monopod_with.html</a></p></li>
<li><p><a href=""http://www.bhphotovideo.com/bnh/controller/home?O=&amp;sku=967703&amp;gclid=CJ3B25_XzL8CFQYxaQodWxQA6Q&amp;Q=&amp;is=REG&amp;A=details"" rel=""nofollow"">http://www.bhphotovideo.com/bnh/controller/home?O=&amp;sku=967703&amp;gclid=CJ3B25_XzL8CFQYxaQodWxQA6Q&amp;Q=&amp;is=REG&amp;A=details</a></p></li>
</ol>

<p>Quality and opinions aside, I'm trying to find out the QR system used for both. What I've found is that Benro ONLY sells the QR plate and not the mounting portion, and Manfrotto has a variety of different QR systems. Can you only adapt Benro products by buying compatible Benro heads (S4 in this case), and which Manfrotto QR system is compatible with the monopod I listed? </p>

<p>Thanks! </p>
","<p>Have you considered buying an entirely separate set of QR plates to unify your entire setup? For instance, in our setup, we use a tripod, a monopod, and a GlideCam with various attachments for each of these (rail systems, teleprompters, etc.). By adding <a href=""http://rads.stackoverflow.com/amzn/click/B0010SIAV2"" rel=""nofollow"">this</a> (relatively) cheap QR plate to the top of each stabilization method, we can switch our cameras around all systems with ease, and the QR plate can go on any standard 3/8"" or 1/4"" screw.</p>
","12365"
"Mix Voice to Video without losing video quality","422","","<p>I have recorded video from Canon HFR 36 camera and recorded voice separately to computer using a clip on mic. (The video camera does not have mic input) Now I want a simple software to remove original audio from the video and add the recorded voice. When I make the edited file, the video quality should not be decreased dramatically.  </p>

<p>I am using windows 8 Operating system. My video files are in MTS format (AVCHD). I recorded audio using windows voice recorder so that voice files are in wma format. </p>
","<p>You can use <a href=""http://www.videohelp.com/tools/tsMuxeR"" rel=""nofollow"">tsMuxer</a> to simply switch out the audio in the MTS without re-encoding the video or audio. MTS is just a <a href=""http://en.wikipedia.org/wiki/Video_container"" rel=""nofollow"">container format</a>, it's the ""housing"" for the video and audio streams. Though your audio has to have the exact same length to be in sync with the video and the audio format has to be supported by the <a href=""http://en.wikipedia.org/wiki/.m2ts"" rel=""nofollow"">M(2)TS file format</a>.</p>
","10855"
"AE - Composition video file/layer missing audio","421","","<p>In my case, I'm working with a file that does have audio, confirmed in the RAM preview and other checks mentioned above, however, I noticed after rendering, the first video file ha no audio (not even in the RAM preview any longer, the video layer is like my other imported video layers and still has the little audio icon with it). I had a audio fade-in on it but it worked before in the RAM preview and my audio fade-out on the last video still works fine. Some little option somewhere got toggled and yeah.... somewhere in there in those many great options AE provides...</p>

<p>The link to my .aep and project files on Google Drive:</p>

<ul>
<li><a href=""http://goo.gl/PJI2nz"" rel=""nofollow"">http://goo.gl/PJI2nz</a></li>
</ul>

<p>and to my audio fade-out working, but no audio in the first half of the video:</p>

<ul>
<li>https://youtu.be/pMf5JLBIPQk?t=1h35m22s</li>
</ul>

<p>Thanks for any help.  :)</p>
","<p>Make sure that audio is on in your output module. In versions prior to CC you had two choices: on and off, recently they've added an automatic function that turns on audio rendering if the comp has audio.</p>

<p>In the render queue you'll see this if you twirl down the output module triangle:</p>

<p><img src=""https://i.stack.imgur.com/8CFYs.png"" alt=""enter image description here""></p>

<p>And you can set audio rendering in the output module window:</p>

<p><img src=""https://i.stack.imgur.com/sv3eu.png"" alt=""output module""></p>

<p>Note that the output module is what determines whether audio gets rendered. It can be on when you do a RAM preview, but if it aint on in the output module, you aint gettin' it.</p>
","15344"
"Properly working with vector files","420","","<p>I have a project that I've edited in Davinci Resolve 12 where I want to add a logo in the beginning. However, Resolve won't import either PDF, SVG or EPS format, so I created a high resolution PNG of the logo instead. Even though the PNG file looks fine on its own when looking at it, when bringing it into Resolve, it gets very pixely.</p>

<p>As a backup solution, I tried importing the original PDF into Final Cut Pro X instead and exporting a MOV file that I could bring into Resolve. However, the logo got pixely when exporting it from FCPX as well.</p>

<p>So, I'm at a bit of a loss here. How could I properly work with vector files so that the result looks smooth?</p>
","<p>Ok, so I solved it myself, and it wasn't pretty.</p>

<p>To be able to work with my vector file in Davinci Resolve 12 (and in Final Cut Pro X without loosing any detail), I first had to convert it into a font. Yes, you read that correctly. I had to create a font where I assigned my logo to a specific character and then use the font in my editing program.</p>

<p>More specifically, to achieve this, I used the excellent <a href=""https://icomoon.io/app/#/select"" rel=""nofollow noreferrer"">Icomoon web app</a> where I could import my vector image, assign it to a character code (in my case, I used ""30"", which stands for numeric zero in standard <a href=""https://en.wikipedia.org/wiki/UTF-8"" rel=""nofollow noreferrer"">UTF-8</a> encoding).</p>

<p><a href=""https://i.stack.imgur.com/s6DID.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s6DID.png"" alt=""enter image description here""></a></p>

<p>Then I simply downloaded my font, installed it on the system as any other custom font, and accessed it as a text object in Davinci Resolve 12.</p>

<p><a href=""https://i.stack.imgur.com/JjwGg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JjwGg.jpg"" alt=""enter image description here""></a></p>

<p>What a time to be alive!</p>
","16888"
"XPS 8900 for Full HD video editing?","419","","<p>I have been doing video for almost 2 years now and currently edit on my father's old ultrabook (i7 quad, 8gb RAM, integrated graphics). Its fine and rendering times are ok, but I recently began toying around with After Effects and the computer begins to lag behind.</p>

<p>I have more or less 1000 dollars to blow and I found a Dell XPS 8900 for around that price. I checked the specs sheet and it seemed really good.</p>

<p>An i7 6700 Quad-Core processor, 16GB of RAM expandable to 32GB, 2TB 7200rpm drive and an Nvidia GeForce GTX 745 with 4GB of RAM. </p>

<p><a href=""http://www.bhphotovideo.com/c/product/1187920-REG/dell_x8900_3131blk_xps_i7_6700_16gb_2tb_windows_10_gtx745.html"" rel=""nofollow"">http://www.bhphotovideo.com/c/product/1187920-REG/dell_x8900_3131blk_xps_i7_6700_16gb_2tb_windows_10_gtx745.html</a></p>

<p>I don't shoot in 4k in my spare time and when they ask me to, I'm not the one editing the footage.</p>

<p>Is this good enough and fast enough? If not, any recommendations for a PC or a laptop is greatly appreciated.</p>
","<p>For editing, I would definitely get a desktop PC, and a custom built one at that, not one from the shelf, as the latter will always be more expensive. Also, this way you only have to pay for the parts you need and can avoid bottlenecks. For example, the PC you mentioned doesn't have an SSD, so hard drive speed will be a limiting factor. Also, the GPU is sub-par compared to the CPU. </p>

<p>Here's what I would recommend, should be doable with 1000 $ (Look out for assembling fees, some dealers will charge you up to 100 bucks, some will do it for 20. You could also do it yourself if you are confident enough):</p>

<ul>
<li>An i7-processor. Since you should be getting a GPU anyway, you can get a Xeon E3 processor instead (basically an i7 without an integrated graphics unit, it's slightly cheaper). For example, I have the <a href=""http://ark.intel.com/products/80910/Intel-Xeon-Processor-E3-1231-v3-8M-Cache-3_40-GHz"" rel=""nofollow"">Xeon E3 V1231 v3</a>.</li>
<li>An nVidia GPU with CUDA support. You could get a GTX 970, even though they are somewhat outdated. Alternatively, you could wait for the new Pascal architecture cards, however those will be much more expensive at launch. Alternatively, you could get an AMD R9 390, which only supports OpenCL but might be ever so slightly more powerful than the GTX 970.</li>
<li>A cheap HDD for storage as well as an SSD for fast editing. You don't need a Samsung Evo, it's a myth that those are better than the cheaper brands. For example, I have a 250GB Crucial BX100 and it's working perfectly fine.</li>
<li>At least 16 GB RAM.</li>
</ul>

<p>It's important that you compare prices and different retailers.</p>
","18489"
"Remote pan/tilt/zoom cameras that aren't expensive","417","","<p>I can't quite tell if shopping/product questions are allowed here, but...I'm not sure where else to turn on the SE network.  I have no a/v experience.</p>

<p>I'm trying to gather info for a project here at work where we need the ability to remotely control various cameras during a webinar presentation from a host computer.</p>

<p>Ideal would be the cameras have the ability to pan/tilt/zoom on their own, but as I've found out PTZ cameras are expensive.</p>

<p>I know we can use something cheap like the Logitech Orbit camera, but we'd really like the ability to have decent video cameras hooked up to a multi-input control video box and then also have them on tripods that can control their p/t/z if that even exists.</p>

<p><strong>tl;dr - So basically:</strong></p>

<p>What options do I have if I want to remotely control multiple cameras via a single computer (pan/tilt/zoom) during a webinar, so that the host/actor can just speak/present and we don't have to manually operate the cameras or have dedicated cameramen?</p>
","<p>You want a Pan/Tilt head.  Zoom will depend on having a camera that can have the zoom remote controlled and a remote control that can hook in to the zoom.  A pan tilt head can generally either be left sitting on a table or threaded on to a tripod.  They are, however, not cheap.  Expect to spend at least $300 for a cheap one and more if the camera ways more than a couple pounds.</p>
","9836"
"Is there a way to burn video on dvd and set ""expire time"" of it to be watched?","416","","<p>I'd like to ask if there is an possibility to make a dvd-disk that would read from dvd-player clock and compare it to ""expiry"" data on dvd disk itself.</p>

<p>If the date is after ""expiry"" date then dvd wouldn't show its content.</p>

<p>I'm aware that someone could just change dvd-player time, but I would like to do it another way.</p>
","<p>There is not only no way to do this with DVD, but there is no way to do this in any platform where you release the content.  If they can see it, they can record it.  Any measures that you can put in their way are simply DRM and have the analog hole.</p>

<p>Using streaming media is probably the closest you could come, but that still doesn't guarantee that it won't be copied.  The only sure option is to have a guard play it for someone and have them ensure that they do not have a means of recording going.</p>
","9497"
"How can I have a music track fade to a low gain/volume while I talk over it with a second audio track in Adobe Premiere Pro CC?","415","","<p>I'd like to know how I can setup two audio tracks where the following happens:</p>

<ol>
<li>The first audio track (music) plays at full power/volume/gain.</li>
<li>A second audio track (narration) causes the first audio track to fade to a much lower volume (but still able to hear it) while the narration happens.</li>
<li>The second audio track (narration) finishes which causes the first audio track (music) to resume to it's normal/full volume.</li>
</ol>

<p>I don't want any audio discontinuity to occur. Meaning, I want the first audio track (music) to play through while the narration happens.</p>

<p>I am using <code>Adobe Premiere Pro CC 7.2.2</code></p>
","<p>Ducking is a great solution for broadcast audio (for example, muting crowd noise when a commentator is trying to explain what's happening on the field).  In production, the limitations of ducking become apparent: sometimes it's too much or too little, sometimes it needs to be faster than the default, or slower.  These are things that are not necessarily obvious the first time you listen, but become obvious after the second or third listening.</p>

<p>Broadcast audio production presumes you hear things only once, so the limitations of ducking are acceptable.  If you are producing material that people might watch more than once, you really should do the ducking manually, setting audio keyframes and moving the volume manually to maximize not only quality and intelligibility, but also emotion and impact.</p>
","16775"
"DVD playback jagged","412","","<p>One of my DVD plays jagged when objects moves fast in the picture, I want to know what is this effect called and hope someone could give a solution. I have VLC. It is like this: </p>

<p><img src=""https://i.stack.imgur.com/RBwH4.png"" alt=""http://i.stack.imgur.com/2UZl4.png""></p>
","<p>This happens with <code>interlaced video</code>. Interlaced video is drawn to the screen in two passes, first the odd lines from top to bottom, then the even lines from bottom to top.</p>

<p>This is a legacy thing inherited from the days of good ol' CRT tubes when the dots on the screen was drawn using a beam; and to optimize the physical limitations of that the beam could not be all places at once this method where used together with phosphoric layer to reduce flicker.</p>

<p>With this you end up with two half frames (odd and even lines). Now, this is also the way video is captured on many video cameras. The exception is progressive recording where a single frame is recorded at once, like with analogue film cameras.</p>

<p>The problem you see happens when something moves fast and the odd lines captures one position of the object, while the process is about to capture the even lines the object has moved enough to be visible on the two merged half-frames.</p>

<p>Of course, you can drop one half-frame and use various techniques to for example interpolate the remaining lines, but always with a loss of quality as a result.</p>

<p>The only way to really eliminate this problem is to record everything progressively.</p>

<p>Hope this will give some input.</p>
","5713"
"What software do I need to edit commercials out of recorded sports games?","412","","<p>I have been using HD PVR to capture some of my football games to PC and now I want to edit out the commercials.  So I'm wondering what is the software that will do all these needs:</p>

<ul>
<li>Allow me to edit out commercials anywhere in the video.</li>
<li>After editing, it keeps the 5.1 Surround Sound.</li>
<li>Burn the video to a Blu-Ray player or save it to disk.</li>
<li>Ability to add menus too my Blu-Rays.</li>
<li>If there are logos on the screen(say ESPN logo), be able tell the software to find all instances and remove it.</li>
</ul>

<p>The HD PVR allows me to record in .TS, .M2TS or MP4.  Which format is best to allow me to do video editing?</p>
","<blockquote>
  <p>Allow me to edit out commercials anywhere in the video.</p>
</blockquote>

<p>Any modern editor can do this (<a href=""http://www.adobe.com/products/premiere.html"">Adobe Premier</a>, <a href=""http://www.sonycreativesoftware.com/vegassoftware"">Sony Vegas,</a> <a href=""https://www.apple.com/finalcutpro/"">Final Cut</a> etc) including light-weight (ex. <a href=""http://www.pinnaclesys.com/PublicSite/us/Products/Consumer+Products/Home+Video/Studio+Family/"">Pinnacle Studio</a>) and <a href=""http://www.nchsoftware.com/videopad/index.html"">free editors</a>.</p>

<blockquote>
  <p>After editing, it keeps the 5.1 Surround Sound.</p>
</blockquote>

<p>Provided your original clip contains 5.1 channels and your editing environment is set up with (and can support, see more professional packages) 5.1 support, and you export in a format that support 5.1, this is granted.</p>

<blockquote>
  <p>Burn the video to a Blu-Ray player or save it to disk.</p>
</blockquote>

<p>As long as you export in a format supported by the BD-disk creator software, or a format that can be trans-coded, no problem.</p>

<blockquote>
  <p>Ability to add menus too my Blu-Rays.</p>
</blockquote>

<p>This most BD-disk creator software can do (see for example <a href=""http://www.adobe.com/products/encore.html"">Adobe Encore</a>).</p>

<blockquote>
  <p>If there are logos on the screen(say ESPN logo), be able tell the software to find all instances and remove it.</p>
</blockquote>

<p>This is a very specific task and as you say, you need to tell the software to find these. You can do this by combining a tracker to the logo and a script that can follow. When logo is ""lost"" for the tracker, init a method in the script that do what you want. This as said, will be very specific and it will require you to program your script very specific.</p>

<p>IMO it will be quicker to go through these manually. especially if you know the material you're working with or at least what to expect of it.</p>
","5680"
"Adding 3d animations in a video","411","","<p>For a project we need 3d animations to start <em>unfolding</em> whilst a person is walking. The video camera will be moving. Using After effects, achieve such a thing?</p>

<p>Also what is this called? So I can also continue my search on google.</p>

<p>Thanks!</p>
","<p>You will need motion tracking at a minimum if the camera is moving during the shot and you want to composite a 3d animation.  It will need to be able to do 3d motion tracking, which is the hardest to do.  I believe that Mocha (which comes with recent versions of After Effects) can handle this passably enough for a simple project.  I'm not entirely sure what you are trying to do with the animation though based on how you phrased the question.</p>
","7389"
"What is the significance of this ""RED, Digital, and Film Format Size Chart""?","411","","<p>Little context is offered when this chart:</p>

<p><img src=""https://i.stack.imgur.com/BWQ5y.jpg"" alt=""enter image description here""></p>

<p>is linked. What does it mean?</p>
","<p>Based on this illustration, you are able to compare the <a href=""http://en.wikipedia.org/wiki/Image_sensor_format"" rel=""nofollow"">sensor sizes</a> of red cameras with film formats like <a href=""http://en.wikipedia.org/wiki/16_mm_film"" rel=""nofollow"">16mm</a>, <a href=""http://en.wikipedia.org/wiki/35_mm_film"" rel=""nofollow"">35mm</a> or <a href=""http://en.wikipedia.org/wiki/70_mm_film"" rel=""nofollow"">70mm</a> imax and so on.</p>
","15728"
"What's the best setup to transfer a digital or analog video over 100 ft?","411","","<p>I'm looking for the best (price/quality) solution to send a digital or analog video signal to an lcd tv or beamer over 80 - 100 ft. The input can be hdmi, vga or dvi (all must be possible). The output (80 - 100 ft. away) doesn't need HD quality.</p>

<p>tv has hdmi, vga, dvi, s-video, composite and coax.</p>

<p>So far i've got this setup in mind for relative low cost and acceptable quality:</p>

<ul>
<li>80-100 ft. vga cable connected to tv (should cover inputs with vga or dvi)</li>
<li>vga switch 1-4 for switching between inputs</li>
<li>hdmi --> vga converter (to cover hdmi inputs)</li>
</ul>

<p>Does anyone know a better setup ?
is vga the best option in this case to bridge the 80-100 ft. ?</p>
","<p>A long VGA cable with a proper amp/spliter on the TX side, would be the simplest option. You can get good VGA cables up to 150' at modest prices from <a href=""http://www.pccables.com"" rel=""nofollow"">http://www.pccables.com</a> .</p>

<p>Another good option is a CAT5 extender kit.
<a href=""http://milestek.com/p-16209-vga-over-cat5ecat6-decora-wall-plate-set.aspx"" rel=""nofollow"">http://milestek.com/p-16209-vga-over-cat5ecat6-decora-wall-plate-set.aspx</a></p>

<p>Without a more detailed description of what you are trying to accomplish, it's hard to give a better answer than that.</p>
","2893"
"ffmpeg / mencoder converting a video with correct setting","410","","<p>I have an avi video file that plays on my Onn-W7 player <a href=""https://www.dropbox.com/s/nfpnso35rjefs55/ONN.avi"" rel=""nofollow noreferrer"">Link to avi video file that works </a> and <a href=""http://speedy.sh/A7rdf/ONN.avi"" rel=""nofollow noreferrer"">backup link to avi file that works.</a> I'm trying to convert some other video files over so that they will also play on it.  I've tried handbrake, ffmpeg and mencoder but the video fails to work most likely do to the fact I have some settings wrong in the conversion process but I don't know which ones any ideas how to fix this</p>

<p><strong>Here's the settings of the avi file that works below:</strong></p>

<pre><code>General
Complete name                            : /tmp/ONN.avi
Format                                   : AVI
Format/Info                              : Audio Video Interleave
File size                                : 11.0 MiB
Duration                                 : 3mn 1s
Overall bit rate                         : 508 Kbps
Writing application                      : MEncoder Sherpya-MinGW-20060312-4.1.0
Writing library                          : MPlayer

Video
ID                                       : 0
Format                                   : MPEG-4 Visual
Format profile                           : Simple@L3
Format settings, BVOP                    : No
Format settings, QPel                    : No
Format settings, GMC                     : No warppoints
Format settings, Matrix                  : Default (H.263)
Codec ID                                 : XVID
Codec ID/Hint                            : XviD
Duration                                 : 3mn 1s
Bit rate                                 : 371 Kbps
Width                                    : 160 pixels
Height                                   : 128 pixels
Display aspect ratio                     : 5:3
Original display aspect ratio            : 5:4
Frame rate                               : 15.000 fps
Color space                              : YUV
Chroma subsampling                       : 4:2:0
Bit depth                                : 8 bits
Scan type                                : Progressive
Compression mode                         : Lossy
Bits/(Pixel*Frame)                       : 1.207
Stream size                              : 8.00 MiB (73%)
Writing library                          : XviD 1.1.0 (UTC 2005-11-22)

Audio
ID                                       : 1
Format                                   : MPEG Audio
Format version                           : Version 1
Format profile                           : Layer 2
Codec ID                                 : 50
Duration                                 : 3mn 1s
Bit rate mode                            : Constant
Bit rate                                 : 128 Kbps
Channel(s)                               : 2 channels
Sampling rate                            : 44.1 KHz
Compression mode                         : Lossy
Stream size                              : 2.77 MiB (25%)
Alignment                                : Split accross interleaves
Interleave, duration                     : 26 ms (0.39 video frame)
Interleave, preload duration             : 522 ms
</code></pre>

<p><strong>The ffmpeg command I tried is:</strong></p>

<pre><code>ffmpeg -i ""video_to_convert.mp4"" -c:v libxvid -vf scale=160x128,setsar=1 -b:v 800k -r 15 -c:a libtwolame -ac 2 -ar 44100 -b:a 128k -y video_converted.avi
</code></pre>

<p><strong>Here's the requested info of the command above on a file that doesn't work:</strong></p>

<pre><code>mediainfo --fullscan video_converted.avi
General
Count                                    : 322
Count of stream of this kind             : 1
Kind of stream                           : General
Kind of stream                           : General
Stream identifier                        : 0
Count of video streams                   : 1
Count of audio streams                   : 1
Video_Format_List                        : xvid
Video_Format_WithHint_List               : xvid
Codecs Video                             : xvid
Audio_Format_List                        : MPEG Audio
Audio_Format_WithHint_List               : MPEG Audio
Audio codecs                             : MPEG-1 Audio layer 2
Complete name                            : video_converted.avi
File name                                : video_converted
File extension                           : avi
Format                                   : AVI
Format                                   : AVI
Format/Info                              : Audio Video Interleave
Format/Extensions usually used           : avi
Commercial name                          : AVI
Internet media type                      : video/vnd.avi
Interleaved                              : Yes
Codec                                    : AVI
Codec                                    : AVI
Codec/Info                               : Audio Video Interleave
Codec/Extensions usually used            : avi
File size                                : 4297704
File size                                : 4.10 MiB
File size                                : 4 MiB
File size                                : 4.1 MiB
File size                                : 4.10 MiB
File size                                : 4.099 MiB
Duration                                 : 165267
Duration                                 : 2mn 45s
Duration                                 : 2mn 45s 267ms
Duration                                 : 2mn 45s
Duration                                 : 00:02:45.267
Duration                                 : 00:02:45:04
Duration                                 : 00:02:45.267 (00:02:45:04)
Overall bit rate                         : 208037
Overall bit rate                         : 208 Kbps
Frame rate                               : 15.000
Frame rate                               : 15.000 fps
Frame count                              : 2479
Stream size                              : 228958
Stream size                              : 224 KiB (5%)
Stream size                              : 224 KiB
Stream size                              : 224 KiB
Stream size                              : 224 KiB
Stream size                              : 223.6 KiB
Stream size                              : 224 KiB (5%)
Proportion of this stream                : 0.05327
File last modification date              : UTC 2016-09-16 10:15:46
File last modification date (local)      : 2016-09-16 06:15:46
Writing application                      : Lavf56.40.101
Writing application                      : Lavf56.40.101

Video
Count                                    : 332
Count of stream of this kind             : 1
Kind of stream                           : Video
Kind of stream                           : Video
Stream identifier                        : 0
ID                                       : 0
ID                                       : 0
Format                                   : xvid
Commercial name                          : xvid
Codec ID                                 : xvid
Codec                                    : xvid
Codec                                    : xvid
Codec/CC                                 : xvid
Duration                                 : 165267
Duration                                 : 2mn 45s
Duration                                 : 2mn 45s 267ms
Duration                                 : 2mn 45s
Duration                                 : 00:02:45.267
Duration                                 : 00:02:45:04
Duration                                 : 00:02:45.267 (00:02:45:04)
Bit rate                                 : 69281
Bit rate                                 : 69.3 Kbps
Width                                    : 160
Width                                    : 160 pixels
Height                                   : 128
Height                                   : 128 pixels
Pixel aspect ratio                       : 1.000
Display aspect ratio                     : 1.250
Display aspect ratio                     : 5:4
Frame rate                               : 15.000
Frame rate                               : 15.000 fps
Frame count                              : 2479
Scan type                                : Progressive
Scan type                                : Progressive
Bits/(Pixel*Frame)                       : 0.226
Delay                                    : 0
Delay                                    : 00:00:00.000
Stream size                              : 1431221
Stream size                              : 1.36 MiB (33%)
Stream size                              : 1 MiB
Stream size                              : 1.4 MiB
Stream size                              : 1.36 MiB
Stream size                              : 1.365 MiB
Stream size                              : 1.36 MiB (33%)
Proportion of this stream                : 0.33302

Audio
Count                                    : 272
Count of stream of this kind             : 1
Kind of stream                           : Audio
Kind of stream                           : Audio
Stream identifier                        : 0
StreamOrder                              : 1
ID                                       : 1
ID                                       : 1
Format                                   : MPEG Audio
Commercial name                          : MPEG Audio
Format version                           : Version 1
Format profile                           : Layer 2
Internet media type                      : audio/mpeg
Codec ID                                 : 50
Codec ID/Url                             : http://www.iis.fraunhofer.de/amm/index.html
Codec                                    : MPA1L2
Codec                                    : MPEG-1 Audio layer 2
Codec/Family                             : MPEG-1
Codec/Url                                : http://www.iis.fraunhofer.de/amm/index.html
Codec/CC                                 : 50
Duration                                 : 165224
Duration                                 : 2mn 45s
Duration                                 : 2mn 45s 224ms
Duration                                 : 2mn 45s
Duration                                 : 00:02:45.224
Duration                                 : 00:02:45.224
Bit rate mode                            : CBR
Bit rate mode                            : Constant
Bit rate                                 : 128000
Bit rate                                 : 128 Kbps
Channel(s)                               : 2
Channel(s)                               : 2 channels
Sampling rate                            : 44100
Sampling rate                            : 44.1 KHz
Samples count                            : 7286378
Compression mode                         : Lossy
Compression mode                         : Lossy
Delay                                    : 0
Delay                                    : 00:00:00.000
Delay, origin                            : Stream
Delay, origin                            : Raw stream
Delay relative to video                  : 0
Delay relative to video                  : 00:00:00.000
Video0 delay                             : 0
Video0 delay                             : 00:00:00.000
Stream size                              : 2637525
Stream size                              : 2.52 MiB (61%)
Stream size                              : 3 MiB
Stream size                              : 2.5 MiB
Stream size                              : 2.52 MiB
Stream size                              : 2.515 MiB
Stream size                              : 2.52 MiB (61%)
Proportion of this stream                : 0.61371
Alignment                                : Aligned
Alignment                                : Aligned on interleaves
Interleave, duration                     : 0.39
Interleave, duration                     : 26
Interleave, duration                     : 26 ms (0.39 video frame)
</code></pre>

<p><strong>Also without the fullscan option:</strong></p>

<pre><code>mediainfo video_converted.avi
General
Complete name                            : video_converted.avi
Format                                   : AVI
Format/Info                              : Audio Video Interleave
File size                                : 4.10 MiB
Duration                                 : 2mn 45s
Overall bit rate                         : 208 Kbps
Writing application                      : Lavf56.40.101

Video
ID                                       : 0
Format                                   : xvid
Codec ID                                 : xvid
Duration                                 : 2mn 45s
Bit rate                                 : 69.3 Kbps
Width                                    : 160 pixels
Height                                   : 128 pixels
Display aspect ratio                     : 5:4
Frame rate                               : 15.000 fps
Scan type                                : Progressive
Bits/(Pixel*Frame)                       : 0.226
Stream size                              : 1.36 MiB (33%)

Audio
ID                                       : 1
Format                                   : MPEG Audio
Format version                           : Version 1
Format profile                           : Layer 2
Codec ID                                 : 50
Duration                                 : 2mn 45s
Bit rate mode                            : Constant
Bit rate                                 : 128 Kbps
Channel(s)                               : 2 channels
Sampling rate                            : 44.1 KHz
Compression mode                         : Lossy
Stream size                              : 2.52 MiB (61%)
Alignment                                : Aligned on interleaves
Interleave, duration                     : 26 ms (0.39 video frame)
</code></pre>

<p>The products website is
<a href=""http://en.onnchina.com/mp3_mp4/product172.html"" rel=""nofollow noreferrer"">Onn-w7 website</a></p>

<p>Here's the little manual in Chinese which I can't read
<a href=""https://i.stack.imgur.com/Ck6wG.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ck6wG.jpg"" alt=""page 1""></a>
<a href=""https://i.stack.imgur.com/jaNgZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jaNgZ.jpg"" alt=""page 2""></a></p>
","<p>Looks like the player is using the video codec tag to vet files. And it is picky about case. So try</p>

<pre><code>ffmpeg -i ""video_to_convert.mp4"" -c:v libxvid -vtag XVID -level 1
       -vf scale=160x128,setsar=1 -b:v 800k -r 15
       -c:a libtwolame -ac 2 -ar 44100 -b:a 128k -y video_converted.avi
</code></pre>

<p>(The level may not be necessary. I was testing something.)</p>
","19404"
"What is this strange second camera?","410","","<p>I don't know is this is the correct place to ask this but ...</p>

<p>I've watched several ""making of"" of current movies and noticed a thing i cannot explain. </p>

<p><strong>What is this strange ""secondary"" camera you can see near the main camera?</strong></p>

<p>Image: 
<img src=""https://i.stack.imgur.com/F4I99.png"" alt=""enter image description here""></p>

<p>Reference:  (5:52 and 6:50)</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/e3-VCVLtlBQ?start=350""></iframe>
            </div></div></p>
","<p>That is a specially built enclosure called a <a href=""http://en.wikipedia.org/wiki/Sound_blimp"">Sound Blimp</a>, for a still camera, which renders it completely silent, so they can shoot stills without clicks and beeps disturbing the filming.</p>
","7132"
"How to select multiple keyframes in Premiere?","406","","<p>I had accidentally created numerous wrong keyframes, trying to tune volume. Now I would like to delete them. Unfortunately, I can't select multiple keyframes by mouse:</p>

<p><a href=""https://i.stack.imgur.com/47EfJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/47EfJ.jpg"" alt=""enter image description here""></a></p>

<p>When I release mouse, all keyrames remain not selected.</p>
","<p>You can do it in the Effects Control window:
<a href=""https://i.stack.imgur.com/GuEka.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GuEka.png"" alt=""enter image description here""></a>
Even if keyframes were added on the timeline, it will work.</p>

<p>Good luck!</p>
","17256"
"PC instead of external recorder","405","","<p>I have a camera which outputs uncompressed HDMI. I understand that I can buy an external recorder (e.g., Atomos) to record the video, but is it possible to use an ordinary laptop (or desktop) computer instead?</p>
","<p>In the end, I found out that you can buy a dongle (in my case an INOGENI 4K USB) that will connect the camera to the computer via USB (amazingly, USB 3 is fast enough) or, in the case of BlackMagic Mini Recorder (which does 1080p only), thunderbolt.</p>

<p>The next challenge is, of course, to encode 4K fast enough on a laptop.</p>
","18090"
"What are people using to connect SSD drives from a camera to Thunderbolt or Firewire?","405","","<p>We're shooting on a Blackmagic Cinema Camera, and at the moment are using the guts of a little portable drive enclosure as our cradle for transferring to our Macs via Firewire 800. It's got a very guerrilla vibe to it, but it's not exactly what I want to see when I'm transferring irreplaceable footage.</p>

<p>I've looked around, but there doesn't seem to be any purpose built SSD cradle for Firewire 800 or Thunderbolt (our editing machines are being replaced by Thunderbolt-only ones), only USB3. Has anyone got any suggestions?</p>
","<p><a href=""http://www.highpoint-tech.com/USA_new/series_RS5212-overview.htm"" rel=""nofollow"">HighPoint RocketStor RS5212</a> is a Thunderbolt Dock that will accept two hard drives or SSDs.</p>

<p>I would not use FireWire for SSDs, unless you don't have USB 3.0 and you're on a budget. USB 3.0 is often fast enough to get the maximum out of your SSD, FireWire not.</p>
","10226"
"Convert video from 24 to 25 fps while keeping video and audio length and also correct audio pitch","404","","<p>With ffmpeg is possible to convert an input 24 fps video/audio to a 25 fps one while keeping the same exact duration, and also keeping as much as possible the original audio pitch ?</p>

<p>I was thinking about the -r option and also a sound filter but I don't know what to add to my command line:</p>

<pre><code>ffmpeg -i input.mov -r 25 -c:v libx264 -crf 18 -c:a libfdk_aac -global_quality 3 output.mov
</code></pre>
","<p>Your command already achieves that. When specifying <code>-r</code> as an output option, ffmpeg will change the framerate of the video, duplicating or dropping frames as necessary to achieve the same overall duration. The audio stream will be processed independently, thus not affecting the pitch (since the overall duration is not changed) or sync (since the timestamps of both streams stay aligned).</p>

<p><code>-global_quality</code> is not valid. For <code>libfdk_aac</code> you should use the <code>-vbr</code> option as mentioned in the <a href=""http://trac.ffmpeg.org/wiki/Encode/AAC"" rel=""nofollow noreferrer"">AAC encoding guide</a>.</p>
","21733"
"How to add Subtitles to already created video file for .avi format on Windows 8?","404","","<p>Can I create subtitle (.srt) file manually (to add subtitle) for .avi file?</p>

<p>I have created .avi file using Snagit but there is no option to add subtitle as well. So can I create subtitle file manually by myself? Or Do I have to download a specific Software for it?</p>

<p>Does it follows any structure?</p>
","<p>I researched on net and found that it's very to add subtitle to any <code>.avi</code> file format. Here is a link for a site called <a href=""http://www.any-video-converter.com/add-srt-subtitle-to-output-video.php"" rel=""nofollow"">Any Video Converter</a> where an example has been given that How to create <code>.srt</code> file for Subtitles?</p>

<p>For quick view, below I am specifying format for Subtitle file (.srt)</p>

<pre><code>1
00:00:03,000 --&gt; 00:00:07,000
Hi Friends, This is First Track

2
00:00:14,000 --&gt; 00:00:17,000
Second Track goes here

3
00:00:18,000 --&gt; 00:00:22,000
Third Track goes here
</code></pre>

<p>So, we have to just follow this format. There should be a new line between each track. That's it.</p>

<p><code>00:00:18,000</code> is format of <code>hh:mm:ss:ms</code>. So it's pretty simple.</p>
","9291"
"How was this magic smoke animation done?","404","","<p>Hi I was hoping any experienced video producer could tell me how an animation like the one showing at ~0:29-0:50 on the left side of this video is done?:
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/633CHC6TTYY?start=0""></iframe>
            </div></div></p>

<p>and is it possible to convert such an animation to separate transparent .png frames?</p>
","<p>If you want to do this from scratch you would need a software that has a <a href=""https://en.wikipedia.org/wiki/Particle_system"" rel=""nofollow"">particle simulation system</a>. A popular tool for that is <a href=""http://www.adobe.com/products/aftereffects.html"" rel=""nofollow"">After Effects</a> in conjunction with the plugin <a href=""http://www.redgiant.com/products/trapcode-particular/"" rel=""nofollow"">Particular</a> from Red Giant. Its rather easy to use.</p>

<p>If you want even more realistic results you probably want to go with a more sophisticated solution often found in 3D packages like <a href=""http://www.autodesk.com/products/3ds-max/overview-dts?s_tnt=69291:1:0"" rel=""nofollow"">3Ds Max</a>, <a href=""http://www.maxon.net/products/cinema-4d-studio/who-should-use-it.html"" rel=""nofollow"">Cinema 4D</a> or the the free and open source <a href=""https://www.blender.org/features/"" rel=""nofollow"">Blender</a>.</p>

<p>If you dont want to render something yourself you can also buy stock-footage. A popular package is the <a href=""https://www.videocopilot.net/products/action2/"" rel=""nofollow"">Action Essentials 2</a> from Video Copilot. It offers pre-keyed (e.g. with transparency) footage from real smoke and fire and other misc. effects. You can easily colorize these to your liking and export them as a transparent png sequence.</p>
","15057"
"Using tablet/smartphone as external dslr monitor","403","","<p>I know there are apps like DslrDashboard and more but I would like to just get a real time monitor for my Nikon D5100. How is that possible?`
Of course connected wiyh a Hdmi-cable.</p>
","<p>You can't do that. There is no tablet on the market (right now) that offers an HDMI input or any other physical display input. Your only option would be to stream compressed video over the USB (or Lightning on iPads) connection or Wifi to an app that displays that stream.</p>
","12404"
"FCP X 10.1.1 - Transitioning to libraries without blending","402","","<p>As excited as I am about the new asset management possibilities of the .1 release of Final Cut Pro X, I'm nervous about screwing up my existing projects and events. My projects and events are currently spread across 2 hard drives (my boot drive and an external drive). Some are ""hidden"" via Event Manager X. Some projects and events belong together, and others do not. I performed the upgrade to FCP X 10.1.1 (apparently they've already released some bug fixes to 10.1.0). When I did this, I kept my projects and events ""hidden"". </p>

<p>Now it's time to import them, but I want to make sure I pair the correct projects and events in the correct libraries. (In other words, I want to have 1 library for each client, not 1 library for each hard drive, nor 1 library for each project.) There is not an menu option for importing and pairing projects and eventsat least not that I can find. How can I conduct combine projects and events into libraries in a controlled way?</p>
","<p>I got it to work. The key is to update Events and Projects to Libraries (which, incidentally, contain Events and Projects) in stages:</p>

<ol>
<li>Hide everything except the Events and Projects that belong together in the same Library. Event Manager X is a great tool for this, as Jason suggested in the comments. Whatever tool you use, just make sure that only the Events and Projects that belong together are in the folders ""Final Cut Events"" and ""Final Cut Projects"".</li>
<li>Next, open Final Cut Pro X.</li>
<li>Go to File and click ""Update Projects and Events..."" near the bottom of the menu:<img src=""https://i.stack.imgur.com/WoZB7.png"" alt=""Update Projects and Events""></li>
<li>Click ""Update All""</li>
</ol>

<p>Final Cut Pro X will do its thing and you'll have a new library with just the Projects and Events you want inside of it.</p>
","10363"
"How to calculate the length of the video file that was encoded so far?","402","","<p>I am using <code>ffmpeg</code> to convert a video file to H.264 format. If I call <code>ffprobe</code> while <code>ffmpeg</code> is still running the duration of the video is not known. Only when <code>ffmpeg</code> finishes the <code>ffprobe</code> will display the duration.</p>

<p>Similar, if I run <code>mplayer</code> to play the video while it is still being encoded the duration is either unknown or displays incorrectly.</p>

<p>Is there a way to know duration of the resulted video (h264 in .mkv container) while it is still being encoded by <code>ffmpeg</code>?</p>

<p><strong>Edit</strong>: <em>I will make the question a little bit more clear.</em></p>

<p>I am interested in knowing how long is the video which was encoded <strong>so far</strong>. Let's say the input file is 1 minute 45 seconds long. The resulting file should preserve the length. But I would like to know how much did <code>ffmpeg</code> encode <strong>so far</strong>. If we can determine the length of the video after conversion is done, shouldn't there be a way to determine the length of the video that was already processed and ready to be played?</p>
","<p>Short answer is No.</p>

<p>Longer answer is, it depends.</p>

<p>If you're encoding a file, then generally the output is the duration of the input, unless there's speed change or trim filters or <code>-ss, -to, -t</code> options applied. For a live input, FFmpeg will stop the encode when it encounters EOF on the input, so unless you know that, you won't know the output duration. For multiple inputs of differing duration, it will depend on what the command is e.g. is there a <code>-shortest</code> in there, is there an <code>amix</code>, which defaults to the longest of the inputs, or <code>amerge</code> which terminates upon the shortest input.</p>

<p>By default, the ffmpeg console readout will display the progress of the encoding in realtime i.e. the duration encoded, so far.</p>
","18376"
"How to convert an mkv file with hevc encoding to mkv with avc","402","","<p>I have some mkv files with a hevc encoding which is not allowing me to burn to disc in covertxtodvd as it shows no video track as the encoding isn't supported, i was wondering if it was possible to convert the files encoding into avc encoding whilst keeping embedded subtitles and the same mkv format.
Thank you for reading,
Nick W</p>
","<p>ffmpeg, a command-line tool, can do this:</p>

<pre><code>ffmpeg -i input.mkv -c:v libx264 -crf 23 -c:a copy -c:s copy -fflags +genpts output.mkv
</code></pre>

<p>Adjust the CRF for larger or smaller size (lower number produces better quality but larger size)</p>

<p>Download ffmpeg (32-bit static version) from <a href=""http://ffmpeg.zeranoe.com/builds/"" rel=""nofollow"">here</a>.</p>
","17169"
"Container Type that doesn't corrupt on Encoding Crash","400","","<p>I've been produce videos on YouTube for a few years. I understand a vast majority of encoding options for x264 (My preferred encoder).</p>

<p>I used to use MPEG-4 containers (.mp4) with my videos, however my system isn't the most powerful and the encoder will sometimes crash mid-encoding, causing the file to be unreadable because of how MPEG-4 file data is organized.</p>

<p>So I switched to using FLV as the container, and remuxing it to MPEG-4 after recording is done. FLV's organization of the data allows the file to be read, even if the file was never completed. I don't understand much about the actual formats, but Id suspect it's an absence of a footer in the format for FLV.</p>

<p>The problem I'm having with FLV, is that when I used MP4, I record with multiple audio tracks, so that I can edit them independently. FLV does not support multiple audio streams.</p>

<p>I'm wondering if there is a container for x264, that's still high quality, allows for multiple audio tracks, and can still be read if encoding crashes.</p>

<p>I am recording with a program called OBS-Studio (OBS Multi-Platform), which allows the following containers for x264: FLV, MP4, MOV, MKV, TS</p>

<p>Are any of those last 3 formats capable of such functionality. I have tried a few days of research and can't find anything regarding this.</p>
","<p>Yes, TS is capable of doing what you are asking. Weather OBS is, is another question (that I do not know the answer to). mkv MAY be also to as well. But mp4 and mov can not do this.</p>
","16415"
"Do I need to encode video in both ogg and webm?","400","","<p>I'm doing some HTML5 video work; I'm taking uploaded video and using ffmpeg and related packages to convert the original videos (usually .mov, but whatever) into mp4, ogg and webm versions for presentation via video.js.  The ogg and webm encodings are both working nicely in Firefox across Windows, Mac, and Linux (the main reason for producing both, it seems); my question is whether, as a practical matter, I need to produce both ogg and webm versions.  Both seem to be working well; I'm asking out of an interest in not producing stuff that I don't really need.  Any thoughts?  Thanks!</p>
","<p>Probably better suited for Stackoverflow but I think its still a valid question for this SE.</p>

<p>Producing theora(ogg) versions of your videos is actually redundant. Any browser version supporting ogg is also supporting webm.</p>

<p>For questions like these I recommend using <a href=""http://caniuse.com"" rel=""nofollow"">caniuse.com</a>.
If you compare <a href=""http://caniuse.com/webm"" rel=""nofollow"">webm</a> with <a href=""http://caniuse.com/ogv"" rel=""nofollow"">ogg</a> you quickly see that you cover the same browser versions and more with webm.
Unless you need to cover Firefox 3.5+.</p>
","12279"
"Burn a MP4 file with SRT subtitles","399","","<p>I have a movie and I want to burn it on a DVD, but I don't know how I do it and what the best options are. I want the DVD be able to play on PlayStation 4.
I have a movie in MP4 format and the subtitles are in SRT format. Are you able to tell me what the best programs are? I have installed Nero 2015, but I can't figure out how to work with it.</p>
","<p>I think most HW players that can play mp4/avi/mkv/etc files-in-a-directory will load external-file subtitles if you just put them in the same directory as the video.  Burn it to a data DVD, or put it on a USB stick, and you're done.</p>
","15773"
"How to extract Xtra atom from MP4 file?","397","","<p>Can one dump/extract the <code>Xtra</code> atom from an MP4 file ? I am looking for a solution on a linux system (debian/jessie if possible).</p>

<p>Apparently this atom is used by WMP, as per <a href=""http://www.ventismedia.com/mantis/view.php?id=12017"" rel=""nofollow"">link</a>. I'd like to be able to use such information (rating, publisher or leveling inf) and pass that to my DLNA server (minidlna).</p>

<p>Currently all I see using <code>AtomicParsley</code> is:</p>

<pre><code>[...]
                     Atom data @ 678585 of size: 529, ends @ 679114
         Atom Xtra @ 679114 of size: 206, ends @ 679320 ~
Atom free @ 679320 of size: 48, ends @ 679368
Atom mdat @ 679368 of size: 248835382, ends @ 249514750

 ~ denotes an unknown atom
[...]
</code></pre>

<p>And same thing with <code>MP4Box</code>:</p>

<pre><code> &lt;UDTARecord Type=""Xtra""&gt;
 &lt;UnknownBox&gt;
 &lt;BoxInfo Size=""206"" Type=""Xtra""/&gt;
 &lt;/UnknownBox&gt;
</code></pre>

<p>Which cannot be used/parsed.</p>
","<p>I've reported the issue on Debian bug tracker <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=782124"" rel=""nofollow"">here</a>, as well as upstream <a href=""https://bitbucket.org/wez/atomicparsley/issue/45"" rel=""nofollow"">here</a>.</p>

<p>Really the only available reference implementation can be found <a href=""http://code.google.com/p/mp4v2/issues/detail?id=113"" rel=""nofollow"">here</a>.</p>
","15373"
"Is there a text effect for Sony Vegas Movie Studio for light reflecting off metal?","395","","<p>I found a picture to illustrate the effect I would like to have. <a href=""http://www.bugei.com/scart/public/database/repository/images_import/peace/habuchi.jpg"" rel=""nofollow"">http://www.bugei.com/scart/public/database/repository/images_import/peace/habuchi.jpg</a></p>

<p>Does anyone know of text effect for this, or another way to do it?</p>
","<p>Vegas Movie Studio or Pro versions do not have a text option already set up for this but there are a couple of ways to get what you want.</p>

<p>One way is to make a composite by creating two video tracks one with the text and the other with the image you want to bleed through the text, here are instructions on how to do what is called a 'masking track':</p>

<p><a href=""http://www.sonycreativesoftware.com/creating_masking_tracks_in_vegas"" rel=""nofollow"">http://www.sonycreativesoftware.com/creating_masking_tracks_in_vegas</a></p>
","8794"
"How to make multiple shots when there's just one camera?","394","","<p>I just want to have a Full shot at first and then a Medium full shot next in my video.</p>

<p><img src=""https://i.stack.imgur.com/1Bs6O.png"" alt=""enter image description here""></p>

<p>Is it possible if there's just one camera? </p>

<p>*My camera shoot on Full HD, and the video composition in the editing software is set on Full HD too, so it's not possible to scale the video to achieve the Medium shot out of the Full.</p>
","<p>This is basic film making, you would do this in multiple takes.</p>

<ol>
<li>Take your wide shot running the whole scene you want with this shot</li>
<li>Take you scene back to the start point, adjust the camera to the new shot and run through the scene again</li>
<li>Rinse and repeat until you have all the shots needed to make your scene. </li>
</ol>

<p>You would/should have already made a shot list knowing what needs to be shot with the different camera angles, sometimes only have to act parts of scenes over and over again to get the sections of the shots needed.</p>

<p>Other option would be to get a 4k/5k/6k camera and be able to punch in the shots during editing but this could cause further issues in post with file sizes, performaces, converting, encoding etc etc</p>
","15675"
"What is the best way to convert .vob files copied directly from a DVD into a format usable by Premiere Pro?","391","","<p>I need to import some video material that was copied directly from a DVD into Premiere Pro. (Disclaimer: This is for educational purposes, in that context it is lawful to use excerpts of commercial films in Germany). </p>

<p>They were directly copied, meaning I don't have an image of the DVD, but the original file structure from the DVD including the folders <code>VIDEO_TS</code> and <code>AUDIO_TS</code>, containing <code>.VOB</code> files. Premiere Pro can't import them. </p>

<p>What is the best way to convert/copy those files to get them into a format that Premiere Pro can use? By 'best' I mean with the least quality loss possible and in a way that will produce either one file that has the entire movie in them or an array of sorted files that will be easy to look through. No I don't have the original DVD. Thanks!</p>
","<p>Using ffmpeg, cat and remux all the VOBs together to a MPEG program stream.</p>

<pre><code>ffmpeg -i ""concat:vts01_1.vob|vts01_2.vob|vts01_3.vob"" -c copy -f dvd dvd.mpg
</code></pre>

<p>(You may have to escape the <code>|</code> character i.e. <code>vts01_1.vob\|vts01_2.vob\|vts01_3.vob</code>)</p>
","18805"
"Creating a sith lightning effect","389","","<p>How do you create a sith lightning effect? I am getting into special effects, and was wondering how you do this. I have FCE 4 &amp; 3.5. I'd rather not have to use a different program, but...</p>
","<p>With FCE I think your best option would be a plugin or something. I know you can make them with Apple Motion and After Effects though, or a really simple one with good ol' iMovie HD. </p>

<p>Good luck in your search for the force.</p>
","2186"
"ffmpeg - burning in subtitles with non-square pixels","387","","<p>I've got a 16:9 standard-def movie into which I need to burn some subtitles. I'm using</p>

<pre><code> ffmpeg -y -i input.mov -vf ass=""./captions.ass"" output.mov
</code></pre>

<p>and in the subtitle.ass file I've tried</p>

<pre><code>PlayResX: 1024
PlayResY: 576
</code></pre>

<p>and</p>

<pre><code>PlayResX: 768
PlayResY: 576
</code></pre>

<p>But in both cases the subtitles came out stretched. Is there anything I can do to get them squareeither with the ffmpeg command or the subtitle file? I'd rather not stretch the video twice, it's already looking a bit shabby.</p>

<p>Command and console output:</p>

<pre><code>PS C:\Users\stib\Desktop&gt; ffmpeg -i .\test.mov -vf ""ass='./test_subs.ass'"" test_out.mp4
ffmpeg version N-76144-ge91cd8a Copyright (c) 2000-2015 the FFmpeg developers
  built with gcc 5.2.0 (GCC)
  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-decklink --enable-zlib
  libavutil      55.  4.100 / 55.  4.100
  libavcodec     57.  7.100 / 57.  7.100
  libavformat    57.  8.102 / 57.  8.102
  libavdevice    57.  0.100 / 57.  0.100
  libavfilter     6. 12.100 /  6. 12.100
  libswscale      4.  0.100 /  4.  0.100
  libswresample   2.  0.100 /  2.  0.100
  libpostproc    54.  0.100 / 54.  0.100
Guessed Channel Layout for  Input Stream #0.1 : stereo
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '.\test.mov':
  Metadata:
    major_brand     : qt
    minor_version   : 537199360
    compatible_brands: qt
    creation_time   : 2014-05-07 03:24:23
  Duration: 00:01:50.84, start: 0.000000, bitrate: 30340 kb/s
    Stream #0:0(eng): Video: dvvideo (dvcp / 0x70637664), yuv420p(bt470bg/smpte170m/bt709), 720x576 [SAR 64:45 DAR 16:9], 28800 kb/s, SAR 118:81 DAR 295:162, 25 fps, 25 tbr, 25 tbn, 25 tbc (default)
    Metadata:
      creation_time   : 2014-05-07 03:24:23
      handler_name    : Apple Alias Data Handler
      encoder         : DV - PAL
      timecode        : 01:00:00:00
    Stream #0:1(eng): Audio: pcm_s16le (sowt / 0x74776F73), 48000 Hz, 2 channels, s16, 1536 kb/s (default)
    Metadata:
      creation_time   : 2014-05-07 03:24:23
      handler_name    : Apple Alias Data Handler
    Stream #0:2(eng): Data: none (tmcd / 0x64636D74) (default)
    Metadata:
      creation_time   : 2014-05-07 03:24:34
      handler_name    : Apple Alias Data Handler
      timecode        : 01:00:00:00
[Parsed_ass_0 @ 0000000002f61880] Shaper: FriBidi 0.19.6 (SIMPLE)
[Parsed_ass_0 @ 0000000002f61880] Using font provider directwrite
[Parsed_ass_0 @ 0000000002f61880] Added subtitle file: './test_subs.ass' (2 styles, 34 events)
[libx264 @ 0000000002f50cc0] using SAR=118/81
[libx264 @ 0000000002f50cc0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX
[libx264 @ 0000000002f50cc0] profile High, level 3.0
[libx264 @ 0000000002f50cc0] 264 - core 148 r2638 7599210 - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, mp4, to 'test_out.mp4':
  Metadata:
    major_brand     : qt
    minor_version   : 537199360
    compatible_brands: qt
    encoder         : Lavf57.8.102
    Stream #0:0(eng): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuv420p, 720x576 [SAR 118:81 DAR 295:162], q=-1--1, 25 fps, 12800 tbn, 25 tbc (default)
    Metadata:
      creation_time   : 2014-05-07 03:24:23
      handler_name    : Apple Alias Data Handler
      timecode        : 01:00:00:00
      encoder         : Lavc57.7.100 libx264
    Stream #0:1(eng): Audio: aac (libvo_aacenc) ([64][0][0][0] / 0x0040), 48000 Hz, stereo, s16, 128 kb/s (default)
    Metadata:
      creation_time   : 2014-05-07 03:24:23
      handler_name    : Apple Alias Data Handler
      encoder         : Lavc57.7.100 libvo_aacenc
Stream mapping:
  Stream #0:0 -&gt; #0:0 (dvvideo (native) -&gt; h264 (libx264))
  Stream #0:1 -&gt; #0:1 (pcm_s16le (native) -&gt; aac (libvo_aacenc))
Press [q] to stop, [?] for help
[Parsed_ass_0 @ 0000000002f61880] fontselect: (Univers Com 55 Roman, 400, 0) -&gt; UniversCom-55Roman, 0, UniversCom-55Roman
frame= 2771 fps=345 q=-1.0 Lsize=    8949kB time=00:01:50.85 bitrate= 661.3kbits/s
video:7133kB audio:1733kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.944520%
[libx264 @ 0000000002f50cc0] frame I:12    Avg QP:20.94  size: 50712
[libx264 @ 0000000002f50cc0] frame P:789   Avg QP:23.41  size:  6548
[libx264 @ 0000000002f50cc0] frame B:1970  Avg QP:26.27  size:   776
[libx264 @ 0000000002f50cc0] consecutive B-frames:  1.3%  5.5% 19.1% 74.2%
[libx264 @ 0000000002f50cc0] mb I  I16..4: 13.0% 72.4% 14.6%
[libx264 @ 0000000002f50cc0] mb P  I16..4:  1.0%  2.2%  0.4%  P16..4: 31.9%  9.3%  7.5%  0.0%  0.0%    skip:47.7%
[libx264 @ 0000000002f50cc0] mb B  I16..4:  0.0%  0.1%  0.0%  B16..8: 21.4%  0.9%  0.2%  direct: 0.4%  skip:76.9%  L0:41.3% L1:54.7% BI: 4.0%
[libx264 @ 0000000002f50cc0] 8x8 transform intra:64.5% inter:75.4%
[libx264 @ 0000000002f50cc0] coded y,uvDC,uvAC intra: 66.4% 69.9% 34.2% inter: 5.9% 10.1% 0.9%
[libx264 @ 0000000002f50cc0] i16 v,h,dc,p: 18% 65%  3% 14%
[libx264 @ 0000000002f50cc0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 18% 22%  4%  4%  6%  4%  5%  6%
[libx264 @ 0000000002f50cc0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 26% 11%  4%  5%  8%  5%  5%  5%
[libx264 @ 0000000002f50cc0] i8c dc,h,v,p: 42% 25% 28%  5%
[libx264 @ 0000000002f50cc0] Weighted P-Frames: Y:2.3% UV:2.3%
[libx264 @ 0000000002f50cc0] ref P L0: 55.6% 13.1% 21.8%  9.5%  0.0%
[libx264 @ 0000000002f50cc0] ref B L0: 82.5% 13.7%  3.8%
[libx264 @ 0000000002f50cc0] ref B L1: 92.9%  7.1%
[libx264 @ 0000000002f50cc0] kb/s:527.13
</code></pre>

<p>ASS file:</p>

<pre><code>[Script Info]
ScriptType: v4.00+
PlayResX: 1024
PlayResY: 576

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, BorderStyle, Outline, Shadow, Alignment, MarginL, Margi nR, MarginV, AlphaLevel, Encoding
Style: Default,Univers Com 55 Roman,36,&amp;Hffffff,&amp;Hffffff,&amp;H0,&amp;H0,0,0,0,1,2,0,2,10,10,16,0,0

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:00.54,0:00:04.70,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:04.70,0:00:06.96,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:06.96,0:00:11.78,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:11.78,0:00:14.81,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:14.81,0:00:17.39,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:17.39,0:00:20.60,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:20.60,0:00:25.22,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:25.22,0:00:30.31,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:30.31,0:00:34.48,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:34.48,0:00:37.11,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:37.11,0:00:41.28,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:41.28,0:00:45.14,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:45.14,0:00:49.98,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:49.98,0:00:53.79,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:53.79,0:00:54.97,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:54.97,0:00:58.59,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:00:58.59,0:01:01.88,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:01.88,0:01:05.73,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:05.73,0:01:07.95,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:07.95,0:01:10.56,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:10.56,0:01:14.00,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:14.00,0:01:17.27,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:17.27,0:01:19.99,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:19.99,0:01:25.51,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:25.51,0:01:26.48,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:26.48,0:01:27.45,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:27.45,0:01:28.80,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:28.80,0:01:31.37,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:31.37,0:01:31.92,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:31.92,0:01:35.05,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:35.05,0:01:39.18,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:39.18,0:01:42.72,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:42.72,0:01:46.37,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
Dialogue: 0,0:01:46.37,0:01:51.25,Default,,0,0,0,,Lorem ipsum dolor sit amet,\Nconsectetur adipiscing
</code></pre>
","<p>You can try adding the <code>original_size</code> filter option, such as:</p>

<pre><code>-vf ""ass=test_subs.ass:original_size=768x576""
</code></pre>

<blockquote>
  <p>Specify the size of the original video, the video for which the ASS
  file was composed. For the syntax of this option, check the
  <a href=""https://ffmpeg.org/ffmpeg-utils.html#Video-size"" rel=""nofollow"">Video size section in the ffmpeg-utils manual</a>. Due to
  a misdesign in ASS aspect ratio arithmetic, this is necessary to
  correctly scale the fonts if the aspect ratio has been changed.</p>
</blockquote>
","17000"
"What is the name of the device used in live action filming that includes information like scene, take and roll and has a part that can snap together?","387","","<p>What is the name of the device used in live action filming that includes information like scene, take and roll and has a part that can snap together?</p>

<p>It can also include information like production name, date, cameraman, director, timecode, etc. </p>

<p><img src=""https://i.stack.imgur.com/Myiln.jpg"" alt=""enter image description here""></p>
","<p>That gadget actually combines TWO different functions:</p>

<ol>
<li>SLATE which is the lower portion that identifies Scene / Take / Roll, etc. in written form so that the camera(s) can document exactly what this clip is. Without that information editing would be an absolute nightmare for big productions.</li>
<li>CLAPPER which is the part at the top. Essentially two sticks that are snapped together to establish picture vs. sound SYNCHRONIZATION. The camera(s) can see exactly when the two parts come together, and the microphone can ""hear"" when the sticks snap. That sound transient is used to exactly synchronize the sound recording (which is typically done on a separate recorder) with the film or video recording. </li>
</ol>

<p>Sometimes they even put a wireless mic on the slate/clapper to properly ""hear"" the snap, especially for huge scenes where there is no microphone near where the slate/clapper needs to be (to be seen by all the cameras).</p>

<p>What people CALL that gadget depends on tradition and the preferences of the director, etc.</p>
","18653"
"ffmpeg ""Error while opening encoder for output stream #0:1""","386","","<p>Ok, this is probably a very stupid mistake on my part, but I am losing my head here. I have been using a simple command for transcoding MOV file for months now:</p>

<pre><code>ffmpeg -i SOURCE.mov -c:v libx264 -profile:v baseline -pix_fmt yuv420 TRANSCODED.mp4
</code></pre>

<p>I never had a problem with that... until today. I am getting the aforementioned encoder error with this MOV (PCM24 LE, MPEG-2). Here is the log:</p>

<pre><code>ffmpeg version 2.8.4 Copyright (c) 2000-2015 the FFmpeg developers   built with gcc 5.2.0 (GCC)   configuration: --enable-gpl
--enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libdcadec --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-aacenc --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-lzma --enable-decklink --enable-zlib   libavutil      54. 31.100 / 54. 31.100   libavcodec     56. 60.100 /
56. 60.100   libavformat    56. 40.101 / 56. 40.101   libavdevice    56.  4.100 / 56.  4.100   libavfilter     5. 40.101 /  5. 40.101   libswscale      3.  1.101 /  3.  1.101   libswresample   1.  2.101 / 
1.  2.101   libpostproc    53.  3.100 / 53.  3.100 Guessed Channel Layout for  Input Stream #0.0 : 7.1 Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'BL2.mov':   Metadata:
    major_brand     : qt  
    minor_version   : 0
    compatible_brands: qt  
    creation_time   : 2017-03-14 21:48:34
    encoder         : FFmbc 0.7
    timecode        : 00:13:29;19   Duration: 00:12:53.57, start: 0.000000, bitrate: 59235 kb/s
    Stream #0:0(eng): Audio: pcm_s24le (in24 / 0x34326E69), 48000 Hz, 8 channels, s32 (24 bit), 9216 kb/s (default)
    Metadata:
      creation_time   : 2017-03-14 21:48:34
      handler_name    : Apple Alias Data Handler
    Stream #0:1(eng): Data: none (tmcd / 0x64636D74) (default)
    Metadata:
      creation_time   : 2017-03-14 21:48:34
      handler_name    : Apple Alias Data Handler
      timecode        : 00:13:29;19
    Stream #0:2(eng): Video: mpeg2video (4:2:2) (xd5b / 0x62356478), yuv422p(tv, bt709), 1920x1080 [SAR 1:1 DAR 16:9], 50002 kb/s, 29.97 fps, 29.97 tbr, 30k tbn, 59.94 tbc (default)
    Metadata:
      creation_time   : 2017-03-14 22:56:29
      handler_name    : Apple Alias Data Handler
      timecode        : 00:13:29;19 [libx264 @ 0354dc20] using SAR=1/1 [libx264 @ 0354dc20] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.1 Cache64 [libx264 @ 0354dc20] profile Constrained Baseline, level 4.0 [libx264 @ 0354dc20] 264 - core 148 r2638 7599210 - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html - options: cabac=0 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00 [libvo_aacenc @ 03bb0200] Unable to set encoding parameters
Output #0, mp4, to 'bl2.mp4':   Metadata:
    major_brand     : qt  
    minor_version   : 0
    compatible_brands: qt  
    timecode        : 00:13:29;19
    encoder         : FFmbc 0.7
    Stream #0:0(eng): Video: h264 (libx264), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 29.97 fps, 29.97 tbn, 29.97 tbc (default)
    Metadata:
      creation_time   : 2017-03-14 22:56:29
      handler_name    : Apple Alias Data Handler
      timecode        : 00:13:29;19
      encoder         : Lavc56.60.100 libx264
    Stream #0:1(eng): Audio: aac, 0 channels, 128 kb/s (default)
    Metadata:
      creation_time   : 2017-03-14 21:48:34
      handler_name    : Apple Alias Data Handler
      encoder         : Lavc56.60.100 libvo_aacenc
Stream mapping:   Stream #0:2 -&gt; #0:0 (mpeg2video (native) -&gt; h264 (libx264))   Stream
#0:0 -&gt; #0:1 (pcm_s24le (native) -&gt; aac (libvo_aacenc)) Error while opening encoder for output stream #0:1 - maybe incorrect parameters such as bit_rate, rate, width or height
</code></pre>

<p>This seems to tell me that it is an audio issue (even though I transcoded an identically encoded file just last friday...):</p>

<blockquote>
  <p>[libvo_aacenc @ 03bb0200] Unable to set encoding parameters</p>
</blockquote>

<p>So I try to copy the audio codec (not sure if this works for MP4...) with ""-c:a copy"" and I get this:</p>

<blockquote>
<pre><code>Stream #0:1(eng): Audio: pcm_s24le (in24 / 0x34326E69), 48000 Hz, 7.1 (24 bit), 9216 kb/s (default)
Metadata:
  creation_time   : 2017-03-14 21:48:34
  handler_name    : Apple Alias Data Handler Stream mapping:   Stream #0:2 -&gt; #0:0 (mpeg2video (native) -&gt; h264 (libx264))   Stream
</code></pre>
  
  <h1>0:0 -> #0:1 (copy) Could not write header for output file #0 (incorrect codec parameters ?): Invalid argument</h1>
</blockquote>

<p>What is going on here?</p>
","<p>Your AAC encoder only supports 2 channels. Add <code>-ac 2</code>.</p>

<p>But actually, you should upgrade your FFmpeg. It's very old, the VO AAC encoder is no longer available as the native encoder is better and supports 7.1 CH audio, so no downmixing required.</p>
","21032"
"Looking for an accurate Playback FPS from a video","385","","<p>I've made a 1080P video using After Effects, and after it has been rendered the playback is a bit stutter-y. Now I am fairly certain that this stuttering is a result of the computer struggling with the playback, but I need to be able to prove this.</p>

<p>I was kinda hoping VLC has a way of displaying the FPS of the video being played, but as far as I can tell it has no such thing (only display the FPS of the rendering).</p>

<p>Can anyone point me in the direction of some software that will give me the Playback FPS of a video?</p>

<p>Many thanks!</p>
","<p>Personally, I use fraps. The only reason I use fraps is because I used to record desktop footage a ton. However, if you get the <a href=""http://www.fraps.com/download.php"" rel=""nofollow"">free</a> version of fraps, it should allow you to see the framerate of the video playing back while the program is open. </p>
","12361"
"Cropping the video and removing the black portion","383","","<p>I know that the below command will crop 25 from top and bottom </p>

<pre><code>ffplay -i input.mp4 -vf ""crop=in_w:in_h-50""
</code></pre>

<p>However I just want to crop 25 from bottom(not top).
Also How can i remove the black portion in the output so that the output video appears as full screen?</p>

<p>EDIT</p>

<p>Here is the screenshot of the black portion after crop</p>

<p><a href=""https://i.stack.imgur.com/5iD5Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5iD5Z.png"" alt=""aftercrop""></a></p>

<p>Before crop</p>

<p><a href=""https://i.stack.imgur.com/rVnjU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rVnjU.png"" alt=""Before crop""></a></p>
","<p>Use</p>

<pre><code>ffplay -i input.mp4 -vf ""crop=in_w:in_h-25:0:0""
</code></pre>

<p>If you're watching this full screen, then the display will show black bars if the resized video is not the same aspect ratio as your monitor. To achieve that, you'll have to crop horizontally as well. If your crop X pixels vertically, you'll have to crop (16/9)*X pixels horizontally.</p>

<p>To do this, use</p>

<p>(left and right)</p>

<pre><code>ffplay -i input.mp4 -vf ""crop=in_w-25*16/9:in_h-25:(ow-iw)/2:0""
</code></pre>

<p>(right only)</p>

<pre><code>ffplay -i input.mp4 -vf ""crop=in_w-25*16/9:in_h-25:0:0""
</code></pre>
","22104"
"How to scale a layer in pixelated fashion in After Effects?","383","","<p>How to scale a layer in pixelated fashion? I.e. each pixel of the image will grow to the block of pixels in the final scene without smoothing or blending (see picture below):</p>

<p><a href=""https://i.stack.imgur.com/67281.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/67281.png"" alt=""Scaling in progress, from left to right""></a></p>

<p><strong>WORKAROUND</strong>  </p>

<p>To achieve desired effect you could use the <em>Draft</em> option quality of the layer:<br>
<code>Layer &gt; Quality &gt; Draft</code></p>
","<p>Ok, I read your mind. What you are talking about called <strong>Quality and Sampling</strong>. You can find it in the timeline on your layer when ""Switches"" are activated:</p>

<p><a href=""https://i.stack.imgur.com/7EeW8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7EeW8.png"" alt=""Look at the rad arrow""></a></p>

<p>Adjust it to pixelated view, and you will be able to reach your goal without any effects.</p>
","16323"
"Avid import wmv file, just audio no video","382","","<p>I've got a few wmv files, when I import them into avid it seems that the video channel is not being imported. There's just audio. </p>

<p>Am I missing something in the import stage? Or in the handling of the sequence?</p>
","<p>This is likely due to the video codec that you are using. First, an explanation of terms is required: WMV is a container format. These can contain one or more video or audio streams - with video this is normally one video &amp; one audio stream; and these are encoded with various codecs (coder/decoder). I'm not very familiar with the WMV format - for editing I use proper intermediate codecs (Apple ProRes in my case; more in this in a moment), and I prefer MP4 as a delivery codec.</p>

<p>There are broadly three 'types' of video codec: capture codecs, that a video camera will use to record the video; intermediate codecs, that are more suited to post-production editing (they trade larger files for requiring less processing power &amp; the ability to easily cut on any frame); and delivery codecs, that are more compressed (so they can be streamed over the internet, or so you can fit a decent length on a physical medium like a DVD, or for broadcast). There is some overlap between these (many DSLRs use h.264, which is designed to be a delivery codec, to capture video).</p>

<p>This stuff also applies to audio - though there you should use uncompressed PCM audio. However, since audio decompression requires far less processing power, some NLEs will allow you to get away with compressed audio.</p>

<p>I have never heard of the WMV format containing any intermediate formats, as far as I know it's a purely delivery format. That's probably the cause of your problem. I've never used Avid, so I don't know  exactly which intermediate codec you should use. Since it's designed by Avid, the DNxHD codec is a good bet for video (you should convert your audio to PCM audio while you're at it).</p>

<p>Your video editing software probably came with a media conversion tool. You could also use the command-line-based <a href=""https://ffmpeg.org/"" rel=""nofollow"">ffmpeg</a> (<a href=""http://ffmpeg.org/trac/ffmpeg/wiki"" rel=""nofollow"">wiki</a>) to encode to DNxHD. For the following command, you would need to adjust the bit rate - here it is 36 megabits/second - see <a href=""http://www.itbroadcastanddigitalcinema.com/ffmpeg_howto.html#Encoding_VC-3"" rel=""nofollow"">here</a> for some information on what you should use. Note that the ffmpeg syntax used in that guide is somehat out of date; you should use <code>-c:v</code> instead of <code>-vcodec</code> and <code>-b:v</code> instead of <code>-b</code>. This following command will also convert the audio to uncompressed 16-bit PCM, for maximum compatibility:</p>

<pre><code>ffmpeg -i input.wmv -c:v dnxhd -b:v 36M -c:a pcm_s16le output.mov
</code></pre>
","7483"
"How can I insert a censor bar into a mp4 movie?","382","","<p>I have a feature film in mp4 format. There is one scene where I just want to place a black censor box to conceal one part of the screen, no more than 7 seconds.</p>

<p>I'm a beginner, so my concern is that if I import the two-hour, 700MB file into, say, Windows Movie Maker, make the minor change, and then convert the finished product back into mp4, then the finished file will have a huge size that won't fit on a 4.7GB DVD.</p>

<p>Is that a valid concern? Are there video editing softwares out there that somewhat retain original file size?</p>
","<p>Any decent video editing software will allow you to specify bitrate and resolution in export settings. Sony Vegas Pro or Movie Studio, Adobe Premiere Elements, as should Cyberlink Powerdirector among others.</p>

<p>For a 2 hour movie with 700MB as output size, you want a target or average bitrate of 670 kbps for the video and 128 kbps for the audio.</p>
","17338"
"Is there any ffmpeg command for knowing max bit rate of a video?","382","","<p>I have an <code>MP4</code> file <code>input.mp4</code> which is encoded in <code>libx264</code>. By <code>ffprobe</code> command:</p>

<pre><code>ffprobe -v error -show_streams input.mp4
</code></pre>

<p>I can know the bit rate of <code>input.mp4</code>, but it does not give the information of <code>max bit rate</code> (<code>max_bit_rate=N/A</code> ).</p>

<p>Is there any <code>ffmpeg</code> (<code>ffprobe</code>) command for getting <code>max bit rate</code> of a video?</p>
","<p>That <code>max_bit_rate</code> is a a tag written by some encoders, when set by the user in the export dialog box. It is not an indication of the maximum bitrate found if you were to survey the encoded bitstream.</p>

<hr>

<p>It's possible to get a good idea of the maximum bitrate of a stream this way:</p>

<pre><code>ffmpeg -i in.mp4 -map 0:v -c copy
       -f segment -segment_time 1 -break_non_keyframes 1 folder\seg%d.264
</code></pre>

<p>This will break up the video stream into 1 second segments. Sort the directory listing by size to find out the largest file. That's your maximum bitrate. Not totally accurate as it's not a rolling survey but should be close enough.</p>
","21667"
"How to display subtitles as default in an MKV file","381","","<p>I have an MKV file with multiple subtitles.  I would like to mark one set of those subtitles as default, so that it is turned on by default when the video starts playing.  NOTE: I do not want to ""burn"" this subtitles set into the video (I want to be able to turn these subtitles off at any point in time if I so choose).</p>

<p>Is there an MKV editing tool that would allow me to do this?</p>
","<p>There are two aspects to your query - 1)setting one of the subtitle streams as the default and 2)automatically displaying it during playback.I'm not aware of a method to force a player to display subtitles if that facility has been turned off in its settings.</p>

<p>For the first facility, try mkvpropedit from <a href=""https://mkvtoolnix.download/index.html"" rel=""nofollow"">MKVToolNix</a> to set the default flag for a subtitle stream. See 2nd <a href=""https://mkvtoolnix.download/doc/mkvpropedit.html#mkvpropedit.examples"" rel=""nofollow"">example</a>.</p>
","17051"
"How to help the autofocus feature of an amateur camcorder","380","","<p>I just acquired an amateur camcorder (Samsung HMX-Q10) that has only a few settings available, most of them are automatic.</p>

<p>I tried to shoot a video of me playing a bass guitar cover but it seems that the image is always slightly out of focus. I'm just slightly blurred but with HD image this kind of detail is really noticeable.</p>

<p><strong>Shooting environment</strong> : me standing in front of the camcorder (~2-3 meters) with an instrument, in my appartment (walls mostly white), natural winter/cloudy lighting, white /  bright brown background, black clothes, red guitar.</p>

<p><strong>Cam specifications/settings</strong> : Full HD 1920x1080/50i, Exposure Value : 0, Shutter 1/50 of a second (there is a ""low light mode"" that set it to 1/25 but it adds colored artifacts).</p>

<p>I can't notice that the image is blurred on the camcorder small monitoring screen, but when I open the video on the computer it is more obvious (especially because the guitar brand label is blurred), so without proper feedback it is quite hard to manually set the focus.</p>

<p>So what can I do to help the autofocus ?<br>
What can I change in the shooting environment to improve its accuracy ?</p>

<p>I'm thinking, maybe the lighting, the colors (background / clothes), some camcorder settings, another element I forgot ?</p>
","<p>I reviewed the specifications for the Samsung HMX-Q10 and have to say there is not a whole lot of information e.g., optics has no mention of ""Minimum Focus Distance"". So we are pretty much in the dark on the near boundary of the depth of field.</p>

<p>(a little rant on auto-focus)</p>

<p>Most auto focus systems have two modes: Single AF and Full-time AF. Single AF translates into setting the focus once, focusing what ever the sensor picks up first usually in the center of the frame (there are other possible areas depending on the camera, but I'm guessing this Samsung is limited to center weighted focus). Single AF keeps the same focus area during the whole shot. Full-time AF changes focus according to what is changing in the frame. This would be good for tracking a sports event or something moving. Unfortunately the Samsung specs don't mention any of this so I am not sure if this camera has two modes, or one, or if one mode, which one Samsung made the default.</p>

<p>Here is how I would approach shooting your bass playing.</p>

<p>1) mount the camera on a tripod so the camera is steady.</p>

<p>2) Likely 1 meter to 2 meters will be far enough away to be in focus. I recommend that you
   experiment with this to see if 1 meter is better than 2 or if 2 is better than 3 meters.</p>

<p>3) light the set with as much light as you can as this will force the auto-exposure to
   'stop-down' the iris as in a smaller aperture, which translates into more depth of 
   field.</p>

<p>4) in front of the camera, dead center, put a chair or something where you will be playing,
   (use some gaffers tape to mark this spot off camera on the floor assuming your not
   going to show your feet) this will force the auto-focus to focus on the space you are
   going to be.</p>

<p>5) set the camera zoom to full wide (the wider you set the lens, the greater the depth
   of field).</p>

<p>6) press the record button, walk to your spot toss the chair or prop that was standing in
   for you and start your action. All the stuff that happened before you started playing
   your bass can be cut out in post production.</p>

<p>Alternately you can have a friend stand in for you on your mark before you start the camera, you could even have them give an intro then decide later to keep it or cut it and go direct to the bass playing. Also, you could have an assistant do all this camera work for you while you stay in place to start the show.</p>

<p>Quick review:
1) tripod to steady the camera,
2) lots of light,
3) set zoom to widest angle,
4) have your spot in focus before you start.</p>
","3202"
"What -preset medium stand for in ffmpeg","380","","<p><a href=""https://trac.ffmpeg.org/wiki/Encode/H.264"" rel=""nofollow noreferrer"">Here</a> i found </p>

<pre><code>-preset medium
</code></pre>

<p>I was wondering for what it stand for is it for video quality ?</p>
","<p><code>preset</code> is a private option that may be defined for an encoder, such as libx264. It is a shorthand method to set a whole bunch of options at once, and won't have any effect if set when using an encoder which doesn't define them.</p>

<hr>

<p>When using x264 to encode to H.264 with preset medium, these are the arguments set:</p>

<pre><code>cabac=1
ref=3
deblock=1:0:0
analyse=0x3:0x113
me=hex
subme=7
psy=1
psy_rd=1.00:0.00
mixed_ref=1
me_range=16
chroma_me=1
trellis=1
8x8dct=1
cqm=0
deadzone=21,11
fast_pskip=1
chroma_qp_offset=-2
threads=12
lookahead_threads=2
sliced_threads=0
nr=0
decimate=1
interlaced=0
bluray_compat=0
constrained_intra=0
bframes=3
b_pyramid=2
b_adapt=1
b_bias=0
direct=1
weightb=1
open_gop=0
weightp=2
keyint=250
keyint_min=25
scenecut=40
intra_refresh=0
rc_lookahead=40
rc=crf
mbtree=1
crf=23.0
qcomp=0.60
qpmin=0
qpmax=69
qpstep=4
ip_ratio=1.40
aq=1:1.00
</code></pre>

<p>The default preset is <code>medium</code>. You can override any of the individual variables by adding <code>-x264opts key=value</code> <strong>after</strong> the preset option. But you shouldn't unless you need to.</p>
","20172"
"How to make Sony Vegas Pro 13 work on a 4K display","379","","<p>I bought Sony Vegas Pro 13 recently for my 4K laptop, but when I fired it up, the loading screen did not look good. The text was running into itself, and it didn't fit in the window.</p>

<p><a href=""https://i.stack.imgur.com/bAOo8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bAOo8.png"" alt=""mangled loading screen""></a></p>

<p>When the application loaded, the buttons and strips on the editor were only about 5mm tall.</p>

<p><a href=""https://i.stack.imgur.com/8ikLo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8ikLo.png"" alt=""Vegas main window with shrunken buttons and strips""></a></p>

<p>Is there a way to make Vegas work on a 4K screen? Are there settings I can change, or a patch I can install? And if not, is there something in the works for this? I will add that I did try changing the screen resolution, but it still looks like this.</p>
","<p>There's apparently a hack which alters the Vegas executable to solve this problem. There's a setting of <code>dpiAware</code> in the application manifest which falsely tells Windows that Vegas is DPI aware and will scale up text and icons on a HiDPI display. Vegas does not do this. So, the value should be changed from true to false.</p>

<p>The hack tutorial alongwith link to required tool is available at <code>https://www.youtube.com/watch?v=xmB6773HfwE</code></p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/xmB6773HfwE?start=0""></iframe>
            </div></div></p>

<p>I haven't needed to do this so I can't vouch for the method but it appears to work.</p>
","18474"
"Near natural lighting with softbox wattage between 2400 to 2700?","379","","<p>I record video with a large window as a softbox during the day, but I want to record during the night keeping those nice ambient shadows and even lighting.</p>

<p>I'm planing to buy a 3 piece softbox kit. I currently have DSLR with 50 mm f 1.8 and 80 mm f1.4 lenses which give a nice light aperture. The more natural the light is the better. </p>

<p>Here's a sample of my setup with natural light (large window, no additional lights):</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/olV-0YDTJ3U?start=0""></iframe>
            </div></div></p>

<p>I found these two kits that caught my attention, my budget is around 200 bucks.</p>

<p>ePhoto 2400:
<a href=""http://goo.gl/0GfWdf"" rel=""nofollow"">http://goo.gl/0GfWdf</a></p>

<p>ePhoto 2700:
<a href=""http://goo.gl/6Wov5S"" rel=""nofollow"">http://goo.gl/6Wov5S</a></p>

<p>What is the best wattage between these two for a near natural lighting? If not what is the right ammount of wattage to archive it?</p>
","<p>Wattage has nothing to do with achieving natural lighting.  The amount of diffusion and placement does.  More light is ALWAYS better because it results in more signal to the camera and more options as far as ISO, aperture and shutter speed.  </p>

<p>Nothing you setup with artificial lights (particularly on your budget) will come close to the level of the sun on even a cloudy day.  This doesn't matter though as long as you provide enough light for the camera to pick it up and spread it out through diffusers and light placement and reflectors such that you get a natural look.</p>

<p>I'd suggest reading some on 3 point lighting if you want more information on light placement and get the biggest diffusers you can, though on your budget, you will be hard pressed to get even a decent 3 point light kit, let alone any kind of diffusers.  (A decent large diffuser alone will run over $100.)  Your best bet is to put all the money in to the best light kit you can get and get diffusers later.  Proper three point lighting will still be fairly natural looking.</p>
","9513"
"removing an x265 audio track without conversion?","379","","<p>I have an x265 video file with dual audio tracks in different languages. Obviously one is useless to me and I want to remove it without waiting through a lengthy conversion that will lead to quality loss.</p>

<p>I have been using <a href=""https://sourceforge.net/projects/mkv2mp4/"" rel=""nofollow"">mkv2mp4</a> to do this with x264 in the past, but it spits out an error when I try any x265 coded video. Is there a codec I can install? Any other suggestions?</p>
","<p>Use <a href=""https://ffmpeg.org/download.html"" rel=""nofollow"">ffmpeg</a>*, a free command-line tool: </p>

<pre><code>ffmpeg -i input.mkv -c copy -map 0:v -map 0:a:0 singleaudio.mkv
</code></pre>

<p>where <code>0:a:0</code> is the index of the audio stream you wish to keep. In ffmpeg, numbering starts from zero, so <code>0:a:0</code> refers to the first audio stream in the source file. Running <code>ffprobe input.mkv</code> beforehand will show the contents of the input.</p>

<p>*download the latest snapshot/nightly build.</p>
","17880"
"How to make sense of video file formats?","378","","<p>In digital video, there are all these different kinds of files: m4v, mov, mkv, mts, mp4, mpeg, and a host of others, and I can't seem to find a clear answer as to what each is good for.</p>

<p>(By contrast, when you're taking still pictures, you have two choices: JPG or RAW; with JPG, you basically know what the file contains, and with RAW, it's going to vary by camera manufacturer. I know it's a little more complicated than that, but it's nothing like the explosion of options available in digital video.)</p>

<p>Furthermore, it seems like the video's file extension doesn't tell you what's inside the file. From what I understand, a .mov file (for example) could contain just about anything.</p>

<p>Then there is this whole thing about h.264, AIC, ProRes, iFrame, AVCHD, DV, HDV -- at least I think those all belong to the same category, but I'm probably mistaken.</p>

<p>Can anyone help me understand all the different variables that are in play here so that I can make intelligent choices about what I do with the files coming off my camcorder?</p>
","<p>File formats are essentially wrappers, a container of sorts. The video information is encoded in a codec (Coder/Decoder). Some file formats only work with certain codecs. This is due (in part) to corporate/organizational pissing contests (or format wars - remember dvd+ vs. dvd -?). Codecs come with varying degrees of compression. The more compressed a codec is, the smaller your files will be, but they will also potentially be losing information, in the same way that a RAW file saves everything from CCD, and JPEG compresses the file by dropping repeated information. Also, most pro level NLEs (like Final Cut Pro (not FCPX)) play nicer with codecs that are less compressed. The more compressed it is the more the computer has to work to decode and re-enconde the information when you make changes (moving on the time line, cutting, adding effects etc.)</p>

<p>Recommendations:
For recording and editing (HD), pick a low compression, high quality codec such as DVCProHD, AVCHD, Apple ProRes.</p>

<p>To export for sharing I recommend H.264. It is supported nearly universally and is very good quality with relatively small files. BluRay videos are actually encoded in an ultra high quality H.264</p>

<p>One other note: if your camcorder saves files in a weird format (ie. not mov, mpeg, mp4), you will need to use a transfer process in your NLE. Be sure to save the ENTIRE folder structure from your memory card, not just the mts, mxf, or whatever files.</p>

<p>More tips: if you're not sure what codec a file is in, you can open it in QuickTime and hit cmd+i to open the inspector window. It will have all the codec information there.
Additionally, on Windows there is a small utility called G-SPOT which can show you what codec are you using on that file etc.</p>
","2294"
"How do I resize the canvas of a image layer in After Effects?","377","","<p>I'm using <strong>After Effects CC 2017</strong>. I've imported a PNG image with transparency, then I added a simple <strong>CC Light Wipe</strong> effect:</p>

<p><a href=""https://i.stack.imgur.com/CXonX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CXonX.png"" alt=""enter image description here""></a></p>

<p>The problem is that the range of the lights produced by this effect are limited to the image size, so it produces an annonying thingy. Is there a way to resize the canvas of the layer of the image or another way to solve this (avoiding the need to resize the image canvas using external programs like <strong>Photoshop</strong>)?. </p>

<p>I really tried to find out the answer researching in Google before asking here but I only found questions about resizing the entire compossition in the compossition settings...</p>
","<p>Add an adjustment layer on top (right-click in empty area of the layer names. Go to New menu) Apply the effect to that.</p>
","20226"
"Color calibration with a color chart","375","","<p>Even if I set up a custom white balance in my software (with the traditional way to do it, using a <em>grey card</em> or <em>white balance card</em>, etc.), sometimes the colors are not exactly like I would like.</p>

<p>I would like to be able to :</p>

<ul>
<li>shoot a color chart with <strong>normalized colors</strong> (example: <a href=""http://www.ephotozine.com/articles/Kodak-EasyShare-V803-8mp-5253/images/kodak_V803_colour_chart.jpg"" rel=""nofollow noreferrer"">GretagMacbeth</a>)</li>
<li>import this video in Premiere (or any other software), and match all colors between left and right panel :</li>
</ul>

<p><img src=""https://i.stack.imgur.com/7AobZ.jpg"" alt=""enter image description here""></p>

<p>Then the colors would be really calibrated.</p>

<p><strong>Does such a tool exist?</strong></p>
","<p>I'm not aware of any that work directly in Premiere, however I can use my Spyder Checkr24 to either adjust my videos in Adobe Lightroom or DaVinci Resolve.  For Lightroom, I use the software that came with my Spyder Checkr to make a preset I can apply.  In Resolve, there is actually built in native support for working with the swatches.</p>

<p>There are also other third party tools available that I haven't really looked in to, so I can't say for certain that there isn't a way to do it directly in Premiere with a third party tool, but the way I've been doing it has worked well enough that I have not bothered to look any further in to doing it directly in Premiere.</p>

<p>For more details on how things work in Resolve, you can check out <a href=""https://www.youtube.com/watch?v=onom8tpiof8"" rel=""nofollow noreferrer"">this video</a> or if you have a decent idea about how to use Resolve already, take note of the steps in this screenshot.  Start with 0 by choosing the target you are using on the Color workspace.  Then select the color match grid tool on the preview screen(1).  Position the grid over your color target during a portion of the clip that has the target visible (2).  Finally, hit Match(3) in order to have it generate the color correction profile for the clip.</p>

<p><img src=""https://i.stack.imgur.com/zwLGA.jpg"" alt=""enter image description here""></p>
","15922"
"Configure FreeStyler to work with DMXking ultraDMX RDM Pro/Micro","374","","<p>I have a USB DMX product called <a href=""http://www.dmxking.com/usbdmx/ultradmxrdmpro"" rel=""nofollow"" title=""DMXking ultraDMX RDM Pro"">DMXking ultraDMX RDM Pro</a> (I've also tried the DMXking ultraDMX Micro, same issue) and I'm using the free software <a href=""http://www.freestylerdmx.be/"" rel=""nofollow"" title=""FreeStyler DMX"">FreeStyler</a> and trying to figure out how to get it to work. These products are for controlling DMX light fixtures.</p>
","<p>Select either <strong>DMXking USB DMX512-A</strong> or <strong>Enttec Pro</strong>. Do not choose ultraDMX Pro unless you are actually using this device. Also make sure you've selected the right COM port, plus make sure the latest driver from FTDI Chip is installed (<a href=""http://www.ftdichip.com/Drivers/VCP.htm"" rel=""nofollow noreferrer"">FTDI Drivers</a>). A good practice is to use the <a href=""http://www.dmxking.com/downloads/ultraDMX_Configuration.zip"" rel=""nofollow noreferrer"">ultraDMX Configuration Utility</a> to help locate this if necessary. Sometimes FreeStyler gives an error message during device selection, just restart it.</p>
","17289"
"Glidecam focus control","373","","<p>Is there puller for controlling the lens focus when using a glidecam? Is there also something similar for the zoom, in case it's a zoom lens?</p>

<p>I have not found anything off the shelf that I can buy online.</p>
","<p>I don't think there is anything specifically made for use with a glidecam.  My impression of most stabilization systems is that you still operate the camera directly, you just are not supporting it without the stabilization.  I'm not aware of any follow focus/pull systems that are designed specifically for use with a gimble or gimble and spring-arm stabilization system.</p>
","7831"
"Tool to transcode Canon PF24 MTS files to native 24p?","373","","<p>Many Canon AVCHD camcorders record 24fps in a format they call PF24, which essentially disguises the 24 fps in a 60i stream by applying 3:2 pulldown.</p>

<p>Unfortunately they do this so well that to my knowledge editors can't detect it automatically and apply the reverse transformation. I have verified that this is true for Sony Vegas Pro 10 on Windows and FCPX on OS X, which treat these files as regular 60i material. When these clips are added to a 24p timeline they get the regular deinterlacing treatment, which looks awful.</p>

<p>Are there any tools (command line preferred) that can take a .MTS file with PF24 material and perform the inverse telecine process to convert to native 24p with minimal quality loss?</p>
","<p>The usual tool recommended for this is JES Deinterlacer, which apparently has an adaptive method of finding and removing the 'smear' frames. I haven't used it myself.</p>

<p>The reason this is an issue is that the field cadence used for PF24 is (stupidly, IMO) different from the typical telecine cadence, which is what most 'inverse telecine' algos detect and remove. So applying standard inverse telecine just adds another layer of mess-up to the video.</p>
","4365"
"Android - Media Encoder HEVC Plays, FFMPEG HEVC Won't","367","","<h2>The Problem</h2>

<p>I've been fighting with this for days. I'm writing a video application for Android to be used on an Odroid C2. I've been simultaneously developing some scripts that utilize ffmpeg to produce files to be played on the Odroid.</p>

<p>My issue is that videos encoded with FFMPEG won't play. At first I thought it was my application, but they won't play from a jump drive, either. Videos produced by Adobe's Media Encoder work fine.</p>

<p>Both are encoding the same tiff image sequence. I've attached the MediaInfo screenshots below. If anyone could shed some light on something I don't know, it would be greatly appreciated.</p>

<h2>Media Encoder</h2>

<p><a href=""https://i.stack.imgur.com/nvbsI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nvbsI.png"" alt=""Media Encoder""></a></p>

<h2>FFMPEG</h2>

<p><a href=""https://i.stack.imgur.com/J6BET.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J6BET.png"" alt=""FFMPEG""></a></p>

<h2>Console Output</h2>

<p>Here are some related console messages from Android. It looks like Android can't identify the codec for some reason. =/</p>

<pre><code>12-19 22:14:56.216 16588-16602/? V/AmThumbnail: vp_open=amthumb:AmlogicPlayer=[f68dfff0:972000f],AmlogicPlayer_fd=[f68e0008:971fff7]
12-19 22:14:56.216 16588-16602/? V/AmThumbnail: android_open amthumb:AmlogicPlayer=[f68dfff0:972000f],AmlogicPlayer_fd=[f68e0008:971fff7] OK,h-&gt;priv_data=0xf68e0008
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: No profile indication! (4)
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: Error decoding profile tier level.
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: Error parsing NAL unit #0.
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: No profile indication! (4)
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: error decoding profile tier level
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: Error parsing NAL unit #0.
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: SPS does not exist 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.220 16588-16602/? I/exffmpeg: Error parsing NAL unit #0.

...

12-19 22:14:56.336 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.336 16588-16602/? I/exffmpeg: PPS id out of range: 0
12-19 22:14:56.336 16588-16602/? I/exffmpeg: [hevc @ 0xf68c5800] 
12-19 22:14:56.336 16588-16602/? I/exffmpeg: Error parsing NAL unit #0.
12-19 22:14:56.336 16588-16602/? I/exffmpeg: [mov,mp4,m4a,3gp,3g2,mj2 @ 0xf6887c00] 
12-19 22:14:56.336 16588-16602/? I/exffmpeg: decoding for stream 0 failed
12-19 22:14:56.337 16588-16602/? I/exffmpeg: [mov,mp4,m4a,3gp,3g2,mj2 @ 0xf6887c00] 
12-19 22:14:56.337 16588-16602/? I/exffmpeg: Could not find codec parameters for stream 0 (Video: hevc (hev1 / 0x31766568), 3840x2160, 33528 kb/s): unspecified pixel format
                                             Consider increasing the value for the 'analyzeduration' and 'probesize' options
</code></pre>

<h2>Edit</h2>

<p>This is the ffmpeg command being used. I'm encoding a test sequence of 300 frames.</p>

<pre><code>ffmpeg -framerate 29.97 -start_number 2000 -i example_%05d.tif -c:v libx265 -preset ultrafast -crf 18 -vf ""colormatrix=fcc:bt709"" -pix_fmt yuv420p -color_range 2 output.mp4
</code></pre>

<p>FFMPEG Console Output</p>

<pre><code>ffmpeg -y -framerate 29.97 -start_number 2000 -i attd_1p_%05d.tif -c:v libx265 -preset ultrafast -crf 18 -vf ""colormatrix=fcc:bt709"" -pix_fmt yuv420p -color_range 2 ../H265_TEST.mp4
ffmpeg version 3.1.5-1~xenial Copyright (c) 2000-2016 the FFmpeg developers
  built with gcc 5.4.0 (Ubuntu 5.4.0-6ubuntu1~16.04.2) 20160609
  configuration: --prefix=/usr --extra-version='1~xenial' --libdir=/usr/lib/ffmpeg --shlibdir=/usr/lib/ffmpeg --disable-static --disable-debug --toolchain=hardened --enable-pthreads --enable-runtime-cpudetect --enable-gpl --enable-shared --disable-decoder=libopenjpeg --disable-decoder=libschroedinger --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxvid --enable-libzvbi --enable-openal --enable-opengl --enable-x11grab --enable-version3 --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libvo-amrwbenc --enable-libdc1394 --enable-libiec61883 --enable-libzmq --enable-frei0r --enable-libx264 --enable-libopencv --enable-libkvazaar --enable-libopenh264 --enable-nonfree --enable-libfdk-aac --enable-libfaac
  libavutil      55. 28.100 / 55. 28.100
  libavcodec     57. 48.101 / 57. 48.101
  libavformat    57. 41.100 / 57. 41.100
  libavdevice    57.  0.101 / 57.  0.101
  libavfilter     6. 47.100 /  6. 47.100
  libavresample   3.  0.  0 /  3.  0.  0
  libswscale      4.  1.100 /  4.  1.100
  libswresample   2.  1.100 /  2.  1.100
  libpostproc    54.  0.100 / 54.  0.100
Input #0, image2, from 'attd_1p_%05d.tif':
  Duration: 00:00:10.01, start: 0.000000, bitrate: N/A
    Stream #0:0: Video: tiff, rgb24, 3840x2160 [SAR 1:1 DAR 16:9], 29.97 tbr, 29.97 tbn, 29.97 tbc
x265 [info]: HEVC encoder version 1.9
x265 [info]: build info [Linux][GCC 5.3.1][64 bit] 8bit+10bit+12bit
x265 [info]: using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX AVX2 FMA3 LZCNT BMI2
x265 [info]: Main profile, Level-5 (Main tier)
x265 [info]: Thread pool created using 16 threads
x265 [info]: frame threads / pool features       : 5 / wpp(68 rows)
x265 [info]: Coding QT: max CU size, min CU size : 32 / 16
x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra
x265 [info]: ME / range / subpel / merge         : dia / 57 / 0 / 2
x265 [info]: Keyframe min / max / scenecut       : 25 / 250 / 0
x265 [info]: Lookahead / bframes / badapt        : 5 / 3 / 0
x265 [info]: b-pyramid / weightp / weightb       : 1 / 0 / 0
x265 [info]: References / ref-limit  cu / depth  : 1 / 0 / 0
x265 [info]: AQ: mode / str / qg-size / cu-tree  : 1 / 0.0 / 32 / 1
x265 [info]: Rate Control / qCompress            : CRF-18.0 / 0.60
x265 [info]: tools: rd=2 psy-rd=2.00 early-skip tmvp fast-intra
x265 [info]: tools: strong-intra-smoothing lslices=8 deblock
[mp4 @ 0x17c7d00] Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead.
Output #0, mp4, to '../H265_TEST.mp4':
  Metadata:
    encoder         : Lavf57.41.100
    Stream #0:0: Video: hevc (libx265) ([35][0][0][0] / 0x0023), yuv420p(pc), 3840x2160 [SAR 1:1 DAR 16:9], q=2-31, 29.97 fps, 11988 tbn, 29.97 tbc
    Metadata:
      encoder         : Lavc57.48.101 libx265
Stream mapping:
  Stream #0:0 -&gt; #0:0 (tiff (native) -&gt; hevc (libx265))
Press [q] to stop, [?] for help
frame=  300 fps= 12 q=-0.0 Lsize=  224166kB time=00:00:09.90 bitrate=185304.4kbits/s speed=0.389x    x    
video:224158kB audio:0kB subtitle:0kB other streams:0kB global headers:1kB muxing overhead: 0.003441%
x265 [info]: frame I:      2, Avg QP:20.61  kb/s: 544482.73
x265 [info]: frame P:     75, Avg QP:22.74  kb/s: 341028.20
x265 [info]: frame B:    223, Avg QP:25.66  kb/s: 127208.97
x265 [info]: consecutive B-frames: 2.6% 1.3% 0.0% 96.1% 

encoded 300 frames in 25.62s (11.71 fps), 183445.62 kb/s, Avg QP:24.90
</code></pre>
","<p>With <a href=""https://video.stackexchange.com/users/1871/mulvya"">Mulvya's</a> help, I figured out the issue. FFMPEG was using a codec tag of <code>hev1</code>, while Media Encoder was using <code>hvc1</code>. Nothing I tried in FFMPEG would change this. </p>

<p><a href=""https://video.stackexchange.com/users/1871/mulvya"">Mulvya</a> suggested I use mp4box to repackage. <code>mp4box -add out.mp4 -new final.mp4</code> didn't work. But, after a little more searching, I ended up following the suggestion <a href=""https://stackoverflow.com/questions/32152090/encode-h265-to-hvc1-codec"">here</a> and used mp4box to extract the raw .hvc file from the file that FFMPEG gave me, and repackaged it into a new mp4 file. That seemed to fix it.</p>

<hr>

<h2>Note</h2>

<p>Something that <strong>doesn't</strong> make sense is the fact that I ran a Media Encoder file through FFMPEG with <code>-c copy</code>, and the resulting file had a codec tag of <code>hev1</code>, but it still played on the Odroid players. Based on that, I wouldn't have expected the solution above to work, but it did.</p>

<p><strong>Edit</strong> - As pointed out by <a href=""https://video.stackexchange.com/users/1871/mulvya"">Mulvya</a>, the explanation for the above note is that even with the tag <code>hev1</code>, the stream from Media Encoder has the PS where the player expects it.</p>
","20192"
"How are 3-dimensional overlays synchronized with videos?","367","","<p>One popular feature on TV nowadays is to create 3D overlays on top of the recorded content. Also, as the camera changes direction or position, the 3D content view is subsequently adjusted. How is this implemented?</p>
","<p>There are two different approaches that can be used.  Motion/Camera tracking and Motion Control.</p>

<p>In Motion or Camera tracking, multiple non-coplanar fixed points in the scene are tracked and from that it is possible to extrapolate the position of the camera within the scene.  This information can then be fed into a virtual camera used to generate the output from the 3d model such that when the 3d model output is superimposed over the video, the movements line up.</p>

<p>Motion Control is similar but uses a special camera rig to either track the camera movement exactly or to reproduce a movement plotted in the 3d software exactly in real life.  This is the more precise but also much more sophisticated approach as it requires far more elaborate gear to do.  It also allows for more accurate tracking of things like change in zoom verses actually moving the camera in (which are otherwise only evident by fairly subtle shifting of perspective and vanishing points.)</p>

<p>The result of both is effectively similar though.  You end up with data on how the camera moves in the real life scene and it matches the movement done in the 3d scene.  There are other factors such as lighting that also have to be matched for the best looking composition.  Then both outputs are brought in to a composition package like After Effects where they can be overlayed and blended together.  Sometimes various additional effects like depth of field and other virtual camera effects may be applied to help it blend.</p>

<p>Another thing is that if anything in the real scene needs to move in front of the 3d object, then they need something called a mask layer.  This is normally produced by rotoscoping, which is a process of updating the image on every frame to follow the subject.  This mask then tells the compositing software that the real video plate should be placed over the 3d plate on the areas where the mask is painted.  Otherwise the 3d plate is used over the real plate.  Alpha channel blending is used to convey information about the transparency of a 3d shape such that the image behind it can be visible.</p>

<p>If the 3d object has any refractive properties, then the real life plate may be fed into the 3d software as an environment.  Then the 3d render engine can actually sample information from the environment in order to figure out refraction in things like a glass object or a 3d object with a mirror.  Since it is mapping a 2d environment onto a 3d one, a lot of the selection of how the environment is applied would be done manually though.</p>
","8019"
"Add Black background to a vertical video","366","","<p>I have several vertical videos which I would like to display in the 16/9 or 4/3 format.</p>

<p>I would like to have the video centered and have a black Background.</p>

<p>I normally use Handbrake to compress my videos but did not find an option to do that.</p>

<p>Is there another Video compressor (open source) who allows to do that? Other wise I have to compress it with handbrake and add the background with Premiere, which takes ages.</p>

<p>Really would like an easier solution.</p>

<p>Thanks in advance for any response.</p>
","<p>You can use <a href=""https://ffmpeg.org/download.html"" rel=""nofollow"">ffmpeg</a>*, a command-line tool, to do this.</p>

<p>Let's say your source video is <code>360x640</code>, then to make it 4:3, use</p>

<pre><code>ffmpeg -i input.mp4 -vf pad=854:640:247:0 -c:v libx264 -crf 20 -c:a copy output.mp4
</code></pre>

<p>where <code>854</code> is used because it's <code>4/3</code> of <code>640</code>, and <code>247</code> places the video in the center of the padded canvas. See details for the pad filter <a href=""https://ffmpeg.org/ffmpeg-filters.html#pad-1"" rel=""nofollow"">here</a>.</p>

<p>Similarly for 16:9, use</p>

<pre><code>ffmpeg -i input.mp4 -vf pad=1138:640:389:0 -c:v libx264 -crf 20 -c:a copy output.mp4
</code></pre>

<p>where <code>1138</code> is used because it's <code>16/9</code> of <code>640</code>, and <code>389</code> for centering.</p>

<p>The width value you specify should be an even integer, so you may have to round up/down your calculated width.</p>

<p>*get the latest nightly or snapshot binary for your platform.</p>
","17699"
"Can I use h.264 codec inside .mp4 container for my website","364","","<p>I would like to use h.264 as the codec inside .mp4 container for my user uploaded videos for my website. But I saw somewhere that h.264 is non free. I am unsure whether I can use this technology or not. My question whether I can use it to compress my user uploaded videos in my website? If yes, then its great. And if no, what other solution can I go with? Thank you.</p>
","<p>You are correct: h.264 is non-free and content producers as well as developers are obliged to pay royalties to MPEGLA for its use - but <strong>only</strong> if they are charging for their content, <strong>and</strong> the volume goes over the threshold MPEGLA sets (which is in the order of 100,000 paying subscribers or > 12 minutes in length if charging title-by-title). Thjey have also said that they will <em>never</em> charge for h.264 video that is freely distributed.</p>

<p>See <a href=""https://video.stackexchange.com/questions/14694/mp4-h-264-patent-issues/14699#14699"">MP4 / h.264 patent issues?</a>
for more info.</p>

<p><strong>TL;DR</strong> if your video is free to view, then h.264 is free for you to use, and as @LordNeckbeard points out, it's a ripper of a codec, so knock yourself out.</p>
","14872"
"Is H.264 subject to exploits / are some H.264 codecs subject to exploits?","364","","<p>How safe are H.264 streams from exploits? More exactly, are the consumer-grade H.264 codes generally ""safe""?</p>

<p>I am currently gratuitously transcoding A/V files to avoid being exposed to possible exploits. This was after I noticed that some older video files will have very curious errors in them, errors that make me wonder if they are indeed exploit attempts (""strange, why's there a STRING in the middle of the stream?""). I do know that in some older formats/codecs, there were indeed past exploits.</p>

<p>Since H.264 codecs are a bit more modern than some of the older ones, I would like to know if such an elementary stream is safe in the ""usual suspect"" players: Windows Media, VLC, Quicktime and the major browsers (I use Chrome and Firefox, but should include Safari and InternetExploder just in case).</p>

<p>In that case, if the video is an H.264 elementary stream, I would not need to do a sanitization transcode step, and just create a new MP4 container. This would at least remove any container-based exploits. In other words:</p>

<pre><code>ffmpeg -i &lt;file&gt; -c copy newcontainer.mp4
</code></pre>

<p>(or -c:v copy, and transcode the audio, etc).</p>

<p>Clearly, white room safety would require transcoding everything, but that's getting all Felix Unger.</p>
","<p>All anyone might be able to say is that there are no <strong>known</strong> holes in the various players.  (I don't know if that's true, just that it's impossible to know that there are no undiscovered bugs in a complex piece of code.)  H.264 streams are complex enough to have lots of corner cases.  They're parsed with speed-optimized code written in C and assembly.</p>

<p>You're quite correct that this is a HUGE attack surface, esp. when you allow arbitrary input formats, including ones with crufty old demuxers / decoders written by some random guy a long time ago, who might or might not have had security in mind.  <a href=""http://googleonlinesecurity.blogspot.ca/2014/01/ffmpeg-and-thousand-fixes.html"" rel=""nofollow"">This article</a> about ffmpeg security fixes is quite interesting.</p>

<p>re: ascii strings inside video files.  That's not uncommon at all.  x264 output embeds a string like</p>

<pre><code>x264 - core 142 r2455 021c0dc - H.264/MPEG-4 AVC codec - Copyleft 2003-2014 - http://www.videolan.org/x264.html - options: cabac=1 ref=6 deblock=1:2:2 analyse=0x3:0x133 me=umh subme=10 psy=1 psy_rd=1.00:0.50 mixed_ref=1 me_range=24 chroma_me=1 trellis=2 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-4 threads=6 lookahead_threads=1 sliced_threads=0 nr=200 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=6 b_pyramid=2 b_adapt=2 b_bias=0 direct=3 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=60 rc=crf mbtree=1 crf=25.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:0.50
</code></pre>

<p>And of course <code>strings</code> will find lots of sequences of characters in the ascii range, just by random chance.  Some containers use sequences of ascii characters as the magic values to indicate what type of object something is.  e.g. mp4 has ""moov"", ""mdat"", and other atoms.  If you can pick any sequence of 4 bytes for your format, you might as well pick ASCII characters to make things easier to debug.</p>

<p>I think transcoding all incoming video would incur a HUGE quality / CPU / storage overhead.  Transcoding with nightly builds of ffmpeg would potentially shield you from known bugs in the typically old version of ffmpeg in VLC stable releases, and whatever bugs are in closed source stuff from Microsoft and Apple.</p>

<p>Remuxing is not a bad compromise.</p>

<p>There's also a balance between destroying metadata vs. passing through more potentially harmful stuff.  (e.g. chapters, subtitles, ...)</p>

<p>If I were actually doing this, I'd probably store my output in mkv files.  Using a different container is maybe better insurance against any weirdness from an input mp4 getting passed on to an output mp4.</p>

<p>Also, if you're actually doing this, I guess you'd run ffmpeg (or MP4Box for just mp4->mp4 remuxing) in a sandbox chroot or vm, or even just as an un-privileged user, so successful exploits are contained.</p>
","15060"
"avconv: What are the mpegvideo private options?","363","","<p>I'm using avconv on LinuxMint 17.3 to convert video.</p>

<p>The <strong>avconv man page</strong> states for multiple options, that they are ""<code>Deprecated, use mpegvideo private options instead</code>""</p>

<p>Can someone tell me what these private options are and where they are documented?</p>

<p>I noticed that the WinFF program uses <code>-flags +cbp</code> but I don't know what +cbp does or what the equivalent might be. Can anyone enlighten me or point me to more detailed documentation.</p>

<p>Thanks</p>
","<p>Private options are switches/flags only applicable to specific encoders or decoders, and ignored by anything else.</p>

<p>To see the available private options for an encoder, use <code>avconv -h encoder=name</code> e.g. <code>avconv -h encoder=flv</code></p>

<p>For LibAV, these are the private options available for the MPEG-1 &amp; 2 encoders.</p>

<pre><code>{ ""mpv_flags"",      ""Flags common for all mpegvideo-based encoders."", FF_MPV_OFFSET(mpv_flags), AV_OPT_TYPE_FLAGS, { .i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""skip_rd"",        ""RD optimal MB level residual skipping"", 0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_SKIP_RD },    0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""strict_gop"",     ""Strictly enforce gop size"",             0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_STRICT_GOP }, 0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""qp_rd"",          ""Use rate distortion optimization for qp selection"", 0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_QP_RD },  0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""cbp_rd"",         ""use rate distortion optimization for CBP"",          0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_CBP_RD }, 0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""naq"",            ""normalize adaptive quantization"",                   0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_NAQ },    0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""mv0"",            ""always try a mb with mv=&lt;0,0&gt;"",                     0, AV_OPT_TYPE_CONST, { .i64 = FF_MPV_FLAG_MV0 },    0, 0, FF_MPV_OPT_FLAGS, ""mpv_flags"" },\
{ ""luma_elim_threshold"",   ""single coefficient elimination threshold for luminance (negative values also consider dc coefficient)"",\
                                                                      FF_MPV_OFFSET(luma_elim_threshold), AV_OPT_TYPE_INT, { .i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS },\
{ ""chroma_elim_threshold"", ""single coefficient elimination threshold for chrominance (negative values also consider dc coefficient)"",\
                                                                      FF_MPV_OFFSET(chroma_elim_threshold), AV_OPT_TYPE_INT, { .i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS },\
{ ""quantizer_noise_shaping"", NULL,                                  FF_MPV_OFFSET(quantizer_noise_shaping), AV_OPT_TYPE_INT, { .i64 = 0 },       0, INT_MAX, FF_MPV_OPT_FLAGS },\
{ ""error_rate"", ""Simulate errors in the bitstream to test error concealment."",                                                                                                  \
                                                                    FF_MPV_OFFSET(error_rate),              AV_OPT_TYPE_INT, { .i64 = 0 },       0, INT_MAX, FF_MPV_OPT_FLAGS },\
{""qsquish"", ""how to keep quantizer between qmin and qmax (0 = clip, 1 = use differentiable function)"",                                                                          \
                                                                    FF_MPV_OFFSET(rc_qsquish), AV_OPT_TYPE_FLOAT, {.dbl = 0 }, 0, 99, FF_MPV_OPT_FLAGS},                        \
{""rc_qmod_amp"", ""experimental quantizer modulation"",                FF_MPV_OFFSET(rc_qmod_amp), AV_OPT_TYPE_FLOAT, {.dbl = 0 }, -FLT_MAX, FLT_MAX, FF_MPV_OPT_FLAGS},           \
{""rc_qmod_freq"", ""experimental quantizer modulation"",               FF_MPV_OFFSET(rc_qmod_freq), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS},             \
{""rc_eq"", ""Set rate control equation. When computing the expression, besides the standard functions ""                                                                           \
          ""defined in the section 'Expression Evaluation', the following functions are available: ""                                                                             \
          ""bits2qp(bits), qp2bits(qp). Also the following constants are available: iTex pTex tex mv ""                                                                           \
          ""fCode iCount mcVar var isI isP isB avgQP qComp avgIITex avgPITex avgPPTex avgBPTex avgTex."",                                                                         \
                                                                    FF_MPV_OFFSET(rc_eq), AV_OPT_TYPE_STRING,                           .flags = FF_MPV_OPT_FLAGS },            \
{""rc_init_cplx"", ""initial complexity for 1-pass encoding"",          FF_MPV_OFFSET(rc_initial_cplx), AV_OPT_TYPE_FLOAT, {.dbl = 0 }, -FLT_MAX, FLT_MAX, FF_MPV_OPT_FLAGS},       \
{""rc_buf_aggressivity"", ""currently useless"",                        FF_MPV_OFFSET(rc_buffer_aggressivity), AV_OPT_TYPE_FLOAT, {.dbl = 1.0 }, -FLT_MAX, FLT_MAX, FF_MPV_OPT_FLAGS}, \
{""border_mask"", ""increase the quantizer for macroblocks close to borders"", FF_MPV_OFFSET(border_masking), AV_OPT_TYPE_FLOAT, {.dbl = 0 }, -FLT_MAX, FLT_MAX, FF_MPV_OPT_FLAGS},    \
{""lmin"", ""minimum Lagrange factor (VBR)"",                           FF_MPV_OFFSET(lmin), AV_OPT_TYPE_INT, {.i64 =  2*FF_QP2LAMBDA }, 0, INT_MAX, FF_MPV_OPT_FLAGS },            \
{""lmax"", ""maximum Lagrange factor (VBR)"",                           FF_MPV_OFFSET(lmax), AV_OPT_TYPE_INT, {.i64 = 31*FF_QP2LAMBDA }, 0, INT_MAX, FF_MPV_OPT_FLAGS },            \
{""ibias"", ""intra quant bias"",                                       FF_MPV_OFFSET(intra_quant_bias), AV_OPT_TYPE_INT, {.i64 = FF_DEFAULT_QUANT_BIAS }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS },   \
{""pbias"", ""inter quant bias"",                                       FF_MPV_OFFSET(inter_quant_bias), AV_OPT_TYPE_INT, {.i64 = FF_DEFAULT_QUANT_BIAS }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS },   \
{""rc_strategy"", ""ratecontrol method"",                               FF_MPV_OFFSET(rc_strategy), AV_OPT_TYPE_INT, {.i64 = 0 }, 0, 1, FF_MPV_OPT_FLAGS },   \
{""motion_est"", ""motion estimation algorithm"",                       FF_MPV_OFFSET(motion_est), AV_OPT_TYPE_INT, {.i64 = FF_ME_EPZS }, FF_ME_ZERO, FF_ME_XONE, FF_MPV_OPT_FLAGS, ""motion_est"" },   \
{ ""zero"", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = FF_ME_ZERO }, 0, 0, FF_MPV_OPT_FLAGS, ""motion_est"" }, \
{ ""epzs"", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = FF_ME_EPZS }, 0, 0, FF_MPV_OPT_FLAGS, ""motion_est"" }, \
{ ""xone"", NULL, 0, AV_OPT_TYPE_CONST, { .i64 = FF_ME_XONE }, 0, 0, FF_MPV_OPT_FLAGS, ""motion_est"" }, \
{""b_strategy"", ""Strategy to choose between I/P/B-frames"",           FF_MPV_OFFSET(b_frame_strategy), AV_OPT_TYPE_INT, {.i64 = 0 }, 0, 2, FF_MPV_OPT_FLAGS }, \
{""b_sensitivity"", ""Adjust sensitivity of b_frame_strategy 1"",       FF_MPV_OFFSET(b_sensitivity), AV_OPT_TYPE_INT, {.i64 = 40 }, 1, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""brd_scale"", ""Downscale frames for dynamic B-frame decision"",      FF_MPV_OFFSET(brd_scale), AV_OPT_TYPE_INT, {.i64 = 0 }, 0, 3, FF_MPV_OPT_FLAGS }, \
{""skip_threshold"", ""Frame skip threshold"",                          FF_MPV_OFFSET(frame_skip_threshold), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""skip_factor"", ""Frame skip factor"",                                FF_MPV_OFFSET(frame_skip_factor), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""skip_exp"", ""Frame skip exponent"",                                 FF_MPV_OFFSET(frame_skip_exp), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""skip_cmp"", ""Frame skip compare function"",                         FF_MPV_OFFSET(frame_skip_cmp), AV_OPT_TYPE_INT, {.i64 = FF_CMP_DCTMAX }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS, ""cmp_func"" }, \
{""sc_threshold"", ""Scene change threshold"",                          FF_MPV_OFFSET(scenechange_threshold), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""noise_reduction"", ""Noise reduction"",                              FF_MPV_OFFSET(noise_reduction), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""mpeg_quant"", ""Use MPEG quantizers instead of H.263"",              FF_MPV_OFFSET(mpeg_quant), AV_OPT_TYPE_INT, {.i64 = 0 }, 0, 1, FF_MPV_OPT_FLAGS }, \
{""ps"", ""RTP payload size in bytes"",                             FF_MPV_OFFSET(rtp_payload_size), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""mepc"", ""Motion estimation bitrate penalty compensation (1.0 = 256)"", FF_MPV_OFFSET(me_penalty_compensation), AV_OPT_TYPE_INT, {.i64 = 256 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
{""mepre"", ""pre motion estimation"", FF_MPV_OFFSET(me_pre), AV_OPT_TYPE_INT, {.i64 = 0 }, INT_MIN, INT_MAX, FF_MPV_OPT_FLAGS }, \
</code></pre>
","18372"
"Easiest way to narrate video with freeze frames etc.?","363","","<p>I currently have a bit of experience with Premiere Pro and Windows Movie Maker. What I'm looking for is a way to basically narrate a video in real time and pause it (hold the frame) whenever I want while narrating my way through it.</p>

<p>Example: <div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/irB8eZJFc70?start=0""></iframe>
            </div></div></p>

<p>How is he manipulating the video while seemlessly narrating in real time like that?</p>
","<p>Thunderf00t created that video in the following way;</p>

<ol>
<li>he wrote a script and then recorded his voice separately as a audio file</li>
<li>After that, he probably edited that audio file to get rid of any mistakes, and mastered it to make it sound nice.</li>
<li>He then imported that audio file to a video editor </li>
<li>In the video editor he added 3rd party images and video to fit his voice over</li>
</ol>

<p>The video editing process is where the magic happens and is what makes the video so slinky. There are many how-to videos on youtube on the video editing proccess.</p>

<p>That being said, what you talk about in your question, reminds me of what TJ is doing on his drunken peasants channel/podcast where they play a video and pause it at curtain points to discuss whatever was in the video - all of this done in real time. In order to do that, you need to screen capture the video along with the audio of the video accompanied with your voice.</p>
","12599"
"Best practice recording VHS to digital","362","","<p>I have a home video on VHS which I want to transfer to someone else. As I don't have two VHS recorders I am looking into other ways to duplicate the footage.</p>

<p>I am considering two options to do this:</p>

<ul>
<li>Using a Canon 550D with a 
Sigma AF 18-50mm f/2.8-4.5 DC HSM OS lens to film the footage from a full HD 27 inch TV in 1920x1080 at 24 fps.</li>
<li>Using a cheap <a href=""http://www.ebay.com/itm/USB-AV-Video-Grabber-Capture-Recorder-from-VCR-VHS-to-PC-DVD-/230989456789"" rel=""nofollow"">VCR to USB</a> adapter to capture the footage directly to the hard-disk.</li>
</ul>

<p>What is the preferable option in terms of quality?</p>
","<p>The best option would be either to send it out to someone that has a good capture capability or invest in a decent quality RCA capture card.  The USB option will work the best of your two suggestions, but it is still likely going to be a decent quality loss (granted, VHS in general may not be that high to begin with.)</p>

<p>The first option is not worth considering as the result would be completely substandard.</p>
","8537"
"How can I record audio with the USB mic and record with the camera without having to sync them?","362","","<p>I want to start making some YouTube videos, but I want to work with what I have for the moment, to see how it goes and learn using them for a while.</p>

<p>So I have a Behringer C1-U microphone and a Samsung NX1000 mirrorless camera. Is it possible to connect them both to the pc and record video and audio at the same time?</p>
","<p>I don't think Samsung NX1000 can stream video to PC. So you can't ""connect them both to the pc and record"". You shall record first, and only then copy footage on the PC.</p>

<p>However some cameras have audio jacks inputs, to connect external microphone to them. And your Samsung NX1000 have one. But your microphone don't have audio jack output, so it outputs only digital signal. And your DSLR can receive only analog signal. So it is impossible to connect your microphone to your DSLR directly.</p>

<p>You need computer to convert digital signal to analog, and then connect it to your DSLR. It can be achieved with some kind of microcomputer like Raspberry Pi. Or, if you going to record only near your PC, you can plug your mic into PC, and then connect your PC's audio output to your DSLR's audio input. Maybe some settings on the PC should be adjusted for this to work properly. But it depends on your operating system.</p>

<p>After recording, do not forget to copy files from your DSLR to your PC.</p>

<p>Good Luck!</p>
","17227"
"How to use a layer as mask for another","362","","<p>In After Effect CC, how can I use a layer, for example with a black shape, to use that shape as a mask for another layer? I know I can draw a transparency map on the layer I want to mask itself, but wheneven I move / resize that layer (it's a simple image, in my case) the mask gets moved / resized as well and I want the mask to stay put.</p>
","<p>I think you need to use a track matte - this lets you use one layer to control the transparency of another:</p>

<p><a href=""https://helpx.adobe.com/after-effects/using/alpha-channels-masks-mattes.html"" rel=""nofollow"">https://helpx.adobe.com/after-effects/using/alpha-channels-masks-mattes.html</a></p>
","19627"
"Is it possible to relight filmed footage with NUKE or After Effects?","361","","<p>I have a Video of a face that I filmed with soft lighting and almost no shadows. The camera is a static portrait shot. Now I want a spot light to move slowly around the head. Is something like this possible in NUKE or After Effects?</p>

<p><img src=""https://i.stack.imgur.com/KqVPv.jpg"" alt=""illustration"">
Note: Only the light should be done in post. The head should be real footage filmed by a static camera.</p>
","<p>I suggest build a real lighting rig and maybe stabilizing her neck.</p>

<p>If you decide to do it afterwards you have to:</p>

<ul>
<li>Modeling the head and her hair (scan it if this is possible)</li>
<li>Tracking the shots with blender or pftrack</li>
<li>Replacing the head in your favourite package</li>
</ul>

<p>There are some obstacles: </p>

<ul>
<li>To get realistic shadows and highlights you probably need a better rendering engine for nuke </li>
<li>Detailed modeling is necessary to get the shadow right (espacially for the nose and the hair)</li>
<li>Modeling her hair can be a very difficult task</li>
</ul>
","14517"
"Does the MP4 HTML5 compatible video type allow for mono audio?","360","","<p>I'm doing some rendering in Vegas Pro and notice that if I select MainConcept (which gives me an HTML5 compatible MP4) the audio tab only has options for the bit rate and the sample rate. Before rendering, I combined both left and right channels on the timeline, but once rendered, the video file has two audio channels.</p>

<p>This is normally not a problem, but in this case, the audio is only voice and I'm concerned about file size, so if I could render with a mono channel, I would be able to get a higher bit rate for audio quality, but keep the file size small.</p>

<p>What Vegas is doing is taking the single channel audio from the timeline, duplicating it, setting a left and right channel, then cuts the set bitrate in half for each channel. So if I want 96kbps mono, I actually end up with  stereo, 48kbps on each channel.</p>

<p>Is this an inherent issue with MainConcept file types? I notice that with others I can actually select mono in the audio tab, but not with MainConcept. I thought I read a short half sentence saying as much on some forum, but not sure.</p>

<p>If there is a way to have an HTML5 compatible MP4 with mono audio, please tell me how.</p>
","<p>The Mainconcept or Sony codecs in Vegas are subpar for low-bitrate output such as needed for HTML5 use. Use ffmpeg to generate your HTML5 videos. Yes, mono audio is acceptable for HTML5 use, but once you use ffmpeg/x264 for generating those videos, the 6kB/s savings you get from switching to mono audio won't matter much if at all.</p>

<p>Get the 32-bit static build of ffmpeg from <a href=""http://ffmpeg.zeranoe.com/builds/"" rel=""nofollow noreferrer"">here</a>. Render a high bitrate Mainconcept MP4 from Vegas, say, 12 Mbps. And then run the following ffmpeg command:</p>

<pre><code>ffmpeg -i ""vegas-output.mp4"" -crf 23 -c:a aac -b:a 96k -strict -2 -movflags +faststart ""html5-video.mp4""
</code></pre>

<p>If you still want mono audio, insert</p>

<pre><code>-ac 1
</code></pre>

<p>after aac.</p>

<p>If you don't like the quality or size is too large, then play around with the CRF value. They go from 0 to 63 with lower being better. 18 to 28 is the usual range you want to stick with. </p>
","16943"
"ffmpeg - 'Unable to find a suitable output format for 'libfdk_aac''","360","","<p>I'm trying to run the following command:</p>

<pre><code>ffmpeg -y -loglevel info -threads 0 -f lavfi -i aevalsrc=0 -framerate 60 -i image.jpg -start_number 000000 -apply_trc bt709 -framerate 60 -i /myseq.%06d.exr -r 60 -preset medium -codec:v libx264 -profile:v main -tune fastdecode-c:a libfdk_aac -ac 2 -vbr 5 -shortest -strict experimental -sn -vsync 1 -pix_fmt yuv420p -b:v 31457280 -movflags +faststart -vf scale=3072:1536 -map ""2:0"" -map ""0:0"" /output.mp4 2&gt;&amp;1 | tee -a /log_file.log
</code></pre>

<p>And i get the following error:</p>

<pre><code>Unable to find a suitable output format for 'libfdk_aac'
</code></pre>

<p>Any idea why?</p>

<p>Thanks!</p>
","<p>There is a space missing in <code>-tune fastdecode-c:a libfdk_aac</code>, so FFmpeg thinks the value of tune is <code>fastdecode-c:a</code> and thus the next word is the output filename. Insert space after <code>fastdecode</code>.</p>
","18962"
"Ghost position keyframes in After Effects","360","","<p>I have an animation in After Effects where I am animating a slider to move up and down at certain points. The slider should be mainly stationary, but moves occasionally in line with other actions on the project.</p>

<p><img src=""https://i.stack.imgur.com/sK56t.png"" alt=""enter image description here""></p>

<p>What I am finding though, is the slider is moving a small amount even when the keyframes are set such that it should be static. E.g. the following two keyframes are the same (Y position = 524.6) but if I move the marker in between, you'll see the value deviates from this. (As a result the slider moves.)</p>

<p>1st Keyframe (Y Position = 524.6):
<img src=""https://i.stack.imgur.com/yZrVb.png"" alt=""1st Keyframe""></p>

<p>2nd Keyframe (Y Position = 524.6):
<img src=""https://i.stack.imgur.com/5EJFt.png"" alt=""2nd Keyframe""></p>

<p>In between (Y position varies, up to 525.4):
<img src=""https://i.stack.imgur.com/ihVtB.png"" alt=""In between""></p>

<p>This is driving me nuts! As far as I can see the anchor point does not move, so I don't understand why this should happen.</p>
","<p>It's because of spatial interpolation.  Here's a previous answer:</p>

<p><a href=""https://video.stackexchange.com/questions/14687/object-moving-to-a-position-i-didnt-specify/14697#14697"">Object moving to a position I didn&#39;t specify</a></p>
","15501"
"how to use h264_omx on RaspberryPi","359","","<p>I'm using RaspberryPi 1gen B+, for push stream to web through rtmp. But I find my CPU usage came up to 90%+. This is how I use it:</p>

<pre><code>ffmpeg -re -f concat -safe 0 -i playlist.txt -vcodec copy -acodec aac -f flv ""rtmp://example.com:1060""
</code></pre>

<p>so I want to use the GPU for decode/encode. After google research, I found ""h264_omx"", and I've implement the h264_omx: </p>

<pre><code>pi@pi:/usr/src/ffmpeg $ sudo ./configure --enable-omx --enable-omx-rpi
sudo make
sudo make install
</code></pre>

<p>so I use:</p>

<pre><code>ffmpeg -re -f concat -safe 0 -i playlist.txt -vcodec h264_omx -acodec aac -f flv ""rtmp://example.com:1060""
</code></pre>

<p>here is the output:
<a href=""https://i.stack.imgur.com/YRClP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YRClP.png"" alt=""enter image description here""></a></p>

<p>But the CPU usage still runs to 90%+, what's worse, the video become indistinct and only have 5fps. </p>

<p>so, what's wrong with me? Is it possible for 1gen B+ to hardware codec?</p>
","<p>The first priority here is to determine the bottleneck, particularly on the encoding end first, then moving on elsewhere.</p>

<p>This is noted by the comment on the choppy video stream, indicating frame-skipping.</p>

<p>To do that, we need to isolate the task that takes up the heaviest CPU load.</p>

<p>Try running the FFmpeg snippet with audio encoding only, as you copy the video stream, then monitor CPU load:</p>

<pre><code>ffmpeg -re -f concat -safe 0 -i playlist.txt -c:v copy -c:a aac -f flv ""rtmp://example.com:1060""
</code></pre>

<p>That omits the video encoding phase, so see the new CPU load.</p>

<p>Next step, enable the video encoder but copy the audio stream intact:</p>

<pre><code>ffmpeg -re -f concat -safe 0 -i playlist.txt -c:v h264_omx -c:a copy -f flv ""rtmp://example.com:1060""
</code></pre>

<p>Now, with that snippet, monitor the CPU load again.</p>

<p>You're likely to observe that its' either workload (video or audio encoding) that takes up the larger share of the CPU load, and from there, you can then tune the encoder(s) as you see fit.</p>

<p>To see encoder options for the <code>h264_omx</code> encoder, run:</p>

<pre><code>ffmpeg -h encoder=h264_omx
</code></pre>

<p>And for the <code>aac</code> encoder:</p>

<pre><code>ffmpeg -h encoder=aac
</code></pre>

<p>Then tune the culprit with an acceptable preset and/or settings <em>until an acceptable compromise in quality vs CPU load is reached</em>. Note that some functions such as auto-inserted filters tend to run on the CPU, contributing to the heavy load on the encoder's end.</p>

<p>You can confirm this by running:</p>

<pre><code>ffmpeg -loglevel debug -re -f concat -safe 0 -i playlist.txt -c:v h264_omx -c:a aac -f flv ""rtmp://example.com:1060""
</code></pre>

<p>And monitoring the output. Learn what filters are being auto-inserted, and see if modifying the filter strings (via <code>-vf</code>) assists with the processor load.</p>
","21806"
"If I shoot in H.264 is it worth it to export the product in ProRes?","359","","<p>I shoot video in H.264. I assume that my footage is forever compressed really small and that exporting to ProRes in FCPX would do nothing but make my file bigger. Is this true? Or am I missing something? Would exporting in ProRes give me better quality than I started with?</p>
","<p>Your source sets the quality ceiling, so transcoding to ProRes won't improve the video quality.</p>

<p>Most likely, the H264 files generated by your camera will be constrained baseline profile i.e. I- and P-frames only, and on a modern machine, decoding shouldn't be an issue. I edit H264 (baseline or main profile) all the time on my i7 and occasionally i5 with nVidia CUDA-enabled GPUs.</p>

<p>ProRes is useful when your shoot is in some exotic or heavier-duty codec because of bitrate or compression complexity. Or you need to collaborate with someone using FCP and they prefer to receive ProRes.</p>
","16063"
"Mask not working","359","","<p>In After Effects, I want to use a mask to prevent an area from beein affected by a layer. I created a mask using the pen tool, but instead of masking the shape, the program masks the bounds of the shape. It ""draws"" a rectangle arround the path and masks everything except the rectangle.<br>
I am a beginner. What am I doing wrong?<br>
Thanks!  </p>

<p>Edit:<br>
The mask works for simple layers, but my layer has an beam effect on it.</p>
","<p>Masks happen before effects so some act strange. Two main options:</p>

<ol>
<li>Apply the mask to a solid above your beam and then use Track Matte</li>
<li>Pre-comp your Beam and mask that instead</li>
</ol>
","17039"
"Which software a dubbing studio use for subtitles and closed captioning workflow?","359","","<p>Many video production and dubbing studios offer services of closed captioning, subtitles and so on. Their audience is generally big broadcast networks.</p>

<p>I was wondering, what kind of software this professionals do they use and which is their workflow?</p>

<p>I don't think they use programs like premiere with this weird interface:
<div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/4LAnZPDUOiI?start=0""></iframe>
            </div></div></p>

<p>Do they use even common free prosumer software like the ones used by subbers who create srt files like Subtitle Edit ?</p>

<p><a href=""https://i.stack.imgur.com/eBGNX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eBGNX.png"" alt=""Subtitle Edit""></a></p>

<p>Given they have to produce this for many series episodes, do they use custom heavy automatition tools, or is possible use also manual tools?</p>

<p>How is possible learn something about the topic?</p>

<p>Where I can start if tomorrow someone would ask me to work inside a company that have some big networks as customer and need to produce for them closed captioning and subtitles ?</p>
","<p>Most episodic (network and cable) TV is captioned by service bureaus who use proprietary software. Some of the big players are <a href=""http://www.vitac.com/"" rel=""nofollow""><strong>VITAC</strong></a>, <a href=""http://www.ncicap.org/"" rel=""nofollow""><strong>NCI</strong></a>, and <a href=""http://main.wgbh.org/wgbh/pages/mag/services/captioning/faq/"" rel=""nofollow""><strong>WGBH</strong></a>, and there are several more. They all provide various levels of service, and some rely more heavily on automation / voice recognition etc than others. This is all offline captioning and the goal is accuracy in copy, placement and timing.</p>

<p>The other main thread (and skill) is live writing or real-time captioning, which is done with stenowriters (chord typing) as used by court reporters. These are always delayed by some time because of the need to hear, interpret and input. It's almost always done remotely, with the caption writer networked into a local encoder. The same bureaus usually offer both live and offline.</p>

<p>To learn more, check out the links above -- and google for more -- then maybe contact the providers that look interesting to you.</p>
","17805"
"Need to reach a low data rate of about 1.5 - 2 MB/min (200 - 267 kb/s)","358","","<p>I am working on a project involving live streaming. My goal is to reach a data rate of no more than 200 - 267 kb/s when streaming. Is it possible to reach such low rate while keeping an acceptable fps and resolution? If so what would the fps and frame resolution?</p>

<p>Also, what compression method will I need to use? </p>

<p>Thank you.</p>
","<p>summary: yes, depending on content, with a good encoder h.264 can look pretty good at 240kib/s video bitrate, at NTSC 720x480p30.</p>

<p>With modern codecs, you don't need to downscale the rez too far.  If there isn't a lot of noise in your source, you're just putting the same complexity into fewer pixels.  You shouldn't encode 1080p at 270kb/s, though :P.  There is overhead to having more macroblocks, and diluting the complexity across too many pixels probably means worse results from quantizing the DCT coefficients.  Lower resolution lets you turn up the CPU-usage knob on your codec while still being realtime, though.</p>

<p>For a test encode, I have a lossless source of the <a href=""https://www.youtube.com/watch?v=JHVkegpW_ok"" rel=""nofollow"">sintel trailer</a>.  (3D animation with fades between scenes, and significant amount of changeless).  So it's probably actually a really bad test case...</p>

<pre><code>ffmpeg -i sintel.y4m -pix_fmt yuv420p -vf crop=1920:816:0:132,scale=640:-1 -ssim 1 -psnr -tune animation -preset slow -b:v 270k 640x272.x264.270k.mkv
</code></pre>

<p>That does quite well, with an average QP of 18.03(I) / 23.44(P) / 24.93(B).  SSIM Mean Y:0.9852979, PSNR Global:44.801 at 231.37 kib/s.  (a still section at the end leaves x264 undershooting the target bitrate for one-pass).  I left psy optimizations enabled, so x264 is trying to do what looks best, not what scores highest on SSIM or PSNR.  Just useful as a ballpark figure.</p>

<p>I have a live-action source that's not too noisy.  I picked a portion that just has one actor moving around on stage, everyone else standing still, and no camera motion.  So it might be similar in compressibility to a someone talking while looking at a webcam, in front of a mostly-constant background.  (stuff like this matters a LOT for compressibility).  About 1/3 of the screen is black background.  (so black the encoder knows you can't see anything, and hardly spends any bits, I assume.)</p>

<pre><code>ffmpeg -ss 2410 -t 30 -i vid.yadif3.1,mcdeint3.1.10.ffvhuff.mkv -c:a opus -b:a 32k -pix_fmt yuv420p -r 30 -vf framestep=2 -ssim 1 -psnr  -preset slow -b:v 240k  2410+30.slow.opus.230k.mkv
</code></pre>

<p>Source is 60p deinterlace output, from NTSC DV or something.  Anyway, with those ffmpeg options, it stays as 720x480@30p (with non-square pixels, display aspect ratio = 16:9).  Output looks pretty darn good considering the bitrate, and the singing sounds good, but has some artifacts.  (audio source is not noisy, and there's no accompaniment, just soloist + chorus.)</p>

<pre><code>frame=  900 fps= 35 q=-1.0 LPSNR=Y:inf U:inf V:inf *:inf size=     990kB time=00:00:30.01 bitrate= 270.3kbits/s
video:854kB audio:120kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.635424%
[libx264 @ 0x2400360] frame I:4     Avg QP:22.63  size: 23294  PSNR Mean Y:44.02 U:48.68 V:47.86 Avg:45.00 Global:42.71
[libx264 @ 0x2400360] frame P:239   Avg QP:22.23  size:  2492  PSNR Mean Y:43.64 U:48.67 V:47.52 Avg:44.67 Global:44.47
[libx264 @ 0x2400360] frame B:657   Avg QP:24.97  size:   282  PSNR Mean Y:43.38 U:48.52 V:47.37 Avg:44.43 Global:44.24
[libx264 @ 0x2400360] consecutive B-frames:  0.6%  0.0% 19.0% 80.4%
[libx264 @ 0x2400360] mb I  I16..4: 27.0% 58.0% 15.0%
[libx264 @ 0x2400360] mb P  I16..4:  0.2%  0.4%  0.1%  P16..4: 22.1%  5.1%  6.7%  0.0%  0.0%    skip:65.4%
[libx264 @ 0x2400360] mb B  I16..4:  0.0%  0.0%  0.0%  B16..8: 12.5%  0.4%  0.1%  direct: 0.1%  skip:87.0%  L0:23.6% L1:74.4% BI: 2.1%
[libx264 @ 0x2400360] final ratefactor: 22.98
[libx264 @ 0x2400360] 8x8 transform intra:59.0% inter:71.2%
[libx264 @ 0x2400360] direct mvs  spatial:99.7% temporal:0.3%
[libx264 @ 0x2400360] coded y,uvDC,uvAC intra: 50.7% 48.1% 24.7% inter: 2.8% 5.1% 0.2%
[libx264 @ 0x2400360] i16 v,h,dc,p: 36% 38%  7% 19%
[libx264 @ 0x2400360] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15%  4% 22%  8% 12% 11% 12%  8%  8%
[libx264 @ 0x2400360] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14%  3%  5% 12% 15% 16% 12% 12% 10%
[libx264 @ 0x2400360] i8c dc,h,v,p: 48% 27% 18%  7%
[libx264 @ 0x2400360] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 0x2400360] ref P L0: 66.9%  8.4% 14.9%  3.5%  4.6%  1.6%
[libx264 @ 0x2400360] ref B L0: 85.5% 10.8%  2.9%  0.8%
[libx264 @ 0x2400360] ref B L1: 92.3%  7.7%
[libx264 @ 0x2400360] SSIM Mean Y:0.9730952 (15.702db)
[libx264 @ 0x2400360] PSNR Mean Y:43.450 U:48.564 V:47.412 Avg:44.498 Global:44.292 kb/s:232.89
</code></pre>

<p>And 35fps on my E6600 (2.4GHz dual core first-gen c2duo) is not bad.  If you're trying to live-stream from a phone, you're going to be much more constrained, though.  You could use lower rez or lower framerate (25 fps, or 24, or 20?)</p>

<p>I was actually surprised at how well x264 did with <code>-preset veryfast</code>.  I guess on this content, even <code>veryfast</code> finds most of the same redundancy to eliminate.</p>

<pre><code>frame=  900 fps= 90 q=-1.0 LPSNR=Y:inf U:inf V:inf *:inf size=     994kB time=00:00:30.01 bitrate= 271.5kbits/s    
video:859kB audio:120kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.612045%
[libx264 @ 0x37b6420] frame I:4     Avg QP:21.51  size: 24840  PSNR Mean Y:44.16 U:48.09 V:47.20 Avg:44.99 Global:42.15
[libx264 @ 0x37b6420] frame P:227   Avg QP:21.22  size:  2649  PSNR Mean Y:43.57 U:48.08 V:46.72 Avg:44.50 Global:44.26
[libx264 @ 0x37b6420] frame B:669   Avg QP:22.30  size:   267  PSNR Mean Y:43.32 U:48.04 V:46.67 Avg:44.29 Global:44.02
[libx264 @ 0x37b6420] consecutive B-frames:  0.9%  0.0%  0.0% 99.1%
[libx264 @ 0x37b6420] mb I  I16..4: 24.0% 45.9% 30.1%
[libx264 @ 0x37b6420] mb P  I16..4:  1.4%  2.8%  0.1%  P16..4: 20.9%  4.2%  3.1%  0.0%  0.0%    skip:67.5%
[libx264 @ 0x37b6420] mb B  I16..4:  0.1%  0.2%  0.0%  B16..8:  1.7%  0.6%  0.1%  direct: 2.1%  skip:95.2%  L0:23.9% L1:45.7% BI:30.3%
[libx264 @ 0x37b6420] final ratefactor: 20.78
[libx264 @ 0x37b6420] 8x8 transform intra:60.6% inter:38.4%
[libx264 @ 0x37b6420] coded y,uvDC,uvAC intra: 39.1% 40.2% 10.5% inter: 2.7% 3.1% 0.1%
[libx264 @ 0x37b6420] i16 v,h,dc,p: 73% 11% 13%  3%
[libx264 @ 0x37b6420] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12%  7% 73%  1%  1%  1%  2%  1%  1%
[libx264 @ 0x37b6420] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 13% 14%  5%  8%  9%  8%  6%  6%
[libx264 @ 0x37b6420] i8c dc,h,v,p: 76%  8% 15%  1%
[libx264 @ 0x37b6420] Weighted P-Frames: Y:0.0% UV:0.0%
[libx264 @ 0x37b6420] SSIM Mean Y:0.9737951 (15.816db)
[libx264 @ 0x37b6420] PSNR Mean Y:43.390 U:48.047 V:46.685 Avg:44.343 Global:44.068 kb/s:234.13
</code></pre>

<p>bitrate ended up 2kib/s higher, with about the same PSNR, and higher SSIM.  </p>

<p>With <code>-tune ssim</code> to disable psychovisual optimizations (don't do this for real use),<br>
veryfast gets: Y-SSIM: 15.891db, PSNR Global:44.294, kb/s:233.26.<br>
slow gets: Y-SSIM: 16.097db, PSNR Global:44.752, kb/s:232.86</p>

<p>So that's what x264 can do with h.264 High@L3.0 profile, at 0.023 bits/pixel/frame.</p>

<p>tested x265:<br>
<code>ffmpeg -ss 2410 -t 30 -i vid.yadif3.1,mcdeint3.1.10.ffvhuff.mkv -c:a opus -b:a 32k -pix_fmt yuv420p -r 30 -vf framestep=2 -ssim 1 -psnr -c:v libx265 -preset slow -b:v 240k -aspect 16:9 x265.2410+30.slow.opus.230k.mkv</code></p>

<pre><code>ffmpeg version N-68044-gb9dd809 Copyright (c) 2000-2015 the FFmpeg developers
  built on Jan 14 2015 23:21:08 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)
  configuration: --enable-gpl --enable-version3 --enable-nonfree --disable-doc --disable-ffserver --enable-libbluray --enable-libschroedinger --enable-libtheora --enable-libx264 --enable-libx265 --enable-libmp3lame --enable-libopus --enable-libwebp --enable-libvpx --disable-outdev=oss --disable-indev=oss --disable-encoder=vorbis --enable-libvorbis --enable-libfdk-aac --disable-encoder=aac --disable-decoder=jpeg2000 --enable-libvidstab
  libavutil      54. 16.100 / 54. 16.100
  libavcodec     56. 20.100 / 56. 20.100
  libavformat    56. 18.101 / 56. 18.101
  libavdevice    56.  4.100 / 56.  4.100
  libavfilter     5.  7.101 /  5.  7.101
  libswscale      3.  1.101 /  3.  1.101
  libswresample   1.  1.100 /  1.  1.100
  libpostproc    53.  3.100 / 53.  3.100
Input #0, matroska,webm, from 'vid.yadif3.1,mcdeint3.1.10.ffvhuff.mkv':
  Metadata:
    ENCODER         : Lavf56.15.102
  Duration: 01:22:11.94, start: 0.000000, bitrate: 95843 kb/s
    Stream #0:0: Video: ffvhuff (FFVH / 0x48564646), yuv420p, 720x480, SAR 186:157 DAR 279:157, 59.94 fps, 59.94 tbr, 1k tbn, 1k tbc (default)
    Metadata:
      ENCODER         : Lavc56.14.100 ffvhuff
    Stream #0:1: Audio: ac3, 48000 Hz, stereo, fltp, 192 kb/s (default)
Codec AVOption ssim (Calculate and print SSIM stats.) specified for output file #0 (x265.2410+30.veryfast.opus.230k.mkv) has not been used for any stream. The most likely reason is either wrong type (e.g. a video option with no video streams) or that it is a private option of some encoder which was not actually used for any stream.
x265 [info]: HEVC encoder version 1.4+286-a12080554342
x265 [info]: build info [Linux][GCC 4.8.2][64 bit] 8bpp
x265 [info]: using cpu capabilities: MMX2 SSE2Fast SSSE3 Cache64 SlowShuffle
x265 [warning]: --psnr used with AQ on: results will be invalid!
x265 [warning]: --tune psnr should be used if attempting to benchmark psnr!
x265 [info]: Main profile, Level-3 (Main tier)
x265 [info]: WPP streams / frame threads / pool  : 15 / 1 / 2
x265 [info]: CTU size / RQT depth inter / intra  : 32 / 1 / 1
x265 [info]: ME / range / subpel / merge         : hex / 57 / 1 / 2
x265 [info]: Keyframe min / max / scenecut       : 25 / 250 / 40
x265 [info]: Lookahead / bframes / badapt        : 15 / 4 / 0
x265 [info]: b-pyramid / weightp / weightb / refs: 1 / 1 / 0 / 1
x265 [info]: Rate Control / AQ-Strength / CUTree : ABR-240 kbps / 1.0 / 0
x265 [info]: tools: rd=2 early-skip deblock sao signhide fast-intra tmvp 
Output #0, matroska, to 'x265.2410+30.veryfast.opus.230k.mkv':
  Metadata:
    encoder         : Lavf56.18.101
    Stream #0:0: Video: hevc (libx265), yuv420p, 720x480 [SAR 32:27 DAR 16:9], q=2-31, 240 kb/s, 30 fps, 1k tbn, 30 tbc (default)
    Metadata:
      encoder         : Lavc56.20.100 libx265
    Stream #0:1: Audio: opus (libopus) ([255][255][255][255] / 0xFFFFFFFF), 48000 Hz, stereo, flt, 32 kb/s (default)
    Metadata:
      encoder         : Lavc56.20.100 libopus
Stream mapping:
  Stream #0:0 -&gt; #0:0 (ffvhuff (native) -&gt; hevc (libx265))
  Stream #0:1 -&gt; #0:1 (ac3 (native) -&gt; opus (libopus))
Press [q] to stop, [?] for help
frame=  900 fps= 24 q=0.0 LPSNR=Y:inf U:inf V:inf *:inf size=     998kB time=00:00:30.01 bitrate= 272.4kbits/s    
video:861kB audio:120kB subtitle:0kB other streams:0kB global headers:1kB muxing overhead: 1.713292%
x265 [info]: frame I:      4, Avg QP:21.87  kb/s: 5097.48   PSNR Mean: Y:43.899 U:48.345 V:47.187  SSIM Mean: 0.968502 (15.017dB)
x265 [info]: frame P:    177, Avg QP:18.74  kb/s: 710.27    PSNR Mean: Y:43.347 U:48.274 V:47.041  SSIM Mean: 0.973469 (15.763dB)
x265 [info]: frame B:    719, Avg QP:20.83  kb/s: 89.99     PSNR Mean: Y:43.113 U:48.231 V:47.010  SSIM Mean: 0.972552 (15.615dB)
x265 [info]: global :    900, Avg QP:20.42  kb/s: 234.24    PSNR Mean: Y:43.163 U:48.240 V:47.017  SSIM Mean: 0.972714 (15.641dB)
x265 [info]: Weighted P-Frames: Y:0.0% UV:0.0%
x265 [info]: consecutive B-frames: 0.6% 0.0% 0.0% 0.6% 98.9% 
</code></pre>

<p>(Note that I needed <code>-x265-params ssim=1</code>, because the ffmpeg libx265 interface doesn't pick up a lot of the commandline options.)</p>

<pre><code>    slow: 253.245kbit/s fps=1.7  Y PSNR:44.228 unknown SSIM, forgot to turn it on
veryfast: 244.347kbit/s fps= 24  Y PSNR:43.163 15.641dB SSIM.
</code></pre>

<p>perceptually, the x265 veryfast output has less blocking than the x264 slow output.  I think x265 looks better.</p>

<p>x264 and x265 both from the git / hg repos, updated Jan 10th.</p>
","14580"
"how can I smoothly move a camera without a dolly?","357","","<p>I have a small camera and this is the first time I'm filming anything that is serious. I'm wonder if there is a cheap way I can smoothly shoot footage in a room while moving without a camera dolly.</p>
","<p>You want something like a <a href=""http://www.google.com/search?client=safari&amp;rls=en&amp;q=steadicam&amp;ie=UTF-8&amp;oe=UTF-8"" rel=""nofollow"">Steadicam</a>.</p>

<p>The idea is to add weight to the camera assembly so that it is stabilized by an increase of <a href=""http://en.wikipedia.org/wiki/Moment_of_inertia"" rel=""nofollow"">moment of inertia</a>. For larger rigs, the entire thing might be supported by the cameraperson's body on a harness.</p>

<p>For smaller applications, the mount point of the camera is right above where you grip the assembly, and the pivot point is at the top of this grip. Then, a rigid strut extends from the mount point to below the grip, and weight is added at the bottom of this strut so that the center of gravity of the entire assembly (camera included) is at the pivot point.</p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/B005PYZG6E"" rel=""nofollow"">This</a> is an example of something like this that you can purchase, but if you're on a tight budget you might even be able to <a href=""http://www.google.com/search?client=safari&amp;rls=en&amp;q=diy+steadicam&amp;ie=UTF-8&amp;oe=UTF-8"" rel=""nofollow"">make something yourself</a>.</p>
","4371"
"possible to lighten almost black video?","356","","<p>My caravan was recently broken into. The good news is I have a surveillance camera installed and I captured and recorded the breakin. The bad news is the footage is so underexposed that it is almost black.</p>

<p>Is there anyway I can lighten the footage in Premiere Pro? Is premiere pro even the best software for that? I am not after high quality stuff, just lighten and cleanup enough to be able to identify the suspect. I have tried adding a brightness and contrast filter to the footage, and I can see he goes right up to the camera, but there is just not enough detail to even tell if he is male or female.</p>

<p>Any help would be appreciated.</p>
","<p>You can brighten the footage, but it's doubtful you'll get anything out of the footage. Odds are, the areas you see as black are clipped which means they'll just turn into grey when you brighten the footage.</p>
","16922"
"Color correction/grading then editing in Premiere","355","","<p>I'm trying to incorporate color correction/grading into my Adobe Premiere CC workflow. I have an x-rite color checker, and have had some success calibrating in Davinci Resolve Lite, but I don't want to render everything in Resolve, just to have to rerender/edit in Premiere. Is it possible to simply output the calibration information (Looks &amp; LUTS?) created from the color matching into Premiere so it's just another effect being rendered on my clips?</p>

<p>If not, is there a way to use Adobe Speedgrade CC to match the colors from the x-rite color checker and then send it back into Premiere without having to render all the clips before other editing?</p>

<p>I guess the final option would be to edit and render in Premiere and then use Davinci Resolve as a post-processing calibration step and rerender afterwards, but this seems like it wouldn't be as convenient and would be very time consuming.</p>
","<p>In turns out all I had to do was grade in DaVinci Resolve and right click on the graded clip's thumbnail in the color environment and select Generate 3D LUT (CUBE).</p>

<p><a href=""https://i.stack.imgur.com/RBA0O.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RBA0O.jpg"" alt=""DaVinci Resolve exporting 3D LUT""></a></p>

<p>Then in Adobe Premiere CC add the Lutemtri effect to an adjustment layer, expand basic correction, Browse for Input LUT.</p>

<p><a href=""https://i.stack.imgur.com/plOQ5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/plOQ5.jpg"" alt=""Premiere CC importing 3D LUT""></a></p>
","19699"
"Smartphone app for JVC Adixxion","355","","<p>For JVC Adixxion, there is ""ADIXXION sync."" app, available for Android and iOS devices. The app's page states, that it is dedicated to JVC GC-XA2. </p>

<p>Will this app work also with GC-XA1 model? Are there any limitations, or features, that would not work with XA1?</p>
","<p>""ADIXXION sync."" does not work wit XA1. </p>

<p>However, another app, WiVideo works with XA1. It is available for Android and iOS. For PC, the camera comes with Windows version of WiVideo.</p>
","10222"
"Text fading in a clip in Shotcut","355","","<p>I make YouTube videos sometimes and Shotcut has always been my go-to for video editing, but I am working on a video now and can't figure out how to fade text in and out while a clip is playing. Help?</p>
","<p>It has been a long time since you asked the question. Maybe you have found your answer to your question in mean time. I am going to answer it anyway for you and for all others that are using this video editor.</p>

<p>I am using Shotcut in version 17.03.02.</p>

<p>There are probably more elegant and simple ways of doing this. The way I like to do it create a SVG file with the text I want (for example in Inkscape). I import this SVG into Shotcut and put in a separate video track above the main track and leave compositing enabled. Then I add fade-in and fade-out video filters to the SVG-Text video and enable the option ""Adjust opacity instead of fade with black"" for both filters. You can also adjust the duration of the filter/filters.</p>

<p>So to sum up:</p>

<ol>
<li>create <strong>svg</strong> file with your text</li>
<li>add a <strong>video track</strong> with the svg file </li>
<li>leave <strong>compositing on</strong></li>
<li>add <strong>fade-in/out filters</strong> and adjust duration</li>
<li><strong>check</strong> ""Adjust opacity instead of fade with black"" in both fade filters</li>
</ol>

<p>Enjoy!</p>
","20973"
"After Effect CC - How to combine and crop videos","354","","<p>I have 4 videos which i want to combine it into 1 video file.
Drag and drop should do the combining but i don't know how to crop it.</p>

<p><a href=""https://i.stack.imgur.com/DzaTt.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DzaTt.jpg"" alt=""Final result of the video""></a></p>

<p>I am new in video editing, so any suggestion or even keyword will help.</p>

<p>Thank you.</p>
","<p>After Effects and masking are both total overkill for this problem.</p>

<p>First you want a video editor, not a complex VFX system, as your baseline.  If you have After Effects, you likely also have Premiere Pro.  Drag and drop each video onto a separate video layer.  In Premiere Pro, each video layer has an Effects tab that includes a Motion effect.  The Motion effect includes scaling and translating video (shifting it left/right or up/down).  Scale each video to 50% (they should all scale toward the middle of the screen), then translate each video to their respective corner.</p>

<p>The last piece of the puzzle is to crop the videos as you wish.  In Premiere Pro, there's a video effect in the Transitions category called Crop.  By cropping left, right, top, and bottom by a certain number of pixels, you will get the rectangle (or square) you want.</p>

<p>But almost any video editor at all has Layers, Scale, Translate, and Crop.  That's what you need to solve your problem.</p>
","20946"
"Can I set the start number for image sequence output?","354","","<p>I am using <a href=""https://en.wikibooks.org/wiki/FFMPEG_An_Intermediate_Guide/image_sequence"" rel=""nofollow"">ffmpeg to convert from video to an image sequence</a> as follows:</p>

<pre><code>ffmpeg -i video.webm image-%03d.png
</code></pre>

<p>The first image is labelled <code>001</code>, but I want it to be labelled <code>098</code>. Is it possible to set that value in the ffmpeg invocation?</p>
","<p>The <a href=""https://ffmpeg.org/ffmpeg-formats.html#image2-2"" rel=""nofollow"">image sequence muxer</a> has a <a href=""https://ffmpeg.org/ffmpeg-formats.html#Options-5"" rel=""nofollow"">start number option</a>, so</p>

<pre><code>ffmpeg -i video.webm -start_number 98 image-%03d.png
</code></pre>
","19514"
"ffmpeg - warnings when i try to merge several audio files with an .mp4 file","354","","<p>I'm trying to merge 8 audio (.wav) files with a video (.mp4) file. I have the following command:</p>

<pre><code>ffmpeg -i video.mp4 -i audio1.wav -i audio2.wav -i audio3.wav -i audio4.wav -i audio5.wav -i audio6.wav -i audio7.wav -i audio8.wav -filter_complex ""[1:a][2:a]amerge=inputs=2[a0]"" -filter_complex ""[3:a][4:a]amerge=inputs=2[a90]"" -filter_complex ""[5:a][6:a]amerge=inputs=2[a180]"" -filter_complex ""[7:a][8:a]amerge=inputs=2[a270]"" -map 0:0 -map ""[a0]"" -map ""[a90]"" -map ""[a180]"" -map ""[a270]"" -c:v copy -c:a:0 libfdk_aac -vbr:0 5 -c:a:1 libfdk_aac -vbr:1 5 -c:a:2 libfdk_aac -vbr:2 5 -c:a:3 libfdk_aac -vbr:3 5 -movflags +faststart output.mp4
</code></pre>

<p>I can successfully run the command and the output is good, but i get the following warnings displayed (3 times each):</p>

<blockquote>
  <p>[Parsed_amerge_0 @ 0x7fcaca708da0] No channel layout for input 1
  [Parsed_amerge_0 @ 0x7fcaca708da0] Input channel layouts overlap:
  output layout will be determined by the number of distinct input
  channels</p>
</blockquote>

<p>And:</p>

<blockquote>
  <p>[libfdk_aac @ 0x7fcacc87c800] Note, the VBR setting is unsupported and
  only works with some parameter combinations</p>
</blockquote>

<p>What do they mean and how can i ""get rid of them""?</p>

<p>Thanks!</p>
","<p>The first warning means one of the WAV inputs has no channel layout defined i.e. channel 1 is Front Left, channel 2 is Front Center..etc. <code>amerge</code> decides the channel layout of the output stream based on the input, but since that designation is missing for one of the inputs, amerge will assign a channel layout which is default for the number of channels present in the output.</p>

<p>One thing you could try is rewrap all of the audio inputs i.e.</p>

<pre><code>ffmpeg -i audio1.wav -c copy audio1-new.wav
</code></pre>

<hr>

<p>The 2nd warning is displayed whenever VBR mode is invoked. Unless you get the error: <code>Unable to set the VBR bitrate mode</code>, you should be fine.</p>
","18994"
"Is 8k UHD or even 4k UHD actually necessary?","353","","<p>Hello I'm new to StackExchange so I apologize if I'm asking this in the wrong place.</p>

<p>My question though, at what point does the resolution of a tablet, monitor, television etc. make no difference to the human eye? Will the standards ever stop or will companies keep on milking our wallets forever (silly question really)?</p>
","<p>It all depends, of course, on what you are watching and where you are watching it. For instance, if you are showing something on a huge screen in a movie theater, you could definitely perceive a difference between 1080p and 4K. If you are looking at your phone, perhaps not as much. For standard computer monitor sizes, it's still fairly easy for many to see a difference at 4K, purely because we sit so damn close to our monitors.</p>

<p>Also keep in mind that there are scenarios in which you need to <em>shoot</em> in 4K in order to <em>distribute</em> in 2K or 1080p. You may need to zoom into one part of your shot and crop most of it out, which you can get away with if the footage you shot was a higher resolution than what you need to output at. This also applies to any footage that you may need to stabilize (i.e. windy drone footage) since stabilization requires cropping your frame. So in these situations, we not only need cameras to shoot in 4K+, but also monitors and devices that can play them back in their full resolutions.</p>

<p>While there are certainly diminishing returns on ever-increasing resolution on small consumer devices, there are people who absolutely perceive the differences between 4K and lower resolutions. Up to 8K and beyond, I'm not so sure, but for the moment, we have not actually crossed that threshold.</p>
","19277"
"If I can buy SSD, how do I best use it to improve video editing?","351","","<p>I am a small hobby editor. Just for fun, nothing serious. Hence: I don't know really much about the technical side at all, and most things I just don't understand. I'm really just a consumer.</p>

<p>I do have a good PC already: i7 4.00 Ghz, GTX 980.</p>

<p>I have 4 HDD's now, all 7200rpm: 1TB, 2TB, and 5TB(external). And one 500GB as primary containing my OS.</p>

<p>Now money restricts me a lot, so I can buy now ONE SSD and not even the greatest. I see an EVO of 500GB I can afford. </p>

<p>So how do I use that the best for video edit preformance?</p>

<p>I will have to keep all my source material on the HDD's. So I will import files from HDD into my Edit-program to edit.</p>

<p>I'm thinking to have my OS and all other programs on the SSD. But install only my Edit-program on the 2TB HDD and use that entire HDD ONLY for the Edit-program.</p>

<p>Or even the exact other way around: keep my OS and other on the HDD as it is, since I don't care much about fast boot or 'quickly open programs'. And install my Edit-program on the SSD and use the SSD only for that.</p>

<p>I searched google and internet for days trying to find a good answer for me, but I guess all other editors know too much to have such a low profile question lol.
I did find lots of editors claiming to have great benefit from SSD, but all of them had many SSD's so they could have their OS, Edit program AND their source material on SSD's. I can't possibly afford that now and I'm even wondering if one SSD will do me any good. If all the benefit I will get is that my Edit-software starts up really really fast, I wont bother...</p>

<p>Sorry for my rambling but I truly have no idea how I could make it work best. The ONLY reason I want to buy the SSD is to improve video editing, I have absolutely no other desire for it. </p>

<p>Any tips or advice I would highly appreciate!</p>

<p>ADDED: Maybe also useful info? My Edit-program is Cyberlink Powerdirector 15. I's very affordable and can still do nice things for it's price.
I mostly edit 1080p using quite a lot of multiple track splitscreens, multiple effects and transisions.
(Wow, I got 6 reputation and Bronze medal already? That wasn't hard lol :-)</p>
","<p>All computer performance optimization is based on understanding critical paths (bottlenecks), then removing or relaxing the bottleneck until there is nothing more you can afford to improve (possibly because there is simply no known widget, at any price, that will improve whatever else is on your critical path).</p>

<p>Some very common critical paths in video editing are</p>

<ol>
<li>Reading video data from disk</li>
<li>Rendering video data (for display and/or for writing to disk)</li>
<li>Writing video data to disk</li>
</ol>

<p>When you import video data into a project, rarely is it all loaded into RAM, though it could be.  If it is loaded into RAM, then buying an SSD is only going to speed up the initial loading of your RAM--a one-time benefit.</p>

<p>More often, video is buffered into RAM, but it fundamentally lives on disk, and thus disk speed becomes important as you play or scrub through your files.  If you have 1080p ACVHD stream, it uses approximately 28Mb/sec of bandwidth.  Modern HDDs can average 25MB/sec to 120MB/sec or more, depending on whether they are connected via USB2, USB3, USB3.1, SATA, eSATA, etc.  But if you need to load and display from 8 streams, it can try to read 28Mb/sec times 8 streams = 28MB/sec from the disk, but because the streams are likely scattered across the disk, it has to issue many seeks and buffer lots of data, leading to uneven performance.  An SSD is an advantage here because the seek time is virtually zero.  A single SATA SSD could easily handle 8 and perhaps 24 streams without choking.</p>

<p>However, the computer's CPU and GPU are typically designed to handle HD and perhaps quad HD video, meaning that they are going to begin to choke if they have to display 24 simultaneous streams.  But 4-8 should be OK.</p>

<p>To optimize your system, you should use a system profiling tool to see just how much disk bandwidth your system delivers under load vs. the calculated bandwidth of your video streams.  And you should measure how much video rendering performance your CPU/GPU can do vs. the estimated complexity of your streams (are they all just passed through a virtual switcher, or does every frame have scaling, translation, cropping, blur, sharpening, noise reduction, color correction, etc)?  You can then do some math: how much does it cost to close X% of the gap for each different gap you can measure.  Which dollars have the greatest impact on closing the gaps you can close?</p>

<p>You might find that indeed an SSD is just the ticket.  Or it might be that your GPU is really your rate-limiting step.  Most serious video editing workstations are a collection of many high-end components.  Having just one piece of the puzzle might only speed things up by 5%, whereas having all of the pieces, even if only half as good as the best, can speed things up by 50%.</p>

<p>It does take a little technical knowledge to profile and tune a system, but what it really takes is dedication to the process and good measurement tools.  Or enough money to keep buying bits until you can no longer see any improvements.</p>
","20044"
"How do I get screen captures from VLC to import properly into Premiere?","349","","<p>I recorded my screen in VLC, and I saved that file as an .mp4 on my desktop. I can open and play the recording just fine, but I can't seem to import it properly into Premiere. Have you ever seen this error before?</p>

<p><img src=""https://i.stack.imgur.com/o5YzH.png"" alt=""enter image description here""></p>
","<p>Looks like a codec that Premiere is unable to read.</p>

<p>I encountered a similar issue with Final Cut when trying to import an MP4 file encoded with H264.  Then came across this advice:  ""H.264 is a delivery format, not a production format. You need to convert the media into a suitable format. What that is depends on what the files are and what their frame size is."" <a href=""https://forums.creativecow.net/thread/8/986671"" rel=""nofollow"">https://forums.creativecow.net/thread/8/986671</a></p>

<p>So if you can't recapture with a suitable codec, you'll have to decompress your H264 MP4 file into something Premiere can import.</p>
","15955"
"Canon 600d vs Sony PD170 for video production","348","","<p>If you were to develop a short film, which one of these two cameras would you use?
In a recent discussion with a friend he was claiming that a camera like the Canon 600d can produce videos of much better quality compared to a Sony PD170. Is he correct? Can somebody show me some evidence to support this (or the opposite)?</p>
","<p>It's a little bit subjective. DSLR footage, straight off the camera, is very crisp and clean. Whereas a PD170 has a bit more of a 'video' look. So first off you'd have to decide what kind of look you wanted to go for. </p>

<p>In most cases you would choose to shoot on a DSLR, simply because the image is more visually appealing. </p>

<p>Pros of DSLR:</p>

<ul>
<li>better looking footage (subjective, although a common view)</li>
<li>can attach a variety of lenses (i.e. fish-eye)</li>
<li>smaller body, more mobility</li>
<li>no need to import footage off a tape, just copy and paste off SD card onto computer</li>
<li>it's cheaper</li>
</ul>

<p>Pros of PD170:</p>

<ul>
<li>don't have to worry about the camera overheating (can shoot as long as the tape and/or batter allow. DSLRs tend to overheat after 10minutes of straight shooting)</li>
<li>larger body. Easier to do handheld with just the body alone</li>
<li>can plug in a microphone directly into the camera. This avoids having to use a clapper before each take and avoids the need of syncing audio and video</li>
</ul>

<p>For me, those are the main points. If you're looking to make a film cheaply, DSLR is the way to go. But don't forget about sound; good sound is integral to a film.</p>
","2805"
"Trying to sync audio in mpeg2 video whose video freezes first 8 seconds in playback","348","","<p>So I have a handful of old mpeg2 (<code>.mpg</code>) files with the following profiles as provided by ffmpeg (the timecodes on each are different):</p>

<pre><code>Stream #0:0(eng): Video: mpeg4 (Advanced Simple Profile) (mp4v / 0x7634706D), yuv420p, 640x480 [SAR 1:1 DAR 4:3], 1385 kb/s, 29.88 fps, 29.97 tbr, 2997 tbn, 1k tbc (default)
Metadata:
  creation_time   : 2006-XX-XX XX:XX:XX
  handler_name    : Apple Alias Data Handler
  encoder         : MPEG-4 Video
  timecode        : 00:00:08;07
Stream #0:1(eng): Data: none (tmcd / 0x64636D74)
Metadata:
  creation_time   : 2006-XX-XX XX:XX:XX
  handler_name    : Apple Alias Data Handler
  timecode        : 00:00:08;07
Stream #0:2(eng): Audio: pcm_s16be (twos / 0x736F7774), 48000 Hz, 2 channels, s16, 1536 kb/s (default)
Metadata:
  creation_time   : 2006-XX-XX XX:XX:XX
  handler_name    : Apple Alias Data Handler
</code></pre>

<p>it appears that for each of them in playback (using mplayer and vlc) the first frame will be frozen for about 8 seconds while the audio starts from 00:00:00. So the audio remains about 8 seconds ahead of the video stream throughout.</p>

<p>When I try to convert them with FFmpeg, copying the video and audio codecs, I get a ton of ""buffer underflow"" and ""packet too large"" errors (which probably has to do with the codec etc. as per: <a href=""https://superuser.com/questions/835871/how-to-make-an-mpeg2-video-file-with-the-highest-quality-possible-using-ffmpeg"">https://superuser.com/questions/835871/how-to-make-an-mpeg2-video-file-with-the-highest-quality-possible-using-ffmpeg</a>). If I convert it to another video and audio codec, it's fine - no buffer underflow/packet too large errors. But the audio remains out of sync with the video. </p>

<p>I have also tried passing the ffmpeg conversion <code>-itsoffset -8.0</code> to the input file and the audio is still out of sync, even though the video then does appear to advance from 00:00:00 in playback for the converted file. </p>

<p>Not sure if there is anything else I can do about this but I am wondering if it's possible to get this audio and video synced. I have also tried mencoder with <code>-mc 0</code>, <code>-noskip</code> and <code>-vf harddup</code> options and that actually syncs the video for the first portion, but then the audio and video fall out of sync after about 30 minutes or so in.</p>

<p><strong>Update - some input and output details:</strong></p>

<blockquote>
  <p>ffmpeg -threads 1 -itsoffset -8.0 -i in.mpg -f mp4 -g 60 -c:v libx264 -pix_fmt yuvj420p -c:a libfdk_aac -y out.mp4</p>
</blockquote>

<pre><code>ffmpeg version 2.7.2 Copyright (c) 2000-2015 the FFmpeg developers
built with gcc 4.8.5 (Gentoo 4.8.5 p1.3, pie-0.6.2)
configuration: --prefix=/usr --libdir=/usr/lib64 --shlibdir=/usr/lib64 --mandir=/usr/share/man --enable-shared --    cc=x86_64-pc-linux-gnu-gcc --cxx=x86_64-pc-linux-gnu-g++ --ar=x86_64-pc-linux-gnu-ar --optflags=' ' --disable-    static --enable-avfilter --enable-avresample --disable-stripping --enable-version3 --disable-indev=v4l2 --disable    -outdev=v4l2 --disable-indev=oss --disable-indev=jack --disable-outdev=oss --disable-outdev=sdl --enable-nonfree     --enable-bzlib --disable-runtime-cpudetect --disable-debug --disable-doc --disable-gnutls --enable-gpl --enable-    hardcoded-tables --enable-iconv --disable-lzma --enable-network --disable-openssl --enable-postproc --disable-    libsmbclient --disable-ffplay --disable-vaapi --disable-vdpau --enable-xlib --disable-libxcb --disable-libxcb-    shm --disable-libxcb-xfixes --enable-zlib --disable-libcdio --disable-libiec61883 --disable-libdc1394 --disable-    libcaca --disable-openal --disable-opengl --disable-libv4l2 --disable-libpulse --disable-libopencore-amrwb --    disable-libopencore-amrnb --enable-libfdk-aac --disable-libopenjpeg --disable-libbluray --disable-libcelt --    disable-libgme --disable-libgsm --disable-libmodplug --disable-libopus --disable-libquvi --disable-librtmp --    disable-libssh --disable-libschroedinger --disable-libspeex --disable-libvorbis --enable-libvpx --disable-    libzvbi --disable-libbs2b --disable-libflite --disable-frei0r --disable-libfribidi --disable-fontconfig --disable    -ladspa --disable-libass --disable-libfreetype --disable-libsoxr --enable-pthreads --enable-libvo-aacenc --    disable-libvo-amrwbenc --enable-libmp3lame --disable-libaacplus --disable-libfaac --disable-libtheora --disable-    libtwolame --disable-libwavpack --disable-libwebp --enable-libx264 --disable-libx265 --disable-libxvid --enable-    x11grab --disable-amd3dnow --disable-amd3dnowext --disable-fma4 --disable-xop --cpu=core-avx-i
libavutil      54. 27.100 / 54. 27.100
libavcodec     56. 41.100 / 56. 41.100
libavformat    56. 36.100 / 56. 36.100
libavdevice    56.  4.100 / 56.  4.100
libavfilter     5. 16.101 /  5. 16.101
libavresample   2.  1.  0 /  2.  1.  0
libswscale      3.  1.101 /  3.  1.101
libswresample   1.  2.100 /  1.  2.100
libpostproc    53.  3.100 / 53.  3.100
Guessed Channel Layout for  Input Stream #0.2 : stereo
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'in.mpg':
  Metadata:
    major_brand     : qt  
    minor_version   : 537199360
    compatible_brands: qt  
    creation_time   : 2006-XX-XX XX:XX:XX
  Duration: 01:00:19.88, start: 0.000000, bitrate: 2940 kb/s
    Stream #0:0(eng): Video: mpeg4 (Advanced Simple Profile) (mp4v / 0x7634706D), yuv420p, 640x480 [SAR 1:1 DAR 4:3],     1385 kb/s, 29.88 fps, 29.97 tbr, 2997 tbn, 1k tbc (default)
    Metadata:
      creation_time   : 2006-XX-XX XX:XX:XX
      handler_name    : Apple Alias Data Handler
      encoder         : MPEG-4 Video
      timecode        : 00:00:08;07
    Stream #0:1(eng): Data: none (tmcd / 0x64636D74)
    Metadata:
      creation_time   : 2006-XX-XX XX:XX:XX
      handler_name    : Apple Alias Data Handler
      timecode        : 00:00:08;07
    Stream #0:2(eng): Audio: pcm_s16be (twos / 0x736F7774), 48000 Hz, 2 channels, s16, 1536 kb/s (default)
    Metadata:
      creation_time   : 2006-XX-XX XX:XX:XX
      handler_name    : Apple Alias Data Handler
[swscaler @ 0x1297040] deprecated pixel format used, make sure you did set range correctly
[libx264 @ 0x12c7be0] using SAR=1/1
[libx264 @ 0x12c7be0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 AVX2 LZCNT BMI2
[libx264 @ 0x12c7be0] profile High, level 3.0
[libx264 @ 0x12c7be0] 264 - core 148 - H.264/MPEG-4 AVC codec - Copyleft 2003-2015 - http://www.videolan.org/x264.html     - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16     chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2     sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_    bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=60 keyint_min=6 scenecut=40 intra_refresh=0 rc_lookahead=40     rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00
Output #0, mp4, to 'out.mp4':
  Metadata:
    major_brand     : qt  
    minor_version   : 537199360
    compatible_brands: qt  
    encoder         : Lavf56.36.100
    Stream #0:0(eng): Video: h264 (libx264) ([33][0][0][0] / 0x0021), yuvj420p(pc), 640x480 [SAR 1:1 DAR 4:3],     q=-1--1, 29.97 fps, 11988 tbn, 29.97 tbc (default)
    Metadata:
      creation_time   : 2006-XX-XX XX:XX:XX
      handler_name    : Apple Alias Data Handler
      timecode        : 00:00:08;07
      encoder         : Lavc56.41.100 libx264
    Stream #0:1(eng): Audio: aac (libfdk_aac) ([64][0][0][0] / 0x0040), 48000 Hz, stereo, s16, 139 kb/s (default)
    Metadata:
      creation_time   : 2006-XX-XX XX:XX:XX
      handler_name    : Apple Alias Data Handler
      encoder         : Lavc56.41.100 libfdk_aac
Stream mapping:
  Stream #0:0 -&gt; #0:0 (mpeg4 (native) -&gt; h264 (libx264))
  Stream #0:2 -&gt; #0:1 (pcm_s16be (native) -&gt; aac (libfdk_aac))
Press [q] to stop, [?] for help
Past duration 239.760002 too large
frame=   98 fps=0.0 q=26.0 size=     345kB time=00:00:02.26 bitrate=1250.8kbits/
frame=  186 fps=185 q=29.0 size=     933kB time=00:00:05.18 bitrate=1474.3kbits/
...
frame=108248 fps=218 q=-1.0 Lsize=  708299kB time=01:00:11.81 bitrate=1606.5kbits/s dup=78 drop=0    
video:643485kB audio:61559kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.461699%
[libx264 @ 0x12c7be0] frame I:1846  Avg QP:19.79  size: 25621
[libx264 @ 0x12c7be0] frame P:93233 Avg QP:24.01  size:  6332
[libx264 @ 0x12c7be0] frame B:13169 Avg QP:25.95  size:  1618
[libx264 @ 0x12c7be0] consecutive B-frames: 80.5%  8.5%  3.6%  7.3%
[libx264 @ 0x12c7be0] mb I  I16..4: 17.5% 76.4%  6.1%
[libx264 @ 0x12c7be0] mb P  I16..4:  1.9%  9.2%  0.4%  P16..4: 35.5%  9.0%  6.3%  0.0%  0.0%    skip:37.6%
[libx264 @ 0x12c7be0] mb B  I16..4:  0.4%  1.7%  0.0%  B16..8: 28.2%  3.0%  0.7%  direct: 4.2%  skip:61.8%  L0:59.1%     L1:33.8% BI: 7.1%
[libx264 @ 0x12c7be0] 8x8 transform intra:79.2% inter:86.3%
[libx264 @ 0x12c7be0] coded y,uvDC,uvAC intra: 65.8% 77.0% 11.3% inter: 24.3% 16.3% 1.4%
[libx264 @ 0x12c7be0] i16 v,h,dc,p: 42% 32% 24%  3%
[libx264 @ 0x12c7be0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 24% 40%  3%  2%  2%  2%  2%  4%
[libx264 @ 0x12c7be0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33% 27% 12%  3%  5%  5%  5%  4%  5%
[libx264 @ 0x12c7be0] i8c dc,h,v,p: 40% 28% 29%  3%
[libx264 @ 0x12c7be0] Weighted P-Frames: Y:0.6% UV:0.2%
[libx264 @ 0x12c7be0] ref P L0: 73.9% 12.5%  9.0%  4.5%  0.0%
[libx264 @ 0x12c7be0] ref B L0: 82.4% 16.2%  1.4%
[libx264 @ 0x12c7be0] ref B L1: 91.0%  9.0%
[libx264 @ 0x12c7be0] kb/s:1459.47
</code></pre>

<blockquote>
  <p>ffprobe out.mp4</p>
</blockquote>

<pre><code>[mov,mp4,m4a,3gp,3g2,mj2 @ 0xaf7c40] Could not find codec parameters for stream 0 (Video: h264 (avc1 / 0x31637661),     none, 640x480, 1459 kb/s): unspecified pixel format
Consider increasing the value for the 'analyzeduration' and 'probesize' options
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'out.mp4':
  Metadata:
    major_brand     : isom
    minor_version   : 512
    compatible_brands: isomiso2avc1mp41
    encoder         : Lavf56.36.100
  Duration: 01:00:11.88, start: -8.042667, bitrate: 1606 kb/s
    Stream #0:0(eng): Video: h264 (avc1 / 0x31637661), none, 640x480, 1459 kb/s, 29.97 fps, 29.97 tbr, 11988 tbn,     23976 tbc (default)
    Metadata:
      handler_name    : VideoHandler
    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 139 kb/s (default)
    Metadata:
      handler_name    : SoundHandler
</code></pre>

<blockquote>
  <p>mencoder in.mpg -oac pcm -ovc raw -o out2.avi</p>
</blockquote>

<pre><code>tons of 1 duplicate frame(s)! warnings. video not synced
</code></pre>

<blockquote>
  <p>mencoder in.mpg -ovc copy -mc 0 -noskip -vf harddup -oac pcm -o out2.mpg</p>
</blockquote>

<pre><code>works partially, seems to be the closest fix, no warnings in the output, but loses sync (in this example) at 00:28:48, where the video stream seems to jump some frames at that time
</code></pre>
","<p>You didn't specify if you want the first 8 seconds of Audio (it appears you do not) but you can split the input into separate Video and Audio files using ffmpeg (deMux).</p>

<p>Then with a second call to ffmpeg use ""-ss"" before the Audio's ""-i"" to chop off the first 8.07 seconds and use ""-copy"" to Mux them back  together (this is lossless).</p>

<p>Using ""-itsoffset"" before the ""-i"" of the Stream with ""-copy"" is another solution. Using ""-aresample"" can change the length of the Audio but you'll ruin Lipsync if it's a Video with people talking (as opposed to, for example, a Video with people walking down the street with traffic noise).</p>

<p>An OLD question that went without an answer for a long time (and sat in the unanswered Queue). Maybe good for a Necromancer Badge.</p>

<p>Reference: <a href=""https://ffmpeg.org/ffmpeg.html"" rel=""nofollow noreferrer"">https://ffmpeg.org/ffmpeg.html</a></p>
","20929"
"Convert sRGB color to linear Color Space","347","","<p>Just wondering if there is a simple and fast way to convert a specific color from my Illustrator layout file (#e3000b) to a linear Color Space.</p>

<p>I am working in After Effects in a linear color space. When I add a new solid and enter my hex code for color, its not the same red as in my layout.</p>

<p>Thanks in advance</p>
","<p>Using the calculator at <a href=""http://davengrace.com/dave/cspace/"" rel=""nofollow"">http://davengrace.com/dave/cspace/</a>,</p>

<p>Gamma-weighted <code>#E3000B</code> translates to linear sRGB <code>#C50001</code></p>
","17100"
"Handbrake CLI command not working","347","","<p>I am trying to get the HB CLI to run this command removing the framerate (as the default ATV2 is 30fps and in the UK that looks rubbish, I want it to use the source fps</p>

<p>./HandBrakeCLI -i DVD -o ~/Movies/movie.mp4 -e x264  -q 20.0 -a 1,1 -E faac,copy:ac3 -B 160,160 -6 dpl2,none -R Auto,Auto -D 0.0,0.0 --audio-copy-mask aac,ac3,dtshd,dts,mp3 --audio-fallback ffac3 -f mp4 -4 -X 1280 -Y 720 --loose-anamorphic --modulus 2 -m --x264-preset medium --h264-profile high --h264-level 3.1</p>

<p>This is the default I changed the <strong>Bold</strong> well removed it</p>

<p>./HandBrakeCLI -i DVD -o ~/Movies/movie.mp4 -e x264  -q 20.0 <strong>-r 30 --pfr</strong>  -a 1,1 -E faac,copy:ac3 -B 160,160 -6 dpl2,none -R Auto,Auto -D 0.0,0.0 --audio-copy-mask aac,ac3,dtshd,dts,mp3 --audio-fallback ffac3 -f mp4 -4 -X 1280 -Y 720 --loose-anamorphic --modulus 2 -m --x264-preset medium --h264-profile high --h264-level 3.1</p>

<p>but it doesn't seem to work, I don't know if I have missed something, any help would be useful</p>
","<p>figured it out by the looks of it</p>

<p>{{{
   ./HandBrakeCLI -i DVD -o ~/Movies/movie.mp4 -e x264  -q 20.0 -a 1,1 -E faac,copy:ac3 -B 160,160 -6 dpl2,none -R Auto,Auto -D 0.0,0.0 --audio-copy-mask aac,ac3,dtshd,dts,mp3 --audio-fallback ffac3 -f mp4 -4  -4 -X 1280 -Y 720 --decomb --loose-anamorphic --modulus 2 -m --x264-preset medium --h264-profile high --h264-level 4.1
}}}</p>

<p>I was missing the {{{ }}} which there was nothing about in the manual unless I missed it</p>
","15044"
"What types of cables support closed captioning?","347","","<p>For example, I know that HDMI cables explicitly do not send a CC signal to the TV, even if the source and TV both support it.</p>

<p>I assume that coax cable does support it since I can use a digital antenna and receive CC on my TV from digital over the air broadcasts (in the US).</p>

<p>Is there any other cabling that will carry the actual CC signal for the TV to decode?</p>

<p>I am currently sending a DVI signal to projectors in an auditorium and I am sending the same image by VGA over CAT5 to HDTVs in our lobby. We may need to add Closed Captioning to the lobby feed only so I do not want to change the image going to the projectors in the auditorium.</p>
","<p>Your question was initially confusing because you had focused on ""cables"" which is not the correct way of looking at this.</p>

<p>Once you've decoded a video signal to components, the <strong><em>closed</em></strong> captions are no longer available. If they were not inserted as <strong><em>open</em></strong> captions (aka subtitles) then they're simply not there.</p>

<p>So any transport that removes the Line 21 caption data or the digital caption packets eliminates your ability to defer captioning at the distant end -- you must insert the captions / subtitles before transport. That's the case with DVI / VGA / HDMI.</p>

<p>If you must allow the ability to choose a captioned or non-captioned display at the distant end, you would have to send two component feeds (e.g. two CAT5s), one captioned and one plain, and A/B switch them at that end.</p>
","15024"
"Lighting for ""no-shadow"" effect","345","","<p>I'm really impressed by <a href=""https://vimeo.com/101053847"" rel=""nofollow noreferrer"">this video</a>.</p>

<p><img src=""https://i.stack.imgur.com/11ugG.jpg"" alt=""enter image description here""></p>

<p>How to produce this no-shadow effect?</p>

<p><strong>How many lights do you think are required?</strong> What kind of light? Would some classical bulbs that I have at home work?</p>
","<blockquote>
  <p>How to produce this no-shadow effect?</p>
</blockquote>

<p>Some examples will explain this best.</p>

<p><img src=""https://i.stack.imgur.com/2mPxh.jpg"" alt=""enter image description here""><br>
<sub>Lighting with a soft box makes the shadow soft and unobtrusive.</sub></p>

<p><img src=""https://i.stack.imgur.com/2wwUA.jpg"" alt=""enter image description here""><br>
<sub>When close to a subject, the light rays from a large light source strike the subject from many angles. The closer the light is, the softer its shadows are.</sub></p>

<p><img src=""https://i.stack.imgur.com/4h056.jpg"" alt=""enter image description here""><br>
<sub>Results of the lighting setup. A fill card was used to lighten the front of the gourd by reflecting some light from the overhead soft box.</sub></p>

<p>Images and captions from <em>Light  Science &amp; Magic: An Introduction to Photographic Lighting</em>.</p>

<blockquote>
  <p>How many lights do you think are required?</p>
</blockquote>

<p>This could be done with one light, but it should be a large light source relative to the object. This doesn't mean the light itself was large, but the area the the light is being emitted from was large enough to cause scattered, soft light.</p>

<p>This is often done by placing diffuse material between the light source and the subject such as a soft box attached to the light, a diffusion panel from a cheap collapsible reflector, a light shed/tent that you can make yourself, or even some tissue paper (make sure it doesn't catch on fire). This results in a light quality similar to an overcast sky.</p>

<blockquote>
  <p>Would some classical bulbs that I have at home work?</p>
</blockquote>

<p>I don't know what a classical bulb is, but sure, you could possibly use typical household bulbs if they provide enough light for your camera and if you properly setup the scene. Don't forget to set the white balance. Use the same type of bulbs; don't mix types due to the potential differences in color temperature.</p>
","15834"
"Trimming mp4 videos","345","","<p>I have a GoPro4 video, came out in mp4. If I try and trim in GoPro studio, not only have to jump through hoops but I then have to export - choose an encoding etc...itsends up being a .mov file (i assume for further editing) but  is larger than the original, even after being trimmed. If I compress I lose quality !!</p>

<p>Now if I use quicktime trim feature - it really does seem to trim it and leave it I'm its exact same format.</p>

<p>How is quicktime doing this? Why doesn't GoPro studio, nor iMovie, Nor Final CutPro X seem to be able to do a simple trim ???</p>
","<p>Cutting a video at a keyframe, without the slow and lossy process of decoding and re-encoding it, is only possible in the special case where you aren't applying any effects, overlays, rescaling, or anything.  And where you're happy with bitrate and decoder requirements of the input.  (e.g. h.264 High Profile, Level 4.0)</p>

<p>Perhaps fancier editting software thinks it's beneath them to detect that special case, and copy a subsection of the h.264 video and AAC audio bitstreams to a different file?  (in either the same or a different container format.  e.g. you could remux to mkv, or m2ts.)  Or maybe you just need to look in a menu somewhere to activate that special case.</p>

<p>Using ffmpeg, you can do what I think you're saying Quicktime's trim is doing:</p>

<pre><code>ffmpeg -ss start_offset_seconds -t length_seconds -i input.mp4  -shortest -codec copy -movflags faststart trimmed.mp4
</code></pre>

<p>That runs about as fast as your hard drive can copy, since it's not re-encoding the video.  (codec = copy, not libx264).</p>

<p>To more directly answer your ""how is it doing this?"", google up remuxing.  ""mux"" (short for multiplex) is the term for putting multiple streams into a single output stream/file.  (1 video + 1 audio + 0 subtitles is the usual case.)</p>
","14652"
"How do I do this Text animation in Sony Vegas (..or AE)?","343","","<p>Sometimes, when I have a look at music videos, I see lyric texts coming up. They are shaped in a rectangle, with different sized words. An example is this, which I made quickly.
<a href=""https://i.stack.imgur.com/FGxeW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FGxeW.jpg"" alt=""enter image description here""></a></p>

<p>What is this effect called?</p>

<p>Let's say I have a music video. And I want to add lyrics to it, just like the example. How would I do this, animated, showing one by one in Sony Vegas Pro 13, or Adobe After Effects (Preferably Sony Vegas, I at used to the controls more)?</p>
","<p>It's called <a href=""http://www.premiumbeat.com/blog/10-free-after-effects-templates-typography/"" rel=""nofollow noreferrer"">Kinetic Type</a> generally (that link is for Templates). Depending on what you're looking to do you either need to do it manually or can use scripting to auto-fill a list of words. If you auto-fill with a list its generally more randomized, so if sequence is important. Like you want the Lyrics to appear in the order they're spoken you must do it manually.</p>

<p>You can use After Effects or Flash to create something like this.</p>

<ul>
<li><p><a href=""https://video.stackexchange.com/questions/1824/is-there-any-plugin-for-after-effects-to-create-a-kinetic-typography-video"">Tools to help make Kinetic Typography</a> (Video Stack Exchange for After Effects)</p></li>
<li><p><a href=""http://levitated.net/daily/levEmotionFractal.html"" rel=""nofollow noreferrer"">Using a Script to do it automatically with random words</a> (Levitated.net for Flash)</p></li>
</ul>
","16791"
"Is there any automated online service for making white board animation?","342","","<p>I want to know is there any website or software available to make white board animation as following?</p>

<p><div class=""youtube-embed""><div>
                <iframe width=""640px"" height=""395px"" src=""https://www.youtube.com/embed/NugRZGDbPFU?start=0""></iframe>
            </div></div></p>
","<p>The video you showed is simply a stop motion video, not an animation.</p>

<p>So you aren't going to be able to simply send a jpg off somewhere and have it come back as a video.</p>

<p>There are plenty of video houses that will happily make you a video like this, but it is time consuming. Usually they video the entire process of drawing the art, then cut out frames - to ensure they keep in important key frames.</p>
","7101"