title,viewcount,title,body,body,acceptedanswerid
"How do I get the size of a directory on the command line?","1477045","","<p>I tried to obtain the size of a directory (containing directories and sub directories) by using the <code>ls</code> command with option <code>l</code>. It seems to work for files (<code>ls -l file name</code>), but if I try to get the size of a directory (for instance, <code>ls -l /home</code>), I get only 4096 bytes, although altogether it is much bigger.</p>
","<p><code>du -sh file_path</code></p>

<p><strong>Explanation</strong></p>

<ul>
<li><code>du</code> command estimates file_path space usage</li>
<li><p>The options <code>-sh</code> are (from <code>man du</code>):</p>

<pre><code>  -s, --summarize
         display only a total for each argument

  -h, --human-readable
         print sizes in human readable format (e.g., 1K 234M 2G)
</code></pre>

<p>To check more than one directory and see the total, use <code>du -sch</code>:</p>

<pre><code>  -c, --total
         produce a grand total
</code></pre></li>
</ul>
","185765"
"How to correctly add a path to PATH?","1439693","","<p>I'm wondering where a new path has to be added to the <code>PATH</code> environment variable. I know this can be accomplished by editing <code>.bashrc</code> (for example), but it's not clear how to do this.</p>

<p>This way:</p>

<pre class=""lang-bsh prettyprint-override""><code>export PATH=~/opt/bin:$PATH
</code></pre>

<p>or this?</p>



<pre><code>export PATH=$PATH:~/opt/bin
</code></pre>
","<h3>The simple stuff</h3>

<pre><code>PATH=$PATH:~/opt/bin
PATH=~/opt/bin:$PATH
</code></pre>

<p>depending on whether you want to add <code>~/opt/bin</code> at the end (to be searched after all other directories, in case there is a program by the same name in multiple directories) or at the beginning (to be searched before all other directories).</p>

<p>You can add multiple entries at the same time. <code>PATH=$PATH:~/opt/bin:~/opt/node/bin</code> or variations on the ordering work just fine.</p>

<p>You don't need <code>export</code> if the variable is already in the environment: any change of the value of the variable is reflected in the environment.¹ <code>PATH</code> is pretty much always in the environment; all unix systems set it very early on (usually in the very first process, in fact).</p>

<p>If your <code>PATH</code> gets built by many different components, you might end up with duplicate entries. See <a href=""https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command"">How to add home directory path to be discovered by Unix which command?</a> and <a href=""https://unix.stackexchange.com/questions/40749/remove-duplicate-path-entries-with-awk-command"">Remove duplicate $PATH entries with awk command</a> to avoid adding duplicates or remove them.</p>

<h3>Where to put it</h3>

<p>Note that <code>~/.bash_rc</code> is not read by any program, and <code>~/.bashrc</code> is the configuration file of interactive instances of bash. You should not define environment variables in <code>~/.bashrc</code>. The right place to define environment variables such as <code>PATH</code> is <code>~/.profile</code> (or <code>~/.bash_profile</code> if you don't care about shells other than bash). See <a href=""https://superuser.com/questions/183870/difference-between-bashrc-and-bash-profile/183980#183980"">What's the difference between them and which one should I use?</a></p>

<h3>Notes on shells other than bash</h3>

<p>In bash, ksh and zsh, <code>export</code> is special syntax, and both <code>PATH=~/opt/bin:$PATH</code> and <code>export PATH=~/opt/bin:$PATH</code> do the right thing even. In other Bourne/POSIX-style shells such as dash (which is <code>/bin/sh</code> on many systems), <code>export</code> is parsed as an ordinary command, which implies two differences:</p>

<ul>
<li><code>~</code> is only parsed at the beginning of a word, except in assignments (see <a href=""https://unix.stackexchange.com/questions/25605/how-to-add-home-directory-path-to-be-discovered-by-unix-which-command/25704#25704"">How to add home directory path to be discovered by Unix which command?</a> for details);</li>
<li><code>$PATH</code> outside double quotes <a href=""https://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters"">breaks if <code>PATH</code> contains whitespace or <code>\[*?</code></a>.</li>
</ul>

<p>So in shells like dash, <code>export PATH=~/opt/bin:$PATH</code> sets <code>PATH</code> to the literal string <code>~/opt/bin/:</code> followed by the value of <code>PATH</code> up to the first space.
<code>PATH=~/opt/bin:$PATH</code> (a bare assignment) <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748"">doesn't require quotes</a> and does the right thing. If you want to use <code>export</code> in a portable script, you need to write <code>export PATH=""$HOME/opt/bin:$PATH""</code>, or <code>PATH=~/opt/bin:$PATH export PATH</code> (or <code>PATH=$HOME/opt/bin:$PATH export PATH</code> for portability to even the Bourne shell that didn't accept <code>export var=value</code> and didn't do tilde expansion).</p>

<p>¹ <sub> This wasn't true in Bourne shells (as in the actual Bourne shell, not modern POSIX-style shells), but you're highly unlikely to encounter such old shells these days. </sub>  </p>
","26059"
"How to switch between users on one terminal?","1411894","","<p>I'd like to log in as a different user without logging out of the current one (on the same terminal). How do I do that?</p>
","<p>How about using the <code>su</code> command?</p>

<pre><code>$ whoami
user1
$ su - user2
Password:
$ whoami
user2
$ exit
logout
</code></pre>

<p>If you want to log in as root, there's no need to specify username:</p>

<pre><code>$ whoami
user1
$ su -
Password:
$ whoami
root
$ exit
logout
</code></pre>

<p>Generally, you can use <code>sudo</code> to launch a new shell as the user you want; the <code>-u</code> flag lets you specify the username you want:</p>

<pre><code>$ whoami
user1
$ sudo -u user2 zsh
$ whoami
user2
</code></pre>

<p>There are more circuitous ways if you don't have sudo access, like ssh username@localhost, but <code>sudo</code> is probably simplest, provided that it's installed and you have permission to use it.</p>
","3572"
"How to check OS and version using a Linux command","1185380","","<p>What is the Linux command to check the server OS and its version?</p>

<p>I am connected to the server using shell.</p>
","<h2>Kernel Version</h2>

<p>If you want kernel version information, use uname(1). For example:</p>

<pre><code>$ uname -a
Linux localhost 3.11.0-3-generic #8-Ubuntu SMP Fri Aug 23 16:49:15 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
</code></pre>

<h2>Distribution Information</h2>

<p>If you want distribution information, it will vary depending on your distribution and whether your system supports the <a href=""http://en.wikipedia.org/wiki/Linux_Standard_Base"">Linux Standard Base</a>. Some ways to check, and some example output, are immediately below.</p>

<pre><code>$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu Saucy Salamander (development branch)
Release:    13.10
Codename:   saucy

$ cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=13.10
DISTRIB_CODENAME=saucy
DISTRIB_DESCRIPTION=""Ubuntu Saucy Salamander (development branch)""

$ cat /etc/issue.net
Ubuntu Saucy Salamander (development branch)

$ cat /etc/debian_version 
wheezy/sid
</code></pre>
","88647"
"How can I replace a string in a file(s)?","1131423","","<p>Replacing strings in files based on certain search criteria is a very common task. How can I </p>

<ul>
<li>replace string <code>foo</code> with <code>bar</code> in all files in the current directory?</li>
<li>do the same recursively for sub directories?</li>
<li>replace only if the file name matches another string?</li>
<li>replace only if the string is found in a certain context?</li>
<li>replace if the string is on a certain line number?</li>
<li>replace multiple strings with the same replacement</li>
<li>replace multiple strings with different replacements</li>
</ul>
","<h2>1. Replacing all occurrences of one string with another in all files in the current directory:</h2>

<p>These are for cases where you  <em>know</em> that the directory contains only regular files and that you want to process all non-hidden files. If that is not the case, use the approaches in 2.</p>

<p>All <code>sed</code> solutions in this answer assume GNU <code>sed</code>. If using FreeBSD or OS/X, replace <code>-i</code> with <code>-i ''</code>. Also note that the use of the <code>-i</code> switch with any version of <code>sed</code> has certain filesystem <a href=""http://lists.gnu.org/archive/html/bug-gnu-utils/2013-09/msg00000.html"" rel=""noreferrer"">security implications</a> and is inadvisable in any script which you plan to distribute in any way.</p>

<ul>
<li><p>Non recursive, files in this directory only:</p>

<pre><code>sed -i -- 's/foo/bar/g' *
perl -i -pe 's/foo/bar/g' ./* 
</code></pre>

<p>(the <a href=""https://unix.stackexchange.com/q/170013/22565""><code>perl</code> one will fail for file names ending in <code>|</code> or space)</a>).</p></li>
<li><p>Recursive, regular files (<strong>including hidden ones</strong>) in this and all subdirectories</p>

<pre><code>find . -type f -exec sed -i 's/foo/bar/g' {} +
</code></pre>

<p>If you are using zsh:</p>

<pre><code>sed -i -- 's/foo/bar/g' **/*(D.)
</code></pre>

<p>(may fail if the list is too big, see <code>zargs</code> to work around).</p>

<p>Bash can't check directly for regular files, a loop is needed (braces avoid setting the options globally):</p>

<pre><code>( shopt -s globstar dotglob;
    for file in **; do
        if [[ -f $file ]] &amp;&amp; [[ -w $file ]]; then
            sed -i -- 's/foo/bar/g' ""$file""
        fi
    done
)
</code></pre>

<p>The files are selected when they are actual files (-f) and they are writable (-w).</p></li>
</ul>

<h2>2. Replace only if the file name matches another string / has a specific extension / is of a certain type etc:</h2>

<ul>
<li><p>Non-recursive, files in this directory only:</p>

<pre><code>sed -i -- 's/foo/bar/g' *baz*    ## all files whose name contains baz
sed -i -- 's/foo/bar/g' *.baz    ## files ending in .baz
</code></pre></li>
<li><p>Recursive, regular files in this and all subdirectories</p>

<pre><code>find . -type f -name ""*baz*"" -exec sed -i 's/foo/bar/g' {} +
</code></pre>

<p>If you are using bash (braces avoid setting the options globally):</p>

<pre><code>( shopt -s globstar dotglob
    sed -i -- 's/foo/bar/g' **baz*
    sed -i -- 's/foo/bar/g' **.baz
)
</code></pre>

<p>If you are using zsh:</p>

<pre><code>sed -i -- 's/foo/bar/g' **/*baz*(D.)
sed -i -- 's/foo/bar/g' **/*.baz(D.)
</code></pre>

<p>The <code>--</code> serves to tell <code>sed</code> that no more flags will be given in the command line. This is useful to protect against file names starting with <code>-</code>. </p></li>
<li><p>If a file is of a certain type, for example, executable (see <code>man find</code> for more options):</p>

<pre><code>find . -type f -executable -exec sed -i 's/foo/bar/g' {} +
</code></pre>

<p><code>zsh</code>:</p>

<pre><code>sed -i -- 's/foo/bar/g' **/*(D*)
</code></pre></li>
</ul>

<h2>3. Replace only if the string is found in a certain context</h2>

<ul>
<li><p>Replace <code>foo</code> with <code>bar</code> only if there is a <code>baz</code> later on the same line:</p>

<pre><code>sed -i 's/foo\(.*baz\)/bar\1/' file
</code></pre>

<p>In <code>sed</code>, using <code>\( \)</code> saves whatever is in the parentheses and you can then access it with <code>\1</code>.  There are many variations of this theme, to learn more about such regular expressions, see <a href=""http://www.regular-expressions.info/"" rel=""noreferrer"">here</a>. </p></li>
<li><p>Replace <code>foo</code> with <code>bar</code> only if <code>foo</code> is found on the 3d column (field) of the input file (assuming whitespace-separated fields):</p>

<pre><code>gawk -i inplace '{gsub(/foo/,""baz"",$3); print}' file
</code></pre>

<p>(needs <code>gawk</code> 4.1.0 or newer). </p></li>
<li><p>For a different field just use <code>$N</code> where <code>N</code> is the number of the field of interest. For a different field separator (<code>:</code> in this example) use:</p>

<pre><code>gawk -i inplace -F':' '{gsub(/foo/,""baz"",$3);print}' file
</code></pre>

<p>Another solution using <code>perl</code>:</p>

<pre><code>perl -i -ane '$F[2]=~s/foo/baz/g; $"" = "" ""; print ""@F\n""' foo 
</code></pre>

<p>NOTE: both the <code>awk</code> and <code>perl</code> solutions will affect spacing in the file (remove the leading and trailing blanks, and convert sequences of blanks to one space character in those lines that match). For a different field, use <code>$F[N-1]</code> where <code>N</code> is the field number you want and for a different field separator use (the <code>$""="":""</code> sets the output field separator to <code>:</code>):</p>

<pre><code>perl -i -F':' -ane '$F[2]=~s/foo/baz/g; $""="":"";print ""@F""' foo 
</code></pre></li>
<li><p>Replace <code>foo</code> with <code>bar</code> only on the 4th line:</p>

<pre><code>sed -i '4s/foo/bar/g' file
gawk -i inplace 'NR==4{gsub(/foo/,""baz"")};1' file
perl -i -pe 's/foo/bar/g if $.==4' file
</code></pre></li>
</ul>

<h2>4. Multiple replace operations: replace with different strings</h2>

<ul>
<li><p>You can combine <code>sed</code> commands:</p>

<pre><code>sed -i 's/foo/bar/g; s/baz/zab/g; s/Alice/Joan/g' file
</code></pre>

<p>Be aware that order matters (<code>sed 's/foo/bar/g; s/bar/baz/g'</code> will substitute <code>foo</code> with <code>baz</code>).</p></li>
<li><p>or Perl commands</p>

<pre><code>perl -i -pe 's/foo/bar/g; s/baz/zab/g; s/Alice/Joan/g' file
</code></pre></li>
<li><p>If you have a large number of patterns, it is easier to save your patterns and their replacements in a <code>sed</code> script file:</p>

<pre><code>#! /usr/bin/sed -f
s/foo/bar/g
s/baz/zab/g
</code></pre></li>
<li><p>Or, if you have too many pattern pairs for the above to be feasible, you can read pattern pairs from a file (two space separated patterns, $pattern and $replacement, per line):</p>

<pre><code>while read -r pattern replacement; do   
    sed -i ""s/$pattern/$replacement/"" file
done &lt; patterns.txt
</code></pre></li>
<li><p>That will be quite slow for long lists of patterns and large data files so you might want to read the patterns and create a <code>sed</code> script from them instead. The following assumes a <em>&lt;space></em> delimiter separates a list of <em>MATCH&lt;space>REPLACE</em> pairs occurring one-per-line in the file <em><code>patterns.txt</code></em> :</p>

<pre><code>sed 's| *\([^ ]*\) *\([^ ]*\).*|s/\1/\2/g|' &lt;patterns.txt |
sed -f- ./editfile &gt;outfile
</code></pre>

<p>The above format is largely arbitrary and, for example, doesn't allow for a <em>&lt;space></em> in either of <em>MATCH</em> or <em>REPLACE</em>. The method is very general though: basically, if you can create an output stream which looks like a <code>sed</code> script, then you can source that stream as a <code>sed</code> script by specifying <code>sed</code>'s script file as <code>-</code>stdin. </p></li>
<li><p>You can combine and concatenate multiple scripts in similar fashion:</p>

<pre><code>SOME_PIPELINE |
sed -e'#some expression script'  \
    -f./script_file -f-          \
    -e'#more inline expressions' \
./actual_edit_file &gt;./outfile
</code></pre>

<p>A POSIX <code>sed</code> will concatenate all scripts into one in the order they appear on the command-line. None of these need end in a <code>\n</code>ewline.</p></li>
<li><p><code>grep</code> can work the same way:</p>

<pre><code>sed -e'#generate a pattern list' &lt;in |
grep -f- ./grepped_file
</code></pre></li>
<li><p>When working with fixed-strings as patterns, it is good practice to escape regular expression <em>metacharacters</em>. You can do this rather easily:</p>

<pre><code>sed 's/[]$&amp;^*\./[]/\\&amp;/g
     s| *\([^ ]*\) *\([^ ]*\).*|s/\1/\2/g|
' &lt;patterns.txt |
sed -f- ./editfile &gt;outfile
</code></pre></li>
</ul>

<h2>5. Multiple replace operations: replace multiple patterns with the same string</h2>

<ul>
<li><p>Replace any of <code>foo</code>, <code>bar</code> or <code>baz</code> with <code>foobar</code></p>

<pre><code>sed -Ei 's/foo|bar|baz/foobar/g' file
</code></pre></li>
<li><p>or</p>

<pre><code>perl -i -pe 's/foo|bar|baz/foobar/g' file
</code></pre></li>
</ul>
","112024"
"Using 'sed' to find and replace","1007761","","<p>I know this question has probably been answered before.  I have seen many threads about this in various places, but the answers are usually hard to extract for me.  I am looking for help with an example usage of the 'sed' command.</p>

<p>Say I wanted to act upon the file ""hello.txt"" (in same directory as prompt).  Anywhere it contained the phrase ""few"", it should be changed to ""asd"".  What would the command look like?</p>
","

<p><code>sed</code> is the <a href=""https://www.gnu.org/software/sed/manual/sed.txt"" rel=""noreferrer"" title=""manpage""><strong>s</strong>tream <strong>ed</strong>itor</a>, in that you can use <code>|</code> (pipe) to send <em>standard streams</em> (STDIN and STDOUT specifically) through <code>sed</code> and alter them programmatically on the fly, making it a handy tool in the Unix philosophy tradition; but can edit files directly, too, using the <code>-i</code> parameter mentioned below.<br>
<strong>Consider the following</strong>:</p>

<pre class=""lang-sh prettyprint-override""><code>sed -i -e 's/few/asd/g' hello.txt
</code></pre>

<p><code>s/</code> is used to <strong>s</strong>ubstitute the found expression <code>few</code> with <code>asd</code>:</p>

<blockquote>
  <p>The few, the brave.</p>
  
  <hr>
  
  <p>The asd, the brave.</p>
</blockquote>

<p><code>/g</code> stands for ""global"", meaning to do this for the whole line. If you leave off the <code>/g</code> (with <code>s/few/asd/</code>, there always needs to be three slashes no matter what) and <code>few</code> appears twice on the same line, only the first <code>few</code> is changed to <code>asd</code>:</p>

<blockquote>
  <p>The few men, the few women, the brave.</p>
  
  <hr>
  
  <p>The asd men, the few women, the brave.</p>
</blockquote>

<p>This is useful in some circumstances, like altering special characters at the beginnings of lines (for instance, replacing the greater-than symbols some people use to quote previous material in email threads with a horizontal tab while leaving a quoted algebraic inequality later in the line untouched), but in your example where you specify that <em>anywhere</em> <code>few</code> occurs it should be replaced, make sure you have that <code>/g</code>.</p>

<p>The following two options (flags) are combined into one, <code>-ie</code>:</p>

<p><code>-i</code> option is used to edit <strong>i</strong>n place on the file <code>hello.txt</code>.</p>

<p><code>-e</code> option indicates the <strong>e</strong>xpression/command to run, in this case <code>s/</code>.</p>

<p>Note: It's important that you use <code>-i -e</code> to search/replace. If you do <code>-ie</code>, you create a backup of every file with the letter 'e' appended.</p>
","159369"
"Zip all files in directory?","864653","","<p>Is there a way to zip all files in a given directory with the <code>zip</code> command? I've heard of using <code>*.*</code>, but I want it to work for extensionless files, too.</p>
","<p>You can just use <code>*</code>; there is no need for <code>*.*</code>. File extensions are not special on Unix. <code>*</code> matches zero or more characters—including a dot. So it matches <code>foo.png</code>, because that's zero or more characters (seven, to be exact).</p>

<p>Note that <code>*</code> by default doesn't match files beginning with a dot (neither does <code>*.*</code>). This is often what you want. If not, in bash, if you <code>shopt -s dotglob</code> it will (but will still exclude <code>.</code> and <code>..</code>). Other shells have different ways (or none at all) of including dotfiles.</p>

<p>Alternatively, <code>zip</code> also has a <code>-r</code> (recursive) option to do entire directory trees at once (and not have to worry about the dotfile problem):</p>

<pre><code>zip -r myfiles.zip mydir
</code></pre>

<p>where <code>mydir</code> is the directory containing your files. Note that the produced zip will contain the directory structure as well as the files. As peterph points out in his comment, this is usually seen as a good thing: extracting the zip will neatly store all the extracted files in one subdirectory.</p>

<p>You can also tell zip to not store the paths with the <code>-j</code>/<code>--junk-paths</code> option.</p>

<p>The <code>zip</code> command comes with documentation telling you about all of its (many) options; type <code>man zip</code> to see that documentation. This isn't unique to zip; you can get documentation for most commands this way.</p>
","57014"
"How do I grep for multiple patterns?","832506","","<p>I want to find all lines in several files that match one of two patterns. I tried to find the patterns I'm looking for by typing </p>

<pre><code>grep (foo|bar) *.txt
</code></pre>

<p>but the shell interprets the <code>|</code> as a pipe and complains when <code>bar</code> isn't an executable.</p>

<p>How can I grep for multiple patterns in the same set of files?</p>
","<p>First, you need to protect the pattern from expansion by the shell. The easiest way to do that is to put single quotes around it. Single quotes prevent expansion of anything between them (including backslashes); the only thing you can't do then is have single quotes in the pattern.</p>

<pre><code>grep 'foo*' *.txt
</code></pre>

<p>If you do need a single quote, you can write it as <code>'\''</code> (end string literal, literal quote, open string literal).</p>

<pre><code>grep 'foo*'\''bar' *.txt
</code></pre>

<p>Second, grep supports two syntaxes for patterns. The old, default syntax (<a href=""https://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended"">basic regular expressions</a>) doesn't support the alternation (<code>|</code>) operator, though some versions have it as an extension, but written with a backslash.</p>

<pre><code>grep 'foo\|bar' *.txt
</code></pre>

<p>The portable way is to use the newer syntax, <a href=""https://en.wikipedia.org/wiki/Regular_expression#POSIX_extended"">extended regular expressions</a>. You need to pass the <code>-E</code> option to <code>grep</code> to select it. On Linux, you can also type <code>egrep</code> instead of <code>grep -E</code> (on other unices, you can make that an alias).</p>

<pre><code>grep -E 'foo|bar' *.txt
</code></pre>

<p>Another possibility when you're just looking for any of several patterns (as opposed to building a complex pattern using disjunction) is to pass multiple patterns to <code>grep</code>. You can do this by preceding each pattern with the <code>-e</code> option.</p>

<pre><code>grep -e foo -e bar *.txt
</code></pre>
","37316"
"How to install a deb file, by dpkg -i or by apt?","743621","","<p>I have a deb package for installation.</p>

<p>Shall I install by <code>dpkg -i my.deb</code>, or by apt?</p>

<p>Will both handle the software dependency problem well?</p>

<p>If by apt, how can I install from the deb by apt?</p>
","<p>When you use <code>apt</code> to install a package, internally it uses <code>dpkg</code>. When you install a package using apt, it first creates a list of all the dependencies and downloads it from the repository.</p>

<p>Once the download is finished it calls <code>dpkg</code> to install all those files, satisfying all the dependencies.</p>

<p>So if you have a <code>.deb</code> file:</p>

<ul>
<li><p>You can install it using <code>sudo dpkg -i /path/to/deb/file</code> followed by <code>sudo apt-get install -f</code>. </p></li>
<li><p>You can install it using <code>sudo apt install ./name.deb</code> (or <code>/path/to/package/name.deb</code>). <br/>With old <code>apt-get</code> versions you must first move your deb file to <code>/var/cache/apt/archives/</code> directory. For both, after executing this command, it will automatically download its dependencies.</p></li>
<li><p>Install <code>gdebi</code> and open your .deb file using it (<em>Right-click</em> -> <em>Open with</em>). It will install your .deb package with all its dependencies.</p>

<p><em>(<strong>Note</strong>: APT maintains the package index which is a database of available packages available in repo defined in <code>/etc/apt/sources.list</code> file and in the <code>/etc/apt/sources.list.d</code> directory. All these  methods will fail to satisfy the  software dependency if the dependencies required by the deb is not present in the package index.)</em></p></li>
</ul>

<p><hr>
<strong>Why to use <code>sudo apt-get install -f</code> after <code>sudo dpkg -i /path/to/deb/file</code></strong> (mentioned in first method).</p>

<p>From <code>man apt-get</code></p>

<pre><code> -f, --fix-broken
           Fix; attempt to correct a system with broken dependencies in place.
</code></pre>

<p>When <code>dpkg</code> install a package and package dependency is not satisfied, it leaves the package in <code>unconfigured</code> state and that package is considered as broken. </p>

<p><code>sudo apt-get install -f</code> command tries to fix this broken package by installing the missing dependency. </p>
","159114"
"How do I find out what hard disks are in the system?","685132","","<p>I need to know what hard disks are available, including ones that aren't mounted and possibly aren't formatted. I can't find them in <code>dmesg</code> or <code>/var/log/messages</code> (too much to scroll through). I'm hoping there's a way to use <code>/dev</code> or <code>/proc</code> to find out this information, but I don't know how. I am using Linux.</p>
","<p>This is highly platform-dependent. Also different methods may treat edge cases differently (“fake” disks of various kinds, RAID volumes, …).</p>

<p>On modern udev installations, there are symbolic links to storage media in subdirectories of <code>/dev/disk</code>, that let you look up a disk or a partition by serial number (<code>/dev/disk/by-id/</code>), by UUID (<code>/dev/disk/by-uuid</code>), by filesystem label (<code>/dev/disk/by-label/</code>) or by hardware connectivity (<code>/dev/disk/by-path/</code>).</p>

<p>Under Linux 2.6, each disk and disk-like device has an entry in <code>/sys/block</code>. Under Linux since the dawn of time, disks and partitions are listed in <code>/proc/partitions</code>. Alternatively, you can use <a href=""http://ezix.org/project/wiki/HardwareLiSter"">lshw</a>: <code>lshw -class disk</code>.</p>

<p>Linux also provides the <a href=""http://linux.die.net/man/8/lsblk""><code>lsblk</code></a> utility which displays a nice tree view of the storage volumes (since util-linux 2.19, not present on embedded devices with BusyBox).</p>

<p>If you have an <code>fdisk</code> or <code>disklabel</code> utility, it might be able to tell you what devices it's able to work on.</p>

<p>You will find utility names for many unix variants on the <a href=""http://bhami.com/rosetta.html"">Rosetta Stone for Unix</a>, in particular the “list hardware configuration” and “read a disk label” lines.</p>
","4563"
"How do I remove a directory and all its contents?","680160","","<p>In bash all I know is that </p>

<pre><code>rmdir directoryname
</code></pre>

<p>will remove the directory but only if it's empty. Is there a way to force remove subdirectories?</p>
","<p>The following command will do it for you. Use caution though.</p>

<pre><code>rm -rf directoryname
</code></pre>
","45677"
"How to install Desktop Environments on CentOS 7?","666213","","<p>I have recently installed CentOS 7 (Minimal Install without GUI) and now I want to install a GUI environment in it.</p>

<p>How can I install Desktop Environments on previously installed CentOS7 without reinstalling it?</p>
","<h3>1. Installing GNOME-Desktop:</h3>

<ol>
<li><p>Install GNOME Desktop Environment on here. </p>

<pre><code># yum -y groups install ""GNOME Desktop"" 
</code></pre></li>
<li><p>Input a command like below after finishing installation:</p>

<pre><code># startx 
</code></pre></li>
<li><p>GNOME Desktop Environment will start. For first booting, initial setup runs and you have to configure it for first time. </p>

<ul>
<li>Select System language first.</li>
<li>Select your keyboard type. </li>
<li>Add online accounts if you'd like to.</li>
<li>Finally click ""Start using CentOS Linux"".</li>
</ul></li>
<li><p>GNOME Desktop Environments starts like follows.</p></li>
</ol>

<p><img src=""https://i.stack.imgur.com/HVDB0.png"" alt=""enter image description here""></p>

<h3>How to use GNOME Shell?</h3>

<p>The default GNOME Desktop of CentOS 7 starts with <em>classic mode</em> but if you'd like to use GNOME Shell, set like follows:</p>

<p><strong>Option A:</strong> If you start GNOME with <code>startx</code>, set like follows. </p>

<pre><code># echo ""exec gnome-session"" &gt;&gt; ~/.xinitrc
# startx 
</code></pre>

<p><strong>Option B:</strong> set the system graphical login <a href=""http://www.server-world.info/en/note?os=CentOS_7&amp;p=runlevel"" rel=""noreferrer""><code>systemctl set-default graphical.target</code></a> and reboot the system. After system starts </p>

<ol>
<li>Click the button which is located next to the ""Sign In"" button.</li>
<li>Select ""GNOME"" on the list. (The default is GNOME Classic)</li>
<li>Click ""Sign In"" and log in with GNOME Shell.</li>
</ol>

<p><img src=""https://i.stack.imgur.com/X7bhJ.png"" alt=""enter image description here""></p>

<ol start=""4"">
<li>GNOME shell starts like follows:</li>
</ol>

<p><img src=""https://i.stack.imgur.com/3mRsl.png"" alt=""enter image description here""></p>

<h3>2. Installing KDE-Desktop:</h3>

<ol>
<li><p>Install KDE Desktop Environment on here.</p>

<pre><code># yum -y groups install ""KDE Plasma Workspaces"" 
</code></pre></li>
<li><p>Input a command like below after finishing installation:</p>

<pre><code># echo ""exec startkde"" &gt;&gt; ~/.xinitrc
# startx
</code></pre></li>
<li>KDE Desktop Environment starts like follows:</li>
</ol>

<p><img src=""https://i.stack.imgur.com/iTACp.png"" alt=""enter image description here""></p>

<h3>3. Installing Cinnamon Desktop Environment:</h3>

<ol>
<li><p>Install Cinnamon Desktop Environment on here.</p>

<p>First Add the EPEL Repository (EPEL Repository which is provided from Fedora project.)<br>
<a href=""https://fedoraproject.org/wiki/EPEL#Extra_Packages_for_Enterprise_Linux_.28EPEL.29"" rel=""noreferrer"">Extra Packages for Enterprise Linux (EPEL)</a></p>

<ul>
<li><p>How to add EPEL Repository?</p>

<pre><code># yum -y install epel-release

# sed -i -e ""s/\]$/\]\npriority=5/g"" /etc/yum.repos.d/epel.repo # set [priority=5]
# sed -i -e ""s/enabled=1/enabled=0/g"" /etc/yum.repos.d/epel.repo # for another way, change to [enabled=0] and use it only when needed
# yum --enablerepo=epel install [Package] # if [enabled=0], input a command to use the repository
</code></pre></li>
<li><p>And now install the Cinnamon Desktop Environment from EPEL Repository:</p>

<pre><code># yum --enablerepo=epel -y install cinnamon*
</code></pre></li>
</ul></li>
<li><p>Input a command like below after finishing installation:</p>

<pre><code># echo ""exec /usr/bin/cinnamon-session"" &gt;&gt; ~/.xinitrc
# startx 
</code></pre></li>
<li><p>Cinnamon Desktop Environment will start. For first booting, initial setup runs and you have to configure it for first time. </p>

<ul>
<li>Select System language first.</li>
<li>Select your keyboard type. </li>
<li>Add online accounts if you'd like to.</li>
<li>Finally click ""Start using CentOS Linux"".</li>
</ul></li>
<li><p>Cinnamon Desktop Environment starts like follows.</p></li>
</ol>

<p><img src=""https://i.stack.imgur.com/b94jQ.png"" alt=""enter image description here""></p>

<h3>4. Installing MATE Desktop Environment:</h3>

<ol>
<li><p>Install MATE Desktop Environment on here.</p>

<pre><code># yum --enablerepo=epel -y groups install ""MATE Desktop""
</code></pre></li>
<li><p>Input a command like below after finishing installation:</p>

<pre><code># echo ""exec /usr/bin/mate-session"" &gt;&gt; ~/.xinitrc 
# startx
</code></pre></li>
<li>MATE Desktop Environment starts.</li>
</ol>

<p><img src=""https://i.stack.imgur.com/PEYSR.png"" alt=""enter image description here""></p>

<h3>5. Installing Xfce Desktop Environment:</h3>

<ol>
<li><p>Install Xfce Desktop Environment on here.</p>

<pre><code># yum -y groupinstall X11
# yum --enablerepo=epel -y groups install ""Xfce"" 
</code></pre></li>
<li><p>Input a command like below after finishing installation:</p>

<pre><code># echo ""exec /usr/bin/xfce4-session"" &gt;&gt; ~/.xinitrc 
# startx
</code></pre></li>
<li>Xfce Desktop Environment starts.</li>
</ol>

<p><img src=""https://i.stack.imgur.com/hPjxx.png"" alt=""enter image description here""></p>
","181504"
"How to permanently set environmental variables","621882","","<p>My variables are </p>

<pre><code>LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
ORACLE_HOME=/usr/lib/oracle/11.2/client64
</code></pre>

<p>How to save these variables permanently ?</p>
","<p>You can add it to the file <code>.profile</code> or your login shell profile file (located in your home directory).</p>

<p>To change the environmental variable ""permanently"" you'll need to consider at least these situations:</p>

<ol>
<li>Login/Non-login shell</li>
<li>Interactive/Non-interactive shell</li>
</ol>

<h3>bash</h3>

<ol>
<li>Bash as login shell will load <code>/etc/profile</code>, <code>~/.bash_profile</code>, <code>~/.bash_login</code>, <code>~/.profile</code> in the order</li>
<li>Bash as non-login interactive shell will load <code>~/.bashrc</code></li>
<li>Bash as non-login non-interactive shell will load the configuration specified in environment variable <code>$BASH_ENV</code></li>
</ol>



<pre><code>$EDITOR ~/.profile
#add lines at the bottom of the file:  
     export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
     export ORACLE_HOME=/usr/lib/oracle/11.2/client64
</code></pre>

<h3>zsh</h3>

<pre><code>$EDITOR ~/.zprofile
#add lines at the bottom of the file:  
     export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
     export ORACLE_HOME=/usr/lib/oracle/11.2/client64
</code></pre>

<h3>ksh</h3>

<pre><code>$EDITOR ~/.profile
#add lines at the bottom of the file:  
     export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
     export ORACLE_HOME=/usr/lib/oracle/11.2/client64
</code></pre>

<h3>bourne</h3>

<pre><code>$EDITOR ~/.profile
#add lines at the bottom of the file:  
     LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib     
     ORACLE_HOME=/usr/lib/oracle/11.2/client64
     export LD_LIBRARY_PATH ORACLE_HOME
</code></pre>

<h3>csh or tcsh</h3>

<pre><code>$EDITOR ~/.login
#add lines at the bottom of the file:  
     setenv LD_LIBRARY_PATH /usr/lib/oracle/11.2/client64/lib
     setenv ORACLE_HOME /usr/lib/oracle/11.2/client64
</code></pre>

<p>If you want to make it permanent for all users, you can edit the corresponding files under <code>/etc/</code>, i.e. <code>/etc/profile</code> for Bourne-like shells, <code>/etc/csh.login</code> for (t)csh, and <code>/etc/zsh/zprofile</code> and <code>/etc/zsh/zshrc</code> for zsh.</p>

<p>Another option is to use <code>/etc/environment</code>, which on Linux systems is read by the PAM module <code>pam_env</code> and supports only simple assignments, not shell-style expansions. (See <a href=""https://wiki.debian.org/EnvironmentVariables"" rel=""noreferrer"">Debian's guide</a> on this.)</p>

<p>These files are likely to already contain some assignments, so follow the syntax you see already present in your file.</p>

<p>Make sure to restart the shell and relogin the user, to apply the changes.</p>
","117470"
"How do you empty the buffers and cache on a Linux system?","575712","","<p>Prior to doing some benchmarking work how would one free up the memory (RAM) that the Linux Kernel is consuming for its buffers and cache?</p>

<hr>

<p>Note that this is mostly useful for benchmarking. Emptying the buffers and cache <em>reduces</em> performance! If you're here because you thought that freeing buffers and cache was a positive thing, go and read <a href=""http://www.linuxatemyram.com/"">Linux ate my RAM!</a>. The short story: free memory is unused memory is <em>wasted</em> memory.</p>
","<h3>Emptying the buffers cache</h3>

<p>If you ever want to empty it you can use this chain of commands.</p>

<pre><code># free &amp;&amp; sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches &amp;&amp; free

             total       used       free     shared    buffers     cached
Mem:       1018916     980832      38084          0      46924     355764
-/+ buffers/cache:     578144     440772
Swap:      2064376        128    2064248
             total       used       free     shared    buffers     cached
Mem:       1018916     685008     333908          0        224     108252
-/+ buffers/cache:     576532     442384
Swap:      2064376        128    2064248
</code></pre>

<p>You can signal the Linux Kernel to drop various aspects of cached items by changing the numeric argument to the above command.</p>

<ul>
<li><p>To free pagecache:</p>

<pre><code># echo 1 &gt; /proc/sys/vm/drop_caches
</code></pre></li>
<li><p>To free dentries and inodes:</p>

<pre><code># echo 2 &gt; /proc/sys/vm/drop_caches
</code></pre></li>
<li><p>To free pagecache, dentries and inodes:</p>

<pre><code># echo 3 &gt; /proc/sys/vm/drop_caches
</code></pre></li>
</ul>

<p>The above are meant to be run as root. If you're trying to do them using <code>sudo</code> then you'll need to change the syntax slightly to something like these:</p>

<pre><code>$ sudo sh -c 'echo 1 &gt;/proc/sys/vm/drop_caches'
$ sudo sh -c 'echo 2 &gt;/proc/sys/vm/drop_caches'
$ sudo sh -c 'echo 3 &gt;/proc/sys/vm/drop_caches'
</code></pre>

<p><strong>NOTE:</strong> There's a more esoteric version of the above command if you're into that:</p>

<pre><code>$ echo ""echo 1 &gt; /proc/sys/vm/drop_caches"" | sudo sh
</code></pre>

<p>Why the change in syntax? The <code>/bin/echo</code> program is running as root, because of <code>sudo</code>, but the shell that's redirecting echo's output to the root-only file is still running as you. Your current shell does the redirection <em>before</em> <code>sudo</code> starts.</p>

<h3>Seeing what's in the buffers and cache</h3>

<p>Take a look at <a href=""http://code.google.com/p/linux-ftools/""><code>linux-ftools</code></a> if you'd like to analyze the contents of the buffers &amp; cache. Specifically if you'd like to see what files are currently being cached.</p>

<h3>fincore</h3>

<p>With this tool you can see what files are being cached within a give directory.</p>

<pre><code>fincore [options] files...

  --pages=false      Do not print pages
  --summarize        When comparing multiple files, print a summary report
  --only-cached      Only print stats for files that are actually in cache.
</code></pre>

<p>For example, <code>/var/lib/mysql/blogindex</code>:</p>

<pre><code>root@xxxxxx:/var/lib/mysql/blogindex# fincore --pages=false --summarize --only-cached * 
stats for CLUSTER_LOG_2010_05_21.MYI: file size=93840384 , total pages=22910 , cached pages=1 , cached size=4096, cached perc=0.004365 
stats for CLUSTER_LOG_2010_05_22.MYI: file size=417792 , total pages=102 , cached pages=1 , cached size=4096, cached perc=0.980392 
stats for CLUSTER_LOG_2010_05_23.MYI: file size=826368 , total pages=201 , cached pages=1 , cached size=4096, cached perc=0.497512 
stats for CLUSTER_LOG_2010_05_24.MYI: file size=192512 , total pages=47 , cached pages=1 , cached size=4096, cached perc=2.127660 
stats for CLUSTER_LOG_2010_06_03.MYI: file size=345088 , total pages=84 , cached pages=43 , cached size=176128, cached perc=51.190476 
stats for CLUSTER_LOG_2010_06_04.MYD: file size=1478552 , total pages=360 , cached pages=97 , cached size=397312, cached perc=26.944444 
stats for CLUSTER_LOG_2010_06_04.MYI: file size=205824 , total pages=50 , cached pages=29 , cached size=118784, cached perc=58.000000 
stats for COMMENT_CONTENT_2010_06_03.MYI: file size=100051968 , total pages=24426 , cached pages=10253 , cached size=41996288, cached perc=41.975764 
stats for COMMENT_CONTENT_2010_06_04.MYD: file size=716369644 , total pages=174894 , cached pages=79821 , cached size=326946816, cached perc=45.639645 
stats for COMMENT_CONTENT_2010_06_04.MYI: file size=56832000 , total pages=13875 , cached pages=5365 , cached size=21975040, cached perc=38.666667 
stats for FEED_CONTENT_2010_06_03.MYI: file size=1001518080 , total pages=244511 , cached pages=98975 , cached size=405401600, cached perc=40.478751 
stats for FEED_CONTENT_2010_06_04.MYD: file size=9206385684 , total pages=2247652 , cached pages=1018661 , cached size=4172435456, cached perc=45.321117 
stats for FEED_CONTENT_2010_06_04.MYI: file size=638005248 , total pages=155763 , cached pages=52912 , cached size=216727552, cached perc=33.969556 
stats for FEED_CONTENT_2010_06_04.frm: file size=9840 , total pages=2 , cached pages=3 , cached size=12288, cached perc=150.000000 
stats for PERMALINK_CONTENT_2010_06_03.MYI: file size=1035290624 , total pages=252756 , cached pages=108563 , cached size=444674048, cached perc=42.951700 
stats for PERMALINK_CONTENT_2010_06_04.MYD: file size=55619712720 , total pages=13579031 , cached pages=6590322 , cached size=26993958912, cached perc=48.533080 
stats for PERMALINK_CONTENT_2010_06_04.MYI: file size=659397632 , total pages=160985 , cached pages=54304 , cached size=222429184, cached perc=33.732335 
stats for PERMALINK_CONTENT_2010_06_04.frm: file size=10156 , total pages=2 , cached pages=3 , cached size=12288, cached perc=150.000000 
---
total cached size: 32847278080
</code></pre>

<p>With the above output you can see that there are several *.MYD, *.MYI, and *.frm files that are currently being cached.</p>

<h3>Swap</h3>

<p>If you want to clear out your swap you can use the following commands.</p>

<pre><code>$ free
             total       used       free     shared    buffers     cached
Mem:       7987492    7298164     689328          0      30416     457936
-/+ buffers/cache:    6809812    1177680
Swap:      5963772     609452    5354320
</code></pre>

<p>Then use this command to disable swap:</p>

<pre><code>$ swapoff -a
</code></pre>

<p>You can confirm that it's now empty:</p>

<pre><code>$ free
             total       used       free     shared    buffers     cached
Mem:       7987492    7777912     209580          0      39332     489864
-/+ buffers/cache:    7248716     738776
Swap:            0          0          0
</code></pre>

<p>And to re-enable it:</p>

<pre><code>$ swapon -a
</code></pre>

<p>And now reconfirm with <code>free</code>:</p>

<pre><code>$ free
             total       used       free     shared    buffers     cached
Mem:       7987492    7785572     201920          0      41556     491508
-/+ buffers/cache:    7252508     734984
Swap:      5963772          0    5963772
</code></pre>
","87909"
"How do I set my DNS when resolv.conf is being overwritten?","558306","","<p>Most of the info I see online says to edit <code>/etc/resolv.conf</code>, but any changes I make there just get overridden.</p>

<pre><code>$ cat /etc/resolv.conf 
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- 
#     YOUR CHANGES WILL BE OVERWRITTEN
nameserver 127.0.1.1
</code></pre>

<p>It seems that 127.0.1.1 is a local instance of <code>dnsmasq</code>. The <code>dnsmasq</code> docs say to edit <code>/etc/resolv.conf</code>. I tried putting custom nameservers in <code>/etc/resolv.conf.d/base</code>, but the changes didn't show up in <code>/etc/resolv.conf</code> after running <code>sudo resolvconf -u</code>.</p>

<p>FYI, I don't want to change DNS on a per-connection basis, I want to set default DNS settings to use for all connections when not otherwise specified.</p>

<p><strong><em>UPDATE:</em></strong></p>

<p>I answered this question myself:
<a href=""https://unix.stackexchange.com/a/163506/67024"">https://unix.stackexchange.com/a/163506/67024</a></p>

<p>I think it's the best solution since:</p>

<ol>
<li>It works.</li>
<li>It requires the least amount of changes and</li>
<li>It still works in conjunction with dnsmasq's DNS cache, rather than bypassing it.</li>
</ol>
","<p>I found out that you can change the nameservers that <code>dnsmasq</code> uses by adding the following lines to <code>/etc/dnsmasq.conf</code>:</p>

<pre><code>server=8.8.8.8
server=8.8.4.4
</code></pre>

<p>I didn't have a <code>/etc/dnsmasq.conf</code> file though, since it's installed by the dnsmasq package, but Ubuntu only comes with dnsmasq-base. I ran <code>sudo apt-get install dnsmasq</code>, then edited <code>/etc/dnsmasq.conf</code>, then <code>sudo service dnsmasq restart</code> and <code>sudo service network-manager restart</code>.</p>

<p>I ran <code>sudo tail -n 200 /var/log/syslog</code> to check my syslog and verify that <code>dnsmasq</code> was using the nameservers I specified:</p>

<pre><code>Oct 21 23:00:54 mylaptop dnsmasq[8611]: using nameserver 8.8.8.8#53
Oct 21 23:00:54 mylaptop dnsmasq[8611]: using nameserver 8.8.4.4#53
</code></pre>
","163506"
"How to forward X over SSH to run graphics applications remotely?","553874","","<p>I have a machine running Ubuntu which I SSH to from my Fedora 14 machine. I want to forward X from the Ubuntu machine back to Fedora so I can run graphical programs remotely. Both machines are on a LAN.</p>

<p>I know that the <code>-X</code> option enables X11 forwarding in SSH, but I feel like I am missing some of the steps.</p>

<p>What are the required steps to forward X from a Ubuntu machine to Fedora over SSH?</p>
","<p>X11 forwarding needs to be enabled on both the client side and the server side.</p>

<p>On the client side, the <code>-X</code> (capital X) option to <code>ssh</code> enables X11 forwarding, and you can make this the default (for all connections or for a specific conection) with <code>ForwardX11 yes</code> in <a href=""http://man.openbsd.org/OpenBSD-current/man5/ssh_config.5""><code>~/.ssh/config</code></a>.</p>

<p>On the server side, <code>X11Forwarding yes</code> must specified in <a href=""http://man.openbsd.org/OpenBSD-current/man5/sshd_config.5""><code>/etc/ssh/sshd_config</code></a>. Note that the default is no forwarding (some distributions turn it on in their default <code>/etc/ssh/sshd_config</code>), and that the user cannot override this setting.</p>

<p>The <code>xauth</code> program must be installed on the server side. If there are any X11 programs there, it's very likely that <code>xauth</code> will be there. In the unlikely case <code>xauth</code> was installed in a nonstandard location, it can be called through <a href=""http://man.openbsd.org/OpenBSD-current/man8/sshd.8""><code>~/.ssh/rc</code></a> (on the server!).</p>

<p>Note that you do not need to set any environment variables on the server. <code>DISPLAY</code> and <code>XAUTHORITY</code> will automatically be set to their proper values. If you run ssh and <code>DISPLAY</code> is not set, it means ssh is not forwarding the X11 connection.</p>

<p>To confirm that ssh is forwarding X11, check for a line containing <code>Requesting X11 forwarding</code> in the <code>ssh -v -X</code> output. Note that the server won't reply either way.</p>
","12772"
"How to copy a file from a remote server to a local machine?","537715","","<p>In my terminal shell, I ssh'ed into a remote server, and I <code>cd</code> to the directory I want. </p>

<p>Now in this directory, there is a file called <code>table</code> that I want to copy to my local machine <code>/home/me/Desktop</code>. </p>

<p>How can I do this?</p>

<p>I tried <code>scp table /home/me/Desktop</code> but it gave an error about no such file or directory. </p>

<p>Does anyone know how to do this?</p>
","<p>The syntax for <code>scp</code> is:</p>

<p>If you are on the computer from which you want to send file to a remote computer:</p>

<pre><code>scp /file/to/send username@remote:/where/to/put
</code></pre>

<p>Here the <code>remote</code> can be a FQDN or an IP address.</p>

<p>On the other hand if you are on the computer wanting to receive file from a remote computer:</p>

<pre><code>scp username@remote:/file/to/send /where/to/put
</code></pre>

<p><code>scp</code> can also send files between two remote hosts:</p>

<pre><code>scp username@remote_1:/file/to/send username@remote_2:/where/to/put
</code></pre>

<p>So the basic syntax is:</p>

<pre><code>scp username@source:/location/to/file username@destination:/where/to/put
</code></pre>

<p>You can read <a href=""http://linux.die.net/man/1/scp""><code>man scp</code></a> to get more ideas on this.</p>
","188289"
"How do I make `ls` show file sizes in megabytes?","520466","","<p>What commands do I need for Linux's <code>ls</code> to show the file size in MB?</p>
","<p><code>ls -l --block-size=M</code> will give you a long format listing (needed to actually see the file size) and round file sizes <em>up</em> to the nearest MiB.</p>

<p>If you want MB (10^6 bytes) rather than MiB (2^20 bytes) units, use <code>--block-size=MB</code> instead.</p>

<p>If you don't want the <code>M</code> suffix attached to the file size, you can use something like <code>--block-size=1M</code>. Thanks <a href=""https://unix.stackexchange.com/users/22565/st%c3%a9phane-chazelas"">Stéphane Chazelas</a> for suggesting this.</p>

<p>This is described in the man page for ls; <code>man ls</code> and search for <code>SIZE</code>. It allows for units other than MB/MiB as well, and from the looks of it (I didn't try that) arbitrary block sizes as well (so you could see the file size as number of 412-byte blocks, if you want to).</p>

<p>Note that the <code>--block-size</code> parameter is a GNU extension on top of <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/ls.html"" rel=""noreferrer"">the Open Group's <code>ls</code></a>, so this may not work if you don't have a GNU userland (which most Linux installations do). The <code>ls</code> from GNU coreutils 8.5 does support --block-size as described above. Thanks to <a href=""https://unix.stackexchange.com/users/5377/kojiro"">kojiro</a> for pointing this out.</p>
","64150"
"Where are Apache file access logs stored?","504296","","<p>Does anyone know where file access logs are stored, so I can run a <code>tail -f</code> command in order to see who is accessing a particular file.</p>

<p>I have XAMPP, which is an Apache server installed on my machine, which automatically logs the accesses. It is stored in my installation folder.</p>
","<p>Ultimately, this depends on your Apache configuration. Look for <code>CustomLog</code> directives in your Apache configuration, see the <a href=""http://httpd.apache.org/docs/2.4/logs.html"" rel=""noreferrer"">manual</a> for examples.</p>

<p>A typical location for all log files is <code>/var/log</code> and subdirectories. Try <code>/var/log/apache/access.log</code> or <code>/var/log/apache2/access.log</code> or <code>/var/log/httpd/access.log</code>. If the logs aren't there, try running <code>locate access.log access_log</code>.</p>
","39008"
"How to mount a device in Linux?","490805","","<p>I read some resources about the mount command for mounting devices on Linux, but none of them is clear enough (at least for me).</p>

<p>On the whole this what most guides state:</p>

<pre><code>$ mount
(lists all currently mounted devices)

$ mount -t type device directory
(mounts that device)

for example (to mount a USB drive):
$ mount -t vfat /dev/sdb1 /media/disk
</code></pre>

<p>What's not clear to me:</p>

<ul>
<li><p>How do I know what to use for ""device"" as in <code>$ mount -t type device directory</code>? That is, how do I know that I should use ""/dev/sdb1"" in this command <code>$ mount -t vfat /dev/sdb1 /media/disk</code> to mount my USB drive?</p></li>
<li><p>what does the ""-t"" parameter define here? type?</p></li>
</ul>

<p>I read the man page (<code>$ man mount</code>) a couple of times, but I am still probably missing something. Please clarify.</p>
","<p>You can use fdisk to have an idea of what kind of partitions you have, for example:</p>

<pre><code>fdisk -l
   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *          63   204796619   102398278+   7  HPFS/NTFS
/dev/sda2       204797952   205821951      512000   83  Linux
/dev/sda3       205821952   976773119   385475584   8e  Linux LVM
</code></pre>

<p>That way you know that you have sda1,2 and 3 partitions. Now the -t option is the filesystem type, means NTFS, FAT, EXT. So in my example sda1 is ntfs so in my example it should be something like:</p>

<pre><code>mount -t ntfs /dev/sda1  /mnt/
</code></pre>

<p>USB devices are usually vfat and Linux are usually ext</p>
","18927"
"How to ""List Disk"" in Linux","489297","","<p>In Windows, all you have to type is ""List Disk"" using DisPart in a command prompt and it'll list all physical storage devices available, their size, format, etc. How do I do this in Linux?</p>
","<p>There are many tools for that, for example <code>fdisk -l</code> or <code>parted -l</code>, but probably the most handy is <code>lsblk</code> (aka <em>list block devices</em>):</p>

<h3>Example</h3>

<pre><code>$ lsblk
NAME           MAJ:MIN  RM  SIZE    RO  TYPE  MOUNTPOINT
sda            8:0      0   238.5G  0   disk
├─sda1         8:1      0   200M    0   part  /boot/efi
├─sda2         8:2      0   500M    0   part  /boot
└─sda3         8:3      0   237.8G  0   part
├─fedora-root  253:0    0   50G     0   lvm   /
├─fedora-swap  253:1    0   2G      0   lvm   [SWAP]
└─fedora-home  253:2    0   185.9G  0   lvm
</code></pre>

<p>It has many additional options, for example to show filesystems, labels, etc. As always <code>man lsblk</code> is your friend.</p>
","157155"
"Why am I still getting a password prompt with ssh with public key authentication?","469450","","<p>I'm working from the URL I found here:</p>

<p><a href=""http://web.archive.org/web/20160404025901/http://jaybyjayfresh.com/2009/02/04/logging-in-without-a-password-certificates-ssh/"">http://web.archive.org/web/20160404025901/http://jaybyjayfresh.com/2009/02/04/logging-in-without-a-password-certificates-ssh/</a></p>

<p>My ssh client is Ubuntu 64 bit 11.10 desktop and my server is Centos 6.2 64 bit. I have followed the directions. I still get a password prompt on ssh.</p>

<p>I'm not sure what to do next.</p>
","<p>Make sure the permissions on the <code>~/.ssh</code> directory and its contents are proper. When I first set up my ssh key auth, I didn't have the <code>~/.ssh</code> folder properly set up, and it yelled at me.</p>

<ul>
<li>Your home directory <code>~</code>, your <code>~/.ssh</code> directory and the <code>~/.ssh/authorized_keys</code> file on the remote machine must be writable only by you: <code>rwx------</code> and <code>rwxr-xr-x</code> are fine, but <code>rwxrwx---</code> is no good¹, even if you are the only user in your group (if you prefer numeric modes: <code>700</code> or <code>755</code>, not <code>775</code>).<br>
If <code>~/.ssh</code> or <code>authorized_keys</code> is a symbolic link, <a href=""https://unix.stackexchange.com/questions/152417/why-cant-i-use-public-private-key-authentication-with-ssh-on-arch-linux"">the canonical path (with symbolic links expanded) is checked</a>.</li>
<li>Your <code>~/.ssh/authorized_keys</code> file (on the remote machine) must be readable (at least 400), but you'll need it to be also writable (600) if you will add any more keys to it.</li>
<li>Your private key file (on the local machine) must be readable and writable only by you: <code>rw-------</code>, i.e. <code>600</code>.</li>
<li>Also, if SELinux is set to enforcing, you may need to run <code>restorecon -R -v ~/.ssh</code> (see e.g. <a href=""https://bugs.launchpad.net/ubuntu/+source/openssh/+bug/965663"" rel=""noreferrer"">Ubuntu bug 965663</a> and <a href=""http://bugs.debian.org/658675"" rel=""noreferrer"">Debian bug report #658675</a>; this is <a href=""http://wiki.centos.org/Manuals/ReleaseNotes/CentOS6.2#head-652041430eedc0752937ec8252c52132e574fd2a"" rel=""noreferrer"">patched in CentOS 6</a>).</li>
</ul>

<p>¹ <sub> Except on some distributions (Debian and derivatives) which have patched the code to allow group writability if you are the only user in your group. </sub>  </p>
","36687"
"How can I monitor disk io?","462385","","<p>I'd like to do some general disk io monitoring on a debian linux server. What are the tools I should know about that monitor disk io so I can see if a disk's performance is maxed out or spikes at certain time throughout the day?</p>
","<p>For disk I/O trending there are a few options. My personal favorite is the <code>sar</code> command from <code>sysstat</code>. By default, it gives output like this:</p>

<pre><code>09:25:01 AM     CPU     %user     %nice   %system   %iowait    %steal     %idle
09:35:01 AM     all      0.11      0.00      0.01      0.00      0.00     99.88
09:45:01 AM     all      0.12      0.00      0.01      0.00      0.00     99.86
09:55:01 AM     all      0.09      0.00      0.01      0.00      0.00     99.90
10:05:01 AM     all      0.10      0.00      0.01      0.02      0.01     99.86
Average:        all      0.19      0.00      0.02      0.00      0.01     99.78
</code></pre>

<p>The <code>%iowait</code> is the time spent waiting on I/O. Using the Debian package, you must enable the stat collector via the <code>/etc/default/sysstat</code> config file after package installation.</p>

<p>To see current utilization broken out by device, you can use the <code>iostat</code> command, also from the sysstat package:</p>

<pre><code>$ iostat -x 1
Linux 3.5.2-x86_64-linode26 (linode)    11/08/2012      _x86_64_        (4 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.84    0.00    0.08    1.22    0.07   97.80

Device:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util
xvda              0.09     1.02    2.58    0.49   112.79    12.11    40.74     0.15   48.56   3.88   1.19
xvdb              1.39     0.43    4.03    1.82    43.33    18.43    10.56     0.66  112.73   1.93   1.13
</code></pre>

<p>Some other options that can show disk usage in trending graphs is <a href=""http://munin-monitoring.org/"">munin</a> and <a href=""http://www.cacti.net/"">cacti</a>.</p>
","55215"
"How to append multiple lines to a file","459089","","<p>I am writing a bash script to look for a file if it doesn't exist then create it and append this to it:</p>

<pre><code>Host localhost
    ForwardAgent yes
</code></pre>

<p>So <code>""line then new line 'tab' then text""</code> I think its a sensitive format. 
I know you can do this:</p>

<pre><code>cat temp.txt &gt;&gt; data.txt
</code></pre>

<p>But it seems weird since its two lines. Is there a way to append that in this format:</p>

<pre><code>echo ""hello"" &gt;&gt; greetings.txt
</code></pre>
","<pre><code># possibility 1:
echo ""line 1"" &gt;&gt; greetings.txt
echo ""line 2"" &gt;&gt; greetings.txt

# possibility 2:
echo ""line 1
line 2"" &gt;&gt; greetings.txt

# possibility 3:
cat &lt;&lt;EOT &gt;&gt; greetings.txt
line 1
line 2
EOT
</code></pre>
","77278"
"Colorizing your terminal and shell environment?","457267","","<p>I spend most of my time working in Unix environments and using terminal emulators. I try to use color on the command line, because color makes the output more useful and intuitive.</p>

<p>What options exist to add color to my terminal environment? What tricks do you use? What pitfalls have you encountered?</p>

<p>Unfortunately support for color varies depending on terminal type, OS, TERM setting, utility, buggy implementations, etc.</p>

<p>Here's are some tips from my setup, after a lot of experimentation:</p>

<ol>
<li>I tend to set <code>TERM=xterm-color</code>, which is supported on most hosts (but not all).</li>
<li>I work on a number of different hosts, different OS versions, etc. I use everything from MacOSX, Ubuntu Linux, RHEL/CentOS/Scientific Linux and FreeBSD. I'm trying to keep things simple and generic, if possible.</li>
<li>I do a bunch of work using GNU <code>screen</code>, which adds another layer of fun.</li>
<li>Many OSs set things like <code>dircolors</code> and by default, and I don't want to modify this on a hundred different hosts. So I try to stick with the defaults. Instead tweak my terminal's color configuration.</li>
<li><p>Use color for some <a href=""http://www.debian.org/doc/manuals/reference/ch09.en.html#_colorized_commands"">unix commands</a> (<code>ls</code>, <code>grep</code>, <code>less</code>, <code>vim</code>) and the <a href=""http://www.cyberciti.biz/faq/bash-shell-change-the-color-of-my-shell-prompt-under-linux-or-unix/"">Bash prompt</a>. These commands seem to the standard ""<a href=""http://en.wikipedia.org/wiki/ANSI_escape_code"">ANSI escape sequences</a>"". For example:</p>

<pre><code>alias less='less --RAW-CONTROL-CHARS'
export LS_OPTS='--color=auto'
alias ls='ls ${LS_OPTS}
</code></pre></li>
</ol>

<p>I'll post my <code>.bashrc</code> and answer my own question Jeopardy Style.</p>
","<p>Here are a couple of things you can do:</p>

<p><strong>Editors + Code</strong><br>
A lot of editors have syntax highlighting support. <code>vim</code> and <code>emacs</code> have it on by default. You can also <a href=""https://askubuntu.com/questions/90013/how-do-i-enable-syntax-highlighting-in-nano"">enable it under <code>nano</code></a>.</p>

<p>You can also syntax highlight code on the terminal by using <a href=""http://pygments.org/faq/#how-can-i-use-pygments"" rel=""noreferrer"">Pygments</a> as a command-line tool.</p>

<p><strong>grep</strong><br>
<code>grep --color=auto</code> highlights all matches. You can also use <code>export GREP_OPTIONS='--color=auto'</code> to make it persistent without an alias. If you use <code>--color=always</code>, it'll <a href=""http://linuxcommando.blogspot.com/2007/10/grep-with-color-output.html"" rel=""noreferrer"">use colour even when piping</a>, which confuses things.</p>

<p><strong>ls</strong></p>

<p><code>ls --color=always</code></p>

<p>Colors specified by:</p>

<pre><code>export LS_COLORS='rs=0:di=01;34:ln=01;36:mh=00:pi=40;33'
</code></pre>

<p>(hint: <code>dircolors</code> can be helpful)</p>

<p><strong>PS1</strong><br>
You can set your PS1 (shell prompt) to use colours. For example:</p>

<pre><code>PS1='\e[33;1m\u@\h: \e[31m\W\e[0m\$ '
</code></pre>

<p>Will produce a PS1 like:</p>

<p>[yellow]lucas@ubuntu: [red]~[normal]$ </p>

<p>You can get really creative with this. As an idea:</p>

<pre><code>PS1='\e[s\e[0;0H\e[1;33m\h    \t\n\e[1;32mThis is my computer\e[u[\u@\h:  \w]\$ '
</code></pre>

<p>Puts a bar at the top of your terminal with some random info. (For best results, also use <code>alias clear=""echo -e '\e[2J\n\n'""</code>.)</p>

<p><strong>Getting Rid of Escape Sequences</strong></p>

<p>If something is stuck outputting colour when you don't want it to, I use this <code>sed</code> line to strip the escape sequences:</p>

<pre><code>sed ""s/\[^[[0-9;]*[a-zA-Z]//gi""
</code></pre>

<p>If you want a more authentic experience, you can also get rid of lines starting with <code>\e[8m</code>, which instructs the terminal to hide the text. (Not widely supported.)</p>

<pre><code>sed ""s/^\[^[8m.*$//gi""
</code></pre>

<p>Also note that those ^[s should be actual, literal ^[s. You can type them by pressing ^V^[ in bash, that is <kbd>Ctrl</kbd> + <kbd>V</kbd>, <kbd>Ctrl</kbd> + <kbd>[</kbd>. </p>
","150"
"Check package version using apt-get/aptitude?","448067","","<p>Before I install a package I'd like to know what version I would get. How do I check the version before installing using <code>apt-get</code> or <code>aptitude</code> on debian or ubuntu?</p>
","<p><strong>apt-get</strong></p>

<p>You can run a simulation to see what would happen if you upgrade/install a package:</p>

<pre><code>apt-get -s install &lt;package&gt;
</code></pre>

<p>To see all possible upgrades, run a <code>upgrade</code> in verbose mode and (to be safe) with simulation, press <kbd>n</kbd> to cancel:</p>

<pre><code>apt-get -V -s upgrade
</code></pre>

<p><strong>apt-cache</strong></p>

<p>The option <code>policy</code> can show the installed and the remote version (install candidate) of a package.</p>

<pre><code>apt-cache policy &lt;package&gt;
</code></pre>

<p><strong>apt-show-versions</strong></p>

<p>If installed, shows version information about one or more packages:</p>

<pre><code>apt-show-versions &lt;package&gt;
</code></pre>

<p>Passing the <code>-u</code> switch with or without a package name will only show upgradeable packages.</p>

<p><strong>aptitude</strong></p>

<p>The console GUI of <code>aptitude</code> can display upgradeable packages with new versions. Open the menu 'Upgradable Packages'. Pressing <kbd>v</kbd> on a package will show more detailed version information.</p>

<p>Or on the command-line:</p>

<pre><code>aptitude versions &lt;package&gt;
</code></pre>

<p>Passing <code>-V</code> will show detailed information about versions, again to be safe with the simulation switch:</p>

<pre><code>aptitude -V -s install &lt;package&gt;
</code></pre>

<p>Substituting <code>install &lt;package&gt;</code> with <code>upgrade</code> will show the versions from all upgradeable packages.</p>
","6286"
"How do I copy a folder keeping owners and permissions intact?","445510","","<p>So I was going to back up my home folder by copying it to an external drive as follows:</p>

<pre><code>sudo cp -r /home/my_home /media/backup/my_home
</code></pre>

<p>With the result that all folders on the external drives are now owned by <code>root:root</code>. How can I have <code>cp</code> keep the ownership and permissions from the original?</p>
","<pre><code>sudo cp -rp /home/my_home /media/backup/my_home
</code></pre>

<p>From cp manpage:</p>

<pre><code> -p     same as --preserve=mode,ownership,timestamps

 --preserve[=ATTR_LIST]
          preserve the specified attributes (default: mode,ownership,timestamps),
          if possible additional attributes: context, links, xattr, all
</code></pre>
","43608"
"Unix/Linux undelete/recover deleted files","438284","","<p>Is there a command to recover/undelete deleted files by <code>rm</code>?</p>

<pre><code>$ rm -rf /path/to/myfile
</code></pre>

<p>How can I recover <code>myfile</code>? If there is such a tool how can I use it?</p>
","<p>The link someone provided in the comments is likely your best chance.</p>

<p><a href=""http://www.cyberciti.biz/tips/linux-ext3-ext4-deleted-files-recovery-howto.html"">Linux debugfs Hack: Undelete Files</a></p>

<p>That write-up though looking a little intimidating is actually fairly straight forward to follow. In general the steps are as follows:</p>

<ol>
<li><p>Use debugfs to view a filesystems log</p>

<pre><code>$ debugfs -w /dev/mapper/wks01-root
</code></pre></li>
<li><p>At the debugfs prompt</p>

<pre><code>debugfs: lsdel
</code></pre></li>
<li><p>Sample output</p>

<pre><code>Inode  Owner  Mode    Size    Blocks   Time deleted
23601299      0 120777      3    1/   1 Tue Mar 13 16:17:30 2012
7536655      0 120777      3    1/   1 Tue May  1 06:21:22 2012
2 deleted inodes found.
</code></pre></li>
<li><p>Run the command in debugfs</p>

<pre><code>debugfs: logdump -i &lt;7536655&gt;
</code></pre></li>
<li><p>Determine files inode</p>

<pre><code>...
...
....
output truncated
    Fast_link_dest: bin
    Blocks:  (0+1): 7235938
  FS block 7536642 logged at sequence 38402086, journal block 26711
    (inode block for inode 7536655):
    Inode: 7536655   Type: symlink        Mode:  0777   Flags: 0x0   Generation: 3532221116
    User:     0   Group:     0   Size: 3
    File ACL: 0    Directory ACL: 0
    Links: 0   Blockcount: 0
    Fragment:  Address: 0    Number: 0    Size: 0
    ctime: 0x4f9fc732 -- Tue May  1 06:21:22 2012
    atime: 0x4f9fc730 -- Tue May  1 06:21:20 2012
    mtime: 0x4f9fc72f -- Tue May  1 06:21:19 2012
    dtime: 0x4f9fc732 -- Tue May  1 06:21:22 2012
    Fast_link_dest: bin
    Blocks:  (0+1): 7235938
No magic number at block 28053: end of journal.
</code></pre></li>
<li><p>With the above inode info run the following commands</p>

<pre><code># dd if=/dev/mapper/wks01-root of=recovered.file.001 bs=4096 count=1 skip=7235938
# file recovered.file.001
file: ASCII text, with very long lines
</code></pre></li>
</ol>

<p>Files been recovered to <code>recovered.file.001</code>.</p>

<h3>Other options</h3>

<p>If the above isn't for you I've used tools such as <code>photorec</code> to recover files in the past, but it's geared for image files only. I've written about this method extensively on my blog in this article titled: </p>

<p><a href=""http://www.lamolabs.org/blog/5073/how-to-recover-corrupt-jpeg-and-mov-files-from-a-digital-cameras-sdd-card-on-fedoracentosrhel/"">How to Recover Corrupt jpeg and mov Files from a Digital Camera's SDD Card on Fedora/CentOS/RHEL</a>.</p>
","80285"
"How to run find -exec?","434370","","<p>I'd like to find the files in the current directory that contain the text ""chrome"".</p>

<pre><code>$ find . -exec grep chrome
find: missing argument to `-exec'
</code></pre>

<p>What am I doing wrong?</p>
","<p>You missed a <code>;</code> or a <code>+</code> and a <code>{}</code>:</p>

<pre><code>find . -exec grep chrome {} \;
</code></pre>

<p>or</p>

<pre><code>find . -exec grep chrome {} +
</code></pre>

<p><code>find</code> will execute <code>grep</code> and will substitute <code>{}</code> with the filename(s) found. The difference between <code>;</code> and <code>+</code> is that with <code>;</code> a single <code>grep</code> command for each file is executed whereas with <code>+</code> as many files as possible are given as parameters to <code>grep</code> at once.</p>
","12904"
"Timezone setting in Linux","427546","","<p>I'm setting the timezone to GMT+6 on my Linux machine by copying the zoneinfo file to <code>/etc/localtime</code>, but the <code>date</code> command is still showing the time as <code>UTCtime-6</code>. Can any one explain to me this behavior?</p>

<p>I'm assuming the <code>date</code> command should display <code>UTCtime+6</code> time. Here are steps I'm following:</p>

<pre><code>date
Wed Jan 22 17:29:01 IST 2014

date -u
Wed Jan 22 11:59:01 UTC 2014

cp /usr/share/zoneinfo/Etc/GMT+6 /etc/localtime

date
Wed Jan 22 05:59:21 GMT+6 2014

date -u
Wed Jan 22 11:59:01 UTC 2014
</code></pre>
","<p>Take a look at this blog post titled: <a href=""http://www.thegeekstuff.com/2010/09/change-timezone-in-linux/"" rel=""noreferrer"">How To: 2 Methods To Change TimeZone in Linux</a>.</p>

<h3>Red Hat distros</h3>

<p>If you're using a distribution such as Red Hat then your approach of copying the file would be mostly acceptable.</p>

<pre><code>$ ls /usr/share/zoneinfo/
Africa/      CET          Etc/         Hongkong     Kwajalein    Pacific/     ROK          zone.tab
America/     Chile/       Europe/      HST          Libya        Poland       Singapore    Zulu
Antarctica/  CST6CDT      GB           Iceland      MET          Portugal     Turkey       
Arctic/      Cuba         GB-Eire      Indian/      Mexico/      posix/       UCT          
Asia/        EET          GMT          Iran         MST          posixrules   Universal    
Atlantic/    Egypt        GMT0         iso3166.tab  MST7MDT      PRC          US/          
Australia/   Eire         GMT-0        Israel       Navajo       PST8PDT      UTC          
Brazil/      EST          GMT+0        Jamaica      NZ           right/       WET          
Canada/      EST5EDT      Greenwich    Japan        NZ-CHAT      ROC          W-SU         
</code></pre>

<p>I would recommend linking to it rather than copying however.</p>

<pre><code>$ sudo unlink /etc/localtime 
$ sudo ln -s /usr/share/zoneinfo/Etc/GMT+6 /etc/localtime
</code></pre>

<p>Now date shows the different timezone:</p>

<pre><code>$ date -u
Thu Jan 23 05:40:31 UTC 2014

$ date 
Wed Jan 22 23:40:38 GMT+6 2014
</code></pre>

<h3>Ubuntu/Debian Distros</h3>

<p>To change the timezone on either of these distros you can use this command:</p>

<pre><code>$ sudo dpkg-reconfigure tzdata
</code></pre>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/hdYIX.png"" alt=""ss #1""></p>

<pre><code>$ sudo dpkg-reconfigure tzdata

Current default time zone: 'Etc/GMT-6'
Local time is now:      Thu Jan 23 11:52:16 GMT-6 2014.
Universal Time is now:  Thu Jan 23 05:52:16 UTC 2014.
</code></pre>

<p>Now when we check it out:</p>

<pre><code>$ date -u
Thu Jan 23 05:53:32 UTC 2014

$ date 
Thu Jan 23 11:53:33 GMT-6 2014
</code></pre>

<p><strong>NOTE:</strong> There's also this option in Ubuntu 14.04 and higher with a single command (source: Ask Ubuntu - <a href=""https://askubuntu.com/questions/323131/setting-timezone-from-terminal/524362#524362"">setting timezone from terminal</a>):</p>

<pre><code>$ sudo timedatectl set-timezone Etc/GMT-6
</code></pre>

<h3>On the use of ""Etc/GMT+6""</h3>

<p><em>excerpt from <a href=""https://stackoverflow.com/a/21300212/33204"">@MattJohnson's answer on SO</a></em></p>

<blockquote>
  <p>Zones like <code>Etc/GMT+6</code> are intentionally reversed for backwards compatibility with POSIX standards.  See the comments in <a href=""https://github.com/eggert/tz/blob/master/etcetera"" rel=""noreferrer"">this file</a>.</p>
  
  <p>You should almost never need to use these zones.  Instead you should be using a fully named time zone like <code>America/New_York</code> or <code>Europe/London</code> or whatever is appropriate for your location.  Refer to the list <a href=""http://en.wikipedia.org/wiki/List_of_tz_database_time_zones"" rel=""noreferrer"">here</a>.</p>
</blockquote>
","110529"
"How can I display the contents of a text file on the command line?","424373","","<p>I would like to display the contents of a text file on the command line. The file only contains 5-6 characters. Is there an easy way to do this?</p>
","<h3>Using <code>cat</code></h3>

<p>Since your file is short, you can use
<a href=""http://en.wikipedia.org/wiki/Cat_%28Unix%29""><code>cat</code></a>.</p>

<pre><code>cat filename
</code></pre>

<h3>Using <code>less</code></h3>

<p>If you have to view the contents of a longer file, you can use a pager such as
<a href=""http://en.wikipedia.org/wiki/Less_%28Unix%29""><code>less</code></a>.</p>

<pre><code>less filename
</code></pre>

<p>You can make <code>less</code> behave like <code>cat</code> when invoked on small files and behave
normally otherwise by passing it the <code>-F</code> and <code>-X</code> flags.</p>

<pre><code>less -FX filename
</code></pre>

<p>I have an alias for <code>less -FX</code>. You can make one yourself like so:</p>

<pre><code>alias aliasname='less -FX'
</code></pre>

<p>If you add the alias to your <a href=""http://hacktux.com/bash/bashrc/bash_profile"">shell
configuration</a>, you can use it
forever.</p>

<h3>Using <code>od</code></h3>

<p>If your file contains strange or unprintable characters, you can use
<a href=""http://linux.die.net/man/1/od""><code>od</code></a> to examine the characters. For example,</p>

<pre><code>$ cat file
(ÐZ4 ?o=÷jï
$ od -c test
0000000 202 233   ( 320   K   j 357 024   J 017   h   Z   4 240   ?   o
0000020   = 367  \n
0000023
</code></pre>
","86325"
"Finding the PID of the process using a specific port?","421818","","<p>I am installing hadoop on my Ubuntu system. When I start it, it reports that port 9000 is busy.</p>

<p>I used:</p>

<pre><code>netstat -nlp|grep 9000
</code></pre>

<p>to see if such a port exists and I got this:</p>

<pre><code>   tcp        0      0 127.0.0.1:9000          0.0.0.0:*               LISTEN
</code></pre>

<p>But how can I get the PID of the process which is holding it?</p>
","<p>On Linux, you must be root or the other user to get process information for processes running as other users, so prepending <code>sudo</code> is most of what you need. In addition to that, on modern Linux systems, <code>ss</code> is tool to use to do this:</p>

<pre><code>$ sudo ss -lptn 'sport = :80'
State   Local Address:Port  Peer Address:Port              
LISTEN  127.0.0.1:80        *:*                users:((""nginx"",pid=125004,fd=12))
LISTEN  ::1:80              :::*               users:((""nginx"",pid=125004,fd=11))
</code></pre>

<p>You can also use the same invocation you're currently using, but remember to <code>sudo</code>:</p>

<pre><code>$ sudo netstat -nlp | grep :80
tcp  0  0  0.0.0.0:80  0.0.0.0:*  LISTEN  125004/nginx
</code></pre>

<p>You can also use lsof:</p>

<pre><code>$ sudo lsof -n -i :80 | grep LISTEN
nginx   125004 nginx    3u  IPv4   6645      0t0  TCP 0.0.0.0:80 (LISTEN)
</code></pre>
","106562"
"Compress a folder with tar?","417670","","<p>I'm trying to compress a folder (<code>/var/www/</code>) to <code>~/www_backups/$time.tar</code> where <code>$time</code> is the current date.</p>

<p>This is what I have:</p>

<pre><code>cd /var/www &amp;&amp; sudo tar -czf ~/www_backups $time""
</code></pre>

<p>I am completely lost and I've been at this for hours now. Not sure if <code>-czf</code> is correct. I simply want to copy all of the content in <code>/var/www</code> into a <code>$time.tar</code> file, and I want to maintain the file permissions for all of the files. Can anyone help me out?</p>
","<p>To <code>tar</code> and <code>gzip</code> a folder, the syntax is:</p>

<pre><code>tar czf name_of_archive_file.tar.gz name_of_directory_to_tar
</code></pre>

<p>The <code>-</code> is optional. If you want to <code>tar</code> the current directory, use <code>.</code> to designate that.</p>

<p>To construct your filename, use the <code>date</code> utility (look at its man page for the available format options). For example:</p>

<pre><code>cd /var/www &amp;&amp; sudo tar czf ~/www_backups/$(date +%Y%m%d-%H%M%S).tar.gz .
</code></pre>

<p>This would have created a file named something like <code>20120902-185558.tar.gz</code>.</p>

<p>On Linux, chances are your <code>tar</code> also supports BZip2 compression with the <code>j</code> rather than <code>z</code> option. And possibly others. Check the man page on your local system.</p>
","46971"
"How do I remove a user from a group?","416042","","<p>Which command should I use to remove a user from a group in Debian?</p>

<p>When adding a user to a group, it can be done with:</p>

<pre><code>usermod -a -G group user
</code></pre>

<p>However, I could not find a similar command (accepting a group and user as arguments) for removing the user from the group. The closest I could get is:</p>

<pre><code>usermod -G all,existing,groups,except,for,group user
</code></pre>

<p>Is there a command like <code>usermod OPTION group user</code> with OPTION an option to make <code>usermod</code> (or a similar program) remove the user from group?</p>
","<p>You can use <code>gpasswd</code>:</p>

<pre><code># gpasswd -d user group
</code></pre>

<p>then the new group config will be assigned at the next login, at least on Debian. If the user is logged in, the effects of the command aren't seen immediately.</p>
","29572"
"How to clean log file?","406057","","<p>Is there a better way to clean the log file? I usually delete the old logfile and create a new logfile and I am looking for a shorter type/bash/command program. How can I use an alias?</p>
","<pre><code>&gt; logfile
</code></pre>

<p>or</p>

<pre><code>cat /dev/null &gt; logfile
</code></pre>

<p>or</p>

<pre><code>true | tee logfile
</code></pre>

<p>(fell free to substitute <code>false</code> or any other command that produces no output, like e.g. <code>:</code> does in <code>bash</code>) if you want to be more eloquent, will all empty <code>logfile</code> (actually they will truncate it to zero size).</p>

<p>If you want to know how long it ""takes"", you may use</p>

<pre><code>dd if=/dev/null of=logfile
</code></pre>

<p>(which is the same as <code>dd if=/dev/null &gt; logfile</code>, by the way)</p>

<p>You can also use</p>

<pre><code>truncate --size 0 logfile
</code></pre>

<p>(or <code>truncate -s 0 logfile</code>) to be perfectly explicit or, if you don't want to, </p>

<pre><code>rm logfile
</code></pre>

<p>(in which case you are relying on the common behaviour that applications usually do recreate a logfile if it doesn't exist already).</p>

<p>However, since logfiles are usually useful, you might want to compress and save a copy. While you could do that with your own script, it is a good idea to at least try using an existing working solution, in this case <code>logrotate</code>, which can do exactly that and is reasonably configurable.</p>

<p>Should you need to do it for several files, the safe way is</p>

<pre><code>truncate -s 0 file1 file2 ...
</code></pre>

<p>or</p>

<pre><code>&gt; file1 &gt; file2 ...
</code></pre>

<p>Some shells (<code>zsh</code>) also allow one to specify several redirection targets.</p>

<p>This works (at least in <code>bash</code>) since it creates all the redirections required although only the last one will catch any input (or none in this case). The <code>tee</code> example with several files should work in any case (given your <code>tee</code> does know how to handle several output files)</p>

<p>Of course, the good old shell loop would work as well:</p>

<pre class=""lang-sh prettyprint-override""><code>for f in file1 file2 ... ; do
    # pick your favourite emptying method
done
</code></pre>

<p>although it will be much slower due to the command being run separately for each file. That may be helped by using <code>find</code>:</p>

<pre><code>find &lt;criteria matching required files&gt; \
    -exec &lt;command capable of zeroing several files&gt; {} \+
</code></pre>

<p>or</p>

<pre><code>find &lt;criteria matching required files&gt; -delete
</code></pre>
","92390"
"How to get the complete and exact list of mounted filesystems in Linux?","405094","","<p>I usually use <code>mount</code> to check which filesystems are mounted. I also know there is some connection between <code>mount</code> and <code>/etc/mtab</code> but I'm not sure about the details. After reading <a href=""https://unix.stackexchange.com/questions/16154/how-to-check-if-proc-is-mounted"">How to check if /proc/ is mounted</a> I get more confused.</p>

<p>My question is: How to get the most precise list of mounted filesystems? Should I just use <code>mount</code>, or read the contents of <code>/etc/mtab</code>, or contents of <code>/proc/mounts</code>? What would give the most trustworthy result?</p>
","<p>The definitive list of mounted filesystems in in <code>/proc/mounts</code>.</p>

<p>If you have any form of containers on your system, <code>/proc/mounts</code> only lists the filesystems that are in your present container. For example, in a <a href=""http://en.wikipedia.org/wiki/Chroot"" rel=""noreferrer"">chroot</a>, <code>/proc/mounts</code> lists only the filesystems whose mount point is within the chroot. (<a href=""https://unix.stackexchange.com/questions/14345/how-do-i-tell-im-running-in-a-chroot/14346#14346"">There are ways to escape the chroot, mind.</a>)</p>

<p>There's also a list of mounted filesystems in <code>/etc/mtab</code>. This list is maintained by the <code>mount</code> and <code>umount</code> commands. That means that if you don't use these commands (which is pretty rare), your action (mount or unmount) won't be recorded. In practice, it's mostly in a chroot that you'll find <code>/etc/mtab</code> files that differ wildly from the state of the system (also mounts performed in the chroot will be reflected in the chroot's <code>/etc/mtab</code> but not in the main <code>/etc/mtab</code>). Actions performed while <code>/etc/mtab</code> is on a read-only filesystem are also not recorded there. The reason why you'd sometimes want to consult <code>/etc/mtab</code> in preference to or in addition to <code>/proc/mounts</code> is that because it has access to the mount command line, it's sometimes able to present information in a way that's easier to understand; for example you see mount options as requested (whereas <code>/proc/mounts</code> lists the <code>mount</code> and kernel defaults as well), and bind mounts appear as such in <code>/etc/mtab</code>.</p>
","24230"
"How to get over ""device or resource busy""?","403528","","<p>I tried to <code>rm -rf</code> a folder, and got ""device or resource busy"".</p>

<p>In Windows, I would have used LockHunter to resolve this. What's the linux equivalent? (Please give as answer a simple ""unlock this"" method, and not complete articles like <a href=""http://www.innovationsts.com/blog/?p=658"">this one</a>. Although they're useful, I'm currently interested in just ASimpleMethodThatWorks™)</p>
","<p>The tool you want is <code>lsof</code>, which stands for <em>list open files</em>.</p>

<p>It has a lot of options, so check the man page, but if you want to see all open files under a directory:</p>

<pre><code>lsof +D /path
</code></pre>

<p>That will recurse through the filesystem under <code>/path</code>, so beware doing it on large directory trees.</p>

<p>Once you know which processes have files open, you can exit those apps, or kill them with the <code>kill(1)</code> command.</p>
","11241"
"Create a symbolic link relative to the current directory","402950","","<p>I'm trying to create a symbolic link in my home directory that points to a directory on my external HDD.</p>

<p>It works fine when I specify it like this:</p>

<pre><code>cd ~
ln -s /run/media/name/exhdd/Data/ Data
</code></pre>

<p>However it creates a faulty link when I try this:</p>

<pre><code>cd /run/media/name/exhdd
ln -s Data/ ~/Data
</code></pre>

<p>This creates a link that I cannot <code>cd</code> into.</p>

<p>When I try, bash complains:</p>

<pre><code>bash: cd: Data: Too many levels of symbolic links
</code></pre>

<p>The Data symbolic link in my home is also colored in red when <code>ls</code> is set to display colored output.</p>

<p>Why is this happening? How can I create a link in that manner? (I want to create a symlink to a directory in my working directory in another directory.)</p>

<p><hr />
<strong>Edit:</strong> according to <a href=""https://stackoverflow.com/a/9104384"">this</a> StackOverflow answer, if the second argument (in my case that'd be ~/Data) already exists and is a directory, <code>ln</code> will create a symlink to the target <em>inside</em> that directory.</p>

<p>However, I'm experiencing the same issue with:</p>

<pre><code>ln -s Data/ ~/
</code></pre>
","<p>Here's what's happening. If you make a symlink with a relative path, the symlink will be relative. Symlinks just store the paths that you give them. They never resolve paths to full paths. Running</p>

<pre><code>$ pwd
/usr/bin
$ ln -s ls /usr/bin/ls2
</code></pre>

<p>creates a symlink named <code>ls2</code> in <code>/usr/bin</code> to <code>ls</code>(viz. <code>/usr/bin/ls</code>) relative to the directory that the symlink is in (<code>/usr/bin</code>). The above command would create a functional symlink from any directory.</p>

<pre><code>$ pwd
/home/me
$ ln -s ls /usr/bin/ls2
</code></pre>

<p>If you moved the symlink to a different directory, it would cease to point to the file at <code>/usr/bin/ls</code>.</p>

<p>You are making a symlink that points to <code>Data</code>, and naming it <code>Data</code>. It is pointing to itself. You have to make a symlink with the absolute path of the directory.</p>

<pre><code>ln -s ""$(realpath Data)"" ~/Data
</code></pre>
","84178"
"What if 'kill -9' does not work?","394112","","<p>I have a process I can't kill with <code>kill -9 &lt;pid&gt;</code>. What's the problem in such a case, especially since I am the owner of that process. I thought nothing could evade that <code>kill</code> option.</p>
","<p><code>kill -9</code> (<a href=""http://en.wikipedia.org/wiki/SIGKILL"">SIGKILL</a>) always works, provided you have the permission to kill the process. Basically either the process must be started by you and not be setuid or setgid, or you must be root. There is one exception: even root cannot send a fatal signal to PID 1 (the <code>init</code> process).</p>

<p>However <code>kill -9</code> is not guaranteed to work <em>immediately</em>. All signals, including SIGKILL, are delivered asynchronously: the kernel may take its time to deliver them. Usually, delivering a signal takes at most a few microseconds, just the time it takes for the target to get a time slice. However, if the target has <a href=""http://www.cis.temple.edu/~ingargio/cis307/readings/signals.html"">blocked the signal</a>, the signal will be queued until the target unblocks it.</p>

<p>Normally, processes cannot block SIGKILL. But kernel code can, and processes execute kernel code when they call <a href=""http://en.wikipedia.org/wiki/System_call"">system calls</a>. Kernel code blocks all signals when interrupting the system call would result in a badly formed data structure somewhere in the kernel, or more generally in some kernel invariant being violated. So if (due to a bug or misdesign) a system call blocks indefinitely, there may effectively be no way to kill the process. (But the process <em>will</em> be killed if it ever completes the system call.)</p>

<p>A process blocked in a system call is in <a href=""http://en.wikipedia.org/wiki/Sleep_%28operating_system%29#Uninterruptible_sleep"">uninterruptible sleep</a>. The <code>ps</code> or <code>top</code> command will (on most unices) show it in state <code>D</code> (originally for “<strong>d</strong>isk”, I think).</p>

<p>A classical case of long uninterruptible sleep is processes accessing files over <a href=""http://en.wikipedia.org/wiki/Network_File_System_%28protocol%29"">NFS</a> when the server is not responding; modern implementations tend not to impose uninterruptible sleep (e.g. under Linux, the <code>intr</code> mount option allows a signal to interrupt NFS file accesses).</p>

<p>You may sometimes see entries marked <code>Z</code> (or <code>H</code> under Linux, I don't know what the distinction is) in the <code>ps</code> or <code>top</code> output. These are technically not processes, they are zombie processes, which are nothing more than an entry in the process table, kept around so that the parent process can be notified of the death of its child. They will go away when the parent process <a href=""http://pubs.opengroup.org/onlinepubs/009695399/functions/wait.html"">pays attention</a> (or dies).</p>
","5648"
"How can I pass a command line argument into a shell script?","392604","","<p>I know that shell scripts just run commands as if they were executed in at the command prompt.  I'd like to be able to run shell scripts as if they were functions... That is, taking an input value or string into the script.  How do I approach doing this?</p>
","<p>The shell command and any arguments to that command appear as <em>numbered</em> shell variables: <code>$0</code> has the string value of the command itself, something like <code>script</code>, <code>./script</code>, <code>/home/user/bin/script</code> or whatever. Any arguments appear as <code>""$1""</code>, <code>""$2""</code>, <code>""$3""</code> and so on.  The count of arguments is in the shell variable <code>""$#""</code>.</p>

<p>Common ways of dealing with this involve shell commands <code>getopts</code> and <code>shift</code>. <code>getopts</code> is a lot like the C <code>getopt()</code> library function. <code>shift</code> moves the value of <code>$2</code> to <code>$1</code>, <code>$3</code> to <code>$2</code>, and so on; <code>$#</code> gets decremented.  Code ends up looking at the value of <code>""$1""</code>, doing things using a <code>case</code>…<code>esac</code> to decide on an action, and then doing a <code>shift</code> to move <code>$1</code> to the next argument.  It only ever has to examine <code>$1</code>, and maybe <code>$#</code>.</p>
","31419"
"umount: device is busy. Why?","384047","","<p>When running <code>umount /path</code> I get:</p>

<pre><code>umount: /path: device is busy.
</code></pre>

<p>The filesystem is huge, so <code>lsof +D /path</code> is not a realistic option.</p>

<p><code>lsof /path</code>, <code>lsof +f -- /path</code>, and <code>fuser /path</code> all return nothing. <code>fuser -v /path</code> gives:</p>

<pre><code>                  USER        PID ACCESS COMMAND
/path:            root     kernel mount /path
</code></pre>

<p>which is normal for all unused mounted file systems.</p>

<p><code>umount -l</code> and <code>umount -f</code> is not good enough for my situation.</p>

<p>How do I figure out why the kernel thinks this filesystem is busy?</p>
","<p>It seems the cause for my issue was the <code>nfs-kernel-server</code> was exporting the directory. The <code>nfs-kernel-server</code> probably goes behind the normal open files and thus is not listed by <code>lsof</code> and <code>fuser</code>.</p>

<p>When I stopped the <code>nfs-kernel-server</code> I could <code>umount</code> the directory.</p>

<p>I have made a page with examples of all solutions so far here: <a href=""http://oletange.blogspot.com/2012/04/umount-device-is-busy-why.html"">http://oletange.blogspot.com/2012/04/umount-device-is-busy-why.html</a></p>
","15027"
"Count total number of occurrences using grep","378489","","<p><code>grep -c</code> is useful for finding how many times a string occurs in a file, but it only counts each occurence once per line. How to count multiple occurences per line?</p>

<p>I'm looking for something more elegant than:</p>

<pre><code>perl -e '$_ = &lt;&gt;; print scalar ( () = m/needle/g ), ""\n""'
</code></pre>
","<p>grep's <code>-o</code> will only output the matches, ignoring lines; <code>wc</code> can count them:</p>

<pre><code>grep -o 'needle' file | wc -l
</code></pre>

<p>This will also match 'needles' or 'multineedle'.<br>
Only single words:</p>

<pre><code>grep -o '\bneedle\B' file | wc -l
# or:
grep -o '\&lt;needle\&gt;' file | wc -l
</code></pre>
","6985"
"How can I resolve a hostname to an IP address in a Bash script?","378422","","<p>What's the most concise way to resolve a hostname to an IP address in a Bash script? I'm using <a href=""http://en.wikipedia.org/wiki/Arch_Linux"">Arch Linux</a>.</p>
","<p>You can use <code>getent</code>, which comes with <code>glibc</code> (so you almost certainly have it on Linux). This resolves using gethostbyaddr/gethostbyname2, and so also will check <code>/etc/hosts</code>/NIS/etc:</p>

<pre><code>getent hosts unix.stackexchange.com | awk '{ print $1 }'
</code></pre>

<p>Or, as Heinzi said below, you can use <code>dig</code> with the <code>+short</code> argument (queries DNS servers directly, does not look at <code>/etc/hosts</code>/NSS/etc) :</p>

<pre><code>dig +short unix.stackexchange.com
</code></pre>

<p>If <code>dig +short</code> is unavailable, any one of the following should work. All of these query DNS directly and ignore other means of resolution:</p>

<pre><code>host unix.stackexchange.com | awk '/has address/ { print $4 }'
nslookup unix.stackexchange.com | awk '/^Address: / { print $2 }'
dig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 }'
</code></pre>

<p>If you want to only print one IP, then add the <code>exit</code> command to <code>awk</code>'s workflow.</p>

<pre><code>dig +short unix.stackexchange.com | awk '{ print ; exit }'
getent hosts unix.stackexchange.com | awk '{ print $1 ; exit }'
host unix.stackexchange.com | awk '/has address/ { print $4 ; exit }'
nslookup unix.stackexchange.com | awk '/^Address: / { print $2 ; exit }'
dig unix.stackexchange.com | awk '/^;; ANSWER SECTION:$/ { getline ; print $5 ; exit }'
</code></pre>
","20793"
"How to terminate a background process?","362094","","<p>I have started a wget on remote machine in background using <code>&amp;</code>. Suddenly it stops downloading. I want to terminate its process, then re-run the command. How can I terminate it?</p>

<p>I haven't closed its shell window. But as you know it doesn't stop using <kbd>Ctrl</kbd>+<kbd>C</kbd> and <kbd>Ctrl</kbd>+<kbd>Z</kbd>.</p>
","<p>There are many ways to go about this.</p>

<h3>Method #1 - ps</h3>

<p>You can use the <code>ps</code> command to find the process ID for this process and then use the PID to kill the process.</p>

<h3>Example</h3>

<pre><code>$ ps -eaf | grep [w]get 
saml      1713  1709  0 Dec10 pts/0    00:00:00 wget ...

$ kill 1713
</code></pre>

<h3>Method #2 - pgrep</h3>

<p>You can also find the process ID using <code>pgrep</code>.</p>

<h3>Example</h3>

<pre><code>$ pgrep wget
1234

$ kill 1234
</code></pre>

<h3>Method #3 - pkill</h3>

<p>If you're sure it's the only <code>wget</code> you've run you can use the command <code>pkill</code> to kill the job by name.</p>

<h3>Example</h3>

<pre><code>$ pkill wget
</code></pre>

<h3>Method #4 - jobs</h3>

<p>If you're in the same shell from where you ran the job that's now backgrounded. You can check if it's running still using the <code>jobs</code> command, and also kill it by its job number.</p>

<h3>Example</h3>

<p>My fake job, <code>sleep</code>.</p>

<pre><code>$ sleep 100 &amp;
[1] 4542
</code></pre>

<p>Find it's job number. <strong>NOTE:</strong> the number 4542 is the process ID.</p>

<pre><code>$ jobs
[1]+  Running                 sleep 100 &amp;

$ kill %1
[1]+  Terminated              sleep 100
</code></pre>

<h3>Method #5 - fg</h3>

<p>You can bring a backgrounded job back to the foreground using the <code>fg</code> command.</p>

<h3>Example</h3>

<p>Fake job, <code>sleep</code>.</p>

<pre><code>$ sleep 100 &amp;
[1] 4650
</code></pre>

<p>Get the job's number.</p>

<pre><code>$ jobs
[1]+  Running                 sleep 100 &amp;
</code></pre>

<p>Bring job #1 back to the foreground, and then use <kbd>Ctrl</kbd>+<kbd>C</kbd>.</p>

<pre><code>$ fg 1
sleep 100
^C
$
</code></pre>
","104825"
"How can I instruct yum to install a specific version of package X?","352451","","<p>If there are two (or more) versions of a given RPM available in a YUM repository, how can I instruct <code>yum</code> to install the version I want?</p>

<p>Looking through the Koji build service I notice that there are several versions.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://i.stack.imgur.com/ohSCD.png""><img src=""https://i.stack.imgur.com/ohSCD.png"" alt=""ss #1""></a></p>
","<p>To see what particular versions are available to you via <code>yum</code> you can use the <code>--showduplicates</code> switch.</p>

<pre><code>$ yum --showduplicates list httpd | expand
Loaded plugins: fastestmirror, langpacks, refresh-packagekit
Loading mirror speeds from cached hostfile
 * fedora: mirror.steadfast.net
 * rpmfusion-free: csc.mcs.sdsmt.edu
 * rpmfusion-free-updates: csc.mcs.sdsmt.edu
 * rpmfusion-nonfree: csc.mcs.sdsmt.edu
 * rpmfusion-nonfree-updates: csc.mcs.sdsmt.edu
 * updates: mirror.steadfast.net
Available Packages
httpd.x86_64                        2.4.6-6.fc20                         fedora 
httpd.x86_64                        2.4.10-1.fc20                        updates
</code></pre>

<p>As far as installing a particular version? You can append the version info to the name of the package like so:</p>

<pre><code>$ sudo yum install &lt;package name&gt;-&lt;version info&gt;
</code></pre>

<p>For example in this case if I wanted to install the older version, 2.4.6-6 I'd do the following:</p>

<pre><code>$ sudo yum install httpd-2.4.6-6
</code></pre>

<p>You can also include the release info when specifying a package. In this case since I'm dealing with Fedora 20 (F20) the release info would be ""fc20"", and the architecture info too.</p>

<pre><code>$ sudo yum install httpd-2.4.6-6.fc20
$ sudo yum install httpd-2.4.6-6.fc20.x86_64
</code></pre>

<h3>repoquery</h3>

<p>If you're ever unsure that you're constructing the arguments right you can consult with <code>repoquery</code> too.</p>

<pre><code>$ sudo yum install yum-utils  # (to get `repoquery`)
$ repoquery --show-duplicates httpd-2.4*
httpd-0:2.4.6-6.fc20.x86_64
httpd-0:2.4.10-1.fc20.x86_64
</code></pre>

<h3>downloading &amp; installing</h3>

<p>You can also use one of the following options to download a particular RPM from the web, and then use <code>yum</code> to install it.</p>

<pre><code>$ yum --downloadonly &lt;package&gt;
-or-
$ yumdownloader &lt;package&gt;
</code></pre>

<p>And then install it like so:</p>

<pre><code>$ sudo yum localinstall &lt;path to rpm&gt;
</code></pre>

<p>What if I want to download everything that package X requires?</p>

<pre><code>$ yumdownloader --resolve &lt;package&gt;
</code></pre>

<h3>Example</h3>

<pre><code>$ yumdownloader --resolve vim-X11
Loaded plugins: langpacks, presto, refresh-packagekit
Adding en_US to language list
--&gt; Running transaction check
---&gt; Package vim-X11.x86_64 2:7.3.315-1.fc14 set to be reinstalled
--&gt; Finished Dependency Resolution
vim-X11-7.3.315-1.fc14.x86_64.rpm                              | 1.1 MB     00:01
</code></pre>

<p>Notice it's doing a dependency check, and then downloading the missing pieces. See my answer that covers it in more details here: <a href=""https://unix.stackexchange.com/questions/89222/how-to-download-a-file-from-repo-and-install-it-later-w-o-internet-connection/89250#89250"">How to download a file from repo, and install it later w/o internet connection?</a>.</p>

<h3>References</h3>

<ul>
<li><a href=""https://www.zulius.com/how-to/yum-install-specific-package-version/"" rel=""noreferrer"">Get yum to install a specific package version</a></li>
</ul>
","151690"
"How can get a list of all scheduled cron jobs on my machine?","350815","","<p>My sysadmin has set up a bunch of cron jobs on my machine. I'd like to know exactly what is scheduled for what time. How can I get that list? </p>
","<p>Depending on how your linux system is set up, you can look in:</p>

<ul>
<li><code>/var/spool/cron/*</code> (user crontabs)</li>
<li><code>/etc/crontab</code> (system-wide crontab)</li>
</ul>

<p>also, many distros have:</p>

<ul>
<li><p><code>/etc/cron.d/*</code>
These configurations have the same syntax as <code>/etc/crontab</code></p></li>
<li><p><code>/etc/cron.hourly</code>, <code>/etc/cron.daily</code>, <code>/etc/cron.weekly</code>, <code>/etc/cron.monthly</code></p></li>
</ul>

<p>These are simply directories that contain executables that are executed hourly, daily, weekly or monthly, per their directory name.</p>

<p>On top of that, you can have at jobs (check <code>/var/spool/at/*</code>), anacron (<code>/etc/anacrontab</code> and <code>/var/spool/anacron/*</code>) and probably others I'm forgetting.</p>
","7065"
"How do I set a user environment variable? (permanently, not session)","346742","","<p>This is irritating me. I seen several suggestions (all using different files and syntax) and none of them worked.</p>

<p>How do I set an environment variable for a specific user? I am on debian squeeze.
What is the exact syntax I should put in the file to make ABC = ""123""?</p>
","<p>You have to put the declaration in the initialization files of your shell:</p>

<ul>
<li><p>If you are using bash, ash, ksh or some other Bourne-style shell, you can add</p>

<pre><code>ABC=""123""; export ABC
</code></pre>

<p>in your <code>.profile</code> file (<code>${HOME}/.profile</code>). This is the default situation on most Unix installations, and in particular on Debian. </p>

<p>If your login shell is bash, you can use <code>.bash_profile</code> (<code>${HOME}/.bash_profile</code>) or <code>.bash_login</code> instead. </p>

<p><strong>Note:</strong> If either of these files exists and your login shell is bash, <code>.profile</code> is not read when you log in over ssh or on a text console, but it might still be read instead of <code>.bash_profile</code> if you log in from the GUI. Also, if there is no <code>.bash_profile</code>, then use <code>.bashrc</code>.</p></li>
<li><p>If you've set zsh as your login shell, use <code>~/.zprofile</code> instead of <code>~/.profile</code>.</p></li>
<li><p>If you are using tcsh, add</p>

<pre><code>setenv ABC ""123""
</code></pre>

<p>in <code>.login</code> file (<code>${HOME}/.login</code>)</p></li>
<li><p>if you are using another shell look at the shell manual how to define environment variables and which files are executed at the shell startup.</p></li>
</ul>
","21600"
"How to split the terminal into more than one ""view""?","337420","","<p>From <code>vi</code>, if you issue the command <code>:sp</code>, the screen splits into two ""views"", allowing you to edit more than one file from the same terminal. </p>

<p>Along those same lines, is there a way to have multiple shells open in the same terminal?</p>
","<p>You can do it in <code>screen</code> the terminal multiplexer.</p>

<ul>
<li>To split vertically: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>|</kbd>.</li>
<li>To split horizontally: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>S</kbd> (uppercase 's').</li>
<li>To unsplit: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>Q</kbd> (uppercase 'q').</li>
<li>To switch from one to the other:  <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>tab</kbd></li>
</ul>

<p>Note: After splitting, you need to go into the new region and start a new session via <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>c</kbd> before you can use that area.</p>

<p>EDIT, basic screen usage:</p>

<ul>
<li>New terminal: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>c</kbd>.</li>
<li>Next terminal: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>space</kbd>.</li>
<li>Previous terminal: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>backspace</kbd>.</li>
<li>N'th terminal <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>[n]</kbd>. <em>(works for n∈{0,1…9})</em></li>
<li>Switch between terminals using list: <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>""</kbd> <em>(useful when more than 10 terminals)</em></li>
<li>Send <kbd>ctrl</kbd><kbd>a</kbd> to the underlying terminal <kbd>ctrl</kbd><kbd>a</kbd> then <kbd>a</kbd>.</li>
</ul>
","7455"
"What DNS servers am I using?","334409","","<p>How can I check which DNS server am I using (in Linux)? I am using network manager and a wired connection to my university's LAN. (I am trying to find out why my domain doesn't get resolved)</p>
","<p>You should be able to get some reasonable information in:</p>

<pre><code>$ cat /etc/resolv.conf 
</code></pre>
","28944"
"Does curl have a --no-check-certificate option like wget?","320196","","<p>I am trying to make a curl request to one of our local development servers running a dev site with a self-signed SSL cert. I am using curl from the command line.</p>

<p>I saw some blog posts mentioning that you can add to the list of certificates or specify a specific (self signed) certificate as valid, but is there a catch-all way of saying ""don't verify"" the ssl cert - like the <code>--no-check-certificate</code> that wget has?</p>
","<p>Yes. From the manpage:</p>

<blockquote>
  <p>-k, --insecure
                (SSL)  This  option explicitly allows curl to perform ""insecure""
                SSL connections and transfers. All SSL connections are attempted
                to  be  made secure by using the CA certificate bundle installed
                by default. This makes  all  connections  considered  ""insecure""
                fail unless -k, --insecure is used.</p>
</blockquote>
","60752"
"Why does find -mtime +1 only return files older than 2 days?","319345","","<p>I'm struggling to wrap my mind around <em>why</em> the <code>find</code> interprets file modification times the way it does.  Specifically, I don't understand why the <code>-mtime +1</code> doesn't show files less than 48 hours old. </p>

<p>As an example test I created three test files with different modified dates: </p>

<pre><code>[root@foobox findtest]# ls -l
total 0
-rw-r--r-- 1 root root 0 Sep 25 08:44 foo1
-rw-r--r-- 1 root root 0 Sep 24 08:14 foo2
-rw-r--r-- 1 root root 0 Sep 23 08:14 foo3
</code></pre>

<p>I then ran find with the <code>-mtime +1</code> switch and got the following output:</p>

<pre><code>[root@foobox findtest]# find -mtime +1
./foo3
</code></pre>

<p>I then ran find with the <code>-mmin +1440</code> and got the following output: </p>

<pre><code>[root@foobox findtest]# find -mmin +1440
./foo3
./foo2
</code></pre>

<p>As per the man page for find, I understand that this is expected behavior:</p>

<pre><code> -mtime n
        File’s  data was last modified n*24 hours ago.  See the comments
        for -atime to understand how rounding affects the interpretation
        of file modification times.


-atime n
       File  was  last  accessed n*24 hours ago.  When find figures out
       how many 24-hour periods ago the file  was  last  accessed,  any
       fractional part is ignored, so to match -atime +1, a file has to
       have been accessed at least two days ago.
</code></pre>

<p>This still doesn't make sense to me though.  So if a file is 1 day, 23 hours, 59 minutes, and 59 seconds old, <code>find -mtime +1</code> ignores all that and just treats it like it's 1 day, 0 hours, 0 minutes, and 0 seconds old?  In which case, it's not technically older that 1 day and ignored? </p>

<p>Does... not... compute. </p>
","<p>Well, the simple answer is, I guess, that your find implementation is following the POSIX/SuS standard, which says it must behave this way. Quoting from <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/find.html"">SUSv4/IEEE Std 1003.1, 2013 Edition, ""find""</a>:</p>

<blockquote>
  <p>-mtime  n<br>
       The primary shall evaluate as true if the file modification time subtracted<br>
       from the initialization time, divided by 86400 (with any remainder discarded), is n.</p>
</blockquote>

<p>(Elsewhere in that document it explains that <code>n</code> can actually be <code>+n</code>, and the meaning of that as ""greater than"").</p>

<p>As to why the standard says it shall behave that way—well, I'd guess long in the past a programmer was lazy or not thinking about it, and just wrote the C code <code>(current_time - file_time) / 86400</code>. C integer arithmetic discards the remainder. Scripts started depending on that behavior, and thus it was standardized.</p>

<p>The spec'd behavior would also be portable to a hypothetical system that only stored a modification date (not time). I don't know if such a system has existed.</p>
","92351"
"How can I fix ""cannot find a valid baseurl for repo"" errors on CentOS?","318639","","<p>I finished installing CentOS 6, but when I tried running <code>yum update</code> I got:</p>

<pre><code>[root@centos6test ~]# yum update
Loaded plugins: fastestmirror, refresh-packagekit
Determining fastest mirrors
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=6&amp;arch=i386&amp;repo=os
error was 14: PYCURL ERROR 6 - """" Error: Cannot find a valid baseurl for repo: base
</code></pre>

<p>Why is that happening? How can I fix it?</p>
","<p>First you need to get connected, AFAIK CentOS 6 minimal set your network device to <code>ONBOOT=No</code>, just do a <code>dhclient</code> with admin privileges to your network interface and you should be up and running:</p>

<p><code>$ sudo dhclient</code></p>
","23159"
"How to add repository from shell in Debian?","317474","","<p>In Ubuntu one can add a repository via following command - </p>

<pre><code>sudo add-apt-repository ppa:yannubuntu/boot-repair
</code></pre>

<p>As Ubuntu is based on Debian code base, I was expecting that the same would work in Debian too, but it doesn't.</p>

<ul>
<li>What is the reason for this?</li>
<li>Is there some other shell command I can use to achieve the same?</li>
</ul>

<p><em>Note: I already know I can edit <code>/etc/apt-get/sources.list</code>, but I want to achieve this from shell. I also want to know why the same command won't work when the code base is the same.</em></p>
","<h2>Debian Jessie and later (2014-)</h2>

<p>As pointed out by @voltagex in the comments, it can now be found in the <code>software-properties-common</code> package:</p>

<pre><code>sudo apt-get install software-properties-common
</code></pre>

<hr>

<h2>Debian Wheezy and earlier:</h2>

<p>The program <code>add-apt-repository</code> <strong>is</strong> available in Debian.  It's in the <code>python-software-properties</code> package:</p>

<pre><code>sudo apt-get install python-software-properties
</code></pre>

<p>It was added to that package in version 0.75.  The current version in Debian Stable ('squeeze"") is 0.60, so it doesn't have it.  The version currently in Debian Testing (""wheezy"") is 0.82.7.1debian1, so it's available there.</p>
","45905"
"How can I test if a variable is empty or contains only spaces?","314896","","<p>The following bash syntax verifies if <code>param</code> isn't empty:</p>

<pre><code> [[ !  -z  $param  ]]
</code></pre>

<p>For example:</p>

<pre><code>param=""""
[[ !  -z  $param  ]] &amp;&amp; echo ""I am not zero""
</code></pre>

<p>No output and its fine.</p>

<p>But when <code>param</code> is empty except for one (or more) space characters, then the case is different:</p>

<pre><code>param="" "" # one space
[[ !  -z  $param  ]] &amp;&amp; echo ""I am not zero""
</code></pre>

<p>""I am not zero"" is output.</p>

<p>How can I change the test to consider variables that contain only space characters as empty?</p>
","<p>First, note that the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Bash-Conditional-Expressions""><code>-z</code> test</a> is explicitly for:</p>

<blockquote>
  <p>the length of string is zero</p>
</blockquote>

<p>That is, a string containing only spaces should <strong>not</strong> be true under <code>-z</code>, because it has a non-zero length.</p>

<p>What you want is to remove the spaces from the variable using the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"">pattern replacement parameter expansion</a>:</p>

<pre><code>[[ -z ""${param// }"" ]]
</code></pre>

<p>This expands the <code>param</code> variable and replaces all matches of the pattern <code></code> (a single space) with nothing, so a string that has only spaces in it will be expanded to an empty string.</p>

<hr>

<p>The <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"">nitty-gritty of how that works</a> is that <code>${var/pattern/string}</code> replaces the first longest match of <code>pattern</code> with <code>string</code>. When <code>pattern</code> starts with <code>/</code> (as above) then it replaces <em>all</em> the matches. Because the replacement is empty, we can omit the final <code>/</code> and the <code>string</code> value:</p>

<blockquote>
  <p>${parameter/pattern/string}</p>
  
  <p>The <em>pattern</em> is expanded to produce a pattern just as in filename expansion. <em>Parameter</em> is expanded and the longest match of <em>pattern</em> against its value is replaced with <em>string</em>. If <em>pattern</em> begins with ‘/’, all matches of <em>pattern</em> are replaced with <em>string</em>. Normally only the first match is replaced. ... If <em>string</em> is null, matches of <em>pattern</em> are deleted and the / following <em>pattern</em> may be omitted.</p>
</blockquote>

<p>After all that, we end up with <code>${param// }</code> to delete all spaces.</p>

<p><sub>Note that though present in <code>ksh</code> (where it originated), <code>zsh</code> and <code>bash</code>, that syntax is not POSIX and should not be used in <code>sh</code> scripts.</sub></p>
","146945"
"What is the ""eval"" command in bash?","310758","","<p>What can you do with the <code>eval</code> command? Why is it useful? Is it some kind of a built-in function in bash? There is no <code>man</code> page for it..</p>
","<p><code>eval</code> is part of POSIX. Its an interface which can be a shell built-in.</p>

<p>Its described in the ""POSIX Programmer's Manual"": <a href=""http://www.unix.com/man-page/posix/1posix/eval/"">http://www.unix.com/man-page/posix/1posix/eval/</a></p>

<pre><code>eval - construct command by concatenating arguments
</code></pre>

<p>It will take an argument and construct a command of it, which will be executed by the shell. This is the example of the manpage:</p>

<pre><code>1) foo=10 x=foo
2) y='$'$x
3) echo $y
4) $foo
5) eval y='$'$x
6) echo $y
7) 10
</code></pre>

<ol>
<li>In the first line you define <code>$foo</code> with the value <code>'10'</code> and <code>$x</code> with the value <code>'foo'</code>. </li>
<li>Now define <code>$y</code>, which consists of the string <code>'$foo'</code>. The dollar sign must be escaped
with <code>'$'</code>. </li>
<li>To check the result, <code>echo $y</code>.</li>
<li>The result will be the string <code>'$foo'</code></li>
<li>Now we repeat the assignment with <code>eval</code>. It will first evaluate <code>$x</code> to the string <code>'foo'</code>. Now we have the statement <code>y=$foo</code> which will get evaluated to <code>y=10</code>.</li>
<li>The result of <code>echo $y</code> is now the value <code>'10'</code>.</li>
</ol>

<p>This is a common function in many languages, e.g. Perl and JavaScript.
Have a look at perldoc eval for more examples: <a href=""http://perldoc.perl.org/functions/eval.html"">http://perldoc.perl.org/functions/eval.html</a></p>
","23117"
"How to change hostname on CentOS 6.5?","309112","","<p>I can't seem to change the hostname on my CentOS 6.5 host.
I am following the instructions I found here: <a href=""http://www.rackspace.com/knowledge_center/article/centos-hostname-change"">http://www.rackspace.com/knowledge_center/article/centos-hostname-change</a></p>

<p>I set my <code>/etc/hosts</code> like so ...</p>

<pre><code>    [root@mig-dev-006 ~]# cat /etc/hosts
    127.0.0.1   localhost localhost.localdomain 
    192.168.32.128  ost-dev-00.domain.com ost-dev-00
    192.168.32.129  ost-dev-01.domain.com ost-dev-01
</code></pre>

<p>... then I make my <code>/etc/sysconfig/network</code> file like so ...</p>

<pre><code>    [root@mig-dev-006 ~]# cat /etc/sysconfig/network
    NETWORKING=yes
    HOSTNAME=ost-dev-00.domain.com
    NTPSERVERARGS=iburst
</code></pre>

<p>... then I run <code>hostname</code> like so ...</p>

<pre><code>    [root@mig-dev-006 ~]# hostname ost-dev-00.domain.com
</code></pre>

<p>... and then I run bash and all seems well ...</p>

<pre><code>    [root@mig-dev-006 ~]# bash
</code></pre>

<p>... but when I restart my network the old hostname comes back:</p>

<pre><code>    [root@ost-dev-00 ~]# /etc/init.d/network restart
    Shutting down interface eth0:  Device state: 3 (disconnected)
                                                               [  OK  ]
    Shutting down loopback interface:                          [  OK  ]
    Bringing up loopback interface:                            [  OK  ]
    Bringing up interface eth0:  Active connection state: activating
    Active connection path: /org/freedesktop/NetworkManager/ActiveConnection/6
    state: activated
    Connection activated
                                                               [  OK  ]
    [root@ost-dev-00 ~]# bash
    [root@mig-dev-006 ~]# 
</code></pre>

<p>... I can't figure it out.  What am I doing wrong here.</p>
","<p>to change the hostname permanently, you need to change it in two places:</p>

<pre><code>vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=newHostName
</code></pre>

<p>and:
a good idea if you have any applications that need to resolve the IP of the hostname)</p>

<pre><code>vi /etc/hosts 
127.0.0.1 newHostName
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
</code></pre>

<p>and then</p>

<pre><code> rebooting the system
</code></pre>
","145022"
"How to write startup script for systemd","300211","","<p>I have 2 graphics cards on my laptop. One is IGP and another discrete. </p>

<p>I've written a shell script to to turn off the discrete graphics card.</p>

<p>How can I convert it to systemd script to run it at start-up?</p>
","<p>There are mainly two approaches to do that:</p>

<ul>
<li>If you have to run a script, you don't convert it but rather run the script via a <code>systemd</code> service. </li>
</ul>

<p>Therefore you need two files: the script and the <code>.service</code> file (unit configuration file).<br>
Make sure your script is executable and the first line (the <em>shebang</em>) is <code>#!/bin/sh</code>. Then create the <code>.service</code> file in <code>/etc/systemd/system</code> (a plain text file, let's call it <code>vgaoff.service</code>).<br>
For example:</p>

<ol>
<li>the script: <code>/usr/bin/vgaoff</code></li>
<li>the unit file: <code>/etc/systemd/system/vgaoff.service</code></li>
</ol>

<p>Now, edit the unit file. Its content depends on how your script works:</p>

<p>If <code>vgaoff</code> just powers off the gpu, e.g.:</p>

<pre><code>exec blah-blah pwrOFF etc 
</code></pre>

<p>then the content of <code>vgaoff.service</code> should be:</p>

<pre><code>[Unit]
Description=Power-off gpu

[Service]
Type=oneshot
ExecStart=/usr/bin/vgaoff

[Install]
WantedBy=multi-user.target
</code></pre>

<p>If <code>vgaoff</code> is used to power off the GPU and also to power it back on, e.g.:</p>

<pre><code>start() {
  exec blah-blah pwrOFF etc
}

stop() {
  exec blah-blah pwrON etc
}

case $1 in
  start|stop) ""$1"" ;;
esac
</code></pre>

<p>then the content of <code>vgaoff.service</code> should be:</p>

<pre><code>[Unit]
Description=Power-off gpu

[Service]
Type=oneshot
ExecStart=/usr/bin/vgaoff start
ExecStop=/usr/bin/vgaoff stop
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
</code></pre>

<ul>
<li>For the most trivial cases, you can do without the script and execute a certain command directly:</li>
</ul>

<p>To power off:</p>

<pre><code>[Unit]
Description=Power-off gpu

[Service]
Type=oneshot
ExecStart=/bin/sh -c ""echo OFF &gt; /whatever/vga_pwr_gadget/switch""

[Install]
WantedBy=multi-user.target
</code></pre>

<p>To power off &amp; on:</p>

<pre><code>[Unit]
Description=Power-off gpu

[Service]
Type=oneshot
ExecStart=/bin/sh -c ""echo OFF &gt; /whatever/vga_pwr_gadget/switch""
ExecStop=/bin/sh -c ""echo ON &gt; /whatever/vga_pwr_gadget/switch""
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
</code></pre>

<hr>

<p>Once you're done with the files, enable the service:</p>

<pre><code>systemctl enable vgaoff.service
</code></pre>

<p>It should start automatically after rebooting the machine.</p>

<hr>

<p>For more details see <a href=""http://www.freedesktop.org/software/systemd/man/systemd.service.html""><code>systemd.service</code></a> man page.</p>
","47715"
"How can I get a count of files in a directory using the command line?","295205","","<p>I have a directory with a large number of files. I don't see a <code>ls</code> switch to provide the count. Is there some command line magic to get a count of files?</p>
","<p>Using a broad definition of ""file""</p>

<pre><code>ls | wc -l
</code></pre>

<p>(note that it doesn't count hidden files and assumes that file names don't contain newline characters).</p>

<p>To include hidden files (except <code>.</code> and <code>..</code>) and avoid problems with newline characters, the canonical way is:</p>

<pre><code>find . ! -name . -prune -print | grep -c /
</code></pre>

<p>Or recursively:</p>

<pre><code>find .//. ! -name . -print | grep -c //
</code></pre>
","1126"
"How to set default file permissions for all folders/files in a directory?","295011","","<p>I want to set a folder such that anything created within it (directories, files) inherit default permissions and group.  </p>

<p>Lets call the group ""media"".  And also, the folders/files created within the directory should have g+rw automatically.</p>
","<p>I found it: <a href=""http://www.linuxquestions.org/questions/linux-desktop-74/applying-default-permissions-for-newly-created-files-within-a-specific-folder-605129/"">Applying default permissions</a></p>

<p>From the article:</p>

<pre><code>chmod g+s &lt;directory&gt;  //set gid 
setfacl -d -m g::rwx /&lt;directory&gt;  //set group to rwx default 
setfacl -d -m o::rx /&lt;directory&gt;   //set other
</code></pre>

<p>Next we can verify: </p>

<pre><code>getfacl /&lt;directory&gt;
</code></pre>

<p>Output: </p>

<pre><code># file: ../&lt;directory&gt;/
# owner: &lt;user&gt;
# group: media
# flags: -s-
user::rwx
group::rwx
other::r-x
default:user::rwx
default:group::rwx
default:other::r-x
</code></pre>
","1315"
"How to move all files and folders via mv command","292745","","<p>How can I move all files and folders from one directory to another via mv command?</p>
","<p>Try with this:</p>

<pre><code>mv /path/sourcefolder/* /path/destinationfolder/
</code></pre>
","50488"
"Create file in folder: permission denied","291453","","<p>I'm a newbie in the GNU/Linux-Ubuntu world. I have a problem copying files to a directory on Ubuntu 12.04. I create a directory in the home directory so that the path where I want to copy to is: </p>

<pre><code>/home/sixven/camp_sms/inputs
</code></pre>

<p>But when ini run the following command in the terminal to create a sample file as follows:</p>

<pre><code>francisco-vergara@Francisco-Vergara:/home/sixven/camp_sms/inputs$ touch test_file.txt
touch: can not make `touch' on «test_file.txt»: permission denied
</code></pre>

<p>I can not copy files directly in that directory. How can I assign permissions with the <code>chown</code> &amp; <code>chmod</code> commands to copy the files?</p>

<p>I do not know which user and group to use.</p>
","<p>First of all you have to know that the default permission of directories in Ubuntu is 644 which means you can't create a file in a directory you are not the owner.</p>

<p>you are trying as <code>user:francisco-vergara</code> to create a file in a directory <code>/home/sixven/camp_sms/inputs</code> which is owned by <code>user:sixven</code>.</p>

<p><strong>So how to solve this:</strong></p>

<ol>
<li><p>You can either change the permission of the directory and enable others to create files inside.</p>

<pre><code>sudo chmod -R 777 /home/sixven/camp_sms/inputs
</code></pre>

<p>This command will change the permission of the directory recursively and enable all other users to create/modify and delete files and directories inside.</p></li>
<li><p>You can change the owner ship of this directory and make <code>user:francisco-vergara</code> as the owner</p>

<pre><code>sudo chown -R francisco-vergara:francisco-vergara /home/sixven/camp_sms/inputs
</code></pre>

<p>But like this the <code>user:sixven</code> can't write in this folder again and thus you  may moving in a circular infinite loop.</p></li>
</ol>

<p>So i advise you to use Option 1.</p>

<p>Or if this directory will be accessed by both users you can do the following trick:</p>

<p>change ownership of the directory to <code>user:francisco-vergara</code> and keep the group owner <code>group:sixven</code>.</p>

<pre><code>sudo chown -R francisco-vergara /home/sixven/camp_sms/inputs
</code></pre>

<p>Like that both users can still use the directory.</p>

<p>But as I said you before It's easiest and more efficient to <strong>use option 1.</strong></p>
","119836"
"How can I count the number of lines of a file with common tools?","290885","","<p>I am redirecting <code>grep</code> results to a file, and then using <code>cat</code> to show its contents on the screen. I want to know how many lines of results I have in my results file and then add it to some counter.</p>

<p>What will be the best way? Any relevant flag to <code>grep</code> or <code>cat</code>?</p>
","<p>If you have already collected the <code>grep</code> output in a file, you could output a numbered list with:</p>

<pre><code>cat -n myfile
</code></pre>

<p>If you only want the number of lines, simply do:</p>

<pre><code>wc -l myfile
</code></pre>

<p>There is absolutely no reason to do:</p>

<pre><code>cat myfile | wc -l
</code></pre>

<p>...as this needlessly does I/O (the <code>cat</code>) that <code>wc</code> has to repeat.  Besides, you have two processes where one suffices.</p>

<p>If you want to <code>grep</code> to your terminal and print a count of the matches at the end, you can do:</p>

<pre><code>grep whatever myfile | tee /dev/tty | wc -l
</code></pre>
","25348"
"ifconfig command not found","289874","","<p>I've just installed CentOS7 as a virtual machine on my mac (osx10.9.3 + virtualbox) .Running <code>ifconfig</code> returns command not found. Also running <code>sudo /sbin/ifconfig</code> returns commmand not found. I am root. The output of
<code>echo $PATH</code> is as below. </p>

<pre><code>/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/robbert/.local/bin:/home/robbert/bin
</code></pre>

<p>Is my path normal? If not, how can I change it?</p>

<p>Also, I don't have an internet connection on virtual machine yet, maybe that's a factor.</p>
","<p><strong>TL/DR:</strong> <code>ifconfig</code> is now <code>ip a</code>.</p>

<p>Your path looks OK, but does not include <code>/sbin</code>, which may be intended.</p>

<p>You were probably looking for the command <code>/sbin/ifconfig</code>.</p>

<p>If this file does not exist (try <code>ls /sbin/ifconfig</code>), the command may just be not installed.</p>

<p>It is part of the package <code>net-tools</code>,
 which is not installed by default, because it's <strong>deprecated</strong>
and <strong>superseded</strong> by the command
 <strong><code>ip</code></strong> from the package <strong><code>iproute2</code></strong>.</p>

<p>The function of <code>ifconfig</code> without options is replaced by <code>ip</code> specifying the <em>object</em> <code>address</code>.</p>

<pre><code>ifconfig
</code></pre>

<p>is equivalent to</p>

<pre><code>ip addr show
</code></pre>

<p>and, because the <em>object</em> argument can be abbreviated and <em>command</em> defaults to <code>show</code>, also to</p>

<pre><code>ip a
</code></pre>

<p>The output format is somewhat different:</p>

<pre><code>$ ifconfig
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:10553 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10553 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:9258474 (9.2 MB)  TX bytes:9258474 (9.2 MB)
[ ... ]
</code></pre>

<p>and</p>

<pre><code>$ ip address
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
[ ... ]
</code></pre>

<p>Note the output is more terse:
It does not show counts of packets handled in normal or other ways.</p>

<p>For that, add the option <code>-s</code> (<code>-stats</code>, <code>-statistics</code>):</p>

<pre><code>$ ip -s addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
    RX: bytes  packets  errors  dropped overrun mcast
    74423      703      0       0       0       0
    TX: bytes  packets  errors  dropped carrier collsns
    74423      703      0       0       0       0
</code></pre>

<p>But what you actually want to see may be this:</p>

<pre><code>$ ip -stats -color -human addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
    RX: bytes  packets  errors  dropped overrun mcast
    74.3k      700      0       0       0       0
    TX: bytes  packets  errors  dropped carrier collsns
    74.3k      700      0       0       0       0
</code></pre>

<p>It shows counts with suffixes like <code>26.1M</code> or <code>79.3k</code> and colors some relevant terms and addresses.</p>

<p><strong>Oh, you feel the command is too long?
Easy!</strong> This is the same:</p>

<pre><code>ip -s -c -h a
</code></pre>
","145449"
"How can I delete all lines in a file using vi?","288153","","<p>How can I delete all lines in a file using vi?</p>

<p>At moment I do that using something like this to remove all lines in a file:</p>

<pre><code>echo &gt; test.txt
</code></pre>

<p>How can I delete all lines using <code>vi</code>?</p>

<p><strong>Note:</strong>
Using <code>dd</code> is not a good option. There can be many lines.</p>
","<p>In <code>vi</code> do</p>

<pre><code>:1,$d
</code></pre>

<p>to delete all lines. </p>

<p>The <code>:</code> introduces a command (and moves the cursor to the bottom).<br>
The <code>1,$</code> is an indication of which lines the following command (<code>d</code>) should work on. In this case the range from line one to the last line (indicated by <code>$</code>, so you don't need to know the number of lines in the document).<br>
The final <code>d</code> stands for delete the indicated lines.</p>

<p>There is a shorter form (<code>:%d</code>) but I find myself never using it. The <code>:1,$d</code> can be more easily ""adapted"" to e.g. <code>:4,$-2d</code> leaving only the first 3 and last 2 lines, deleting the rest.</p>
","161825"
"How can I restart the SSH daemon on Ubuntu?","287122","","<p>When I type in <code>service sshd restart</code> I get a <code>sshd: unrecognized service</code> error.</p>

<p>I do have, in <code>/etc/ssh/</code> a file <code>sshd_config</code> that I use to set config. I can also putty into the Ubuntu box (it is remote).</p>

<p>When I type in <code>/etc/init.d/sshd restart</code> I get <code>No such file or directory</code></p>

<p>Under <code>/usr/sbin/</code> there is an sshd file, but it is binary.</p>

<p>Is something wrong with my sshd? What do I do to fix this? To be clear, I want to be able to type <code>service sshd restart</code> (like all the online tutourials say) to be able to, well, restart my sshd. So that my port changes take effect.</p>
","<p>Ubuntu calls the service <code>ssh</code>, not <code>sshd</code>.</p>

<pre><code>service ssh restart
</code></pre>

<p>The service is also controlled by upstart, and not sysvinit. So you'll find it at <code>/etc/init/ssh.conf</code> instead of <code>/etc/init.d/ssh</code>.</p>
","127887"
"Empty the contents of a file","286999","","<p>I am aware of three methods to delete all entries from a file. </p>

<p>They are </p>

<ul>
<li><code>&gt;filename</code></li>
<li><code>touch filename</code></li>
<li><code>filename &lt; /dev/null</code></li>
</ul>

<p>Of these three I abuse <code>&gt;filename</code> the most as that requires the least number of keystrokes. </p>

<p>However, I would like to know which is the most efficient of the three (if there are any  more efficient methods) with respect to large log files and small files.</p>

<p>Also, how does the three codes operate and delete the contents?</p>
","<p>Actually, the second form <code>touch filename</code> doesn't delete anything from the file - it only creates an empty file if one did not exist, or updates the last-modified date of an existing file.</p>

<p>And the third <code>filename &lt; /dev/null</code> tries to run filename with <code>/dev/null</code> as input.</p>

<p><code>cp /dev/null filename</code> works.</p>

<p>As for efficient, the most efficient would be <code>truncate -s 0 filename</code>; see here: <a href=""http://linux.die.net/man/1/truncate"">http://linux.die.net/man/1/truncate</a>.</p>

<p>Otherwise, <code>cp /dev/null filename</code> or <code>&gt; filename</code> are both fine.  They both open and then close the file, using the truncate-on-open setting.  <code>cp</code> also opens <code>/dev/null</code>, so that makes it marginally slower.</p>

<p>On the other hand, <code>truncate</code> would likely be slower than <code>&gt; filename</code> when run from a script since running the truncate command requires the system to open the executable, load it, and the run it.</p>
","88810"
"Linux: set date through command line","285732","","<p>How to change the system date in Linux ?</p>

<p>I want to change:</p>

<ul>
<li>Only Year</li>
<li>Only Month</li>
<li>Only Date</li>
<li>Any combination of above three</li>
</ul>
","<p>Use <a href=""http://linux.die.net/man/1/date""><code>date -s</code></a>:</p>

<pre><code>date -s '2014-12-25 12:34:56'
</code></pre>

<p>Run that as root or under <code>sudo</code>. Changing only one of the year/month/day is more of a challenge and will involve repeating bits of the current date. There are also GUI date tools built in to the major desktop environments, usually accessed through the clock.</p>

<p>To change only part of the time, you can use command substitution in the date string:</p>

<pre><code>date -s ""2014-12-25 $(date +%H:%M:%S)""
</code></pre>

<p>will change the date, but keep the time. See <a href=""http://linux.die.net/man/1/date""><code>man date</code></a> for formatting details to construct other combinations: the individual components are <code>%Y</code>, <code>%m</code>, <code>%d</code>, <code>%H</code>, <code>%M</code>, and <code>%S</code>.</p>
","151548"
"How to create a FTP user with specific /dir/ access only on a Centos / linux installation","284815","","<p>So I'm on a VPS - CentOS  Linux installation. I have vsFTPd on the server.
I currently have SFTP access to the server via my root user, but am now trying to create a new user with FTP access to a specific directory only on the server, I've done the following:</p>

<pre><code>1. mkdir /var/www/mydomain.com
2. mkdir /var/www/mydomain.com/html
3. useradd &lt;-username&gt;
4. passwd &lt;-username&gt;
5. chown –R &lt;-username&gt; /var/www/mydomain.com
5. groupadd &lt;-groupname&gt;
6. gpasswd -a &lt;-username&gt; &lt;-groupname&gt;
7. chgrp -R &lt;-groupname&gt; /var/www/mydomain.com
8. chmod -R g+rw /var/www/mydomain.com
</code></pre>

<p><strong>What I'm struggling to do is to create the user to ONLY have access to</strong> <code>/var/www/mydomain.com</code> - I observed that the user correctly logs into the right folder, however the user can then browse ""back"" to other directories. <strong>I want the user to stick in the specific folder and not being able to ""browse"" back</strong>.</p>

<p>Any ideas?</p>

<p>I've found different articles on chrooting, but simply haven't figured it out to use it in the steps included above.</p>
","<p>It's quite simple.</p>

<p>You have to add the following option on the vsftpd.conf file</p>

<pre><code>chroot_local_user=YES
</code></pre>

<p>The documentation inside the configuration file is self-explanatory:</p>

<pre><code># You may specify an explicit list of local users to chroot() to their home
# directory. If chroot_local_user is YES, then this list becomes a list of
# users to NOT chroot().
</code></pre>

<p>This means, that the user will just have access on the folder you configured as HOME of the user.Below, i have an example of a user passwd entry:</p>

<pre><code>upload_ftp:x:1001:1001::/var/www/sites/:/bin/bash
</code></pre>

<p>Set the home directory of the user with the following command</p>

<pre><code>usermod -d /var/www/my.domain.example/ exampleuser
</code></pre>

<p><strong>Note:</strong> In my example, this user is also a valid user for some scheduled tasks inside Linux. If you don't have this need, please change the shell of the user to <code>/sbin/nologin</code> instead of <code>bash</code>.</p>
","83225"
"How to unfreeze after accidentally pressing Ctrl-S in a terminal?","284641","","<p>It's a situation that has happened quite often to me: after I press (with a different intention) <kbd>Ctrl-S</kbd> in a terminal, the interaction (input or output) with it is frozen. It's probably a kind of ""scroll lock"" or whatever.</p>

<p>How do I unfreeze the terminal after this?</p>

<p>(This time, I have been working with <code>apt-shell</code> inside a <code>bash</code> inside <code>urxvt</code>--not sure which of them is responsible for the special handling of <kbd>Ctrl-S</kbd>: I was searching the history of commands backwards with <kbd>C-r</kbd>, as usual for readline, but then I wanted to go ""back"" forwards through the history with the usual--at least in Emacs--<kbd>C-s</kbd> (<a href=""https://unix.stackexchange.com/questions/541/best-way-to-search-my-shells-history/2120#2120"">1</a>, <a href=""https://unix.stackexchange.com/questions/4079/put-history-command-onto-command-line-without-executing-it/4086#4086"">2</a>, <a href=""https://unix.stackexchange.com/questions/7061/how-to-search-the-whole-of-bash-history-without-needing-to-go-forwards-and-backwa/7064#7064"">3</a>), but that caused the terminal to freeze. Well, scrolling/paging to view past things still works in the terminal, but no interaction with the processes run there.)</p>
","<p><kbd>Ctrl</kbd>-<kbd>Q</kbd></p>

<p>To disable this altogether, stick <code>stty -ixon</code> in a startup script. To allow any key to get things flowing again, use <code>stty ixany</code>.</p>

<p>ps: It's neither the terminal nor the shell that does this, but the OS's terminal driver.</p>
","12108"
"Why does man print ""gimme gimme gimme"" at 00:30?","284584","","<p>We've noticed that some of our automatic tests fail when they run at 00:30 but work fine the rest of the day. They fail with the message ""gimme gimme gimme"" in stderr, which wasn't expected. Why are we getting this output?</p>
","<p>This is an easter egg in <code>man</code>. When you run <code>man</code> without specifying the page or with <code>-w</code>, it outputs ""gimme gimme gimme"" to stderr, but only at 00:30:</p>

<pre><code># date +%T -s ""00:30:00""
00:30:00
# man -w
gimme gimme gimme
/usr/local/share/man:/usr/share/man:/usr/man
</code></pre>

<p>The exit code is always 0.</p>

<p>The correct output should always be:</p>

<pre><code># man -w
/usr/local/share/man:/usr/share/man:/usr/man
# echo $?
0
# man
What manual page do you want?
# echo $?
1
</code></pre>

<p>The string ""gimme gimme gimme"" can be found in RHEL, OpenSUSE, Fedora, Debian and probably more, so it's not really distro specific. You can <code>grep</code> your <code>man</code> binary to verify.</p>

<p><a href=""http://git.savannah.nongnu.org/cgit/man-db.git/tree/src/man.c#n4122"" rel=""noreferrer"">This code is responsible for the output</a>, added by <a href=""http://git.savannah.nongnu.org/cgit/man-db.git/commit/src/man.c?id=002a6339b1fe8f83f4808022a17e1aa379756d99"" rel=""noreferrer"">this commit</a>:</p>

<pre><code>src/man.c-1167- if (first_arg == argc) {
src/man.c-1168-   /* 
http://twitter.com/#!/marnanel/status/132280557190119424 */
src/man.c-1169-   time_t now = time (NULL);
src/man.c-1170-   struct tm *localnow = localtime (&amp;now);
src/man.c-1171-   if (localnow &amp;&amp;
src/man.c-1172-       localnow-&gt;tm_hour == 0 &amp;&amp; localnow-&gt;tm_min == 30)
src/man.c:1173:     fprintf (stderr, ""gimme gimme gimme\n"");
</code></pre>

<p>I have contacted RHEL support about this issue.</p>

<p>The string comes from well known <a href=""https://www.youtube.com/watch?v=XEjLoHdbVeE&amp;t=1m10s"" rel=""noreferrer"">ABBA song Gimme! Gimme! Gimme! (A Man After Midnight)</a>.</p>

<hr>

<p>The developer of the man-db, Colin Watson, decided that there was enough fun and the story won't get forgotten and <a href=""https://git.savannah.gnu.org/cgit/man-db.git/commit/?id=b225d9e76fbb0a6a4539c0992fba88c83f0bd37e"" rel=""noreferrer"">removed the easter egg completely</a>.</p>

<p>Thank you Colin!</p>
","405784"
"How to find application's path from command line?","279453","","<p>For example, I have <code>git</code> installed on my system.
But I don't remember where I installed it, so which command is fit to find this out?</p>
","<p>If it is in your path, then you can run either <code>type git</code> or <code>which git</code>.  The <code>which</code> command has had problems getting the proper path (confusion between environment and dot files).  For <code>type</code>, you can get just the path with the <code>-p</code> argument.</p>

<p>If it is not in your path, then it's best to look for it with <code>locate -b git</code>  It will find anything named 'git'.  It'll be a long list, so might be good to qualify it with <code>locate -b git | fgrep -w bin</code>.</p>
","28560"
"Recovering from 'grub rescue>' crash","277975","","<p><em>Originally posted to AskUbuntu.com ...</em></p>

<blockquote>
  <p><a href=""http://meta.askubuntu.com/questions/5611/should-we-allow-questions-that-are-about-releases-which-are-considered-end-of-li"">AskUbuntu has adopted a policy of closing</a> questions about EOL
  (End Of Life) versions. There's a vocal contingent to remove them as well.
  To prevent possible loss of this popular question (342335 views to date),
  am placing a revised version here.
  --- docsalvager</p>
</blockquote>

<p><em>The ""classic"" system...</em></p>

<ul>
<li>Puppy Linux 5.2.8 (Lucid) based on Ubuntu 10.04 (Lucid Lynx)</li>
<li><a href=""http://en.wikipedia.org/wiki/GNU_GRUB"">GRUB 2</a> boot loader</li>
</ul>

<p><a href=""http://en.wikipedia.org/wiki/GNU_GRUB"">GRUB 2</a> puts a number of <code>*.mod</code> files (kernel modules) in <code>/boot/grub</code>. Deleting these files (thinking they were misplaced sound files) resulted in failure on reboot and the prompt <code>grub rescue&gt;</code>.</p>

<p><em>How to recover in this situation?</em></p>
","<p><strong>Recovering from a grub rescue crash ...</strong></p>

<ul>
<li><code>grub rescue&gt;</code> does not support <code>cd</code>, <code>cp</code> or any other filesystem commands except its own variation of <code>ls</code> which is really a kind of <code>find</code> command.</li>
<li><p>So first, had to find the partition with the <code>/boot</code> directory containing the <code>vmlinuz</code> and other boot image files...</p>

<pre><code>grub rescue&gt;  ls  
(hd0,4) (hd0,3) (hd0,2) (hd0,1)  

grub rescue&gt;  ls (hd0,4)/boot
... some kind of 'not found' message

grub rescue&gt;  ls (hd0,3)/boot
... some kind of 'not found' message

grub rescue&gt;  ls (hd0,2)/boot
... grub ... initrd.img-2.6.32-33-generic ... vmlinuz-2.6.32-33-generic 
</code></pre>

<ul>
<li><code>ls</code> without arguments returns the four partitions on this system.</li>
<li><code>ls (hd0,4)/boot</code> does not find a <code>/boot</code> directory on partition <code>(hd0,4)</code>.</li>
<li><code>ls (hd0,3)/boot</code> does not find a <code>/boot</code> directory on partition <code>(hd0,3)</code>.</li>
<li><code>ls (hd0,2)/boot</code> finds a <code>/boot</code> directory on partition <code>(hd0,2)</code> and it contains a <code>vmlinuz</code> and other boot image files we want.</li>
</ul></li>
<li><p>To manually boot from the <code>grub rescue&gt;</code> prompt ...</p>

<pre><code>grub rescue&gt;  set root=(hd0,2)/boot  
grub rescue&gt;  insmod linux  
grub rescue&gt;  linux (hd0,2)/boot/vmlinuz-2.6.32-33-generic  
grub rescue&gt;  initrd (hd0,2)/boot/initrd.img-2.6.32-33-generic  
grub rescue&gt;  boot  
</code></pre>

<ul>
<li>Set <code>root</code> to use the <code>/boot</code> directory on partition <code>(hd0,2)</code>.</li>
<li>Load kernel module <code>linux</code>.</li>
<li>Set that module to use the kernel image <code>vmlinuz-2.6.32-33-generic</code>.</li>
<li>Set initrd(init RAM disk) to use the image <code>initrd.img-2.6.32-33-generic</code>.</li>
<li>Boot Linux.</li>
</ul></li>
<li><p>This boots to a <a href=""http://www.busybox.net/screenshot.html"" rel=""nofollow noreferrer"">BusyBox</a> commandline prompt which has all the basic filesystem commands (<em>and then some!</em>).</p></li>
<li><p>Then could move the <code>*.mod</code> files back to the <code>/boot/grub</code> directory ...</p>

<pre><code>busybox&gt;  cd /boot  
busybox&gt;  mv mod/* grub
busybox&gt;  reboot
</code></pre></li>
<li><p><em>Successful Reboot!</em></p></li>
</ul>

<p><em>See also ...</em></p>

<ul>
<li><a href=""https://unix.stackexchange.com/q/119965/27437"">stuck at grub rescue on boot, no bios, no live cd, ls returns hd0</a></li>
<li><a href=""https://askubuntu.com/questions/197833/recovering-from-grub-rescue-crash"">recovering from grub rescue crash (AskUbuntu)</a></li>
<li><a href=""https://askubuntu.com/questions/142300/how-to-fix-error-unknown-filesystem-grub-rescue"">how to fix error unknown filesystem grub rescue (AskUbuntu)</a></li>
<li><a href=""http://members.iinet.net/~herman546/p20/GRUB2%20CLI%20Mode%20Commands.html#cli_ls"" rel=""nofollow noreferrer"">A nice Grub 2 ls command reference page</a></li>
<li><a href=""http://www.busybox.net/screenshot.html"" rel=""nofollow noreferrer"">BusyBox (website)</a></li>
<li><a href=""http://www.busybox.net/live_bbox/live_bbox.html"" rel=""nofollow noreferrer"">BusyBox online emulator</a></li>
</ul>
","148042"
"Use scp to transfer a file from local directory X to remote directory Y","276072","","<p>I took a look around at other questions here and at various ""scp usage"" tutorials on Internet, but I can't sort out what's wrong.</p>

<p>I'm using Linux Mint and I'm trying to figure out how <strong>scp</strong> works.</p>

<p>I've a file <strong>file.ext</strong> (on my computer) in directory <strong>/home/name/dir/</strong></p>

<p>I connect to a remote machine using <strong>ssh</strong>, like:</p>

<pre><code>ssh -p 2222 username@domain
</code></pre>

<p>it asks me the password and the shell displays:</p>

<pre><code>username@domain ~ $
</code></pre>

<p>now, If I issue the command (before I ran ssh I was in the local directory  <strong>/home/name/dir</strong> ):</p>

<pre><code>scp -r -P 2222 file.ext username@domain 
</code></pre>

<p>output is:</p>

<pre><code>cp: cannot stat ‘file.ext’: No such file or directory
</code></pre>

<p>Same result if instead of file.ext I write the complete path</p>

<pre><code>scp -r -P 2222 /home/name/dir/file.ext username@domain 
</code></pre>

<p>Also, the server admin told me that I shall upload the file to my remote home directory (instead of root), like:</p>

<pre><code>scp -r -P 2222 file.ext username@domain:~/
</code></pre>

<p>but when I do it and press ""Enter"" nothing happens, as If the shell was waiting for further input.</p>

<p>Summary of my problems: </p>

<ul>
<li>cp: no such file or directory</li>
<li>shell ""stuck"" on ~/</li>
</ul>

<p>Any suggestions?</p>
","<p>You need to run the <code>scp</code> command from the local machine, not on the remote. You don't need the <code>ssh</code> at all:</p>

<pre><code>dragonmnl@local $ scp -P 2222 file.ext username@domain:~/ 
</code></pre>

<p>You also don't need the <code>-r</code>:</p>

<pre><code> -r      Recursively copy entire directories.  
</code></pre>

<p>If you are already logged into the remote machine and want to copy from your local, you need to make sure that your local machine is accessible via the internet and has <code>ssh</code> set up. I don't think this is what you are after but if it is, just run this from the remote:</p>

<pre><code>username@domain $ scp dragonmnl@local:/path/to/file.ext ~/
</code></pre>
","115564"
"What are the pros and cons of Vim and Emacs?","273729","","<p>How would you compare these editors? What are the pros and cons of each?</p>

<p>[<strong>note</strong>] This is not meant to be answered by those who ""hate one and love another"" or those who haven't used both.</p>
","<p>I use both, although if I had to choose one, I know which one I would pick. Still, I'll try to make an objective comparison on a few issues.</p>

<ul>
<li><p><strong>Available everywhere?</strong> If you're a professional system administrator who works with Unix systems, or a power user on embedded devices (routers, smartphones with Busybox, …), you need to know vi (not Vim), because it's available on all Unix systems and most Unix-like systems, whether desktop, server or embedded. For an ordinary user, this argument is irrelevant: Emacs is easily available for every desktop/server OS, and since it supports remote editing, it's enough to have it on your desktop machine anyway.</p></li>
<li><p><strong>Bloated?</strong> Emacs once stood humorously for “Eight Megabytes And Constantly Swapping”. Right now, on my machine, Google Chrome needs about as much RAM per tab as Emacs does for 100 open files, and I won't even mention Firefox. In the 21st century, Emacs bloat is just a myth.</p>

<p><em>Feature</em> bloat isn't a problem either. If you don't use it, you don't have to know it's there. Emacs features keep out of the way when you don't use them and the documentation is very well organized.</p></li>
<li><p><strong>Startup time</strong>: Vi(m) proponents complain about Emacs's startup time. Yes, Emacs is slow to start up, but this is not a big deal: you start Emacs once per session, then connect to the running process with <code>emacsclient</code>. So Emacs's slow startup is mostly a myth.</p>

<p>There's one exception, which is when you log in to a remote machine and want to edit a file there. Starting a remote Emacs is (usually) slower than starting a remote Vim. In some situations you can keep an Emacs running inside Screen. You can also edit remote files from within Emacs, but it does break the flow if you're in an ssh session in a terminal. (Since XEmacs 21 or GNU Emacs 23, you can open an Emacs window from a running X instance inside a terminal.)</p>

<p>Turning the tables, I have observed Vim taking noticeably longer to load than Emacs (<code>vim -u /dev/null</code> vs. <code>emacs -q</code>). Admittedly this was on a weird platform (Cygwin).</p></li>
<li><p><strong>Initial learning curve:</strong> This varies from person to person. <a href=""https://unix.stackexchange.com/questions/986/vim-vs-emacs-and-no-this-is-not-a-flame-war/988#988"">Michael Mrozek's graph</a> made me chuckle. Seriously, I agree that Vim's learning curve starts steep, steeper than any other editor, although this can be lessened by using gvim.</p>

<p>Since I've dispelled a couple of Emacs myths, let me dispel a vi myth: a modal editor is not hard or painful to use. It takes a little habit, but after a while it feels very natural. If I was to redesign vi(m), I'd definitely keep the modes.</p></li>
<li><p><strong>Asymptotic learning curve:</strong> Both Vim and Emacs have a lot of features, and you will keep discovering new ones after years of use.</p></li>
<li><p><strong>Productivity</strong>: This is an extremely hard topic. Proponents of vi(m) argue that you can do pretty much everything without leaving the home row, and that makes you more efficient when you need it most. Proponents of Emacs retort that Emacs has a lot of commands that are not frequently used, so don't warrant a key binding, but are damn convenient when you need them (<a href=""http://xkcd.com/378/"" rel=""noreferrer"">obligatory xkcd reference</a>).<br>
<sub>My personal opinion is that Emacs ultimately wins unless you have a typing disability (and even then you can configure Emacs to require only key sequences and not combinations like <kbd>Ctrl</kbd>+letter). Home row keys are nice, but they often aren't that much of a win because you have to switch modes. I don't think there's anything Vim can do <em>significantly</em> more efficiently than Emacs, whereas the converse is true.</sub></p></li>
<li><p><strong>Customizability</strong>: Both editors are programmable, and there is an extensive body of available packages for both. However, Vim is an editor with a macro language; Emacs is an editor written in Lisp with some ad-hoc primitives. Emacs wins spectacularly when you try to do something that the authors just didn't think of. This doesn't happen every day, but it does accumulate over the years.</p></li>
<li><p><strong>More than an editor</strong>: Vim is an editor. Emacs is not just an editor: it's also an IDE, a file manager, a terminal emulator, a web browser, a mail client, a news client, ... Whether that's a good thing or a bad thing is a matter for debate. But you can use Emacs as a mere editor (see “feature bloat” above).</p></li>
<li><p><strong>As an IDE</strong>: Both Vim and Emacs have support for a lot of programming languages and other text formats. Beyond the basics such as syntactic coloring and automatic indentation, both have advanced IDE features such as code and documentation cross-reference lookups, assisted insertions and refactoring, integrated version control, and the ability to initiate a compilation and jump to the first error.</p>

<p>One domain where Emacs is plain better than Vim is interaction with <em>asynchronous</em> subprocesses. That's when you start a long compilation and want to do something else inside the same editor instance while the compiler is churning. Or when you want to interact with a <a href=""http://en.wikipedia.org/wiki/Read-eval-print_loop"" rel=""noreferrer"">Read-eval-print loop</a> — Emacs really shines at this, Vim only has clumsy hacks to offer. Nevertheless, a new fork of vim, <a href=""https://github.com/Neovim/Neovim"" rel=""noreferrer"">Neovim</a> has proved to have fixed this and implemented other various bug-fixes not implemented in stock vim.</p></li>
</ul>
","1010"
"Limit FTP access only to the /var/www with vsftpd","273249","","<p>I am running vsftpd as ftp server on my linux (rasbian), I log in to the machine as a root user.</p>

<p>I would like to be still locked to using only /var/www, how can I configure vsftpd conf to accomplish it?</p>
","<h2>Method 1: Changing the user's home directory</h2>

<p>Make sure the following line exists</p>

<pre><code>chroot_local_user=YES
</code></pre>

<p>Set user HOME Directory to <code>/var/www/</code> , if you want to change for existing user then you can use:</p>

<pre><code>usermod --home /var/www/ username
</code></pre>

<p>then set required permission on <code>/var/www/</code></p>

<h2>Method 2: Use <code>user_sub_token</code></h2>

<p>If you don't want to change user's Home directory then you can use:</p>

<pre><code>chroot_local_user=YES
local_root=/ftphome/$USER
user_sub_token=$USER
</code></pre>

<h3>About <code>user_sub_token</code>:</h3>

<blockquote>
  <p>Automatically generate a home directory for each virtual user, based on a template.
  For example, if the home directory of the real user specified via guest_username is
  /ftphome/$USER, and user_sub_token is set to $USER, then when virtual user test 
  logs in, he will end up (usually chroot()'ed) in the directory /ftphome/test.
  This option also takes affect if local_root contains user_sub_token.</p>
</blockquote>

<p>Create directory and set up permissions:</p>

<pre><code>mkdir -p /ftphome/{test,user1,user2}
chmod 770 -R /ftphome
chown -R ftp. /ftphome
usermod -G ftp test
</code></pre>

<p>Once restart <code>vsftpd</code> and test your setup.</p>

<p>Sample success output:</p>

<pre><code>[root@mail tmp]# ftp localhost
Connected to mail.linuxian.local.
220 (vsFTPd 2.0.5)
530 Please login with USER and PASS.
530 Please login with USER and PASS.
KERBEROS_V4 rejected as an authentication type
Name (localhost:root): test
331 Please specify the password.
Password:
230 Login successful.
Remote system type is UNIX.
Using binary mode to transfer files.
ftp&gt; mput vhosts
mput vhosts? 
227 Entering Passive Mode (127,0,0,1,146,41)
150 Ok to send data.
226 File receive OK.
24 bytes sent in 3.3e-05 seconds (7.1e+02 Kbytes/s)
ftp&gt; ls -rlt
227 Entering Passive Mode (127,0,0,1,97,90)
150 Here comes the directory listing.
-rw-r--r--    1 787      787            24 Oct 11 19:57 vhosts
226 Directory send OK.
ftp&gt; 221 Goodbye.
</code></pre>
","94673"
"Adding two numbers using expr","270293","","<p>I'm learning Shell scripting for a diploma in IT I'm currently doing. I'm trying to write a small script that adds two numbers as shown as in one of the tutorials we were given.</p>

<pre><code>echo ""Enter two numbers""
read num1 num2
sum = 'expr $num1 + $num2'
echo ""The sum is = $sum""
</code></pre>

<p>However when I give it the execution permission and run the script, it gives me this error.</p>

<pre><code>sum: =. No such file or directory.
sum: expr $num1 + $num2: No such file or directory
</code></pre>

<p><img src=""https://i.stack.imgur.com/AOK8W.png"" alt=""enter image description here""></p>

<p>I tried running this on both Ubuntu and Fedora but same error occurs. Can anyone please tell me what I'm missing here?</p>
","<p>First you have to get rid of the spaces for the assignment, e.g</p>

<pre><code>sum='expr $num1 + $num2'
</code></pre>

<p>then you have to change <code>'</code> to a <code>`</code> or even better to <code>$()</code>:</p>

<pre><code>sum=$(expr ""$num1"" + ""$num2"")
</code></pre>

<p>instead of using <code>expr</code> you can also do the calculation directly in your shell:</p>

<pre><code>sum=$((num1 + num2))
</code></pre>
","44996"
"Delete the last character of a string using string manipulation in shell script","265908","","<p>I would like to delete the last character of a string, I tried this little script :</p>

<pre><code>#! /bin/sh 

t=""lkj""
t=${t:-2}
echo $t
</code></pre>

<p>but it prints ""lkj"", what I am doing wrong?</p>
","<p>In a POSIX shell, the syntax <code>${t:-2}</code> means something different - it expands to the value of <code>t</code> if <code>t</code> is set and non null, and otherwise to the value <code>2</code>. To trim a single character by parameter expansion, the syntax you probably want is <code>${t%?}</code></p>

<p>Note that in <code>ksh93</code>, <code>bash</code> or <code>zsh</code>, <code>${t:(-2)}</code> or <code>${t: -2}</code> (note  the space) <em>are</em> legal as a substring expansion but are probably not what you want, since they return the substring starting at a position 2 characters in from the end (i.e. it removes the <em>first</em> character <code>i</code> of the string <code>ijk</code>).</p>

<p>See the Shell Parameter Expansion section of the Bash Reference Manual for more info:</p>

<ul>
<li><a href=""http://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion"">Bash Reference Manual – Shell Parameter Expansion</a></li>
</ul>
","144308"
"Execute shell commands in Python","264278","","<p>I'm currently studying <a href=""https://en.wikipedia.org/wiki/Penetration_test"">penetration testing</a> and Python programming. I just want to know how I would go about executing a Linux command in Python. The commands I want to execute are:</p>

<pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward
iptables -t nat -A PREROUTING -p tcp --destination-port 80 -j REDIRECT --to-port 8080
</code></pre>

<p>If I just use <code>print</code> in Python and run it in the terminal will it do the same as executing it as if you was typing it yourself and pressing <kbd>Enter</kbd>?</p>
","<p>You can use <code>os.system()</code>, like this:</p>

<pre><code>import os
os.system('ls')
</code></pre>

<p>Or in your case:</p>

<pre><code>os.system('echo 1 &gt; /proc/sys/net/ipv4/ip_forward')
os.system('iptables -t nat -A PREROUTING -p tcp --destination-port 80 -j REDIRECT --to-port 8080')
</code></pre>

<p>Better yet, you can use subprocess's call, it is safer:</p>

<pre><code>from subprocess import call
call('ls')
</code></pre>

<p><a href=""https://docs.python.org/2/library/subprocess.html"">https://docs.python.org/2/library/subprocess.html</a></p>
","238185"
"How do I exit or cancel a bad bash command?","264269","","<p>I expect to get some flak for this, but I can't find the answer anywhere. It seems like it should be so obvious. Sometimes, when I type a bad command in a bash terminal, the cursor just jumps down to the next line without any error or anything. I can't tell what I did wrong. It's like I'm stuck in the program. Reenactment:</p>

<pre><code>$ tidy
</code></pre>

<p>Me: ""Oops! That's not what I meant to type...""</p>

<pre><code>:q
</code></pre>

<p>Me: ""That didn't work...""</p>

<pre><code>:exit
:quit
exit
quit
/exit
/quit
-exit
-quit
-wtf???
</code></pre>

<p>I know I screwed up but how do I get back to the prompt without closing the terminal?</p>
","<p>You can always try the obvious things like <code>^C</code>, <code>^D</code> (eof), Escape etc., but if all fails I usually end up suspending the command with <code>^Z</code> (Control-Z) which puts me back into the shell. </p>

<p>I then do a <code>ps</code> command and note the PID (process id) of the command and then issue a <code>kill thePID</code> (<code>kill -9 thePID</code> if the former didn't work) command to terminate the application. </p>

<p>Note that this is not a <em>tidy</em> (no pun intended) way to terminate the application/command and you run the risk of perhaps no saving some data etc.</p>

<p>An example (I'd have used <code>tidy</code> but I don't have it installed):</p>

<pre><code>$ gnuplot

    G N U P L O T
    Version 4.2 patchlevel 6 
     ....
    Send bug reports and suggestions to &lt;http://sourceforge.net/projects/gnuplot&gt;

Terminal type set to 'wxt'
gnuplot&gt; 
gnuplot&gt;               #####  typed ^Z here
[1]+  Stopped                 gnuplot
$ ps
  PID TTY          TIME CMD
 1681 pts/1    00:00:00 tcsh
 1690 pts/1    00:00:00 bash
 1708 pts/1    00:00:00 gnuplot
 1709 pts/1    00:00:00 ps


$ kill 1708            ###### didn't kill the command as ps shows

$ ps
  PID TTY          TIME CMD
 1681 pts/1    00:00:00 tcsh
 1690 pts/1    00:00:00 bash
 1708 pts/1    00:00:00 gnuplot
 1710 pts/1    00:00:00 ps
$ kill -9 1708           ### -9 did the trick
$ 
[1]+  Killed                  gnuplot

$ ps
  PID TTY          TIME CMD
 1681 pts/1    00:00:00 tcsh
 1690 pts/1    00:00:00 bash
 1711 pts/1    00:00:00 ps
</code></pre>
","45647"
"How to reload udev rules without reboot?","262918","","<p>How should one reload udev rules, so that newly created one can function?</p>

<p>I'm running Arch Linux, and I don't have a <code>udevstart</code> command here.</p>

<p>Also checked <code>/etc/rc.d</code>, no udev service there.</p>
","<pre><code># udevadm control --reload-rules &amp;&amp; udevadm trigger
</code></pre>
","39371"
"How can I close a terminal without killing the command running in it?","262279","","<p>Sometimes I want to start a process and forget about it. If I start it from the command line, like this:</p>

<pre><code>redshift
</code></pre>

<p>I can't close the terminal, or it will kill the process. Can I run a command in such a way that I can close the terminal without killing the process?</p>
","<p>One of the following 2 should work:</p>

<pre><code>$ nohup redshift &amp;
</code></pre>

<p>or</p>

<pre><code>$ redshift &amp;
$ disown
</code></pre>

<p>See the following for a bit more information on how this works: </p>

<ul>
<li><p><code>man nohup</code></p></li>
<li><p><code>help disown</code></p></li>
<li><p><a href=""https://unix.stackexchange.com/questions/3886/difference-between-nohup-disown-and"">Difference between nohup, disown and &amp;</a> (be sure to read the comments too)</p></li>
</ul>
","4006"
"How to overwrite target files with mv?","260975","","<p>I have a ton of files and dirs in a subdirectory I want to move to the parent directory. There are already some files and dirs in the target directory which need to be overwritten. Files that are only present in the target should be left untouched. Can I force <code>mv</code>to do that? It (<code>mv * ..</code>) complains</p>

<pre><code>mv: cannot move `xyz' to `../xyz': Directory not empty
</code></pre>

<p>What am I missing?</p>
","<p>You will have to copy them to the destination and then delete the source, using the commands <code>cp -r * ..</code> followed by <code>rm -rf *</code>.</p>

<p>I don't think you can ""merge"" directories using <code>mv</code>.</p>
","9902"
"How can I get the size of a file in a bash script?","258630","","<p>How can I get the size of a file in a bash script? </p>

<p>How do I assign this to a bash variable so I can use it later?</p>
","<p>Your best bet if on a GNU system:</p>

<pre><code>stat --printf=""%s"" file.any
</code></pre>

<p>NOTE: see <a href=""https://unix.stackexchange.com/a/185039/9071"">@chbrown</a> answer below how to use stat in terminal on Mac OS X 
or</p>

<pre><code>#!/bin/bash
FILENAME=/home/heiko/dummy/packages.txt
FILESIZE=$(stat -c%s ""$FILENAME"")
echo ""Size of $FILENAME = $FILESIZE bytes.""
</code></pre>
","16644"
"What characters do I need to escape when using sed in a sh script?","257248","","<p>Take the following script:</p>

<pre><code>#!/bin/sh
sed 's/(127\.0\.1\.1)\s/\1/' [some file]
</code></pre>

<p>If I try to run this in <code>sh</code> (<code>dash</code> here), it'll fail because of the parentheses, which need to be escaped. But I <em>don't</em> need to escape the backslashes themselves (between the octets, or in the <code>\s</code> or <code>\1</code>). What's the rule here? What about when I need to use <code>{...}</code> or <code>[...]</code>? Is there a list of what I do and don't need to escape?</p>
","<p>There are two levels of interpretation here: the shell, and sed.</p>

<p>In the shell, everything between single quotes is interpreted literally, except for single quotes themselves. You can effectively have a single quotes between single quotes by writing <code>'\''</code> (close single quote, one literal single quote, open single quote).</p>

<p>Sed uses <a href=""http://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended"" rel=""noreferrer"">basic regular expressions</a>. In a BRE, the characters <code>$.*[\]^</code> need to be quoted by preceding them by a backslash, except inside character sets (<code>[…]</code>). Letters, digits and <code>(){}+?|</code> must not be quoted (you can get away with quoting some of these in some implementations). The sequences <code>\(</code>, <code>\)</code>, <code>\n</code>, and in some implementations <code>\{</code>, <code>\}</code>, <code>\+</code>, <code>\?</code>, <code>\|</code> and other backslash+alphanumerics have special meanings. You can get away with not quoting <code>$^]</code> in some positions in some implementations.</p>

<p>Furthermore, you need a backslash before <code>/</code> if it is to appear in the regex outside of bracket expressions. You can choose an alternate character as the delimiter by writing e.g. <code>s~/dir~/replacement~</code> or <code>\~/dir~p</code>; you'll need a backslash before the delimiter if you want to include it in the BRE. If you choose a character that has a special meaning in a BRE and you want to include it literally, you'll need three backslashes; I do not recommend this.</p>

<p>In a nutshell, for <code>sed 's/…/…/'</code>:</p>

<ul>
<li>Write the regex between single quotes.</li>
<li>Use <code>'\''</code> to end up with a single quote in the regex.</li>
<li>Put a backslash before <code>$.*/[\]^</code> and only those characters (but not inside bracket expressions).</li>
<li>Inside a bracket expression, for <code>-</code> to be treated literally, make sure it is first or last (<code>[abc-]</code> or <code>[-abc]</code>, not <code>[a-bc]</code>)</li>
<li>Inside a bracket expression, for <code>^</code> to be treated literally, make sure it is <em>not</em> first (use <code>[abc^]</code>, not <code>[^abc]</code>)</li>
<li>To include <code>]</code> in the list of characters matched by a bracket expression, make it the first character  (or first after <code>^</code> for a negated set): <code>[]abc]</code> or <code>[^]abc]</code> (not <code>[abc]]</code> nor <code>[abc\]]</code>).</li>
</ul>

<p>In the replacement text:</p>

<ul>
<li><code>&amp;</code> and <code>\</code> need to be quoted, as do the delimiter (usually <code>/</code>) and newlines.</li>
<li><code>\</code> followed by a digit has a special meaning. <code>\</code> followed by a letter has a special meaning (special characters) in some implementations, and <code>\</code> followed by some other character means <code>\c</code> or <code>c</code> depending on the implementation.</li>
<li>With single quotes around the argument (<code>sed 's/…/…/'</code>), use <code>'\''</code> to put a single quote in the replacement text.</li>
</ul>

<p>If the regex or replacement text comes from a shell variable, remember that</p>

<ul>
<li>the regex is a BRE, not a literal string;</li>
<li>in the regex, a newline needs to be expressed as <code>\n</code> (which will never match unless you have other <code>sed</code> code adding newline characters to the pattern space). But note that it won't work inside bracket expressions with some <code>sed</code> implementations;</li>
<li>in the replacement text, <code>&amp;</code>, <code>\</code> and newlines need to be quoted;</li>
<li>the delimiter needs to be quoted (but not inside bracket expressions).</li>
<li>Use double quotes for interpolation: <code>sed -e ""s/$BRE/$REPL/""</code></li>
</ul>
","33005"
"How to suspend and bring a background process to foreground","254219","","<p>I have a process originally running in the foreground. I suspended by <kbd>Ctrl</kbd>+<kbd>Z</kbd>, and then resume its running in the background by <code>bg &lt;jobid&gt;</code>.</p>

<p>I wonder how to suspend a process running in the background?</p>

<p>How can I bring a background process to foreground?</p>

<p><strong>Edit:</strong></p>

<p>The process outputs to stderr, so how shall I issue the command <code>fg &lt;jobid&gt;</code> while the process is outputting to the terminal?</p>
","<p>As Tim said, type <code>fg</code> to bring the last process back to foreground.</p>

<p>If you have more than one process running in the background, do this:</p>

<pre><code>$ jobs
[1]   Stopped                 vim
[2]-  Stopped                 bash
[3]+  Stopped                 vim 23
</code></pre>

<p><code>fg %3</code> to bring the <code>vim 23</code> process back to foreground.</p>

<p>To suspend the process running in the background, use: </p>

<pre><code>kill -STOP %job_id
</code></pre>

<p>The SIGSTOP signal stops (pauses) a process
in essentially the same way <kbd>Ctrl</kbd>+<kbd>Z</kbd> does.</p>

<p>example: <code>kill -STOP %3</code>. </p>

<p>sources: <a href=""http://linuxg.net/how-to-kill-processes-in-linux-and-unix/"" rel=""noreferrer"">How to send signals to processes in Linux and Unix</a>
and <a href=""http://linuxg.net/how-to-manage-background-and-foreground-processes"" rel=""noreferrer"">How to manage background and foreground jobs</a>.</p>


","45029"
"How to get execution time of a script effectively?","252437","","<p>I would like to display the completion time of a script. </p>

<p>What I currently do is -</p>

<pre><code>#!/bin/bash
date  ## echo the date at start
# the script contents
date  ## echo the date at end
</code></pre>

<p>This just show's the time of start and end of the script. Would it be possible to display a fine grained output like processor time/ io time , etc?</p>
","<p>Just use <code>time</code> when you call the script:</p>

<pre><code>time yourscript.sh
</code></pre>
","52315"
"How can I get my external IP address in a shell script?","251705","","<p>I need to find my external IP address from a shell script. At the moment I use this function: </p>

<pre><code>myip () { 
    lwp-request -o text checkip.dyndns.org | awk '{ print $NF }'
}
</code></pre>

<p>But it depends on <code>perl-libwww</code>, <code>perl-html-format</code>, <code>perl-html-tree</code> installed. What other ways can I get my external IP?</p>
","<p>I'd recommend getting it directly from a DNS server.</p>

<p>Most of the answers here all go over HTTP to a remote server. Some of them require parsing of the output, or rely on the User-Agent header to make the server respond in plain text. They also change quite frequently (go down, change their name, put up ads, might change output format etc.).</p>

<ol>
<li>The DNS response protocol is standardised (the format will stay compatible).</li>
<li>Historically DNS services (<a href=""https://en.wikipedia.org/wiki/OpenDNS"">OpenDNS</a>, <a href=""https://en.wikipedia.org/wiki/Google_Public_DNS"">Google Public DNS</a>, ..) tend to survive much longer and are more stable, more scalable, and generally more looked-after than whatever new hip whatismyip.com HTTP service is hot today.</li>
<li>(for those geeks that care about micro-optimisation), this method should be inherently faster (be it only by a few micro seconds).</li>
</ol>

<p>Using <a href=""http://linux.die.net/man/1/dig""><code>dig</code></a> with <a href=""https://en.wikipedia.org/wiki/OpenDNS"">OpenDNS</a> as resolver:</p>

<pre><code>dig +short myip.opendns.com @resolver1.opendns.com
</code></pre>

<p>Perhaps alias it in your <code>bashrc</code> so it's easy to remember</p>

<pre><code>alias wanip='dig +short myip.opendns.com @resolver1.opendns.com'
</code></pre>

<p>Responds with a plain ip address:</p>

<pre><code>$ wanip
80.100.192.168
</code></pre>
","81699"
"Delete files older than X days +","246372","","<p>I have found the command to delete files older than 5 days in a folder</p>

<pre><code>find /path/to/files* -mtime +5 -exec rm {} \;
</code></pre>

<p>But how do I also do this for subdirectories in that folder?</p>
","<p>Be careful with special file names (spaces, quotes) when piping to rm.</p>

<p>There is a safe alternative - the <strong>-delete</strong> option:</p>

<pre><code>find /path/to/directory/ -mindepth 1 -mtime +5 -delete
</code></pre>

<p>That's it, no separate rm call and you don't need to worry about file names.</p>

<p>Replace <code>-delete</code> with <code>-depth -print</code> to test this command before you run it (<code>-delete</code> implies <code>-depth</code>).</p>
","205539"
"How can I get the current working directory?","245292","","<p>I want to have a script that takes the current working directory to a variable. The section that needs the directory is like this <code>dir = pwd</code>. It just prints <code>pwd</code> how do I get the current working directory into a variable?</p>
","<p>There's no need to do that, it's already <em>in</em> a variable:</p>

<pre><code>$ echo $PWD
/home/terdon
</code></pre>

<p>The <code>PWD</code> variable is <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html"" rel=""noreferrer"">defined by POSIX</a> and will work on all POSIX-compliant shells:</p>

<blockquote>
  <p>PWD </p>
  
  <p>Set by the shell and by the cd utility. In the shell the value
  shall be initialized from the environment as follows. If a value for
  PWD is passed to the shell in the environment when it is executed, the
  value is an absolute pathname of the current working directory that is
  no longer than {PATH_MAX} bytes including the terminating null byte,
  and the value does not contain any components that are dot or dot-dot,
  then the shell shall set PWD to the value from the environment.
  Otherwise, if a value for PWD is passed to the shell in the
  environment when it is executed, the value is an absolute pathname of
  the current working directory, and the value does not contain any
  components that are dot or dot-dot, then it is unspecified whether the
  shell sets PWD to the value from the environment or sets PWD to the
  pathname that would be output by pwd -P. Otherwise, the sh utility
  sets PWD to the pathname that would be output by pwd -P. In cases
  where PWD is set to the value from the environment, the value can
  contain components that refer to files of type symbolic link. In cases
  where PWD is set to the pathname that would be output by pwd -P, if
  there is insufficient permission on the current working directory, or
  on any parent of that directory, to determine what that pathname would
  be, the value of PWD is unspecified. Assignments to this variable may
  be ignored. If an application sets or unsets the value of PWD, the
  behaviors of the cd and pwd utilities are unspecified.</p>
</blockquote>

<hr>

<p>For the more general answer, the way to save the output of a command in a variable is to enclose the command in <code>$()</code> or <code>` `</code> (backticks):</p>

<pre><code>var=$(command)
</code></pre>

<p>or</p>

<pre><code>var=`command`
</code></pre>

<p>Of the two, the <code>$()</code> is preferred since it is easier to build complex commands like <code>command0 $(command1 $(command2 $(command3)))</code>.</p>
","188191"
"How to find out from the logs what caused system shutdown?","244584","","<p>E.g. I'm seeing this in <code>/var/log/messages</code>:</p>

<pre><code>Mar 01 23:12:34 hostname shutdown: shutting down for system halt
</code></pre>

<p>Is there a way to find out what caused the shutdown?  E.g. was it run from console, or someone hit power button, etc.?</p>
","<p>Only root privileged programs can gracefully shutdown a system. So when a system shuts down in a normal way, it is either a user with root privileges or an acpi script. In both cases you can find out by checking the logs. An acpi shutdown can be caused by power button press, overheating or low battery (laptop). I forgot the third reason, UPS software when power supply fails, which will send an alert anyway.</p>

<p>Recently I had a system that started repeatedly to power off ungracefully, turned out that it was overheating and the mobo was configured to just power off early. The system didn't have a chance to save logs, but fortunately monitoring the system's temperature showed it was starting to increase just before powering off.</p>

<p>So if it is a normal shutdown it will be logged, if it is an intrusion... good luck, and if it is a cold shutdown your best chance to know is to control and monitor its environment.</p>
","10522"
"What is the exact difference between a 'terminal', a 'shell', a 'tty' and a 'console'?","244525","","<p>I think these terms almost refer to the same thing, when used loosely:</p>

<ul>
<li>terminal</li>
<li>shell</li>
<li>tty</li>
<li>console</li>
</ul>

<p>What exactly does each of these terms refer to?</p>
","<p>A terminal is at the end of an electric wire, a shell is the home of a turtle, tty is a strange abbreviation and a console is a kind of cabinet.</p>

<p>Well, etymologically speaking, anyway.</p>

<p>In unix terminology, the short answer is that</p>

<ul>
<li>terminal = tty = text input/output environment</li>
<li>console = physical terminal</li>
<li>shell = command line interpreter</li>
</ul>

<hr>

<p>Console, terminal and tty are closely related. Originally, they meant a piece of equipment through which you could interact with a computer: in the early days of unix, that meant a <a href=""http://en.wikipedia.org/wiki/Teleprinter"" rel=""noreferrer"">teleprinter</a>-style device resembling a typewriter, sometimes called a teletypewriter, or “tty” in shorthand. The name “terminal” came from the electronic point of view, and the name “console” from the furniture point of view. Very early in unix history, electronic keyboards and displays became the norm for terminals.</p>

<p>In unix terminology, a <strong>tty</strong> is a particular kind of <a href=""http://en.wikipedia.org/wiki/Device_file"" rel=""noreferrer"">device file</a> which implements a number of additional commands (<a href=""http://en.wikipedia.org/wiki/Ioctl#Terminals"" rel=""noreferrer"">ioctls</a>) beyond read and write. In its most common meaning, <strong>terminal</strong> is synonymous with tty. Some ttys are provided by the kernel on behalf of a hardware device, for example with the input coming from the keyboard and the output going to a text mode screen, or with the input and output transmitted over a serial line. Other ttys, sometimes called <strong>pseudo-ttys</strong>, are provided (through a thin kernel layer) by programs called <a href=""http://en.wikipedia.org/wiki/Terminal_emulator"" rel=""noreferrer""><strong>terminal emulators</strong></a>, such as <a href=""http://en.wikipedia.org/wiki/Xterm"" rel=""noreferrer"">Xterm</a> (running in the <a href=""http://en.wikipedia.org/wiki/X_Window_System"" rel=""noreferrer"">X Window System</a>), <a href=""http://en.wikipedia.org/wiki/Gnu_screen"" rel=""noreferrer"">Screen</a> (which provides a layer of isolation between a program and another terminal), <a href=""http://en.wikipedia.org/wiki/Secure_shell"" rel=""noreferrer"">Ssh</a> (which connects a terminal on one machine with programs on another machine), <a href=""http://en.wikipedia.org/wiki/Expect"" rel=""noreferrer"">Expect</a> (for scripting terminal interactions), etc.</p>

<p>The word terminal can also have a more traditional meaning of a device through which one interacts with a computer, typically with a keyboard and display. For example an X terminal is a kind of <a href=""http://en.wikipedia.org/wiki/Thin_client"" rel=""noreferrer"">thin client</a>, a special-purpose computer whose only purpose is to drive a keyboard, display, mouse and occasionally other human interaction peripherals, with the actual applications running on another, more powerful computer.</p>

<p>A <strong>console</strong> is generally a terminal in the physical sense that is by some definition the primary terminal directly connected to a machine. The console appears to the operating system as a (kernel-implemented) tty. On some systems, such as Linux and FreeBSD, the console appears as several ttys (special key combinations switch between these ttys); just to confuse matters, the name given to each particular tty can be “console”, ”virtual console”, ”virtual terminal”, and other variations.</p>

<p>See also <a href=""https://askubuntu.com/q/14284/1059"">Why is a Virtual Terminal “virtual”, and what/why/where is the “real” Terminal?</a>.</p>

<hr>

<p>A <a href=""http://en.wikipedia.org/wiki/Shell_%28computing%29"" rel=""noreferrer""><strong>shell</strong></a> is the primary interface that users see when they log in, whose primary purpose is to start other programs. (I don't know whether the original metaphor is that the shell is the home environment for the user, or that the shell is what other programs are running in.)</p>

<p>In unix circles, <strong>shell</strong> has specialized to mean a <a href=""http://en.wikipedia.org/wiki/Shell_%28computing%29#Text_.28CLI.29_shells"" rel=""noreferrer"">command-line shell</a>, centered around entering the name of the application one wants to start, followed by the names of files or other objects that the application should act on, and pressing the Enter key. Other types of environments don't use the word “shell”; for example, window systems involve “<a href=""http://en.wikipedia.org/wiki/Window_manager"" rel=""noreferrer"">window managers</a>” and “<a href=""http://en.wikipedia.org/wiki/Desktop_environment"" rel=""noreferrer"">desktop environments</a>”, not a “shell”.</p>

<p>There are many different unix shells.
Popular shells for interactive use include <a href=""http://en.wikipedia.org/wiki/Bash_(Unix_shell)"" rel=""noreferrer"">Bash</a> (the default on most Linux installations), <a href=""http://en.wikipedia.org/wiki/Zsh"" rel=""noreferrer"">zsh</a> (which emphasizes power and customizability) and <a href=""http://en.wikipedia.org/wiki/Friendly_interactive_shell"" rel=""noreferrer"">fish</a> (which emphasizes simplicity).</p>

<p>Command-line shells include flow control constructs to combine commands. In addition to typing commands at an interactive prompt, users can write scripts. The most common shells have a common syntax based on the <a href=""http://en.wikipedia.org/wiki/Bourne_shell"" rel=""noreferrer"">Bourne_shell</a>. When discussing “<strong>shell programming</strong>”, the shell is almost always implied to be a Bourne-style shell. Some shells that are often used for scripting but lack advanced interactive features include <a href=""http://en.wikipedia.org/wiki/Korn_shell"" rel=""noreferrer"">the Korn shell (ksh)</a> and many <a href=""http://en.wikipedia.org/wiki/Almquist_shell"" rel=""noreferrer"">ash</a> variants. Pretty much any Unix-like system has a Bourne-style shell installed as <code>/bin/sh</code>, usually ash, ksh or bash.</p>

<p>In unix system administration, a user's <strong>shell</strong> is the program that is invoked when they log in. Normal user accounts have a command-line shell, but users with restricted access may have a <a href=""http://en.wikipedia.org/wiki/Restricted_shell"" rel=""noreferrer"">restricted shell</a> or some other specific command (e.g. for file-transfer-only accounts).</p>

<hr>

<p>The division of labor between the terminal and the shell is not completely obvious. Here are their main tasks.</p>

<ul>
<li>Input: the terminal converts keys into control sequences (e.g. <kbd>Left</kbd> → <code>\e[D</code>). The shell converts control sequences into commands (e.g. <code>\e[D</code> → <code>backward-char</code>).</li>
<li>Line editing, input history and completion are provided by the shell.

<ul>
<li>The terminal may provide its own line editing, history and completion instead, and only send a line to the shell when it's ready to be executed. The only common terminal that operates in this way is <code>M-x shell</code> in Emacs.</li>
</ul></li>
<li>Output: the shell emits instructions such as “display <code>foo</code>”, “switch the foreground color to green”, “move the cursor to the next line”, etc. The terminal acts on these instructions.</li>
<li>The prompt is purely a shell concept.</li>
<li>The shell never sees the output of the commands it runs (unless redirected). Output history (scrollback) is purely a terminal concept.</li>
<li>Inter-application copy-paste is provided by the terminal (usually with the mouse or key sequences such as <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>V</kbd> or <kbd>Shift</kbd>+<kbd>Insert</kbd>). The shell may have its own internal copy-paste mechanism as well (e.g. <kbd>Meta</kbd>+<kbd>W</kbd> and <kbd>Ctrl</kbd>+<kbd>Y</kbd>).</li>
<li><a href=""http://en.wikipedia.org/wiki/Job_control"" rel=""noreferrer"">Job control</a> (launching programs in the background and managing them) is mostly performed by the shell. However, it's the terminal that handles key combinations like <kbd>Ctrl</kbd>+<kbd>C</kbd> to kill the foreground job and <kbd>Ctrl</kbd>+<kbd>Z</kbd> to suspend it.</li>
</ul>
","4132"
"SSH easily copy file to local system","244001","","<p>If I'm logged in to a system via SSH, is there a way to copy a file back to my local system without firing up another terminal or screen session and doing scp or something similar or without doing SSH from the remote system back to the local system?</p>
","<h3>Master connection</h3>

<p>It's easiest if you plan in advance.</p>

<p>Open a master connection the first time. For subsequent connections, route slave connections through the existing master connection. In your <a href=""http://www.freebsd.org/cgi/man.cgi?query=ssh_config&amp;sektion=5"" rel=""noreferrer""><code>~/.ssh/config</code></a>, set up connection sharing to happen automatically:</p>

<pre><code>ControlMaster auto
ControlPath ~/.ssh/control:%h:%p:%r
</code></pre>

<p>If you start an ssh session to the same (user, port, machine) as an existing connection, the second session will be tunneled over the first. Establishing the second connection requires no new authentication and is very fast.</p>

<p>So while you have your active connection, you can quickly:</p>

<ul>
<li>copy a file with <code>scp</code> or <code>rsync</code>;</li>
<li><a href=""https://unix.stackexchange.com/questions/16206/copy-files-from-remote-server-to-local-ignoring-existing-files-rsync-not-avail/16235#16235"">mount a remote filesystem with sshfs</a>.</li>
</ul>

<h3>Forwarding</h3>

<p>On an existing connection, you can establish a reverse ssh tunnel. On the ssh command line, create a remote forwarding by passing <code>-R 22042:localhost:22</code> where 22042 is a randomly chosen number that's different from any other port number on the remote machine. Then <code>ssh -p 22042 localhost</code> on the remote machine connects you back to the source machine; you can use <code>scp -P 22042 foo localhost:</code> to copy files.</p>

<p>You can automate this further with <code>RemoteForward 22042 localhost:22</code>. The problem with this is that if you connect to the same computer with multiple instances of ssh, or if someone else is using the port, you don't get the forwarding.</p>

<p>If you haven't enabled a remote forwarding from the start, you can do it on an existing ssh session. Type <kbd>Enter</kbd> <code>~C</code> <kbd>Enter</kbd> <code>-R 22042:localhost:22</code> <kbd>Enter</kbd>.
See “Escape characters” in the manual for more information.</p>

<p>There is also some interesting information in <a href=""https://serverfault.com/questions/175798/ssh-back-to-the-local-machine-from-a-remote-ssh-session"">this Server Fault thread</a>.</p>

<h3>Copy-paste</h3>

<p>If the file is small, you can type it out and copy-paste from the terminal output. If the file contains non-printable characters, use an encoding such as <a href=""http://en.wikipedia.org/wiki/Base64"" rel=""noreferrer"">base64</a>.</p>

<pre>
remote.example.net$ base64 &lt;myfile
<em>(copy the output)</em>
</pre>

<pre>
local.example.net$ base64 -d >myfile
<em>(copy the output)</em>
<kbd>Ctrl</kbd>+<kbd>D</kbd>
</pre>

<p>More conveniently, if you have X forwarding active, copy the file on the remote machine and paste it locally. You can pipe data in and out of <a href=""http://sourceforge.net/projects/xclip/"" rel=""noreferrer""><code>xclip</code></a> or <a href=""http://www.vergenet.net/~conrad/software/xsel/"" rel=""noreferrer""><code>xsel</code></a>. If you want to preserve the file name and metadata, copy-paste an archive.</p>

<pre><code>remote.example.net$ tar -czf - myfile | xsel
</code></pre>

<p></p>

<pre><code>local.example.net$ xsel | tar -xzf -
</code></pre>
","2869"
"How to do integer & float calculations, in bash or other languages/frameworks?","243721","","<p>Using <code>echo ""20+5""</code> literally produces the text ""<code>20+5</code>"".</p>

<p>What command can I use to get the numeric sum, <code>25</code> in this case?</p>

<p>Also, what's the easiest way to do it just using bash for floating
point? For example, <code>echo $((3224/3807.0))</code> prints <code>0</code> :(.</p>

<p>I am looking for answers using either the basic command shell ('command
line') itself or through using languages that are available from the
command line.</p>
","<h2>There are lots of options!!!</h2>

<h2>Summary</h2>

<pre><code>$ echo ""$((20.0/7))""  # (ksh93/zsh/yash, not bash)
$ awk ""BEGIN {print (20+5)/2}""
$ zcalc
$ bc &lt;&lt;&lt; 20+5/2
$ bc &lt;&lt;&lt; ""scale=4; (20+5)/2""
$ dc &lt;&lt;&lt; ""4 k 20 5 + 2 / p""
$ expr 20 + 5
$ calc 2 + 4
$ node -pe 20+5/2  # Uses the power of JavaScript, e.g. : node -pe 20+5/Math.PI
$ echo 20 5 2 / + p | dc 
$ echo 4 k 20 5 2 / + p | dc 
$ perl -E ""say 20+5/2""
$ python -c ""print 20+5/2""
$ python -c ""print 20+5/2.0""
$ clisp -x ""(+ 2 2)""
$ lua -e ""print(20+5/2)""
$ php -r 'echo 20+5/2;'
$ ruby -e 'p 20+5/2'
$ ruby -e 'p 20+5/2.0'
$ guile -c '(display (+ 20 (/ 5 2)))'
$ guile -c '(display (+ 20 (/ 5 2.0)))'
$ slsh -e 'printf(""%f"",20+5/2)'
$ slsh -e 'printf(""%f"",20+5/2.0)'
$ tclsh &lt;&lt;&lt; 'puts [expr 20+5/2]'
$ tclsh &lt;&lt;&lt; 'puts [expr 20+5/2.0]'
$ sqlite3 &lt;&lt;&lt; 'select 20+5/2;'
$ sqlite3 &lt;&lt;&lt; 'select 20+5/2.0;'
$ echo 'select 1 + 1;' | sqlite3 
$ psql -tAc 'select 1+1'
$ R -q -e 'print(sd(rnorm(1000)))'
$ r -e 'cat(pi^2, ""\n"")'
$ r -e 'print(sum(1:100))'
$ smjs
$ jspl
</code></pre>

<h2>Details</h2>

<h3>Shells</h3>

<p>You can use POSIX arithmetic expansion for <em>integer</em> arithmetic <strong><code>echo ""$((...))""</code></strong>:</p>

<pre><code>$ echo ""$((20+5))""
25
$ echo ""$((20+5/2))""
22
</code></pre>

<hr>

<p><strong><code>ksh93</code></strong>, <strong><code>yash</code></strong> and <strong><code>zsh</code></strong> do support floats there:</p>

<pre><code>$ echo ""$((1.2 / 3))""
0.4
$ echo ""$((4*atan(1)))""
3.14159265358979324
</code></pre>

<p>(in <code>zsh</code>, you need <code>zmodload zsh/mathfunc</code> to get the math functions like <code>atan</code> above, <code>yash</code> doesn't support them).</p>

<hr>

<p>Interactively with zsh:</p>

<pre><code>$ autoload zcalc
$ zcalc
1&gt; PI/2
1.5708
2&gt; cos($1)
6.12323e-17
3&gt; :sci 12
6.12323399574e-17
</code></pre>

<hr>

<p>With (t)csh (integer only):</p>

<pre><code>% @ a=25 / 3; echo $a
8
</code></pre>

<hr>

<p>In the <code>rc</code> shell family, <code>akanga</code> is the one with arithmetic expansion:</p>

<pre><code>; echo $:25/3
8
</code></pre>

<hr>

<h3>POSIX toolchest</h3>

<p><strong><code>bc</code></strong> (see below for interactive mode), <a href=""https://www.gnu.org/software/bc/manual/html_mono/bc.html"" rel=""noreferrer"">manual here</a></p>

<p>Mnemonic: <kbd>b</kbd>est <kbd>c</kbd>alculator (though the <code>b</code> is in fact for <em>basic</em>).</p>

<pre><code>$ echo 20+5/2 | bc
22
$ echo 'scale=4;20+5/2' | bc
22.5000
</code></pre>

<p>(supports arbitrary precision numbers)</p>

<hr>

<p>bc interactive mode:</p>

<pre><code>$ bc
bc 1.06.95
Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006 Free Software Foundation, Inc.
This is free software with ABSOLUTELY NO WARRANTY.
For details type `warranty'. 
5+5
10

2.2+3.3
5.5
</code></pre>

<hr>

<p><a href=""https://unix.stackexchange.com/users/13003/rush"">Rush</a>'s solution, <strong><code>expr</code></strong> (no interactive mode):</p>

<pre><code>$ expr 20 + 5
25
$ expr 20 + 5 / 2
22
</code></pre>

<hr>

<p><a href=""https://unix.stackexchange.com/a/40897"">Joshua's solution</a>: <strong><code>awk</code></strong> (no interactive mode):</p>

<pre><code>$ calc() { awk ""BEGIN{print $*}""; }
$ calc 1/3
0.333333
</code></pre>

<h3>Other more or less portable tools</h3>

<p><a href=""https://unix.stackexchange.com/users/4194/arcege"">Arcege</a>'s solution, <strong><code>dc</code></strong> (interactive mode: <code>dc</code>):</p>

<p>Which is even more fun since it works by reverse polish notation.</p>

<pre><code>$ echo 20 5 2 / + p | dc 
22
$ echo 4 k 20 5 2 / + p | dc 
22.5000
</code></pre>

<p>But not as practical unless you work with reverse polish notation a lot.</p>

<p><sup>Note that <code>dc</code> predates <code>bc</code> and <code>bc</code> has been historically implemented as a wrapper around <code>dc</code> but <code>dc</code> was not standardised by POSIX</sup></p>

<hr>

<p><a href=""https://unix.stackexchange.com/users/6234/dqdlm"">DQdims</a>'s <strong><code>calc</code></strong> (required <code>sudo apt-get install apcalc)</code>:</p>

<pre><code>$ calc 2 + 4
6
</code></pre>

<h3>General purpose language interpreters:</h3>

<p><a href=""https://unix.stackexchange.com/users/10412/manatwork"">manatwork</a>'s solution, <strong><code>node</code></strong> (interactive mode: <code>node</code>; output function not needed):</p>

<pre><code>$ node -pe 20+5/2  # Uses the power of JavaScript, e.g. : node -pe 20+5/Math.PI
22.5
</code></pre>

<hr>

<p><strong>Perl</strong> (interactive mode: <code>perl -de 1</code>):</p>

<pre><code>$ perl -E ""say 20+5/2""
22.5
</code></pre>

<hr>

<p><strong>Python</strong> (interactive mode: <code>python</code>; output function not needed):</p>

<pre><code>$ python -c ""print(20+5/2)""
22 # 22.5 with python3
$ python -c ""print(20+5/2.0)""
22.5
</code></pre>

<p>Also supports arbitrary precision numbers:</p>

<pre><code>$ python -c 'print(2**1234)'
295811224608098629060044695716103590786339687135372992239556207050657350796238924261053837248378050186443647759070955993120820899330381760937027212482840944941362110665443775183495726811929203861182015218323892077355983393191208928867652655993602487903113708549402668624521100611794270340232766099317098048887493809023127398253860618772619035009883272941129544640111837184
</code></pre>

<hr>

<p>If you have <strong><code>clisp</code></strong> installed, you can also use polish notation:</p>

<pre><code>$ clisp -x ""(+ 2 2)""
</code></pre>

<hr>

<p><a href=""https://unix.stackexchange.com/users/12779/marco"">Marco</a>'s solution, <strong><code>lua</code></strong> (interactive mode: <code>lua</code>):</p>

<pre><code>$ lua -e ""print(20+5/2)""
22.5
</code></pre>

<hr>

<p><strong>PHP</strong> (interactive mode: <code>php -a</code>):</p>

<pre><code>$ php -r 'echo 20+5/2;'
22.5
</code></pre>

<hr>

<p><strong>Ruby</strong> (interactive mode: <code>irb</code>; output function not needed):</p>

<pre><code>$ ruby -e 'p 20+5/2'
22
$ ruby -e 'p 20+5/2.0'
22.5
</code></pre>

<hr>

<p><strong>Guile</strong> (interactive mode: <code>guile</code>):</p>

<pre><code>$ guile -c '(display (+ 20 (/ 5 2)))'
45/2
$ guile -c '(display (+ 20 (/ 5 2.0)))'
22.5
</code></pre>

<hr>

<p><strong>S-Lang</strong> (interactive mode: <code>slsh</code>; output function not needed, just a <code>;</code> terminator):</p>

<pre><code>$ slsh -e 'printf(""%f"",20+5/2)'
22.000000
$ slsh -e 'printf(""%f"",20+5/2.0)'
22.500000
</code></pre>

<hr>

<p><strong>Tcl</strong> (interactive mode: <code>tclsh</code>; output function not needed, but <code>expr</code> is):</p>

<pre><code>$ tclsh &lt;&lt;&lt; 'puts [expr 20+5/2]'
22
$ tclsh &lt;&lt;&lt; 'puts [expr 20+5/2.0]'
22.5
</code></pre>

<hr>

<p><strong>Javascript</strong> shells:</p>

<pre><code>$ smjs
js&gt; 25/3
8.333333333333334
js&gt;

$ jspl
JSC: 25/3

RP: 8.33333333333333
RJS: [object Number]
JSC:
Good bye...

$ node
&gt; 25/3
8.333333333333334
&gt;
</code></pre>

<hr>

<h3>Various SQL's:</h3>

<p><strong>SQLite</strong> (interactive mode: <code>sqlite3</code>):</p>

<pre><code>$ sqlite3 &lt;&lt;&lt; 'select 20+5/2;'
22
$ sqlite3 &lt;&lt;&lt; 'select 20+5/2.0;'
22.5
</code></pre>

<hr>

<p><strong>MySQL</strong>:</p>

<pre><code>mysql -BNe 'select 1+1'
</code></pre>

<hr>

<p><strong>PostgreSQL</strong>:</p>

<pre><code>psql -tAc 'select 1+1
</code></pre>

<p>_The options on mysql and postgres stop the 'ascii art' image !</p>

<h3>Specialised math-oriented languages:</h3>

<p><strong>R</strong> in plain mode - lets generate 1000 Normal random numbers and get the standard deviation and print it</p>

<pre><code>$ R -q -e 'print(sd(rnorm(1000)))'
&gt; print(sd(rnorm(1000)))
[1] 1.031997
</code></pre>

<hr>

<p><strong>R</strong> using the <strong>littler</strong> script - lets print pi squared</p>

<pre><code>$ r -e 'cat(pi^2, ""\n"")'
9.869604
$  r -e 'print(sum(1:100))'
[1] 5050
</code></pre>

<hr>

<p><strong>PARI/GP</strong>, an extensive computer algebra system for number theory, linear algebra, and many other things</p>

<pre><code>$ echo ""prime(1000)""|gp -q
7919                        // the 1000th prime
$ echo ""factor(1000)"" | gp -q
[2 3]
[5 3]                       // 2^3*5^3
$ echo ""sum(x=1,5,x)"" | gp -q
15                          // 1+2+3+4+5
</code></pre>

<hr>

<p><strong>GNU Octave</strong> (a high-level interpreted language, primarily intended for numerical computations)</p>

<p>Also supports complex numbers:</p>

<pre><code>$ octave
&gt;&gt; 1.2 / 7
ans =  0.17143
&gt;&gt; sqrt(-1)
ans =  0 + 1i
</code></pre>
","40787"
"How can I calculate the size of a directory?","243560","","<p>How to know the size of a directory? Including subdirectories and files.</p>
","<pre><code>du -s directory_name
</code></pre>

<p>Or to get human readable output:</p>

<pre><code>du -sh directory_name
</code></pre>

<p>The <code>-s</code> option means that it won't list the size for each subdirectory, only the total size.</p>
","3021"
"How to find which processes are taking all the memory?","241331","","<p>I'm looking for somthing like top is to CPU usage. Is there a command line argument for top that does this? Currently, my memory is so full that even 'man top' fails with out of memory :)</p>
","<p>From inside <code>top</code> you can try the following:</p>

<ul>
<li>Press <kbd>SHIFT</kbd>+<kbd>f</kbd></li>
<li>Press the Letter corresponding to %MEM</li>
<li>Press <kbd>ENTER</kbd> </li>
</ul>

<p>You might also try:</p>

<pre><code>$ ps -eo pmem,pcpu,vsize,pid,cmd | sort -k 1 -nr | head -5
</code></pre>

<p>This will give the top 5 processes by memory usage.</p>
","5001"
"Find where inodes are being used","240992","","<p>So I received a warning from our monitoring system on one of our boxes that the number of free inodes on a filesystem was getting low.</p>

<p><code>df -i</code> output shows this:</p>

<pre><code>Filesystem       Inodes  IUsed    IFree IUse% Mounted on
/dev/xvda1       524288 422613   101675   81% /
</code></pre>

<p>As you can see, the root partition has 81% of its inodes used.<br>
I suspect they're all being used in a single directory. But how can I find where that is at?</p>
","<p><em>I saw <a href=""https://stackoverflow.com/questions/347620/where-are-all-my-inodes-being-used"">this</a> question over on stackoverflow, but I didn't like any of the answers, and it really is a question that should be here on U&amp;L anyway.</em></p>

<p>Basically an inode is used for each file on the filesystem. So running out of inodes generally means you've got a lot of small files laying around. So the question really becomes, ""what directory has a large number of files in it?""</p>

<p>In this case, the filesystem we care about is the root filesystem <code>/</code>, so we can use the following command:</p>

<pre><code>find / -xdev -printf '%h\n' | sort | uniq -c | sort -k 1 -n
</code></pre>

<p>This will dump a list of every directory on the filesystem prefixed with the number of files (and subdirectories) in that directory. Thus the directory with the largest number of files will be at the bottom.</p>

<p>In my case, this turns up the following:</p>

<pre><code>   1202 /usr/share/man/man1
   2714 /usr/share/man/man3
   2826 /var/lib/dpkg/info
 306588 /var/spool/postfix/maildrop
</code></pre>

<p>So basically <code>/var/spool/postfix/maildrop</code> is consuming all the inodes.</p>

<p><em>Note, this answer does have three caveats that I can think of. It does not properly handle anything with newlines in the path. I know my filesystem has no files with newlines, and since this is only being used for human consumption, the potential issue isn't worth solving (and one can always replace the <code>\n</code> with <code>\0</code> and use <code>sort -z</code> above). It also does not handle if the files are spread out among a large number of directories. This isn't likely though, so I consider the risk acceptable. It will also count hard links to a same file (so using only one inode) several times. Again, unlikely to give false positives</em></p>

<hr>

<p>The key reason I didn't like any of the answers on the stackoverflow answer is they all cross filesystem boundaries. Since my issue was on the root filesystem, this means it would traverse every single mounted filesystem. Throwing <code>-xdev</code> on the find commands wouldn't even work properly.<br>
For example, the most upvoted answer is this one:</p>

<pre><code>for i in `find . -type d `; do echo `ls -a $i | wc -l` $i; done | sort -n
</code></pre>

<p>If we change this instead to </p>

<pre><code>for i in `find . -xdev -type d `; do echo `ls -a $i | wc -l` $i; done | sort -n
</code></pre>

<p>even though <code>/mnt/foo</code> is a mount, it is also a directory on the root filesystem, so it'll turn up in <code>find . -mount -type d</code>, and then it'll get passed to the <code>ls -a $i</code>, which will dive into the mount.</p>

<p>The <code>find</code> in my answer instead lists the directory of every single file on the mount. So basically with a file structure such as:</p>

<pre><code>/foo/bar
/foo/baz
/pop/tart
</code></pre>

<p>we end up with</p>

<pre><code>/foo
/foo
/pop
</code></pre>

<p>So we just have to count the number of duplicate lines.</p>
","117094"
"Unzipping a .gz file without removing the gzipped file","240171","","<p>I have a file <code>file.gz</code>, when I try to unzip this file by using <code>gunzip file.gz</code>, it unzipped the file but only contains extracted and removes the <code>file.gz</code> file.</p>

<p>How can I unzip by keeping both unzipped file and zipped file?</p>
","<p>Here are several alternatives:</p>

<ul>
<li><p>Give <code>gunzip</code> the <code>--keep</code> option (version 1.6 or later)</p>

<blockquote>
  <p><code>-k</code>   <code>--keep</code><br>
  &nbsp; &nbsp; &nbsp; &nbsp; Keep (don't delete) input files during compression or decompression.</p>
</blockquote>

<pre><code>gunzip -k file.gz
</code></pre></li>
<li><p>Pass the file to <code>gunzip</code> as stdin</p>

<pre><code>gunzip &lt; file.gz &gt; file
</code></pre></li>
<li><p>Use <code>zcat</code> (or, on older systems, <code>gzcat</code>)</p>

<pre><code>zcat file.gz &gt; file
</code></pre></li>
</ul>
","156324"
"Return only the portion of a line after a matching pattern","237794","","<p>So pulling open a file with <code>cat</code> and then using <code>grep</code> to get matching lines only gets me so far when I am working with the particular log set that I am dealing with. It need a way to match lines to a pattern, but only to return the portion of the line after the match. The portion before and after the match will consistently vary. I have played with using <code>sed</code> or <code>awk</code>, but have not been able to figure out how to filter the line to either delete the part before the match, or just return the part after the match, either will work.
This is an example of a line that I need to filter:</p>

<pre><code>2011-11-07T05:37:43-08:00 &lt;0.4&gt; isi-udb5-ash4-1(id1) /boot/kernel.amd64/kernel: [gmp_info.c:1758](pid 40370=""kt: gmp-drive-updat"")(tid=100872) new group: &lt;15,1773&gt;: { 1:0-25,27-34,37-38, 2:0-33,35-36, 3:0-35, 4:0-9,11-14,16-32,34-38, 5:0-35, 6:0-15,17-36, 7:0-16,18-36, 8:0-14,16-32,34-36, 9:0-10,12-36, 10-11:0-35, 12:0-5,7-30,32-35, 13-19:0-35, 20:0,2-35, down: 8:15, soft_failed: 1:27, 8:15, stalled: 12:6,31, 20:1 }
</code></pre>

<p>The portion I need is everything after ""stalled"".</p>

<p>The background behind this is that I can find out how often something stalls:</p>

<pre><code>cat messages | grep stalled | wc -l
</code></pre>

<p>What I need to do is find out how many times a certain node has stalled (indicated by the portion before each colon after ""stalled"". If I just grep for that (ie 20:) it may return lines that have soft fails, but no stalls, which doesn't help me. I need to filter only the stalled portion so I can then grep for a specific node out of those that have stalled.</p>

<p>For all intents and purposes, this is a freebsd system with standard GNU core utils, but I cannot install anything extra to assist.</p>
","<p>The canonical tool for that would be <code>sed</code>.</p>

<pre><code>sed -n -e 's/^.*stalled: //p'
</code></pre>

<p>Detailed explanation:</p>

<ul>
<li><code>-n</code> means not to print anything by default.</li>
<li><code>-e</code> is followed by a sed command.</li>
<li><code>s</code> is the pattern replacement command.</li>
<li>The regular expression <code>^.*stalled:</code> matches the pattern you're looking for, plus any preceding text (<code>.*</code> meaning any text, with an initial <code>^</code> to say that the match begins at the beginning of the line). Note that if <code>stalled:</code> occurs several times on the line, this will match the last occurrence.</li>
<li>The match, i.e. everything on the line up to <code>stalled:</code>, is replaced by the empty string (i.e. deleted).</li>
<li>The final <code>p</code> means to print the transformed line.</li>
</ul>

<p>If you want to retain the matching portion, use a backreference: <code>\1</code> in the replacement part designates what is inside a group <code>\(…\)</code> in the pattern. Here, you could write <code>stalled:</code> again in the replacement part; this feature is useful when the pattern you're looking for is more general than a simple string.</p>

<pre><code>sed -n -e 's/^.*\(stalled: \)/\1/p'
</code></pre>

<p>Sometimes you'll want to remove the portion of the line after the match. You can include it in the match by including <code>.*$</code> at the end of the pattern (any text <code>.*</code> followed by the end of the line <code>$</code>). Unless you put that part in a group that you reference in the replacement text, the end of the line will not be in the output.</p>

<p>As a further illustration of groups and backreferences, this command swaps the part before the match and the part after the match.</p>

<pre><code>sed -n -e 's/^\(.*\)\(stalled: \)\(.*\)$/\3\2\1/p'
</code></pre>
","24151"
"How do I loop through only directories in bash?","237378","","<p>I have a folder with some directories and some files (some are hidden, beginning with dot).</p>

<pre><code>for d in *; do
 echo $d
done
</code></pre>

<p>will loop through all files, but I want to loop only through directories. How do I do that?</p>
","<p>If you need to select more specific files than only directories use <code>find</code> and pass it to <code>while read</code>:</p>

<pre><code>shopt -s dotglob
find * -prune -type d | while IFS= read -r d; do 
    echo ""$d""
done
</code></pre>

<p>Use <code>shopt -u dotglob</code> to exclude hidden directories (or <code>setopt dotglob</code>/<code>unsetopt dotglob</code> in zsh).</p>

<p><code>IFS=</code> to avoid splitting filenames containing one of the <code>$IFS</code>, for example: <code>'a b'</code></p>

<p>see <a href=""https://unix.stackexchange.com/a/96912/20661"">AsymLabs answer</a> below for more <code>find</code> options</p>

<hr>

<p>edit:<br>
In case you need to create an exit value from within the while loop, you can circumvent the extra subshell by this trick:</p>

<pre><code>while IFS= read -r d; do 
    if [ ""$d"" == ""something"" ]; then exit 1; fi
done &lt; &lt;(find * -prune -type d)
</code></pre>
","97074"
"Show top five CPU consuming processes with `ps`","237360","","<p>How to show top five CPU consuming processes with ps? </p>
","<p>Why use <code>ps</code> when you can do it easily with the <code>top</code> command?</p>

<p>If you must use <code>ps</code>, try this:</p>

<pre><code>ps aux | sort -nrk 3,3 | head -n 5
</code></pre>

<p>If you want something that's truly 'top'esq with constant updates, use watch</p>

<pre><code>watch ""ps aux | sort -nrk 3,3 | head -n 5""
</code></pre>
","13969"
"Sorting down processes by memory usage","236761","","<p>I am able to see the list of all the processes and the memory via </p>

<pre><code>ps aux 
</code></pre>

<p>and going through the VSZ and RSS</p>

<p>Is there a way to sort down the output of this command by the descending order on RSS value?</p>
","<p>Use the following command:</p>

<pre><code>ps aux --sort -rss
</code></pre>

<p>Check here for more <a href=""http://alvinalexander.com/linux/unix-linux-process-memory-sort-ps-command-cpu"">Linux process memory usage</a></p>
","92498"
"What is the difference between `grep`, `egrep`, and `fgrep`?","236702","","<p>Can anyone tell me the technical differences between <code>grep</code>, <code>egrep</code>, and <code>fgrep</code> and provide suitable examples?</p>

<p>When do I need to use <code>grep</code> over <code>egrep</code> and vice versa?</p>
","<ul>
<li><code>egrep</code> is 100% equivalent to <a href=""https://www.gnu.org/software/grep/manual/grep.html#index-matching-extended-regular-expressions"" rel=""noreferrer""><code>grep -E</code></a></li>
<li><code>fgrep</code> is 100% equivalent to <a href=""https://www.gnu.org/software/grep/manual/grep.html#index-matching-fixed-strings"" rel=""noreferrer""><code>grep -F</code></a></li>
</ul>

<p>Historically these switches were provided in separate binaries. On some really old Unix systems you will find that you need to call the separate binaries, but on all modern systems the switches are preferred. The man page for grep has details about this. </p>

<p>As for what they do, <code>-E</code> switches grep into a special mode so that the expression is evaluated as an ERE (Extended Regular Expression) as opposed to its normal pattern matching. Details of this syntax are on the man page.</p>

<blockquote>
  <p><code>-E, --extended-regexp</code><br>
     Interpret PATTERN as an extended regular expression</p>
</blockquote>

<p>The <code>-F</code> switch switches grep into a different mode where it accepts a pattern to match, but then splits that pattern up into one search string per line and does an OR search on any of the strings without doing any special pattern matching.</p>

<blockquote>
  <p><code>-F, --fixed-strings</code><br>
     Interpret  PATTERN  as  a  list of fixed strings, separated by newlines, any of which is to be matched.</p>
</blockquote>

<p>Here are some example scenarios:</p>

<ul>
<li><p>You have a file with a list of say ten Unix usernames in plain text. You want to search the group file on your machine to see if any of the ten users listed are in any special groups:</p>

<pre><code>grep -F -f user_list.txt /etc/group
</code></pre>

<p>The reason the <code>-F</code> switch helps here is that the usernames in your pattern file are interpreted as plain text strings. Dots for example would be interpreted as dots rather than wild-cards.</p></li>
<li><p>You want to search using a fancy expression. For example parenthesis <code>()</code> can be used to indicate groups with <code>|</code> used as an OR operator. You could run this search using <code>-E</code>:</p>

<pre><code>grep -E '^no(fork|group)' /etc/group
</code></pre>

<p>...to return lines that start with either ""nofork"" or ""nogroup"". Without the <code>-E</code> switch you would have to escape the special characters involved because with normal pattern matching they would just search for that exact pattern;</p>

<pre><code>grep '^no\(fork\|group\)' /etc/group
</code></pre></li>
</ul>
","17951"
"Remove all files/directories except for one file","235501","","<p>I have a directory containing a large number of files. I want to delete all files except for file.txt . How do I do this?</p>

<p>There are too many files to remove the unwanted ones individually and their names are too diverse to use * to remove them all except this one file.</p>

<p>Someone suggested using </p>

<pre><code>rm !(file.txt)
</code></pre>

<p>But it doesn't work. It returns:</p>

<pre><code>Badly placed ()'s 
</code></pre>

<p>My OS is Scientific Linux 6.</p>

<p>Any ideas? </p>
","<p>POSIXly:</p>

<pre><code>find . ! -name 'file.txt' -type f -exec rm -f {} +
</code></pre>

<p>will remove all regular files (recursively, including hidden ones) except <code>file.txt</code>. To remove directories, change <code>-type f</code> to <code>-type d</code> and add <code>-r</code> option to <code>rm</code>.</p>

<p>In <code>bash</code>, to use <code>rm -- !(file.txt)</code>, you must <a href=""http://www.gnu.org/software/bash/manual/html_node/Pattern-Matching.html#Pattern-Matching"">enable extglob</a>:</p>

<pre><code>$ shopt -s extglob 
$ rm -- !(file.txt)
</code></pre>

<p>(or calling <code>bash -O extglob</code>)</p>

<p>Note that <code>extglob</code> only works in <code>bash</code> and Korn shell family. And using <code>rm -- !(file.txt)</code> can cause an <code>Argument list too long</code> error.</p>

<p>In <code>zsh</code>, you can use <code>^</code> to negate pattern with <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#Glob-Operators"">extendedglob</a> enabled:</p>

<pre><code>$ setopt extendedglob
$ rm -- ^file.txt
</code></pre>

<p>or using the same syntax with <code>ksh</code> and <code>bash</code> with options <code>ksh_glob</code> and <code>no_bare_glob_qual</code> enabled.</p>
","153863"
"Trying to SSH to local VM Ubuntu with Putty","234448","","<p>I have set up a VM of Ubuntu server, have installed OpenSSH, and am now trying to connect to it using Putty. Within Putty, under ""Host name"", I put ""Ubuntu"", given this is what I thought it was called when I set up the VM. However, I just get the error: ""Connection Timed Out"". </p>

<p>I also tried putting ""127.0.0.1"" into the host name within Putty and just get ""Connection Refused"". Note that I have done the port forwarding for SSH and HTTP within Oracle VM, so I am at a loss as to how to get it running.</p>
","<p>VirtualBox will create a private network (10.0.2.x) which will be connected to your host network using <a href=""http://en.wikipedia.org/wiki/Network_address_translation"" rel=""nofollow noreferrer"">NAT</a>. (Unless configured otherwise.)</p>

<p>This means that you cannot directly access any host of the private network from the host network. To do so, you need some port forwarding. In the network preferences of your VM you can, for example, configure VirtualBox to open port 22 on 127.0.1.1 (a <a href=""http://en.wikipedia.org/wiki/Loopback#Virtual_loopback_interface"" rel=""nofollow noreferrer"">loopback</a> address of your host) and forward any traffic to port 22 of 10.0.2.1 (the internal address of your VM)</p>

<p>This way, you can point putty to Port 22 of 127.0.1.1 and VirtualBox will redirect this connection to your VM where its ssh daemon will answer it, allowing you to log in.</p>
","146028"
"How to check how long a process has been running?","234314","","<p>I would like to avoid doing this by launching the process from a monitoring app.</p>
","<p>On Linux with the <code>ps</code> from <code>procps(-ng)</code> (and most other systems since this is specified by POSIX):</p>

<pre><code>ps -o etime= -p ""$$"" 
</code></pre>

<p>Where <code>$$</code> is the PID of the process you want to check. This will return the elapsed time in the format <code>[[dd-]hh:]mm:ss</code>. </p>

<p>Using <code>-o etime</code> tells <code>ps</code> that you just want the elapsed time field, and the <code>=</code> at the end of that suppresses the header (without, you get a line which says <code>ELAPSED</code> and then the time on the next line; with, you get just one line with the time).</p>

<p>Or, with newer versions of the procps-ng tool suite (3.3.0 or above) on Linux or on FreeBSD 9.0 or above (and possibly others), use:</p>

<pre><code>ps -o etimes= -p ""$$""
</code></pre>

<p>(with an added <code>s</code>) to get time formatted just as seconds, which is more useful in scripts.</p>

<p>On Linux, the <code>ps</code> program gets this from <code>/proc/$$/stat</code>, where one of the fields (see <code>man proc</code>) is process start time. This is, unfortunately, specified to be the time in jiffies (an arbitrary time counter used in the Linux kernel) since the system boot. So you have to determine the time at which the system booted (from <code>/proc/stat</code>), the number of jiffies per second on this system, and then do the math to get the elapsed time in a useful format.</p>

<p>It turns out to be ridiculously complicated to find the value of HZ (that is, jiffies per second). From comments in <code>sysinfo.c</code> in the procps package, one can A) include the kernel header file and recompile if a different kernel is used, B) use the posix <code>sysconf()</code> function, which, sadly, uses a hard-coded value compiled into the C library, or C) ask the kernel, but there's no official interface to doing that. So, the <code>ps</code> code includes a series of kludges by which it determines the correct value. Wow.</p>

<p>So it's convenient that <code>ps</code> does that all for you. :)</p>

<p>As user @336_ notes, on Linux (this is not portable), you can use the <code>stat</code> command to look at the access, modification, or status change dates for the directory <code>/proc/$$</code> (where again <code>$$</code> is the process of interest). All three numbers should be the same, so</p>

<pre><code>stat -c%X /proc/$$
</code></pre>

<p>will give you the time that process <code>$$</code> started, in seconds since the epoch. That still isn't quite what you want, since you still need to do the math to subtract that from the current time to get elapsed time — I guess something like <code>date +%s --date=""now - $( stat -c%X /proc/$$ ) seconds""</code> would work, but it's a bit ungainly. One <em>possible</em> advantage is that if you use the long-format output like <code>-c%x</code> instead of <code>-c%X</code>, you get greater resolution than whole-number seconds. But, if you need that, you should probably use process-auditing approach because the timing of running the stat command is going to interfere with accuracy.</p>
","7871"
"How do I run a command as the system administrator (root)","233981","","<p>I need to run a command with administrative privileges. Someone said I should run a command as root. How do I do this?</p>
","<p>The main two commandline possibilities are:</p>

<ul>
<li>Use <strong><code>su</code></strong> and enter the root password when prompted.</li>
<li>Put <strong><code>sudo</code></strong> in front of the command, and enter your password when prompted.</li>
</ul>

<hr>

<h1>Running a shell command as root</h1>

<h2>sudo (preferred when not running a graphical display)</h2>

<p>This is the preferred method on most systems, including Ubuntu, Linux Mint, (arguably) Debian, and others. If you don't know a separate root password, use this method.</p>

<p>Sudo requires that you type your own password. (The purpose is to limit the damage if you leave your keyboard unattended and unlocked, and also to ensure that you <em>really</em> wish to run that command and it wasn't e.g. a typo.) It is often configured to not ask again for a few minutes so you can run several <code>sudo</code> commands in succession.</p>

<p>Example:</p>

<pre><code>sudo service apache restart
</code></pre>

<p>If you need to run several commands as root, prefix each of them with <code>sudo</code>. Sometimes, it is more convenient to run an interactive shell as root. You can use <code>sudo -i</code> for that:</p>

<pre><code>$ sudo -i
# command 1
# command 2
...
# exit
</code></pre>

<p>Instead of <code>sudo -i</code>, you can use <code>sudo -s</code>. The difference is that <code>-i</code> re<strong>i</strong>nitializes the environment to sane defaults, whereas <code>-s</code> uses your configuration files for better or for worse.</p>

<p>For more information, see the <a href=""http://www.sudo.ws/"" rel=""nofollow noreferrer"">sudo website</a>, or type <a href=""http://www.sudo.ws/sudo/sudo.man.html"" rel=""nofollow noreferrer""><code>man sudo</code></a> on your system. Sudo is very configurable; for example it can be configured to let a certain user only execute certain commands as root. Read the <a href=""http://www.sudo.ws/sudo/sudoers.man.html"" rel=""nofollow noreferrer""><code>sudoers</code> man page</a> for more information; use <code>sudo visudo</code> to edit the sudoers file.</p>

<h2>su</h2>

<p>The <code>su</code> command exists on most unix-like systems. It lets you run a command as another user, provided you know that user's password. When run with no user specified, <code>su</code> will default to the root account.</p>

<p>Example:</p>

<pre><code>su -c 'service apache restart'
</code></pre>

<p>The command to run must be passed using the <code>-c</code> option. Note that you need quotes so that the command is not parsed by your shell, but passed intact to the root shell that <code>su</code> runs.</p>

<p>To run multiple commands as root, it is more convenient to start an interactive shell. </p>

<pre><code>$ su
# command 1
# command 2
...
# exit
</code></pre>

<p>On some systems, you need to be in group number 0 (called <code>wheel</code>) to use <code>su</code>. (The point is to limit the damage if the root password is accidentally leaked to someone.)</p>

<h2>Logging in as root</h2>

<p>If there is a root password set and you are in possession of it, you can simply type <code>root</code> at the login prompt and enter the root password. Be very careful, and avoid running complex applications as root as they might do something you didn't intend. Logging in directly as root is mainly useful in emergency situations, such as disk failures or when you've locked yourself out of your account.</p>

<h2>Single User Mode</h2>

<p>Single user mode, or run-level 1, also gives you root privileges.  This is intended primarily for emergency maintenance situations where booting into a multi-user run-level is not possible.  You can boot into single user mode by passing <code>single</code> or <code>emergency</code> on the kernel command line.  Note that booting into single-user mode is not the same as booting the system normally and logging in as root.  Rather, the system will only start the services defined for run-level 1.  Typically, this is the smallest number of services required to have a usable system.</p>

<p>You can also get to single user mode by using the telinit command: <code>telinit 1</code>; however, this command requires you to already have gotten root privileges via some other method in order to run.</p>

<p>On many systems booting into single user mode will give the user access to a root shell without prompting for a password. Notably, <code>systemd</code>-based systems will prompt you for the root password when you boot this way.</p>

<h2>Other programs</h2>

<h3>Calife</h3>

<p><a href=""http://www.keltia.net/programs/calife/"" rel=""nofollow noreferrer"">Calife</a> lets you run commands as another user by typing your own password, if authorized. It is similar to the much more widespread sudo (see above). Calife is more light-weight than sudo but also less configurable.</p>

<h3>Op</h3>

<p><a href=""http://linux.die.net/man/1/op"" rel=""nofollow noreferrer"">Op</a> lets you run commands as another user, including root. This not a full-blown tool to run arbitrary commands: you type <code>op</code> followed by a <em>mnemonic</em> configured by the system administrator to run a specific command.</p>

<h3>Super</h3>

<p>Super lets you run commands as another user, including root. The command must have been allowed by the system administrator.</p>

<h2>Running a graphical command as root</h2>

<p>See also <a href=""http://en.wikipedia.org/wiki/Comparison_of_privilege_authorization_features"" rel=""nofollow noreferrer"">Wikipedia</a>.</p>

<h3>PolicyKit (preferred when using GNOME)</h3>

<p>Simply prefix your desired command with the command <code>pkexec</code>. Be aware that while this works in most cases, it does not work universally.</p>

<p>See <code>man pkexec</code> for more information.</p>

<h3>KdeSu, KdeSudo (preferred when using KDE)</h3>

<p><a href=""http://docs.kde.org/stable/en/kdebase-runtime/kdesu/"" rel=""nofollow noreferrer""><code>kdesu</code></a> and <a href=""http://ksudo.sourceforge.net/kdesudo/"" rel=""nofollow noreferrer""><code>kdesudo</code></a> are graphical front-ends to <code>su</code> and <code>sudo</code> respectively. They allow you to run X Window programs as root with no hassle. They are part of <a href=""http://www.kde.org/"" rel=""nofollow noreferrer"">KDE</a>. Type</p>

<pre><code>kdesu -c 'command --option argument'
</code></pre>

<p>and enter the root password, or type</p>

<pre><code>kdesudo -c 'command --option argument'
</code></pre>

<p>and enter your password (if authorized to run <code>sudo</code>). If you check the “keep password” option in KdeSu, you will only have to type the root password once per login session.</p>

<h3>Other programs</h3>

Ktsuss

<p><a href=""http://developer.berlios.de/projects/ktsuss"" rel=""nofollow noreferrer"">Ktsuss</a> (“keep the su simple, stupid”) is a graphical version of su.</p>

Beesu

<p><a href=""http://www.honeybeenet.altervista.org/beesu/"" rel=""nofollow noreferrer"">Beesu</a> is a graphical front-end to the su command that has replaced Gksu in Red Hat-based operating systems. It has been developed mainly for RHEL and Fedora.</p>

<h3>Obsolete methods</h3>

<code>gksu</code> and <code>gksudo</code>

<p><code>gksu</code> and <code>gksudo</code> are graphical front-ends to <code>su</code> and <code>sudo</code> respectively. They allow you to run X Window programs as root with no hassle. They are part of <a href=""http://www.gnome.org/"" rel=""nofollow noreferrer"">Gnome</a>. Type</p>

<pre><code>gksu command --option argument
</code></pre>

<p>and enter the root password, or type</p>

<pre><code>gksudo command --option argument
</code></pre>

<p>and enter your password (if authorized to run <code>sudo</code>).</p>

<p><code>gksu</code> and <code>gksudo</code> are obsolete. They have been replaced by PolicyKit in GNOME, and many distributions (such as Ubuntu) no longer install them by default. You should not depend on them being available or working properly.</p>

<h3>Manually via one of the shell-based methods</h3>

<p>Use one of the methods in the ""running a shell command as root section"". You will need to ensure that neither the <code>DISPLAY</code> environment variable nor the <code>XAUTHORITY</code> environment get reset during the transition to root. This may require additional configuration of those methods that is outside the scope of this question.</p>

<p>Overall, this is a bad idea, mostly because graphical applications will read and write configuration files as root, and when you try to use those applications again as your normal user, those applications won't have permission to read their own configurations.</p>

<h1>Editing a file as root</h1>

<p>See <a href=""https://unix.stackexchange.com/questions/104429/how-do-i-edit-a-file-as-root/104430#104430"">How do I edit a file as root?</a></p>
","3064"
"Sending messages to another user","232685","","<p>Is there any command to send messages through the Linux shell to other people on the same network? I'm using <code>write user</code> and then write the message itself. But there's any command that doesn't show my username or that I'm trying to message them</p>

<p>The command I'm using will show this to the user I'm trying to contact (code taken from the web):</p>

<pre><code>Message from root@dev.example.com on pts/1 at 17:11 ...
</code></pre>
","<p>The only straightforward way I know of doing this is to use the <code>wall</code> command. This can be used to omit the sender's identification, via the <code>-n</code> switch.</p>

<h3>Example</h3>

<pre><code>$ sudo wall -n hi

Remote broadcast message (Fri Nov  8 13:49:18 2013):

hi
</code></pre>

<h3>using echo</h3>

<p>This alternative method is more of a hack, since it isn't done through an explicit tool but you can echo text out to a users' terminal assuming you know which one they're on.</p>

<h3>Example</h3>

<pre><code>$ w
 13:54:26 up 2 days, 36 min,  4 users,  load average: 4.09, 4.20, 3.73
USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT
saml     tty1     :0               Wed13    2days  3:55m  0.04s pam: gdm-password
saml     pts/0    :0.0             Wed13   24:16m  0.35s  0.35s bash
saml     pts/1    :0.0             Wed20    0.00s  3.71s  0.00s w
saml     pts/4    :0.0             01:20   12:33m  0.36s  0.05s man rsync
</code></pre>

<p>Assuming you know user <code>saml</code> is in fact on one of the pseudo terminals you can echo text to that device directly like so. From terminal <code>pts/1</code>:</p>

<pre><code>$ sudo echo ""Let's go have lunch... ok?"" &gt; /dev/pts/4
$ 
</code></pre>

<p>Result on <code>pts/4</code>:</p>

<pre><code>$ man rsync
$ Let's go have lunch... ok?
</code></pre>
","99462"
"Does curl have a timeout?","229465","","<p>So far I couldn't find anything really, but is it true that <code>curl</code> doesn't really time out at all?</p>

<pre><code> user@host:~# curl http://localhost/testdir/image.jpg
</code></pre>

<p>I'm asking because I'm redirecting any request for images in <code>testdir</code> to a separate Apache module which generates those pictures on the fly. It can take up to 15 minutes before the picture is actually ready and delivered to the requesting client. </p>

<p>Will <code>curl</code> always wait (or is it depending on configuration) or is there any sort of timeout?</p>
","<p>Yes.</p>

<h1>Timeout parameters</h1>

<p><code>curl</code> has two options: <code>--connect-timeout</code> and <code>--max-time</code>.</p>

<p>Quoting from the manpage:</p>

<pre><code>--connect-timeout &lt;seconds&gt;
    Maximum  time  in  seconds  that you allow the connection to the
    server to take.  This only limits  the  connection  phase,  once
    curl has connected this option is of no more use.  Since 7.32.0,
    this option accepts decimal values, but the actual timeout  will
    decrease in accuracy as the specified timeout increases in deci‐
    mal precision. See also the -m, --max-time option.

    If this option is used several times, the last one will be used.
</code></pre>

<p>and:</p>

<pre><code>-m, --max-time &lt;seconds&gt;
    Maximum  time  in  seconds that you allow the whole operation to
    take.  This is useful for preventing your batch jobs from  hang‐
    ing  for  hours due to slow networks or links going down.  Since
    7.32.0, this option accepts decimal values, but the actual time‐
    out will decrease in accuracy as the specified timeout increases
    in decimal precision.  See also the --connect-timeout option.

    If this option is used several times, the last one will be used.
</code></pre>

<h1>Defaults</h1>

<p>Here (on Debian) it stops trying to connect after 2 minutes, regardless of the time specified with <code>--connect-timeout</code> and although the default connect timeout value seems to be <em>5 minutes</em> according to the <code>DEFAULT_CONNECT_TIMEOUT</code> macro in <a href=""https://github.com/curl/curl/blob/master/lib/connect.h"" rel=""noreferrer"">lib/connect.h</a>.</p>

<p>A default value for <code>--max-time</code> doesn't seem to exist, making <code>curl</code> wait forever for a response if the initial connect succeeds.</p>

<h1>What to use?</h1>

<p>You are probably interested in the latter option, <code>--max-time</code>. For your case set it to <code>900</code> (15 minutes).</p>

<p>Specifying option <code>--connect-timeout</code> to something like <code>60</code> (one minute) might also be a good idea. Otherwise <code>curl</code> will try to connect again and again, apparently using some backoff algorithm.</p>
","94612"
"Why do I need a tty to run sudo if I can sudo without a password?","227405","","<p>I have configured <code>sudo</code> to run without a password, but when I try to <code>ssh 'sudo Foo'</code>, I still get the error message <code>sudo: sorry, you must have a tty to run sudo</code>.</p>

<p>Why does this happen and how can I work around it?</p>
","<p>That's probably because your <code>/etc/sudoers</code> file (or any file it includes) has: </p>

<pre><code>Defaults requiretty
</code></pre>

<p>...which makes <code>sudo</code> require a TTY. Red Hat systems (RHEL, Fedora...) have been known to require a TTY in default <code>sudoers</code> file. That provides no real security benefit and can be safely removed.</p>

<p><a href=""https://bugzilla.redhat.com/show_bug.cgi?id=1020147"" rel=""noreferrer"">Red Hat have acknowledged the problem</a> and it will be removed in future releases.</p>

<p>If changing the configuration of the server is not an option, as a work-around for that mis-configuration, you could use the <code>-t</code> or <code>-tt</code> options to <code>ssh</code> which spawns a pseudo-terminal on the remote side, but beware that it has a number of side effects.</p>

<p><code>-tt</code> is meant for interactive use. It puts the local terminal in <code>raw</code> mode so that you interact with the remote terminal. That means that if <code>ssh</code> I/O is not from/to a terminal, that will have side effects. For instance, all the input will be echoed back, special terminal characters (<code>^?</code>, <code>^C</code>, <code>^U</code>) will cause special processing; on output, <code>LF</code>s will be converted to <code>CRLF</code>s... (see <a href=""https://unix.stackexchange.com/a/151963/22565"">this answer to <em>Why is this binary file being changed?</em></a> for more details.</p>

<p>To minimise the impact, you could invoke it as:</p>

<pre><code>ssh -tt host 'stty raw -echo; sudo ...' &lt; &lt;(cat)
</code></pre>

<p>The <code>&lt; &lt;(cat)</code> will avoid the setting of the local terminal (if any) in <code>raw</code> mode. And we're using <code>stty raw -echo</code> to set the line discipline of the remote terminal as pass through (effectively so it behaves like the pipe that would be used instead of a pseudo-terminal without <code>-tt</code>, though that only applies after that command is run, so you need to delay sending something for input until that happens).</p>

<p>Note that since the output of the remote command will go to a terminal, that will still affect its buffering (which will be line-based for many applications) and bandwidth efficiency since <code>TCP_NODELAY</code> is on. Also with <code>-tt</code>, <code>ssh</code> sets the IPQoS to <code>lowdelay</code> as opposed to <code>throughput</code>. You could work around both with:</p>

<pre><code>ssh -o IPQoS=throughput -tt host 'stty raw -echo; sudo cmd | cat' &lt; &lt;(cat)
</code></pre>

<p>Also, note that it means  the remote command cannot detect end-of-file  on its stdin and the stdout and stderr of the remote command are merged into a single stream.</p>

<p>So, not so good a work around after all.</p>

<p>If you've a got a way to spawn a pseudo-terminal on the remote host (like with <code>expect</code>, <code>zsh</code>, <code>socat</code>, <code>perl</code>'s <code>IO::Pty</code>...), then it would be better to use that to create the pseudo-terminal to attach <code>sudo</code> to (but not for I/O), and use <code>ssh</code> without <code>-t</code>.</p>

<p>For example, with <code>expect</code>:</p>

<pre><code>ssh host 'expect -c ""spawn -noecho sh -c {
     exec sudo cmd &gt;&amp;4 2&gt;&amp;5 &lt;&amp;6 4&gt;&amp;- 5&gt;&amp;- 6&lt;&amp;-}
 exit [lindex [wait] 3]"" 4&gt;&amp;1 5&gt;&amp;2 6&lt;&amp;0'
</code></pre>

<p>(here assuming the login shell of the remote user is Bourne-like).</p>
","122624"
"Is it possible to see cp speed and percent copied?","226412","","<p>I'm having problems when copying large files using nautilus (it gets stuck). I need to copy, using <code>cp</code>. I would like to know if there are any parameters that shows the % copied and also the transfer speed. </p>
","<p><code>rsync</code> has a flag called <code>progress2</code> which shows the overall percentage:</p>

<pre><code>rsync --info=progress2 source dest
</code></pre>
","65222"
"Finding all large files in the root filesystem","223908","","<p>I have a linux server, which currently has below space usage:</p>

<pre><code>/dev/sda3              20G   15G  4.2G  78% /
/dev/sda6              68G   42G   23G  65% /u01
/dev/sda2              30G  7.4G   21G  27% /opt
/dev/sda1              99M   19M   76M  20% /boot
tmpfs                  48G  8.2G   39G  18% /dev/shm
</code></pre>

<p>As you can see. <code>/</code> is at 78%. I want to check, which files or folders are consuming space.</p>

<p>I tried this:</p>

<pre><code>find . -type d -size +100M
</code></pre>

<p>Which shows result like this:</p>

<pre><code>./u01/app/june01.dbf
./u01/app/temp01.dbf
./u01/app/smprd501.dbf
./home/abhishek/centos.iso
./home/abhishek/filegroup128.jar
</code></pre>

<p>Now this is my issue. I only want the name of those files located in folders that are consuming space at <code>/</code> and not at <code>/u01</code> or <code>/home</code>. Since <code>/</code> is base of everything, it is showing me every file of my server.</p>

<p>Is is possible to get big files that is contributing to 78% of <code>/</code> ? </p>
","<p>Try:</p>

<pre><code>find / -xdev -type f -size +100M
</code></pre>

<p>It lists all files that has size bigger than 100M.</p>

<p>If you want to know about directory, you can try <code>ncdu</code>.</p>

<p>If you aren't running Linux, you may need to use <code>-size +204800</code> or <code>-size +104857600c</code>, as the <code>M</code> suffix to mean megabytes isn't in POSIX.</p>

<pre><code>find / -xdev -type f -size +102400000c
</code></pre>
","140369"
"What does '>/dev/null 2>&1' mean in this article of crontab basics?","223868","","<p>I am reading an article about <a href=""http://www.adminschoice.com/crontab-quick-reference"">crontab</a></p>

<p>There is something about disabling automatically sending emails.</p>

<blockquote>
  <ol start=""6"">
  <li><p>Disable Email By default cron jobs sends an email to the user account executing the cronjob. If this is not needed put the following
  command At the end of the cron job line.</p>

<pre><code>&gt;/dev/null 2&gt;&amp;1
</code></pre></li>
  </ol>
</blockquote>

<p>What is the detailed meaning for <code>2</code> <code>&gt;</code> <code>&amp;</code> and <code>1</code>? Why putting this to the end of a crontab file would turn off the email-sending thing?</p>
","<p><code>&gt;</code> is for redirect</p>

<p><code>/dev/null</code> is a black hole where any data sent, will be discarded</p>

<p><code>2</code> is the file descriptor for Standard Error</p>

<p><code>&gt;</code> is for redirect</p>

<p><code>&amp;</code> is the symbol for file descriptor (without it, the following <code>1</code> would be considered a filename)</p>

<p><code>1</code> is the file descriptor for Standard Out</p>

<p>Therefore <code>&gt;/dev/null 2&gt;&amp;1</code> is redirect the output of your program to <code>/dev/null</code>.  Include both the <code>Standard Error</code> and <code>Standard Out</code>.</p>

<p>Much more information is available at The Linux Documentation Project's <a href=""http://www.tldp.org/LDP/abs/html/io-redirection.html"">I/O Redirection</a> page.</p>

<p><code>cron</code> will only email you if there is some output from you job.  With everything redirected to <code>null</code>, there is no output and hence <code>cron</code> will not email you.</p>
","163359"
"What to do when a Linux desktop freezes?","223280","","<p>I'm a Windows guy, dual booted recently, and now I'm using Linux Mint 12</p>

<p>When a Windows desktop freezes I <code>refresh</code>, or if I am using a program I use <kbd>alt</kbd> + <kbd>F4</kbd> to exit the program or I can use  <kbd>ctrl</kbd> + <kbd>alt</kbd> + <kbd>delete</kbd> and this command will allow me to fix the Windows desktop by seeing what program is not responding and so on.</p>

<p>Mint freezes fewer times than my XP, but when it does, I don't know what to do, I just shut down the pc and restart it.</p>

<p>So is there a command to fix Linux when it freezes?</p>
","<p>You can try <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>*</kbd> to kill the front process (<a href=""http://seclists.org/oss-sec/2012/q1/191"">Screen locking programs on Xorg 1.11</a>) or <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F1</kbd> to open a terminal, launch a command like <code>ps</code>, <code>top</code>, or <code>htop</code> to see running processes and launch kill on not responding process.</p>

<p>Note: if not installed, install <code>htop</code> with <code>sudo apt-get install htop</code>. </p>

<p>Also, once done in your <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F1</kbd> virtual console, return to the desktop with <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F7</kbd>.</p>
","31819"
"How do I make my pc speaker beep","223249","","<p>Using bash, how can I make the pc speaker beep?</p>

<p>Something like <code>echo 'beepsound' &gt; /dev/pcspkr</code> would be nice.</p>
","<p>I usually use the little utility <code>beep</code> installed on many systems.
This command will try different approaches to create a system sound.</p>

<p>There are 3 ways of creating a sound from the <code>beep</code> manpage:</p>

<ol>
<li><p>The traditional method of producing a beep in a shell script is to write an <code>ASCII BEL</code> (<code>\007</code>) character to standard output, by means of a shell command such as </p>

<pre><code>echo -ne '\007'
</code></pre>

<p>This only  works if the calling shell's standard output is currently directed to a terminal device of some sort; if not, the beep will produce no sound and might even cause unwanted corruption in whatever file the output is directed to.</p></li>
<li><p>There are other ways to cause a beeping noise. A slightly more reliable method is to open <code>/dev/tty</code> and send your BEL character there. This is robust against I/O redirection, but still fails in the case where the shell script wishing to generate a beep does not have a controlling terminal, for example because it is run from an X window  manager. </p></li>
<li><p>A third approach is to connect to your X display and send it a bell command. This does not depend on a Unix terminal device, but does (of course) require an X display.</p></li>
</ol>

<p><code>beep</code> will simply try these 3 methods.</p>
","1980"
"Is there a way to see details of all the threads that a process has in Linux?","221664","","<p>For Windows, I think <a href=""http://technet.microsoft.com/en-us/sysinternals/bb896653.aspx"">Process Explorer</a> shows you all the threads under a process.</p>

<p>Is there a similar command line utility for Linux that can show me details about all the threads a particular process is spawning?</p>

<hr>

<p>I think I should have made myself more clear. <strong>I do not want to see the process hierarcy, but a list of all the threads spawned by a particular process</strong></p>

<p>See this screenshot</p>

<p><img src=""https://i.stack.imgur.com/ZfcGv.png"" alt=""alt text""></p>

<p><strong>How can this be achieved in Linux?</strong> Thanks!</p>
","<p>The classical tool <code>top</code> shows processes by default but can be told to show threads with the <code>H</code> key press or <code>-H</code> command line option. There is also <a href=""http://htop.sourceforge.net"">htop</a>, which is similar to <code>top</code> but has scrolling and colors; it shows all threads by default (but this can be turned off). <code>ps</code> also has a few options to show threads, especially <code>H</code> and <code>-L</code>.</p>

<p>There are also GUI tools that can show information about threads, for example <a href=""http://qps.kldp.net/projects/qps"">qps</a> (a simple GUI wrapper around <code>ps</code>) or <a href=""http://sourceforge.net/projects/conky/"">conky</a> (a system monitor with lots of configuration options).</p>

<p>For each process, a lot of information is available in <code>/proc/12345</code> where <code>12345</code> is the process ID. Information on each thread is available in <code>/proc/12345/task/67890</code> where <code>67890</code> is the kernel thread ID. This is where <code>ps</code>, <code>top</code> and other tools get their information.</p>
","901"
"How do I use pushd and popd commands?","219483","","<p>What are the practical uses of both <code>pushd</code> and <code>popd</code> when there is an advantage of using these two commands over <code>cd</code> and <code>cd -</code>?</p>

<p><strong>EDIT</strong>: I'm looking for some practical examples of uses for both of these commands or reasons for keeping stack with directories (when you have tab completion, <code>cd -</code>, aliases for shortening <code>cd ..</code>, etc.).</p>
","<p><a href=""http://www.gnu.org/software/bash/manual/html_node/Directory-Stack-Builtins.html#Directory-Stack-Builtins""><code>pushd</code></a>, <a href=""http://www.gnu.org/software/bash/manual/html_node/Directory-Stack-Builtins.html#Directory-Stack-Builtins""><code>popd</code></a>, and <a href=""http://www.gnu.org/software/bash/manual/html_node/Directory-Stack-Builtins.html#Directory-Stack-Builtins""><code>dirs</code></a> are shell builtins which allow you manipulate the <a href=""http://www.gnu.org/software/bash/manual/html_node/The-Directory-Stack.html#The-Directory-Stack"">directory stack</a>. This can be used to change directories but return to the directory from which you came.</p>

<h2>For example</h2>

<h3>start up with the following directories:</h3>

<pre><code>$ ls
dir1  dir2  dir3
</code></pre>

<h3>pushd to dir1</h3>

<pre><code>$ pushd dir1
~/somedir/dir1 ~/somedir
$ dirs
~/somedir/dir1 ~/somedir
</code></pre>

<p><code>dirs</code> command confirms that we have 2 directories on the stack now. <code>dir1</code> and the original dir, <code>somedir</code>.</p>

<h3>pushd to ../dir3 (because we're inside <code>dir1</code> now)</h3>

<pre><code>$ pushd ../dir3
~/somedir/dir3 ~/somedir/dir1 ~/somedir
$ dirs
~/somedir/dir3 ~/somedir/dir1 ~/somedir
$ pwd
/home/saml/somedir/dir3
</code></pre>

<p><code>dirs</code> shows we have 3 directories in the stack now. <code>dir3</code>, <code>dir1</code>, and <code>somedir</code>. Notice the direction. Every new directory is getting added to the left. When we start poping directories off, they'll come from the left as well.</p>

<h3>manually change directories to <code>../dir2</code></h3>

<pre><code>$ cd ../dir2
$ pwd
/home/saml/somedir/dir2
</code></pre>

<h3>Now start popping directories</h3>

<pre><code>$ popd
~/somedir/dir1 ~/somedir
$ pwd
/home/saml/somedir/dir1
</code></pre>

<p>Notice we popped back to <code>dir1</code>.</p>

<h3>Pop again...</h3>

<pre><code>$ popd
~/somedir    
$ pwd
/home/saml/somedir
</code></pre>

<p>And we're back where we started, <code>somedir</code>.</p>

<p>Might get a little confusing, but the head of the stack is the directory that you're currently in. Hence when we get back to <code>somedir</code>, even though <code>dirs</code> shows this:</p>

<pre><code>$ dirs
~/somedir
</code></pre>

<p>Our stack is infact empty.</p>

<pre><code>$ popd
bash: popd: directory stack empty
</code></pre>
","77081"
"How can I use sed to replace a multi-line string?","219156","","<p>I've noticed that, if I add <code>\n</code> to a pattern for substituting using <code>sed</code>, it does not match. Example:</p>

<pre><code>$ cat &gt; alpha.txt
This is
a test
Please do not
be alarmed

$ sed -i'.original' 's/a test\nPlease do not/not a test\nBe/' alpha.txt

$ diff alpha.txt{,.original}

$ # No differences printed out
</code></pre>

<p>How can I get this to work?</p>
","<p>In the simplest calling of <em>sed</em>, it has <em>one</em> line of text in the pattern space, ie. 1 line of <code>\n</code> delimited text from the input. The single line in the pattern space has no <code>\n</code>... That's why your regex is not finding anything.   </p>

<p>You can read multiple lines into the pattern-space and manipulate things surprisingly well, but with a more than normal effort..  Sed has a set of commands which allow this type of thing...  Here is a link to a <a href=""http://docstore.mik.ua/orelly/unix/sedawk/appa_03.htm"">Command Summary for sed</a>. It is the best one I've found, and got me rolling.  </p>

<p>However forget the ""one-liner"" idea once you start using sed's micro-commands. It is useful to lay it out like a structured program until you get the feel of it... It is surprisingly simple, and equally unusual. You could think of it as the ""assembler language"" of text editing.   </p>

<p>Summary: Use sed for simple things, and maybe a bit more, but in general, when it gets beyond working with a single line, most people prefer something else...<br>
I'll let someone else suggest something else.. I'm really not sure what the best choice would be (I'd use sed, but that's because I don't know perl well enough.)     </p>

<hr>

<pre><code>sed '/^a test$/{
       $!{ N        # append the next line when not on the last line
         s/^a test\nPlease do not$/not a test\nBe/
                    # now test for a successful substitution, otherwise
                    #+  unpaired ""a test"" lines would be mis-handled
         t sub-yes  # branch_on_substitute (goto label :sub-yes)
         :sub-not   # a label (not essential; here to self document)
                    # if no substituion, print only the first line
         P          # pattern_first_line_print
         D          # pattern_ltrunc(line+nl)_top/cycle
         :sub-yes   # a label (the goto target of the 't' branch)
                    # fall through to final auto-pattern_print (2 lines)
       }    
     }' alpha.txt  
</code></pre>

<hr>

<p>Here it is the same script, condensed into what is obviously harder to read and work with, but some would dubiously call <em>a one-liner</em>   </p>

<pre><code>sed '/^a test$/{$!{N;s/^a test\nPlease do not$/not a test\nBe/;ty;P;D;:y}}' alpha.txt
</code></pre>

<hr>

<p>Here is my command ""cheat-sheet"" </p>

<pre><code>:  # label
=  # line_number
a  # append_text_to_stdout_after_flush
b  # branch_unconditional             
c  # range_change                     
d  # pattern_delete_top/cycle          
D  # pattern_ltrunc(line+nl)_top/cycle 
g  # pattern=hold                      
G  # pattern+=nl+hold                  
h  # hold=pattern                      
H  # hold+=nl+pattern                  
i  # insert_text_to_stdout_now         
l  # pattern_list                       
n  # pattern_flush=nextline_continue   
N  # pattern+=nl+nextline              
p  # pattern_print                     
P  # pattern_first_line_print          
q  # flush_quit                        
r  # append_file_to_stdout_after_flush 
s  # substitute                                          
t  # branch_on_substitute              
w  # append_pattern_to_file_now         
x  # swap_pattern_and_hold             
y  # transform_chars                   
</code></pre>
","26290"
"What is the purpose of the lost+found folder in Linux and Unix?","219139","","<p>There is a folder at the root of Linux and Unix operating systems called <code>/lost+found/</code></p>

<p>What is it for? Under what circumstances would I interact with it? How would I interact with it?</p>
","<p>If you run <a href=""http://en.wikipedia.org/wiki/Fsck""><code>fsck</code></a>, the filesystem check and repair command, it might find data fragments that are not referenced anywhere in the filesystem. In particular, <code>fsck</code> might find data that looks like a complete file but doesn't have a name on the system — an <a href=""http://en.wikipedia.org/wiki/Inode"">inode</a> with no corresponding file name. This data is still using up space, but it isn't accessible by any normal means.</p>

<p>If you tell <code>fsck</code> to repair the filesystem, it will turn these almost-deleted files back into files. The thing is, the file had a name and location once, but that information is no longer available. So <code>fsck</code> deposits the file in a specific directory, called <code>lost+found</code> (after <a href=""http://en.wikipedia.org/wiki/Lost_and_found"">lost and found</a> property).</p>

<p>Files that appear in <code>lost+found</code> are typically files that were already unlinked (i.e. their name had been erased) but still opened by some process (so the data wasn't erased yet) when the system halted suddenly (kernel panic or power failure). If that's all that happened, these files were slated for deletion anyway, you don't need to care about them.</p>

<p>Files can also appear in <code>lost+found</code> because the filesystem was in an inconsistent state due to a software or hardware bug. If that's the case, it's a way for you to find files that were lost but that the system repair managed to salvage. The files may or may not contain useful data, and even if they do they may be incomplete or out of date; it all depends how bad the filesystem damage was.</p>

<p>On many filesystems, the <code>lost+found</code> directory is a bit special because it preallocates a bit of space for <code>fsck</code> to deposit files there. (The space isn't for the file data, which <code>fsck</code> leaves in place; it's for the directory entries which <code>fsck</code> has to make up.) If you accidentally delete <code>lost+found</code>, don't re-create it with <code>mkdir</code>, use <a href=""http://man7.org/linux/man-pages/man8/mklost+found.8.html""><code>mklost+found</code></a> if available.</p>
","18157"
"Delete First line of a file","216629","","<p>How can I delete the first line of a file and keep the changes?</p>

<p>I tried this but it erases the whole content of the file.</p>

<pre><code>$sed 1d file.txt &gt; file.txt
</code></pre>
","<p>The reason file.txt is empty after that command is the order in which the shell does things. The first thing that happens with that line is the redirection. The file ""file.txt"" is opened and truncated to 0 bytes. After that the sed command runs, but at the point the file is already empty. </p>

<p>There are a few options, most involve writing to a temporary file.</p>

<pre><code>sed '1d' file.txt &gt; tmpfile; mv tmpfile file.txt # POSIX
sed -i '1d' file.txt # GNU sed only, creates a temporary file

perl -ip -e '$_ = undef if $. == 1' file.txt # also creates a temporary file
</code></pre>
","96229"
"Can grep return true/false or are there alternative methods","216376","","<p>I'm trying to write this script for an assignment - it's only the second one I've written, so bear with me. </p>

<p>As a part of this script, I need to be able to check if the first argument given matches the first word of file. If it does, exit with an error message; if it doesn't, append the arguments to the file. I understand how to write the <code>if</code> statement, but not how to use <code>grep</code> within a script. I understand that <code>grep</code> will look something like this</p>

<pre><code>grep ^$1 schemas.txt
</code></pre>

<p>I feel like this should be much easier than I am making it.</p>

<p>I'm getting an error ""too many arguments"" on the <code>if</code> statement. I got rid of the space between <code>grep -q</code> and then got an error binary operator expected. I've been poking around at it, but I don't see what it sees.</p>

<pre><code>if [ grep -q ^$1 schemas.txt ]
then
        echo ""Schema already exists. Please try again""
        exit 1
else
        echo ""$@"" &gt;&gt; schemas.txt
fi
</code></pre>
","<p><code>grep</code> returns a different exit code if its found something (zero) vs. if it hasn't found something (non-zero). In an <code>if</code> statement, a zero exit code is mapped to ""true"" and a non-zero exit code is mapped to false. In addition, grep has a <code>-q</code> argument to not output the matched text (but only return the exit status code)</p>

<p>So, you can use grep like this:</p>

<pre class=""lang-shell prettyprint-override""><code>if grep -q PATTERN file.txt; then
    echo found
else
    echo not found
fi
</code></pre>

<p>As a quick note, when you do something like <code>if [ -z ""$var"" ]…</code>, it turns out that <code>[</code> is actually a command you're running, just like grep. On my system, it's <code>/usr/bin/[</code>. (Well, technically, your shell probably has it built-in, but that's an optimization. It behaves as if it were a command). It works the same way, <code>[</code> returns a zero exit code for true, a non-zero exit code for false. (<code>test</code> is the same thing as <code>[</code>, except for the closing <code>]</code>)</p>
","48536"
"How can I find available network interfaces?","215278","","<p>This is in regard to linux, but if anyone knows of a general *nix method that would be good.</p>

<p>I booted a system yesterday with an ethernet cable plugged in. ""NetworkManager"" is not installed, so once it started I went to look for the name of the ethernet interface with <code>ifconfig</code> to start a DHCP client manually, but it did not show anything other than <code>lo</code>.</p>

<p>The NIC was listed via <code>lspci</code>, and the appropriate kernel driver was loaded.  The system normally uses wifi, and I could remember the interface name for that was <code>wlan0</code>.  When I tried <code>ifconfig wlan0 up</code>, <code>wlan0</code> appeared.  But the only ethernet interface names I could remember were <code>eth[N]</code> and <code>em[N]</code> -- neither of which worked.</p>

<p><a href=""http://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/"">This document</a> refers to ""predictable interface names"" but does not do a good job of explaining what they might be in simple terms.  It does refer to a piece of source code which implies the name in this case might be deduced from the the PCI bus and slot numbers, which seems like an unnecessarily complicated hassle.</p>

<p>Other searching around led me to believe that this might be determined by <code>systemd</code> in conjunction with <code>udev</code>, but there are almost 100 files in <code>/usr/lib/udev/rules.d</code> and spending an hour trying to determine where (and <em>if</em>) there's a systemd config file for this also seems ridiculous.</p>

<p>It would also be nice to know for certain that they are available, not just how they might be named <em>if</em> they are, so I can rule out hardware problems, etc. <strong>Isn't there a simple way to find the names of available network interfaces on linux?</strong></p>
","<p>The simplest method I know to list all of your interfaces is</p>

<pre><code>ifconfig -a
</code></pre>

<p><strong>EDIT</strong></p>

<p>If you're on a system where that has been made obsolete, you can use</p>

<pre><code>ip link show
</code></pre>
","125406"
"How to run a specific program as root without a password prompt?","209794","","<p>I need to run something as sudo without a password, so I used <code>visudo</code> and added this to my <code>sudoers</code> file:</p>

<pre><code>MYUSERNAME ALL = NOPASSWD: /path/to/my/program
</code></pre>

<p>Then I tried it out:</p>

<pre><code>$ sudo /path/to/my/program
[sudo] password for MYUSERNAME: 
</code></pre>

<p>Why does it ask for a password? How can I run/use commands as root with a non-root user, without asking for a password?</p>
","<p>You have another entry in the <a href=""https://www.sudo.ws/man/sudoers.man.html"" rel=""noreferrer""><code>sudoers</code></a> file which also matches your user. The <code>NOPASSWD</code> rule needs to be after that one in order for it to take precedence.</p>

<p>Having done that, <code>sudo</code> will prompt for a password normally for all commands except <code>/path/to/my/program</code>, which it will always let you run without asking for your password.</p>
","18833"
"What is the difference between reboot , init 6 and shutdown -r now?","209303","","<p>I just want to know difference between in  </p>

<ul>
<li><code>reboot</code> </li>
<li><code>init 6</code></li>
<li><code>shutdown -r now</code></li>
</ul>

<p>and which is the safest and the best?</p>
","<p>There is no difference in them. Internally they do exactly the same thing:</p>

<ul>
<li><p><code>reboot</code> uses the <code>shutdown</code> command (with the -r switch). The shutdown command used to kill all the running processes, unmount all the file systems and finally tells the kernel to issue the ACPI power command. The source can be <a href=""http://util-linux.sourcearchive.com/documentation/2.14/shutdown_8c-source.html"">found here</a>.
In older distros the reboot command was forcing the processes to exit by issuing the <code>SIGKILL</code> signal (still found in sources, can be invoked with <code>-f</code> option), in most recent distros it defaults to the more graceful and init friendly <code>init 1 -&gt; shutdown -r</code>. This ensures that daemons clean up themselves before shutdown.</p></li>
<li><p><code>init 6</code> tells the <code>init</code> process to shutdown all of the spawned processes/daemons as written in the init files (in the inverse order they started) and lastly invoke the <code>shutdown -r now</code> command to reboot the machine</p></li>
</ul>

<p>Today there is not much difference as both commands do exactly the same, and they respect the init scripts used to start services/daemons by invoking the shutdown scripts for them. Except for <code>reboot -f -r now</code> as stated below</p>

<p>There is a small explanation taken from manpages of why the <code>reboot -f</code> is not safe:</p>

<pre>
  -f, --force
    Force immediate halt, power-off, reboot. Don't contact the init system.
</pre>

<p>Edit:</p>

<p>Forgot to mention, in upcoming RHEL distributions you should use the new <code>systemctl</code> command to issue poweroff/reboot. As stated in the manpages of <code>reboot</code> and <code>shutdown</code> they are ""a legacy command available for compatibility only."" and the <code>systemctl</code> method will be the only one safe.</p>
","64385"
"do changes in /etc/security/limits.conf require a reboot?","209258","","<p>do changes in <code>/etc/security/limits.conf</code> require a reboot before taking effect?  </p>

<p>Like if I have a script that sets the following limits in <code>/etc/security/limits.conf</code>, does this require me to reboot the system before those limits will take effect?</p>

<pre><code>* hard nofile 94000
* soft nofile 94000
* hard noproc 64000
* soft noproc 64000
</code></pre>
","<p>No but you should close all active sessions windows. They still remember the old values. In other words, log out and back in.
Every remote new session or a local secure shell take effect of the <em>limits</em> changes.</p>
","108605"
"Sort based on the third column","206384","","<p>I'm facing a huge 4-columns file. I'd like to display the sorted file in stdout based on its 3rd column: </p>

<pre><code>cat myFile | sort -u -k3
</code></pre>

<p>Is that enough to perform the trick?</p>
","<pre><code>sort -k 3,3 myFile
</code></pre>

<p>would display the file sorted by the 3<sup>rd</sup> column assuming the columns are separated by sequences of blanks (ASCII SPC and TAB characters in the POSIX/C locale), according to the sort order defined by the current locale.</p>

<p>Note that the leading blanks are <em>included</em> in the column (the default separator is the transition from a non-blank to a blank), that can make a difference in locales where spaces are not ignored for the purpose of comparison, use the <code>-b</code> option to ignore the leading blanks.</p>

<p>Note that it's completely independent from the shell (all the shells would parse that command line the same, shells generally don't have the <code>sort</code> command built in).</p>

<p><code>-k 3</code> is to sort on the portion of the lines starting with the 3<sup>rd</sup> column (including the leading blanks). In the C locale, because the space and tab characters ranks before all the printable characters, that will generally give you the same result as <code>-k 3,3</code> (except for lines that have an identical third field), </p>

<p><code>-u</code> is to retain only one of the lines if there are several that sort identically (that is where the sort key sorts the same (that's not necessarily the same as <em>being equal</em>)).</p>

<p><code>cat</code> is the command to con<strong>cat</strong>enate. You don't need it here.</p>

<p>If the columns are separated by something else, you need the <code>-t</code> option to specify the separator.</p>

<p>Given example file <code>a</code></p>

<pre><code>$ cat a
a c c c
a b ca d
a b  c e
a b c d
</code></pre>

<p>With <code>-u -k 3</code>:</p>

<pre><code>$ echo $LANG
en_GB.UTF-8

$ sort -u -k 3 a
a b ca d
a c c c
a b c d
a b  c e
</code></pre>

<p>Line 2 and 3 have the same third column, but here the sort key is from the third column to the end of line, so <code>-u</code> retains both. <code>␠ca␠d</code> sorts before <code>␠c␠c</code> because spaces are ignored in the first pass in my locale, <code>cad</code> sorts before <code>cc</code>.</p>

<pre><code>$ sort -u -k 3,3 a
a b c d
a b  c e
a b ca d
</code></pre>

<p>Above only one is retained for those where the 3rd column is <code>␠c</code>. Note how the one with <code>␠␠c</code> (2 leading spaces) is retained.</p>

<pre><code>$ sort -k 3 a
a b ca d
a c c c
a b c d
a b  c e
$ sort -k 3,3 a
a b c d
a c c c
a b  c e
a b ca d
</code></pre>

<p>See how the order of <code>a b c d</code> and <code>a c c c</code> are reversed. In the first case, because <code>␠c␠c</code> sorts before <code>␠c␠d</code>, in the second case because the sort key is the same (<code>␠c</code>), the last resort comparison that compares the lines in full puts <code>a b c d</code> before <code>a c c c</code>.</p>

<pre><code>$ sort -b -k 3,3 a
a b c d
a b  c e
a c c c
a b ca d
</code></pre>

<p>Once we ignore the blanks, the sort key for the first 3 lines is the same (<code>c</code>), so they are sorted by the last resort comparison.</p>

<pre><code>$ LC_ALL=C sort -k 3 a
a b  c e
a c c c
a b c d
a b ca d
$ LC_ALL=C sort -k 3,3 a
a b  c e
a b c d
a c c c
a b ca d
</code></pre>

<p>In the C locale, <code>␠␠c</code> sorts before <code>␠c</code> as there is only one pass there where characters (then single bytes) sort based on their code point value (where space has a lower code point than <code>c</code>).</p>
","104527"
"How to monitor only the last n lines of a log file?","205920","","<p>I have a growing log file for which I want to display <strong>only</strong> the last 15 lines. Here is what I know I can do:</p>

<pre><code>tail -n 15 -F mylogfile.txt
</code></pre>

<p>As the log file is filled, <code>tail</code> appends the last lines to the display.</p>

<p>I am looking for a solution that only displays the last 15 lines and get rid of the lines before the last 15 after it has been updated. Would you have an idea?</p>
","<p>It might suffice to use watch:</p>

<pre>
$ watch tail -n 15 mylogfile.txt
</pre>
","29459"
"ssh_exchange_identification: Connection closed by remote host (not using hosts.deny)","205351","","<p>I'm <strong>not</strong> using <code>hosts.allow</code> or <code>hosts.deny</code>, further more SSH works from my windows-machine (same laptop, different hard drive) but not my Linux machine.</p>

<p><code>ssh -vvv root@host -p port</code> gives:<br></p>

<pre><code>OpenSSH_6.6, OpenSSL 1.0.1f 6 Jan 2014
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 20: Applying options for *
debug2: ssh_connect: needpriv 0
debug1: Connecting to host [host] port &lt;port&gt;.
debug1: Connection established.
debug1: identity file /home/torxed/.ssh/id_dsa type -1
debug1: identity file /home/torxed/.ssh/id_dsa-cert type -1
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_6.6
ssh_exchange_identification: read: Connection reset by peer
</code></pre>

<p>On the windows machine everything works fine, so I checked the security logs and the lines in there are identical, the server treats the two different ""machines"" no different and they are both allowed via public-key authentication..</p>

<p>So that leads to the conclusion that this must be a issue with my local ArchLinux laptop.. but what?</p>

<pre><code>[torxed@archie ~]$ cat .ssh/known_hosts 
[torxed@archie ~]$ 
</code></pre>

<p>So that's not the problem..</p>

<pre><code>[torxed@archie ~]$ sudo iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination 
</code></pre>

<p>No conflicts with the firewall settings (for now)..</p>

<pre><code>[torxed@archie ~]$ ls -la .ssh/
total 20
drwx------  2 torxed users 4096 Sep  3  2013 .
drwx------ 51 torxed users 4096 May 11 11:11 ..
-rw-------  1 torxed users 1679 Sep  3  2013 id_rsa
-rw-r--r--  1 torxed users  403 Sep  3  2013 id_rsa.pub
-rw-r--r--  1 torxed users  170 May 11 11:21 known_hosts
</code></pre>

<p>Permissions appears to be fine (same on the server)..
Also tried without configuring <code>/etc/ssh/ssh_config</code> with the same result except for a lot of auto-configuration going on in the client which ends up with the same error.</p>
","<p>If you have ruled out any ""external"" factors, the following set of steps usually helps to narrow it down. So while this doesn't directly answer your question, it may help tracking down the error cause.</p>

<h2>Troubleshooting <code>sshd</code></h2>

<p>What I find generally very useful in any such cases is to start <code>sshd</code> without letting it daemonize. The problem in my case was that neither <code>syslog</code> nor <code>auth.log</code> showed anything meaningful.</p>

<p>When I started it from the terminal I got:</p>

<pre><code># $(which sshd) -Ddp 10222
/etc/ssh/sshd_config line 8: address family must be specified before ListenAddress.
</code></pre>

<p>Much better! This error message allowed me to see what's wrong and fix it. Neither of the log files contained this output.</p>

<p><strong>NB:</strong> at least on Ubuntu the <code>$(which sshd)</code> is the best method to satisfy <code>sshd</code> requirement of an absolute path. Otherwise you'll get the following error: <code>sshd re-exec requires execution with an absolute path</code>. The <code>-p 10222</code> makes <code>sshd</code> listen on that alternative port, overriding the configuration file - this is so that it doesn't clash with potentially running <code>sshd</code> instances. Make sure to choose a free port here.</p>

<p>Finally: connect to the alternative port (<code>ssh -p 10222 user@server</code>).</p>

<p>This method has helped me many many times in finding issues, be it authentication issues or other types. To get really verbose output to <code>stdout</code>, use <code>$(which sshd) -Ddddp 10222</code> (note the added <code>dd</code> to increase verbosity). For more debugging goodness check <code>man sshd</code>.</p>
","128910"
"crontab's @reboot only works for root?","205055","","<p><code>man 5 crontab</code> is pretty clear on how to use crontab to run a script on boot:</p>

<pre><code>   These special time specification ""nicknames"" are supported, which replace the 5 initial time and date
   fields, and are prefixed by the `@` character:
   @reboot    :    Run once after reboot.
</code></pre>

<p>So I happily added a single line to my crontab (under my user account, not root):</p>

<pre><code>@reboot     /home/me/myscript.sh
</code></pre>

<p>But for some reason, myscript.sh wouldn't run on machine reboot.
(it runs fine if I invoke it from the command line, so it's not a permissions problem)</p>

<p>What am I missing?</p>

<hr>

<p>Update to answer @Anthon's questions:</p>

<ol>
<li>Oracle-linux version: 5.8 (uname: 2.6.32-300.39.2.el5uek #1 SMP)</li>
<li>Cron version: vixie-cron-4.1-81.el5.x86_64</li>
<li>Yes, <code>/home</code> <strong>is</strong> a mounted partition. Looks like this is the problem. How do I workaround this?</li>
<li>Currently, <code>myscript.sh</code> only echos a text message to a file in <code>/home/me</code>.</li>
</ol>
","<p>This can be a bit of a confusing topic because there are different implementations of cron. Also there were several bugs that broke this feature, and there are also some use cases where it simply won't work, specifically if you do a shutdown/boot vs. a reboot.</p>

<h3>Bugs</h3>

<p><em>datapoint #1</em></p>

<p>One such bug in Debian is covered here, titled: <a href=""http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=635473"" rel=""noreferrer"">cron: @reboot jobs are not run</a>. This seems to have made it's way into Ubuntu as well, which I can't confirm directly. </p>

<p><em>datapoint #2</em></p>

<p>Evidence of the bug in Ubuntu would seem to be confirmed here in this SO Q&amp;A titled: <a href=""https://stackoverflow.com/questions/18158427/reboot-cronjob-not-executing"">@reboot cronjob not executing</a>.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>comment #1: .... 3) your version of crond may not support @reboot are you using vix's crond? ... show results of crontab -l -u user </p>
  
  <p>comment #2: ... It might be a good idea to set it up as an init script instead of relying on a specific version of cron's @reboot.</p>
  
  <p>comment #3: ... @MarkRoberts removed the reboot and modified the 1 * * * * , to */1 * * * * , problem is solved! Where do I send the rep pts Mark? Thank you!</p>
</blockquote>

<p>The accepted answer in that Q&amp;A also had this comment:</p>

<blockquote>
  <p>Seems to me Lubuntu doesn't support the @Reboot Cron syntax.</p>
</blockquote>

<h3>Additional evidence</h3>

<p><em>datapoint #3</em></p>

<p>As additional evidence there was this thread that someone was attempting the very same thing and getting frustrated that it didn't work. It's titled: <a href=""http://ubuntuforums.org/showthread.php?t=1530652"" rel=""noreferrer"">Thread: Cron - @reboot jobs not working</a>.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>Re: Cron - @reboot jobs not working</p>
  
  <blockquote>
    <p>Quote Originally Posted by ceallred  View Post
    This is killing me... Tried the wrapper script. Running manually generates the log file... rebooting and the job doesn't run or create log file.</p>
    
    <p>Syslog shows that CRON ran the job... but again, no output and the process isn't running.
    Jul 15 20:07:45 RavenWing cron[1026]: (CRON) INFO (Running @reboot jobs)
    Jul 15 20:07:45 RavenWing CRON[1053]: (ceallred) CMD (/home/ceallred/Scripts/run_spideroak.sh > /home/ceallred/Scripts/SpiderOak.log 2>&amp;1 &amp;)</p>
    
    <p>It's seems like cron doesn't like the @reboot command.... Any other ideas?</p>
  </blockquote>
  
  <p>Okay... Partially solved. I'll mark this one as solved and start a new thread with the new issue.....</p>
  
  <p>I think the answer was my encrypted home directory wasn't mounted when CRON was trying to run the script (stored in /home/username/scripts). Moved to /usr/scripts and the job runs as expected.</p>
  
  <p>So now it appears to be a spideroak issue. Process starts, but by the time the boot process is finished, it's gone. I'm guessing a crash for some reason.... New thread to ask about that.</p>
  
  <p>Thanks for all the help!</p>
</blockquote>

<p>Once this above user figured out his issue he was able to get <code>@reboot</code> working out of the crontab entry of a user. </p>

<p>I'm not entirely sure what version of cron is used on Ubuntu, but this would seem to indicate that user's can use <code>@reboot</code> too, or that the bug was fixed at some point in subsequent versions of cron.</p>

<p><em>datapoint #4</em></p>

<p>I tested on CentOS 6 the following and it worked.</p>

<h3>Example</h3>

<pre><code>$ crontab -l
@reboot echo ""hi"" &gt; /home/sam/reboot.txt 2&gt;&amp;1
</code></pre>

<p>I then rebooted the system.</p>

<pre><code>$ sudo reboot
</code></pre>

<p>After the reboot.</p>

<pre><code>$ cat reboot.txt 
hi
</code></pre>

<h3>Take aways</h3>

<ol>
<li>This feature does seem to be supported for both system and user crontab entries. </li>
<li>You have to make sure that it's supported/working in your particular distro and/or version of the cron package.</li>
</ol>

<p>For more on how the actual mechanism works for <code>@reboot</code> I did come across this blog post which discusses the innards. It's titled: <a href=""http://www.unixdaemon.net/linux/how-does-cron-reboot-work.html"" rel=""noreferrer"">@reboot - explaining simple cron magic</a>.</p>

<h3>Debugging crond</h3>

<p>You can turn up the verbosity of <code>crond</code> by adding the following to this configuration file on RHEL/CentOS/Fedora based distros.</p>

<pre><code>$ more crond 
# Settings for the CRON daemon.
# CRONDARGS= :  any extra command-line startup arguments for crond
CRONDARGS=""-L 2""
</code></pre>

<p>The valid levels are 0, 1, or 2. To revert this file back to it's default logging level simply remove the <code>""-L 2""</code> when you're done debugging the situation.</p>
","109805"
"What do the fields in ls -al output mean?","204893","","<p>The <code>ls -al</code> command shows the following output;</p>

<pre><code>-rwxrw-r--    10    root   root 2048    Jan 13 07:11 afile.exe
</code></pre>

<p>What are all the fields in the preceding display?</p>
","<p>In the order of output;</p>

<pre><code>-rwxrw-r--    1    root   root 2048    Jan 13 07:11 afile.exe
</code></pre>

<ul>
<li>file permissions, </li>
<li>number of links, </li>
<li>owner name, </li>
<li>owner group, </li>
<li>file size, </li>
<li>time of last modification, and </li>
<li>file/directory name</li>
</ul>

<p>File permissions is displayed as following;</p>

<ul>
<li>first character is <code>-</code> or <code>l</code> or <code>d</code>, d indicates a directory, a line represents a file, l is a symlink (or soft link) - special type of file</li>
<li>three sets of characters, three times, indicating permissions for owner, group and other:

<ul>
<li>r = readable</li>
<li>w = writable</li>
<li>x = executable</li>
</ul></li>
</ul>

<p>In your example <code>-rwxrw-r--</code>, this means the line displayed is:</p>

<ul>
<li>a regular file (displayed as -)</li>
<li>readable, writable and executable by owner (rwx)</li>
<li>readable, writable, but not executable by group (rw-)</li>
<li>readable but not writable or executable by other (r--)</li>
</ul>
","103118"
"What does aux mean in `ps aux`?","204527","","<p><code>ps aux</code> seems to conveniently list all processes and their status and resource usage (Linux/BSD/MacOS), however I cannot comprehend the meaning of parameter <code>aux</code> using <code>man ps</code>.</p>

<p>What does <code>aux</code> mean?</p>
","<p>a = show processes for all users<br>
u = display the process's user/owner<br>
x = also show processes not attached to a terminal</p>

<p>By the way, <code>man ps</code> is a good resource.</p>

<p>Historically, BSD and AT&amp;T developed incompatible versions of <code>ps</code>.  The options without a leading dash (as per the question) are the BSD style while those with a leading dash are AT&amp;T Unix style.  On top of this, Linux developed a version which supports both styles and then adds to it a third style with options that begin with double dashes.</p>

<p>All (or nearly all) non-embedded Linux distributions use a variant of the <a href=""http://procps.sourceforge.net/"">procps</a> suite.  The above options are as defined in the <a href=""http://linux.die.net/man/1/ps"">procps <code>ps</code> man page</a>.</p>

<p>In the comments, you say you are using Apple MacOS (OSX, I presume).  The OSX man page for <code>ps</code> is <a href=""https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/ps.1.html"">here</a> and it shows support only for AT&amp;T style.</p>
","106848"
"In Linux ""top"" command what are us, sy, ni, id, wa, hi, si and st (for CPU usage)?","203861","","<p>When I issue <code>$ top</code> in linux, I get a result similar to this (<a href=""https://i.stack.imgur.com/ijaSR.jpg"">see here</a>).</p>

<p>One of the lines has CPU usage information represented like this:</p>

<pre><code>Cpu(s): 87.3%us,  1.2%sy,  0.0%ni, 27.6%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
</code></pre>

<p>While I know the definitions of each of them (far below), I don't understand what these tasks exactly mean.</p>

<ul>
<li>hi - what does servicing hardware interrupts mean?</li>
<li>si - what does servicing software interrupts mean?</li>
<li>st - they say it's the time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) Time stolen from a virtual machine. But what does it actually mean? can someone be more clear?</li>
</ul>

<p>{I listed all (us, sy, ni etc) because it would help others searching for the same. This information is not in the man pages.}</p>

<pre><code>us: user cpu time (or) % CPU time spent in user space
sy: system cpu time (or) % CPU time spent in kernel space
ni: user nice cpu time (or) % CPU time spent on low priority processes
id: idle cpu time (or) % CPU time spent idle
wa: io wait cpu time (or) % CPU time spent in wait (on disk)
hi: hardware irq (or) % CPU time spent servicing/handling hardware interrupts
si: software irq (or) % CPU time spent servicing/handling software interrupts
st: steal time - - % CPU time in involuntary wait by virtual cpu while hypervisor is servicing another processor (or) % CPU time stolen from a virtual machine
</code></pre>
","<p><code>hi</code> is the time spent processing hardware interrupts. Hardware interrupts are generated by hardware devices (network cards, keyboard controller, external timer, hardware senors, ...) when they need to signal something to the CPU (data has arrived for example).</p>

<p>Since these can happen very frequently, and since they essentially block the current CPU while they are running, kernel hardware interrupt handlers are written to be as fast and simple as possible.</p>

<p>If long or complex processing needs to be done, these tasks are deferred using a mechanism call <code>softirqs</code>. These are scheduled independently, can run on any CPU, can even run concurrently (non of that is true of hardware interrupt handlers).</p>

<p><sub>The part about hard IRQs blocking the current CPU, and the part about softirqs being able to run anywhere are not exactly correct, there can be limitations, and some hard IRQs can interrupt others.
</sub></p>

<p>(As an example, a ""data received"" hardware interrupt from a network card could simply store the information ""card ethX needs to be serviced"" somewhere and schedule a softirq. The softirq would be the thing that triggers the actual packet routing.)</p>

<p><code>si</code> represents the time spent in these softirqs.</p>

<p>A good read about the softirq mechanism (with a bit of history too) is Matthew Wilcox's <a href=""http://www.cs.unca.edu/brock/classes/Spring2013/csci331/notes/paper-1130.pdf"" rel=""noreferrer"">I'll Do It Later: Softirqs, Tasklets, Bottom Halves, Task Queues,
Work Queues and Timers</a> (PDF, 64k). </p>

<p><code>st</code>, ""steal time"", is only relevant in virtualized environments. It represents time when the real CPU was not available to the current virtual machine - it was ""stolen"" from that VM by the hypervisor (either to run another VM, or for its own needs).</p>

<p>The <a href=""http://www.ibm.com/developerworks/linux/linux390/perf/tuning_cputimes.html"" rel=""noreferrer"">CPU time accounting </a> document from IBM has more information about steal time, and CPU accounting in virtualized environments. (It's aimed at zSeries type hardware, but the general idea is the same for most platforms.)</p>
","18920"
"How to see process created by specific user in Unix/linux","203432","","<p>I want to see list  of  process created by specific user or group of user in Linux
Can I do it using <code>ps</code> command or is there any other command to achieve this?</p>
","<p>To view only the processes owned by a specific user, use the following command:</p>

<pre><code>top -U [username]
</code></pre>

<p>Replace the [username] with the required username</p>

<p>If you want to use ps then</p>

<pre><code>ps -u [username]
</code></pre>

<p>OR</p>

<pre><code> ps -ef | grep &lt;username&gt;
</code></pre>

<p>OR</p>

<pre><code>ps -efl | grep &lt;username&gt;
</code></pre>

<p>for the extended listing</p>

<p><strong>Check out the man ps page for options</strong></p>

<p>Another alternative is to use pstree wchich prints the process tree of the user</p>

<pre><code>pstree &lt;username or pid&gt;
</code></pre>
","85467"
"How to know number of cores of a system in Linux?","202697","","<p>I wanted to find out how many cores my system has. So I searched the same question in google. I got some commands such as:
$ <strong><em>lscpu</em></strong> command.
When I tried this command, it gave me the following result:
<a href=""https://i.stack.imgur.com/tVcHt.jpg""><img src=""https://i.stack.imgur.com/tVcHt.jpg"" alt=""enter image description here""></a></p>

<p>In the picture it says </p>

<ul>
<li>CPU(s)             : 4</li>
<li>Core(s) per socket : 4</li>
<li>CPU family         : 6</li>
</ul>

<p>Now I am confused that 
Which of those indicates cores of a Linux system?</p>

<p>Is there any other command to tell the number of cores?
or am I assuming it is completely wrong?</p>
","<p>You have to look at sockets and cores per socket. In this case you have 1 physical CPU (socket) which has 4 cores (cores per socket).</p>
","218081"
"How to get IP Address using shell script?","202021","","<p>Want to get the IP address using Shell script. Without knowing the eth0 or eth1 or eth2 How to get the particular IP address.</p>

<p>I am not interest to get localhost address, want to get <strong>private IP address</strong>.</p>
","<p>If you want list all ip address, regardless its name, try this:</p>

<pre><code>ifconfig | perl -nle 's/dr:(\S+)/print $1/e'
</code></pre>

<p>or:</p>

<pre><code>ifconfig | awk '/inet addr/{print substr($2,6)}'
</code></pre>
","119271"
"mount.nfs: access denied by server while mounting on Ubuntu machines?","201903","","<p>I am a linux newbie and I have a very basic question. I have three machines in production - </p>

<pre><code>machineA    10.66.136.129
machineB    10.66.138.181
machineC    10.66.138.183
</code></pre>

<p>and all those machines have Ubuntu 12.04 installed in it and I have root access to all those three machines.</p>

<p>Now I am supposed to do below things in my above machines - </p>

<pre><code>Create mount point /opt/exhibitor/conf
Mount the directory in all servers.
 sudo mount &lt;NFS-SERVER&gt;:/opt/exhibitor/conf /opt/exhibitor/conf/
</code></pre>

<p>I have already created <code>/opt/exhibitor/conf</code> directory in all those three machines as mentioned above. </p>

<p>Now I am trying to create a Mount Point. So I followed the below process - </p>

<p>Install NFS support files and NFS kernel server in all the above three machines</p>

<pre><code>$ sudo apt-get install nfs-common nfs-kernel-server
</code></pre>

<p>Create the shared directory in all the above three machines</p>

<pre><code>$ mkdir /opt/exhibitor/conf/
</code></pre>

<p>Edited the <code>/etc/exports</code> and added the entry like this in all the above three machines - </p>

<pre><code># /etc/exports: the access control list for filesystems which may be exported
#               to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#
/opt/exhibitor/conf/     10.66.136.129(rw)
/opt/exhibitor/conf/     10.66.138.181(rw)
/opt/exhibitor/conf/     10.66.138.183(rw)
</code></pre>

<p>I have tried mounting on machineA like below from machineB and machineC and it gives me this error- </p>

<pre><code>root@machineB:/# sudo mount -t nfs 10.66.136.129:/opt/exhibitor/conf /opt/exhibitor/conf/
mount.nfs: access denied by server while mounting 10.66.136.129:/opt/exhibitor/conf

root@machineC:/# sudo mount -t nfs 10.66.136.129:/opt/exhibitor/conf /opt/exhibitor/conf/
mount.nfs: access denied by server while mounting 10.66.136.129:/opt/exhibitor/conf
</code></pre>

<p>Did my <code>/etc/exports</code> file looks good? I am pretty sure, I have messed up my <code>exports</code> file. As I have the same content in all the three machines in exports file.</p>

<p>Any idea what wrong I am doing here? And what will be the correct <code>/exports</code> file here?</p>
","<h3>exportfs</h3>

<p>When you create a <code>/etc/exports</code> file on a server you need to make sure that you export it. Typically you'll want to run this command:</p>

<pre><code>$ exportfs -a
</code></pre>

<p>This will export all the entries in the exports file.</p>

<h3>showmount</h3>

<p>The other thing I'll often do is from other machines I'll check any machine that's exporting NFS shares to the network using the <code>showmount</code> command.</p>

<pre><code>$ showmount -e &lt;NFS server name&gt;
</code></pre>

<h3>Example</h3>

<p>Say for example I'm logged into scully.</p>

<pre><code>$ showmount -e mulder
Export list for mulder:
/export/raid1/isos     192.168.1.0/24
/export/raid1/proj     192.168.1.0/24
/export/raid1/data     192.168.1.0/24
/export/raid1/home     192.168.1.0/24
/export/raid1/packages 192.168.1.0/24
</code></pre>

<h3>fstab</h3>

<p>To mount these upon boots you'd add this line to your client machines that want to consume the NFS mounts.</p>

<pre><code>server:/shared/dir /opt/mounted/dir nfs rsize=8192,wsize=8192,timeo=14,intr
</code></pre>

<h3>automounting</h3>

<p>If you're going to be rebooting these servers then I highly suggest you look into setting up automounting (<code>autofs</code>) instead of adding these entries to <code>/etc/fstab</code>. It's a bit more work but is well worth the effort. </p>

<p>Doing so will allow you to reboot the servers more independently from one another and also will only create the NFS mount when it's actually needed and/or being used. When it goes idle it will get unmounted.</p>

<h3>References</h3>

<ul>
<li><a href=""http://www.centos.org/docs/5/html/Deployment_Guide-en-US/s1-nfs-client-config.html"">18.2. NFS Client Configuration - CentOS 5 Deployment Guide</a></li>
</ul>
","106124"
"List of available services","200673","","<p>Is there any command that would show all the <strong>available services</strong> in my wheezy Debian based OS?</p>

<p>I know that in order to see all the <strong>running services</strong> you can use <code>service --status-all</code>.</p>
","<p>Wheezy uses SysV init, and all the services are controlled with special shell scripts in <code>/etc/init.d</code>, so <code>ls /etc/init.d</code> will list them.  These files also contain a description of the service at the top, and the directory contains a <code>README</code>.</p>

<p>Some but not all of them have a <code>.sh</code> suffix, you should leave that off when using, eg., <code>update-rc.d</code>.</p>
","108593"
"Keeping a process running after PuTTY or terminal has been closed","200135","","<p>I'm running a <a href=""http://en.wikipedia.org/wiki/Node.js"" rel=""noreferrer"">Node.js</a> server off of a Raspbian (Debian) machine, and I'd like to start and stop the server remotely. This for me means using <a href=""http://en.wikipedia.org/wiki/PuTTY"" rel=""noreferrer"">PuTTY</a> to access the shell, except when I close out of the PuTTY terminal or it times out, my server goes down with it, because I just execute my server in the foreground.</p>

<p>Is there a way to keep it going, but still have a way to kill the process afterwards?</p>
","<p>Your question was a little lacking in details, so I'm assuming that you mean that you typed the command to start your server on the console of your Pi, and it executed in the foreground.</p>

<p>If this is the case, you have five options, ordered by complexity to implement:</p>

<ol>
<li><p>Use <a href=""https://unix.stackexchange.com/a/89487/29146"">@f-tussel's answer</a>. Since you're new to GNU/Linux, the <code>&amp;</code> symbol tells the shell that it should execute the process in the background and return you to the prompt immediately, instead of what it normally does (which is wait for the command to finish before returning you to the prompt). This is technically called forking the command to the background.</p></li>
<li><p>Do what you did before, but do it in a <code>screen</code> process. Basically this entails installing <code>screen</code> (<code>sudo apt-get install screen</code> on your Debian system), and then sometime before you type the command to start your server, you execute <code>screen</code>. This opens a new shell that you can then reconnect to later, even if your PuTTY connection dies. So it will act as if you've never disconnected.</p>

<p>If you're unfamiliar with <code>screen</code>, you may want to do some reading on <a href=""https://en.wikipedia.org/wiki/GNU_Screen"" rel=""noreferrer"">Wikipedia</a> and <a href=""http://manpages.ubuntu.com/manpages/raring/en/man1/screen.1.html"" rel=""noreferrer"">in the manpages</a>. You can also accomplish this same thing with <code>tmux</code>.</p></li>
<li><p>Use the <a href=""https://npmjs.org/package/forever"" rel=""noreferrer"">forever</a> node.js module. See <a href=""https://stackoverflow.com/questions/4797050/how-to-run-process-as-background-and-never-die"">https://stackoverflow.com/questions/4797050/how-to-run-process-as-background-and-never-die</a> for where I got this.</p></li>
<li><p>Put your server in a <code>screen</code> process in the background. This means that you'll create a new <code>screen</code> session in the background but never attach to it. And, instead of running a shell, the screen process will be running your server. Here's what you'll type:</p>

<pre><code>screen -d -m exec_your_server --put-args-here
</code></pre>

<p>If you like, you can make this run at boot. Basically you need to put the screen command in the file <code>/etc/rc.local</code> or <code>/etc/rc.d/rc.local</code>, I forget which. If you run into trouble doing this, ask a new question.</p>

<p>Again, you can do this with <code>tmux</code> too.</p></li>
<li><p>Write a service script. Since you're on Debian and are new, you're presumably using the default <code>init</code> that Debian provides, which is System V Init. I've never looked at service files for System V Init, only systemd and a little Upstart, so I can't help you here. Ask a new question if you want to pursue this.</p>

<p>This is the least ""hacky"" way, IMHO, and this is what you should consider doing if you're running your server long-term, as you can then manage it like other services on the system through commands like <code>sudo service your_server stop</code>, etc. Doing it this way will start your server at boot automatically, and you don't need <code>screen</code> because it also automatically happens in the background.</p>

<p>It also automatically executes as root, which is dangerous - you should put logic in your server to drop the privileges that you have by becoming an unprivileged user that you have created specifically for the server. (This is in case the server gets compromised - imagine if someone could run things as root, through your server! Eugh. <a href=""https://serverfault.com/questions/57962/whats-wrong-with-always-being-root"">This question</a> does an OK job of talking about this.)</p></li>
</ol>
","89493"
"GPU usage monitoring (CUDA)","198964","","<p>I installed CUDA toolkit on my computer and started BOINC project on GPU. In BOINC I can see that it is running on GPU, but is there a tool that can show me more details about that what is running on GPU - GPU usage and memory usage?</p>
","<p>For Nvidia GPUs there is a tool <code>nvidia-smi</code> that can show memory usage, GPU utilization and temperature of GPU. There also is a list of compute processes and few more options but my graphic card (GeForce 9600 GT) is not fully supported.</p>

<pre><code>Sun May 13 20:02:49 2012       
+------------------------------------------------------+                       
| NVIDIA-SMI 3.295.40   Driver Version: 295.40         |                       
|-------------------------------+----------------------+----------------------+
| Nb.  Name                     | Bus Id        Disp.  | Volatile ECC SB / DB |
| Fan   Temp   Power Usage /Cap | Memory Usage         | GPU Util. Compute M. |
|===============================+======================+======================|
| 0.  GeForce 9600 GT           | 0000:01:00.0  N/A    |       N/A        N/A |
|   0%   51 C  N/A   N/A /  N/A |  90%  459MB /  511MB |  N/A      Default    |
|-------------------------------+----------------------+----------------------|
| Compute processes:                                               GPU Memory |
|  GPU  PID     Process name                                       Usage      |
|=============================================================================|
|  0.           Not Supported                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
","38581"
"how to connect ssh 'with' specified port?","198344","","<p>I know how to connect 'to' a certain port when ssh'ing.</p>

<pre><code>ssh user@remotehostip -p XXX
</code></pre>

<p>but is there a way to establish a ssh 'with' a certain port?</p>

<p>I mean, is there a way to specify the port which my local computer will be using?</p>
","<p>That is not easily possible. How that can be done depends on where the source port shall be seen: Locally, too, or is it enough if it's the right port from the perspective of the external network?</p>

<p>You can run the SSH client in an LXC container. I have never done that thus I cannot explain it to you in detail. But you create a virtual network interface and attach it to this container so that <code>ssh</code> uses this interface because it is the only (external) interface it sees.</p>

<p>On the host system it should be possible to detect that a packet comes from this interface. Thus you can use Netfilter's NAT (SNAT) for rewriting the source address with something like:</p>

<pre><code>iptables -t nat -A POSTROUTING -o vnet0 -p tcp --dport 22 -j SNAT --to-source :1234
</code></pre>

<p>Of course, this does not work (or becomes more complicated) if you connect to ports different from 22.</p>
","138864"
"How to get the pid of the last executed command in shell script?","198198","","<p>I want to have a shell script like this:</p>

<pre><code>my-app &amp;
echo $my-app-pid
</code></pre>

<p>But I do not know how the get the pid of the just executed command.</p>

<p>I know I can just use the <code>jobs -p my-app</code> command to grep the pid. But if I want to execute the shell multiple times, this method will not work. Because the <em>jobspec</em> is ambiguous.</p>
","<p>It is in the <a href=""https://www.gnu.org/software/bash/manual/html_node/Special-Parameters.html#index-_0021-1""><code>!</code></a> shell variable:</p>

<pre><code>my-app &amp;
echo $!
</code></pre>
","30371"
"Viewing all iptables rules","197974","","<p>I just wanted to ask if there's a way to view iptables rules in a bit more detail</p>

<p>I recently added masquerade to a range of ips</p>

<pre><code>iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE
service iptables save
service iptables restart
</code></pre>

<p>Which has done what I want it to but when I use</p>

<pre><code>iptables -L
</code></pre>

<p>I get the same output as I normally get</p>

<pre><code>Chain INPUT (policy ACCEPT)
target    prot opt source        destination

Chain FORWARD (policy ACCEPT)
target    prot opt source        destination

Chain OUTPUT (policy ACCEPT)
target    prot opt source        destination
</code></pre>

<p>How can I see the rules including the ones I add? (system is CentOS 6)</p>
","<p>You just need to specify the appropriate Netfilter table (one of <code>filter</code>, <code>nat</code>, <code>mangle</code>, <code>raw</code> or <code>security</code>). If you’ve added a rule for the <code>nat</code> table, you should explicitly specify this table using the <code>-t, --table</code> option:</p>

<pre><code>iptables --table nat --list
</code></pre>

<p>Or using the options short form:</p>

<pre><code>iptables -t nat -L
</code></pre>

<p>If you don’t specify a specific table, the <code>filter</code> table is used as the default. You can get even more information by including the <code>-v</code> or <code>--verbose</code> option.</p>
","205871"
"Execute vs Read bit. How do directory permissions in Linux work?","195631","","<p>In my CMS, I noticed that directories need the executable bit (<code>+x</code>) set for the user to open them. Why is the execute permission required to read a directory, and how do directory permissions in Linux work?</p>
","<p>When applying permissions to directories on Linux, the permission bits have different meanings than on regular files.</p>

<ul>
<li>The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory's attributes</li>
<li>The read bit allows the affected user to list the files within the directory</li>
<li>The execute bit allows the affected user to enter the directory, and access files and directories inside</li>
<li>The sticky bit states that files and directories within that directory may only be deleted or renamed by their owner (or root)</li>
</ul>
","21252"
"How to change primary group?","195334","","<p>I have a user like this:</p>

<pre><code>uid=501(ironsand) gid=500(foo) groups=500(foo),10(wheel),497(git),501(ironsand)
</code></pre>

<p>And to change primary group to ironsand like <code>gid=501(ironsand)</code>, I typed this command:</p>

<pre><code>sudo usermod -g ironsand ironsand
</code></pre>

<p>It changed groups order but didn't change main group like:</p>

<pre><code>uid=501(ironsand) gid=500(foo) groups=501(ironsand),10(wheel),497(git),500(foo)
</code></pre>

<p>I thought someone already asked a question like this, but I couldn't find one.</p>

<p>How can I change primary group?</p>
","<p>Usually you do it like the following.</p>

<p>To assign a primary group to an user:</p>

<pre><code>$ usermod -g primarygroupname username
</code></pre>

<p>To assign secondary groups to a user (<code>-a</code> keeps already existing secondary groups intact otherwise they'll be removed):</p>

<pre><code>$ usermod -a -G secondarygroupname username
</code></pre>

<p>From man-page:</p>

<pre><code>...
-g (primary group assigned to the users)
-G (Other groups the user belongs to)
-a (Add the user to the supplementary group(s))
...
</code></pre>
","151123"
"How can we change root password?","195311","","<p>If we don't know the root password and don't have root access to the machine, how can we change the root password?</p>
","<p>Here are a few ways I can think of, from the least intrusive to the most intrusive.</p>

<h1>Without Rebooting</h1>

<p><strong>With sudo:</strong> if you have <code>sudo</code> permissions to run <code>passwd</code>, you can do:</p>

<pre><code>sudo passwd root
</code></pre>

<p>Enter <em>your</em> password, then enter a new password for root twice. Done.</p>

<p><strong>Editing files</strong>: this works in the unlikely case you don't have full <code>sudo</code> access, but you <em>do</em> have access to edit <code>/etc/{passwd,shadow}</code>. Open <code>/etc/shadow</code>, either with <code>sudoedit /etc/shadow</code>, or with <code>sudo $EDITOR /etc/shadow</code>. Replace root's password field (all the random characters between the second and third colons <code>:</code>) with your own user's password field. Save. The local has the same password as you. Log in and change the password to something else.</p>

<p>These are the easy ones.</p>

<h1>Reboot Required</h1>

<p><strong>Single User mode</strong>: This was just explained by Renan. It works if you can get to GRUB (or your boot loader) and you can edit the Linux command line. It doesn't work if you use Debian, Ubuntu, and some others. Some boot loader configurations require a password to do so, and you must know that to proceed. Without further ado:</p>

<ol>
<li>Reboot.</li>
<li>Enter boot-time password, if any.</li>
<li>Enter your boot loader's menu.</li>
<li>If single user mode is available, select that (Debian calls it ‘Recovery mode’).</li>
<li>If not, and you run GRUB:
<ol>
<li>Highlight your normal boot option.</li>
<li>Press <kbd>e</kbd> to enter edit mode. You may be asked for a GRUB password there.</li>
<li>Highlight the line starting with <code>kernel</code> or <code>linux</code>.</li>
<li>Press <kbd>e</kbd>.</li>
<li>Add the word ‘single’ at the end. (don't forget to prepend a space!)</li>
<li>Press <kbd>Enter</kbd> and boot the edited stanza. Some GRUBs use <kbd>Ctrl</kbd>-<kbd>X</kbd>, some use <kbd>b</kbd>. It says which one it is at the bottom of the screen.</li>
</ol></li>
</ol>

<p>Your system will boot up in single user mode. Some distributions won't ask you for a root password at this point (Debian and Debian-based ones do). You're root now. Change your password:</p>

<pre class=""lang-sh prettyprint-override""><code>mount / -o remount,rw
passwd # Enter your new password twice at the prompts
mount / -o remount,ro
sync # some people sync multiple times. Do what pleases you.
reboot
</code></pre>

<p>and <code>reboot</code>, or, if you know your normal runlevel, say <code>telinit 2</code> (or whatever it is).    </p>

<p><strong>Replacing <code>init</code></strong>: superficially similar to the single user mode trick, with largely the same instructions, but requires much more prowess with the command line. You boot your kernel as above, but instead of <code>single</code>, you add <code>init=/bin/sh</code>. This will run <code>/bin/sh</code> in place of <code>init</code>, and will give you a <strong>very</strong> early shell with almost no amenities. At this point your aim is to:</p>

<ol>
<li>Mount the root volume.</li>
<li>Get <code>passwd</code> running.</li>
<li>Change your password with the <code>passwd</code> command.</li>
</ol>

<p>Depending on your particular setup, these may be trivial (identical to the instructions for single user mode), or highly non-trivial: loading modules, initialising software RAID, opening encrypted volumes, starting LVM, et cetera. Without <code>init</code>, you aren't running dæmons or any other processes but <code>/bin/sh</code> and its children, so you're pretty literally on your own. You also don't have job control, so be careful what you type. One misplaced <code>cat</code> and you may have to reboot if you can't get out of it.</p>

<p><strong>Rescue Disk</strong>: this one's easy. Boot a rescue disk of your choice. Mount your root filesystem. The process depends on how your volumes are layered, but eventually boils down to:</p>

<pre class=""lang-sh prettyprint-override""><code> # do some stuff to make your root volume available.
 # The rescue disk may, or may not do it automatically.
 mkdir /tmp/my-root
 mount /dev/$SOME_ROOT_DEV /tmp/my-root
 $EDITOR /tmp/my-root/etc/shadow
 # Follow the `/etc/shadow` editing instructions near the top
 cd /
 umount /tmp/my-root
 reboot
</code></pre>

<p>Obviously, <code>$SOME_ROOT_DEV</code> is whatever block device name is assigned to your root filesystem by the rescue disk and <code>$EDITOR</code> is your favourite editor (which may have to be <code>vi</code> on the rescue system). After the <code>reboot</code>, allow the machine to boot normally; root's password will be that of your own user. Log in as root and change it immediately.</p>

<h1>Other Ways</h1>

<p>Obviously, there are countless variations to the above. They all boil down to two steps:</p>

<ol>
<li>Get root access to the computer (catch-22 — and the real trick)</li>
<li>Change root's password somehow.</li>
</ol>
","35938"
"How can I change the default gateway?","195275","","<p>Currently I'm running a FreeBSD 9.1 and the default gateway is already configured in the <code>rc.conf</code>.</p>

<p><code>rc.conf</code>:</p>

<pre><code>defaultrouter = ""10.0.0.1""
</code></pre>

<p>But now I want to change the default gateway without rebooting the system, is this possible?</p>
","<pre><code>route del default
route add default 1.2.3.4
</code></pre>

<p>Where <code>1.2.3.4</code> is the new gateway. You can even concatenate them onto the same line with a <code>;</code></p>

<p>Edit: This is FreeBSD, <strong>not</strong> Linux. The command is different. Please do not edit this Answer if you haven't read the Question carefully enough to determine the operating system being used.</p>
","87546"
"remove particular characters from a variable using bash","194158","","<p>I want to parse a <strong>variable</strong> (in my case it's development kit version) to make it dot(<code>.</code>) free. If <code>version='2.3.3'</code>, desired output is <code>233</code>.</p>

<p>I tried as below, but it requires <code>.</code> to be replaced with another character giving me <code>2_3_3</code>. It would have been fine if <code>tr . ''</code> would have worked.</p>

<pre><code>  1 VERSION='2.3.3' 
  2 echo ""2.3.3"" | tr . _
</code></pre>
","<p>There is no need to execute an external program. <code>bash</code>'s <a href=""http://tldp.org/LDP/abs/html/string-manipulation.html"">string manipulation</a> can handle it (also available in <code>ksh</code> (where it comes from) and <code>zsh</code>):</p>

<pre><code>VERSION='2.3.3'
echo ""${VERSION//.}""
</code></pre>

<p>(In those shells' manuals you can find this in the parameter expansion section.)</p>
","104887"
"how to concatenate string variables into a third?","194058","","<p>I need to concatenate two strings in bash, so that:</p>

<pre><code>string1=hello
string2=world

mystring=string1+string2
</code></pre>

<p><code>echo mystring</code> should produce</p>

<blockquote>
  <p>helloworld</p>
</blockquote>
","<p>simply concatenate the variables:</p>

<pre><code>mystring=""$string1$string2""
</code></pre>
","10264"
"How to show the filesystem type via the terminal?","193396","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/34623/how-to-tell-what-type-of-filesystem-youre-on"">How to tell what type of filesystem you’re on?</a><br>
  <a href=""https://unix.stackexchange.com/questions/43237/find-filesystem-of-an-unmounted-partition-from-a-script"">Find filesystem of an unmounted partition from a script</a>  </p>
</blockquote>



<p>How can I quickly check the filesystem of the partition? Can I do that by using <code>df</code>?</p>
","<p>Yes, according to man <code>df</code> you can:</p>

<blockquote>
<pre><code>-T, --print-type      print file system type
</code></pre>
</blockquote>

<p>Another way is to use the <code>mount</code> command. Without parameters it lists the currently mounted devices, including their file systems.</p>

<p>In case you need to find out only one certain file system, is easier to use the <code>stat</code> command's <code>-f</code> option instead of parsing out one value from the above mentioned commands' output.</p>
","53314"
"Can I set up system mail to use an external SMTP server?","192735","","<p>Is it possible to set up system mail on a linux box to be sent via a different smtp server - maybe even with authentication? If so, how do I do this?</p>

<p>If that's unclear, let give an example. If I'm at the command line and type:</p>

<pre><code>cat body.txt | mail -s ""just a test"" myfriend@hisdomain.com
</code></pre>

<p>is it possible to have that be sent via an external SMTP server, like G-mail ?</p>

<p>I'm not looking for ""a way to send mail from gmail from the command line"" but rather an option to configure the entire system to use a specific SMTP server, or possibly one account on an SMTP server (maybe overriding the from address).</p>
","<p>I found sSMTP very simple to use.</p>

<p>In Debian based systems:</p>

<pre><code>apt-get install ssmtp
</code></pre>

<p>Then edit the configuration file in /etc/ssmtp/ssmtp.conf</p>

<p>A sample configuration to use your gmail for sending e-mails:</p>

<pre><code># root is the person who gets all mail for userids &lt; 1000
root=your@email.com

# Here is the gmail configuration (or change it to your private smtp server)
mailhub=smtp.gmail.com:587
AuthUser=your@gmail.com
AuthPass=yourGmailPass
UseTLS=YES
UseSTARTTLS=YES
</code></pre>

<p><strong>Note</strong>: Make sure the ""mail"" command is present in your system. mailutils package should provide this one in Debian based systems.</p>

<p><strong>Update</strong>: There are people (and bug reports for different Linux distributions) reporting that sSMTP will not accept passwords with a 'space' or '#' character. If sSMTP is not working for you, this may be the case.</p>
","37061"
"linux: How can I view all UUIDs for all available disks on my system?","192025","","<p>My <code>/etc/fstab</code> contains this:</p>

<pre><code># / was on /dev/sda1 during installation
UUID=77d8da74-a690-481a-86d5-9beab5a8e842 /               ext4    errors=remount-ro 0       1
</code></pre>

<p>There are several other disks on this system, and not all disks are being mounted to the correct location (For example, /dev/sda1 and /dev/sdb1 are sometimes reversed).</p>

<p>How can I see the UUIDs for all disks on my system? Can I see the UUID for the third disk on this system?</p>
","<p>In <code>/dev/disk/by-uuid</code> there are symlinks mapping each drive's UUID to its entry in <code>/dev</code> (e.g. <code>/dev/sda1</code>)</p>
","660"
"Recursively delete all files with a given extension","191205","","<p>I want to delete all *.o files in a directory and its sub-directories. However, I get an error:</p>

<pre><code>sashoalm@aspire:~/.Workspace.OLD$ rm -r *.o
rm: cannot remove `*.o': No such file or directory
</code></pre>

<p>On the other hand, <code>rm *.o</code> works, but it's not recursive.</p>
","<p>That is evil: <code>rm -r</code> is not for deleting files but for deleting directories. Luckily there are probably no directories matching <code>*.o</code>.</p>

<p>What you want is possible with <code>zsh</code> but not with <code>sh</code> or <code>bash</code> (new versions of bash cannot do this by default but if the shell option <code>globstar</code> is enabled: <code>shopt -s globstar</code>). The globbing pattern is <code>**/*.o</code> but that would not be limited to files, too (maybe <code>zsh</code> has tricks for the exclusion of non-files, too).</p>

<p>But this is rather for <code>find</code>:</p>

<pre><code>find . -type f -name '*.o' -delete
</code></pre>

<p>or (as I am not sure whether <code>-delete</code> is POSIX)</p>

<pre><code>find . -type f -name '*.o' -exec rm {} +
</code></pre>
","116390"
"ssh-add complains: Could not open a connection to your authentication agent","189807","","<p>I've been trying to get <code>ssh-add</code> working on a RaspberryPi running Raspbian.  </p>

<p>I can start <code>ssh-agent</code>, when I do it gives the following output into the terminal:</p>

<pre><code>SSH_AUTH_SOCK=/tmp/ssh-06TcpPflMg58/agent.2806; export SSH_AUTH_SOCK;
SSH_AGENT_PID=2807; export SSH_AGENT_PID;
echo Agent pid 2807;
</code></pre>

<p>If I run <code>ps aux | grep ssh</code> I can see it is running.  </p>

<p>Then I try to run <code>ssh-add</code> in order to add my key passphrase, and I get the following:</p>

<pre><code>Could not open a connection to your authentication agent.
</code></pre>

<p>Any ideas?</p>
","<p>Your shell is meant to evaluate that shell code output by <code>ssh-agent</code>. Run this instead:</p>

<pre><code>eval ""$(ssh-agent)""
</code></pre>

<p>Or if you've started ssh-agent already, copy paste it to your shell prompt (assuming you're running a Bourne-like shell).</p>

<p><code>ssh</code> commands need to know how to talk to the <code>ssh-agent</code>, they know that from the <code>SSH_AUTH_SOCK</code> environment variable.</p>
","48868"
"How to define 'tab' delimiter with 'cut' in BASH?","188635","","<p>Here is an example of using <code>cut</code> to break input into fields using a space delimiter, and obtaining the second field:</p>

<p><code>cut -f2 -d' '</code> </p>

<p>How can the delimiter be defined as a tab, instead of a space?</p>
","<p>Two ways:</p>

<p>Press <code>Ctrl-v + Tab</code></p>

<pre><code>cut -f2 -d'   ' infile
</code></pre>

<p>or write it like this:</p>

<pre><code>cut -f2 -d$'\t' infile
</code></pre>
","35370"
"There are stopped jobs (on bash exit)","188441","","<p>I get the message <code>There are stopped jobs.</code> when I try to exit a bash shell sometimes. Here is a reproducible scenario in python 2.x:</p>

<ul>
<li><kbd>ctrl</kbd>+<kbd>c</kbd> is handled by the interpreter as an exception.</li>
<li><kbd>ctrl</kbd>+<kbd>z</kbd> 'stops' the process.</li>
<li><kbd>ctrl</kbd>+<kbd>d</kbd> exits python for reals.</li>
</ul>

<p>Here is some real-world terminal output:</p>

<pre><code>example_user@example_server:~$ python
Python 2.7.3 (default, Sep 26 2013, 20:03:06) 
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; 
</code></pre>

<p><kbd>ctrl+z</kbd></p>

<pre><code>[1]+  Stopped                 python
example_user@example_server:~$ exit
logout
There are stopped jobs.
</code></pre>

<p>Bash did not exit, I must <code>exit</code> again to exit the bash shell.</p>

<ul>
<li><strong>Q:</strong> What is a 'stopped job', or what does this mean?</li>
<li><strong>Q:</strong> Can a stopped process be resumed?</li>
<li><strong>Q:</strong> Does the first <code>exit</code> kill the stopped jobs?</li>
<li><strong>Q:</strong> Is there a way to exit the shell the first time? (without entering <code>exit</code> twice)</li>
</ul>
","<p>A stopped job is one that has been temporarily put into the background and is no longer running, but is still using resources such (i.e. system memory). Because that job is not attached to the current terminal, it cannot produce output and is not receiving input from the user.</p>

<p>You can see jobs you have running using the <code>jobs</code> builtin command in bash, probably other shells as well. Example:</p>

<pre><code>user@mysystem:~$ jobs
[1] + Stopped                python
user@mysystem:~$ 
</code></pre>

<p>You can resume a stopped job by using the <code>fg</code> (foreground) bash built-in command. If you have multiple commands that have been stopped you must specify which one to resume by passing jobspec number on the command line with <code>fg</code>. If only one program is stopped, you may use <code>fg</code> alone:</p>

<pre><code>user@mysystem:~$ fg 1
python
</code></pre>

<p>At this point you are back in the python interpreter and may exit by using control-D.</p>

<p>Conversely, you may <code>kill</code> the command with either it's jobspec or PID. For instance:</p>

<pre><code>user@mysystem:~$ ps
  PID TTY          TIME CMD
16174 pts/3    00:00:00 bash
17781 pts/3    00:00:00 python
18276 pts/3    00:00:00 ps
user@mysystem:~$ kill 17781
[1]+  Killed                  python
user@mysystem:~$ 
</code></pre>

<p>To use the jobspec, precede the number with the percent (%) key:</p>

<pre><code>user@mysystem:~$ kill %1
[1]+  Terminated              python
</code></pre>

<p>If you issue an exit command with stopped jobs, the warning you saw will be given. The jobs will be left running for safety. That's to make sure you are aware you are attempting to kill jobs you might have forgotten you stopped. The second time you use the exit command the jobs are terminated and the shell exits. This may cause problems for some programs that aren't intended to be killed in this fashion.</p>

<p>In bash it seems you can use the <code>logout</code> command which will kill stopped processes and exit. This may cause unwanted results.</p>

<p>Also note that some programs may not exit when terminated in this way, and you system could end up with a lot of orphaned processes using up resources if you make a habit of doing that.</p>

<p>Note that you can create background process that will stop if they require user input:</p>

<pre><code>user@mysystem:~$ python &amp;
[1] 19028
user@mysystem:~$ jobs
[1]+  Stopped                 python
</code></pre>

<p>You can resume and kill these jobs in the same way you did jobs that you stopped with the <code>Ctrl-z</code> interrupt.</p>
","116968"
"How do I quit from Vi?","188302","","<p>I started Vi on my Ubuntu machine. However I'm new to Vi, and now I can not quit. I see the editor and I can write text, at the bottom line there is a label ""recording"".</p>

<p>How do I quit Vi?</p>
","<p><a href=""http://vimdoc.sourceforge.net/htmldoc/usr_02.html#02.2"">vim is a modal editor</a>. Hit the <kbd>ESC</kbd> key to get into <em>Normal</em> (command) mode then type <kbd>:q</kbd> and press <kbd>Enter</kbd>.</p>

<p>To quit without saving any changes, type <kbd>:q!</kbd> and press <kbd>Enter</kbd>.</p>

<p>See also <a href=""http://vimdoc.sourceforge.net/htmldoc/usr_02.html#02.7"">Getting out</a> in Vim documentation.</p>
","3336"
"Match exact string using grep","187938","","<p>I have a text file:</p>

<pre><code>deiauk 1611516 afsdf 765
minkra 18415151 asdsf 4152
linkra sfsfdsfs sdfss 4555
deiauk1 sdfsfdsfs 1561 51
deiauk2 115151 5454 4
deiauk 1611516 afsdf ddfgfgd
luktol1 4545 4 9
luktol 1
</code></pre>

<p>and I want to match exactly <code>deiauk</code>.  When I do this:</p>

<pre><code>grep ""deiauk"" file.txt
</code></pre>

<p>I get this result:</p>

<pre><code>deiauk 1611516 afsdf 765
deiauk1 sdfsfdsfs 1561 51
deiauk2 115151 5454 4
</code></pre>

<p>but I only need this:</p>

<pre><code>deiauk 1611516 afsdf 765
deiauk 1611516 afsdf ddfgfgd
</code></pre>

<p>I know there's a <code>-w</code> option, but then my string has to mach whole line.</p>
","<p>Try one of:</p>

<pre><code>grep -w ""deiauk"" textfile

grep ""\&lt;deiauk\&gt;"" textfile
</code></pre>
","206910"
"How to monitor CPU/memory usage of a single process?","186321","","<p>I would like to monitor one process's memory / cpu usage in real time. Similar to <code>top</code> but targeted at only one process, preferably with a history graph of some sort.</p>
","<p>On Linux, <code>top</code> actually supports focusing on a single process, although it naturally doesn't have a history graph:</p>

<pre><code>top -p PID
</code></pre>

<p>This is also available on Mac OS X with a different syntax:</p>

<pre><code>top -pid PID
</code></pre>
","556"
"How to get only files created after a date with ls?","186295","","<p>With the <code>ls</code> command, is it possible to show only the files created after a specific date, hour...?</p>

<p>I'm asking it because I have a directory with thousand of files.</p>

<p>I want so see all files that were created since yesterday. </p>

<p>I use <code>ls -ltr</code> but I have to wait to see all files...</p>

<p>There is an equivalent of <code>DIRECTORY/SINCE=date</code> from OpenVMS ?</p>
","<p>You can use the <code>find</code> command to find all files that have been modified after a certain number of days.</p>

<p>For example, to find all files in the current directory that have been modified since yesterday (24 hours ago) use:</p>

<pre><code>find . -maxdepth 1 -mtime -1
</code></pre>

<p>Note that to find files modified <em>before</em> 24 hours ago, you have to use <code>-mtime +1</code> instead of <code>-mtime -1</code>.</p>
","10043"
"count lines in a file","186227","","<p>I'm sure there are many ways to do this: how can I count the number of lines in a text file?</p>

<pre><code>$ &lt;cmd&gt; file.txt
1020 lines
</code></pre>
","<p>The standard way is with <code>wc</code>, which takes arguments to specify what it should count (bytes, chars, words, etc.); <code>-l</code> is for lines:</p>

<pre><code>$ wc -l file.txt
1020 file.txt
</code></pre>
","4479"
"GUI for GIT similar to SourceTree","186207","","<p>Is there a similar piece of software to <a href=""http://www.sourcetreeapp.com/"">SourceTree</a>, a GUI for git, for Linux? I know about Giggle, git cola, etc. I'm looking for a beautiful, easy to use GUI for git.</p>
","<p>A nice alternative is <a href=""http://www.syntevo.com/smartgithg/"">SmartGit</a>. It has some very similar features to SourceTree and has built in 3-column conflict resolution, visual logs, pulling, pushing, merging, syncing, tagging and all things git :)</p>
","75852"
"Using the not equal operator for string comparison","186169","","<pre><code>if [ ""$PHONE_TYPE"" != ""NORTEL"" ] || [ ""$PHONE_TYPE"" != ""NEC"" ] || [ ""$PHONE_TYPE"" != ""CISCO"" ]
then
echo ""Phone type must be nortel,cisco or nec""
exit
fi
</code></pre>

<p>The above code did not work for me, so I tried this instead:</p>

<pre><code>if [ ""$PHONE_TYPE"" == ""NORTEL"" ] || [ ""$PHONE_TYPE"" == ""NEC"" ] || [ ""$PHONE_TYPE"" == ""CISCO"" ]
then
:
else
echo ""Phone type must be nortel,cisco or nec""
exit
fi
</code></pre>

<p>Are there cleaner ways for this type of task?</p>
","<p>I guess you're looking for:</p>

<pre><code>if [ ""$PHONE_TYPE"" != ""NORTEL"" ] &amp;&amp; [ ""$PHONE_TYPE"" != ""NEC"" ] &amp;&amp; [ ""$PHONE_TYPE"" != ""CISCO"" ]
</code></pre>

<p>The rules for these equivalents are called <a href=""https://en.wikipedia.org/wiki/De_Morgan%27s_laws"">De Morgan's laws</a> and in your case meant:</p>

<pre><code>not(A || B || C) =&gt; not(A) &amp;&amp; not(B) &amp;&amp; not (C)
</code></pre>

<p>Note the change in the boolean operator or and and.</p>

<p>Whereas you tried to do:</p>

<pre><code>not(A || B || C) =&gt; not(A) || not(B) || not(C)
</code></pre>

<p>Which obviously doesn't work.</p>
","67900"
"Why do we use ""./"" to execute a file?","186155","","<p>Why do we use <code>./filename</code> to execute a file in linux?</p>

<p>Why not just enter it like other commands <code>gcc</code>, <code>ls</code> etc...</p>
","<p>In Linux, UNIX and related operating systems, <code>.</code> denotes the current directory. Since you want to run a file in your current directory and that directory is not in your <code>$PATH</code>, you need the <code>./</code> bit to tell the shell where the executable is. So, <code>./foo</code> means run the executable called <code>foo</code> that is in this directory.</p>

<p>You can use <a href=""http://linux.die.net/man/1/type""><code>type</code></a> or <a href=""http://linux.die.net/man/1/which""><code>which</code></a> to get the full path of any commands found in your <code>$PATH</code>.</p>
","4432"
"recursive mkdir","185928","","<p>Is there a linux command that I'm overlooking that makes it possible to do something along the lines of:
(pseudo)</p>

<pre><code>$ mkdir -R foo/bar/zoo/andsoforth
</code></pre>

<p>Or is there no alternative but to make the directories one at a time?</p>
","<pre><code>$ mkdir -p foo/bar/zoo/andsoforth
</code></pre>
","49264"
"cat line X to line Y on a huge file","183938","","<p>Say I have a huge text file (>2GB) and I just want to <code>cat</code> the lines <code>X</code> to <code>Y</code> (e.g. 57890000 to 57890010).</p>

<p>From what I understand I can do this by piping <code>head</code> into <code>tail</code> or viceversa, i.e. </p>

<pre><code>head -A /path/to/file | tail -B
</code></pre>

<p>or alternatively</p>

<pre><code>tail -C /path/to/file | head -D
</code></pre>

<p>where <code>A</code>,<code>B</code>,<code>C</code> and <code>D</code> can be computed from the number of lines in the file, <code>X</code> and <code>Y</code>.</p>

<p>But there are two problems with this approach:</p>

<ol>
<li>You have to compute <code>A</code>,<code>B</code>,<code>C</code> and <code>D</code>.</li>
<li>The commands could <code>pipe</code> to each other <strong>many more</strong> lines than I am interested in reading (e.g. if I am reading just a few lines in the middle of a huge file)</li>
</ol>

<p>Is there a way to have the shell just work with and output the lines I want? (while providing only <code>X</code> and <code>Y</code>)?</p>
","<p>I suggest the <code>sed</code> solution, but for the sake of completeness, </p>

<pre><code>awk 'NR &gt;= 57890000 &amp;&amp; NR &lt;= 57890010' /path/to/file
</code></pre>

<p>To cut out after the last line:</p>

<pre><code>awk 'NR &lt; 57890000 { next } { print } NR == 57890010 { exit }' /path/to/file
</code></pre>

<hr>

<p>Speed test: </p>

<ul>
<li>100,000,000-line file generated by <code>seq 100000000 &gt; test.in</code></li>
<li>Reading lines 50,000,000-50,000,010</li>
<li>Tests in no particular order</li>
<li><code>real</code> time as reported by <code>bash</code>'s builtin <code>time</code></li>
</ul>



<pre><code> 4.373  4.418  4.395    tail -n+50000000 test.in | head -n10
 5.210  5.179  6.181    sed -n '50000000,50000010p;57890010q' test.in
 5.525  5.475  5.488    head -n50000010 test.in | tail -n10
 8.497  8.352  8.438    sed -n '50000000,50000010p' test.in
22.826 23.154 23.195    tail -n50000001 test.in | head -n10
25.694 25.908 27.638    ed -s test.in &lt;&lt;&lt;""50000000,50000010p""
31.348 28.140 30.574    awk 'NR&lt;57890000{next}1;NR==57890010{exit}' test.in
51.359 50.919 51.127    awk 'NR &gt;= 57890000 &amp;&amp; NR &lt;= 57890010' test.in
</code></pre>

<p>These are by no means precise benchmarks, but the difference is clear and repeatable enough* to give a good sense of the relative speed of each of these commands.</p>

<p>*: Except between the first two, <code>sed -n p;q</code> and <code>head|tail</code>, which seem to be essentially the same.</p>
","47423"
"Is CentOS exactly the same as RHEL?","183616","","<p>I'm sure this question has been asked again and again elsewhere (I did not find anything specific to CentOS vs RHEL in SE), but I would still like to ask and confirm a few specific points.</p>

<p>I am well aware that CentOS removes all RH trademarks, logos, etc. and is based on the same codes with packages built by the community.</p>

<ul>
<li>Are the packages built for CentOS exactly the same? Will the contents of the packages and the behavior of the programs be identical to those found on RHEL?</li>
<li>What is RHN other than a medium for license registration? What is it to CentOS?</li>
</ul>

<p>I'm an Ubuntu desktop user. Attended a RH299 course which did not really touch anything about the support aspect (i.e. RHN). Other than that I've no professional Linux knowledge or experience.</p>

<p><strong>EDIT</strong></p>

<p>I did read the <a href=""http://wiki.centos.org/Manuals/ReleaseNotes/CentOS6.2"">CentOS 6.2 release notes</a>, but I found the details unsatisfactory. The release notes mentions <a href=""http://wiki.centos.org/Manuals/ReleaseNotes/CentOS6.2#head-3e3a2a9e549acbade82109b54c05b18fca6c431c"">packages modified</a>, <a href=""http://wiki.centos.org/Manuals/ReleaseNotes/CentOS6.2#head-fbd270a1a0d6daaf1df0f32ec0df441cf4ae326d"">removed</a> or <a href=""http://wiki.centos.org/Manuals/ReleaseNotes/CentOS6.2#head-91dede0d0bf87a52468c52594e14228b2fd6e5c0"">added</a> to upstream. But it neither explains nor links to any document detailing what exactly is different in the modified packages. Granted the branding packages are self-explanatory, but it mentions packages like <code>kernel</code>, <code>ntp</code>, <code>anaconda</code>, etc. which have nothing to do with branding as far as I'm aware.</p>
","<p>CentOS is very close to being RHEL without the branding and support. In particular, the library versions are the same, so binaries that work on one will work on the other. The administration tools are the same and configured in similar ways. However, there are a few differences, as the two distributions sometimes apply different minor patches. For example, in <a href=""https://unix.stackexchange.com/questions/4663/does-rhel-execute-all-cronjob-files-under-etc-cron-d"">this question</a>, it was apparent that RHEL 5 and CentOS 5 apply different rules to identify files under <code>/etc/cron.d</code>.</p>

<p>In other words, at the level of your course, you can treat CentOS and RHEL as interchangeable. But if you needed to look up the precise behavior of a program in a corner of the man page, you may encounter differences.</p>
","27330"
"delete line in vi","183570","","<p>How can I delete a line in VI?</p>

<p>Here what I am doing right now:</p>

<ol>
<li>Open up the terminal <kbd>alt</kbd> + <kbd>ctrl</kbd> + <kbd>t</kbd></li>
<li><code>vi a.txt</code></li>
<li>I move my cursor to the line which I wan to delete, then <strong>what key-combination is should use to delete line in vi editor</strong>?</li>
</ol>
","<p>Pressing <code>dd</code> will remove that line (actually it will cut it). So you can paste it via <code>p</code>.</p>
","11370"
"How to make a machine accessible from the LAN using its hostname","183274","","<p>Here are details of the machine I want to access using its hostname:</p>

<pre><code>$ hostname
hostname
$ cat /etc/hosts
127.0.0.1   localhost
127.0.1.1   hostname.company.local  hostname
</code></pre>

<p>It's a default Debian 6 (Squeeze) install, so I didn't fiddle with anything yet.</p>

<p>This is what I get from a machine (running Debian Unstable) trying to access above machine:</p>

<pre><code>$ ping hostname
ping: unknown host hostname
$ ping hostname.company.local
ping: unknown host hostname.company.local
$ cat /etc/resolv.conf
nameserver 192.168.2.21
nameserver 192.168.2.51
search company.local
</code></pre>
","<p>On the Internet, including local networks, machines call each other by <a href=""http://en.wikipedia.org/wiki/IP_address"" rel=""noreferrer"">IP addresses</a>. In order to access machine B from machine A using the name of machine B, machine A has to have some way to map the name of B to its IP address. There are three ways to declare machine names on A:</p>

<ul>
<li>a <a href=""http://en.wikipedia.org/wiki/Hosts.txt"" rel=""noreferrer"">hosts file</a>. This is a simple text file that maps names to addresses.</li>
<li>the <a href=""http://en.wikipedia.org/wiki/Domain_Name_System"" rel=""noreferrer"">domain name system (DNS)</a>. This is the method used on the global Internet. For example, when you load this page in a browser, the first thing your computer does is to make a DNS request to know the address of <code>unix.stackexchange.com</code>.</li>
<li>other name databases such as <a href=""http://en.wikipedia.org/wiki/Network_Information_Service"" rel=""noreferrer"">NIS</a>, <a href=""http://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol"" rel=""noreferrer"">LDAP</a> or <a href=""http://en.wikipedia.org/wiki/Active_Directory"" rel=""noreferrer"">Active Directory</a>. These are used in some corporate networks, but not very often (many networks that use NIS, LDAP or AD for user databases use DNS for machine names). If your network uses one of these, you have a professional network administrator and should ask him what to do.</li>
</ul>

<p>There are many ways in which these can work in practice; it's impossible to cover them all. In this answer, I'll describe a few common situations.</p>

<h2>Hosts file</h2>

<p>The hosts file method has the advantage that it doesn't require any special method. It can be cumbersome if you have several machines, because you have to update every machine when the name of one machine changes. It's not suitable if the IP address of B is assigned dynamically (so that you get a different one each time you connect to the network).</p>

<p>A hosts file is a simple list of lines mapping names to IP addresses. It looks like this:</p>

<pre><code>127.0.0.1       localhost localhost.localdomain
198.51.100.42   darkstar darkstar.bands
</code></pre>

<p>On unix systems, the hosts file is <code>/etc/hosts</code>. On Windows, it's <code>c:\windows\system32\drivers\etc\hosts</code>. Just about every operating system that you can connect to the Internet has a similar file; <a href=""http://en.wikipedia.org/wiki/Hosts.txt#Location_in_the_file_system"" rel=""noreferrer"">Wikipedia has a list</a>.</p>

<p>To add an entry for B in the hosts file of A:</p>

<ol>
<li><p>Determine the IP address of B. On B, run the command <code>ifconfig</code> (if the command is not found, try <code>/sbin/ifconfig</code>). The output will contain lines like this:</p>

<pre><code>eth1      Link encap:Ethernet  HWaddr 01:23:45:67:89:ab
          inet addr:10.3.1.42  Bcast:10.3.1.255  Mask:255.255.255.0
</code></pre>

<p>In this example, the IP address of B is 10.3.1.42. If there are several <code>inet addr:</code> lines, pick the one that corresponds to your network card, never the <code>lo</code> entry or a tunnel or virtual entry.</p></li>
<li>Edit the hosts file on A. If A is running some unix system, you'll need to edit <code>/etc/hosts</code> as the super user; see <a href=""https://unix.stackexchange.com/questions/3063/how-do-i-run-a-command-as-the-system-administrator-root"">How do I run a command as the system administrator (root)</a>.</li>
</ol>

<h2>DHCP+DNS on home or small office networks</h2>

<p>This method is by far the simplest if you have the requisite equipment. You only need to configure one device, and all your computers will know about each other's names. This method assumes your computers get their IP addresses over <a href=""http://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol"" rel=""noreferrer"">DHCP</a>, which is a method for computers to automatically retrieve an IP address when they connect to the network. If you don't know what DHCP is, they probably do.</p>

<p>If your network has a <a href=""http://en.wikipedia.org/wiki/Home_router"" rel=""noreferrer"">home router</a>, it's the best place to configure names for machines connected to that router. First, you need to figure out the <a href=""http://en.wikipedia.org/wiki/MAC_address"" rel=""noreferrer"">MAC address</a> of B. Each network device has a unique MAC address. On B, run the command <code>ifconfig -a</code> (if the command is not found, try <code>/sbin/ifconfig -a</code>). The output will contain lines like this:</p>

<pre><code>    eth1      Link encap:Ethernet  HWaddr 01:23:45:67:89:ab
</code></pre>

<p>In this example the MAC address is <code>01:23:45:67:89:ab</code>. You must pick the HWaddr line that corresponds to the network port that's connected to the router via a cable (or the wifi card if you're connected over wifi). If you have several entries and you don't know which is which, plug the cable and see which network device receives an IP address (<code>inet addr</code> line just below).</p>

<p>Now, on your router's web interface, look for a setting like “DHCP”. The name and location of the setting is completely dependent on the router model, but most have a similar set of basic settings. Here's what it looks like on a <a href=""http://en.wikipedia.org/wiki/Tomato_%28firmware%29"" rel=""noreferrer"">Tomato firmware</a>:</p>

<p><img src=""https://i.stack.imgur.com/WOC1y.jpg"" alt=""tomato screenshot""></p>

<p>Enter the MAC address, an IP address and the desired name. You can pick any IP address on your local network's address range. Most home routers are preconfigured for an address range of the form 192.168.<em>x</em>.<em>y</em> or 10.<em>x</em>.<em>y</em>.<em>z</em>. For example, on the Tomato router shown above, in the “Network” tab, there's a “router IP address” setting with the value 10.3.0.1 and a “subnet mask” setting with the value 255.255.255.0, which means that computers on the local network must have an address of the form 10.3.0.<em>z</em>. There's also a range of addresses for automatically assigned DHCP addresses (10.3.0.129–10.3.0.254); for your manually assigned DHCP address, pick one that isn't in this range.</p>

<p>Now connect B to the network, and it should get the IP address you specified and it'll be reachable by the specified name from any machine in the network.</p>

<h2>Make your own DNS server with Dnsmasq</h2>

<p>If you don't have a capable home router, you can set up the same functionality on any Linux machine. I'll explain how to use <a href=""http://en.wikipedia.org/wiki/Dnsmasq"" rel=""noreferrer"">Dnsmasq</a> to set up <a href=""http://en.wikipedia.org/wiki/Domain_Name_System"" rel=""noreferrer"">DNS</a>. There are many other similar programs; I chose Dnsmasq because it's easy to configure and lightweight (it's what the Tomato router illustrated above uses, for example). Dnsmasq is available on most Linux and BSD distributions for PCs, servers and network equipment.</p>

<p>Pick a computer that's always on, that has a static IP address, and that's running some kind of Linux or BSD; let's call it S (for server). On S, install the <code>dnsmasq</code> package (if it's not already there). Below I'll assume that the configuration file is <code>/etc/dnsmasq.conf</code>; the location may vary on some distribution. Now you need to do several things.</p>

<ul>
<li>Tell Dnsmasq to serve your host names in addition to the ones it gets from the Internet. The simplest way is to enter the names and IP addresses in <code>/etc/hosts</code> (see the “Hosts file” section above), and make sure that <code>/etc/dnsmasq.conf</code> does not have the <code>no-hosts</code> directive uncommented. (Lines that begin with a <code>#</code> are commented out.) You can put the names in a different file; if you do, put a line <code>addn-hosts=/path/to/hosts/file</code> in <code>/etc/dnsmasq.conf</code>.</li>
<li><p>Tell Dnsmasq how to obtain IP addresses for names of machines on the Internet.</p>

<ul>
<li>If you're running Debian, Ubuntu or a derivative, install the <code>resolvconf</code> package. In most common cases, everything will work out of the box.</li>
<li><p>If your network administrator or your ISP gave you the addresses of DNS servers, enter them in <code>/etc/dnsmasq.conf</code>, for example:</p>

<pre><code>server=8.8.8.8
server=8.8.4.4
</code></pre></li>
<li><p>If you don't know what your current DNS settings are, look in the file <code>/etc/resolv.conf</code>. If you see a line like <code>nameserver 8.8.8.8</code>, put a line <code>server=8.8.8.8</code> in <code>/etc/dnsmasq.conf</code>.
After you've changed <code>/etc/dnsmasq.conf</code>, restart Dnsmasq. The command to do that depends on the distribution; typical possibilities include <code>restart dnsmasq</code> or <code>/etc/init.d/dnsmasq restart</code>.</p></li>
</ul></li>
<li>Tell S to use the Dnsmasq service for all host name requests. Edit the file <code>/etc/resolv.conf</code> (as root), remove every <code>nameserver</code> line, and put <code>nameserver 127.0.0.1</code> instead.

<ul>
<li>If you're using resolvconf on Debian or Ubuntu, the <code>/etc/resolv.conf</code> may be suboptimal if you installed the <code>resolvconf</code> package with the network up and running. Make sure that the files <code>base</code>, <code>head</code> and <code>tail</code> in the <code>/etc/resolvconf/resolv.conf.d/</code> directory don't contain any <code>nameserver</code> entries, then run <code>resolvconf -u</code> (as root).</li>
</ul></li>
<li>Tell the other machines to use S as the DNS server. Edit <code>/etc/resolv.conf</code> and replace all <code>nameserver</code> lines with a single <code>nameserver 10.3.0.2</code> where 10.3.0.2 is the IP address of S (see above for how to find out S's IP address).</li>
</ul>

<p>You can also use Dnsmasq as a <a href=""http://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol"" rel=""noreferrer"">DHCP</a> server, so that machines can obtain the address corresponding to their name automatically. This is beyond the scope of this answer; consult the Dnsmasq documentation (it's not difficult). Note that there can only be a single DHCP server on a given local network (the exact definition of local network is beyond the scope of this answer).</p>

<h2>Names on the global Internet</h2>

<p>So far, I've assumed a local network. What if you want to give a name to a machine that's in a different corner of the world? You can still use any of the techniques above, except that the parts involving DHCP are only applicable within a local network. Alternatively, if your machines have public IP addresses, you can register your own public name for them. (You can assign a private IP address to a public name, too; it's less common and less useful, but there's no technical difficulty.)</p>

<h3>Getting your own domain name</h3>

<p>You can get your own <a href=""http://en.wikipedia.org/wiki/Domain_name"" rel=""noreferrer"">domain name</a> and assign IP addresses to host names inside this domain. You need to register the domain name with a domain name provider; this typically costs $10–$15/year (for the cheapest domains). Use your domain name provider's web interface to assign addresses to host names.</p>

<h3>Dynamic DNS</h3>

<p>If your machines have a dynamic IP address, you can use the <a href=""http://en.wikipedia.org/wiki/Dynamic_DNS"" rel=""noreferrer"">dynamic DNS</a> protocol to update the IP address associated to the machine's name when the address changes. Not all domain name providers support dynamic DNS, so shop before you buy. For personal use, <a href=""http://www.noip.com/"" rel=""noreferrer"">No-IP</a> provides a free dynamic DNS service, if you use their own domains (e.g. <code>example.ddns.net</code>).</p>
","16901"
"How do I clear the terminal History?","182539","","<p>I am using Linux Mint 17.1 Rebecca for about 2 days and accidentally typed my password into the terminal which is now displayed in the history list of commands I have previously typed.</p>

<p>I want to clear the terminal history completely. I have tried using the following commands in the terminal which I thought would clear the history forever but they do not:</p>

<pre><code>history -c
reset
tput reset
</code></pre>

<p>The above commands ""will"" clear the history from the terminal but when I exit and bring up a new one all my previous history is still there and can all be listed again using the - history command and also by pressing the UP arrow on my keyboard. I do not want this to happen until I have totally cleared my history, then I want to continue using it.</p>

<p>How can I clear my terminal history completely - forever and start fresh?</p>

<p>Please Note: I do not want to exit the terminal without saving history just clear it forever in this one instance.</p>
","<p><code>reset</code> or <code>tput reset</code> only does things to the terminal. The history is entirely managed by the shell, which remains unaffected.</p>

<p><code>history -c</code> clears your history in the current shell. That's enough (but overkill) if you've just typed your password and haven't exited that shell or saved its history explicitly.</p>

<p>When you exit bash, the history is saved to the history file, which by default is <code>.bash_history</code> in your home directory. More precisely, the history created during the current session is appended to the file; entries that are already present are unaffected. To overwrite the history file with the current shell's history, run <code>history -w</code>.</p>

<p>Instead of removing all your history entries, you can open <code>.bash_history</code> in an editor and remove the lines you don't want to keep. You can also do that inside bash, less conveniently, by using <code>history</code> to display all the entries, then <code>history -d</code> to delete the entries you don't want, and finally <code>history -w</code> to save.</p>

<p>Note that even after you've edited the history file, it's possible that your password is still present somewhere on the disk from an earlier version of the file. It can't be retrieved through the filesystem anymore, but it might still be possible (but probably not easy) to find it by accessing the disk directly. If you use this password elsewhere and your disk gets stolen (or someone gets access to the disk), this could be a problem.</p>
","203295"
"How to add a newline to the end of a file?","182315","","<p>Using version control systems I get annoyed at the noise when the diff says <code>No newline at end of file</code>.</p>

<p>So I was wondering: How to add a newline at the end of a file to get rid of those messages?</p>
","<p>To recursively sanitize a project I use this oneliner:</p>

<pre><code>git ls-files | while read f; do tail -n1 $f | read -r _ || echo &gt;&gt; $f; done
</code></pre>
","161853"
"How can I make a script in /etc/init.d start at boot?","182239","","<p>I think  I read something a while back about this, but I can't remember how it's done. Essentially, I have a service in <code>/etc/init.d</code> which I'd  like to start automatically at boot time. I remember it has something to do with symlinking the script into the <code>/etc/rc.d</code> directory, but I can't remember at the present. What is the command for this? </p>

<p>I believe I'm on a Fedora/CentOS derivative.</p>
","<p>If you are on a Red Hat based system, as you mentioned, you can do the following:</p>

<ol>
<li>Create a script and place in <code>/etc/init.d</code> (e.g <code>/etc/init.d/myscript</code>). The script should have the following format:</li>
</ol>

<pre class=""lang-bsh prettyprint-override""><code>#!/bin/bash
# chkconfig: 2345 20 80
# description: Description comes here....

# Source function library.
. /etc/init.d/functions

start() {
    # code to start app comes here 
    # example: daemon program_name &amp;
}

stop() {
    # code to stop app comes here 
    # example: killproc program_name
}

case ""$1"" in 
    start)
       start
       ;;
    stop)
       stop
       ;;
    restart)
       stop
       start
       ;;
    status)
       # code to check status of app comes here 
       # example: status program_name
       ;;
    *)
       echo ""Usage: $0 {start|stop|status|restart}""
esac

exit 0 
</code></pre>

<p>The format is pretty standard and you can view existing scripts in <code>/etc/init.d</code>. You can then use the script like so <code>/etc/init.d/myscript start</code> or <code>chkconfig myscript start</code>. The <code>ckconfig</code> man page explains the header of the script:</p>

<pre><code> &gt; This says that the script should be started in levels 2,  3,  4, and
 &gt; 5, that its start priority should be 20, and that its stop priority
 &gt; should be 80.
</code></pre>

<p>The example start, stop and status code uses helper functions defined in <code>/etc/init.d/functions</code></p>

<ol start=""2"">
<li><p>Enable the script  </p>

<pre><code>$ chkconfig --add myscript 
$ chkconfig --level 2345 myscript on 
</code></pre></li>
<li><p>Check the script is indeed enabled - you should see ""on"" for the levels you selected. </p>

<pre><code>$ chkconfig --list | grep myscript
</code></pre></li>
</ol>

<p>Hope this is what you were looking for. </p>
","20361"
"What is the best text-mode browser?","181252","","<p>I am looking for one that is updated frequently and has support for many things.</p>
","<p>There are many text web browsers, as there are many graphical web browsers, so it really depends on what you're looking for. <a href=""http://lynx.isc.org/""><code>lynx</code></a> is a common slim choice, <a href=""http://www.elinks.cz/""><code>Elinks</code></a> has many features. Both of these support other protocols, such as <code>ftp</code> and <code>gopher</code> (<code>Elinks</code> even supports <code>bittorrent</code>). <code>Elinks</code> may also be built with support for JavaScript, using Mozilla's former JavaScript implementation, Spidermonkey.</p>

<p>There is also <a href=""http://w3m.sourceforge.net/""><code>w3m</code></a>, which can also be used through <code>Emacs</code>.</p>

<p>If you want to try one at random, Wikipedia has a <a href=""https://en.wikipedia.org/wiki/Text-based_web_browser"">list of text based browsers</a>.</p>

<p>How to install these has more to do with how your distribution manages packages than with the browsers themselves. Some of them are probably in the package repositories for your distro.</p>
","40352"
"How to tell gzip to keep original file?","180757","","<p>I would like to compress a text file using gzip command line tool while <strong>keeping</strong> the original file. By default running the following command</p>

<pre><code>gzip file.txt
</code></pre>

<p>results in modifying this file and renaming it <code>file.txt.gz</code>. instead of this behavior I would like to have this new compressed file in <strong>addition</strong> to the existing one <code>file.txt</code>. For now I am using the following command to do that</p>

<pre><code>gzip -c file.txt &gt; file.txt.gz
</code></pre>

<p>It works but I am wondering why there is no easier solution to do such a common task ? Maybe I missed the option doing that ?</p>
","<p>For GNU <code>gzip</code> 1.6 or above, FreeBSD and derivatives or recent versions of NetBSD, see <a href=""https://unix.stackexchange.com/a/79411/22565"">don_cristi's answer</a>.</p>

<p>With any version, you can use shell redirections as in:</p>

<pre><code>gzip &lt; file &gt; file.gz
</code></pre>

<p>When not given any argument, <code>gzip</code> reads its standard input, compresses it and writes the compressed version to its standard output. As a bonus, when using shell redirections, you don't have to worry about files called <code>""--help""</code> or <code>""-""</code> (that latter one still being a problem for <code>gzip -c --</code>).</p>

<p>Another benefit over <code>gzip -c file &gt; file.gz</code> is that if <code>file</code> can't be opened, the command will fail without creating an empty <code>file.gz</code> (or truncating an already existing <code>file.gz</code>) and without running <code>gzip</code> at all.</p>

<p>A significant difference compared to <code>gzip -k</code> though is that there will be no attempt at copying the <code>file</code>'s metadata (ownership, permissions, modification time, name of uncompressed file) to <code>file.gz</code>.</p>

<p>Also if <code>file.gz</code> already existed, it will silently override it unless you have turned the <code>noclobber</code> option on in your shell (with <code>set -o noclobber</code> for instance in POSIX shells).</p>
","58814"
"How to determine the filesystem of an unmounted device?","180723","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/43237/find-filesystem-of-a-partition-from-a-script"">Find filesystem of a partition from a script</a><br>
  <a href=""https://unix.stackexchange.com/questions/53313/how-to-show-the-filesystem-type-via-the-terminal"">How to show the filesystem type via the terminal?</a>  </p>
</blockquote>



<p>I'm looking for a command that yields the filesystem type as <code>mount</code> would use/detect it, without actually mounting it. It should also work e.g. for LUKS encrypted devices (where <code>file -s</code> yields ""LUKS encrypted file"" instead of ""crypto_LUKS""). Surely there is a more convenient way than parsing <code>fsck -N /dev/whatever</code>'s output (which may use stderr depending on the existence of a corresponding <code>fsck.TYPE</code>)?</p>
","<p>There are multiple ways to get this information. Most of them require you to parse the output of another command.</p>

<ul>
<li><p>Run <code># fdisk /dev/sdX -l</code> to get a basic idea of the filesystem structure. The output is something like this:</p>

<pre><code>Disk /dev/sda: 320.1 GB, 320072933376 bytes, 625142448 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x9f7685a8

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1              63      289169      144553+  83  Linux
/dev/sda2          289170   459121634   229416232+  83  Linux
/dev/sda3       459121635   461129759     1004062+  82  Linux swap / Solaris
/dev/sda4   *   461129760   625142447    82006344    7  HPFS/NTFS/exFAT
</code></pre>

<p>But this will only tell you the partition type. </p></li>
<li><p>You could also use <code># blkid</code> to get the following output:</p>

<pre><code>/dev/sda1: LABEL=""boot"" UUID=""aa84c5a8-6408-4952-b577-578f2a67af86"" TYPE=""ext2"" 
/dev/sda2: LABEL=""root"" UUID=""a430e0ef-fd35-432f-8b9a-75a49b89ad8a"" TYPE=""ext4"" 
/dev/sda3: LABEL=""swap"" UUID=""e388806a-dc27-4f4e-a136-3d1ff4e53962"" TYPE=""swap"" 
/dev/sda4: UUID=""088E027A8E026114"" TYPE=""ntfs"" 
</code></pre></li>
<li><p>Also, for a well formatted output, you could run <code># parted /dev/sdX -l</code> for the following output:</p>

<pre><code>Model: ATA WDC WD3200BEVT-7 (scsi)
Disk /dev/sda: 320GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 

Number  Start   End    Size    Type     File system     Flags
 1      32.3kB  148MB  148MB   primary  ext2
 2      148MB   235GB  235GB   primary  ext4
 3      235GB   236GB  1028MB  primary  linux-swap(v1)
 4      236GB   320GB  84.0GB  primary  ntfs            boot
</code></pre></li>
<li><p><code>$ df -T</code>. This is another command that does not require super user privileges to execute. However, this will report for every mount point</p>

<pre><code>Filesystem     Type     1K-blocks     Used Available Use% Mounted on
rootfs         rootfs   225815276 99381340 114963128  47% /
dev            devtmpfs   1538396        0   1538396   0% /dev
run            tmpfs      1541260      416   1540844   1% /run
/dev/sda2      ext4     225815276 99381340 114963128  47% /
tmpfs          tmpfs      1541260      360   1540900   1% /dev/shm
tmpfs          tmpfs      1541260        0   1541260   0% /sys/fs/cgroup
tmpfs          tmpfs      1541260      900   1540360   1% /tmp
/dev/sda1      ext2        139985    30386    102372  23% /boot
/dev/sda4      fuseblk   82006340 79676036   2330304  98% /mnt
</code></pre></li>
</ul>

<p>Another command that can come handy is <code># file -sL /dev/sdXY</code>. This has one downside in that it does not work with the full block device. Requires the exact device to be passed. The output is quite neat though:</p>

<pre><code>/dev/sda1: Linux rev 1.0 ext2 filesystem data (mounted or unclean), UUID=aa84c5a8-6408-4952-b577-578f2a67af86, volume name ""boot""
</code></pre>

<p>All of these will always be output to stdout. You can parse them in a script if required.</p>
","53552"
"How to force ssh client to use only password auth?","180685","","<p>If I use pubkey auth from e.g.: an Ubuntu 11.04 how can I set the ssh client to use only password auth to a server? (just needed because of testing passwords on a server, where I default log in with key)</p>

<p>I found a way:</p>

<pre><code>mv ~/.ssh/id_rsa ~/.ssh/id_rsa.backup
mv ~/.ssh/id_rsa.pub ~/.ssh/id_rsa.pub.backup
</code></pre>

<p>and now I get prompted for password, but are there any offical ways?</p>
","<p>I recently needed this too, and came up with this:</p>

<pre><code>ssh -o PreferredAuthentications=password -o PubkeyAuthentication=no example.com
</code></pre>
","15141"
"What is the purpose of .bashrc and how does it work?","180415","","<p>I found the <code>.bashrc</code> file and I want to know the purpose/function of it. Also how and when is it used?</p>
","<p><code>.bashrc</code> is a <a href=""http://en.wikipedia.org/wiki/Shell_script"">shell script</a> that Bash runs whenever it is started interactively. You can put any command in that file that you could type at the command prompt.</p>

<p>You put commands here to set up the shell for use in your particular environment, or to customize things to your preferences. A common thing to put in <code>.bashrc</code> are <a href=""http://en.wikipedia.org/wiki/Alias_%28command%29"">aliases</a> that you want to always be available.</p>

<p><code>.bashrc</code> runs on <em>every</em> interactive shell launch. If you say:</p>

<pre><code>$ bash ; bash ; bash
</code></pre>

<p>and then hit <kbd>Ctrl-D</kbd> three times, <code>.bashrc</code> will run three times.  But if you say this instead:</p>

<pre><code>$ bash -c exit ; bash -c exit ; bash -c exit
</code></pre>

<p>then <code>.bashrc</code> won't run at all, since <code>-c</code> makes the Bash call non-interactive. The same is true when you run a shell script from a file.</p>

<p>Contrast <code>.bash_profile</code> and <code>.profile</code> which are only run at the start of a new login shell. (<code>bash -l</code>) You choose whether a command goes in <code>.bashrc</code> vs <code>.bash_profile</code> depending on on whether you want it to run once or for every interactive shell start.</p>

<p>As a counterexample to aliases, which I prefer to put in <code>.bashrc</code>, you want to do <code>PATH</code> adjustments in <code>.bash_profile</code> instead, since these changes are typically not <a href=""http://en.wikipedia.org/wiki/Idempotent"">idempotent</a>:</p>

<pre><code>export PATH=""$PATH:/some/addition""
</code></pre>

<p>If you put that in <code>.bashrc</code> instead, every time you launched an interactive sub-shell, <code>:/some/addition</code> would get tacked on to the end of the <code>PATH</code> again, creating extra work for the shell when you mistype a command.</p>

<p>You get a new interactive Bash shell whenever you <a href=""http://web.physics.ucsb.edu/~pcs/apps/editors/vi/vi_unix.html"">shell out of <code>vi</code></a> with <code>:sh</code>, for example.</p>
","129144"
"timestamp, modification time, and created time of a file","180107","","<p>I just know that <code>ls -t</code> and <code>ls -f</code> give different sorting of files and subdirectories under a directory.</p>

<ul>
<li>What are the differences between timestamp, modification time, and created time of a file?</li>
<li>How to get and change these kinds of information by commands?</li>
<li>In terms of what kind of information do people say a file is ""newer"" than the other?</li>
<li>What kinds of information's change will not make the
file different?</li>
</ul>

<p>For example, I saw someone wrote:</p>

<blockquote>
  <p>By default, the rsync program only looks to see if the files are different in size and timestamp. It doesn't care which file is newer, if it is different, it gets overwritten. You can pass the '--update' flag to rsync which will cause it to skip files on the destination if they are newer than the file on the source, but only so long as they are the same type of file. What this means is that if, for example, the source file is a regular file and the destination is a symlink, the destination file will be overwritten, regardless of timestamp.</p>
</blockquote>

<p>On a side note, does the file type here mean only regular file and simlink, not the type such as pdf, jpg, htm, txt etc?</p>
","<p>There are 3 kind of ""timestamps"":</p>

<ul>
<li>Access - the last time the file was read</li>
<li>Modify - the last time the file was modified (content has been modified)</li>
<li>Change - the last time meta data of the file was changed (e.g. permissions)</li>
</ul>

<p>To display this information, you can use <a href=""http://www.gnu.org/software/coreutils/manual/html_node/stat-invocation.html""><code>stat</code></a> which is part of the coreutils.</p>

<p><code>stat</code> will show you also some more information like the device, inodes, links, etc.</p>

<p>Remember that this sort of information depends highly on the filesystem and mount options. For example if you mount a partition with the <code>noatime</code> option, no access information will be written.</p>

<p>A utility to change the timestamps would be <code>touch</code>.
There are some arguments to decide which timestamp to change (e.g. -a for access time, -m for modification time, etc.) and to influence the parsing of a new given timestamp.
See <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?touch""><code>man touch</code></a> for more details.</p>

<p><code>touch</code> can become handy in combination with <code>cp -u</code> (<em>""copy only when the SOURCE file is newer than the destination file or when the destination file is missing""</em>) or for the creation of empty marker files.</p>
","2465"
"Why is printf better than echo?","179837","","<p>I have heard that <code>printf</code> is better than <code>echo</code>. I can recall only one instance from my experience where I had to use <code>printf</code> because <code>echo</code> didn't work for feeding some text into some program on RHEL 5.8 but <code>printf</code> did.  But apparently, there are other differences, and I would like to inquire what they are as well as if there are specific cases when to use one vs the other.</p>
","<p>Basically, it's a portability (and reliability) issue.</p>

<p>Initially, <code>echo</code> didn't accept any option and didn't expand anything. All it was doing was outputting its arguments separated by a space character and terminated by a newline character.</p>

<p>Now, someone thought it would be nice if we could do things like <code>echo ""\n\t""</code> to output newline or tab characters, or have an option not to output the trailing newline character.</p>

<p>They then thought harder but instead of adding that functionality to the shell (like <code>perl</code> where inside double quotes, <code>\t</code> actually means a tab character), they added it to <code>echo</code>.</p>

<p>David Korn realized the mistake and introduced a new form of shell quotes: <code>$'...'</code> which was later copied by <code>bash</code> and <code>zsh</code> but it was far too late by that time.</p>

<p>Now when a standard UNIX <code>echo</code> receives an argument which contains the two characters <code>\</code> and <code>t</code>, instead of outputting them, it outputs a tab character. And as soon as it sees <code>\c</code> in an argument, it stops outputting (so the trailing newline is not output either).</p>

<p>Other shells/Unix vendors/versions chose to do it differently: they added a <code>-e</code> option to expand escape sequences, and a <code>-n</code> option to not output the trailing newline. Some have a <code>-E</code> to disable escape sequences, some have <code>-n</code> but not <code>-e</code>, the list of escape sequences supported by one <code>echo</code> implementation is not necessarily the same as supported by another.</p>

<p>Sven Mascheck has a <a href=""http://www.in-ulm.de/~mascheck/various/echo+printf/"" rel=""noreferrer"">nice page that shows the extent of the problem</a>.</p>

<p>On those <code>echo</code> implementations that support options, there's generally no support of a <code>--</code> to mark the end of options (zsh and possibly others support <code>-</code> for that though), so for instance, it's difficult to output <code>""-n""</code> with <code>echo</code> in many shells.</p>

<p>On some shells like <code>bash</code><sup>1</sup> or <code>ksh93</code><sup>2</sup> or <code>yash</code> (<code>$ECHO_STYLE</code> variable), the behavior even depends on how the shell was compiled or the environment (GNU <code>echo</code>'s behaviour will also change if <code>$POSIXLY_CORRECT</code> is in the environment). So two <code>bash</code> <code>echo</code>s, even from the same version of <code>bash</code> are not guaranteed to behave the same.</p>

<p>POSIX says: <em>if the first argument is <code>-n</code> or any argument contains backslashes, then the behavior is unspecified</em>. <code>bash</code> echo in that regard is not POSIX in that for instance <code>echo -e</code> is not outputting <code>-e&lt;newline&gt;</code> as POSIX requires. The UNIX specification is stricter, it prohibits <code>-n</code> and requires expansion of some escape sequences including the <code>\c</code> one to stop outputting.</p>

<p>Those specifications don't really come to the rescue here given that many implementations are not compliant.</p>

<p>All in all, you don't know what <code>echo ""$var""</code> will output unless you can make sure that <code>$var</code> doesn't contain backslash characters and doesn't start with <code>-</code>. The POSIX specification actually does tell us to use <code>printf</code> instead in that case.</p>

<p><b>So what that means is that you can't use <code>echo</code> to display uncontrolled data. In other words, if you're writing a script and it is taking external input (from the user as arguments, or file names from the file system...), you can't use <code>echo</code> to display it.</b></p>

<p>This is OK:</p>

<pre><code>echo &gt;&amp;2 Invalid file.
</code></pre>

<p>This is not:</p>

<pre><code>echo &gt;&amp;2 ""Invalid file: $file""
</code></pre>

<p>(Though it will work OK with some (non UNIX compliant) <code>echo</code> implementations like <code>bash</code>'s when the <code>xpg_echo</code> option has not been enabled in one way or another like at compilation time or via the environment).</p>

<p><code>printf</code>, on the other hand is more reliable, at least when it's limited to the basic usage of <code>echo</code>.</p>

<pre><code>printf '%s\n' ""$var""
</code></pre>

<p>Will output the content of <code>$var</code> followed by a newline character regardless of what character it may contain.</p>

<pre><code>printf '%s' ""$var""
</code></pre>

<p>Will output it without the trailing newline character.</p>

<p>Now, there also are differences between <code>printf</code> implementations. There's a core of features that is specified by POSIX, but then there are a lot of extensions. For instance, some support a <code>%q</code> to quote the arguments but how it's done varies from shell to shell, some support <code>\uxxxx</code> for unicode characters. The behavior varies for <code>printf '%10s\n' ""$var""</code> in multi-byte locales, there are at least three different outcomes for <code>printf %b '\123'</code></p>

<p>But in the end, if you stick to the POSIX feature set of <code>printf</code> and don't try doing anything too fancy with it, you're out of trouble.</p>

<p>But remember the first argument is the format, so shouldn't contain variable/uncontrolled data.</p>

<p>A more reliable <code>echo</code> can be implemented using <code>printf</code>, like:</p>

<pre><code>echo() ( # subshell for local scope for $IFS
  IFS="" "" # needed for ""$*""
  printf '%s\n' ""$*""
)

echo_n() (
  IFS="" ""
  printf %s ""$*""
)

echo_e() (
  IFS="" ""
  printf '%b\n' ""$*""
)
</code></pre>

<p>The subshell (which implies spawning an extra process in most shell implementations) can be avoided using <code>local IFS</code> with many shells, or by writing it like:</p>

<pre><code>echo() {
  if [ ""$#"" -gt 0 ]; then
     printf %s ""$1""
     shift
  fi
  if [ ""$#"" -gt 0 ]; then
     printf ' %s' ""$@""
  fi
  printf '\n'
}
</code></pre>

<hr>

<h2>Notes</h2>

<h3>1. how <code>bash</code>'s <code>echo</code> behaviour can be altered.</h3>

<p>With <code>bash</code>, at run time, there are two things that control the behaviour or <code>echo</code> (beside <code>enable -n echo</code> or redefining <code>echo</code> as a function or alias):
the <code>xpg_echo</code> <code>bash</code> option and whether <code>bash</code> is in posix mode. <code>posix</code> mode can be enabled if <code>bash</code> is called as <code>sh</code> or if <code>POSIXLY_CORRECT</code> is in the environment or with the the <code>posix</code> option:</p>

<p>Default behaviour on most systems:</p>

<pre><code>$ bash -c 'echo -n ""\0101""'
\0101% # the % here denotes the absence of newline character
</code></pre>

<p><code>xpg_echo</code> expands sequences as UNIX requires:</p>

<pre><code>$ BASHOPTS=xpg_echo bash -c 'echo ""\0101""'
A
</code></pre>

<p>It still honours <code>-n</code> and <code>-e</code> (and <code>-E</code>):</p>

<pre><code>$ BASHOPTS=xpg_echo bash -c 'echo -n ""\0101""'
A%
</code></pre>

<p>With <code>xpg_echo</code> and POSIX mode:</p>

<pre><code>$ env BASHOPTS=xpg_echo POSIXLY_CORRECT=1 bash -c 'echo -n ""\0101""'
-n A
$ env BASHOPTS=xpg_echo sh -c 'echo -n ""\0101""' # (where sh is a symlink to bash)
-n A
$ env BASHOPTS=xpg_echo SHELLOPTS=posix ARGV0=sh bash -c 'echo -n ""\0101""'
-n A
$ env BASHOPTS=xpg_echo SHELLOPTS=posix bash -c 'echo -n ""\0101""'
-n A
</code></pre>

<p>This time, <code>bash</code> is both POSIX and UNIX conformant. Note that in POSIX mode, <code>bash</code> is still not POSIX conformant as it doesn't output <code>-e</code> in:</p>

<pre><code> $ env SHELLOPTS=posix bash -c 'echo -e'

 $
</code></pre>

<p>The default values for xpg_echo and posix can be defined at compilation time with the <code>--enable-xpg-echo-default</code> and <code>--enable-strict-posix-default</code> options to the <code>configure</code> script. That's typically what recent versions of OS/X do to build their <code>/bin/sh</code>. No Unix/Linux implementation/distribution in their right mind would typically do that for <code>/bin/bash</code> though.</p>

<h3>2. How <code>ksh93</code>'s <code>echo</code> behaviour can be altered.</h3>

<p>In <code>ksh93</code>, whether <code>echo</code> expands escape sequences or not and recognises options depends on the content of <code>$PATH</code>.</p>

<p>If <code>$PATH</code> contains a component that contains <code>/5bin</code> or <code>/xpg</code> before the <code>/bin</code> or <code>/usr/bin</code> component then it behave the SysV/UNIX way (expands sequences, doesn't accept options). If it finds <code>/ucb</code> or <code>/bsd</code> first, then it behaves the BSD<sup>3</sup> way (<code>-e</code> to enable expansion, recognises <code>-n</code>). The default is system dependant, BSD on Debian:</p>

<pre><code>$ ksh93 -c 'echo -n' # default -&gt; BSD (on Debian)
$ PATH=/foo/xpgbar:$PATH ksh93 -c 'echo -n' # /xpg before /bin or /usr/bin -&gt; XPG
-n
$ PATH=/5binary:$PATH ksh93 -c 'echo -n' # /5bin before /bin or /usr/bin -&gt; XPG
-n
$ PATH=/ucb:/foo/xpgbar:$PATH ksh93 -c 'echo -n' # /ucb first -&gt; BSD
$ PATH=/bin:/foo/xpgbar:$PATH ksh93 -c 'echo -n' # /bin before /xpg -&gt; default -&gt; BSD
</code></pre>

<h2>3. BSD for echo -e?</h2>

<p>The reference to BSD for the handling of the <code>-e</code> option is misleading here. All those different and incompatible <code>echo</code> behaviours were all introduced at Bell labs:</p>

<ul>
<li><code>\n</code>, <code>\0ooo</code> in Programmer's Work Bench UNIX (based on Unix V6), and the rest (<code>\b</code>, <code>\c</code>...) in Unix System III<sup><a href=""http://www.in-ulm.de/~mascheck/various/echo+printf/"" rel=""noreferrer"">Ref</a></sup>.</li>
<li><code>-n</code> in Unix V7 (by Denis Ritchie<sup><a href=""http://www.cs.dartmouth.edu/~doug/reader.pdf"" rel=""noreferrer"">Ref</a></sup>)</li>
<li><code>-e</code> in Unix V8 (by Denis Ritchie<sup><a href=""http://www.cs.dartmouth.edu/~doug/reader.pdf"" rel=""noreferrer"">Ref</a></sup>)</li>
</ul>

<p>BSDs just descended from Unix V7. <a href=""http://www.freebsd.org/cgi/man.cgi?query=echo&amp;apropos=0&amp;sektion=0&amp;manpath=FreeBSD%2011.0-stable&amp;arch=default&amp;format=html"" rel=""noreferrer"">FreeBSD <code>echo</code></a> still doesn't support <code>-e</code>, though it does support <code>-n</code> like Unix V7 did.</p>
","65819"
"Tell fs to free space from deleted files NOW","179657","","<p>Is there a way to tell the kernel to give back the free disk space now? Like a write to something in /proc/ ?  Using Ubuntu 11.10 with ext4.</p>

<p>This is probably an <a href=""https://unix.stackexchange.com/questions/2827/what-happened-to-my-free-space"">old</a> and very repeated theme.
After hitting 0 space only noticed when my editor couldn't save source code files I have open, which to my horror now have 0 byte size in the folder listing, I went on a deleting spree.</p>

<p>I deleted 100's of MB of large files both from user and from root, and did some hardlinking too.</p>

<p>Just before I did <code>apt-get clean</code> there was over 900MB in /var/cache/apt/archives, now there is only 108KB:</p>

<pre><code># du
108 /var/cache/apt/archives
</code></pre>

<p>An hour later still no free space and cannot save my precious files opened in the editor, but notice the disparity below:</p>

<pre><code># sync; df
Filesystem           1K-blocks      Used Available Use% Mounted on
/dev/sda4             13915072  13304004         0 100% /
</code></pre>

<p>Any suggestions?  I shut off some services/processes but not sure how to check who might be actively eating disk space.</p>

<p>More info</p>

<pre><code># dumpe2fs  /dev/sda4
Filesystem state:         clean
Errors behavior:          Continue
Filesystem OS type:       Linux
Inode count:              884736
Block count:              3534300
Reserved block count:     176715
Free blocks:              422679
Free inodes:              520239
First block:              0
Block size:               4096
Fragment size:            4096
</code></pre>
","<p>Check with <code>lsof</code> to see if there are files held open. Space will not be freed until they are closed.</p>

<pre><code>sudo /usr/sbin/lsof | grep deleted
</code></pre>

<p>will tell you which deleted files are still held open.</p>
","34143"
"List only regular files (but not directories) in current directory","179277","","<p>I can use <code>ls -ld */</code> to list all the directory entries in the current directory. Is there a similarly easy way to just list all the regular files in the current directory? I know I can use find</p>

<pre><code>find . -maxdepth 1 -type f
</code></pre>

<p>or stat</p>

<pre><code>stat -c ""%F %n"" * | grep ""regular file"" | cut -d' ' -f 3-
</code></pre>

<p>but these do not strike me as being overly elegant. Is there a nice short way to list only the regular files (I don't care about devices, pipes, etc.) but not the sub-directories of the current directory? Listing symbolic links as well would be a plus, but is not a necessity.</p>
","<p>With <code>zsh</code> and <a href=""http://manpages.debian.net/cgi-bin/man.cgi?query=zshexpn&amp;apropos=0&amp;sektion=0&amp;manpath=Debian%206.0%20squeeze&amp;format=html&amp;locale=en"">Glob Qualifiers</a> you can easily express it directly, e.g:</p>

<pre><code>echo *(.)
</code></pre>

<p>will either only return the list of regular files or an error depending on your configuration.</p>

<p>For the non-directories:</p>

<pre><code>echo *(^/)
</code></pre>

<p>(will include symlinks (including to directories), named pipes, devices, sockets, doors...)</p>

<pre><code>echo *(-.)
</code></pre>

<p>for regular files and symlinks to regular files.</p>

<pre><code>echo *(-^/)
</code></pre>

<p>for non-directories and no symlinks to directories either.</p>

<p>Also, see the <code>D</code> globbing qualifier if you want to include <strong>D</strong>ot files (hidden files), like <code>*(D-.)</code>.</p>
","48495"
"How to merge all (text) files in a directory into one?","176261","","<p>I've got 14 files all being parts of one text. I'd like to merge them into one. How to do that?</p>
","<p>This is technically what <code>cat</code> (""concatenate"") is supposed to do, even though most people just use it for outputting files to stdout. If you give it multiple filenames it will output them all sequentially, and then you can redirect that into a new file; in the case of all files just use <code>*</code> (or <code>/path/to/directory/*</code> if you're not in the directory already) and your shell will expand it to all the filenames</p>

<pre><code>$ cat * &gt; merged-file
</code></pre>
","3771"
"linux + add X days to date and get new virtual date","175439","","<p>I  have Linux ( RH 5.3) machine</p>

<p>I need to add/calculate 10 days plus date so then I will get new date (expiration date))</p>

<p>for example</p>

<pre><code> # date 
 Sun Sep 11 07:59:16 IST 2012
</code></pre>

<p>So I need to get </p>

<pre><code>     NEW_expration_DATE = Sun Sep 21 07:59:16 IST 2012
</code></pre>

<p>Please advice how to calculate the new expiration date ( with bash , ksh , or manipulate date command ?)</p>
","<p>You can just use the <code>-d</code> switch and provide a date to be calculated</p>

<pre><code>date
Sun Sep 23 08:19:56 BST 2012
NEW_expration_DATE=$(date -d ""+10 days"")
echo $NEW_expration_DATE
Wed Oct 3 08:12:33 BST 2012 
</code></pre>

<blockquote>
<pre><code>  -d, --date=STRING
          display time described by STRING, not ‘now’
</code></pre>
</blockquote>

<p>This is quite a powerful tool as you can do things like </p>

<pre><code>date -d ""Sun Sep 11 07:59:16 IST 2012+10 days""
Fri Sep 21 03:29:16 BST 2012
</code></pre>

<p>or</p>

<pre><code>TZ=IST date -d ""Sun Sep 11 07:59:16 IST 2012+10 days""
Fri Sep 21 07:59:16 IST 2012
</code></pre>

<p>or</p>

<pre><code>prog_end_date=`date '+%C%y%m%d' -d ""$end_date+10 days""`
</code></pre>

<p>So if $end_date=20131001 then $prog_end_date=20131011</p>
","49055"
"How can I get distribution name and version number in a simple shell script?","174650","","<p>I'm working on a simple bash script that should be able to run on Ubuntu and CentOS distributions (support for Debian and Fedora/RHEL would be a plus) and I need to know the name and version of the distribution the script is running (in order to trigger specific actions, for instance the creation of repositories). So far what I've got is this:</p>

<pre><code>OS=$(awk '/DISTRIB_ID=/' /etc/*-release | sed 's/DISTRIB_ID=//' | tr '[:upper:]' '[:lower:]')
ARCH=$(uname -m | sed 's/x86_//;s/i[3-6]86/32/')
VERSION=$(awk '/DISTRIB_RELEASE=/' /etc/*-release | sed 's/DISTRIB_RELEASE=//' | sed 's/[.]0/./')

if [ -z ""$OS"" ]; then
    OS=$(awk '{print $1}' /etc/*-release | tr '[:upper:]' '[:lower:]')
fi

if [ -z ""$VERSION"" ]; then
    VERSION=$(awk '{print $3}' /etc/*-release)
fi

echo $OS
echo $ARCH
echo $VERSION
</code></pre>

<p>This <em>seems</em> to work, returning <code>ubuntu</code> or <code>centos</code> (I haven't tried others) as the release name. However, I have a feeling that there must be an easier, more reliable way of finding this out -- is that true?</p>

<p>It doesn't work for RedHat.
/etc/redhat-release contains :
Redhat Linux Entreprise release 5.5</p>

<p>So, the version is not the third word, you'd better use :</p>

<pre><code>OS_MAJOR_VERSION=`sed -rn 's/.*([0-9])\.[0-9].*/\1/p' /etc/redhat-release`
OS_MINOR_VERSION=`sed -rn 's/.*[0-9].([0-9]).*/\1/p' /etc/redhat-release`
echo ""RedHat/CentOS $OS_MAJOR_VERSION.$OS_MINOR_VERSION""
</code></pre>
","<p>To get <code>OS</code> and <code>VER</code>, the latest standard seems to be <code>/etc/os-release</code>. 
 Before that, there was <code>lsb_release</code> and <code>/etc/lsb-release</code>.  Before that, you had to look for different files for each distribution.</p>

<p>Here's what I'd suggest</p>

<pre><code>if [ -f /etc/os-release ]; then
    # freedesktop.org and systemd
    . /etc/os-release
    OS=$NAME
    VER=$VERSION_ID
elif type lsb_release &gt;/dev/null 2&gt;&amp;1; then
    # linuxbase.org
    OS=$(lsb_release -si)
    VER=$(lsb_release -sr)
elif [ -f /etc/lsb-release ]; then
    # For some versions of Debian/Ubuntu without lsb_release command
    . /etc/lsb-release
    OS=$DISTRIB_ID
    VER=$DISTRIB_RELEASE
elif [ -f /etc/debian_version ]; then
    # Older Debian/Ubuntu/etc.
    OS=Debian
    VER=$(cat /etc/debian_version)
elif [ -f /etc/SuSe-release ]; then
    # Older SuSE/etc.
    ...
elif [ -f /etc/redhat-release ]; then
    # Older Red Hat, CentOS, etc.
    ...
else
    # Fall back to uname, e.g. ""Linux &lt;version&gt;"", also works for BSD, etc.
    OS=$(uname -s)
    VER=$(uname -r)
fi
</code></pre>

<p>I think <code>uname</code> to get <code>ARCH</code> is still the best way.  But the example you gave obviously only handles Intel systems.  I'd either call it <code>BITS</code> like this:</p>

<pre><code>case $(uname -m) in
x86_64)
    BITS=64
    ;;
i*86)
    BITS=32
    ;;
*)
    BITS=?
    ;;
esac
</code></pre>

<p>Or change <code>ARCH</code> to be the more common, yet unambiguous versions: <code>x86</code> and <code>x64</code> or similar:</p>

<pre><code>case $(uname -m) in
x86_64)
    ARCH=x64  # or AMD64 or Intel64 or whatever
    ;;
i*86)
    ARCH=x86  # or IA32 or Intel32 or whatever
    ;;
*)
    # leave ARCH as-is
    ;;
esac
</code></pre>

<p>but of course that's up to you.</p>
","6348"
"how can I add (subtract, etc.) two numbers with bash?","174199","","<p>I can read the numbers and operation in with:</p>

<pre><code>echo ""First number please""
read num1
echo ""Second number please""
read num2
echo ""Operation?""
read op
</code></pre>

<p>but then all my attempts to add the numbers fail:</p>

<pre><code>case ""$op"" in
  ""+"")
    echo num1+num2;;
  ""-"")
    echo `num1-num2`;;
esac
</code></pre>

<p>Run:</p>

<pre><code>First number please
1
Second mumber please
2
Operation?
+
</code></pre>

<p>Output:</p>

<pre><code>num1+num2
</code></pre>

<p>...or...</p>

<pre><code>echo $num1+$num2;;

# results in: 1+2    
</code></pre>

<p>...or...</p>

<pre><code>echo `$num1`+`$num2`;;

# results in: ...line 9: 1: command not found
</code></pre>

<p>Seems like I'm getting strings still perhaps when I try add add (""2+2"" instead of ""4"").</p>
","<p><a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06_04"" rel=""nofollow noreferrer"">Arithmetic in POSIX shells</a> is done with <code>$</code> and double parentheses:</p>

<pre><code>echo ""$(($num1+$num2))""
</code></pre>

<p>You can assign from that (sans <code>echo</code>):</p>

<pre><code>num1=""$(($num1+$num2))""
</code></pre>

<p>There is also <code>expr</code>:</p>

<pre><code>expr $num1 + $num2
</code></pre>

<p>In scripting <code>$(())</code> is preferable since it avoids a fork/execute for the <code>expr</code> command.</p>
","93030"
"How to install the latest Python version on Debian separately or upgrade?","173752","","<p>I'm still new to Linux, so I'm still trying to understand where executables and their libraries are and how to install packages, so I have Debian Wheezy 7.3 which has these Python versions:</p>

<ul>
<li>Python 2.7.3 (default)</li>
<li>Python 2.6.8</li>
</ul>

<p>So in the directory <code>/usr/bin/</code> there are these files that I can call from the terminal:</p>

<ul>
<li>python (which is a link to python2.7)</li>
<li>python2 (also a link to python2.7)</li>
<li>python2.6 (Python 2.6.8 executable)</li>
<li>python2.7 (Python 2.7.3 executable)</li>
</ul>

<p>and in <code>/usr/lib/</code>, the main folders:</p>

<ul>
<li>python2.6</li>
<li>python2.7</li>
</ul>

<p>Currently the latest version of Python is 2.7.6 which I want to install, but I don't know how, I tried using apt-get:</p>

<pre><code>apt-get install python
</code></pre>

<p>it outputs <code>python is already the newest version.</code>.</p>

<p>So how can I install the latest version of Python ? on the <a href=""http://python.org/download/"">Python download page</a> there is the source tarball, how can I use that to install it separately like having another folder in <code>/usr/lib/</code> like <code>python2.7.6</code> and make the <code>python</code> link in <code>/usr/bin/</code> point to the new executable, or maybe upgrade the current version if it won't break anything.</p>
","<p>You probably are looking for <code>virtualenv</code> or <code>pyenv</code> or some other non-system-wide method to install Python. The method using APT (Advance Package Tool) and <code>dpkg</code>, ensures that all parts of the system are working in harmony, so you maybe want to install python in a separated path, hidden of all the other programs that you can call at will, which is the purpose of <code>pyenv</code>/<code>virtualenv</code>. This answers how to install the latest version of python <strong>without breaking the system</strong>.</p>

<p>BTW, the latest version of <code>python</code> is 3.3.something, while the latest version of python 2 is the one you pointed out:</p>

<pre><code>➜  ~  apt-cache policy python3 python
python3:
  Installed: 3.3.2-17
  Candidate: 3.3.2-17
  Version table:
 *** 3.3.2-17 0
        500 http://ftp.us.debian.org/debian/ testing/main i386 Packages
        100 /var/lib/dpkg/status
python:
  Installed: 2.7.5-5
  Candidate: 2.7.5-5
  Version table:
 *** 2.7.5-5 0
        500 http://ftp.us.debian.org/debian/ testing/main i386 Packages
        100 /var/lib/dpkg/status
</code></pre>

<p>(pythonbrew is not longer maintained).</p>
","110016"
"Error using SCP: ""not a regular file""","173242","","<p>I have been searching for a while and I can't find the definition of a regular file. My path is permanent (I start at <code>/</code>) and I am connecting to</p>

<pre><code>scp root@IP: /path/to/picture.jpg
</code></pre>

<p>Results in an inquiry for a password and then...</p>

<pre><code>scp: .: not a regular file
</code></pre>
","<p>A regular file is a file that isn't a directory or more exotic kinds of <a href=""http://en.wikipedia.org/wiki/Unix_file_types"">“special” files</a> such as named pipes, devices, sockets, doors, etc. <a href=""http://en.wikipedia.org/wiki/Symbolic_link"">Symbolic links</a> are not regular files either, but they behave like their target when it an application is accessing the content of the file.</p>

<p>You passed <code>root@IP:</code> as the source of the copy and <code>/path/to/picture.jpg</code> as the destination. The source is the home directory of the user <code>root</code> on the machine <code>IP</code>. This is useful as a destination, but not as a source. What you typed required to copy a directory onto a file; <code>scp</code> cannot copy a directory unless you ask for a recursive copy with the <code>-r</code> option (and it would refuse to overwrite an existing file with a directory even with <code>-r</code>, but it would quietly overwrite a regular file if the source was a regular file).</p>

<p>If <code>/path/to/picture.jpg</code> is the path on the remote machine of the file you want to copy, you need to stick the file name to the host specification. It's the colon <code>:</code> that separates the host name from the remote path. You'll need to specify a destination as well.</p>

<pre><code>scp root@IP:/path/to/picture.jpg /some/destination
</code></pre>

<p>If you want to copy the local file <code>/path/to/picture.jpg</code> to the remote host, you need to swap the arguments. Unix copy commands put the source(s) first and the destination last.</p>

<pre><code>scp /path/to/picture.jpg root@IP:
</code></pre>

<p>If you want to copy the remote file <code>/path/to/picture.jpg</code> to the same location locally, you need to repeat the path. You can have your shell does the work of repeating for you (less typing, less readability).</p>

<pre><code>scp root@IP:/path/to/picture.jpg /path/to/picture.jpg
scp {root@IP:,}/path/to/picture.jpg
</code></pre>
","52740"
"Delete from cursor to end of line on VI","173181","","<p>I know I've probably looked over this a million times in all the VI documents I've read, but I can't seem to find the delete from cursor to end of line command.</p>
","<p>The command <kbd>d</kbd><kbd>w</kbd> will delete from the current cursor position to the beginning of the next word character. The command <kbd>d</kbd><kbd>$</kbd> (note, that's a dollar sign, not an 'S') will delete from the current cursor position to the end of the current line. <kbd>D</kbd> is a synonym for <kbd>d</kbd><kbd>$</kbd>.</p>
","4416"
"How to find out if httpd is running or not via command line?","172682","","<p>I'm working on a small control panel for my server. I need a command that will say if <code>httpd</code> is running or stopped.</p>

<p>Will probably be using the same code for other services as well.</p>
","<p>Most people run their httpd (Apache, Nginx, etc) through an init system. That's almost certainly the case if you've installed from a package. Almost all of these init systems have a method work working out if it's running. In my case I'm using nginx which ships a SysV-style init script and that accepts a <code>status</code> argument, like so:</p>

<pre><code>$ /etc/init.d/nginx status
 * nginx is running
</code></pre>

<p>Obviously if you're running a different httpd, script or init system, you're going to have a slightly different syntax but unless you're manually launching the httpd yourself (which feels like the worst idea in the world), you're probably using a nice, managed start-up script that will allow you to query the status.</p>

<p><a href=""https://unix.stackexchange.com/a/135020/880"">slm's answer</a> has more about this sort of init querying but the problem with trusting that is it only really tells you if a process is still running. Your httpd's main process <em>could</em> be running but in some way deadlocked. It makes a lot of sense to skip simple init tests and move on to behavioural tests.</p>

<p>One thing we know about httpds is they listen. Usually on port <code>*:80</code>, but if yours doesn't, you can adapt the code following code. Here I'm just <code>awk</code>ing the output of <code>netstat</code> to see if it's listening on the right port.</p>

<pre><code>$ sudo netstat -ntlp | awk '$4==""0.0.0.0:80""'
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      2079/nginx
</code></pre>

<p>We could also check <em>which</em> process is running too to make sure the <em>right</em> httpd is running. We could do all sorts of checks. Depends how paranoid you want to be :)</p>

<p>But even that is only a reflection of an httpd. Want to really test it? Well let's <em>test</em> it.</p>

<pre><code>$ wget --spider -S ""http://localhost"" 2&gt;&amp;1 | awk '/HTTP\// {print $2}'
200
</code></pre>

<p>I'm just looking at the response code (200 means ""A-Okay!"") but again, we could dig in and actually test the output to make sure it's being generated correctly.</p>

<p>But even this isn't that thorough. You're checking <code>localhost</code> and it's reporting 200, nothing wrong? What if beavers chewed through the network cable that supplies the httpd (but not the rest of the system)? Then what?! You're reporting uptime when you're actually down. Few things look more unprofessional than incorrect status data.</p>

<p>So let's talk to an external server (ideally on a completely different connection, in another galaxy far, far away) and ask it to query our server:</p>

<pre><code>$ ssh tank 'wget --spider -S ""http://bert"" 2&gt;&amp;1' | awk '/HTTP\// {print $2}'
200
</code></pre>

<p>By this point, any issues have reported are either in-app issues (which can have their own error -handling and -reporting, or they're at the client's end).</p>

<p>A combination of these tests can help nail down where the issue is too.</p>
","135017"
"How can I see what processes are running?","172661","","<p>I use Ubuntu Server 10.10 and I would like to see what processes are running. I know that PostgreSQL is running on my machine but I can not see it with the <code>top</code> or <code>ps</code> commands, so I assume that they aren't showing all of the running processes. Is there another command which will show all running processes or is there any other parameters I can use with <code>top</code> or <code>ps</code> for this?</p>
","<p>From the <code>ps</code> man page:</p>

<blockquote>
  <p>-e              Select all processes. Identical to -A.</p>
</blockquote>

<p>Thus, <code>ps -e</code> will display all of the processes.  The common options for ""give me everything"" are <code>ps -ely</code> or <code>ps aux</code>, the latter is the BSD-style. Often, people then pipe this output to <code>grep</code> to search for a process, as in xenoterracide's answer.  In order to avoid also seeing <code>grep</code> itself in the output, you will often see something like:</p>

<pre><code> ps -ef | grep [f]oo
</code></pre>

<p>where foo is the process name you are looking for.</p>

<p>However, if you are looking for a particular process, I recommend using the <code>pgrep</code> command if it is available.  I believe it is available on Ubuntu Server.  Using <code>pgrep</code> means you avoid the race condition mentioned above. It also provides some other features that would require increasingly complicated <code>grep</code> trickery to replicate.  The syntax is simple:</p>

<pre><code>pgrep foo
</code></pre>

<p>where foo is the process for which you are looking.  By default, it will simply output the Process ID (PID) of the process, if it finds one.  See <code>man pgrep</code> for other output options.  I found the following page very helpful:</p>

<p><a href=""http://mywiki.wooledge.org/ProcessManagement"">http://mywiki.wooledge.org/ProcessManagement</a></p>
","3342"
"How do I set an environment variable on the command line and have it appear in commands?","172101","","<p>If I run</p>

<pre><code>export TEST=foo
echo $TEST
</code></pre>

<p>It outputs foo.</p>

<p>If I run</p>

<pre><code>TEST=foo echo $TEST
</code></pre>

<p>It does not. How can I get this functionality without using export or a script?</p>
","<p>This is because the shell expands the variable in the command line <strong>before</strong> it actually runs the command and at that time the variable doesn't exist. If you use</p>

<pre><code>TEST=foo; echo $TEST
</code></pre>

<p>it will work.</p>

<p><code>export</code> will make the variable appear in the environment of subsequently executed commands (for on how this works in bash see <code>help export</code>). If you only need the variable to appear in the environment of one command, use what you have tried, i.e.:</p>

<pre><code>TEST=foo your-application
</code></pre>
","56449"
"How to rename multiple files in single command or script in Unix?","171269","","<p>I have the below list of files</p>

<pre><code>aro_tty-mIF-45875564pmo_opt
aro_tty-mIF-45875664pmo_opt
aro_tty-mIF-45875964pmo_opt
aro_tty-mIF-45875514pmo_opt
aro_tty-mIF-45875524pmo_opt
</code></pre>

<p>that I need to rename to</p>

<pre><code>aro_tty-mImpFRA-45875564pmo_opt
aro_tty-mImpFRA-45875664pmo_opt
aro_tty-mImpFRA-45875964pmo_opt
aro_tty-mImpFRA-45875514pmo_opt
aro_tty-mImpFRA-45875524pmo_opt
</code></pre>

<p>Please help me out.</p>
","<p>Most standard shells provide a way to do simple text substitution within shell variables. <a href=""http://tldp.org/LDP/abs/html/parameter-substitution.html"">http://tldp.org/LDP/abs/html/parameter-substitution.html</a> explains as follows:</p>

<pre><code>${var/Pattern/Replacement}

First match of Pattern, within var replaced with Replacement.
</code></pre>

<p>So use this script to loop through all the appropriate files and rename each of them:</p>

<pre><code>for file in aro_tty-mIF-*_opt
do
    mv -i ""${file}"" ""${file/-mIF-/-mImpFRA-}""
done
</code></pre>

<p>I have added a -i option so you have the chance to confirm each renaming operation. As always, you should make a backup of all your files before doing any large amount of renaming or deleting.</p>
","102653"
"How do I convert an epoch timestamp to a human readable format on the cli?","170727","","<p>How do I convert an epoch timestamp to a human readable format on the cli? I think there's a way to do it with date but the syntax eludes me (other ways welcome).</p>
","<p>On *BSD:</p>

<pre><code>date -r 1234567890
</code></pre>

<p>On Linux (specifically, with GNU coreutils ≥5.3):</p>

<pre><code>date -d @1234567890
</code></pre>

<p>With older versions of GNU date, you can calculate the relative difference to the UTC epoch:</p>

<pre><code>date -d '1970-01-01 UTC + 1234567890 seconds'
</code></pre>

<p>If you need portability, you're out of luck. The only time you can format with a POSIX shell command (without doing the calculation yourself) line is the current time. In practice, Perl is often available:</p>

<pre><code>perl -le 'print scalar localtime $ARGV[0]' 1234567890
</code></pre>
","2993"
"List the files containing a particular word in their text","170292","","<p>I would like to list the files recursively and uniquely that contain the given word. </p>

<p><strong>Example</strong>: Checking for word 'check', I normal do is a grep</p>

<pre><code>$ grep check * -R
</code></pre>

<p>But as there are many occurrence of this word, I get a lot of output. So I just need to list the filenames that contain the given search word. I guess some trick with <code>find</code> and <code>xargs</code> would suffice here, but not sure.</p>

<p>Any ideas?</p>
","<p>Use the <code>-l</code> or <code>--files-with-matches</code> option which is documented as follows:</p>

<blockquote>
  <p>Suppress  normal  output;  instead  print the name of each input file
  from which output would normally have  been  printed.   The scanning 
  will  stop  on  the  first match.  (<code>-l</code> is specified by POSIX.)</p>
</blockquote>

<p>So, for you example you can use the following:</p>

<pre><code>$ grep check * -lR
</code></pre>
","65182"
"How can I assign the output of a command to a shell variable?","169900","","<p>I want to assign the result of an expression to a variable and concatenate it with a string, then echo it. Here's what I've got:</p>

<pre><code>#!/bin/bash
cd ~/Desktop;
thefile= ls -t -U | grep -m 1 ""Screen Shot"";
echo ""Most recent screenshot is: ""$thefile;
</code></pre>

<p>But that outputs:</p>

<pre><code>Screen Shot 2011-07-03 at 1.55.43 PM.png
Most recent screenshot is: 
</code></pre>

<p>So, it looks like that isn't getting assigned to <code>$thefile</code>, and is being printed as it's executed.</p>
","<p>A shell assignment is a single word, with no space after the equal sign. So what you wrote assigns an empty value to <code>thefile</code>; furthermore, since the assignment is grouped with a command, it makes <code>thefile</code> an environment variable and the assignment is local to that particular command, i.e. only the call to <code>ls</code> sees the assigned value.</p>

<p>You want to capture the output of a command, so you need to use <a href=""http://tldp.org/LDP/abs/html/commandsub.html"">command substitution</a>:</p>

<pre><code>thefile=$(ls -t -U | grep -m 1 ""Screen Shot"")
</code></pre>

<p>(Some literature shows an alternate syntax <code>thefile=`ls …`</code>; the backquote syntax is equivalent to the dollar-parentheses syntax except that quoting inside backquotes is weird sometimes, so just use <code>$(…)</code>.)</p>

<p>Other remarks about your script:</p>

<ul>
<li>Combining <code>-t</code> (sort by time) with <code>-U</code> (don't sort) doesn't make sense; just use <code>-t</code>.</li>
<li><p>Rather than using <code>grep</code> to match screenshots, it's clearer to pass a wildcard to <code>ls</code> and use <code>head</code> to capture the first file:</p>

<pre><code>thefile=$(ls -t *""Screen Shot""* | head -n 1)
</code></pre></li>
<li><p>It's generally a <a href=""http://mywiki.wooledge.org/ParsingLs"">bad idea to parse the output of <code>ls</code></a>. This could fail quite badly if you have file names with nonprintable characters. However, sorting files by date is difficult without <code>ls</code>, so it's an acceptable solution if you know you won't have unprintable characters or backslashes in file names.</p></li>
<li><p><strong>Always use double quotes around variable substitutions</strong>, i.e. here write</p>

<pre><code>echo ""Most recent screenshot is: $thefile""
</code></pre>

<p>Without double quotes, the value of the variable is reexpanded, which will cause trouble if it contains whitespace or other special characters.</p></li>
<li>You don't need semicolons at the end of a line. They're redundant but harmless.</li>
<li>In a shell script, it's often a good idea to include <a href=""http://www.davidpashley.com/articles/writing-robust-shell-scripts.html""><code>set -e</code></a>. This tells the shell to exit if any command fails (by returning a nonzero status).</li>
</ul>

<p>If you have GNU find (in particular if you're running non-embedded Linux or Cygwin), there's another approach to finding the most recent file: have <code>find</code> list the files and their dates, and use <code>sort</code> and <code>tail</code> to extract the youngest file.</p>

<pre><code>thefile=$(find -maxdepth 1 -type f -name ""*Screen Shot*"" -printf ""%T@ %p"" |
          sort -k 1n | tail -n 1)
</code></pre>

<p>If you're willing to write this script in zsh instead of bash, there's a much easier way to catch the newest file, because zsh has <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#Glob-Qualifiers"">glob qualifiers</a> that permit wildcard matches not only on names but also on file metadata. The <code>(om[1])</code> part after the pattern is the glob qualifiers; <code>om</code> sorts matches by increasing age (i.e. by modification time, newest first) and <code>[1]</code> extracts the first match only. The whole match needs to be in parentheses because it's technically an array, since globbing returns a list of files, even if the <code>[1]</code> means that in this particular case the list contains (at most) one file.</p>

<pre><code>#!/bin/zsh
set -e
cd ~/Desktop
thefile=(*""Screen Shot""*(om[1]))
echo ""Most recent screenshot is: $thefile""
</code></pre>
","16027"
"How to insert text after a certain string in a file?","168655","","<p>Right now I'm using <code>echo ""Hello World"" &gt;&gt; file.txt</code> to append some text to a file but I also need to add text below a certain string let's say ""[option]"", is it possible with <code>sed</code>?</p>

<p>EG:</p>

<p>Input file</p>

<pre><code>Some text
Random
[option]
Some stuff
</code></pre>

<p>Output file</p>

<pre><code>Some text
Random
[option]
*inserted text*
Some stuff
</code></pre>
","<p><strong>Append line after match</strong></p>

<ul>
<li><code>sed  '/\[option\]/a Hello World' input</code></li>
</ul>

<p><strong>Insert line before match</strong></p>

<ul>
<li><code>sed  '/\[option\]/i Hello World' input</code></li>
</ul>

<p>Additionally you can take backup and edit input file in-place using <code>-i.bkp</code> option to sed</p>
","121173"
"How can I find the hardware model in Linux?","168451","","<p>I used a system information utility to take the model number of a system, and also of the motherboard.</p>

<pre><code>DMI System Manufacturer     LENOVO
DMI System Product          2306CTO
DMI System Version          ThinkPad X230
DMI Motherboard Product     2306CTO  
</code></pre>

<p>Is there a way to get model number, in this case <code>2306CTO</code>, in Linux?   </p>
","<p>using the <code>dmidecode | grep -A3 '^System Information'</code> command. There you'll find all information from BIOS and hardware. These are examples on three different machines (this is an excerpt of the complete output):</p>

<pre><code>System Information
    Manufacturer: Dell Inc.
    Product Name: Precision M4700

System Information
    Manufacturer: MICRO-STAR INTERANTIONAL CO.,LTD
    Product Name: MS-7368

System Information
    Manufacturer: HP
    Product Name: ProLiant ML330 G6
</code></pre>
","75752"
"Changing extension to multiple files","168110","","<p>I would like to change a file extension from <code>*.txt</code> to <code>*.text</code>. I tried using the <code>basename</code> command, but I'm having trouble on changing more than one file.</p>

<p>I'm getting this error:</p>

<pre><code>basename: too many arguments Try basename --help' for more information
</code></pre>

<p>Here's my code:</p>

<pre><code>files=`ls -1 *.txt`

for x in $files
do
mv $x ""`basename $files .txt`.text""
done
</code></pre>
","<p>Straight from <a href=""http://mywiki.wooledge.org/BashFAQ/030"">Greg's Wiki</a>:</p>

<pre><code># Rename all *.txt to *.text
for f in *.txt; do 
mv -- ""$f"" ""${f%.txt}.text""
done</code></pre>

<p>Also see the entry on why you <a href=""http://mywiki.wooledge.org/ParsingLs"">shouldn't parse <code>ls</code></a>.</p>

<p>Edit: if you have to use <code>basename</code> your syntax would be:</p>

<pre><code>for f in *.txt; do
mv ""$f"" ""$(basename ""$f"" .txt).text""
done</code></pre>
","19656"
"How to append date to backup file","168001","","<p>I need to make a backup of a file, and I would like to have a timestamp as part of the name to make it easier to differentiate. </p>

<p>How would you inject the current date into a copy command? </p>

<pre><code>[root@mongo-test3 ~]# cp foo.txt {,.backup.`date`}
cp: target `2013}' is not a directory

[root@mongo-test3 ~]# cp foo.txt {,.backup. $((date)) }
cp: target `}' is not a directory  

[root@mongo-test3 ~]# cp foo.txt foo.backup.`date`
cp: target `2013' is not a directory
</code></pre>
","<p>This isn't working because the command <code>date</code> returns a string with spaces in it.</p>

<pre><code>$ date
Wed Oct 16 19:20:51 EDT 2013
</code></pre>

<p>If you truly want filenames like that you'll need to wrap that string in quotes.</p>

<pre><code>$ touch ""foo.backup.$(date)""

$ ll foo*
-rw-rw-r-- 1 saml saml 0 Oct 16 19:22 foo.backup.Wed Oct 16 19:22:29 EDT 2013
</code></pre>

<p>You're probably thinking of a different string to be appended would be my guess though. I usually use something like this:</p>

<pre><code>$ touch ""foo.backup.$(date +%F_%R)""
$ ll foo*
-rw-rw-r-- 1 saml saml 0 Oct 16 19:25 foo.backup.2013-10-16_19:25
</code></pre>

<p>See the <a href=""http://linux.die.net/man/1/date"">man page for date</a> for more formatting codes around the output for the date &amp; time.</p>

<h3>Additional formats</h3>

<p>If you want to take full control if you consult the man page you can do things like this:</p>

<pre><code>$ date +""%Y%m%d""
20131016

$ date +""%Y-%m-%d""
2013-10-16

$ date +""%Y%m%d_%H%M%S""
20131016_193655
</code></pre>
","96383"
"How to cycle through reverse-i-search in BASH?","167171","","<p>In the terminal, I can type <kbd>CTRL-R</kbd> to search for a matching command previously typed in BASH. E.g., if I type <kbd>CTRL-R</kbd> then <code>grep</code> is lists my last <code>grep</code> command, and I can hit enter to use it. This only gives one suggestion though. Is there any way to cycle through other previously typed matching commands?</p>
","<p>If I understand the question correctly you should be able to cycle through
alternatives by repeatedly hitting <kbd>Ctrl+R</kbd>.</p>

<p>E.g.:</p>

<p><kbd>Ctrl+R</kbd>
<code>grep</code>
<kbd>Ctrl+R</kbd>
<kbd>Ctrl+R</kbd>
...</p>
","73499"
"How to fix boot into initramfs prompt and ""mount: can't read '/etc/fstab': No such file or directory"" and ""No init found""?","166859","","<p>Installing a new system using a <strong>GPT</strong> partitioned disk dedicated to a single partition, <strong>ext4</strong> formatted, <strong>extlinux</strong> (version 4.05) as bootloader, Ubuntu Core version 13.10 amd64 as rootfs, and Ubuntu linux-image-3.11.0-18-generic as kernel, and extlinux-update to generate bootloader configuration.</p>

<p>The result after reboot (still inside a KVM based virtual machine) is an (initramfs) prompt and these messages:</p>

<pre><code>mount: can't read '/etc/fstab': No such file or directory
mount: mounting /dev on /root/dev failed: No such file or directory
mount: mounting /sys on /root/sys failed: No such file or directory
mount: mounting /proc on /root/proc failed: No such file or directory
The filesystem doesn't have requested /sbin/init.
No init found. Try passing init= bootarg.
</code></pre>

<p>BusyBox is v1.20.2.</p>

<p>Regression:</p>

<ul>
<li>the file system has been checked with <code>fsck.ext4</code></li>
</ul>

<h3>Check for root existence</h3>

<pre><code>(initramfs) ls -l /dev/[hs]da*
ls: /dev/[hs]da*: No such file or directory
</code></pre>

<h3>Root boot argument</h3>

<pre><code>(initramfs) cat /proc/cmdline
initrd=/boot/initrd.img-3.11.0-18-generic ro quiet BOOT_IMAGE=/boot/vmlinuz-3.11.0-18-generic
</code></pre>

<h3>Check loaded modules</h3>

<pre><code>(initramfs) cat /proc/modules
e1000 145368 0 - Live 0xffffffffa0000000
</code></pre>

<h3>/boot folder content</h3>

<pre><code>$ sudo ls -l boot
-rw------- 1 root root 3296162 Feb 18 22:37 System.map-3.11.0-18-generic
-rw-r--r-- 1 root root 1007681 Feb 18 22:37 abi-3.11.0-18-generic
-rw-r--r-- 1 root root  163258 Feb 18 22:37 config-3.11.0-18-generic
drwxr-xr-x 2 root root    4096 Mar 17 20:13 extlinux
-rw-r--r-- 1 root root 4995000 Mar 16 23:35 initrd.img-3.11.0-18-generic
-rw------- 1 root root 5634192 Feb 18 22:37 vmlinuz-3.11.0-18-generic
</code></pre>

<p>How to make this system boot to the expected default bash prompt?</p>
","<p>Modify your kernel boot parameter by setting the <code>root=/dev/sdaX</code> option. <code>sdaX</code> would be your <code>/</code> or <code>root</code> partition. Upon booting the next time, you will see that your <code>initramfs</code> tries to mount the partition before trying to access <code>/etc/fstab</code> and mounting the file systems. </p>

<p>See question ""<a href=""https://unix.stackexchange.com/questions/92720/does-initramfs-use-etc-fstab"">Does initramfs use /etc/fstab?</a>"" for more details. </p>
","120862"
"In a bash script, using the conditional ""or"" in an ""if"" statement","166812","","<p>This question is a sequel of sorts to my <a href=""https://unix.stackexchange.com/questions/47557/in-a-bash-shell-script-writing-a-for-loop-that-iterates-over-string-values"">earlier question</a>.  The users on this site kindly helped me determine how to write a bash <code>for</code> loop that iterates over string values.  For example, suppose that a loop control variable <code>fname</code> iterates over the strings <code>""a.txt"" ""b.txt"" ""c.txt""</code>.  I would like to <code>echo</code> ""yes!"" when <code>fname</code> has the value <code>""a.txt""</code> or <code>""c.txt""</code>, and <code>echo</code> ""no!"" otherwise.  I have tried the following bash shell script:</p>

<pre><code>#!/bin/bash

for fname in ""a.txt"" ""b.txt"" ""c.txt""
do
 echo $fname
 if [ ""$fname"" = ""a.txt"" ] | [ ""$fname"" = ""c.txt"" ]; then
 echo ""yes!""
else
 echo ""no!""
fi
done
</code></pre>

<p>I obtain the output:</p>

<blockquote>
  <p>a.txt</p>
  
  <p>no!</p>
  
  <p>b.txt</p>
  
  <p>no!</p>
  
  <p>c.txt</p>
  
  <p>yes!</p>
</blockquote>

<p>Why does the <code>if</code> statement apparently yield true when <code>fname</code> has the value <code>""a.txt""</code>?  Have I used <code>|</code> incorrectly?</p>
","<p>If you want to say <code>OR</code> use double pipe (<code>||</code>).</p>

<pre><code>if [ ""$fname"" = ""a.txt"" ] || [ ""$fname"" = ""c.txt"" ]
</code></pre>

<p>(The original OP code using <code>|</code> was simply piping the output of the left side to the right side, in the same way any ordinary pipe works.)</p>
","47585"
"command to determine ports of a device (like /dev/ttyUSB0)","166242","","<p>I have a question regarding the ports in Linux. If I connect my device via USB and want to check its port I can't do it using the command lsusb, which only specifies bus number and device number on this bus:</p>

<pre><code>[ziga@Ziga-PC ~]$ lsusb
Bus 003 Device 007: ID 0403:6001 Future Technology Devices International, Ltd FT232 USB-Serial (UART) IC
</code></pre>

<p><strong>Is there a command that tells me the port the device is connected to directly?</strong> Only way to do this until now was to disconect and reconnect and using the command:</p>

<pre><code>[ziga@Ziga-PC ~]$ dmesg | grep tty
[    0.000000] console [tty0] enabled
[    0.929510] 00:09: ttyS0 at I/O 0x3f8 (irq = 4) is a 16550A
[    4.378109] systemd[1]: Starting system-getty.slice.
[    4.378543] systemd[1]: Created slice system-getty.slice.
[    8.786474] usb 3-4.4: FTDI USB Serial Device converter now attached to ttyUSB0
</code></pre>

<p>In the last line it can be seen that my device is connected to <strong>/dev/ttyUSB0</strong>.</p>
","<p>I'm not quite certain what you're asking. You mention 'port' several times, but then in your example, you say the answer is <code>/dev/ttyUSB0</code>, which is a device dev path, not a port. So this answer is about finding the dev path for each device.</p>

<p>Below is a quick and dirty script which walks through devices in <code>/sys</code> looking for USB devices with a <code>ID_SERIAL</code> attribute. Typically only real USB devices will have this attribute, and so we can filter with it. If we don't, you'll see a lot of things in the list that aren't physical devices.</p>

<pre><code>#!/bin/bash

for sysdevpath in $(find /sys/bus/usb/devices/usb*/ -name dev); do
    (
        syspath=""${sysdevpath%/dev}""
        devname=""$(udevadm info -q name -p $syspath)""
        [[ ""$devname"" == ""bus/""* ]] &amp;&amp; continue
        eval ""$(udevadm info -q property --export -p $syspath)""
        [[ -z ""$ID_SERIAL"" ]] &amp;&amp; continue
        echo ""/dev/$devname - $ID_SERIAL""
    )
done
</code></pre>

<p>On my system, this results in the following:</p>

<pre><code>/dev/ttyACM0 - LG_Electronics_Inc._LGE_Android_Phone_VS930_4G-991c470
/dev/sdb - Lexar_USB_Flash_Drive_AA26MYU15PJ5QFCL-0:0
/dev/sdb1 - Lexar_USB_Flash_Drive_AA26MYU15PJ5QFCL-0:0
/dev/input/event5 - Logitech_USB_Receiver
/dev/input/mouse1 - Logitech_USB_Receiver
/dev/input/event2 - Razer_Razer_Diamondback_3G
/dev/input/mouse0 - Razer_Razer_Diamondback_3G
/dev/input/event3 - Logitech_HID_compliant_keyboard
/dev/input/event4 - Logitech_HID_compliant_keyboard
</code></pre>

<hr>

<h3>Explanation:</h3>

<pre><code>find /sys/bus/usb/devices/usb*/ -name dev
</code></pre>

<p>Devices which show up in <code>/dev</code> have a <code>dev</code> file in their <code>/sys</code> directory. So we search for directories matching this criteria.<br>
&nbsp;</p>

<pre><code>syspath=""${sysdevpath%/dev}""
</code></pre>

<p>We want the directory path, so we strip off <code>/dev</code>.<br>
&nbsp;</p>

<pre><code>devname=""$(udevadm info -q name -p $syspath)""
</code></pre>

<p>This gives us the path in <code>/dev</code> that corresponds to this <code>/sys</code> device.<br>
&nbsp;</p>

<pre><code>[[ ""$devname"" == ""bus/""* ]] &amp;&amp; continue
</code></pre>

<p>This filters out things which aren't actual devices. Otherwise you'll get things like USB controllers &amp; hubs.<br>
&nbsp;</p>

<pre><code>eval ""$(udevadm info -q property --export -p $syspath)""
</code></pre>

<p>The <code>udevadm info -q property --export</code> command lists all the device properties in a format that can be parsed by the shell into variables. So we simply call <code>eval</code> on this. This is also the reason why we wrap the code in the parenthesis, so that we use a subshell, and the variables get wiped on each loop.<br>
&nbsp;</p>

<pre><code>[[ -z ""$ID_SERIAL"" ]] &amp;&amp; continue
</code></pre>

<p>More filtering of things that aren't actual devices.<br>
&nbsp;</p>

<pre><code>echo ""/dev/$devname - $ID_SERIAL""
</code></pre>

<p>I hope you know what this line does :-)</p>
","144735"
"How to display open file descriptors but not using lsof command","164931","","<p>Hi I have read <a href=""http://www.netadmintools.com/art295.html"">Here</a> that lsof is not an accurate way of getting the number of File Descriptors that are currently open. He recommended to use this command instead</p>

<pre><code> cat /proc/sys/fs/file-nr
</code></pre>

<p>While this command displays the number of FD's, how do you display the list of open file descriptors that the command above just counted?</p>
","<p>There are two reasons <code>lsof | wc -l</code> doesn't count file descriptors. One is that it lists things that aren't open files, such as loaded dynamically linked libraries and current working directories; you need to filter them out. Another is that <code>lsof</code> takes some time to run, so can miss files that are opened or closed while it's running; therefore the number of listed open files is approximate. Looking at <code>/proc/sys/fs/file-nr</code> gives you an exact value at a particular point in time.</p>

<p><code>cat /proc/sys/fs/file-nr</code> is only useful when you need the exact figure, mainly to check for resource exhaustion. If you want to list the open files, you need to call <code>lsof</code>, or use some equivalent method such as trawling <code>/proc/*/fd</code> manually.</p>
","66237"
"Can grep output only specified groupings that match?","163616","","<p>Say I have a file:</p>

<pre><code># file: 'test.txt'
foobar bash 1
bash
foobar happy
foobar
</code></pre>

<p>I only want to know what words appear after ""foobar"", so I can use this regex:</p>

<pre><code>""foobar \(\w\+\)""
</code></pre>

<p>The parenthesis indicate that I have a special interest in the word right after foobar.  But when I do a <code>grep ""foobar \(\w\+\)"" test.txt</code>, I get the entire lines that match the entire regex, rather than just ""the word after foobar"":</p>

<pre><code>foobar bash 1
foobar happy
</code></pre>

<p>I would much prefer that the output of that command looked like this:</p>

<pre><code>bash
happy
</code></pre>

<p>Is there a way to tell grep to only output the items that match the grouping (or a specific grouping) in a regular expression?</p>
","<p>GNU grep has the <code>-P</code> option for perl-style regexes, and the <code>-o</code> option to print only what matches the pattern. These can be combined using look-around assertions (described under <a href=""http://perldoc.perl.org/perlre.html#Extended-Patterns"">Extended Patterns in the perlre manpage</a>) to remove part of the grep pattern from what is determined to have matched for the purposes of <code>-o</code>.</p>

<pre><code>$ grep -oP 'foobar \K\w+' test.txt
bash
happy
$
</code></pre>

<p>The <code>\K</code> is the short-form (and more efficient form) of <code>(?&lt;=pattern)</code> which you use as a zero-width look-behind assertion before the text you want to output. <code>(?=pattern)</code> can be used as a zero-width look-ahead assertion after the text you want to output.</p>

<p>For instance, if you wanted to match the word between <code>foo</code> and <code>bar</code>, you could use:</p>

<pre><code>$ grep -oP 'foo \K\w+(?= bar)' test.txt
</code></pre>

<p>or (for symmetry)</p>

<pre><code>$ grep -oP '(?&lt;=foo )\w+(?= bar)' test.txt
</code></pre>
","13472"
"Limit memory usage for a single Linux process","162834","","<p>I'm running <code>pdftoppm</code> to convert a user-provided PDF into a 300DPI image.  This works great, except if the user provides an PDF with a very large page size.  <code>pdftoppm</code> will allocate enough memory to hold a 300DPI image of that size in memory, which for a 100 inch square page is 100*300 * 100*300 * 4 bytes per pixel = 3.5GB.  A malicious user could just give me a silly-large PDF and cause all kinds of problems.</p>

<p>So what I'd like to do is put some kind of hard limit on memory usage for a child process I'm about to run--just have the process die if it tries to allocate more than, say, 500MB of memory.  Is that possible?</p>

<p>I don't think ulimit can be used for this, but is there a one-process equivalent?</p>
","<p>There's some problems with ulimit. Here's a useful read on the topic: <a href=""http://coldattic.info/shvedsky/pro/blogs/a-foo-walks-into-a-bar/posts/40"" rel=""noreferrer"">Limiting time and memory consumption of a program in Linux</a>, which lead to the <a href=""https://github.com/pshved/timeout"" rel=""noreferrer"">timeout</a> tool, which lets you cage a process (and its forks) by time or memory consumption.</p>

<p>The timeout tool requires Perl 5+ and the <code>/proc</code> filesystem mounted. After that you copy the tool to e.g. <code>/usr/local/bin</code> like so:</p>

<pre><code>curl https://raw.githubusercontent.com/pshved/timeout/master/timeout \
  sudo tee /usr/local/bin/timeout &amp;&amp; sudo chmod 755 /usr/local/bin/timeout
</code></pre>

<p>After that, you can 'cage' your process by memory consumption as in your question like so:</p>

<pre><code>timeout -m 500 pdftoppm Sample.pdf
</code></pre>

<p>Alternatively you could use <code>-t &lt;seconds&gt;</code> and <code>-x &lt;hertz&gt;</code> to respectively limit the process by time or cpu constraints.</p>

<p>The way this tool works is by checking multiple times per second if the spawned process has not oversubscribed its set boundaries. This means there actually is a small window where a process <em>could potentially</em> be oversubscribing before timeout notices and kills the process.</p>

<p>A more correct approach would hence likely involve cgroups, but that is much more involved to set up, even if you'd use Docker or runC, which among things, offer a more user-friendly abstraction around cgroups.</p>
","44988"
"Show sum of file sizes in directory listing","162553","","<p>The Windows <code>dir</code> directory listing command has a line at the end showing the total amount of space taken up by the files listed. For example, <code>dir *.exe</code> shows all the <code>.exe</code> files in the current directory, their sizes, and the sum total of their sizes. I'd love to have similar functionality with my <code>dir</code> alias in bash, but I'm not sure exactly how to go about it.</p>

<p>Currently, I have <code>alias dir='ls -FaGl'</code> in my <code>.bash_profile</code>, showing </p>

<pre><code>drwxr-x---+  24 mattdmo  4096 Mar 14 16:35 ./
drwxr-x--x. 256 root    12288 Apr  8 21:29 ../
-rw-------    1 mattdmo 13795 Apr  4 17:52 .bash_history
-rw-r--r--    1 mattdmo    18 May 10  2012 .bash_logout
-rw-r--r--    1 mattdmo   395 Dec  9 17:33 .bash_profile
-rw-r--r--    1 mattdmo   176 May 10  2012 .bash_profile~
-rw-r--r--    1 mattdmo   411 Dec  9 17:33 .bashrc
-rw-r--r--    1 mattdmo   124 May 10  2012 .bashrc~
drwx------    2 mattdmo  4096 Mar 24 20:03 bin/
drwxrwxr-x    2 mattdmo  4096 Mar 11 16:29 download/
</code></pre>

<p>for example. Taking the answers from <a href=""https://unix.stackexchange.com/questions/21678/is-there-a-way-to-sum-up-the-size-of-files-listed"">this question</a>:</p>

<pre><code>dir | awk '{ total += $4 }; END { print total }'
</code></pre>

<p>which gives me the total, but doesn't print the directory listing itself. Is there a way to alter this into a one-liner or shell script so I can pass any <code>ls</code> arguments I want to <code>dir</code> and get a full listing plus sum total? For example, I'd like to run <code>dir -R *.jpg *.tif</code> to get the listing and total size of those file types in all subdirectories. Ideally, it would be great if I could get the size of each subdirectory, but this isn't essential.</p>
","<p>The following function does most of what you're asking for:</p>

<pre><code>dir () { ls -FaGl ""${@}"" | awk '{ total += $4; print }; END { print total }'; }
</code></pre>

<p>... but it won't give you what you're asking for from <code>dir -R *.jpg *.tif</code>, because that's not how <code>ls -R</code> works. You might want to play around with the <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?find""><code>find</code></a> utility for that.</p>
","72665"
"How do I list all installed programs?","162353","","<p>How do I list both programs that came with my distribution and those I manually installed?</p>
","<p>That depends on your distribution.</p>

<ul>
<li>Aptitude-based distributions (Ubuntu, Debian, etc): <code>dpkg -l</code></li>
<li>RPM-based distributions (Fedora, RHEL, etc): <code>rpm -qa</code></li>
<li>pkg*-based distributions (OpenBSD, FreeBSD, etc): <code>pkg_info</code></li>
<li>Portage-based distributions (Gentoo, etc): <code>equery list</code> or <a href=""http://eix.berlios.de/""><code>eix -I</code></a></li>
<li>pacman-based distributions (Arch Linux, etc): <code>pacman -Q</code></li>
<li>Cygwin: <code>cygcheck --check-setup --dump-only *</code></li>
<li>Slackware: <code>slapt-get --installed</code></li>
</ul>

<p>All of these will list the <em>packages</em> rather than the <em>programs</em> however. If you truly want to list the programs, you probably want to list the executables in your <code>$PATH</code>, which can be done like so using bash's <code>compgen</code>:</p>

<pre><code>compgen -c
</code></pre>

<p>Or, if you don't have <code>compgen</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>#!/bin/bash
IFS=: read -ra dirs_in_path &lt;&lt;&lt; ""$PATH""

for dir in ""${dirs_in_path[@]}""; do
    for file in ""$dir""/*; do
        [[ -x $file &amp;&amp; -f $file ]] &amp;&amp; printf '%s\n' ""${file##*/}""
    done
done
</code></pre>
","20981"
"Shell script fails: Syntax error: ""("" unexpected","162181","","<p>I've been working on a script that automates setting up a development environment for Raspberry Pi development (step by step details that work are <a href=""http://bassetlug.org.uk/node/15"">here</a>). The script is linked in that article but convenience you can find it <a href=""https://github.com/kemra102/bash_scripts/blob/master/raspberrypi/pi_dev_env_install.sh"">here</a> also. Now when run this script install and sets up the environment without error but you have to enter your sudo password more than once due to sudo's time-out value by default. So I started experimenting by removing all the sudo lines and running the whole script via sudo at the command line like so:</p>

<pre><code>kemra102@ubuntuvm:~$ sudo ./pi_dev_env_install.sh
</code></pre>

<p>This works fine as expected and gets most of the way through until this point:</p>

<pre><code>./pi_dev_env_install: 68: ./pi_dev_env_install.sh: Syntax error: ""("" unexpected
</code></pre>

<p>Now this line worked fine previously when not running the whole script with sudo. There is nothing about this line running as sudo that should stop it working to my knowledge, does anyone have any ideas?</p>
","<p>The script does not begin with a <a href=""http://en.wikipedia.org/wiki/Shebang_%28Unix%29"" rel=""nofollow noreferrer"">shebang</a> line, so the system executes it with <code>/bin/sh</code>. On Ubuntu, <code>/bin/sh</code> is <a href=""http://en.wikipedia.org/wiki/Debian_Almquist_shell"" rel=""nofollow noreferrer"">dash</a>, a shell designed for fast startup and execution with only standard features. When dash reaches line 68, it sees a syntax error: that parenthesis doesn't mean anything to it in context.</p>

<p>Since dash (like all other shells) is an interpreter, it won't complain until the execution reaches the problematic line. So even if the script successfully started at some point in your testing, it would have aborted once line 68 was reached.</p>

<p>The shebang line must be the very first thing in the file. Since you use bash features, the first line of the file must be <code>#!/bin/bash</code> or <code>#!/usr/bin/env bash</code>.</p>
","45784"
"How to count the number of a specific character in each line?","161908","","<p>I was wondering how to count the number of a specific character in each line by some text processing utilities?</p>

<p>For example, to count <code>""</code> in each line of the following text</p>

<pre><code>""hello!"" 
Thank you!
</code></pre>

<p>The first line has two, and the second line has 0. </p>

<p>Another example is to count <code>(</code> in each line.</p>
","<p>You can do it with <code>sed</code> and <code>awk</code>:</p>

<pre><code>$ sed 's/[^""]//g' dat | awk '{ print length }'
2
0
</code></pre>

<p>Where <code>dat</code> is your example text, sed deletes (for each line) all non-<code>""</code> characters and <code>awk</code> prints for each line its size (i.e. <code>length</code> is equivalent to <code>length($0)</code>, where <code>$0</code> denotes the current line).</p>

<p>For another character you just have to change the sed expression. For example for <code>(</code> to:</p>

<pre><code>'s/[^(]//g'
</code></pre>

<p><strong>Update:</strong> <code>sed</code> is kind of overkill for the task - <code>tr</code> is sufficient. An equivalent solution with <code>tr</code> is:</p>

<pre><code>$ tr -d -c '""\n' &lt; dat | awk '{ print length; }'
</code></pre>

<p>Meaning that <code>tr</code> deletes all characters which are not (<code>-c</code> means complement) in the character set <code>""\n</code>.</p>
","18742"
"How can I reduce a video's size with ffmpeg?","160681","","<p>How can I use <code>ffmpeg</code> to reduce the size of a video by lowering the quality (as minimally as possible, naturally, but I need it to run on a mobile device that doesn't have much available space)?</p>

<p>I forgot to write one thing yet. When the video can use subtitles (*.srt or *.sub) I'd like to convert them too to fit the parameters of converted video file.</p>
","<p>See <a href=""https://superuser.com/a/4252/128227"">this</a> answer. Quoted below for convenience:</p>

<blockquote>
  <p>Calculate the bitrate you need by dividing 1 GB by the video length in seconds. So, for a video of length 16:40 (1000 seconds), use a bitrate of 1000000 bytes/sec:</p>

<pre><code>ffmpeg -i input.mp4 -b 1000000 output.mp4
</code></pre>
  
  <p>Additional options that might be worth considering is setting the Constant Rate Factor, which lowers the average bit rate, but retains better quality. Vary the CRF between around 18 and 24 — the lower, the higher the bitrate.</p>

<pre><code>ffmpeg -i input.mp4 -vcodec libx264 -crf 20 output.mp4
</code></pre>
</blockquote>
","38380"
"Confusing use of && and || operators","160398","","<p>I was skimming through an <code>/etc/rc.d/init.d/sendmail</code> file (I know this is hardly ever used, but I'm studying for an exam), and I've become a bit confused about the <code>&amp;&amp;</code> and the <code>||</code> operators.  I've read where they can be used in statements such as:</p>

<pre><code>if [ test1 ] &amp;&amp; [ test2 ]; then
     echo ""both tests are true""
elif [ test1 ] || [ test2 ]; then
     echo ""one test is true""
fi
</code></pre>

<p>However, this script shows single line statements such as:</p>

<pre><code>[ -z ""$SMQUEUE"" ] &amp;&amp; SMQUEUE=""QUEUE""
[ -f /usr/sbin/sendmail ] || exit 0
</code></pre>

<p>These seem to be using the <code>&amp;&amp;</code> and <code>||</code> operators to elicit responses based on tests, but I haven't been able to dig up documenation regarding this particular use of these operators.  Can anyone explain what these do in this particular context?</p>
","<p>The right side of <code>&amp;&amp;</code> will only be evaluated if the exit status of the left side is zero. <code>||</code> is the opposite: it will evaluate the right side only if the left side exit status is nonzero.
You can consider <code>[ ... ]</code> to be a program with a return value. If the test inside evaluates to true, it returns zero; it returns nonzero otherwise.</p>

<p><strong>Examples:</strong></p>

<pre><code>$ false &amp;&amp; echo howdy!

$ true &amp;&amp; echo howdy!
howdy!
$ true || echo howdy!

$ false || echo howdy!
howdy!
</code></pre>

<p><strong>Extra notes:</strong></p>

<p>If you do <code>which [</code>, you might see that <code>[</code> actually does point to a program! It's usually not actually the one that runs in scripts, though; run <code>type [</code> to see what actually gets run. If you wan to try using the program, just give the full path like so: <code>/bin/[ 1 = 1</code>.</p>
","24685"
"What does ""LC_ALL=C"" do?","160379","","<p>What does the <code>C</code> value for <code>LC_ALL</code> do in Unix-like systems? </p>

<p>I know that it forces the same locale for all aspects but what does <code>C</code> do?</p>
","<p>It forces applications to use the default language for output, and forces sorting to be bytewise.</p>

<pre><code>$ LC_ALL=es_ES man
¿Qué página de manual desea?
$ LC_ALL=C man
What manual page do you want?
$ LC_ALL=en_US sort &lt;&lt;&lt; $'a\nb\nA\nB'
a
A
b
B
$ LC_ALL=C sort &lt;&lt;&lt; $'a\nb\nA\nB'
A
B
a
b
</code></pre>
","87748"
"What does ` (backquote/backtick) mean in commands?","159484","","<p>I came across the following command:</p>

<pre><code>sudo chown `id -u` /somedir
</code></pre>

<p>and I wonder: what is the meaning of the <code>`</code> symbol. I noticed for instance that while the command above works well, the one below does not:</p>

<pre><code>sudo chown 'id -u' /somedir
</code></pre>
","<p>This is a <strong>backtick</strong>. <strong>Backtick is not a quotation sign</strong>, it has a very special meaning. Everything you type between backticks is evaluated (executed) by the shell before the main command (like <code>chown</code> in your examples), and the <em>output</em> of that execution is used by that command, just as if you'd type that output at that place in the command line.</p>

<p>So, what </p>

<pre><code>sudo chown `id -u` /somedir
</code></pre>

<p>effectively runs (depending on <em>your user ID</em>) is:</p>

<pre><code>sudo chown 1000 /somedir
  \    \     \     \
   \    \     \     `-- the second argument to ""chown"" (target directory)
    \    \     `-- your user ID, which is the output of ""id -u"" command
     \    `-- ""chown"" command (change ownership of file/directory)
      `-- the ""run as root"" command; everything after this is run with root privileges
</code></pre>

<p>Have a look at <a href=""https://unix.stackexchange.com/questions/5778/whats-the-difference-between-stuff-and-stuff"">this question</a> to learn why, in many situations, it is not a good idea to use backticks.</p>

<p>Btw, if you ever wanted to use a backtick literally, <em>e.g.</em> in a string, you can excape it by placing a backslash (<code>\</code>) before it.</p>
","27432"
"Recursive grep vs find / -type f -exec grep {} \; Which is more efficient/faster?","158243","","<p>Which is more efficient for finding which files in an entire filesystem contain a string: recursive grep or find with grep in an exec statement?  I assume find would be more efficient because you can at least do some filtering if you know the file extension or a regex that matches the file name, but when you only know <code>-type f</code> which is better? GNU grep 2.6.3; find (GNU findutils) 4.4.2</p>

<p>Example:</p>

<p><code>grep -r -i 'the brown dog' /</code></p>

<p><code>find / -type f -exec grep -i 'the brown dog' {} \;</code></p>
","<p>I'm not sure:</p>

<blockquote>
<pre><code>grep -r -i 'the brown dog' /*
</code></pre>
</blockquote>

<p>is really what you meant. That would mean grep recursively in all the non-hidden files and dirs in <code>/</code> (but still look inside hidden files and dirs inside those).</p>

<p>Assuming you meant:</p>

<pre><code>grep -r -i 'the brown dog' /
</code></pre>

<p>A few things to note:</p>

<ul>
<li>Not all <code>grep</code> implementations support <code>-r</code>. And among those that do, the behaviours differ: some follow symlinks to directories when traversing the directory tree (which means you may end up looking several times in the same file or even run in infinite loops), some will not. Some will look inside device files (and it will take quite some time in <code>/dev/zero</code> for instance) or pipes or binary files..., some will not.</li>
<li>It's efficient as <code>grep</code> starts looking inside files as soon as it discovers them. But while it looks in a file, it's no longer looking for more files to search in (which is probably just as well in most cases)</li>
</ul>

<p>Your:</p>

<pre><code>find / -type f -exec grep -i 'the brown dog' {} \;
</code></pre>

<p>(removed the <code>-r</code> which didn't make sense here) is terribly inefficient because you're running one <code>grep</code> per file. <code>;</code> should only be used for commands that accept only one argument. Moreover here, because <code>grep</code> looks only in one file, it will not print the file name, so you won't know where the matches are.</p>

<p>You're not looking inside device files, pipes, symlinks..., you're not following symlinks, but you're still potentially looking inside things like <code>/proc/mem</code>.</p>

<pre><code>find / -type f -exec grep -i 'the brown dog' {} +
</code></pre>

<p>would be a lot better because as few <code>grep</code> commands as possible would be run. You'd get the file name unless the last run has only one file. For that it's better to use:</p>

<pre><code>find / -type f -exec grep -i 'the brown dog' /dev/null {} +
</code></pre>

<p>or with GNU <code>grep</code>:</p>

<pre><code>find / -type f -exec grep -Hi 'the brown dog' {} +
</code></pre>

<p>Note that <code>grep</code> will not be started until <code>find</code> has found enough files for it to chew on, so there will be some initial delay. And <code>find</code> will not carry on searching for more files until the previous <code>grep</code> has returned. Allocating and passing the big file list has some (probably negligible) impact, so all in all it's probably going to be less efficient than a <code>grep -r</code> that doesn't follow symlink or look inside devices.</p>

<p>With GNU tools:</p>

<pre><code>find / -type f -print0 | xargs -r0 grep -Hi 'the brown dog'
</code></pre>

<p>As above, as few <code>grep</code> instances as possible will be run, but <code>find</code> will carry on looking for more files while the first <code>grep</code> invocation is looking inside the first batch. That may or may not be an advantage though. For instance, with data stored on rotational hard drives, <code>find</code> and <code>grep</code> accessing data stored at different locations on the disk will slow down the disk throughput by causing the disk head to move constantly. In a RAID setup (where <code>find</code> and <code>grep</code> may access different disks) or on SSDs, that might make a positive difference.</p>

<p>In a RAID setup, running several <em>concurrent</em> <code>grep</code> invocations might also improve things. Still with GNU tools on RAID1 storage with 3 disks,</p>

<pre><code>find / -type f -print0 | xargs -r0 -P2 grep -Hi 'the brown dog'
</code></pre>

<p>might increase the performance significantly. Note however that the second <code>grep</code> will only be started once enough files have been found to fill up the first <code>grep</code> command. You can add a <code>-n</code> option to <code>xargs</code> for that to happen sooner (and pass fewer files per <code>grep</code> invocation).</p>

<p>Also note that if you're redirecting <code>xargs</code> output to anything but a terminal device, then the <code>greps</code>s will start buffering their output which means that the output of those <code>grep</code>s will probably be incorrectly interleaved. You'd have to use <code>stdbuf -oL</code> (where available like on GNU or FreeBSD) on them to work around that (you may still have problems with very long lines (typically >4KiB)) or have each write their output in a separate file and concatenate them all in the end.</p>

<p>Here, the string you're looking for is fixed (not a regexp) so using the <code>-F</code> option might make a difference (unlikely as <code>grep</code> implementations know how to optimise that already).</p>

<p>Another thing that could make a big difference is fixing the locale to C if you're in a multi-byte locale:</p>

<pre><code>find / -type f -print0 | LC_ALL=C xargs -r0 -P2 grep -Hi 'the brown dog'
</code></pre>

<p>To avoid looking inside <code>/proc</code>, <code>/sys</code>..., use <code>-xdev</code> and specify the file systems you want to search in:</p>

<pre><code>LC_ALL=C find / /home -xdev -type f -exec grep -i 'the brown dog' /dev/null {} +
</code></pre>

<p>Or prune the paths you want to exclude explicitly:</p>

<pre><code>LC_ALL=C find / \( -path /dev -o -path /proc -o -path /sys \) -prune -o \
  -type f -exec grep -i 'the brown dog' /dev/null {} +
</code></pre>
","131576"
"Scroll inside Screen, or Pause Output","157836","","<p>I use <a href=""http://www.gnu.org/software/screen/manual/screen.html"">screen</a> for my command-line tasks while managing the servers where I work. I usually run small commands (mostly file-system tasks) but sometimes I run more extensive tasks (like DBA).</p>

<p>The <em>output</em> of those tasks is important to me. Since I use Ubuntu and OS X (both Terminal Windows) for my tasks, yet I need to use screen, the scrolling is not available, so any long output (think a 500-row table from a select) is invisible for me. Mouse-wheel is out of the question.</p>

<p>When I say ""scroll is invisible for me, I mean this:</p>

<p><img src=""https://i.stack.imgur.com/kAIBx.png"" alt=""top while using screen, in Mac OS X, while scrolling""></p>

<p>I was thinking about two options:</p>

<ol>
<li><p>Pause (think <em>paginate</em>) the output of a certain command. When the output begins, it would let me read what's happening, then I press ""Enter"", then the output continues until there's nothing more to show.</p></li>
<li><p>Scroll inside screen. But I don't know if this is possible.</p></li>
</ol>

<p>Of course, I don't know if those options are actually <em>possible</em>. If they are, how can achieve them? Other alternatives will be well received.</p>
","<p>Screen has its own scroll buffer, as it is a terminal multiplexer and has to deal with <em>several</em> buffers. </p>

<p>Maybe there's a better way, but I'm used to scrolling using the ""copy mode"" (which you can use to copy text using screen itself, although that requires the paste command too):</p>

<ul>
<li><p>Hit your screen prefix combination (<code>C-a</code> / <kbd>control</kbd>+<kbd>A</kbd> by default), then hit <kbd>Escape</kbd>.</p></li>
<li><p>Move up/down with the arrow keys (<kbd>&uarr;</kbd> and <kbd>&darr;</kbd>).</p></li>
<li><p>When you're done, hit <kbd>q</kbd> or <kbd>Escape</kbd> to get back to the end of the scroll buffer.</p></li>
</ul>

<p>(If instead of <kbd>q</kbd> or <kbd>Escape</kbd> you hit <kbd>Enter</kbd> or <kbd>Return</kbd> and then move the cursor, you will be selecting text to copy, and hitting <kbd>Enter</kbd> or <kbd>Return</kbd> a second time will copy it. Then you can paste with <code>C-a</code> followed by <code>]</code>.)</p>

<p>Of course, you can always use <code>more</code> and <code>less</code>, two commonly used pagers, which may be enough for some commands.</p>
","40243"
"How do you change the root password on Debian?","157682","","<p>I want to change the password I assigned to root on my Debian webserver to something longer and more secure.</p>

<p>Bit of an obvious newbie question, but how do I do that? I haven’t forgotten/lost the current password, I just want to change it.</p>
","<p>Ah, <a href=""http://www.linuxquestions.org/questions/linux-newbie-8/change-my-root-password-in-debian-linux-if-i-have-the-current-password-693323/"" rel=""noreferrer"">use the <code>passwd</code> program</a> <a href=""https://unix.stackexchange.com/q/3063/250"">as root</a>:</p>

<pre><code>sudo passwd root
</code></pre>

<p>Or, if you’re running as root already (which you shouldn’t be), just:</p>

<pre><code>passwd
</code></pre>

<p>The <code>root</code> argument can be omitted, because when you execute <code>passwd</code> it defaults to the current user (which is root, as only root can change the root password).</p>
","10096"
"Better colors so comments aren't dark blue in Vim?","157197","","<p>Mostly I edit Ruby files, although shell script file comments are also <code>#</code></p>

<p>Currently my comments show as dark blue on black which is really hard to read.</p>

<p>See screenshot.</p>

<p>How can I change their color?</p>

<p>I'm willing to consider different schemas for all colors though I do like the black background as a base.</p>

<p><img src=""https://i.stack.imgur.com/7TlPT.png"" alt=""A screenshot of a terminal window with Vim running in it. The comments are dark blue on black background making them hardly visible.""></p>
","<p>There are many color schemes which are usually distributed together with vim. You can select them with the <code>:color</code> command.</p>

<p>You can see the available color schemes in vim's <code>colors</code> folder, for example in my case: </p>

<pre><code>$ ls /usr/share/vim/vimNN/colors/ # where vimNN is vim version, e.g. vim74
blue.vim  darkblue.vim  default.vim  delek.vim  desert.vim  elflord.vim 
evening.vim  koehler.vim  morning.vim  murphy.vim  pablo.vim  peachpuff.vim
README.txt  ron.vim  shine.vim  slate.vim  torte.vim  zellner.vim
</code></pre>

<p>I usually use <code>desert</code>. So I open <code>vim</code>, then enter <code>:color desert</code> and enter. To have the color scheme by default every time you open <code>vim</code>, add <code>:color desert</code> into your <code>~/.vimrc</code>.</p>

<p>(Michael, OP)  This was good.  The terminal looks like:</p>

<p><img src=""https://i.stack.imgur.com/jRuUn.png"" alt=""enter image description here""></p>
","88880"
"How to recursively find the amount stored in directory?","156578","","<p>I know you are able to see the byte size of a file when you do a long listing with <code>ll</code> or <code>ls -l</code>. But I want to know how much storage is in a directory including the files within that directory and the subdirectories within there, etc. I don't want the number of files, but instead the amount of storage those files take up.</p>

<p>So I want to know how much storage is in a certain directory recursively? I'm guessing, if there is a command, that it would be in bytes.</p>
","<p>Try doing this :</p>

<pre><code>du -s dir
</code></pre>

<p>or</p>

<pre><code>du -sh dir
</code></pre>

<p>needs <code>-h</code> support, depends of your OS.</p>

<p>See </p>

<pre><code>man du
</code></pre>
","67808"
"Manually generate password for /etc/shadow","156504","","<p>I need to manually edit <code>/etc/shadow</code> to change the root password inside of a virtual machine image.</p>

<p>Is there a command-line tool that takes a password and generates an <code>/etc/shadow</code> compatible password hash on standard out? </p>
","<p>You can use following commands for the same:</p>

<h3>Method 1</h3>

<pre><code>openssl passwd -1 -salt xyz  yourpass
</code></pre>

<h3>Method 2</h3>

<pre><code>makepasswd --clearfrom=- --crypt-md5 &lt;&lt;&lt; YourPass
</code></pre>

<h3>Method 3</h3>

<p>As @tink suggested, we can update password using <code>chpasswd</code> using :</p>

<pre><code>echo ""username:password"" | chpasswd 
</code></pre>

<p>Or you can use encrypted password with <code>chpasswd</code>. First generate it using this:</p>

<pre><code>perl -e 'print crypt(""YourPasswd"", ""salt""),""\n""'
</code></pre>

<p>Then later you can use generated password to update:</p>

<pre><code>echo ""username:encryptedPassWd""  | chpasswd -e
</code></pre>

<p>This encrypted password we can use to create a new user with password, for example:</p>

<pre><code>useradd -p 'encryptedPassWd'  username
</code></pre>

<h3>Method 4</h3>

<pre><code>echo -e ""md5crypt\npassword"" | grub | grep -o ""\$1.*""
</code></pre>
","81248"
"Using ""${a:-b}"" for variable assignment in scripts","155844","","<p>I have been looking at a few scripts other people wrote (specifically Red Hat), and a lot of their variables are assigned using the following notation
<code>VARIABLE1=""${VARIABLE1:-some_val}""</code>
or some expand other variables
<code>VARIABLE2=""${VARIABLE2:-`echo $VARIABLE1`}""</code></p>

<p>What is the point of using this notation instead of just declaring the values directly (e.g., <code>VARIABLE1=some_val</code>)?</p>

<p>Are there benefits to this notation or possible errors that would be prevented?</p>

<p>Does the <code>:-</code> have specific meaning in this context?</p>
","<p>This technique allows for a variable to be assigned a value if another variable is either empty or is undefined. <strong>NOTE:</strong> This ""other variable"" can be the same or another variable.</p>

<p><em>excerpt</em></p>

<pre><code>${parameter:-word}
    If parameter is unset or null, the expansion of word is substituted. 
    Otherwise, the value of parameter is substituted.
</code></pre>

<p><strong>NOTE:</strong> This form also works, <code>${parameter-word}</code>. If you'd like to see a full list of all forms of parameter expansion available within Bash then I highly suggest you take a look at this topic in the Bash Hacker's wiki titled: ""<a href=""http://wiki.bash-hackers.org/syntax/pe"">Parameter expansion</a>"".</p>

<h3>Examples</h3>

<em>variable doesn't exist</em>

<pre><code>$ echo ""$VAR1""

$ VAR1=""${VAR1:-default value}""
$ echo ""$VAR1""
default value
</code></pre>

<em>variable exists</em>

<pre><code>$ VAR1=""has value""
$ echo ""$VAR1""
has value

$ VAR1=""${VAR1:-default value}""
$ echo ""$VAR1""
has value
</code></pre>

<p>The same thing can be done by evaluating other variables, or running commands within the default value portion of the notation.</p>

<pre><code>$ VAR2=""has another value""
$ echo ""$VAR2""
has another value
$ echo ""$VAR1""

$

$ VAR1=""${VAR1:-$VAR2}""
$ echo ""$VAR1""
has another value
</code></pre>

<h3>More Examples</h3>

<p>You can also use a slightly different notation where it's just <code>VARX=${VARX-&lt;def. value&gt;</code>.</p>

<pre><code>$ echo ""${VAR1-0}""
has another value
$ echo ""${VAR2-0}""
has another value
$ echo ""${VAR3-0}""
0
</code></pre>

<p>In the above <code>$VAR1</code> &amp; <code>$VAR2</code> were already defined with the string ""has another value"" but <code>$VAR3</code> was undefined, so the default value was used instead, <code>0</code>.</p>

<h3>Another Example</h3>

<pre><code>$ VARX=""${VAR3-0}""
$ echo ""$VARX""
0
</code></pre>

<h3>Checking and assigning using <code>:=</code> notation</h3>

<p>Lastly I'll mention the handy operator, <code>:=</code>. This will do a check and assign a value if the variable under test is empty or undefined.</p>

<h3>Example</h3>

<p>Notice that <code>$VAR1</code> is now set. The operator <code>:=</code> did the test and the assignment in a single operation.</p>

<pre><code>$ unset VAR1
$ echo ""$VAR1""

$ echo ""${VAR1:=default}""
default
$ echo ""$VAR1""
default
</code></pre>

<p>However if the value is set prior, then it's left alone.</p>

<pre><code>$ VAR1=""some value""
$ echo ""${VAR1:=default}""
some value
$ echo ""$VAR1""
some value
</code></pre>

<h3>Handy Dandy Reference Table</h3>

<p>&nbsp;&nbsp;&nbsp;&nbsp;<a href=""https://i.stack.imgur.com/T2Fp8.png""><img src=""https://i.stack.imgur.com/T2Fp8.png"" alt=""ss of table""></a></p>

<h3>References</h3>

<ul>
<li><a href=""http://wiki.bash-hackers.org/syntax/pe"">Parameter Expansions - Bash Hackers Wiki</a></li>
<li><a href=""http://www.tldp.org/LDP/abs/html/parameter-substitution.html"">10.2. Parameter Substitution</a></li>
<li><a href=""http://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html"">Bash Parameter Expansions</a></li>
</ul>
","122848"
"File permission issues with shared folders under Virtual Box (Ubuntu Guest, Windows Host)","155658","","<p>I am using Ubuntu on Virtual Box and I have a folder which is shared between the host (Windows) and the VM (Ubuntu). When I open any file in the share folder in Ubuntu, I can not change it as its owner is set to root.</p>

<p>How can I change the ownership to myself?</p>

<p>Here is the output of <code>ls -l</code> : </p>

<pre><code>-rwxrwxrwx 1 root root 0 2012-10-05 19:17 BuildNotes.txt
</code></pre>

<p>The output of <code>df</code> is:</p>

<pre><code> m@m-Linux:~/Desktop/vbox_shared$ df
 Filesystem           1K-blocks      Used Available Use% Mounted on
 /dev/sda1             29640780  10209652  17925440  37% /
 none                    509032       260    508772   1% /dev
 none                    513252       168    513084   1% /dev/shm
 none                    513252        88    513164   1% /var/run
 none                    513252         0    513252   0% /var/lock
 none                    513252         0    513252   0% /lib/init/rw
 Ubuntu               214153212  31893804 182259408  15% /media/sf_Ubuntu
 /dev/sr0                 53914     53914         0 100% /media/VBOXADDITIONS_4.2.0_80737
 Ubuntu               214153212  31893804 182259408  15% /home/m/Desktop/vbox_shared
</code></pre>

<p>The options in VM is automount and the readoly is not checked.</p>

<p>Tried to use <code>/media/sf_Ubuntu</code>, but getting permission error:</p>

<pre><code>m@m-Linux:/media$ ls -l 
total 10
drwxrwx--- 1 root vboxsf 4096 2012-10-23 15:35 sf_Ubuntu
drwxrwx--- 2 root vboxsf 4096 2012-10-21 23:41 sf_vbox_shared
dr-xr-xr-x 6 m    m      2048 2012-09-13 07:19 VBOXADDITIONS_4.2.0_80737
m@m-Linux:/media$ cd sf_Ubuntu/
bash: cd: sf_Ubuntu/: Permission denied
m@m-Linux:/media$ cd sf_vbox_shared/
bash: cd: sf_vbox_shared/: Permission denied
</code></pre>

<p>Please note that I am in the group <code>vboxsf</code>:</p>

<pre><code>m@m-Linux:~$ id
uid=1000(m) gid=1000(m) groups=4(adm),20(dialout),24(cdrom),46(plugdev),105(lpadmin),119(admin),122(sambashare),1000(m),1001(vboxsf)
</code></pre>
","<p>The regular way of getting access to the files now, is to allow VirtualBox to automount the shared folder (which will make it show up under <code>/media/sf_directory_name</code>) and then to add your regular Ubuntu user to the <code>vboxsf</code> group (as root <code>#</code>).</p>

<pre><code># usermod -aG vboxsf &lt;youruser&gt;
</code></pre>

<p>By default, without manual action, the mounts look like this,</p>

<pre><code>drwxrwx--- 1 root vboxsf 40960 Oct 23 10:42 sf_&lt;name&gt;
</code></pre>

<p>so the <code>vboxsf</code> group has full access.  By adding your user to that group, you gain full access.  So you wouldn't worry about changing their permissions (which don't make sense on the Windows host), you just give yourself access.</p>

<p>In this specific case, this is the automounted Shared Folder,</p>

<pre><code>Ubuntu               214153212  31893804 182259408  15% /media/sf_Ubuntu
</code></pre>

<p>and it is that directory that should be used to access to the Shared Folder, by putting the local user into the <code>vboxsf</code> group.  If you want a 'better' link under your user's home directory, you could always create a symbolic link.</p>

<pre><code>ln -s /media/sf_Ubuntu /home/m/Desktop/vbox_shared
</code></pre>

<h1>You will need to reboot your VM for these changes to take effect</h1>

<p>If you manually mount the shared folder, then you need to use the relevant options on the <code>mount</code> command to set the folder with the right ownership (i.e. the gid, uid and umask options to <code>mount</code>).  This is because the Host OS doesn't support the same permission system as Linux, so VirtualBox has no way of knowing who should own the files.</p>

<p>However, I <strong>strongly recommend just configuring the shared folder to be auto-mounted</strong> (it's a setting on the Shared Folder configuration in VirtualBox itself).</p>

<p><hr />
For the avoidance of doubt, I do not believe you can change permissions normally anyway, on that filesystem if it's mounted in the regular way,</p>

<pre><code>tony@jabba:/media/sf_name$ ls -l tst.txt
-rwxrwx--- 1 root vboxsf 2283 Apr  4  2012 tst.txt
tony@jabba:/media/sf_name$ sudo chown tony tst.txt
[sudo] password for tony: 
tony@jabba:/media/sf_name$ ls -l tst.txt
-rwxrwx--- 1 root vboxsf 2283 Apr  4  2012 tst.txt
tony@jabba:/media/sf_name$ 
</code></pre>
","52670"
"Bash Sudo Command Not Found","155601","","<p>I have set up a VM using turnkey linux redmine and I'm trying to SSH into the server to install some more items.</p>

<p>It doesn't appear to be recognizing the sudo command. Every time I try to sudo something I get an error saying:</p>

<pre><code> -bash: sudo: command not found
</code></pre>

<p>I read somewhere else to type 'whereis sudo' and the output was:</p>

<pre><code> sudo:
</code></pre>

<p>Not sure what that does, but im hoping it might help in the diagnostic :-)</p>

<p>Thanks for your help,
Adam</p>
","<p>It looks from <a href=""http://www.turnkeylinux.org/redmine"">http://www.turnkeylinux.org/redmine</a> like Redmine, unlike Ubuntu, does not use sudo by default. What username are you using to SSH in? If it's <code>root</code>, then you don't need to use <code>sudo</code>, as everything you do when SSHed in to the Redmine system is done as <code>root</code>. If it's something else, like <code>admin</code>, then you could try using the <code>su</code> command to get a <code>root</code> shell in which to run commands as <code>root</code>.</p>
","23575"
"Where do files go when the rm command is issued?","154974","","<p>Recently I accidentally did <code>rm</code> on a set of files and it got me thinking where exactly these files end up?</p>

<p>That is to say, when working with a GUI, deleted files go to the Trash. What's the equivalent for <code>rm</code> and is there a way of undoing an <code>rm</code> command?</p>
","<p>Nowhere, gone, vanished.  Well, more specifically, the file gets unlinked. The data is still sitting there on disk, but the link to it is removed. It used to be possible to retrieve the data, but nowadays the metadata is cleared and nothings recoverable.  There is no Trash can for <code>rm</code>, nor should there be.  If you need a Trash can, you should use a higher-level interface.  There is a command-line utility in <code>trash-cli</code> on Ubuntu, but most of the time GUI file managers like Nautilus or Dolphin are used to provide a standard Trash can. The Trash can is standard itself. Files trashed in Dolphin will be visible in the Trash from Nautilus. Files are usually moved to somewhere like <code>~/.local/share/Trash/files/</code> when trashed. The <code>rm</code> command on UNIX/Linux is comparable to <code>del</code> on DOS/Windows which also deletes and does <strong>not</strong> move files to the Recycle Bin. Another thing to realize is that moving a file across filesystems like to your USB disk from your hard disk drive is really a copy of the file data followed by unlinking the original file. You wouldn't want your Trash to be filled up with these extra copies.</p>
","10884"
"Mount cifs Network Drive: write permissions and chown","154228","","<p>I have access to a cifs network drive. When I mount it under my OSX machine, I can read and write from and to it. </p>

<p>When I mount the drive in ubuntu, using:</p>

<pre><code>sudo mount -t cifs -o username=${USER},password=${PASSWORD} //server-address/folder /mount/path/on/ubuntu
</code></pre>

<p>I am not able to write to the network drive, but I can read from it.
I have checked the permissions and owner of the mount folder, they look like:</p>

<pre><code>4.0K drwxr-xr-x  4 root root    0 Nov 12  2010 Mounted_folder
</code></pre>

<p>I cannot change the owner, because I get the error:</p>

<pre><code>chown: changing ownership of `/Volumes/Mounted_folder': Not a directory
</code></pre>

<p>When I descend deeper into the network drive, and change the ownership there, I get the error that I have no permission to change the folder´s owner.</p>

<p>What should I do to activate my write permission?</p>
","<p>You are mounting the CIFS share as root (because you used <code>sudo</code>), so you cannot write as normal user. If your Linux Distribution and its kernel are recent enough that you could mount the network share as a normal user (but under a folder that the user own), you will have the proper credentials to write file (e.g. mount the shared folder somewhere under your home directory, like for instance <code>$HOME/netshare/</code>. Obviously, you would need to create the folder before mounting it).</p>

<p>An alternative is to specify the user and group ID that the mounted network share should used, this would allow that particular user and potentially group to write to the share. Add the following <a href=""http://manpages.ubuntu.com/manpages/xenial/man8/mount.cifs.8.html#contenttoc3"" rel=""noreferrer"">options to your mount</a>: <code>uid=&lt;user&gt;,gid=&lt;group&gt;</code> and replace <code>&lt;user&gt;</code> and <code>&lt;group&gt;</code> respectively by your own user and default group.</p>

<pre><code>sudo mount -t cifs -o username=${USER},password=${PASSWORD},uid=&lt;user&gt;,gid=&lt;group&gt; //server-address/folder /mount/path/on/ubuntu
</code></pre>

<p>If the server is sending ownership information, you may need to add the <code>forceuid</code> and <code>forcegid</code> options.</p>

<pre><code>sudo mount -t cifs -o username=${USER},password=${PASSWORD},uid=&lt;user&gt;,gid=&lt;group&gt;,forceuid,forcegid, //server-address/folder /mount/path/on/ubuntu
</code></pre>
","68081"
"How does the sticky bit work?","153566","","<p>SUID </p>

<p>The <em>sticky bit</em> applied to executable programs flagging the system to keep an image of the program in memory after the program finished running.</p>

<p>But I don't know that what it's stored in memory. And how I can see them, in this case.? </p>
","<p>This is probably one of my most irksome things that people mess up all the time. The SUID/GUID bit and the sticky-bit are 2 completely different things.</p>

<p>If you do a <code>man chmod</code> you can read about the SUID and sticky-bits. The <a href=""http://linux.die.net/man/1/chmod"" rel=""noreferrer"">man page is available here</a> as well.</p>

<h2>background</h2>

<p><em>excerpt</em></p>

<blockquote>
  <p>The letters <strong>rwxXst</strong> select file mode bits for the affected users: read
  (r), write (w), execute (or search for directories) (x),  execute/search  only
  if  the  file is a directory or already has execute permission for some user
  (X), <strong>set user or group ID on execution (s)</strong>, restricted deletion flag or 
  <strong>sticky bit (t)</strong>.</p>
</blockquote>

<h2>SUID/GUID</h2>

<p>What the above man page is trying to say is that the position that the x bit takes in the rwxrwxrwx for the user octal (1st group of rwx) and the group octal (2nd group of rwx) can take an additional state where the x becomes an s. When this occurs this file when executed (if it's a program and not just a shell script) will run with the permissions of the owner or the group of the file. </p>

<p>So if the file is owned by root and the SUID bit is turned on, the program will run as root. Even if you execute it as a regular user. The same thing applies to the GUID bit.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>SETUID AND SETGID BITS</p>

<pre><code>  chmod clears the set-group-ID bit of a regular file if the file's group ID 
  does not match the user's effective  group  ID or  one  of  the user's 
  supplementary group IDs, unless the user has appropriate privileges.
  Additional restrictions may cause the set-user-ID and set-group-ID bits
  of MODE or RFILE to be ignored.  This behavior  depends  on  the  policy
  and functionality of the underlying chmod system call.  When in doubt,
  check the underlying system behavior.

  chmod  preserves a directory's set-user-ID and set-group-ID bits unless 
  you explicitly specify otherwise.  You can set or clear the bits with 
  symbolic modes like u+s and g-s, and you can set (but not clear) the bits
  with a numeric mode.
</code></pre>
</blockquote>

<h2>SUID/GUID examples</h2>

<p><strong>no suid/guid</strong> - just the bits <em>rwxr-xr-x</em> are set.</p>

<pre><code>$ ls -lt b.pl
-rwxr-xr-x 1 root root 179 Jan  9 01:01 b.pl
</code></pre>

<p><strong>suid &amp; user's executable bit enabled (lowercase s)</strong> - the bits <em>rwsr-x-r-x</em> are set.</p>

<pre><code>$ chmod u+s b.pl 
$ ls -lt b.pl 
-rwsr-xr-x 1 root root 179 Jan  9 01:01 b.pl
</code></pre>

<p><strong>suid enabled &amp; executable bit disabled (uppercase S)</strong> - the bits <em>rwSr-xr-x</em> are set.</p>

<pre><code>$ chmod u-x b.pl
$ ls -lt b.pl 
-rwSr-xr-x 1 root root 179 Jan  9 01:01 b.pl
</code></pre>

<p><strong>guid &amp; group's executable bit enabled (lowercase s)</strong> - the bits <em>rwxr-sr-x</em> are set.</p>

<pre><code>$ chmod g+s b.pl
$  ls -lt b.pl 
-rwxr-sr-x 1 root root 179 Jan  9 01:01 b.pl
</code></pre>

<p><strong>guid enabled &amp; executable bit disabled (uppercase S)</strong> - the bits <em>rwxr-Sr-x</em> are set.</p>

<pre><code>$ chmod g-x b.pl
$  ls -lt b.pl 
-rwxr-Sr-x 1 root root 179 Jan  9 01:01 b.pl
</code></pre>

<h2>sticky bit</h2>

<p>The sticky bit on the other hand is denoted as <code>t</code>, such as with the <code>/tmp</code> directory:</p>

<pre><code>$ ls -l /|grep tmp
drwxrwxrwt. 168 root root 28672 Jun 14 08:36 tmp
</code></pre>

<p>This bit should have always been called the ""restricted deletion bit"" given that's what it really connotes. When this mode bit is enabled, it makes a directory such that users can only delete files &amp; directories within it that they are the owners of.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>RESTRICTED DELETION FLAG OR STICKY BIT</p>

<pre><code>  The restricted deletion flag or sticky bit is a single bit, whose 
  interpretation depends on the file type.  For  directories,  it  
  prevents  unprivileged users from removing or renaming a file in the
  directory unless they own the file or the directory; this is called the 
  restricted deletion flag for the directory, and is commonly found on 
  world-writable  directories  like /tmp.  For regular files on some 
  older systems, the bit saves the program's text image on the swap 
  device so it will load more quickly when run; this is called the sticky
  bit.
</code></pre>
</blockquote>
","79401"
"How to create SHA512 password hashes on command line","153129","","<p>In Linux I can create a SHA1 password hash using <code>sha1pass mypassword</code>. Is there a similar command line tool which lets me create <code>sha512</code> hashes? Same question for <code>Bcrypt</code> and <code>PBKDF2</code>.</p>
","<p>Yes, you're looking for <code>mkpasswd</code>, which (at least on Debian) is part of the <code>whois</code> package. Don't ask why...</p>

<pre><code>anthony@Zia:~$ mkpasswd -m help
Available methods:
des     standard 56 bit DES-based crypt(3)
md5     MD5
sha-256 SHA-256
sha-512 SHA-512
</code></pre>

<p>Unfortunately, my version at least doesn't do bcrypt. If your C library does, it should (and the manpage gives a -R option to set the strength). -R also works on sha-512, but I'm not sure if its PBKDF-2 or not.</p>

<p>If you need to generate bcrypt passwords, you can do it fairly simply with the <code>Crypt::Eksblowfish::Bcrypt</code> Perl module.</p>
","52112"
"mount error 13 = Permission denied","153024","","<p>One of my servers is set up to automatically mount a Windows directory using fstab.  However, after my last reboot it stopped working.  The line in fstab is:</p>

<pre><code>//myserver/myfolder /mnt/backup cifs credentials=home/myfolder/.Smbcredentials
</code></pre>

<p>The <code>.Smbcredentials</code> file is:</p>

<pre><code>username=myaccount
password=mypassword
domain=mydomain
</code></pre>

<p>I do a <code>mount -a</code> and I receive <code>mount error 13 = Permission denied</code>.  If I do this enough it will lock out my Windows account, so I know it's trying.  I've checked that my password is correct.</p>

<p>What am i doing wrong?</p>
","<p>Thanks, but some more googling turned up the solution.  It was using the wrong security type by default; this command worked:</p>

<pre><code>$ sudo mount -t cifs //172.16.1.5/myshare/ /mnt/myshare \
    -osec=ntlmv2,domain=MYDOMAIN,username=myusername,password=mypassword
</code></pre>
","124696"
"Is there a way to redirect nohup output to a log file other than nohup.out?","152602","","<p>I frequently use the program <code>nohup</code> so that my processes are immune to hangups.  So if I want to make the program <code>program</code> immune to hangups, I use the command</p>

<pre><code>nohup program &amp;
</code></pre>

<p>where <code>&amp;</code> puts the process in the background.</p>

<p>When starting, <code>nohup</code> gives me the message:</p>

<blockquote>
  <p>nohup: appending output to `nohup.out'</p>
</blockquote>

<p>Is there any way to send the output to a file other than <code>nohup.out</code>?  Often I want to run many processes in the same directory using <code>nohup</code>, but if I do this, all the output gets lumped together in a single <code>nohup.out</code> file.</p>

<p>The manual page (for example, <a href=""http://cims.nyu.edu/cgi-comment/man.cgi?section=1&amp;topic=nohup"">here</a>) does not seem to have an option for specifying the log file.  Can you please confirm this?  Also, do you have any thoughts of how I can work around this problem?</p>
","<p>GNU coreutils <code>nohup</code> man page indicates that you can use normal redirection:</p>

<blockquote>
  <p>If standard input is a terminal, redirect it from /dev/null. If standard output is a terminal, append output to 'nohup.out' if possible, '$HOME/nohup.out' otherwise. If standard error is a terminal, redirect it to standard output. To save output to FILE, use 'nohup COMMAND > FILE'. </p>
</blockquote>

<p>Edit: I didn't read your link at first; you may have a different version of <code>nohup</code>, although this section suggests that you can still use normal redirection:</p>

<pre><code> nohup.out          The output file of the nohup execution if
                    standard  output is a terminal and if the
                    current directory is writable.
</code></pre>

<p>You can redirect standard output and standard error to different files:</p>

<pre><code>nohup myprogram &gt; myprogram.out 2&gt; myprogram.err
</code></pre>

<p>or to the same file:</p>

<pre><code>nohup myprogram &gt; myprogram.out 2&gt;&amp;1
</code></pre>
","45915"
"Can I redirect output to a log file and background a process at the same time?","152531","","<p>Can I redirect output to a log file and background a process at the same time?</p>

<p>In other words, can I do something like this?</p>

<pre><code>nohup java -jar myProgram.jar 2&gt;&amp;1 &gt; output.log &amp;
</code></pre>

<p>Or, is that not a legal command?  Or, do I need to manually move it to the background, like so:</p>

<pre><code>java -jar myProgram.jar 2&gt;$1 &gt; output.log
jobs
[CTRL-Z]
bg 1
</code></pre>
","<p>One problem with your first command is that you redirect stderr to where stdout is (if you changed the $ to a &amp; as suggested in the comment) and then, you redirected stdout to some log file, but that does not pull along the redirected stderr. You must do it in the other order, first send stdout to where you want it to go, and then send stderr to the address stdout is at</p>

<pre><code>some_cmd &gt; some_file 2&gt;&amp;1 &amp;
</code></pre>

<p>and then you could throw the &amp; on to send it to the background. Jobs can be accessed with the <code>jobs</code> command. <code>jobs</code> will show you the running jobs, and number them. You could then talk about the jobs using a % followed by the number like <code>kill %1</code> or so.  </p>

<p>Also, without the &amp; on the end you can suspend the command with <kbd>Ctrl</kbd><kbd>z</kbd>, use the <code>bg</code> command to put it in the background and <code>fg</code> to bring it back to the foreground.  In combination with the <code>jobs</code> command, this is powerful.</p>

<p>to clarify the above part about the order you write the commands. Suppose stderr is address 1002, stdout is address 1001, and the file is 1008. The command reads left to right, so the first thing it sees in yours is <code>2&gt;&amp;1</code> which moves stderr to the address 1001, it then sees <code>&gt; file</code> which moves stdout to 1008, but keeps stderr at 1001. It does not pull everything pointing at 1001 and move it to 1008, but simply references stdout and moves it to the file.<br>
The other way around, it moves stdout to 1008, and then moves stderr to the point that stdout is pointing to, 1008 as well. This way both can point to the single file.</p>
","106641"
"Script to monitor folder for new files?","152279","","<p>How do I detect new files in a folder with a bash script? I would like to process the files as soon as they are created in the folder. Is this possible to do so or do I have to schedule a script with cron that that check for new files each minute or so? </p>
","<p>You should consider using <code>inotifywait</code>, as an example:</p>

<pre><code>inotifywait -m /path -e create -e moved_to |
    while read path action file; do
        echo ""The file '$file' appeared in directory '$path' via '$action'""
        # do something with the file
    done
</code></pre>

<p>In Ubuntu <code>inotifywait</code> is provided by the <code>inotify-tools</code> package. As of version 3.13 (current in Ubuntu 12.04) <code>inotifywait</code> will include the filename without the -f option. Older versions may need to be coerced. What is important to note is that the <code>-e</code> option to <code>inotifywait</code> is the best way to do event filtering. Also, your <code>read</code> command can assign the positional output into multiple variables that you can choose to use or ignore. There is no need to use grep/sed/awk to preprocess the output.</p>
","24955"
"How does reverse SSH tunneling work?","151611","","<p>As I understand this, firewalls (assuming default settings) deny all incoming traffic that has no prior corresponding outgoing traffic.</p>

<p>Based on <a href=""http://www.vdomck.org/2005/11/reversing-ssh-connection.html"">Reversing an ssh connection</a> and <a href=""http://www.revsys.com/writings/quicktips/ssh-tunnel.html"">SSH Tunneling Made Easy</a>, reverse SSH tunneling can be used to get around pesky firewall restrictions.</p>

<p>I would like to execute shell commands on a remote machine. The remote machine has its own firewall and is behind an additional firewall (router). It has an IP address like 192.168.1.126 (or something similar). I am not behind a firewall and I know the remote machine's IP address as seen from the Internet (not the 192.168.1.126 address). Additionally, I can ask someone to execute <code>ssh (something)</code> as root on the remote machine first.</p>

<p>Could anyone explain me, step by step, how reverse SSH tunneling works to get around the firewalls (local and remote machines' firewalls and the additional firewall between them)?</p>

<p>What is the role of the switches (<code>-R</code>, <code>-f</code>, <code>-L</code>, <code>-N</code>)?</p>
","<p>I love explaining this kind of thing through visualization.  :-)</p>

<p>Think of your SSH connections as tubes.  Big tubes.  Normally, you'll reach through these tubes to run a shell on a remote computer.  The shell runs in a virtual terminal (tty).  But you know this part already.</p>

<p>Think of your tunnel as a tube within a tube.  You still have the big SSH connection, but the -L or -R option lets you set up a smaller tube inside it.</p>

<p>Every tube has a beginning and an end.  The big tube, your SSH connection, started with your SSH client and ends up at the SSH server you connected to.  All the smaller tubes have the same endpoints, except that the role of ""start"" or ""end"" is determined by whether you used <code>-L</code> or <code>-R</code> (respectively) to create them.</p>

<p>(You haven't said, but I'm going to assume that the ""remote"" machine you've mentioned, the one behind the firewall, can access the Internet using Network Address Translation (NAT).  This is kind of important, so please correct this assumption if it is false.)</p>

<p>When you create a tunnel, you specify an address and port on which it will answer, and an address and port to which it will be delivered.  The <code>-L</code> option tells the tunnel to answer on the local side of the tunnel (the host running your client).  The <code>-R</code> option tells the tunnel to answer on the remote side (the SSH server).</p>

<p><img src=""https://i.stack.imgur.com/HbSEM.png"" alt=""ssh tunnel directions""></p>

<p>So...  To be able to SSH from the Internet into a machine behind a firewall, you need the machine in question to open an SSH connection to the outside world and include a <code>-R</code> tunnel whose ""entry"" point is the ""remote"" side of his connection.</p>

<p>Of the two models shown above, you want the one on the right.</p>

<p>From the firewalled host:</p>

<pre><code>ssh -f -N -T -R22222:localhost:22 yourpublichost.example.com
</code></pre>

<p>This tells your client to establish a tunnel with a <code>-R</code>emote entry point.  Anything that attaches to port 22222 on the far end of the tunnel will actually reach ""localhost port 22"", where ""localhost"" is from the perspective of the exit point of the tunnel (i.e. your ssh client).</p>

<p>The other options are:</p>

<ul>
<li><code>-f</code> tells ssh to background itself after it authenticates, so you don't have to sit around running something on the remote server for the tunnel to remain alive.</li>
<li><code>-N</code> says that you want an SSH connection, but you don't actually want to run any remote commands. If all you're creating is a tunnel, then including this option saves resources.</li>
<li><code>-T</code> disables pseudo-tty allocation, which is appropriate because you're not trying to create an interactive shell.</li>
</ul>

<p>There will be a password challenge unless you have set up DSA or RSA keys for a passwordless login.</p>

<p>Note that it is STRONGLY recommended that you use a throw-away account (not your own login) that you set up for just this tunnel/customer/server.</p>

<p>Now, from your shell on <strong>yourpublichost</strong>, establish a connection to the firewalled host through the tunnel:</p>

<pre><code>ssh -p 22222 username@localhost
</code></pre>

<p>You'll get a host key challenge, as you've probably never hit this host before.  Then you'll get a password challenge for the <code>username</code> account (unless you've set up keys for passwordless login).</p>

<p>If you're going to be accessing this host on a regular basis, you can also simplify access by adding a few lines to your <code>~/.ssh/config</code> file:</p>

<pre><code>host remotehostname
    User remoteusername
    Hostname localhost
    Port 22222
</code></pre>

<p>Adjust <code>remotehostname</code> and <code>remoteusername</code> to suit.  The <code>remoteusername</code> field must match your username on the remote server, but <code>remotehostname</code> can be any hostname that suits you, it doesn't have to match anything resolvable.</p>
","46271"
"Nslookup: command not found error on RHEL/CentOS 7","151595","","<p>During linux installation I selected ""minimal"" option:</p>

<p><img src=""https://i.stack.imgur.com/qzIsG.png"" alt=""enter image description here""></p>

<p>When I went to run the nslookup command to look up an IP address I got the error message <code>nslookup: command not found</code> as shown in the example below.</p>

<pre><code>$ nslookup www.google.com
bash: nslookup: command not found
</code></pre>
","<p>The minimal install likely did not come with the <code>bind-utils</code> package, which I believe contains <code>nslookup</code>.</p>

<p>You can install <code>bind-utils</code> with:</p>

<pre><code>sudo yum install bind-utils
</code></pre>

<p>In general, you can search for what package provides a command using the <code>yum provides</code> command:</p>

<pre><code>sudo yum provides '*bin/nslookup'
</code></pre>
","164212"
"How to comment multiple lines at once?","151362","","<p>How can I select a bunch of text and comment it all out?</p>

<p>Currently I go to the first line, go to insert mode then type <code>#</code> <kbd>left-arrow</kbd><kbd>down-arrow</kbd> and then I repeat that sequence, perhaps saving a few keystrokes by using the <code>.</code> repeat feature to do each line.</p>

<p>Is there anyway I could (for instance) select either multiple lines in visual mode or by using a range of lines and an ex ('colon') command and for that range comment out all the lines with a <code>#</code> to make them a ""block comment"".</p>

<p>The ability to quickly 'de-comment' (remove the <code>#</code>'s) for a block comment would also be nice.</p>
","<h2>Ranges:</h2>

<p>You can do it with the following command:</p>

<pre><code>:66,70s/^/#
</code></pre>

<p>for comment, and for uncomment:</p>

<pre><code>:66,70s/^#/
</code></pre>

<p>obviously, here we're commenting lines from 66 to 70 (inclusive).</p>
","120618"
"How can I run a cron command with existing environmental variables?","151113","","<p>How can I run a cron command with existing environmental variables?</p>

<p>If I am at a shell prompt I can type <code>echo $ORACLE_HOME</code> and get a path. This is one of my environmental variables that gets set in my <code>~/.profile</code>. However, it seems that <code>~/.profile</code> does not get loaded fron cron scripts and so my scripts fail because the <code>$ORACLE_HOME</code> variable is not set.</p>

<p>In <a href=""https://stackoverflow.com/questions/2229825/where-can-i-set-environment-variables-that-crontab-will-use"">this question</a> the author mentions creating a <code>~/.cronfile</code> profile which sets up variables for cron, and then he does a workaround to load all his cron commands into scripts he keeps in his <code>~/Cron</code> directory. A file like <code>~/.cronfile</code> sounds like a good idea, but the rest of the answer seems a little cumbersome and I was hoping someone could tell me an easier way to get the same result.</p>

<p>I suppose at the start of my scripts I could add something like <code>source ~/.profile</code> but that seems like it could be redundant. </p>

<p>So how can I get make my cron scripts load the variables from my interactive-shell profile?</p>
","<p>In the crontab, before you command, add <code>. $HOME/.profile</code>.  For example:</p>

<pre><code>0 5 * * * . $HOME/.profile; /path/to/command/to/run
</code></pre>

<p><code>Cron</code> knows nothing about your shell; it is started by the system, so it has a minimal environment.  If you want anything, you need to have that brought in yourself.</p>
","27291"
"How can I use variables when doing a sed?","150598","","<p>I want to do:</p>

<pre><code>cat update_via_sed.sh | sed 's/old_name/new_name/' &gt; new_update_via_sed.sh
</code></pre>

<p>in my program.</p>

<p>But I want to use variables, e.g.</p>

<pre><code>old_run='old_name_952'
new_run='old_name_953'
</code></pre>

<p>I have tried using them but the substitution doesn't happen (no error).
I have tried:</p>

<pre><code>cat update_via_sed.sh | sed 's/old_run/new_run/'
cat update_via_sed.sh | sed 's/$old_run/$new_run/'
cat update_via_sed.sh | sed 's/${old_run}/${new_run}/'
</code></pre>
","<p>'in-place' sed (usng the -i flag) was the answer.  Thanks to peterph.</p>

<pre><code>sed -i ""s@$old@$new@"" file
</code></pre>
","69154"
"Convert Amazon .pem key to Putty .ppk key Linux","149962","","<p>I have generated and downloaded a private <code>.pem</code> key from AWS. However, to use Putty in order to connect to the virtual machine, I must have that key in <code>.ppk</code> format. The process of conversion is detailed in roughly 20 lines here:</p>

<ul>
<li><a href=""http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#prepare-for-putty"">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html#prepare-for-putty</a></li>
</ul>

<p>I am using Linux Mint (an Ubuntu distro) and I know I can use puttygen in the terminal. However, I have no idea how to use this tool, nor how to configure the needed parameters. When I type puttygen --help I get </p>

<pre><code>PuTTYgen unidentified build, Aug  7 2013 12:24:58
Usage: puttygen ( keyfile | -t type [ -b bits ] )
                [ -C comment ] [ -P ] [ -q ]
                [ -o output-keyfile ] [ -O type | -l | -L | -p ]
  -t    specify key type when generating (rsa, dsa, rsa1)
  -b    specify number of bits when generating key
  -C    change or specify key comment
  -P    change key passphrase
  -q    quiet: do not display progress bar
  -O    specify output type:
           private             output PuTTY private key format
           private-openssh     export OpenSSH private key
           private-sshcom      export ssh.com private key
           public              standard / ssh.com public key
           public-openssh      OpenSSH public key
           fingerprint         output the key fingerprint
  -o    specify output file
  -l    equivalent to `-O fingerprint'
  -L    equivalent to `-O public-openssh'
  -p    equivalent to `-O public'
</code></pre>

<p>But I have no idea whatsoever on how to do what the website tells me to do and all my tentatives failed so far.</p>

<p>How do I do what the website tells me to do, using puttygen on the terminal?</p>
","<h3>Using the GUI</h3>

<p>See this SO Q&amp;A on how to do exactly what you want, titled: <a href=""https://stackoverflow.com/questions/3190667/convert-pem-to-ppk-file-format"">Convert PEM to PPK file format</a>.</p>

<p><em>excerpt</em></p>

<ol>
<li>Download your .pem from AWS</li>
<li>Open PuTTYgen, select Type of key to generate as: SSH-2 RSA</li>
<li>Click ""Load"" on the right side about 3/4 down</li>
<li>Set the file type to <code>*.*</code></li>
<li>Browse to, and Open your .pem file</li>
<li>PuTTY will auto-detect everything it needs, and you just need to click ""Save private key"" and you can save your ppk key for use with PuTTY</li>
</ol>

<h3>Using the command line</h3>

<p>If on the other hand you'd like to convert a <code>.pem</code> to <code>.ppk</code> file via the command line tool <code>puttygen</code>, I did come across this solution on SO in this Q&amp;A titled: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/2224066/how-to-convert-ssh-keypairs-generated-using-puttygenwindows-into-key-pairs-use"">How to convert SSH keypairs generated using PuttyGen(Windows) into key-pairs used by ssh-agent and KeyChain(Linux)</a>.</li>
</ul>

<p><em>excerpt</em></p>

<pre><code>$ puttygen keyfile.pem -O private -o avdev.ppk
</code></pre>

<p>For the public key:</p>

<pre><code>$ puttygen keyfile.pem -L
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""http://support.cdh.ucla.edu/help/132-file-transfer-protocol-ftp/583-converting-your-private-key-"" rel=""noreferrer"">Converting Your Private Key (Putty)</a></li>
</ul>
","116318"
"How do I delete the first n lines of an ascii file using shell commands?","149446","","<p>I have multiple files that contain ascii text information in the first 5-10 lines, followed by well-tabulated matrix information.  In a shell script, I want to remove these first few lines of text so that I can use the pure matrix information in another program.  How can I use bash shell commands to do this?  </p>

<p>If it's any help, I'm using RedHat and an Ubuntu linux systems.</p>
","<p>As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below.</p>

<pre><code>$ cat t.txt
12
34
56
78
90
</code></pre>

<h2>sed</h2>

<pre><code>$ sed -e '1,3d' &lt; t.txt
78
90
</code></pre>

<p>You can also use sed in-place without a temp file: <code>sed -i -e 1,3d yourfile</code>. This won't echo anything, it will just modify the file in-place. If you don't need to pipe the result to another command, this is easier.</p>

<h2>tail</h2>

<pre><code>$ tail -n +4 t.txt
78
90
</code></pre>

<h2>awk</h2>

<pre><code>$ awk 'NR &gt; 3 { print }' &lt; t.txt
78
90
</code></pre>
","37791"
"View stdout/stderr of systemd service","149386","","<p>I have created a simple systemd service file for a custom application. The application works well when I run it manually, but my CPU gets maxed out when I run it with systemd.</p>

<p>I'm trying do track down where my problem is, but I don't know where to find the output (or how to configure systemd to put the output somewhere).</p>

<p>Here is my service file:</p>

<pre><code>[Unit]
Description=Syncs files with a server when they change
Wants=network.target
After=network.target

[Service]
ExecStart=/usr/local/bin/filesync-client --port 2500
WorkingDirectory=/usr/local/lib/node_modules/filesync-client
Restart=always

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Throughout the application, I output to stdout and stderr.</p>

<p>How can I read the output of my daemon?</p>

<p>Edit:</p>

<p>I found <code>man systemd.exec</code>, which mentioned the <code>StandardOutput=</code> option, but I'm not sure how to use it.  From the man page:</p>

<blockquote>
  <p>StandardOutput=</p>
  
  <p>Controls where file descriptor 1 (STDOUT) of the executed processes is connected to. Takes one of <strong>inherit, null, tty, syslog, kmsg, kmsg+console, syslog+console or socket</strong>.</p>
  
  <p>If set to inherit the file descriptor of standard input is duplicated for standard output. If set to null standard output will be connected to /dev/null, i.e. everything written to it will be lost. If set to tty standard output will be connected to a tty (as configured via TTYPath=, see below). If the TTY is used for output only the executed process will not become the controlling process of the terminal, and will not fail or wait for other processes to release the terminal.  syslog connects standard output to the syslog(3) system logger.  kmsg connects it with the kernel log buffer which is accessible via dmesg(1).  syslog+console and kmsg+console work similarly but copy the output to the system console as well.  socket connects standard output to a socket from socket activation, semantics are similar to the respective option of StandardInput=. This setting defaults to inherit.</p>
</blockquote>

<p>Does this mean that these are my only options?  I would like, for example, to put output in <code>/dev/shm</code> or something. I suppose I could use a unix domain socket and write a simple listener, but this seems a little unnecessary.</p>

<p>I just need this for debugging, and I'll probably end up removing most of the logs and change the output to syslog.</p>
","<h3>Update</h3>

<p>As mikemaccana notes, the <a href=""http://0pointer.de/blog/projects/journalctl.html"">systemd journal</a> is now the standard logging device for most distros. 
To view the <code>stdout</code> and <code>stderr</code> of a systemd unit use the <a href=""https://www.freedesktop.org/software/systemd/man/journalctl.html""><code>journalctl</code> command.</a></p>

<pre><code>sudo journalctl -u [unit]
</code></pre>

<p><strong>Original Answer</strong></p>

<p>By default <code>stdout</code> and <code>stderr</code> of a systemd unit are sent to syslog. </p>

<p>If you're using the full systemd, this will be accesible via <code>journalctl</code>.
On Fedora, it should be <code>/var/log/messages</code> but syslog will put it where your rules say. </p>

<p>Due to the date of the post, and assuming most people that are exposed to systemd are via fedora, you were probably hit by the bug described here:
<a href=""https://bugzilla.redhat.com/show_bug.cgi?id=754938"">https://bugzilla.redhat.com/show_bug.cgi?id=754938</a>
It has a good explanation of how it all works too =) (This was a bug in selinux-policy that caused error messages to not be logged, and was fixed in <code>selinux-policy-3.10.0-58.fc16</code>)</p>
","57243"
"apache2 Invalid command 'SSLEngine'","149382","","<p>When I restart httpd, I get the following error.  What am I missing?</p>

<pre><code>[root@localhost ~]# service httpd restart
Stopping httpd:                                            [  OK  ]
Starting httpd: Syntax error on line 22 of /etc/httpd/conf.d/sites.conf:
Invalid command 'SSLEngine', perhaps misspelled or defined by a module not included in the server configuration
</code></pre>

<p>I have installed mod_ssl using <code>yum install mod_ssl openssh</code></p>

<pre><code>Package 1:mod_ssl-2.2.15-15.el6.centos.x86_64 already installed and latest version
Package openssh-5.3p1-70.el6_2.2.x86_64 already installed and latest version
</code></pre>

<p>My sites.conf looks like this</p>

<pre><code>&lt;VirtualHost *:80&gt;
#    ServerName shop.itmanx.com
    ServerAdmin webmaster@itmanx.com

    DocumentRoot /var/www/html/magento
    &lt;Directory /var/www/html&gt;
        Options -Indexes
        AllowOverride All
    &lt;/Directory&gt;

    ErrorLog logs/shop-error.log
    CustomLog logs/shop-access.log
&lt;/VirtualHost&gt;

&lt;VirtualHost *:443&gt;
    ServerName secure.itmanx.com
    ServerAdmin webmaster@itmanx.com

    SSLEngine on
    SSLCertificateFile /etc/httpd/ssl/secure.itmanx.com/server.crt
    SSLCertificateKeyFile /etc/httpd/ssl/secure.itmanx.com/server.key
    SSLCertificateChainFile /etc/httpd/ssl/secure.itmanx.com/chain.crt

    DocumentRoot /var/www/html/magento
    &lt;Directory /var/www/html&gt;
        Options -Indexes
        AllowOverride All
    &lt;/Directory&gt;

    ErrorLog logs/shop-ssl-error.log
    CustomLog logs/shop-ssl-access.log    
&lt;/VirtualHost&gt;
</code></pre>
","<p>Probably you do not load the ssl module. You should have a <em>LoadModule</em> directive somewhere in your apache configuration files.</p>

<p>Something like: </p>

<pre><code>LoadModule ssl_module /usr/lib64/apache2-prefork/mod_ssl.so
</code></pre>

<p>Usually apache configuration template has (on any distribution) a file called (something like) <code>loadmodule.conf</code> in which you should find a <code>LoadModule</code> directive for each module you load into apache at server start.</p>
","31386"
"How to avoid being asked passphrase each time I push to Bitbucket","149147","","<p>I set up my ssh stuff with the help of <a href=""http://confluence.atlassian.com/display/BITBUCKET/Using+SSH+to+Access+your+Bitbucket+Repository"">this guide</a>, and it used to work well (I could run <code>hg push</code> without being asked for a passphrase). What could have happened between then and now, considering that I'm still using the same home directory.</p>

<pre><code>$ cat .hg/hgrc 
[paths]
default = ssh://hg@bitbucket.org/tshepang/bloog

$ hg push
Enter passphrase for key '/home/wena/.ssh/id_rsa': 
pushing to ssh://hg@bitbucket.org/tshepang/bloog
searching for changes
...
</code></pre>
","<p>You need to use an ssh agent.  Short answer: try </p>

<pre><code>$ ssh-add
</code></pre>

<p>before pushing.  Supply your passphrase when asked.</p>

<p>If you aren't already running an ssh agent you will get the following message:</p>

<pre><code>Could not open a connection to your authentication agent.
</code></pre>

<p>In that situation, you can start one and set your environment up thusly</p>

<pre><code>eval $(ssh-agent)
</code></pre>

<p>Then repeat the <code>ssh-add</code> command.</p>

<p>It's worth taking a look at the <a href=""http://man.openbsd.org/OpenBSD-current/man1/ssh-agent.1"">ssh agent manpage</a>.</p>
","12201"
"How can I find out how many lines a text file contains without viewing it?","148442","","<p>How can I find how many lines a text file contains without opening the file in an editor or a viewer application? Is there a handy Unix console command to see the number?</p>
","<p>Indeed there is.  It is called <code>wc</code>, originally for word count, I believe, but it can do lines, words, characters, bytes (and with some implementations, the length in bytes of the longest line or the display width of the widest one).  The <code>-l</code> option tells it to count lines (in effect, it counts the newline characters, so only properly delimited lines):</p>

<pre><code>wc -l mytextfile
</code></pre>

<p>Or to only output the number of lines:</p>

<pre><code>wc -l &lt; mytextfile
</code></pre>

<p>(beware that some implementations insert blanks before that number).</p>
","29422"
"How do services in Debian work, and how can I manage them?","147755","","<p>In Windows I have the services manager, where I see all system services, that can be started through Windows itself, I set up the user it uses, the rights management is in there, and I can pass variables and some other information to the services, I can name them, and I can create duplicates services of one program and so on. So I have a main management tool in Windows.</p>

<p>How can I do the same in Linux? How can I trigger to run ""svnserve"" at startup, or how can I configure services to be running in a special context. How can I view all ""programmed"" services?</p>
","<p>There are currently 3 main init systems used by linux. A few years ago, there was just one, SysVinit. But SysVinit was seriously lacking in capabilities such as service dependency graphing, so it's been deprecated in most distros by now. Currently most distros are switching to <a href=""http://www.freedesktop.org/wiki/Software/systemd/"">systemd</a>. Though there is also <a href=""http://upstart.ubuntu.com/"">upstart</a>.</p>

<p>But here's the answer to your question for each of the 3 init systems:</p>

<p>&nbsp;</p>

<h2>SysVinit</h2>

<p>SysVinit currently used by Debian and RedHat. Though the next version of RedHat (7) will be using systemd.</p>

<p>The univeral way of enabling SysVinit services on boot is to symlink them in <code>/etc/rc3.d</code> (or <code>/etc/rc2.d</code>). All services can be found in <code>/etc/init.d</code>. Note however that distros will often have their own tool for managing these files, and that tool should be used instead. (Fedora/RedHat has <code>service</code> and <code>chkconfig</code>, ubuntu has <code>update-rc.d</code>)</p>

<h3>List services:</h3>

<pre><code>ls /etc/init.d/
</code></pre>

<h3>Start service:</h3>

<pre><code>/etc/init.d/{SERVICENAME} start
</code></pre>

<h3>Stop service:</h3>

<pre><code>/etc/init.d/{SERVICENAME} stop
</code></pre>

<h3>Enable service:</h3>

<pre><code>cd /etc/rc3.d
ln -s ../init.d/{SERVICENAME} S95{SERVICENAME}
</code></pre>

<p><em>(the <code>S95</code> is used to specify order. S01 will start before S02, etc)</em></p>

<h3>Disable service:</h3>

<pre><code>rm /etc/rc3.d/*{SERVICENAME}
</code></pre>

<p>&nbsp;</p>

<h2>Systemd</h2>

<p>The most notable distribution using systemd is Fedora. Though it is used by many others. Additionally, with Debian having chosen to go with systemd over upstart, it will become the defacto upstart system for most distributions (ubuntu has already announced they will be dropping upstart for systemd).</p>

<h3>List services:</h3>

<pre><code>systemctl list-unit-files
</code></pre>

<h3>Start service:</h3>

<pre><code>systemctl start {SERVICENAME}
</code></pre>

<h3>Stop service:</h3>

<pre><code>systemctl stop {SERVICENAME}
</code></pre>

<h3>Enable service:</h3>

<pre><code>systemctl enable {SERVICENAME}
</code></pre>

<h3>Disable service:</h3>

<pre><code>systemctl disable {SERVICENAME}
</code></pre>

<p>&nbsp;</p>

<h2>Upstart</h2>

<p>Upstart was developed by the Ubuntu folks. But after debian <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=727708"">decided to go with systemd</a>, Ubuntu <a href=""http://www.markshuttleworth.com/archives/1316"">announced they would drop upstart</a>.</p>

<p>Upstart was also briefly used by RedHat, as it is present in RHEL-6, but it is not commonly used.</p>

<h3>List services:</h3>

<pre><code>initctl list
</code></pre>

<h3>Start service:</h3>

<pre><code>initctl start {SERVICENAME}
</code></pre>

<h3>Stop service:</h3>

<pre><code>initctl stop {SERVICENAME}
</code></pre>

<h3>Enable service:</h3>

<p>2 ways unfortunately:</p>

<ol>
<li><p>There will be a file <code>/etc/default/{SERVICENAME}</code> which contains a line <code>ENABLED=...</code>. Change this line to <code>ENABLED=1</code>.</p></li>
<li><p>There will be a file <code>/etc/init/{SERVICENAME}.override</code>. Make sure it contains <code>start</code> (or is absent entirely), not <code>manual</code>.</p></li>
</ol>

<h3>Disable service:</h3>

<pre><code>echo manual &gt; /etc/init/{SERVICENAME}.override
</code></pre>

<hr>

<p><em>Note: There is also the 'OpenRC' init system which is used by Gentoo. Currently Gentoo is the only distro which uses it, and it is not being considered for use, nor supported by any other distro. So I am not covering it's usage (though if opinion is that I do, I can add it).</em></p>
","106674"
"Find and remove large files that are open but have been deleted","147597","","<p>How does one find large files that have been deleted but are still open in an application? How can one remove such a file, even though a process has it open?</p>

<p>The situation is that we are running a process that is filling up a log file at a terrific rate. I know the reason, and I can fix it. Until then, I would like to rm or empty the log file without shutting down the process. </p>

<p>Simply doing <code>rm output.log</code> removes only references to the file, but it continues to occupy space on disk until the process is terminated. Worse: after <code>rm</code>ing I now have no way to find where the file is or how big it is! Is there any way to find the file, and possibly empty it, even though it is still open in another process?</p>

<p>I specifically refer to Linux-based operating systems such as Debian or RHEL.</p>
","<p>If you can't kill your application, you can truncate instead of deleting the log file to reclaim the space. If the file was not open in append mode (with <code>O_APPEND</code>), then the file will appear as big as before the next time the application writes to it (though with the leading part sparse and looking as if it contained NUL bytes), but the space will have been reclaimed (that does not apply to HFS+ file systems on Apple OS/X that don't support sparse files though).</p>

<p>To truncate it:</p>

<pre><code>: &gt; /path/to/the/file.log
</code></pre>

<p>If it was already deleted, on Linux, you can still truncate it by doing:</p>

<pre><code>: &gt; ""/proc/$pid/fd/$fd""
</code></pre>

<p>Where <code>$pid</code> is the process id of the process that has the file opened, and <code>$fd</code> one file descriptor it has it opened under (which you can check with <code>lsof -p ""$pid""</code>.</p>

<p>If you don't know the pid, and are looking for deleted files, you can do:</p>

<pre><code>lsof -nP | grep '(deleted)'
</code></pre>

<p><code>lsof -nP +L1</code>, <a href=""https://unix.stackexchange.com/a/141639/22565"">as mentioned by @user75021</a> is an even better (more reliable and more portable) option (list files that have fewer than 1 link).</p>

<p>Or (on Linux):</p>

<pre><code>find /proc/*/fd -ls | grep  '(deleted)'
</code></pre>

<p>Or to find the large ones with <code>zsh</code>:</p>

<pre><code>ls -ld /proc/*/fd/*(-.LM+1l0)
</code></pre>

<p>An alternative, if the application is dynamically linked is to attach a debugger to it and make it call <code>close(fd)</code> followed by a new <code>open(""the-file"", ....)</code>.</p>
","68532"
"How to send data to a serial port and see any answer?","147436","","<p>On Linux, I want to send a command string (i.e. some data) to a serial port (containing control characters), and listen to the response (which also usually might contain control characters). </p>

<p>How can I do this as simplest as possible on Linux? An example is appreciated!</p>
","<p>All devices on Unix are mapped to a device file, the serial ports would be  <code>/dev/ttyS0</code> <code>/dev/ttyS1</code> ... .</p>

<p>First have a look at the permissions on that file, lets assume you are using <code>/dev/ttyS1</code>.</p>

<p><code>ls -l /dev/ttyS1</code></p>

<p>You will want read.write access, if this is a shared system then you should consider the security consequences of opening it up for everyone.</p>

<pre><code>chmod o+rw /dev/ttyS1
</code></pre>

<p>A very simple crude method to write to the file, would use the simple <code>echo</code> command.</p>

<pre><code>echo -ne '\033[2J' &gt; /dev/ttyS1
</code></pre>

<p>and to read</p>

<pre><code>cat -v &lt; /dev/ttyS1
</code></pre>

<p>You can have cat running in one terminal, and echo in a 2nd.</p>

<p>If everything is gibberish, then baud rate, bit settings might need setting before you start sending. <code>stty</code> will do that. !! NOTE stty will use stdin as default file descriptor to affect. </p>

<p>Equivilent commands.</p>

<pre><code>stty -speed 19200 &lt; /dev/ttyS1
stty -speed 19200 -f /dev/ttyS1
</code></pre>

<p>This might be enough for you to script something and log ? Not sure what you are trying to achieve.</p>

<p>For a more interactive, remembers your default settings approach would be to use
<code>minicom</code> it is just a program which does everything I've mentioned so far. (similar to hyperterminal in Windows, you might be familiar).</p>

<p>An intermediate solution, would use a terminal program like <code>screen</code> which will work on a serial device.</p>

<pre><code>screen /dev/ttyS1
</code></pre>

<p><code>man screen</code> <code>man minicom</code> <code>man stty</code> for more information</p>
","117064"
"How can I verify SSL certificates on the command line?","147398","","<p>I'm trying to validate/verify that the rsa key, ca-bundle, and certificate stored here are ok. They are not being served by a webserver. How can I verify them?</p>
","<p>Assuming your certificates are in PEM format, you can do:</p>

<pre><code>openssl verify cert.pem
</code></pre>

<p>If your ""ca-bundle"" is a file containing additional intermediate certificates in PEM format:</p>

<pre><code>openssl verify -untrusted ca-bundle cert.pem
</code></pre>

<p>If your openssl isn't set up to automatically use an installed set of root certificates (e.g. in <code>/etc/ssl/certs</code>), then you can use <code>-CApath</code> or <code>-CAfile</code> to specify the CA.</p>
","16232"
"reading from serial from linux command line","147066","","<p>I have a serial port device that I would like to test using linux command line.</p>

<p>I am able to use stty and echo for sending commands to serial port, but when device responds I have no way of reading what is coming from serial port. I am using </p>

<p><code>stty -F /dev/ttyS0 speed 9600 cs8 -cstopb -parenb &amp;&amp;
echo -n ^R^B &gt; /dev/ttyS0</code></p>

<p>to send a command to the device. Device operates and sends a response back in 300 ms's. How do I print that response to the console using command line?</p>
","<p>Same as with output.  Example:</p>

<pre><code>cat /dev/ttyS0
</code></pre>

<p>Or:</p>

<pre><code>cat &lt; /dev/ttyS0
</code></pre>

<p>The first example is an app that opens the serial port and relays what it reads from it to its <code>stdout</code> (your console).  The second is the shell directing the serial port traffic to any app that you like; this particular app then just relays its <code>stdin</code> to its <code>stdout</code>.</p>

<p>To get better visibility into the traffic, you may prefer a hex dump:</p>

<pre><code>od -x &lt; /dev/ttyS0
</code></pre>
","42377"
"Enable Wi-fi on Kali Linux","146910","","<p>I just installed Kali Linux on Dell Inspiron 1545, and am unable to get wireless connection.</p>

<p>I attempted the following</p>

<pre><code>root@kali:~# lspci -knn | grep Net -A2
0c:00.0 Network controller [0280]: Broadcom Corporation BCM4312 802.11b/g LP-PHY [14e4:4315] (rev 01)
    Subsystem: Dell Wireless 1397 WLAN Mini-Card [1028:000c]

# apt-get install firmware-iwlwifi
# modprobe -r iwlwifi; modprobe iwlwifi
</code></pre>

<p>I even tried commands I used when I installed Ubuntu on the same laptop</p>

<pre><code>sudo apt-get install bcmwl-kernel-source
sudo modprobe -r b43 bcma
sudo modprobe wl
</code></pre>

<p>How to fix?</p>

<h1>UPDATE</h1>

<p>I also ran</p>

<p><code>sudo apt-get install kali-linux-wireless</code></p>

<p>but I still cannot detect and connect to my home wi-fi.</p>

<p>Thanks!</p>

<h1>Tried first solution</h1>

<p>I tried first solution, but when I go to Administration > System, I get package manager. I searched for everything <code>Broadcom</code> downloaded, installed, rebooted, but still no wireless.</p>

<p>There has to be an easy solution ......</p>

<p>Am trying this, <a href=""http://www.chokepoint.net/2014/04/installing-broadcom-bcm43142-drivers-on.html"" rel=""noreferrer"">http://www.chokepoint.net/2014/04/installing-broadcom-bcm43142-drivers-on.html</a></p>

<p>I was getting errors with install. After I reboot, still no wireless detector.</p>

<h1>Update</h1>

<p>I'm trying all your suggestions .... seems I will have to read the links you provided. Who knew getting wireless would be so difficult! I will extend bounty time if possible.</p>
","<p>May be your wireless card is in turned off state, does the laptop have any dedicated physical switch or key combo(like Fn+F3 on my acer laptop) to turn on/off Wi-Fi ? most laptops also have a LED to show Wi-Fi card state.</p>

<p>Device firmwares are not pre installed in kali-linux(my last used version 1.0.4 can't tell about latest versions) , so if not already installed, install them.</p>

<p><code>sudo apt-get install firmware-linux firmware-linux-free firmware-linux-nonfree</code></p>

<p>Install Broadcom wireless card firmware </p>

<p><code>sudo apt-get install firmware-brcm80211 firmware-b43-installer firmware-b43legacy-installer broadcom-sta-dkms</code></p>

<p>Then use proper kernel drivers, b43 or b43legacy, iwlwifi is Intel Wi-Fi card driver so <em>firmware-iwlwifi</em> is not necessary.</p>
","210728"
"How to ssh to remote server using a private key?","146892","","<p>I have two servers. Both servers are in CentOS 5.6. I want to SSH from Server 1 to Server 2 using a private key I have (OpenSSH SSH-2 Private Key).</p>

<p>I don't know how to do it over unix. But what I did on windows using Putty was to feed my OpenSSH private key to putty-gen and generate a private key in PPK format. </p>

<p>However, I would be creating a bash script from server 1 that will execute some commands on server 2 via SSH. </p>

<p>How do I SSH to Server 2 using my private key file from Server 1?</p>
","<p>You need your SSH public key and you will need your ssh private key. Keys can be generated with <code>ssh_keygen</code>.
The private key must be kept on Server 1 and the public key must be stored on Server 2.</p>

<p>This is completly described in the manpage of openssh, so I will quote a lot of it. You should read the section 'Authentication'. Also the openSSH manual should be really helpful: <a href=""http://www.openssh.org/manual.html"" rel=""noreferrer"">http://www.openssh.org/manual.html</a></p>

<p>Please be careful with ssh because this affects the security of your server.</p>

<p>From <code>man ssh</code>:</p>

<pre><code> ~/.ssh/identity
 ~/.ssh/id_dsa
 ~/.ssh/id_rsa
     Contains the private key for authentication.  These files contain
     sensitive data and should be readable by the user but not acces-
     sible by others (read/write/execute).  ssh will simply ignore a
     private key file if it is accessible by others.  It is possible
     to specify a passphrase when generating the key which will be
     used to encrypt the sensitive part of this file using 3DES.

 ~/.ssh/identity.pub
 ~/.ssh/id_dsa.pub
 ~/.ssh/id_rsa.pub
     Contains the public key for authentication.  These files are not
     sensitive and can (but need not) be readable by anyone.
</code></pre>

<p>This means you can store your private key in your home directory in .ssh. Another possibility is to tell ssh via the <code>-i</code> parameter switch to use a special identity file.
Also from <code>man ssh</code>:</p>

<pre><code> -i identity_file
     Selects a file from which the identity (private key) for RSA or
     DSA authentication is read.  The default is ~/.ssh/identity for
     protocol version 1, and ~/.ssh/id_rsa and ~/.ssh/id_dsa for pro-
     tocol version 2.  Identity files may also be specified on a per-
     host basis in the configuration file.  It is possible to have
     multiple -i options (and multiple identities specified in config-
     uration files).
</code></pre>

<p>This is for the private key. Now you need to introduce your public key on Server 2. Again a quote from <code>man ssh</code>:</p>

<pre><code>  ~/.ssh/authorized_keys
         Lists the public keys (RSA/DSA) that can be used for logging in
         as this user.  The format of this file is described in the
         sshd(8) manual page.  This file is not highly sensitive, but the
         recommended permissions are read/write for the user, and not
         accessible by others.
</code></pre>

<p>The easiest way to achive that is to copy the file to Server 2 and append it to the authorized_keys file:</p>

<pre><code>scp -p your_pub_key.pub user@host:
ssh user@host
host$ cat id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<p>Authorisation via public key must be allowed for the ssh daemon, see <code>man ssh_config</code>. Usually this can be done by adding the following statement to the config file:</p>

<pre><code>PubkeyAuthentication yes
</code></pre>
","23294"
"Why was '~' chosen to represent the home directory?","146207","","<p>I have often wondered why the <kbd>~</kbd> (tilde) represents the home directory of a user. Is there a reason behind this, or is it just some infrequently used character?</p>
","<p><a href=""http://en.wikipedia.org/wiki/Tilde#Directories_and_URLs"" rel=""noreferrer"">Quoting Wikipedia</a>:</p>

<blockquote>
  <p>On Unix-like operating systems (including BSD, GNU/Linux and Mac OS
  X), tilde often indicates the current user's home directory: for
  example, if the current user's home directory is <code>/home/bloggsj</code>, then
  <code>cd</code>, <code>cd ~</code>, <code>cd /home/bloggsj</code> or <code>cd $HOME</code> are equivalent. This practice
  derives from the Lear-Siegler ADM-3A terminal in common use during the
  1970s, which happened to have the tilde symbol and the word ""Home""
  (for moving the cursor to the upper left) on the same key.</p>
</blockquote>

<p><a href=""http://en.wikipedia.org/wiki/File:KB_Terminal_ADM3A.svg"" rel=""noreferrer""><img src=""https://upload.wikimedia.org/wikipedia/commons/a/a0/KB_Terminal_ADM3A.svg"" /></a></p>

<p>Photo of Lear-Siegler ADM-3A keyboard you can find on <a href=""http://rollmops.wordpress.com/2006/05/01/vintage-computer/"" rel=""noreferrer"">this</a> site.</p>

<p><img src=""https://i.stack.imgur.com/L3esv.jpg"" alt=""""></p>

<p>This terminal is also the <a href=""https://stackoverflow.com/a/6553850/499768"">source of the movement commands</a> used in the <code>vi</code> editor: <kbd>h</kbd>, <kbd>j</kbd>, <kbd>k</kbd>, <kbd>l</kbd> for left, down, up, right.</p>
","34198"
"Difference between nohup, disown and &","146153","","<p>What is the difference between</p>

<pre><code>$ nohup foo
</code></pre>

<p>and</p>

<pre><code>$ foo &amp;
</code></pre>

<p>and</p>

<pre><code>$ foo &amp;
$ disown
</code></pre>
","<p>Let's first look at what happens if a program is started from an interactive shell (connected to a terminal) without <code>&amp;</code> (and without any redirection). So let's assume you've just typed <code>foo</code>:</p>

<ul>
<li>The process running <code>foo</code> is created.</li>
<li>The process inherits stdin, stdout, and stderr from the shell. Therefore it is also connected to the same terminal.</li>
<li>If the shell receives a <code>SIGHUP</code>, it also sends a <code>SIGHUP</code> to the process (which normally causes the process to terminate).</li>
<li>Otherwise the shell waits (is blocked) until the process terminates.</li>
</ul>

<p>Now, let's look what happens if you put the process in the background, that is, type <code>foo &amp;</code>:</p>

<ul>
<li>The process running <code>foo</code> is created.</li>
<li>The process inherits stdout/stderr from the shell (so it still writes to the terminal).</li>
<li>The process in principle also inherits stdin, but as soon as it tries to read from stdin, it is halted.</li>
<li>It is put into the list of background jobs the shell manages, which means especially:

<ul>
<li>It is listed with <code>jobs</code> and can be accessed using <code>%n</code> (where <code>n</code> is the job number).</li>
<li>It can be turned into a foreground job using <code>fg</code>, in which case it continues as if you would not have used <code>&amp;</code> on it (and if it was stopped due to trying to read from standard input, it now can proceed to read from the terminal).</li>
<li>If the shell received a <code>SIGHUP</code>, it also sends a <code>SIGHUP</code> to the process. Depending on the shell and possibly on options set for the shell, when terminating the shell it will also send a <code>SIGHUP</code> to the process.</li>
</ul></li>
</ul>

<p>Now <code>disown</code> removes the job from the shell's job list, so all the subpoints above don't apply any more (including the process being sent a <code>SIGHUP</code> by the shell). However note that it <em>still</em> is connected to the terminal, so if the terminal is destroyed (which can happen if it was a pty, like those created by <code>xterm</code> or <code>ssh</code>, and the controlling program is terminated, by closing the xterm or terminating the <a href=""http://en.wikipedia.org/wiki/Secure_Shell"" rel=""noreferrer"">SSH</a> connection), the program will fail as soon as it tries to read from standard input or write to standard output.</p>

<p>What <code>nohup</code> does, on the other hand, is to effectively separate the process from the terminal:</p>

<ul>
<li>It closes standard input (the program will <em>not</em> be able to read any input, even if it is run in the foreground.  it is not halted, but will receive an error code or <code>EOF</code>).</li>
<li>It redirects standard output and standard error to the file <code>nohup.out</code>, so the program won't fail for writing to standard output if the terminal fails, so whatever the process writes is not lost.</li>
<li>It prevents the process from receiving a <code>SIGHUP</code> (thus the name).</li>
</ul>

<p>Note that <code>nohup</code> does <em>not</em> remove the process from the shell's job control and also doesn't put it in the background (but since a foreground <code>nohup</code> job is more or less useless, you'd generally put it into the background using <code>&amp;</code>). For example, unlike with <code>disown</code>, the shell will still tell you when the nohup job has completed (unless the shell is terminated before, of course).</p>

<p>So to summarize:</p>

<ul>
<li><code>&amp;</code> puts the job in the background, that is, makes it block on attempting to read input, and makes the shell not wait for its completion.</li>
<li><code>disown</code> removes the process from the shell's job control, but it still leaves it connected to the terminal. One of the results is that the shell won't send it a <code>SIGHUP</code>. Obviously, it can only be applied to background jobs, because you cannot enter it when a foreground job is running.</li>
<li><code>nohup</code> disconnects the process from the terminal, redirects its output to <code>nohup.out</code> and shields it from <code>SIGHUP</code>. One of the effects (the naming one) is that the process won't receive any sent <code>SIGHUP</code>. It is completely independent from job control and could in principle be used also for foreground jobs (although that's not very useful).</li>
</ul>
","148698"
"How to go to the previous working directory in terminal?","146006","","<p>In terminal, how can I define a key to go to the previous directory which I was in when changing directory with the <code>cd</code> command?</p>

<p>For example, I'm in <code>/opt/soft/bin</code> and I <code>cd</code> into <code>/etc/squid3</code> and I want to get back to the first directory.</p>
","<p>You can use</p>

<pre><code>cd -
</code></pre>

<p>or you could use </p>

<pre><code>cd $OLDPWD
</code></pre>
","81229"
"How to strip multiple spaces to one using sed?","145501","","<p><code>sed</code> on AIX is not doing what I think it should.
I'm trying to replace multiple spaces with a single space in the output of IOSTAT:</p>

<pre><code># iostat
System configuration: lcpu=4 drives=8 paths=2 vdisks=0

tty:      tin         tout    avg-cpu: % user % sys % idle % iowait
          0.2         31.8                9.7   4.9   82.9      2.5

Disks:        % tm_act     Kbps      tps    Kb_read   Kb_wrtn
hdisk9           0.2      54.2       1.1   1073456960  436765896
hdisk7           0.2      54.1       1.1   1070600212  435678280
hdisk8           0.0       0.0       0.0          0         0
hdisk6           0.0       0.0       0.0          0         0
hdisk1           0.1       6.3       0.5   63344916  112429672
hdisk0           0.1       5.0       0.2   40967838  98574444
cd0              0.0       0.0       0.0          0         0
hdiskpower1      0.2     108.3       2.3   2144057172  872444176

# iostat | grep hdisk1
hdisk1           0.1       6.3       0.5   63345700  112431123

#iostat|grep ""hdisk1""|sed -e""s/[ ]*/ /g""
 h d i s k 1 0 . 1 6 . 3 0 . 5 6 3 3 4 5 8 8 0 1 1 2 4 3 2 3 5 4
</code></pre>

<p>sed should search &amp; replace (s) multiple spaces (/[ ]*/) with a single space (/ /) for the entire group (/g)... but it's not only doing that... its spacing each character.</p>

<p>What am I doing wrong? I know its got to be something simple...
AIX 5300-06</p>

<p><strong>edit:</strong> I have another computer that has 10+ hard drives. I'm using this as a parameter to another program for monitoring purposes.</p>

<p>The problem I ran into was that ""awk '{print $5}' didn't work because I'm using $1, etc in the secondary stage and gave errors with the Print command. I was looking for a grep/sed/cut version. What seems to work is:</p>

<pre><code>iostat | grep ""hdisk1 "" | sed -e's/  */ /g' | cut -d"" "" -f 5
</code></pre>

<p>The []s were ""0 or more"" when I thought they meant ""just one"". Removing the brackets got it working. Three very good answers really quickly make it hard to choose the ""answer"".</p>
","<p>The use of <code>grep</code> is redundant, <code>sed</code> can do the same. The problem is in the use of <code>*</code> that match also 0 spaces, you have to use <code>\+</code> instead:</p>

<pre><code>iostat | sed -n '/hdisk1/s/ \+/ /gp'
</code></pre>

<p>If your <code>sed</code> do not supports <code>\+</code> metachar, then do</p>

<pre><code>iostat | sed -n '/hdisk1/s/  */ /gp'
</code></pre>
","19017"
"refresh changed content of file opened in vi(m)","145359","","<p>I have a config-file that I keep open in vim, but that sometimes gets changed on disk, without these changes being reflected on the terminal. Can I refresh the content on the screen without closing and re-opening the file? If so, how?</p>
","<p>You can use the <code>:edit</code> command, without specifying a file name, to reload
the current file. If you have made modifications to the file, you can use
<code>:edit!</code> to force the reload of the current file (you will lose your
modifications).</p>
","149210"
"Turn off buffering in pipe","145327","","<p>I have a script which calls two commands:</p>

<pre><code>long_running_command | print_progress
</code></pre>

<p>The <code>long_running_command</code> prints a progress but I'm unhappy with it. I'm using <code>print_progress</code> to make it more nice (namely, I print the progress in a single line).</p>

<p>The problem: Connection a pipe to stdout also activates a 4K buffer, to the nice print program gets nothing ... nothing ... nothing ... <em>a whole lot</em> ... :)</p>

<p>How can I disable the 4K buffer for the <code>long_running_command</code> (no, I don't have the source)?</p>
","<p>You can use the <a href=""http://expect.sourceforge.net/""><code>expect</code></a> command <a href=""http://expect.sourceforge.net/example/unbuffer.man.html""><code>unbuffer</code></a>, e.g.</p>

<pre><code>unbuffer long_running_command | print_progress
</code></pre>

<p><code>unbuffer</code> connects to <code>long_running_command</code> via a pseudoterminal (pty), which makes the system treat it as an interactive process, therefore not using the 4-kiB buffering in the pipeline that is the likely cause of the delay.</p>

<p>For longer pipelines, you may have to unbuffer each command (except the final one), e.g.</p>

<pre><code>unbuffer x | unbuffer -p y | z
</code></pre>
","25375"
"Add/update a file to an existing tar.gz archive?","145267","","<p>Is there a way to add/update a file in a tar.gz archive? Basically, I have an archive which contains a file at <code>/data/data/com.myapp.backup/./files/settings.txt</code> and I'd like to pull that file from the archive (already done) and push it back into the archive once the edit has been done. How can I accomplish this? Is it problematic because of the <code>.</code> in the path? </p>
","<p>The tar file format is just a series of files concatenated together with a few headers. It's not a very complicated job to rip it apart, put your contents in, and put it back together. That being said, <a href=""https://unix.stackexchange.com/questions/13093/add-update-a-file-to-an-existing-tar-gz-archive/13094#13094"">Jander described</a> how tar as a program does not have the utility functions to do this and there are additional complications with compression, which has to both before and after making a change.</p>

<p>There are, however, tools for the job! There are at least two system out there which will allow you to to do a loopback mount of a compressed tar archive onto a folder, then make your changes in the file system. When you are done, unmount the folder and your compressed archive is ready to roll.</p>

<p>The one first option would be the <a href=""http://www.cybernoia.de/software/archivemount/"" rel=""nofollow noreferrer"">archivemount</a> project for <a href=""http://fuse.sourceforge.net/"" rel=""nofollow noreferrer"">FUSE</a>. Here is <a href=""http://www.linux.com/archive/feature/132196"" rel=""nofollow noreferrer"">a tutorial on that</a>. Your system probably already has FUSE and if it doesn't your distribution should have an option for it.</p>

<p>The other option is <a href=""http://sourceforge.net/projects/tarfs/"" rel=""nofollow noreferrer"">tarfs</a>. It's <a href=""http://www.vitanuova.com/inferno/man/4/tarfs.html"" rel=""nofollow noreferrer"">simpler to use</a>, but I've heard it has some trouble with corrupting bzip2 archives so you might test that pretty thoroughly  first.</p>
","13097"
"How to boot Linux to command-line mode instead of GUI?","144971","","<p>I am using 32-bit Red Hat Linux in my VM. I want to boot it to command-line mode, not to GUI mode. I know that from there I can switch to GUI mode using <code>startx</code> command. How do I switch back to command-line mode?</p>
","<p>You want to make runlevel 3 your default runlevel. From a terminal, switch to root and do the following:</p>

<pre><code>[user@host]$ su
Password:
[root@host]# cp /etc/inittab /etc/inittab.bak #Make a backup copy of /etc/inittab
[root@host]# sed -i 's/id:5:initdefault:/id:3:initdefault:/' /etc/inittab #Make runlevel 3 your default runlevel
</code></pre>

<p>Anything after (and including) the second <code>#</code> on each line is a comment for you, you don't need to type it into the terminal.</p>

<p>See <a href=""http://en.wikipedia.org/wiki/Runlevel"">the Wikipedia page</a> on runlevels for more information.</p>

<p><strong>Explanation of <code>sed</code> command</strong></p>

<ul>
<li>The <code>sed</code> command is a stream editor (hence the name), you use it to manipulate streams of data, usually through <a href=""http://en.wikipedia.org/wiki/Regular_expression"">regular expressions</a>. </li>
<li>Here, we're telling <code>sed</code> to replace the pattern <code>id:5:initdefault:</code> with the pattern <code>id:3:initdefault:</code> in the file <code>/etc/inittab</code>, which is the file that controls your runlevles. The general syntax for a <code>sed</code> search and replace is <code>s/pattern/replacement_pattern/</code>.</li>
<li>The <code>-i</code> option tells <code>sed</code> to apply the modifications in place. If this were not present, <code>sed</code> would have outputted the resulting file (after substitution) to the terminal (more generally to standard output).</li>
</ul>

<p><strong>Update</strong></p>

<p>To switch back to text mode, simply press <kbd>CTRL</kbd>+<kbd>ALT</kbd>+<kbd>F1</kbd>. This will not stop your graphical session, it will simply switch you back to the terminal you logged in at. You can switch back to the graphical session with <kbd>CTRL</kbd>+<kbd>ALT</kbd>+<kbd>F7</kbd>.</p>
","90556"
"How can I get a full process list in solaris, without truncated lines?","144967","","<p>Is there a way to generate a full process listing in solaris, without truncated lines?  I've  tried the <code>ps</code> command, with the following arguments:   </p>

<pre>
  -f                  Generates a full listing. (See below for
                      significance  of columns in a full list-
                      ing.)
</pre>

<pre>
  -l                  Generates a long listing. (See below.)
</pre>

<p>So, those both seem to do what I want, however, further down in the ps man page, I find this: </p>

<pre> 
 args                    The command with all its arguments as  a
                         string.  The implementation may truncate
                         this value to the  field  width;  it  is
                         implementation-dependent   whether   any
                         further   truncation   occurs.   It   is
                         unspecified     whether    the    string
                         represented is a version of the argument
                         list  as  it  was  passed to the command
                         when it started, or is a version of  the
                         arguments as they may have been modified
                         by the application. Applications  cannot
                         depend  on  being  able  to modify their
                         argument list and having that  modifica-
                         tion  be  reflected in the output of ps.
                         The Solaris  implementation  limits  the
                         string  to  80  bytes; the string is the
                         version of the argument list as  it  was
                         passed to the command when it started.
</pre>

<p>Which basically says the output is going to be truncated and there is nothing I can do about it.  So, I'm coming here.  Surely other people have run into this problem and maybe even have a way around it.  I'm guessing ps can't do it and so I need to use other tools to do this.  Is that accurate? </p>
","<p>you could try</p>

<pre><code>pargs &lt;PID&gt;
</code></pre>

<p>this gives you a list of all arguments</p>

<p>or else use an other ps. If run as root (or any user with enough privileges for that matter)</p>

<pre><code>/usr/ucb/ps auxww
</code></pre>

<p>will give you all arguments. Its part of SUNWscpu, ""Source Compatibility, (Usr)""</p>
","3942"
"How to read first and last line from cat output?","144613","","<p>I have text file. Task - get first and last line from file after </p>

<pre><code>$ cat file | grep -E ""1|2|3|4"" | commandtoprint

$ cat file
1
2
3
4
5
</code></pre>

<p>Need this without cat output (only 1 and 5).</p>

<pre><code>~$ cat file | tee &gt;(head -n 1) &gt;(wc -l)
1
2
3
4
5
5
1
</code></pre>

<p>Maybe awk and more shorter solution exist...</p>
","<p><strong>sed</strong> Solution:</p>

<pre><code>sed -e 1b -e '$!d' file
</code></pre>

<p>When reading from <code>stdin</code> if would look like this (for example <code>ps -ef</code>):</p>

<pre><code>ps -ef | sed -e 1b -e '$!d'
UID        PID  PPID  C STIME TTY          TIME CMD
root      1931  1837  0 20:05 pts/0    00:00:00 sed -e 1b -e $!d
</code></pre>

<p><strong>head &amp; tail</strong> Solution:</p>

<pre><code>(head -n1 &amp;&amp; tail -n1) &lt;file
</code></pre>

<p>When data is coming from a command (<code>ps -ef</code>):</p>

<pre><code>ps -ef 2&gt;&amp;1 | (head -n1 &amp;&amp; tail -n1)
UID        PID  PPID  C STIME TTY          TIME CMD
root      2068  1837  0 20:13 pts/0    00:00:00 -bash
</code></pre>

<p><strong>awk</strong> Solution:</p>

<pre><code>awk 'NR==1; END{print}' file
</code></pre>

<p>And also the piped example with <code>ps -ef</code>:</p>

<pre><code>ps -ef | awk 'NR==1; END{print}'
UID        PID  PPID  C STIME TTY          TIME CMD
root      1935  1837  0 20:07 pts/0    00:00:00 awk NR==1; END{print}
</code></pre>
","139099"
"How to change from csh to bash as default shell","144556","","<p>I have <code>csh</code> as my default shell, as shown by <code>echo $SHELL</code>. I want to switch to <code>bash</code> as my default shell. I tried the following approaches to no avail:</p>

<ol>
<li><p>chsh. I get:</p>

<pre><code>chsh: can only change local entries; use ypchsh instead.
</code></pre></li>
<li><p>ypchsh. I get:</p>

<pre><code>ypchsh: yppasswdd not running on NIS master host (""dcsun2"").
</code></pre></li>
</ol>

<p>I only have <code>.chsrc</code> in my home directory and I cannot find any <code>.profile</code> files in <code>/etc</code>. How can I change my default shell to <code>bash</code>?</p>
","<ol>
<li><p>Make sure you've got <code>bash</code> installed.</p></li>
<li><p>Learn the location of <code>bash</code>:</p>

<pre><code>which bash
</code></pre>

<p>or</p>

<pre><code>whereis bash
</code></pre>

<p>Below, I'll assume the location is <code>/bin/bash</code>.</p>

<p>a) If you have administrative rights, just run as root:</p>

<pre><code>usermod -s /bin/bash YOUR_USERNAME
</code></pre>

<p>(replacing <code>YOUR_USERNAME</code> with your user name).</p>

<p>b) If you don't have adm. rights, you can still just run <code>bash --login</code> at login, by putting the below line <strong>at the end</strong> of your <code>.cshrc</code> or <code>.profile</code> (in your home directory) :</p>

<pre><code>setenv SHELL /bin/bash
exec /bin/bash --login
</code></pre></li>
</ol>
","20635"
"Iptables to allow incoming FTP","143842","","<p>I want to allow incoming FTP traffic.</p>

<p>CentOS 5.4:</p>

<p>This is my <code>/etc/sysconfig/iptables</code> file.</p>

<pre><code># Generated by iptables-save v1.3.5 on Thu Oct  3 21:23:07 2013
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [133:14837]
-A INPUT -p tcp -m tcp --dport 21 -j ACCEPT
-A INPUT -p tcp -m state --state ESTABLISHED -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-port-unreachable
-A OUTPUT -p tcp -m tcp --sport 20 -j ACCEPT
COMMIT
# Completed on Thu Oct  3 21:23:07 2013
</code></pre>

<p>Also, by default, ip_conntrack_netbios_n module is getting loaded.</p>

<pre><code>#service iptables restart

Flushing firewall rules:                                   [  OK  ]
Setting chains to policy ACCEPT: filter                    [  OK  ]
Unloading iptables modules:                                [  OK  ]
Applying iptables firewall rules:                          [  OK  ]
Loading additional iptables modules: ip_conntrack_netbios_n[  OK  ]
</code></pre>

<p>But the problem is not with that module, as I tried unloading it and still no luck.</p>

<p>If I disable iptables, I am able to transfer my backup from another machine to FTP.
If iptables is enforcing, then transfer failed.</p>
","<p>Adding NEW fixed it, I believe.</p>

<p>Now, my iptables file look like this..</p>

<pre><code># Generated by iptables-save v1.3.5 on Thu Oct  3 22:25:54 2013
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [824:72492]

-A INPUT -p tcp -m tcp --dport 21 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 20 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --sport 1024:65535 --dport 20:65535 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m state --state ESTABLISHED -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-port-unreachable
-A OUTPUT -p tcp -m tcp --dport 21 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
-A OUTPUT -p tcp -m tcp --dport 20 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
-A OUTPUT -p tcp -m tcp --sport 1024:65535 --dport 20:65535 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Thu Oct  3 22:25:54 2013
</code></pre>

<p>Typing it as answer, since too many characters are not allowed in comment.. Thank you so much for your help.</p>
","93568"
"How to determine CentOS version?","143769","","<p>How do I determine the version of a CentOS server without access to any graphical interface? I've tried several commands:</p>

<pre><code># cat /proc/version
Linux version 2.6.18-128.el5 (mockbuild@hs20-bc1-7.build.redhat.com)
(gcc version 4.1.2 20080704 (Red Hat 4.1.2-44)) …

# cat /etc/issue
Red Hat Enterprise Linux Server release 5.3 (Tikanga)
</code></pre>

<p>but which one is correct: 4.1.2-4 from <code>/proc/version</code> or 5.3 from <code>/etc/issue</code>?</p>
","<p>As you can see in <code>/etc/issue</code>, you're using CentOS 5.3. (It says Red Hat because CentOS is based upon the RH sources, and some software checks <code>/etc/issue</code> to identify the distro in use; thus, they'd fail if this was changed to CentOS).</p>

<p>The <code>4.1.2-4</code> in <code>/proc/version</code> refers to the version of the <code>gcc</code> C compiler used to build the kernel.</p>
","54988"
"""no public key available"" on apt-get update","143658","","<p>When performing <code>apt-get update</code>, I get the following error: </p>

<pre><code>root@ADS3-Debian6:/home/aluno# apt-get update
Atingido http://sft.if.usp.br squeeze Release.gpg
Ign http://sft.if.usp.br/debian/ squeeze/contrib Translation-en
Ign http://sft.if.usp.br/debian/ squeeze/contrib Translation-pt
Ign http://sft.if.usp.br/debian/ squeeze/contrib Translation-pt_BR
</code></pre>

<p>(...)</p>

<pre><code>Obter:10 http://security.debian.org squeeze/updates/non-free i386 Packages [14 B]
Baixados 612 kB em 4s (125 kB/s)                    
Lendo listas de pacotes... Pronto
There is no public key available for the following key IDs: 8B48AD6246925553
</code></pre>
","<p>the other answers will work, or not, depending on whether or not the key '8B48AD6246925553' is present in the packages they indicate.</p>

<p>if you need a key, you have to get that key, and where to find it, it's in a key server (very probably any key server will do):</p>

<p><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 8B48AD6246925553</code></p>
","205732"
"Changing a file's ""Date Created"" and ""Last Modified"" attributes to another file's","143407","","<p>I'm using merge cap to create a merge pcap file from 15 files. For the merged file, I have changed the name to that of the first of the 15 files. But I would also like to change the merged file's attributes like ""Date Created"" and ""Last Modified"" to that of the first one. Is there anyway to do this? </p>

<pre><code>FILES_dcn=($(find  $dir_dcn -maxdepth 1 -type f -name ""*.pcap""  -print0 | xargs -0 ls -lt | tail -15 | awk '{print $9}'))
TAG1_dcn=$(basename ""${FILES_dcn[14]}"" | sed 's/.pcap//')
mergecap -w  ""${dir_dcn}""/merge_dcn.pcap ""${FILES_dcn[@]}""
mv  ""${dir_dcn}""/merge_dcn.pcap  ""${dir_dcn}""/""${TAG1_dcn}"".pcap
</code></pre>

<p>I try to access the merged files over a samba server (Ubuntu). So that an extractor function can access auto extract the files to D folder. But as the created date will be changed for the merged file the extraction fails. Is there anyway to fix this?</p>
","<p>You can use the <code>touch</code> command along with the <code>-r</code> switch to apply another file's attributes to a file.</p>

<p><strong>NOTE:</strong> There is no such thing as creation date in Unix, there are only access, modify, and change. See this U&amp;L Q&amp;A titled: <a href=""https://unix.stackexchange.com/questions/102691/get-age-of-given-file/102692#102692"">get age of given file</a> for further details.</p>

<pre><code>$ touch -r goldenfile newfile
</code></pre>

<h3>Example</h3>

<p>For example purposes here's a <code>goldenfile</code> that was created with some arbitrary timestamp.</p>

<pre><code>$ touch -d 20120101 goldenfile
$ ls -l goldenfile 
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 goldenfile
</code></pre>

<p>Now I make some new file:</p>

<pre><code>$ touch newfile
$ ls -l newfile 
-rw-rw-r--. 1 saml saml 0 Mar  7 09:06 newfile
</code></pre>

<p>Now apply <code>goldenfile</code>'s attributes to <code>newfile</code>.</p>

<pre><code>$ touch -r goldenfile newfile 
$ ls -l goldenfile newfile
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 newfile
-rw-rw-r--. 1 saml saml 0 Jan  1  2012 goldenfile
</code></pre>

<p>Now <code>newfile</code> has the same attributes.</p>

<h3>Modify via Samba</h3>

<p>I just confirmed that I'm able to do this using my Fedora 19 laptop which includes version 1.16.3-2 connected to a Thecus N12000 NAS (uses a modified version of CentOS 5.x).</p>

<p>I was able to touch a file as I mentioned above and it worked as I described. Your issue is likely a problem with the either the mounting options being used, which may be omitting the tracking of certain time attributes, or perhaps it's related to one of these bugs:</p>

<ul>
<li><a href=""https://bugzilla.redhat.com/show_bug.cgi?id=461505"" rel=""noreferrer"">Bug 461505 - can't set timestamp on samba shares</a></li>
<li><a href=""https://bugzilla.gnome.org/show_bug.cgi?id=693491"" rel=""noreferrer"">Bug 693491 - Unable to set attributes/timestamps on CIFS/Samba share</a></li>
</ul>
","118580"
"How to disable SELinux without restart?","143143","","<p>I need to disable SELinux but cannot restart the machine</p>

<p>i followed <a href=""http://ithelpblog.com/os/linux/redhat/centos-redhat/disable-selinux-without-reboot-on-centos-6-3/"">this</a> link where i get bellow command</p>

<pre><code>setenforce 0
</code></pre>

<p>But after running this command i checked for that</p>

<pre><code>sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   permissive
Mode from config file:          disabled
Policy version:                 24
Policy from config file:        targeted
</code></pre>

<p>Is there any other option?</p>
","<p><code>sestatus</code> is showing the current mode as <code>permissive</code>.</p>

<p>In <code>permissive</code> mode, SELinux will not block anything, but merely warns you. The line will show <code>enforcing</code> when it's actually blocking.</p>

<p>I don't believe it's possible to completely disable SELinux without a reboot.</p>
","148898"
"Make all new files in a directory accessible to a group","142995","","<p>Suppose I have two users A and B and a group G and a folder foo, both users are members of G (using linux and ext3). </p>

<p>If I save as user A a file under foo, the permissions are: <code>-rw-r--r-- A A</code>. However it is possible to achieve that every file saved under some subdirectory of foo has permissions <code>-rwxrwx--- A G</code> (i.e. owner A, group G)? </p>
","<p>You can control the assigned permission bits with <code>umask</code>, and the group by making the directory <em>setgid</em> to <code>G</code>.</p>

<pre><code>$ umask 002            # allow group write; everyone must do this
$ chgrp G .            # set directory group to G
$ chmod g+s .          # files created in directory will be in group G
</code></pre>

<p>Note that you have to do the <code>chgrp</code>/<code>chmod</code> for every subdirectory; it doesn't propagate automatically (that is, neither existing nor subsequently created directories under a <em>setgid</em> directory will be <em>setgid</em>, although the latter will be in group <code>G</code>).</p>

<p>Also note that <code>umask</code> is a process attribute and applies to all files created by that process and its children (which inherit the <code>umask</code> in effect in their parent at <code>fork()</code> time).  Users may need to set this in <code>~/.profile</code>, and may need to watch out for things unrelated to your directory that need different permissions.  <a href=""http://modules.sourceforge.net"" title=""modules — Software Environment Management"">modules</a> may be useful if you need different settings when doing different things.</p>

<p>You can control things a bit better if you can use POSIX ACLs; it should be possible to specify both a permissions mask and a group, and have them propagate sensibly.  Support for POSIX ACLs is somewhat variable, though.</p>
","12845"
"sudo as another user with their environment","142906","","<p></p>

<pre><code>$ whoami
admin
$ sudo -S -u otheruser whoami
otheruser
$ sudo -S -u otheruser /bin/bash -l -c 'echo $HOME'
/home/admin
</code></pre>

<p><strong>Why isn't <code>$HOME</code> being set to <code>/home/otheruser</code> even though bash  is invoked as a login shell?</strong></p>

<p>Specifically, <code>/home/otheruser/.bashrc</code> isn't being sourced.
Also, <code>/home/otheruser/.profile</code> isn't being sourced. - (<code>/home/otheruser/.bash_profile</code> doesn't exist)</p>

<p>EDIT:
The exact problem is actually <a href=""https://stackoverflow.com/questions/27738224/mkvirtualenv-with-fabric-as-another-user-fails"">https://stackoverflow.com/questions/27738224/mkvirtualenv-with-fabric-as-another-user-fails</a></p>
","<p>To invoke a login shell using <code>sudo</code> just use <code>-i</code>. When command is not specified you'll get a login shell prompt, otherwise you'll get the output of your command.</p>

<p>Example (login shell):</p>

<pre><code>sudo -i
</code></pre>

<p>Example (with a specified user):</p>

<pre><code>sudo -i -u user
</code></pre>

<p>Example (with a command):</p>

<pre><code>sudo -i -u user whoami
</code></pre>

<p>Example (print user's <code>$HOME</code>):</p>

<pre><code>sudo -i -u user echo \$HOME
</code></pre>

<p>Note: The backslash character ensures that the dollar sign reaches the target user's shell and is not interpreted in the calling user's shell.</p>

<p>I have just checked the last example with <em>strace</em> which tells you exactly what's happening. The output bellow shows that the shell is being called with <code>--login</code> and with the specified command, just as in your explicit call to bash, but in addition <em>sudo</em> can do its own work like setting the <code>$HOME</code>.</p>

<pre><code># strace -f -e process sudo -S -i -u user echo \$HOME
execve(""/usr/bin/sudo"", [""sudo"", ""-S"", ""-i"", ""-u"", ""user"", ""echo"", ""$HOME""], [/* 42 vars */]) = 0
...
[pid 12270] execve(""/bin/bash"", [""-bash"", ""--login"", ""-c"", ""echo \\$HOME""], [/* 16 vars */]) = 0
...
</code></pre>

<p>I noticed that you are using <code>-S</code> and I don't think it is generally a good technique. If you want to run commands as a different user without performing authentication from the keyboard, you might want to use SSH instead. It works for <code>localhost</code> as well as for other hosts and provides public key authentication that works without any interactive input.</p>

<pre><code>ssh user@localhost echo \$HOME
</code></pre>

<p>Note: You don't need any special options with SSH as the SSH server always creates a login shell to be accessed by the SSH client.</p>
","177011"
"Can I transfer files using SSH?","142667","","<p>I am using <a href=""http://www.chiark.greenend.org.uk/~sgtatham/putty/"">PuTTY</a> on Windows 7 to SSH to my school computer lab. Can I transfer files from my Windows machine to my user on the school machines using SSH?</p>
","<p>Use the PSCP tool from the putty download page:</p>

<p><a href=""http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html"">http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html</a></p>

<p>PSCP is the putty version of scp which is a cp (copy) over ssh command.</p>

<p>PSCP needs to be installed on your windows computer (just downloaded, really, there's no install process). Nothing needs to be installed on the school's servers. PSCP and scp both use ssh to connect.</p>

<p><strong>To answer the usage question from the comments:</strong></p>

<p>To upload from your computer to a remote server:</p>

<pre><code>c:\pscp c:\some\path\to\a\file.txt user@remote:\home\user\some\path
</code></pre>

<p>This will up load the file file.txt to the specified directory on the server.
If the final part of the destination path is NOT a directory, it will be the new file name. You could also do this to upload the file with a different name:</p>

<pre><code>c:\pscp c:\some\path\to\a\file.txt user@remote:\home\user\some\path\newname.txt
</code></pre>

<p>To download a file from a remote server to your computer:</p>

<pre><code>c:\pscp user@remote:\home\user\some\file.txt c:\some\path\to\a\
</code></pre>

<p>or</p>

<pre><code>c:\pscp user@remote:\home\user\some\file.txt c:\some\path\to\a\newfile.txt
</code></pre>

<p>or</p>

<pre><code>c:\pscp user@remote:\home\user\some\file.txt .
</code></pre>

<p>With a lone dot at the end there. This will download the specified file to the current directory.</p>

<p>Since the comment is too far down, I should also point out here that WinSCP exists providing a GUI for all this, if that's of interest: <a href=""http://winscp.net/eng/download.php"">http://winscp.net/eng/download.php</a></p>
","92716"
"How to bring up a wi-fi interface from a command line?","141765","","<p>I can't figure out how to properly bring up the wi-fi card on my laptop. When I turn it on and issue</p>

<pre><code>$ sudo iwconfig wlan0 txpower auto
$ sudo iwlist wlan0 scan
wlan0     Interface doesn't support scanning : Network is down
</code></pre>

<p>it reports that the network is down. Trying to bring it up fails too:</p>

<pre><code>$ sudo ifup wlan0
wlan0     no private ioctls.

Failed to bring up wlan0.
</code></pre>

<p>Apparently I'm missing some basic low-level <code>iw...</code> command.</p>

<p>When I issue <code>dhclient</code> on the interface:</p>

<pre><code>$ sudo dhclient -v wlan0
Internet Systems Consortium DHCP Client 4.2.2
Copyright 2004-2011 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/

^C$
</code></pre>

<p>and interrupt it, it brings the device up somehow and then scanning etc. works. I'd like to avoid this obviously superfluous step.</p>
","<p>Indeed, try <code>sudo ifconfig wlan0 up</code>. To elaborate on the answer by Martin:</p>

<p><code>ifup</code> and <code>ifdown</code> commands are part of <a href=""http://packages.debian.org/search?keywords=ifupdown"" rel=""nofollow noreferrer"">ifupdown package</a>, which now <a href=""http://www.debian.org/doc/manuals/debian-reference/ch05.en.html#_the_basic_network_configuration_with_ifupdown_legacy"" rel=""nofollow noreferrer"">is considered a legacy frontend for network configuration</a>, compared to newer ones, such as <code>network manager</code>. </p>

<p>Upon <code>ifup</code> <code>ifupdown</code> reads configuration settings from <code>/etc/network/interfaces</code>; it runs <code>pre-up</code>, <code>post-up</code> and <code>post-down</code> scripts from <code>/etc/network</code>, which include starting <code>/etc/wpasupplicant/ifupdown.sh</code> that processes additional <code>wpa-*</code> configuration options for wpa wifi, in <code>/etc/network/interfaces</code>  (see <code>zcat /usr/share/doc/wpasupplicant/README.Debian.gz</code> for documentation). For WEP <code>wireless-tools</code> package plays similar role to <code>wpa-supplicant</code>. <code>iwconfig</code> is from <code>wireless-tools</code>, too.</p>

<p><code>ifconfig</code> at the same time <a href=""https://askubuntu.com/questions/1786/what-is-the-difference-between-network-manager-and-ifconfig-ifup-etc"">is a lower level tool</a>, which is used by <code>ifupdown</code> and allows for more flexibility. For instance, there are 6 modes of wifi adapter functioning and IIRC <code>ifupdown</code> covers only managed mode (+ roaming mode, which formally isn't mode?). With <code>iwconfig</code> and <code>ifconfig</code> you <a href=""http://www.noah.org/wiki/Wireshark"" rel=""nofollow noreferrer"">can enable</a> e.g. <a href=""http://en.wikipedia.org/wiki/Monitor_mode"" rel=""nofollow noreferrer"">monitor mode</a> of your wireless card, while with <code>ifupdown</code> you won't be able to do that directly.</p>
","90807"
"Why the default eth0 interface is down by default on CentOS?","141727","","<p>I am already a little bit familiar with Linux distros like Debian or Ubuntu (yeah, very similar) but I wanted to try Red Hat based - CentOS 6.2 . I have installed it on my Windows 7 host in virtualBox and tried to play with it a little. </p>

<p>I have come across a small problem, namely : the default <code>eth0</code> interface is down by default. I use the option with NAT ( the virtual machine is 'behind' the host ). Even if I bring the interface up with </p>

<p><code>ifconfig eth0 up</code> it does not work right away. I get this after bringing the interface up: </p>

<p>What should be done more to configure the network on CentOS machine?<img src=""https://i.stack.imgur.com/DRmqO.png"" alt=""CentOS showing the output of ifconfig.  Note that eth0 is down.""></p>

<p>P.S. Sorry for screenshot but I do not know how to get the text out of the VirtualBox.</p>
","<p>Edit <code>/etc/sysconfig/network-scripts/ifcfg-$IFNAME</code>. Change the <code>ONBOOT</code> line's value to <code>yes</code>.</p>

<p><code>$IFNAME</code> will be <code>eth0</code> on many EL6 boxes, but on EL7 and EL6 boxes using the <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Networking_Guide/ch-Consistent_Network_Device_Naming.html"">Consistent Network Device Naming</a> scheme, it might be something else, like <code>en3p1</code>. Use the command <code>ip link</code> to get a list of network interfaces, including the ones that are currently down.</p>

<p>In your future installs, pay more attention. You blew past an option in the network configuration section that let you tell it to bring the interface up on boot. This on-boot option is off by default in EL6 and EL7, whereas in previous versions, it was on by default.</p>

<p>To make the network interface come up on first boot at install time in EL7, go to the <code>Configure</code> &rarr; <code>General</code> tab in the network configuration screen, then check the box labeled <code>Automatically connect to the network when available</code>.</p>

<p>As to <em>why</em> they changed this, I'd guess security reasons. It gives you a chance to tighten things down a bit from the default setup before bringing up the network interface for the first time.</p>
","36713"
"VSFTPD, 553 Could not create file. - permissions?","141543","","<p>I've set up VSFTPD on Amazon EC2 with the Amazon Linux AMI. I created a user and can now successfully connect via ftp. However, if I try to upload something I get the error message ""553 Could not create file.""</p>

<p>I assume this has to do with permissions, but I don't know enough about it to be able to fix it. So basically, what do I have to do to be able to upload files?</p>
","<p>There are two likely reasons that this could happen -- you do not have write and execute permissions on the directories leading to the directory you are trying to upload to, or <code>vsftpd</code> is configured not to allow you to upload.</p>

<p>In the former case, use <code>chmod</code> and <code>chown</code> as appropriate to make sure that your user has these permissions on every intermediate directory. The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory's attributes, whilst the read bit allows the affected user to list the files within the directory. Since intermediate directories in the path also affect this, the permissions must be set appropriately leading up to the ultimate destination that you intend to upload to.</p>

<p>In the latter case, look at your <code>vsftpd.conf</code>. <code>write_enable</code> must be true to allow writing (and it is false by default). There is good documentation on this configuration file at <code>man 5 vsftpd.conf</code>.</p>
","39479"
"When should I not kill -9 a process?","141470","","<p>I am always very hesitant to run <code>kill -9</code>, but I see other admins do it almost routinely.</p>

<p>I figure there is probably a sensible middle ground, so:</p>

<ol>
<li>When and why should <code>kill -9</code> be used? When and why not?</li>
<li>What should be tried before doing it?</li>
<li>What kind of debugging a ""hung"" process could cause further problems?</li>
</ol>
","<p>Generally, you should use <code>kill -15</code> before <code>kill -9</code> to give the target process a chance to clean up after itself.  (Processes can't catch or ignore <code>SIGKILL</code>, but they can and often do catch <code>SIGTERM</code>.)  If you don't give the process a chance to finish what it's doing and clean up, it may leave corrupted files (or other state) around that it won't be able to understand once restarted.</p>

<p><code>strace</code>/<code>truss</code>, <code>ltrace</code> and <code>gdb</code> are generally good ideas for looking at why a stuck process is stuck.   (<code>truss -u</code> on Solaris is particularly helpful; I find <code>ltrace</code> too often presents arguments to library calls in an unusable format.)  Solaris also has useful <code>/proc</code>-based tools, some of which have been ported to Linux.  (<code>pstack</code> is often helpful).</p>
","8918"
"What options `ServerAliveInterval` and `ClientAliveInterval` in sshd_config exactly do?","141195","","<p>I found <a href=""https://unix.stackexchange.com/questions/2010/what-does-the-broken-pipe-message-mean-in-an-ssh-session"">this question</a>, but I'm sorry I don't quite understand the settings on the two variables <code>ServerAliveInterval</code> and <code>ClientAliveInterval</code> mentioned in the accepted response.  If my local server is timing out, should I set this value to zero?  Will it then never time out?  Should I instead set it to 300 seconds or something?</p>

<p>My question is simply, some of my connections time out when I suspend &amp; then unsuspend my laptop with the response <code>Write failed: Broken pipe</code> and some don't.  How can I correctly configure a local sshd so that they don't fail with a broken pipe?</p>
","<p><a href=""http://man.openbsd.org/ssh_config#ServerAliveInterval"" rel=""noreferrer"">ServerAliveInterval</a>: number of seconds that the <strong>client</strong> will wait before sending a null packet to the server (to keep the connection alive).</p>

<p><a href=""http://man.openbsd.org/sshd_config#ClientAliveInterval"" rel=""noreferrer"">ClientAliveInterval</a>: number of seconds that the <strong>server</strong> will wait before sending a null packet to the client (to keep the connection alive).</p>

<p>Setting a value of 0 (the default) will disable these features so your connection could drop if it is idle for too long.</p>

<p>ServerAliveInterval seems to be the most common strategy to keep a connection alive. To prevent the broken pipe problem, here is the ssh config I use in my .ssh/config file:</p>

<pre><code>Host myhostshortcut
     HostName myhost.com
     User barthelemy
     ServerAliveInterval 60
     ServerAliveCountMax 10
</code></pre>

<p>The above setting will work in the following way,</p>

<ol>
<li>The client will wait idle for 60 seconds (ServerAliveInterval time) and, send a ""no-op null packet"" to the server and expect a response. If no response comes, then it will keep trying the above process till 10 (ServerAliveCountMax) times (600 seconds). If the server still doesn't respond, then the client disconnects the ssh connection.</li>
</ol>

<p>ClientAliveCountMax on the server side might also help. This is the limit of how long a client are allowed to stay unresponsive before being disconnected. The default value is 3, as in three ClientAliveInterval.</p>
","3027"
"Shell Script for logging into a ssh server","140920","","<p>I tried writing a shell script which can do automatic login into a ssh server using password which is mentioned in the script. I have written the following code:</p>

<pre><code>set timeout 30
/usr/bin/ssh -p 8484 root@172.31.72.103
expect 
{
   ""root@172.31.72.103's password"" 
   {
      send ""password\r"" 
   }
}
</code></pre>

<p>This code is not running properly, still it is asking for the password. Can somebody please help me in solving this</p>
","<p>I once wrote an <code>expect</code> script to log in to a ssh server (like your case) and my script was something like this:</p>

<pre><code>#!/usr/bin/expect

spawn ssh MyUserName@192.168.20.20
expect ""password""
send ""MyPassword\r""
interact
</code></pre>

<p>I think maybe the <code>interact</code> is missing in your script.</p>
","31080"
"How can I change a user's default group in Linux?","140607","","<p>Being new to Linux administration, I'm a little confused about the following commands:</p>

<pre><code>useradd
usermod
groupadd
groupmod
</code></pre>

<p>I've just finished reading the user administration book in the Linux/Unix Administrator's handbook, but some things are still a little hazy.</p>

<p>Basically <code>useradd</code> seems straight forward enough:</p>

<pre><code>useradd -c ""David Hilbert"" -d /home/math/hilbert -g faculty -G famous -m -s /bin/sh hilbert
</code></pre>

<p>I can add ""David Hilbert"" with username <code>hilbert</code> , setting his default directory, shell, and groups. And I think that <code>-g</code> is his primary/default group and <code>-G</code> are his other groups.</p>

<p>So these are my next questions:</p>

<ol>
<li>Would this command still work if the groups <code>faculty</code> and <code>famous</code> did not exist? Would it just create them?</li>
<li>If not, what command do I use to create new groups?</li>
<li>If I remove the user <code>hilbert</code> and there are no other users in those groups, will they still exist? Should I remove them?</li>
<li>After I run the <code>useradd</code> command above, how do I remove David from the <code>famous</code> group, and reassign his primary group to <code>hilbert</code> which does not yet exist?</li>
</ol>
","<p>The <code>usermod</code> command will allow you to change a user's primary group, supplementary group or a number of other attributes. The <code>-g</code> switch controls the primary group.</p>

<p>For your other questions... </p>

<ol>
<li><p>If you specify a group, <code>groupname</code>, that does not exist during the <code>useradd</code> stage, you will receive an error - <em>useradd: unknown group groupname</em></p></li>
<li><p>The <code>groupadd</code> command creates new groups. </p></li>
<li><p>The group will remain if you remove all users contained within. You don't necessarily have to remove the empty group.</p></li>
<li><p>Create the <code>hilbert</code> group via <code>groupadd hilbert</code>. Then move David's primary group using <code>usermod -g hilbert hilbert</code>. (Please note that the first <code>hilbert</code> is the group name and the second <code>hilbert</code> is the username. This is important in cases, where you are moving a user to a group with a different name)</p></li>
</ol>

<p>You may be complicating things a bit here, though. In many Linux distributions, a simple <code>useradd hilbert</code> will create the user <code>hilbert</code> and a group of the same name as the primary. I would add supplementary groups specified together using the <code>-G</code> switch.</p>
","26680"
"Automatically run commands over SSH on many servers","140530","","<p>There is a list of IP addresses in a .txt file, ex.:</p>

<pre><code>1.1.1.1
2.2.2.2
3.3.3.3
</code></pre>

<p>Behind every IP address there is a server, and on every server there is an sshd running on port 22. Not every server is in the <code>known_hosts</code> list (on my PC, Ubuntu 10.04 LTS/bash).</p>

<p>How can I run commands on these servers, and collect the output?</p>

<p>Ideally, I'd like to run the commands in parallel on all the servers.</p>

<p>I'll be using public key authentication on all the servers.</p>

<p>Here are some potential pitfalls:</p>

<ul>
<li>The ssh prompts me to put the given servers ssh key to my <code>known_hosts</code> file.</li>
<li>The given commands might return a nonzero exit code, indicating that the output is potentially invalid. I need to recognize that.</li>
<li>A connection might fail to be established to a given server, for example because of a network error.</li>
<li>There should be a timeout, in case the command runs for longer than expected or the server goes down while running the command.</li>
</ul>

<p>The servers are AIX/ksh (but I think that doesn't really matter.</p>
","<p>Assuming that you are not able to get pssh or others installed, you could do something similar to:</p>

<pre><code>tmpdir=${TMPDIR:-/tmp}/pssh.$$
mkdir -p $tmpdir
count=0
while IFS= read -r userhost; do
    ssh -n -o BatchMode=yes ${userhost} 'uname -a' &gt; ${tmpdir}/${userhost} 2&gt;&amp;1 &amp;
    count=`expr $count + 1`
done &lt; userhost.lst
while [ $count -gt 0 ]; do
    wait $pids
    count=`expr $count - 1`
done
echo ""Output for hosts are in $tmpdir""
</code></pre>
","19015"
"why won't x11 display work through ssh login?","140371","","<p>I have logged in to a remote server and am trying to display an x application(e.g. firefox). but an error message appears. the below are my attempts to open firefox</p>

<pre><code>Black@Black-PC ~
$ ssh -X kwagjj@$labserver -p 122
[kwagjj@James5 ~]$ firefox
Error: no display specified
[kwagjj@James5 ~]$ exit
logout
Connection to 143.248.146.204 closed.

Black@Black-PC ~
$ ssh -Y kwagjj@$labserver -p 122
[kwagjj@James5 ~]$ firefox
Error: no display specified
[kwagjj@James5 ~]$ 
</code></pre>

<p>I used -X, -Y because I read somewhere that these two options are related with credentials regarding X11 and these switches will do the job for me. Even without the -X, -Y switches, my attempt failed.</p>

<p>What does the 'no display speicified' error mean?</p>

<p>P.S. The weird thing is that if I connect to the remote server through my PUTTY and repeat the command 'firefox' it works?!?!(firefox is displayed on local computer)</p>

<p>P.S. my local computer is Windows 7 so I have Xming running on background in order to allow X11 display. As for the attempt written on the upper part, the commands were typed in at Cygwin terminal. </p>
","<p>Make sure that you have the DISPLAY-variable set in your cygwin-environment:</p>

<pre><code>export DISPLAY=:0.0
</code></pre>

<p>after connecting with SSH, check if that shell also knows the correct DISPLAY-variable with:</p>

<pre><code>echo $DISPLAY
</code></pre>
","138941"
"How to transfer files from Windows to Ubuntu on Virtualbox?","140333","","<p>How should I transfer files from Windows to Ubuntu installed on Virtualbox? When I plugged in a USB, it only pops up in Windows.</p>

<p>How can I see it in Ubuntu?</p>
","<p>There are 2 ways, which I normally use</p>

<p>Option 1: 
Before booting up Ubuntu, inside Virtualbox Ubuntu VM settings, specify a share folder. Then after logged in to Ubuntu, create a new directory for example <code>/media/vboxshared</code> and mount that drive using the command <code>sudo mount -t vboxsf SHARENAME /media/vboxshared</code>. Enter your password when it prompts for the password.</p>

<p>Option 2: 
Before booting up Ubuntu, add a new Network adapter and select 'Bridged Adapter'. Then after logged in to Ubuntu, run the command <code>ifconfig -a | more</code> to get the ip address of that new network adapter. In Windows, use WinSCP or FileZilla to transfer the file to Ubuntu</p>
","16208"
"Display time stamp in dd/mm/yyyy_hh:mm:ss:ms in Unix or Linux","140180","","<p>I need help to display date and time in desired format in Unix/ Linux. My desired format is:
dd/mm/yyyy hh:mm:ss:ms in Unix or Linux. I got close using the following command:</p>

<pre><code>echo $(date +%x_%r)
</code></pre>

<p>This returns:</p>

<pre><code>08/20/2012_02:26:14 PM
</code></pre>

<p>Any suggestions?</p>
","<pre><code>date +%x_%H:%M:%S:%N
</code></pre>

<p>if you need to print only two first nums as ms:</p>

<pre><code>date +%x_%H:%M:%S:%N | sed 's/\(:[0-9][0-9]\)[0-9]*$/\1/'
</code></pre>

<p>to store it in the var:</p>

<pre><code>VAR=$(date +%x_%H:%M:%S:%N | sed 's/\(:[0-9][0-9]\)[0-9]*$/\1/')
</code></pre>
","45928"
"How can I grep the results of FIND using -EXEC and still output to a file?","139917","","<p>Better to explain on examples.</p>

<p>I can:</p>

<pre><code>find . -name ""*.py"" -type f &gt; output.txt
</code></pre>

<p>But how can I store the output to the same file for:</p>

<pre><code>find . -name ""*.py"" -type f -exec grep ""something"" {} \
</code></pre>

<p>I can't just do</p>

<pre><code>find . -name ""*.py"" -type f -exec grep ""something"" {} \ &gt; output.txt
</code></pre>
","<p>If I understand you correctly this is what you want to do:</p>

<pre><code>find . -name '*.py' -print0 | xargs -0 grep 'something' &gt; output.txt
</code></pre>

<p><code>Find</code> all files with extension <code>py</code>, <code>grep</code> only rows that contain <code>something</code> and save the rows in <code>output.txt</code>. If the file contains anything it will be replaced.</p>

<p>Edit:
Using -exec:</p>

<pre><code>find . -name '*.py' -exec grep 'something' {} \; &gt; output.txt
</code></pre>

<p>I'm incorporating Chris Downs comment here. The above command will result in <code>grep</code> being executed as many times as <code>find</code> finds occurences. However, if you replace the <code>;</code> with a <code>+</code>, <code>grep</code> is called with the output from <code>find</code> concatenated (up to a certain limit). See question <a href=""https://stackoverflow.com/questions/6085156/using-semicolon-vs-plus-with-exec-in-find"">Using semicolon (;) vs plus (+) with exec in find</a> for more on the subject.</p>
","21034"
"Extract only a specific file from a zipped archive to a given directory","139916","","<p>I need to extract a single file from a ZIP file which I know the path to. Is there a command like the following:</p>

<pre><code>unzip -d . myarchive.zip path/to/zipped/file.txt
</code></pre>

<p>Unfortunately, the above command extracts and recreates the entire path to the file at <code>./path/to/zipped/file.txt</code>. Is there a way for me to simply pull the file out into a specified directory?</p>
","<p>You can extract just the text to standard output with the <code>-p</code> option:</p>

<pre><code>unzip -p myarchive.zip path/to/zipped/file.txt &gt;file.txt
</code></pre>

<p>This won't extract the metadata (date, permissions, …), only the file contents. That's the price to pay for the convenience of not having to move the file afterwards.</p>

<p>Alternatively, mount the archive as a directory and just copy the file. With <a href=""http://avf.sourceforge.net/"">AVFS</a>:</p>

<pre><code>mountavfs
cp -p ~/.avfs""$PWD/myarchive.zip#""/path/to/zipped/file.txt .
</code></pre>

<p>Or with <a href=""http://code.google.com/p/fuse-zip"">fuse-zip</a>:</p>

<pre><code>mkdir myarchive.d
fuse-zip myarchive.zip myarchive.d
cp -p myarchive.d/path/to/zipped/file.txt .
fusermount -u myarchive.d; rmdir myarchive.d
</code></pre>
","14125"
"How to run grep on a single column?","139813","","<p>I want to grep the output of my <code>ls -l</code> command:</p>

<pre><code>-rw-r--r--   1 root root       1866 Feb 14 07:47 rahmu.file
-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file
-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file
</code></pre>

<p>I want to run <code>grep rahmu</code> on column $3 only, so the output of my <code>grep</code> command should look like this:</p>

<pre><code>-rw-r--r--   1 rahmu user     95653 Feb 14 07:47 foo.file
-rw-r--r--   1 rahmu user   1073822 Feb 14 21:01 bar.file
</code></pre>

<p>What's the simplest way to do it? The answer must be portable across many Unices, preferably focusing on Linux and Solaris.</p>

<p><em>NB: I'm not looking for a way to find all the files belonging to a given user. This example was only given to make my question clearer.</em></p>
","<p>One more time <code>awk</code> saves the day!</p>

<p>Here's a straightforward way to do it, with a relatively simple syntax:</p>

<pre><code>ls -l | awk '{if ($3 == ""rahmu"") print $0;}'
</code></pre>

<p>or even simpler: <em>(Thanks to Peter.O in the comments)</em></p>

<pre><code>ls -l | awk '$3 == ""rahmu""' 
</code></pre>
","31755"
"How can I execute local script on remote machine and include arguments?","139748","","<p>I have written a script that runs fine when executed locally:</p>

<pre><code>./sysMole -time Aug 18 18
</code></pre>

<p>The arguments <strong>""-time""</strong>, <strong>""Aug""</strong>, <strong>""18""</strong>, and <strong>""18""</strong> are successfully passed on to the script.</p>

<p>Now, this script is designed to be executed on a remote machine but, from a local directory on the local machine. Example:</p>

<pre><code>ssh root@remoteServer ""bash -s"" &lt; /var/www/html/ops1/sysMole
</code></pre>

<p>That also works fine. But the problem arises when I try to include those aforementioned arguments <strong>(-time Aug 18 18)</strong>, for example:</p>

<pre><code>ssh root@remoteServer ""bash -s"" &lt; /var/www/html/ops1/sysMole -time Aug 18 18
</code></pre>

<p>After running that script I get the following error:</p>

<pre><code>bash: cannot set terminal process group (-1): Invalid argument
bash: no job control in this shell
</code></pre>

<p>Please tell me what I'm doing wrong, this greatly frustrating.</p>
","<p>You were pretty close with your example. It works just fine when you use it with arguments such as these.</p>

<p><em><strong>Sample script:</em></strong></p>

<pre><code>$ more ex.bash 
#!/bin/bash

echo $1 $2
</code></pre>

<p><em><strong>Example that works:</em></strong></p>

<pre><code>$ ssh serverA ""bash -s"" &lt; ./ex.bash ""hi"" ""bye""
hi bye
</code></pre>

<p>But it fails for these types of arguments:</p>

<pre><code>$ ssh serverA ""bash -s"" &lt; ./ex.bash ""--time"" ""bye""
bash: --: invalid option
...
</code></pre>

<h3>What's going on?</h3>

<p>The problem you're encountering is that the argument, <code>-time</code>, or <code>--time</code> in my example, is being interpreted as a switch to <code>bash -s</code>. You can pacify <code>bash</code> by terminating it from taking any of the remaining command line arguments for itself using the <code>--</code> argument.</p>

<p>Like this:</p>

<pre><code>$ ssh root@remoteServer ""bash -s"" -- &lt; /var/www/html/ops1/sysMole -time Aug 18 18
</code></pre>

<h3>Examples</h3>

<p><em>#1:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash ""-time"" ""bye""
-time bye
</code></pre>

<p><em>#2:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash ""--time"" ""bye""
--time bye
</code></pre>

<p><em>#3:</em></p>

<pre><code>$ ssh serverA ""bash -s"" -- &lt; ./ex.bash --time ""bye""
--time bye
</code></pre>

<p><em>#4:</em></p>

<pre><code>$ ssh  &lt; ./ex.bash serverA ""bash -s -- --time bye""
--time bye
</code></pre>

<p><strong>NOTE:</strong> Just to make it clear that wherever the redirection appears on the command line makes no difference, because <code>ssh</code> calls a remote shell with the concatenation of its arguments anyway, quoting doesn't make much difference, except when you need quoting on the remote shell like in example <em>#4:</em></p>

<pre><code>$ ssh  &lt; ./ex.bash serverA ""bash -s -- '&lt;--time bye&gt;' '&lt;end&gt;'""
&lt;--time bye&gt; &lt;end&gt;
</code></pre>
","87406"
"Open port 80 in CentOS 6.5","139482","","<p>I'm trying to open the Port 80 in my CentOS 6.5, on my virtual machine, so I can access the apache from my desktop's browser.</p>

<p><img src=""https://i.stack.imgur.com/vw3zn.jpg"" alt=""enter image description here""></p>

<p>If you take a look at the screenshot above.... I've added the line before the blue arrow, as is written on <a href=""http://www.cyberciti.biz/faq/linux-iptables-firewall-open-port-80/"" rel=""noreferrer"">http://www.cyberciti.biz/faq/linux-iptables-firewall-open-port-80/</a>
Now I do get the apache test page when entering the IP-address in my browser, but still when restarting the iptables, I get a ""FAILED"" when CentOS tries to apply the new rule.</p>

<p>Does anyone know a solution for this? Or do I need to ignore the failure? </p>
","<p>Rather than key the rules in manually you can use <code>iptables</code> to add the rules to the appropriate chains and then save them. This will allow you to debug the rules live, confirming they're correct, rather than having to add them to the file like you appear to be doing.</p>

<p>To open port 80 I do this:</p>

<pre><code>$ sudo iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
$ sudo /etc/init.d/iptables save
</code></pre>

<p>The last command will save the added rules. This is the rule I would use to open up the port for web traffic.</p>

<h3>Why your rule is causing issues</h3>

<p>If you notice the rule you're attempting to use:</p>

<pre><code>-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
</code></pre>

<p>Has a chain called ""RH-Firewall-1-INPUT"". If you do not have this chain, or a link from the <code>INPUT</code> chain to this chain, then this rule will never be reachable. This rule could likely be like this:</p>

<pre><code>-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
</code></pre>

<p>Or your <code>INPUT</code> chain should link to this chain <code>RH-Firewall-1-INPUT</code> with a rule like this:</p>

<pre><code>$ sudo iptables --list
Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination
1    RH-Firewall-1-INPUT  all  --  0.0.0.0/0            0.0.0.0/0
....
</code></pre>

<p><strong>NOTE:</strong> You can see what chains you have with this command: </p>

<pre><code>$ sudo iptables -L| grep Chain
Chain INPUT (policy ACCEPT)
Chain FORWARD (policy ACCEPT)
Chain OUTPUT (policy ACCEPT)
...
</code></pre>

<p>Also the states might need to be modified so that existing connections are allowed as well.</p>

<pre><code>-A INPUT -m state --state NEW,ESTABLISHED -m tcp -p tcp --dport 80 -j ACCEPT
</code></pre>

<p>Also when you use the <code>-A</code> switch you're appending the rule to chain <code>INPUT</code>. If there are other rules before it that are blocking and/or interfering with the reaching of this rule, it will never get executed. So you might want to move it to the top by inserting rather than appending, like this:</p>

<pre><code>-I INPUT -m state --state NEW,ESTABLISHED -m tcp -p tcp --dport 80 -j ACCEPT
</code></pre>

<h3>Using the GUI</h3>

<p>Firewalls can be complicated beasts. So you might want to try the TUI instead (TUI's are GUI's for the terminal).</p>

<pre><code>$ sudo system-config-firewall-tui
</code></pre>

<p>You can then go through the various screens setting up <code>iptables</code> rules.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/657D0.png"" alt=""ss #1""></p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/Li3bq.png"" alt=""ss #2""></p>

<h3>References</h3>

<ul>
<li><a href=""http://www.thegeekstuff.com/2011/01/iptables-fundamentals/"" rel=""noreferrer"">Linux Firewall Tutorial: IPTables Tables, Chains, Rules Fundamentals</a></li>
</ul>
","109450"
"Why does my shell script choke on whitespace or other special characters?","139392","","<p>Or, an introductory guide to robust filename handling and other string passing in shell scripts.</p>

<p>I wrote a shell script which works well most of the time. But it chokes on some inputs (e.g. on some file names).</p>

<p>I encountered a problem such as the following:</p>

<ul>
<li>I have a file name containing a space <code>hello world</code>, and it was treated as two separate files <code>hello</code> and <code>world</code>.</li>
<li>I have an input line with two consecutive spaces and they shrank to one in the input.</li>
<li>Leading and trailing whitespace disappears from input lines.</li>
<li>Sometimes, when the input contains one of the characters <code>\[*?</code>, they are
replaced by some text which is is actually the name of files.</li>
<li>There is an apostrophe <code>'</code> (or a double quote <code>""</code>) in the input and things got weird after that point.</li>
<li>There is a backslash in the input (or: I am using Cygwin and some of my file names have Windows-style <code>\</code> separators).</li>
</ul>

<p>What is going on and how do I fix it?</p>
","<h2>Always use double quotes around variable substitutions and command substitutions: <code>""$foo""</code>, <code>""$(foo)""</code></h2>

<p>If you use <code>$foo</code> unquoted, your script will choke on input or parameters (or command output, with <code>$(foo)</code>) containing whitespace or <code>\[*?</code>.</p>

<p>There, you can stop reading. Well, ok, here are a few more:</p>

<ul>
<li><code>read</code> — <strong>To read input line by line with the <code>read</code> builtin, use <code>while IFS= read -r line; do …</code></strong><br>
Plain <code>read</code> treats backslashes and whitespace specially.</li>
<li><code>xargs</code> — <strong>Avoid <code>xargs</code></strong>. If you must use <code>xargs</code>, make that <code>xargs -0</code>. Instead of <code>find … | xargs</code>, <strong>prefer <code>find … -exec …</code></strong>.<br>
<code>xargs</code> treats whitespace and the characters <code>\""'</code> specially.</li>
</ul>

<p>This answer applies to Bourne/POSIX-style shells (<code>sh</code>, <code>ash</code>, <code>dash</code>, <code>bash</code>, <code>ksh</code>, <code>mksh</code>, <code>yash</code>…). Zsh users should skip it and read the end of <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary/68748#68748"">When is double-quoting necessary?</a> instead. If you want the whole nitty-gritty, <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_06"" rel=""noreferrer"">read the standard</a> or your shell's manual.</p>

<hr>

<p>Note that the explanations below contains a few approximations (statements that are true in most conditions but can be affected by the surrounding context or by configuration).</p>

<h2>Why do I need to write <code>""$foo""</code>? What happens without the quotes?</h2>

<p><code>$foo</code> does not mean “take the value of the variable <code>foo</code>”. It means something much more complex:</p>

<ul>
<li>First, take the value of the variable.</li>
<li>Field splitting: treat that value as a whitespace-separated list of fields, and build the resulting list. For example, if the variable contains <code>foo *  bar ​</code> then the result of this step is the 3-element list <code>foo</code>, <code>*</code>, <code>bar</code>.</li>
<li>Filename generation: treat each field as a glob, i.e. as a wildcard pattern, and replace it by the list of file names that match this pattern. If the pattern doesn't match any files, it is left unmodified. In our example, this results in the list containing <code>foo</code>, following by the list of files in the current directory, and finally <code>bar</code>. If the current directory is empty, the result is <code>foo</code>, <code>*</code>, <code>bar</code>.</li>
</ul>

<p>Note that the result is a list of strings. There are two contexts in shell syntax: list context and string context. Field splitting and filename generation only happen in list context, but that's most of the time. Double quotes delimit a string context: the whole double-quoted string is a single string, not to be split. (Exception: <code>""$@""</code> to expand to the list of positional parameters, e.g. <code>""$@""</code> is equivalent to <code>""$1"" ""$2"" ""$3""</code> if there are three positional parameters. See <a href=""https://unix.stackexchange.com/questions/41571/what-is-the-difference-between-and/94200#94200"">What is the difference between $* and $@?</a>)</p>

<p>The same happens to command substitution with <code>$(foo)</code> or with <code>`foo`</code>. On a side note, don't use <code>`foo`</code>: its quoting rules are weird and non-portable, and all modern shells support <code>$(foo)</code> which is absolutely equivalent except for having intuitive quoting rules.</p>

<p>The output of arithmetic substitution also undergoes the same expansions, but that isn't normally a concern as it only contains non-expandable characters (assuming <code>IFS</code> doesn't contain digits or <code>-</code>).</p>

<p>See <a href=""https://unix.stackexchange.com/questions/68694/when-is-double-quoting-necessary"">When is double-quoting necessary?</a> for more details about the cases when you can leave out the quotes.</p>

<p>Unless you mean for all this rigmarole to happen, just remember to always use double quotes around variable and command substitutions. Do take care: leaving out the quotes can lead not just to errors but to <a href=""https://unix.stackexchange.com/questions/171346/security-implications-of-forgetting-to-quote-a-variable-in-bash-posix-shells/171347#171347"">security holes</a>.</p>

<h3>How do I process a list of file names?</h3>

<p>If you write <code>myfiles=""file1 file2""</code>, with spaces to separate the files, this can't work with file names containing spaces. Unix file names can contain any character other than <code>/</code> (which is always a directory separator) and null bytes (which you can't use in shell scripts with most shells).</p>

<p>Same problem with <code>myfiles=*.txt; … process $myfiles</code>. When you do this, the variable <code>myfiles</code> contains the 5-character string <code>*.txt</code>, and it's when you write <code>$myfiles</code> that the wildcard is expanded. This example will actually work, until you change your script to be <code>myfiles=""$someprefix*.txt""; … process $myfiles</code>. If <code>someprefix</code> is set to <code>final report</code>, this won't work.</p>

<p>To process a list of any kind (such as file names), put it in an array. This requires mksh, ksh93, yash or bash (or zsh, which doesn't have all these quoting issues); a plain POSIX shell (such as ash or dash) doesn't have array variables.</p>

<pre><code>myfiles=(""$someprefix""*.txt)
process ""${myfiles[@]}""
</code></pre>

<p>Ksh88 has array variables with a different assignment syntax <code>set -A myfiles ""someprefix""*.txt</code> (see <a href=""https://stackoverflow.com/questions/22862038/assignation-variable-under-different-ksh-environment"">assignation variable under different ksh environment</a> if you need ksh88/bash portability). Bourne/POSIX-style shells have a single one array, the array of positional parameters <code>""$@""</code> which you set with <code>set</code> and which is local to a function:</p>

<pre><code>set -- ""$someprefix""*.txt
process -- ""$@""
</code></pre>

<h3>What about file names that begin with <code>-</code>?</h3>

<p>On a related note, keep in mind that file names can begin with a <code>-</code> (dash/minus), which most commands interpret as denoting an option. If you have a file name that begins with a variable part, be sure to pass <code>--</code> before it, as in the snippet above. This indicates to the command that it has reached the end of options, so anything after that is a file name even if it starts with <code>-</code>.</p>

<p>Alternatively, you can make sure that your file names begin with a character other than <code>-</code>. Absolute file names begin with <code>/</code>, and you can add <code>./</code> at the beginning of relative names. The following snippet turns the content of the variable <code>f</code> into a “safe” way of refering to the same file that's guaranteed not to start with <code>-</code>.</p>

<pre><code>case ""$f"" in -*) ""f=./$f"";; esac
</code></pre>

<p>On a final note on this topic, beware that some commands interpret <code>-</code> as meaning standard input or standard output, even after <code>--</code>. If you need to refer to an actual file named <code>-</code>, or if you're calling such a program and you don't want it to read from stdin or write to stdout, make sure to rewrite <code>-</code> as above. See <a href=""https://unix.stackexchange.com/questions/110750/what-is-the-difference-between-du-sh-and-du-sh/110756#110756"">What is the difference between &quot;du -sh *&quot; and &quot;du -sh ./*&quot;?</a> for further discussion.</p>

<h2>How do I store a command in a variable?</h2>

<p>“Command” can mean three things: a command name (the name as an executable, with or without full path, or the name of a function, builtin or alias), a command name with arguments, or a piece of shell code. There are accordingly different ways of storing them in a variable.</p>

<p>If you have a command name, just store it and use the variable with double quotes as usual.</p>

<pre><code>command_path=""$1""
…
""$command_path"" --option --message=""hello world""
</code></pre>

<p>If you have a command with arguments, the problem is the same as with a list of file names above: this is a list of strings, not a string. You can't just stuff the arguments into a single string with spaces in between, because if you do that you can't tell the difference between spaces that are part of arguments and spaces that separate arguments. If your shell has arrays, you can use them.</p>

<pre><code>cmd=(/path/to/executable --option --message=""hello world"" --)
cmd=(""${cmd[@]}"" ""$file1"" ""$file2"")
""${cmd[@]}""
</code></pre>

<p>What if you're using a shell without arrays? You can still use the positional parameters, if you don't mind modifying them.</p>

<pre><code>set -- /path/to/executable --option --message=""hello world"" --
set -- ""$@"" ""$file1"" ""$file2""
""$@""
</code></pre>

<p>What if you need to store a complex shell command, e.g. with redirections, pipes, etc.? Or if you don't want to modify the positional parameters? Then you can build a string containing the command, and use the <code>eval</code> builtin.</p>

<pre><code>code='/path/to/executable --option --message=""hello world"" -- /path/to/file1 | grep ""interesting stuff""'
eval ""$code""
</code></pre>

<p>Note the nested quotes in the definition of <code>code</code>: the single quotes <code>'…'</code> delimit a string literal, so that the value of the variable <code>code</code> is the string <code>/path/to/executable --option --message=""hello world"" -- /path/to/file1</code>. The <code>eval</code> builtin tells the shell to parse the string passed as an argument as if it appeared in the script, so at that point the quotes and pipe are parsed, etc.</p>

<p>Using <code>eval</code> is tricky. Think carefully about what gets parsed when. In particular, you can't just stuff a file name into the code: you need to quote it, just like you would if it was in a source code file. There's no direct way to do that. Something like <code>code=""$code $filename""</code> breaks if the file name contains any shell special character (spaces, <code>$</code>, <code>;</code>, <code>|</code>, <code>&lt;</code>, <code>&gt;</code>, etc.). <code>code=""$code \""$filename\""""</code> still breaks on <code>""$\`</code>. Even <code>code=""$code '$filename'""</code> breaks if the file name contains a <code>'</code>. There are two solutions.</p>

<ul>
<li><p>Add a layer of quotes around the file name. The easiest way to do that is to add single quotes around it, and replace single quotes by <code>'\''</code>.</p>

<pre><code>quoted_filename=$(printf %s. ""$filename"" | sed ""s/'/'\\\\''/g"")
code=""$code '${quoted_filename%.}'""
</code></pre></li>
<li><p>Keep the variable expansion inside the code, so that it's looked up when the code is evaluated, not when the code fragment is built. This is simpler but only works if the variable is still around with the same value at the time the code is executed, not e.g. if the code is built in a loop.</p>

<pre><code>code=""$code \""\$filename\""""
</code></pre></li>
</ul>

<p>Finally, do you really need a variable containing code? The most natural way to give a name to a code block is to define a function.</p>

<h2>What's up with <code>read</code>?</h2>

<p>Without <code>-r</code>, <code>read</code> allows continuation lines — this is a single logical line of input:</p>

<pre><code>hello \
world
</code></pre>

<p><code>read</code> splits the input line into fields delimited by characters in <code>$IFS</code> (without <code>-r</code>, backslash also escapes those). For example, if the input is a line containing three words, then <code>read first second third</code> sets <code>first</code> to the first word of input, <code>second</code> to the second word and <code>third</code> to the third word. If there are more words, the last variable contains everything that's left after setting the preceding ones. Leading and trailing whitespace are trimmed.</p>

<p>Setting <code>IFS</code> to the empty string avoids any trimming. See <a href=""https://unix.stackexchange.com/questions/18886/why-is-while-ifs-read-used-so-often-instead-of-ifs-while-read/18936#18936"">Why is `while IFS= read` used so often, instead of `IFS=; while read..`?</a> for a longer explanation.</p>

<h2>What's wrong with <code>xargs</code>?</h2>

<p>The input format of <code>xargs</code> is whitespace-separated strings which can optionally be single- or double-quoted. No standard tool outputs this format.</p>

<p>The input to <code>xargs -L1</code> or <code>xargs -l</code> is almost a list of lines, but not quite — if there is a space at the end of a line, the following line is a continuation line.</p>

<p>You can use <code>xargs -0</code> where applicable (and where available: GNU (Linux, Cygwin), BusyBox, BSD, OSX, but it isn't in POSIX). That's safe, because null bytes can't appear in most data, in particular in file names. To produce a null-separated list of file names, use <code>find … -print0</code> (or you can use <code>find … -exec …</code> as explained below).</p>

<h3>How do I process files found by <code>find</code>?</h3>

<pre><code>find … -exec some_command a_parameter another_parameter {} +
</code></pre>

<p><code>some_command</code> needs to be an external command, it can't be a shell function or alias. If you need to invoke a shell to process the files, call <code>sh</code> explicitly.</p>

<pre><code>find … -exec sh -c '
  for x do
    … # process the file ""$x""
  done
' find-sh {} +
</code></pre>

<h2>I have some other question</h2>

<p>Browse the <a href=""/questions/tagged/quoting"" class=""post-tag"" title=""show questions tagged &#39;quoting&#39;"" rel=""tag"">quoting</a> tag on this site, or <a href=""/questions/tagged/shell"" class=""post-tag"" title=""show questions tagged &#39;shell&#39;"" rel=""tag"">shell</a> or <a href=""/questions/tagged/shell-script"" class=""post-tag"" title=""show questions tagged &#39;shell-script&#39;"" rel=""tag"">shell-script</a>. (Click on “learn more…” to see some general tips and a hand-selected list of common questions.) If you've searched and you can't find an answer, <a href=""https://unix.stackexchange.com/questions/ask"">ask away</a>.</p>
","131767"
"How can I read line by line from a variable in bash?","139305","","<p>I have a variable which contains multiline output of a command. What's the most effecient way to read the output line by line from the variable?</p>

<p>For example:</p>

<pre><code>jobs=""$(jobs)""
if [ ""$jobs"" ]; then
    # read lines from $jobs
fi
</code></pre>
","<p>You can use a while loop with process substitution:</p>

<pre><code>while read -r line
do
    echo ""$line""
done &lt; &lt;(jobs)
</code></pre>

<p>To read a multiline variable, a simple way is:</p>

<pre><code># You need printf '%s\n' ""$var"" here because if you use printf '%s' ""$var"" 
# on a variable that doesn't end with a newline then the while loop will
# completely miss the last line of the variable.
printf '%s\n' ""$var"" | while IFS= read -r line
do
   echo ""$line""
done
</code></pre>

<p>Also, please don't call your variable <code>jobs</code> because that is a shell command and may cause confusion.</p>
","9789"
"How to know if a disk is an SSD or an HDD","138755","","<p>I want to know whether a disk is a solid-state drive or hard disk.</p>

<p><code>lshw</code> is not installed. I do <code>yum install lshw</code> and it says there is no package named lshw. I do not know which version of <a href=""http://pkgs.repoforge.org/lshw/"">http://pkgs.repoforge.org/lshw/</a> is suitable for my CentOS.</p>

<p>I search the net and there is nothing that explain how to know whether a drive is SSD or HDD. Should I just format them first?</p>

<p>Result of <code>fdisk -l</code>:</p>

<pre><code>Disk /dev/sda: 120.0 GB, 120034123776 bytes
255 heads, 63 sectors/track, 14593 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00074f7d

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1          14      103424   83  Linux
Partition 1 does not end on cylinder boundary.
/dev/sda2              14         536     4194304   82  Linux swap / Solaris
Partition 2 does not end on cylinder boundary.
/dev/sda3             536       14594   112921600   83  Linux

Disk /dev/sdc: 120.0 GB, 120034123776 bytes
255 heads, 63 sectors/track, 14593 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/sdb: 128.0 GB, 128035676160 bytes
255 heads, 63 sectors/track, 15566 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/sdd: 480.1 GB, 480103981056 bytes
255 heads, 63 sectors/track, 58369 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000
</code></pre>
","<p>Linux automatically detects SSD, and <a href=""http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=1308835ffffe6d61ad1f48c5c381c9cc47f683ec"" rel=""noreferrer"">since</a> kernel version 2.6.29, you may verify <code>sda</code> with: </p>

<pre><code>cat /sys/block/sda/queue/rotational
</code></pre>

<p>You should get <code>1</code> for hard disks and <code>0</code> for a SSD. </p>

<p>It will probably not work if your disk is a logical device emulated by hardware (like a RAID controller).</p>

<p>See <a href=""https://superuser.com/questions/228657/which-linux-filesystem-works-best-with-ssd#answer-550308"">this answer</a> for more information... </p>
","65602"
"What is the difference between /opt and /usr/local?","138716","","<p>According to the <a href=""http://www.pathname.com/fhs/pub/fhs-2.3.html"">Filesystem Hierarchy Standard</a>, <code>/opt</code> is for ""the installation of add-on application software packages"".  <code>/usr/local</code> is ""for use by the system administrator when installing software locally"".  These use cases seem pretty similar.  Software not included with distributions usually is configured by default to install in either <code>/usr/local</code> or <code>/opt</code> with no particular rhyme or reason as to which they chose.</p>

<p>Is there some difference I'm missing, or do both do the same thing, but exist for historical reasons?</p>
","<p>While both are designed to contain files not belonging to the operating system, <code>/opt</code> and <code>/usr/local</code> are not intended to contain the same set of files.</p>

<p><code>/usr/local</code> is a place to install files built by the administrator, typically by using the <code>make</code> command (e.g., <code>./configure; make; make install</code>). The idea is to avoid clashes with files that are part of the operating system, which would either be overwritten or overwrite the local ones otherwise (e.g., <code>/usr/bin/foo</code> is part of the OS while <code>/usr/local/bin/foo</code> is a local alternative).</p>

<p>All files under <code>/usr</code> are shareable between OS instances, although this is rarely done with Linux. This is a part where the FHS is slightly self-contradictory, as <code>/usr</code> is defined to be read-only, but <code>/usr/local/bin</code> needs to be read-write for local installation of software to succeed. The SVR4 file system standard, which was the FHS' main source of inspiration, is recommending to avoid <code>/usr/local</code> and use <code>/opt/local</code> instead to overcome this issue.</p>

<p><code>/usr/local</code> is a legacy from the original BSD. At that time, the source code of <code>/usr/bin</code> OS commands were in <code>/usr/src/bin</code> and <code>/usr/src/usr.bin</code>, while the source of locally developed commands was in <code>/usr/local/src</code>, and their binaries in <code>/usr/local/bin</code>. There was no notion of packaging (outside tarballs).</p>

<p>On the other hand, <code>/opt</code> is a directory for installing unbundled packages, each one in its own subdirectory. They are already built whole packages provided by an independent third party software distributor. Unlike <code>/usr/local</code> stuff, these packages follow the directory conventions (or at least they should). For example, <code>someapp</code> would be installed in <code>/opt/someapp</code>, with one of its command being <code>/opt/someapp/bin/foo</code>, its configuration file would be in <code>/etc/opt/someapp/foo.conf</code>, and its log files in <code>/var/opt/someapp/logs/foo.access</code>.</p>
","11552"
"tar: Removing leading `/' from member names","138302","","<pre><code>root@server # tar fcz bkup.tar.gz /home/foo/
tar: Removing leading `/' from member names
</code></pre>

<p>How can I solve this problem and keep the <code>/</code> on file names ?</p>
","<p>Use the <code>--absolute-names</code> or <code>-P</code> option to disable this feature.</p>
","59244"
"How to create custom commands in Unix/Linux?","137382","","<p>I'm a newbie in Unix/Linux environment. Can anyone guide me to create custom commands in Unix/Linux. Thanks in advance </p>
","<p>Create a bash script in your /usr/bin folder, it should look something like this</p>

<pre><code>#!/bin/bash
Whatever combination of commands you want to run when you type this thing.
</code></pre>

<p>Its really that easy.</p>

<p>Just name the bash script what you want to type in to the terminal, and make it excecutable: <code>chmod +x filename</code> and you're good to go!</p>
","84688"
"How to extract specific file(s) from tar.gz","137354","","<p>How can we extract specific files from a large tar.gz file? I found <a href=""https://unix.stackexchange.com/questions/42198/untar-only-a-certain-number-of-files-from-a-large-tarball/42199#42199"">the process of extracting files from a tar in this question</a> but, when I tried the mentioned command there, I got the error:</p>

<pre><code>$ tar --extract --file={test.tar.gz} {extract11}
tar: {test.tar.gz}: Cannot open: No such file or directory
tar: Error is not recoverable: exiting now
</code></pre>

<p>How do I then extract a file from <code>tar.gz</code>?</p>
","<p>you can also use <code>tar -zxvf &lt;tar filename&gt; &lt;file you want to extract&gt;</code></p>

<pre><code>-x: instructs tar to extract files.
-f: specifies filename / tarball name.
-v: Verbose (show progress while extracting files).
-z: filter archive through gzip, use to decompress .gz files.
</code></pre>
","61464"
"su options - running command as another user","137211","","<p>I was wondering how to run a command as another user from a script.</p>

<p>I have the script's owner set as root. I also have the following command being run within the script to run the command as the hudson user:</p>

<pre><code>su -c command hudson
</code></pre>

<p>Is this the correct syntax?</p>
","<p>Yes. Here's the <code>--help</code>:</p>

<pre><code>$ su --help
Usage: su [options] [LOGIN]

Options:
  -c, --command COMMAND         pass COMMAND to the invoked shell
  -h, --help                    display this help message and exit
  -, -l, --login                make the shell a login shell
  -m, -p,
  --preserve-environment        do not reset environment variables, and
                                keep the same shell
  -s, --shell SHELL             use SHELL instead of the default in passwd
</code></pre>

<p>And some testing (I used <code>sudo</code> as I don't know the password for the <code>nobody</code> account)</p>

<pre><code>$ sudo su -c whoami nobody
[sudo] password for oli: 
nobody
</code></pre>

<p>When your command takes arguments you need to quote it. If you don't, strange things will occur. Here I am —as root— trying to create a directory in /home/oli (as oli) <em>without</em> quoting the full command:</p>

<pre><code># su -c mkdir /home/oli/java oli
No passwd entry for user '/home/oli/java'
</code></pre>

<p>It's only read <code>mkdir</code> as the value for the <code>-c</code> flag and it's trying to use <code>/home/oli/java</code> as the username. If we quote it, it just works:</p>

<pre><code># su -c ""mkdir /home/oli/java"" oli
# stat /home/oli/java
  File: ‘/home/oli/java’
  Size: 4096        Blocks: 8          IO Block: 4096   directory
Device: 811h/2065d  Inode: 5817025     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     oli)   Gid: ( 1000/     oli)
Access: 2016-02-16 10:49:15.467375905 +0000
Modify: 2016-02-16 10:49:15.467375905 +0000
Change: 2016-02-16 10:49:15.467375905 +0000
 Birth: -
</code></pre>
","1088"
"Determine the size of a block device","137056","","<p>How can I find out the size of a block device, such as <code>/dev/sda</code>? Running <code>ls -l</code> gives no useful information.</p>
","<p><code>fdisk</code> doesn't understand the partition layout used by my Mac running Linux, nor any other non-PC partition format.  (Yes, there's <code>mac-fdisk</code> for old Mac partition tables, and <code>gdisk</code> for newer GPT partition table, but those aren't the only other partition layouts out there.)</p>

<p>Since the kernel already scanned the partition layouts when the block device came into service, why not ask it directly?</p>

<pre>
$ cat /proc/partitions
major minor  #blocks  name

   8       16  390711384 sdb
   8       17     514079 sdb1
   8       18  390194752 sdb2
   8       32  976762584 sdc
   8       33     514079 sdc1
   8       34  976245952 sdc2
   8        0  156290904 sda
   8        1     514079 sda1
   8        2  155774272 sda2
   8       48 1465138584 sdd
   8       49     514079 sdd1
   8       50 1464621952 sdd2
</pre>
","52218"
"Quickly calculate date differences","136869","","<p>I often want to make some quick date calculations, such as:</p>

<ul>
<li>What is the difference between these two dates?</li>
<li>What is the date <em>n</em> weeks after this other date?</li>
</ul>

<p>I usually open a calendar and count the days, but I think there should be a program/script that I can use to do these kinds of calculations. Any suggestions?</p>
","<p>The ""n weeks after a date"" is easy with GNU date(1):</p>

<pre><code>$ date -d 'now + 3 weeks'
Tue Dec  6 23:58:04 EST 2011
$ date -d 'Aug 4 + 3 weeks'
Thu Aug 25 00:00:00 EST 2011
$ date -d 'Jan 1 1982 + 11 weeks'
Fri Mar 19 00:00:00 EST 1982
</code></pre>

<p>I don't know of a simple way to calculate the difference between two dates, but you can wrap a little logic around date(1) with a shell function.</p>

<pre><code>datediff() {
    d1=$(date -d ""$1"" +%s)
    d2=$(date -d ""$2"" +%s)
    echo $(( (d1 - d2) / 86400 )) days
}
$ datediff '1 Nov' '1 Aug'
91 days
</code></pre>

<p>Swap <code>d1</code> and <code>d2</code> if you want the date calculation the other way, or get a bit fancier to make it not matter. Furthermore, in case there is a non-DST to <a href=""http://en.wikipedia.org/wiki/Daylight_saving_time"">DST</a> transition in the interval, one of the days will be only 23 hours long; you can compensate by adding ½ day to the sum.</p>

<pre><code>echo $(( (((d1-d2) &gt; 0 ? (d1-d2) : (d2-d1)) + 43200) / 86400 )) days
</code></pre>
","24636"
"Usage of dash (-) in place of a filename","136818","","<p>For a command, if using <code>-</code> as an argument in place of a file name will mean STDIN or STDOUT.</p>

<ol>
<li><p>But in this example, it creates a file with the name <code>-</code>:</p>

<pre><code>echo hello &gt; -
</code></pre>

<p>How can I make <code>-</code> in this example mean STDOUT?</p></li>
<li><p>Conversely, how can I make <code>-</code> mean a file named <code>-</code> in examples such as:</p>

<pre><code>cat -
</code></pre></li>
</ol>
","<p>Using <code>-</code> as a filename to mean stdin/stdout is a convention that a lot of programs use. It is not a special property of the filename. The kernel does not recognise <code>-</code> as special so any system calls referring to <code>-</code> as a filename will use <code>-</code> literally as the filename.</p>

<p>With bash redirection, <code>-</code> is not recognised as a special filename, so bash will use that as the literal filename.</p>

<p>When <code>cat</code> sees the string <code>-</code> as a filename, it treats it as a synonym for stdin. To get around this, you need to alter the string that <code>cat</code> sees in such a way that it still refers to a file called <code>-</code>. The usual way of doing this is to prefix the filename with a path - <code>./-</code>, or <code>/home/Tim/-</code>. This technique is also used to get around similar issues where command line options clash with filenames, so a file referred to as <code>./-e</code> does not appear as the <code>-e</code> command line option to a program, for example.</p>
","16364"
"Set volume from terminal","136626","","<p>Is it possible to set the audio volume using the terminal?, instead of clicking the speaker icon in the topbar, the reason I want need to do this is because my keyboard does not have a volume increase/decrease buttons and I find it annoying to reach for the mouse.</p>
","<p>For interactive usage you can use <code>alsamixer</code>. For scripting (e.g. binding to key combinations) take a look at <code>amixer</code>.</p>

<p><code>alsamixer</code> is included by default in most systems.</p>

<hr>

<p>To set the master volume use:</p>

<pre><code># Gets a list of simple mixer controls
$ amixer scontrols 
</code></pre>

<p>Then set it to the desired volume, as an example</p>

<pre><code>$ amixer sset 'Master' 50%
</code></pre>
","32207"
"How to redirect output to a file from within cron?","136309","","<p>I have a backup script which I need to run at a particular time of a day so I am using <code>cron</code> for this task and from within cron am also trying to redirect the output of backup script to a <code>logfile</code>.</p>

<p><code>crontab -e</code></p>

<pre><code>*/1 * * * * /home/ranveer/backup.sh &amp;&gt;&gt; /home/ranveer/backup.log
</code></pre>

<p>In the above cron entry I am redirecting both <code>stderr and stdout</code> to a log file.</p>

<p>The above cron job executes fine according to <code>syslog</code>  and it performs the task mentioned in the <code>backup.sh</code> file but it doesn't write anything to the log file.</p>

<p><code>/var/log/syslog</code></p>

<pre><code>Oct 19 20:26:01 ranveer CRON[15214]: (ranveer) CMD (/home/ranveer/backup.sh &amp;&gt;&gt; /home/ranveer/backup.log)
</code></pre>

<p>When I run the script from cli it works as required and output is written to a log file</p>

<pre><code>ranveer@ranveer:~$ ./backup.sh &amp;&gt;&gt; backup.log 
ranveer@ranveer:~$ cat backup.log
Fri Oct 19 20:28:01 IST 2012
successfully copied testdir
test.txt successfully copied
-------------------------------------------------------------------------------------
ranveer@ranveer:~$ 
</code></pre>

<p>So, why the output of file is not getting redirected to the file from within cron.</p>
","<p>I solved the problem. There are two ways:</p>

<p><strong>M1</strong></p>

<p>Change the redirection from <code>&amp;&gt;&gt;</code> to <code>2&gt;&amp;1</code>. So now <code>crontab -e</code> looks like</p>

<pre><code>*/1 * * * * /home/ranveer/vimbackup.sh &gt;&gt; /home/ranveer/vimbackup.log 2&gt;&amp;1
</code></pre>

<p><strong>I believe</strong> the above works because by default <code>cron</code> is using <code>sh</code> to run the task instead of <code>bash</code> so <code>&amp;&gt;&gt;</code> is not supported by <code>sh</code>.</p>

<p><strong>M2</strong></p>

<p>Change the default shell by adding <code>SHELL=/bin/bash</code> in the <code>crontab -e</code> file.</p>
","52332"
"How do you extract a single folder from a large tar.gz archive?","135688","","<p>I am using this command on a <strong>5GB</strong> archive</p>

<pre><code>tar -zxvf archive.tar.gz /folder/in/archive
</code></pre>

<p>is this the correct way to do this?  It seems to be taking forever with no command line output...</p>
","<p><code>tar</code> stores relative paths <em>by default</em>. GNU tar even says so if you try to store an absolute path:</p>

<pre><code>tar -cf foo.tar /home/foo
tar: Removing leading `/' from member names
</code></pre>

<p>If you need to extract a particular folder, have a look at what's in the tar file:</p>

<pre><code>tar -tvf foo.tar
</code></pre>

<p>And note the exact filename. In the case of my <code>foo.tar</code> file, I could extract <code>/home/foo/bar</code> by saying:</p>

<pre><code>tar -xvf foo.tar home/foo/bar # Note: no leading slash
</code></pre>

<p>So no, the way you posted isn't (necessarily) the correct way to do it. You have to leave out the leading slash. If you want to simulate absolute paths, do <code>cd /</code> first and make sure you're the superuser. Also, this does the same:</p>

<pre><code>tar -C / -xvf foo.tar home/foo/bar # -C is the ‘change directory’ option
</code></pre>

<p>There are very obvious, good reasons why <code>tar</code> converts paths to relative ones. One is the ability to restore an archive in places other than its original source. The other is security. You could extract an archive, expect its files to appear in your current working directory, and instead overwrite system files (or your own work) elsewhere by mistake.</p>

<p>Note: if you use the <code>-P</code> option, <code>tar</code> <strong>will</strong> archive absolute paths. So it always pays to check the contents of big archives before extracting.</p>
","35315"
"How to get permission number by string : -rw-r--r--","135620","","<p>I need to set the same chmod, how to get number for <strong>-rw-r--r--</strong> ?</p>
","<p>Please check <code>stat</code> output:</p>

<pre><code># stat .xsession-errors 
  File: ‘.xsession-errors’
  Size: 839123          Blocks: 1648       IO Block: 4096   regular file
Device: 816h/2070d      Inode: 3539028     Links: 1
Access: (0600/-rw-------)  Uid: ( 1000/     lik)   Gid: ( 1000/     lik)
Access: 2012-05-30 23:11:48.053999289 +0300
Modify: 2012-05-31 07:53:26.912690288 +0300
Change: 2012-05-31 07:53:26.912690288 +0300
 Birth: -
</code></pre>
","39711"
"What's the difference between /sbin/nologin and /bin/false","135557","","<p>Technically, unless <code>pam</code> is set up to check your shell with <code>pam_shells</code> neither of these can actually prevent your login, if you're not on the shell. On my system they are even different sizes, so I suspect they actually do something. So what's the difference? why do they both exist? Why would I use one over the other?</p>

<pre><code>-rwxr-xr-x 1 root root  21K Feb  4 17:01 /bin/false
-rwxr-xr-x 1 root root 4.7K Mar  2 14:59 /sbin/nologin
</code></pre>
","<p>When <code>/sbin/nologin</code> is set as the shell, if user with that shell logs in, they'll get a polite message saying 'This account is currently not available.' This message can be changed with the file <code>/etc/nologin.txt</code>.</p>

<p><code>/bin/false</code> is just a binary that immediately exits, returning false, when it's called, so when someone who has <code>false</code> as shell logs in, they're immediately logged out when <code>false</code> exits. Setting the shell to <code>/bin/true</code> has the same effect of not allowing someone to log in but <code>false</code> is probably used as a convention over <code>true</code> since it's much better at conveying the concept that person doesn't have a shell.</p>

<p>Looking at <code>nologin</code>'s man page, it says it was created in 4.4 BSD (early 1990s) so it came long after <code>false</code> was created. The use of <code>false</code> as a shell is probably just a convention carried over from the early days of UNIX.</p>

<p><code>nologin</code> is the more user-friendly option, with a customizable message given to the user trying to log in, so you would theoretically want to use that; but both <code>nologin</code> and <code>false</code> will have the same end result of someone not having a shell and not being able to ssh in.</p>
","10867"
"CentOS - install using yum Apache 2.4","135270","","<p>I need Apache 2.4 in CentOS because with Apache 2.2 <code>mod_proxy_wstunnel</code> is not possible. I have tried <a href=""https://fedorahosted.org/SoftwareCollections/"">this</a>, but unfortunately its for RHEL. After installing, I can't execute the Apache 2.4.</p>

<p>Can anyone show me how to install a complete working Apache 2.4 in CentOS, please?</p>

<p><strong>EDIT:</strong></p>

<pre><code># yum list installed | grep httpd
Failed to set locale, defaulting to C
httpd.x86_64                         2.2.15-30.el6.centos       @updates        
httpd-tools.x86_64                   2.2.15-30.el6.centos       @updates        
httpd24.x86_64                       1-6.el6                    @epel-httpd24   
httpd24-apr.x86_64                   1.4.8-2.el6                @epel-httpd24   
httpd24-apr-util.x86_64              1.5.2-5.el6                @epel-httpd24   
httpd24-httpd.x86_64                 2.4.6-5.el6                @epel-httpd24   
httpd24-httpd-tools.x86_64           2.4.6-5.el6                @epel-httpd24   
httpd24-mod_ssl.x86_64               1:2.4.6-5.el6              @epel-httpd24   
httpd24-runtime.x86_64               1-6.el6                    @epel-httpd24   
</code></pre>
","<p>Apache 2.4 on CentOS:</p>

<p>Step 1:</p>

<pre><code>cd /etc/yum.repos.d/
wget http://repos.fedorapeople.org/repos/jkaluza/httpd24/epel-httpd24.repo
</code></pre>

<p>Step 2:</p>

<pre><code>yum install httpd24.x86_64
</code></pre>

<p>Step 3:</p>

<pre><code>$ /opt/rh/httpd24/root/usr/sbin/httpd -version
Server version: Apache/2.4.6 (Red Hat)
Server built:   Sep 25 2013 05:25:46
</code></pre>

<p>NOTE: config files are in: <code>/opt/rh/httpd24/root/etc/httpd</code></p>

<pre><code>$ ls
conf  conf.d  conf.modules.d  logs  modules  run
</code></pre>

<p><strong>EDIT:</strong> in case you want to switch off Apache 2.2 </p>

<pre><code>$ chkconfig httpd off
$ chkconfig --list | grep httpd
httpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off
httpd24-httpd   0:off   1:off   2:off   3:off   4:off   5:off   6:off
</code></pre>

<p><strong>EDIT 2:</strong> <a href=""http://wiki.apache.org/httpd/PHP-FPM"">http://wiki.apache.org/httpd/PHP-FPM</a></p>

<pre><code>yum install php-fpm
/etc/init.d/php-fpm start
</code></pre>
","138903"
"How to have tail -f show colored output","134875","","<p>I'd like to be able to tail the output of a server log file that has messages like:</p>

<pre><code>INFO
SEVERE
</code></pre>

<p>etc, and if it's <code>SEVERE</code>, show the line in red; if it's <code>INFO</code>, in green.  What kind of alias can I setup for a <code>tail</code> command that would help me do this?</p>
","<p>Try out <a href=""http://www.vanheusden.com/multitail/"" rel=""nofollow noreferrer""><strong>multitail</strong></a>. This is an übergeneralization of <code>tail -f</code>. You can watch multiple files in separate windows, highlight lines based on their content, and more.</p>

<pre><code>multitail -c /path/to/log
</code></pre>

<p>The colors are configurable. If the default color scheme doesn't work for you, write your own in the config file. For example, call <code>multitail -cS amir_log /path/to/log</code> with the following <code>~/.multitailrc</code>:</p>

<pre><code>colorscheme:amir_log
cs_re:green:INFO
cs_re:red:SEVERE
</code></pre>

<p>Another solution, if you're on a server where it's inconvenient to install non-<strong>standard</strong> tools, is to combine <code>tail -f</code> with sed or awk to add color selection control sequences. This requires <code>tail -f</code> to flush its standard output without delay even when its standard output is a pipe, I don't know if all implementations do this.</p>

<pre><code>tail -f /path/to/log | awk '
  /INFO/ {print ""\033[32m"" $0 ""\033[39m""}
  /SEVERE/ {print ""\033[31m"" $0 ""\033[39m""}
'
</code></pre>

<p>or with <a href=""https://www.gnu.org/software/sed/manual/sed.html"" rel=""nofollow noreferrer"">sed</a></p>

<pre><code>tail -f /path/to/log | sed --unbuffered \
    -e 's/\(.*INFO.*\)/\o033[32m\1\o033[39m/' \
    -e 's/\(.*SEVERE.*\)/\o033[31m\1\o033[39m/'
</code></pre>

<p>If your sed isn't GNU sed, replace <code>\o033</code> by a literal escape character.</p>

<p>Yet another possibility is to run <code>tail -f</code> in an <strong>Emacs</strong> shell buffer and use Emacs's syntax coloring abilities.</p>
","8419"
"Disable a user's login without disabling the account","134384","","<p>Let's say I create a user named ""bogus"" using the <code>adduser</code> command. How can I make sure this user will NOT be a viable login option, without disabling the account. In short, I want the account to be accessible via <code>su - bogus</code>, but I do not want it to be accessible via a regular login prompt.</p>

<p>Searching around, it seems I need to disable that user's password, but doing <code>passwd -d bogus</code> didn't help. In fact, it made things worse, because I could now login to bogus without even typing a password.</p>

<p>Is there a way to disable regular logins for a given a account?</p>

<p><em>Note: Just to be clear, I know how to remove a user from the menu options of graphical login screens such as gdm, but these methods simply hide the account without actually disabling login. I'm looking for a way to disable regular login completely, text-mode included.</em></p>
","<pre><code>passwd -l user
</code></pre>

<p>is what you want.</p>

<p>That will lock the user account.  But you'll still be able to</p>

<pre><code>su - user
</code></pre>

<p>but you'll have to <code>su - user</code> as root.</p>

<p>Alternatively, you can accomplish the same thing by prepending a <code>!</code> to the user's password in <code>/etc/shadow</code> (this is all <code>passwd -l</code> does behind the scenes). And <code>passwd -u</code> will undo this.</p>
","19335"
"What is a bind mount?","133948","","<p>What is a “bind mount”? How do I make one? What is it good for?</p>

<p>I've been told to use a bind mount for something, but I don't understand what it is or how to use it.</p>
","<h2>What is a bind mount?</h2>

<p>A <em>bind mount</em> is an alternate view of a directory tree. Classically, mounting creates a view of a storage device as a directory tree. A bind mount instead takes an existing directory tree and replicates it under a different point. The directories and files in the bind mount are the same as the original. Any modification on one side is immediately reflected on the other side, since the two views show the same data.</p>

<p>For example, after issuing the Linux command</p>

<pre><code>mount --bind /some/where /else/where
</code></pre>

<p>the directories <code>/some/where</code> and <code>/else/where</code> have the same content.</p>

<p>Unlike a hard link or symbolic link, a bind mount doesn't affect what is stored on the filesystem. It's a property of the live system.</p>

<h2>How do I create a bind mount?</h2>

<h3>bindfs</h3>

<p>The <a href=""http://bindfs.org/"" rel=""noreferrer""><code>bindfs</code></a> filesystem is a <a href=""http://en.wikipedia.org/wiki/Filesystem_in_Userspace"" rel=""noreferrer"">FUSE</a> filesystem which creates a view of a directory tree. For example, the command</p>

<pre><code>bindfs /some/where /else/where
</code></pre>

<p>makes <code>/else/where</code> a mount point under which the contents of <code>/some/where</code> are visible.</p>

<p>Since bindfs is a separate filesystem, the files <code>/some/where/foo</code> and <code>/else/where/foo</code> appear as different files to applications (the bindfs filesystem has its own <code>st_dev</code> value). Any change on one side is “magically” reflected on the other side, but the fact that the files are the same is only apparent when one knows how bindfs operates.</p>

<p>Bindfs has no knowledge of mount points, so if there is a mount point under <code>/some/where</code>, it appears as just another directory under <code>/else/where</code>. Mounting or unmounting a filesystem underneath <code>/some/where</code> appears under <code>/else/where</code> as a change of the corresponding directory.</p>

<p>Bindfs can alter some of the file metadata: it can show fake permissions and ownership for files. See the <a href=""http://bindfs.org/docs/bindfs.1.html"" rel=""noreferrer"">manual</a> for details, and see below for examples.</p>

<p>A bindfs filesystem can be mounted as a non-root user, you only need the privilege to mount FUSE filesystems. Depending on your distribution, this may require being in the <code>fuse</code> group or be allowed to all users. To unmount a FUSE filesystem, use <code>fusermount -u</code> instead of <code>umount</code>, e.g.</p>

<pre><code>fusermount -u /else/where
</code></pre>

<h3>nullfs</h3>

<p>FreeBSD provides the <a href=""https://www.freebsd.org/cgi/man.cgi?query=nullfs&amp;sektion=5"" rel=""noreferrer""><code>nullfs</code></a> filesystem which creates an alternate view of a filesystem. The following two commands are equivalent:</p>

<pre><code>mount -t nullfs /some/where /else/where
mount_nullfs /some/where /else/where
</code></pre>

<p>After issuing either command, <code>/else/where</code> becomes a mount point at which the contents of <code>/some/where</code> are visible.</p>

<p>Since nullfs is a separate filesystem, the files <code>/some/where/foo</code> and <code>/else/where/foo</code> appear as different files to applications (the nullfs filesystem has its own <code>st_dev</code> value). Any change on one side is “magically” reflected on the other side, but the fact that the files are the same is only apparent when one knows how nullfs operates.</p>

<p>Unlike the FUSE bindfs, which acts at the level of the directory tree, FreeBSD's nullfs acts deeper in the kernel, so mount points under <code>/else/where</code> are not visible: only the tree that is part of the same mount point as <code>/some/where</code> is reflected under <code>/else/where</code>.</p>

<p>The nullfs filesystem may be usable under other BSD variants (OS X, OpenBSD, NetBSD) but it is not compiled as part of the default system.</p>

<h3>Linux bind mount</h3>

<p>Under Linux, bind mounts are available as a kernel feature. You can create one with the <a href=""http://man7.org/linux/man-pages/man8/mount.8.html"" rel=""noreferrer""><code>mount</code></a> command, by passing either the <code>--bind</code> command line option or the <code>bind</code> mount option. The following two commands are equivalent:</p>

<pre><code>mount --bind /some/where /else/where
mount -o bind /some/where /else/where
</code></pre>

<p>Here, the “device” <code>/some/where</code> is not a disk partition like in the case of an on-disk filesystem, but an existing directory. The mount point <code>/else/where</code> must be an existing directory as usual. Note that no filesystem type is specified either way: making a bind mount doesn't involve a filesystem driver, it copies the kernel data structures from the original mount.</p>

<p>A Linux bind mount is mostly indistinguishable from the original. The command <code>df -T /else/where</code> shows the same device and the same filesystem type as <code>df -T /some/where</code>. The files <code>/some/where/foo</code> and <code>/else/where/foo</code> are indistinguishable, as if they were hard links. It is possible to unmount <code>/some/where</code>, in which case <code>/else/where</code> remains mounted.</p>

<p>With older kernels (I don't know exactly when, I think until some 3.x), bind mounts were truly indistinguishable from the original. Recent kernels do track bind mounts and expose the information through PID/mountinfo, which allows <a href=""https://unix.stackexchange.com/questions/295525/how-is-findmnt-able-to-list-bind-mounts""><code>findmnt</code> to indicate bind mount as such</a>.</p>

<p>If there are mount points under <code>/some/where</code>, their contents are not visible under <code>/else/where</code>. Instead of <code>bind</code>, you can use <code>rbind</code>, also replicate mount points underneath <code>/some/where</code>. For example, if <code>/some/where/mnt</code> is a mount point then</p>

<pre><code>mount --rbind /some/where /else/where
</code></pre>

<p>is equivalent to</p>

<pre><code>mount --bind /some/where /else/where
mount --bind /some/where/mnt /else/where/mnt
</code></pre>

<p>In addition, Linux allows mounts to be declared as <em>shared</em>, <em>slave</em>, <em>private</em> or <em>unbindable</em>. This affects whether that mount operation is reflected under a bind mount that replicates the mount point. For more details, see <a href=""https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt"" rel=""noreferrer"">the kernel documentation</a>.</p>

<p>Linux also provides a way to move mounts: where <code>--bind</code> copies, <code>--move</code> moves a mount point.</p>

<p>It is possible to have different mount options in two bind-mounted directories. There is a quirk, however: making the bind mount and setting the mount options cannot be done atomically, they have to be two successive operations. (Older kernels  did not allow this.) For example, the following commands create a read-only view, but there is a small window of time during which <code>/else/where</code> is read-write:</p>

<pre><code>mount --bind /some/where /else/where
mount -o remount,ro,bind /else/where
</code></pre>

<h3>I can't get bind mounts to work!</h3>

<p>If your system doesn't support FUSE, a classical trick to achieve the same effect is to run an NFS server, make it export the files you want to expose (allowing access to <code>localhost</code>) and mount them on the same machine. This has a significant overhead in terms of memory and performance, so bind mounts have a definite advantage where available (which is on most Unix variants thanks to FUSE).</p>

<h2>Use cases</h2>

<h3>Read-only view</h3>

<p>It can be useful to create a read-only view of a filesystem, either for security reasons or just as a layer of safety to ensure that you won't accidentally modify it.</p>

<p>With bindfs:</p>

<pre><code>bindfs -r /some/where /mnt/readonly
</code></pre>

<p>With Linux, the simple way:</p>

<pre><code>mount --bind /some/where /mnt/readonly
mount -o remount,ro,bind /mnt/readonly
</code></pre>

<p>This leaves a short interval of time during which <code>/mnt/readonly</code> is read-write. If this is a security concern, first create the bind mount in a directory that only root can access, make it read-only, then move it to a public mount point. In the snippet below, note that it's important that <code>/root/private</code> (the directory above the mount point) is private; the original permissions on <code>/root/private/mnt</code> are irrelevant since they are hidden behind the mount point.</p>

<pre><code>mkdir -p /root/private/mnt
chmod 700 /root/private
mount --bind /some/where /root/private/mnt
mount -o remount,ro,bind /root/private/mnt
mount --move /root/private/mnt /mnt/readonly
</code></pre>

<h3>Remapping users and groups</h3>

<p>Filesystems record users and groups by their numerical ID. Sometimes you end up with multiple systems which assign different user IDs to the same person. This is not a problem with network access, but it makes user IDs meaningless when you carry data from one system to another on a disk. Suppose that you have a disk created with a multi-user filesystem (e.g. ext4, btrfs, zfs, UFS, …) on a system where Alice has user ID 1000 and Bob has user ID 1001, and you want to make that disk accessible on a system where Alice has user ID 1001 and Bob has user ID 1000. If you mount the disk directly, Alice's files will appear as owned by Bob (because the user ID is 1001) and Bob's files will appear as owned by Alice (because the user ID is 1000).</p>

<p>You can use bindfs to remap user IDs. First mount the disk partition in a private directory, where only root can access it. Then create a bindfs view in a public area, with user ID and group ID remapping that swaps Alice's and Bob's user IDs and group IDs.</p>

<pre><code>mkdir -p /root/private/alice_disk /media/alice_disk
chmod 700 /root/private
mount /dev/sdb1 /root/private/alice_disk
bindfs --map=1000/1001:1001/1000:@1000/1001:@1001/1000 /root/private/alice_disk /media/alice_disk
</code></pre>

<p>See <a href=""https://unix.stackexchange.com/questions/190866/how-does-one-permissibly-access-files-on-non-booted-systems-users-home-folder"">How does one permissibly access files on non-booted system&#39;s user&#39;s home folder?</a> and <a href=""https://unix.stackexchange.com/questions/115377/mount-bind-other-user-as-myself"">mount --bind other user as myself</a> another examples.</p>

<h3>Mounting in a jail</h3>

<p>A <a href=""http://en.wikipedia.org/wiki/Chroot"" rel=""noreferrer"">chroot jail</a> runs a process in a subtree of the system's directory tree. This can be useful to run a program with restricted access, e.g. run a network server with access to only its own files and the files that it serves, but not to other data stored on the same computer). A limitation of chroot is that the program is confined to one subtree: it can't access independent subtrees. Bind mounts allow grafting other subtrees onto that main tree.</p>

<p>For example, suppose that a machine runs a service <code>/usr/sbin/somethingd</code> which should only have access to data under <code>/var/lib/something</code>. The smallest directory tree that contains both of these files is the root. How can the service be confined? One possibility is to make hard links to all the files that the service needs (at least <code>/usr/sbin/somethingd</code> and several shared libraries) under <code>/var/lib/something</code>. But this is cumbersome (the hard links need to be updated whenever a file is upgraded), and doesn't work if <code>/var/lib/something</code> and <code>/usr</code> are on different filesystems. A better solution is to create an ad hoc root and populate it with using mounts:</p>

<pre><code>mkdir /run/something
cd /run/something
mkdir -p etc/something lib usr/lib usr/sbin var/lib/something
mount --bind /etc/something etc/something
mount --bind /lib lib
mount --bind /usr/lib usr/lib
mount --bind /usr/sbin usr/sbin
mount --bind /var/lib/something var/lib/something
mount -o remount,ro,bind etc/something
mount -o remount,ro,bind lib
mount -o remount,ro,bind usr/lib
mount -o remount,ro,bind usr/sbin
chroot . /usr/sbin/somethingd &amp;
</code></pre>

<p>Linux's <a href=""http://lwn.net/2001/0301/a/namespaces.php3"" rel=""noreferrer"">mount namespaces</a> generalize chroots. Bind mounts are how namespaces can be populated in flexible ways. See <a href=""https://unix.stackexchange.com/questions/81003/making-a-process-read-a-different-file-for-the-same-filename"">Making a process read a different file for the same filename</a> for an example.</p>

<h3>Running a different distribution</h3>

<p>Another use of chroots is to install a different distribution in a directory and run programs from it, even when they require files at hard-coded paths that are not present or have different content on the base system. This can be useful, for example, to install a 32-bit distribution on a 64-bit system that doesn't support mixed packages, to install older releases of a distribution or other distributions to test compatibility, to install a newer release to test the latest features while maintaining a stable base system, etc. See <a href=""https://unix.stackexchange.com/questions/12956/how-do-i-run-32-bit-programs-on-a-64-bit-debian-ubuntu"">How do I run 32-bit programs on a 64-bit Debian/Ubuntu?</a> for an example on Debian/Ubuntu.</p>

<p>Suppose that you have an installation of your distribution's latest packages under the directory <code>/f/unstable</code>, where you run programs by switching to that directory with <code>chroot /f/unstable</code>. To make home directories available from this installations, bind mount them into the chroot:</p>

<pre><code>mount --bind /home /f/unstable/home
</code></pre>

<p>The program <a href=""https://packages.debian.org/jessie/schroot"" rel=""noreferrer"">schroot</a> does this automatically.</p>

<h3>Accessing files hidden behind a mount point</h3>

<p>When you mount a filesystem on a directory, this hides what is behind the directory. The files in that directory become inaccessible until the directory is unmounted. Because BSD nullfs and Linux bind mounts operate at a lower level than the mount infrastructure, a nullfs mount or a bind mount of a filesystem exposes directories that were hidden behind submounts in the original.</p>

<p>For example, suppose that you have a tmpfs filesystem mounted at <code>/tmp</code>. If there were files under <code>/tmp</code> when the tmpfs filesystem was created, these files may still remain, effectively inaccessible but taking up disk space. Run</p>

<pre><code>mount --bind / /mnt
</code></pre>

<p>(Linux) or</p>

<pre><code>mount -t nullfs / /mnt
</code></pre>

<p>(FreeBSD) to create a view of the root filesystem at <code>/mnt</code>. The directory <code>/mnt/tmp</code> is the one from the root filesystem.</p>

<h3>NFS exports at different paths</h3>

<p>Some NFS servers (such as the Linux kernel NFS server before NFSv4) always advertise the actual directory location when they export a directory. That is, when a client requests <code>server:/requested/location</code>, the server serves the tree at the location <code>/requested/location</code>. It is sometimes desirable to allow clients to request <code>/request/location</code> but actually serve files under <code>/actual/location</code>. If your NFS server doesn't support serving an alternate location, you can create a bind mount for the expected request, e.g.</p>

<pre><code>/requested/location *.localdomain(rw,async)
</code></pre>

<p>in <code>/etc/exports</code> and the following in <code>/etc/fstab</code>:</p>

<pre><code>/actual/location /requested/location bind bind
</code></pre>

<h2>Side effects of bind mounts</h2>

<h3>Recursive directory traversals</h3>

<p>If you use bind mounts, you need to take care of applications that traverse the filesystem tree recursively, such as backups and indexing (e.g. to build a <a href=""http://en.wikipedia.org/wiki/Locate_(Unix)"" rel=""noreferrer"">locate</a> database).</p>

<p>Usually, bind mounts should be excluded from recursive directory traversals, so that each directory tree is only traversed once, at the original location. With bindfs and nullfs, configure the traversal tool to ignore these filesystem types, if possible. Linux bind mounts cannot be recognized as such: the new location is equivalent to the original. With Linux bind mounts, or with tools that can only exclude paths and not filesystem types, you need to exclude the mount points for the bind mounts.</p>

<p>Traversals that stop at filesystem boundaries (e.g. <code>find -xdev</code>, <code>rsync -x</code>, <code>du -x</code>, …) will automatically stop when they encounter a bindfs or nullfs mount point, because that mount point is a different filesystem. With Linux bind mounts, the situation is a bit more complicated: there is a filesystem boundary only if the bind mount is grafting a different filesystem, not if it is grafting another part of the same filesystem.</p>

<h2>Going beyond bind mounts</h2>

<p>Bind mounts provide a view of a directory tree at a different location. They expose the same files, possibly with different mount options and (with bindfs) different ownership and permissions. Filesystems that present an altered view of a directory tree are called <em>overlay filesystems</em> or <em>stackable filesystems</em>. There are many other overlay filesystems that perform more advanced transformations. Here are a few common ones. If your desired use case is not covered here, check the <a href=""http://sourceforge.net/p/fuse/wiki/FileSystems/"" rel=""noreferrer"">repository of FUSE filesystems</a>.</p>

<ul>
<li><a href=""http://sourceforge.net/projects/loggedfs/"" rel=""noreferrer"">loggedfs</a> — log all filesystem access for debugging or monitoring purposes (<a href=""https://unix.stackexchange.com/questions/13794/loggedfs-configuration-file-syntax/13797#13797"">configuration file syntax</a>, <a href=""https://unix.stackexchange.com/questions/6068/is-it-possible-to-find-out-what-program-or-script-created-a-given-file/6080#6080"">Is it possible to find out what program or script created a given file?</a>, <a href=""https://unix.stackexchange.com/questions/18844/list-the-files-accessed-by-a-program/18872#18872"">List the files accessed by a program</a>)</li>
</ul>

<h3>Filter visible files</h3>

<ul>
<li><a href=""http://clamfs.sourceforge.net/"" rel=""noreferrer"">clamfs</a> — run files through a virus scanner when they are read</li>
<li><a href=""http://filterfs.sourceforge.net/"" rel=""noreferrer"">filterfs</a> — hide parts of a filesystem</li>
<li><a href=""https://github.com/cognusion/fuse-rofs"" rel=""noreferrer"">rofs</a> — a read-only view. Similar to <code>bindfs -r</code>, just a little more lightweight.</li>
<li><p><a href=""http://en.wikipedia.org/wiki/Union_mount"" rel=""noreferrer"">Union mounts</a> — present multiple filesystems (called <em>branches</em>) under a single directory: if <code>tree1</code> contains <code>foo</code> and <code>tree2</code> contains <code>bar</code> then their union view contains both <code>foo</code> and <code>bar</code>. New files are written to a specific branch, or to a branch chosen according to more complex rules. There are several implementations of this concept, including:</p>

<ul>
<li><a href=""http://aufs.sourceforge.net/"" rel=""noreferrer"">aufs</a> — Linux kernel implementation</li>
<li><a href=""http://funionfs.apiou.org/?lng=en"" rel=""noreferrer"">funionfs</a> — FUSE implementation</li>
<li><a href=""http://svn.uvw.ru/mhddfs/trunk/README"" rel=""noreferrer"">mhddfs</a> — FUSE, write files to a branch based on free space</li>
<li><a href=""https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/overlayfs.txt"" rel=""noreferrer"">overlay</a> — Linux kernel implementation</li>
<li><a href=""https://github.com/rpodgorny/unionfs-fuse"" rel=""noreferrer"">unionfs-fuse</a> — FUSE, with caching and copy-on-write features</li>
</ul></li>
</ul>

<h3>Modify file names and metadata</h3>

<ul>
<li><a href=""http://brain-dump.org/projects/ciopfs/"" rel=""noreferrer"">ciopfs</a> — case-insensitive filenames (can be useful to mount Windows filesystems)</li>
<li><a href=""http://fuse-convmvfs.sourceforge.net/"" rel=""noreferrer"">convmvfs</a> — convert filenames between character sets (<a href=""https://unix.stackexchange.com/questions/67232/same-file-different-filename-due-to-encoding-problem/67273#67273"">example</a>)</li>
<li><a href=""http://sourceforge.net/projects/posixovl/"" rel=""noreferrer"">posixovl</a> — store Unix filenames and other metadata (permissions, ownership, …) on more restricted filesystems such as VFAT (<a href=""https://unix.stackexchange.com/questions/108890/what-is-the-best-way-to-synchronize-files-to-a-vfat-partition/108937#108937"">example</a>)</li>
</ul>

<h3>View altered file contents</h3>

<ul>
<li><a href=""http://avf.sourceforge.net/"" rel=""noreferrer"">avfs</a> — for each archive file, present a directory with the content of the archive (<a href=""https://unix.stackexchange.com/questions/13749/how-do-i-recursively-grep-through-compressed-archives/13798#13798"">example</a>, <a href=""https://unix.stackexchange.com/search?tab=votes&amp;q=avfs%20is%3aanswer"">more examples</a>). There are also many <a href=""http://sourceforge.net/p/fuse/wiki/ArchiveFileSystems/"" rel=""noreferrer"">FUSE filesystems that expose specific archives as directories</a>.</li>
<li><a href=""http://users.softlab.ntua.gr/~thkala/projects/fuseflt/"" rel=""noreferrer"">fuseflt</a> — run files through a pipeline when reading them, e.g. to recode text files or media files (<a href=""https://unix.stackexchange.com/questions/33574/how-to-use-grep-ack-with-files-in-arbitrary-encoding/33580#33580"">example</a>)</li>
<li><a href=""https://github.com/vasi/lzopfs"" rel=""noreferrer"">lzopfs</a> — transparent decompression of compressed files</li>
<li><a href=""http://khenriks.github.io/mp3fs/"" rel=""noreferrer"">mp3fs</a> — transcode FLAC files to MP3 when they are read (<a href=""https://unix.stackexchange.com/questions/37701/how-to-encode-huge-flac-files-into-mp3-and-other-files-like-aac/115695#115695"">example</a>)</li>
<li><a href=""https://code.google.com/p/scriptfs/"" rel=""noreferrer"">scriptfs</a> — execute scripts to serve content (a sort of local CGI) (<a href=""https://unix.stackexchange.com/questions/181673/using-process-substitution-to-trick-programs-expecting-files-with-specific-exte/181680#181680"">example</a>)</li>
</ul>

<h3>Modify the way content is stored</h3>

<ul>
<li><a href=""https://code.google.com/p/chironfs/"" rel=""noreferrer"">chironfs</a> — replicate files onto multiple underlying storage (<a href=""https://unix.stackexchange.com/questions/14544/raid-1-lvm-at-the-level-of-directories-aka-mknodding-a-directory"">RAID-1 at the directory tree level</a>)</li>
<li><a href=""http://n0x.org/copyfs"" rel=""noreferrer"">copyfs</a> — keep copies of all versions of the files</li>
<li><a href=""http://www.arg0.net/encfs"" rel=""noreferrer"">encfs</a> — encrypt files</li>
<li><a href=""https://code.google.com/p/pcachefs/"" rel=""noreferrer"">pcachefs</a> — on-disk cache layer for slow remote filesystems</li>
<li><a href=""https://github.com/vi/simplecowfs"" rel=""noreferrer"">simplecowfs</a> — store changes via the provided view in memory, leaving the original files intact</li>
<li><a href=""http://wayback.sourceforge.net/"" rel=""noreferrer"">wayback</a> — keep copies of all versions of the files</li>
</ul>
","198591"
"Finding files that use the most disk space","133852","","<p>Is it possible to list the largest files on my hard drive?  I frequently use <code>df -H</code> to display my disk usage, but this only gives the percentage full, GBs remaining, etc.</p>

<p>I do a lot of data-intensive calculations, with a large number of small files and a very small number of very large files.  Since most of my disk space used is in a very small number of files, it can be difficult to track down where these large files are.  Deleting a 1 kB file does not free much space, but deleting a 100 GB file does.  Is there any way to sort the files on the hard drive in terms of their size?</p>

<p>Thanks.</p>
","<p>With standard available tools: </p>

<p>To list the top 10 largest files from the current directory: <code>du . | sort -nr | head -n10</code></p>

<p>To list the largest directories from the current directory: <code>du -s * | sort -nr | head -n10</code></p>

<p><strong>UPDATE</strong> These days I usually use a more readable form (as Jay Chakra explains in another answer and leave off the <code>| head -n10</code>, simply let it scroll off the screen. The last line has the largest file or directory (tree).</p>

<p>Sometimes, eg. when you have lots of mount points in the current directory,  instead of using <code>-x</code> or multiple <code>--exclude=PATTERN</code>, it is handier to mount the filesystem on an unused mount point (<em>often</em> <code>/mnt</code>) and work from there.</p>

<p>Mind you that when working with large (NFS) volumes, you can cause a substantial load on the storage backend (filer) when running <code>du</code> over lots of (sub)directories. In that case it is better to consider setting <code>quota</code> on the volume.</p>
","37224"
"How to find multiple strings in files?","133776","","<p>I use the following command to find files with a given string:</p>

<pre><code>find /var/www/http -type f | xargs grep -iR ""STRING1""
</code></pre>

<p>But how can I find files which include ""STRING1"" OR ""STRING2"" OR ""STRING3""?</p>

<p>This code doesn't work:</p>

<pre><code>find /var/www/http -type f | xargs grep -iR ""STRING1"" | xargs grep -iR ""STRING2""
</code></pre>
","<p>POSIXly, using <code>grep</code> with <code>-E</code> option:</p>

<pre><code>find /var/www/http -type f -exec grep -iE 'STRING1|STRING2' /dev/null {} +
</code></pre>

<p>Or <code>-e</code>:</p>

<pre><code>find /var/www/http -type f -exec grep -i -e 'STRING' -e 'STRING2' /dev/null {} +
</code></pre>

<p>With some implementations, at least on GNU systems, OSX and FreeBSD, you can escape <code>|</code>:</p>

<pre><code>find /var/www/http -type f -exec grep -i 'STRING1\|STRING2' /dev/null {} +
</code></pre>
","85810"
"Logging SSH access attempts","133740","","<p>I've configured an ubuntu server with openssh in order to connect to it and execute commands from a remote system like a phone or a laptop. The problem is... I'm probably not the only one. </p>

<p>Is there a way to know all the login attempts that have been made to the server?</p>
","<p>On Ubuntu servers, you can find who logged in when (and from where) in the file <code>/var/log/auth.log</code>. There, you find entries like:</p>

<pre><code>May  1 16:17:02 owl CRON[9019]: pam_unix(cron:session): session closed for user root
May  1 16:17:43 owl sshd[9024]: Accepted publickey for root from 192.168.0.101 port 37384 ssh2
May  1 16:17:43 owl sshd[9024]: pam_unix(sshd:session): session opened for user root by (uid=0)
</code></pre>
","127436"
"What does the Broken pipe message mean in an SSH session?","133612","","<p>Sometimes my SSH session disconnects with a <code>Write failed: Broken pipe</code> message. What does it mean? And how can I keep my session open?</p>

<p>I know about <code>screen</code>, but that's not the answer I'm looking for. I think this is a <code>sshd</code> config option.</p>
","<p>It's possible that your server closes connections that are idle for too long.
You can update either your client (<code>ServerAliveInterval</code>) or your server (<code>ClientAliveInterval</code>)</p>

<pre><code> ServerAliveInterval
         Sets a timeout interval in seconds after which if no data has
         been received from the server, ssh(1) will send a message through
         the encrypted channel to request a response from the server.  The
         default is 0, indicating that these messages will not be sent to
         the server.  This option applies to protocol version 2 only.

 ClientAliveInterval
         Sets a timeout interval in seconds after which if no data has
         been received from the client, sshd(8) will send a message
         through the encrypted channel to request a response from the
         client.  The default is 0, indicating that these messages will
         not be sent to the client.  This option applies to protocol
         version 2 only.
</code></pre>

<p>To update your server (and restart your <code>sshd</code>)</p>

<pre><code>echo ""ClientAliveInterval 60"" | sudo tee -a /etc/ssh/sshd_config
</code></pre>

<p>Or client-side:</p>

<pre><code>echo ""ServerAliveInterval 60"" &gt;&gt; ~/.ssh/config 
</code></pre>
","2013"
"Preserve bash history in multiple terminal windows","133551","","<p>I consistently have more than one terminal open. Anywhere from two to ten, doing various bits and bobs. Now let's say I restart and open up another set of terminals. Some remember certain things, some forget.</p>

<p>I want a history that:</p>

<ul>
<li>Remembers everything from every terminal</li>
<li>Is instantly accessible from every terminal (eg if I <code>ls</code> in one, switch to another already-running terminal and then press up, <code>ls</code> shows up)</li>
<li>Doesn't forget command if there are spaces at the front of the command.</li>
</ul>

<p>Anything I can do to make bash work more like that?</p>
","<p>Add the following to ~/.bashrc
</p>

<pre><code># Avoid duplicates
export HISTCONTROL=ignoredups:erasedups  
# When the shell exits, append to the history file instead of overwriting it
shopt -s histappend

# After each command, append to the history file and reread it
export PROMPT_COMMAND=""${PROMPT_COMMAND:+$PROMPT_COMMAND$'\n'}history -a; history -c; history -r""
</code></pre>
","1292"
"Understanding %CPU while running top command","133137","","<p>I am trying to understand what does <code>%CPU</code> means when I run <code>top</code>.</p>

<p>I am seeing <code>%CPU</code> for my application as <code>400</code> or <code>500</code> most of the times.</p>

<p>Does anyone knows what does this mean?</p>

<p>What number is a high number?</p>

<blockquote>
  <p>19080 david  20   0 27.9g  24g  12m S   400 19.7 382:31.81 paper_client</p>
</blockquote>

<p><code>lscpu</code> gives me below output:</p>

<pre><code>Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
Socket(s):             2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 45
Stepping:              7
CPU MHz:               2599.928
BogoMIPS:              5199.94
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              20480K
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31
</code></pre>
","<blockquote>
  <p><strong>%CPU</strong>  --  CPU Usage : The percentage of your CPU that is being used by the process. <strong>By default, <code>top</code> displays this as a percentage
  of a single CPU.</strong> On multi-core systems, you can have percentages
  that are greater than 100%. For example, if 3 cores are at 60% use,
  <code>top</code> will show a CPU use of 180%. See <a href=""https://superuser.com/a/457634/333431"">here</a> for more information.
  <strong>You can toggle this behavior by hitting <kbd>Shift</kbd><kbd>i</kbd> while <code>top</code> is running to show the overall percentage of available
  CPUs in use.</strong></p>
</blockquote>

<p><a href=""https://superuser.com/a/575330/333431"">Source for above quote</a>.</p>

<p>You can use <code>htop</code> instead.</p>

<hr>

<p>To answer your question about how many cores and virtual cores you have:</p>

<p>According to your <code>lscpu</code> output:</p>

<ul>
<li>You have 32 cores (<code>CPU(s)</code>) in total.</li>
<li>You have 2 physical sockets (<code>Socket(s)</code>), each contains 1 physical processor.</li>
<li>Each processor of yours has 8 physical cores (<code>Core(s) per socket</code>) inside, which means you have 8 * 2 = 16 real cores.</li>
<li>Each real core can have 2 threads (<code>Thread(s) per core</code>), which means you have real cores * threads = 16 * 2 = 32 cores in total.</li>
</ul>

<p>So you have 16 virtual cores and 16 real cores.</p>

<p>Also see <a href=""http://buildwindows.wordpress.com/2012/12/13/virtualization-processor-core-logical-processor-virtual-processor-what-does-this-mean/"" rel=""noreferrer"">this</a>, <a href=""https://kb.iu.edu/d/avfb"" rel=""noreferrer"">this</a> and <a href=""http://linuxconfig.org/getting-know-a-hardware-on-your-linux-box"" rel=""noreferrer"">this</a> link.</p>
","145249"
"redirecting to /dev/null","133130","","<p>I'm reading an example bash shell script:</p>

<pre><code>#!/bin/bash

# This script makes a backup of my home directory.

cd /home

# This creates the archive
tar cf /var/tmp/home_franky.tar franky &gt; /dev/null 2&gt;&amp;1

# First remove the old bzip2 file.  Redirect errors because this generates some if the archive
# does not exist.  Then create a new compressed file.
rm /var/tmp/home_franky.tar.bz2 2&gt; /dev/null
bzip2 /var/tmp/home_franky.tar

# Copy the file to another host - we have ssh keys for making this work without intervention.
scp /var/tmp/home_franky.tar.bz2 bordeaux:/opt/backup/franky &gt; /dev/null 2&gt;&amp;1

# Create a timestamp in a logfile.
date &gt;&gt; /home/franky/log/home_backup.log
echo backup succeeded &gt;&gt; /home/franky/log/home_backup.log
</code></pre>

<p>I'm trying to understand the use of ""/dev/null 2>&amp;1"" here. At first, I thought this script uses /dev/null in order to gracefully ignore errors, without causing the script to crash (kind of like try catch exception handling in programming languages). Because I don't see how using tar to compress a directory into a tar file could possibly cause any type of errors. </p>
","<p>No, this will not prevent the script from crashing. If any errors occur in the <code>tar</code> process (e.g.: permission denied, no such file or directory, ...) the script will still crash.</p>

<p>Because using <code>&gt; /dev/null 2&gt;&amp;1</code> will redirect all your command output (both <code>stdout</code> and <code>stderr</code>) to <code>/dev/null</code>, meaning no outputs are printed to terminal.</p>

<p>By default:</p>

<pre><code>stdin  ==&gt; fd 0
stdout ==&gt; fd 1
stderr ==&gt; fd 2
</code></pre>

<p>In the script, you use <code>&gt; /dev/null</code> causing:</p>

<pre><code>stdin  ==&gt; fd 0
stdout ==&gt; /dev/null
stderr ==&gt; fd 2
</code></pre>

<p>And then <code>2&gt;&amp;1</code> causing:</p>

<pre><code>stdin  ==&gt; fd 0
stdout ==&gt; /dev/null
stderr ==&gt; stdout
</code></pre>
","119650"
"How to detect SLES version?","132996","","<p>What patchlevel does this SLES machine has? 10.2 or 10.4?</p>

<pre><code>SERVER:~ # cat /etc/issue
SUSE LINUX Enterprise Server 10.2
Kernel \r (\m), \l
SERVER:~ # 

SERVER:~ # cat /etc/SuSE-release 
SUSE Linux Enterprise Server 10 (x86_64)
VERSION = 10
PATCHLEVEL = 4
SERVER:~ # 
</code></pre>

<p>UPDATE: </p>

<pre><code>SERVER:/etc # rpm -V sles-release
S.5....T  c /etc/issue
S.5....T  c /etc/issue.net
S.5....T  c /etc/motd

SERVER:/etc # zypper sl
# | Enabled | Refresh | Type | Name                                                | URI                                                                   
--+---------+---------+------+-----------------------------------------------------+-----------------------------------------------------------------------
1 | No      | No      | YaST | SUSE Linux Enterprise Server 10 SP2                 | cd:///?devices=/dev/hda                                               
2 | Yes     | Yes     | YaST | SUSE Linux Enterprise Server 10 SP2-20110317-171027 | nfs://123.123.123.123/usr/sys/inst.images/Linux/SuSE/SLES10_x86_64/10.2

SERVER:/etc # uname -r
2.6.16.60-0.91.1-smp
</code></pre>

<p>UPDATE #2: </p>

<pre><code>SERVER:/etc # cat /etc/issue.rpmnew

Welcome to SUSE Linux Enterprise Server 10 SP4  (x86_64) - Kernel \r (\l).
</code></pre>

<p>UPDATE #3</p>

<pre><code>SERVER:/etc # 

SERVER:~ # rpm -qi glibc
Name        : glibc                        Relocations: (not relocatable)
Version     : 2.4                               Vendor: SUSE LINUX Products GmbH, Nuernberg, Germany
Release     : 31.95.1                       Build Date: Mon Sep 19 16:43:25 2011
Install Date: Sun Mar 18 08:01:27 2012      Build Host: macintyre
Group       : System/Libraries              Source RPM: glibc-2.4-31.95.1.src.rpm
Size        : 5141247                          License: BSD 3-Clause; GPL v2 or later; LGPL v2.1 or later
Signature   : DSA/SHA1, Mon Sep 19 16:45:00 2011, Key ID a84edae89c800aca
Packager    : http://bugs.opensuse.org
URL         : http://www.gnu.org/software/libc/libc.html
Summary     : Standard Shared Libraries (from the GNU C Library)
Description :
The GNU C Library provides the most important standard libraries used
by nearly all programs: the standard C library, the standard math
library, and the POSIX thread library.  A system is not functional
without these libraries.
Distribution: SUSE Linux Enterprise 10
SERVER:~ # 
</code></pre>
","<p>Most probably you have got a SLES10 SP4.</p>

<p>Do a <code>rpm -V sles-release</code> - if /etc/SuSE-relase does not show ""5"" (i.e. changed md5-checksum) the file content is original.</p>

<p>If you update your question with your exact kernel version (<code>uname -r</code>) I can even tell you more.</p>

<p>You can also check which repositories are active on that system: <code>zypper sl</code></p>

<p>Update on uname/zypper results:</p>

<p><a href=""http://wiki.novell.com/index.php/Kernel_versions"" rel=""nofollow"">Here</a> is a list of SLES-kernels and their release dates. This shows your kernel to be a SLES10 SP4 released on 2011-10-28. There is a more recent SP4 kernel from 2012-01-23.</p>

<p>Your output from zypper sl puzzles me. I can not see how your system got to SLES10 SP4 - there are only SLES10 SP2 repositories shown. </p>

<p>I think it is worth to look into this a bit deeper... (see my current comment to your question)</p>
","37388"
"How to check 'mdadm' RAIDs while running?","132884","","<p>I'm starting to get a collection of computers at home and to support them I have my ""server"" linux box running a RAID array.</p>

<p>Its currently <code>mdadm</code> <code>RAID-1</code>, going to <code>RAID-5</code> once I have more drives (and then <code>RAID-6</code> I'm hoping for). However I've heard various stories about data getting corrupted on one drive and you never noticing due to the other drive being used, up until the point when the first drive fails, and you find your second drive is also screwed (and 3rd, 4th, 5th drive).</p>

<p>Obviously backups are important and I'm taking care of that also, however I know I've previously seen scripts which claim to help against this problem and allow you to check your RAID while its running. However looking for these scripts again now I'm finding it hard to find anything which seems similar to what I ran before and I feel I'm out of date and not understanding whatever has changed.</p>

<p>How would you check a running RAID to make sure all disks are still preforming normally? </p>

<p>I monitor SMART on all the drives and also have <code>mdadm</code> set to email me in case of failure but I'd like to know my drives occasionally ""check"" themselves too.</p>
","<p>The point of RAID with redundancy is that it will keep going as long as it can, but obviously it will detect errors that put it into a degraded mode, such as a failing disk. You can show the current status of an array with <code>mdadm -D</code>:</p>

<pre><code># mdadm -D /dev/md0
&lt;snip&gt;
       0       8        5        0      active sync   /dev/sda5
       1       8       23        1      active sync   /dev/sdb7
</code></pre>

<p>Furthermore the return status of <code>mdadm -D</code> is nonzero if there is any problem such as a failed component (1 indicates an error that the RAID mode compensates for, and 2 indicates a complete failure).</p>

<p>You can also get a quick summary of all RAID device status by looking at <code>/proc/mdstat</code>. You can get information about a RAID device in <code>/sys/class/block/md*/md/*</code> as well; see <a href=""http://kernel.org/doc/Documentation/md.txt""><code>Documentation/md.txt</code></a> in the kernel documentation. Some <code>/sys</code> entries are writable as well; for example you can trigger a full check of <code>md0</code> with <code>echo check &gt;/sys/class/block/md0/md/sync_action</code>.</p>

<p>In addition to these spot checks, mdadm can notify you as soon as something bad happens. Make sure that you have <code>MAILADDR root</code> in <code>/etc/mdadm.conf</code> (some distributions (e.g. Debian) set this up automatically). Then you will <strong>receive an email notification as soon as an error (a degraded array) occurs</strong>.</p>

<p><strong>Make sure that you do receive mail send to root on the local machine</strong> (some modern distributions omit this, because they consider that all email goes through external providers — but receiving local mail is necessary for any serious system administrator). Test this by sending root a mail: <code>echo hello | mail -s test root@localhost</code>. Usually, a proper email setup requires two things:</p>

<ul>
<li>Run an <a href=""http://en.wikipedia.org/wiki/Message_transfer_agent"">MTA</a> on your local machine. The MTA must be set up at least to allow local mail delivery. All distributions come with suitable MTAs, pick anything (but not nullmailer if you want the email to be delivered locally).</li>
<li><p>Redirect mail going to system accounts (at least <code>root</code>) to an address that you read regularly. This can be your account on the local machine, or an external email address. With most MTAs, the address can be configured in <code>/etc/aliases</code>; you should have a line like</p>

<pre><code>root: djsmiley2k
</code></pre>

<p>for local delivery, or</p>

<pre><code>root: djsmiley2k@mail-provider.example.com
</code></pre>

<p>for remote delivery. If you choose remote delivery, make sure that your MTA is configured for that. Depending on your MTA, you may need to run the <code>newaliases</code> command after editing <code>/etc/aliases</code>.</p></li>
</ul>
","28642"
"Adding a self-signed certificate to the ""trusted list""","132711","","<p>I've generated a self-signed certificate for my build server and I'd like to globally trust the certificate on my machine, as I created the key myself and I'm sick of seeing warnings. </p>

<p>I'm on Ubuntu 12.04. How can I take the certificate and globally trust it so that browsers (Google Chrome), CLI utilities (wget, curl), and programming languages (Python, Java, etc.) trust the connection to <a href=""https://mysite.com"">https://mysite.com</a> without asking questions?</p>
","<p>The simple answer to this is that pretty much each application will handle it differently. </p>

<p>Also OpenSSL and GNUTLS (the most widely used certificate processing libraries used to handle signed certificates) behave differently in their treatment of certs which also complicates the issue. Also operating systems utilize different mechanisms to utilize ""root CA"" used by most websites.</p>

<p>That aside, giving Debian as an example. Install the <code>ca-certificates</code> package:</p>

<pre><code>apt-get install ca-certificates
</code></pre>

<p>You then copy the public half of your untrusted CA certificate (the one you use to sign your CSR) into the CA certificate directory (as root):</p>

<pre><code>cp cacert.pem /usr/share/ca-certificates
</code></pre>

<p>And get it to rebuild the directory with your certificate included, run as root:</p>

<pre><code>dpkg-reconfigure ca-certificates
</code></pre>

<p>and select the <code>ask</code> option, scroll to your certificate, mark it for inclusion and select ok. </p>

<p>Most browsers use their own CA database, and so tools like <code>certutil</code> have to be used to modify their contents (on Debian that is provided by the <code>libnss3-tools</code> package). For example, with Chrome you run something along the lines of:</p>

<pre><code>certutil -d sql:$HOME/.pki/nssdb -A -t ""C,,"" -n ""My Homemade CA"" -i /path/to/CA/cert.file
</code></pre>

<p>Firefox will allow you to browse to the certificate on disk, recognize it a certificate file and then allow you to import it to Root CA list.</p>

<p>Most other commands such as <code>curl</code> take command line switches you can use to point at your CA,</p>

<pre><code> curl --cacert  /path/to/CA/cert.file https://...
</code></pre>

<p>or drop the SSL validation altogether</p>

<pre><code> curl --insecure https://...
</code></pre>

<p>The rest will need individual investigation if the <code>ca-certificates</code> like trick does not sort it for that particular application.</p>
","90607"
"Why I can't escape spaces on a bash script?","132570","","<p>I'm trying to escape the space char for a path in Bash, but neither using a <a href=""https://stackoverflow.com/a/12902248/1505348"">backslash</a> or <a href=""https://stackoverflow.com/a/13005744/1505348"">quotes</a> works.</p>

<p>.sh script:</p>

<pre class=""lang-bsh prettyprint-override""><code>ROOT=""/home/hogar/Documents/files/""
FILE=${ROOT}""bdd.encrypted""
DESTINATION=""/home/hogar/Ubuntu\ One/folder""

mv ${FILE} ${DESTINATION}
</code></pre>

<p>After execute the script (<code>./file</code>) this is the result:</p>

<blockquote>
  <p>mv: target ‘One/folder’ is not a directory</p>
</blockquote>

<p>Why does the <code>mv</code> command split the string and who do I stop this?</p>
","<p>You are expanding the DESTINATION variable, if you did <code>echo</code> this is what you would get:</p>

<pre><code>echo ${DESTINATION}
/home/hogar/Ubuntu\ One/folder
</code></pre>

<p>But <code>mv</code> doesn't understand this:</p>

<pre><code>mv ${FILE} ${DESTINATION}                                                
mv: cannot move ‘/home/hogar/Documents/files/bdd.encrypted’ to ‘/home/hogar/Ubuntu\\ One/folder’: No such file or directory
</code></pre>

<p>(for some reason my mv is more verbose)</p>

<p>To prevent this you should use quotes instead:</p>

<pre><code>mv ""${FILE}"" ""${DESTINATION}""
</code></pre>

<p>If you don't need expansion (since you are already expanding before) just using <code>""$...""</code> should suffice:</p>

<pre><code>mv ""$FILE"" ""$DESTINATION""
</code></pre>
","108638"
"Too many levels of symbolic links","132563","","<p>I created this file structure:</p>

<pre><code>test/src
test/firefox
</code></pre>

<p>When I run this command:</p>

<pre><code>ln -s test/src test/firefox
</code></pre>

<p>I would expect a symbolic link <code>test/firefox/src</code> to be created pointing to <code>test/src</code>, however I get this error instead:</p>

<pre><code>-bash: cd: src: Too many levels of symbolic links
</code></pre>

<ul>
<li>What am I doing wrong?</li>
<li>Can you not create a symbolic link to one folder which is stored in a
sibling of that folder?</li>
<li>What's the point of this?</li>
</ul>
","<p>On the surface, what you've suggested you've tried works for me.</p>

<h3>Example</h3>

<pre><code>$ mkdir -p test/src test/firefox

$ tree --noreport -fp
.
`-- [drwxrwxr-x]  ./test
    |-- [drwxrwxr-x]  ./test/firefox
    `-- [drwxrwxr-x]  ./test/src
</code></pre>

<p><em>Make the symbolic link:</em></p>

<pre><code>$ ln -s test/src test/firefox

$ tree --noreport -fp
.
`-- [drwxrwxr-x]  ./test
    |-- [drwxrwxr-x]  ./test/firefox
    |   `-- [lrwxrwxrwx]  ./test/firefox/src -&gt; test/src
    `-- [drwxrwxr-x]  ./test/src
</code></pre>

<p>Running it a 2nd time would typically produce this:</p>

<pre><code>$ ln -s test/src test/firefox
ln: failed to create symbolic link ‘test/firefox/src’: File exists
</code></pre>

<p>So you likely have something else going on here. I would suspect that you have a circular reference where a link is pointing back onto itself.</p>

<p>You can use <code>find</code> to sleuth this out a bit:</p>

<pre><code>$ cd /suspected/directory
$ find -L ./ -mindepth 15
</code></pre>
","141442"
"Can't ifdown eth0 (main interface)","132019","","<p>I can't <code>ifdown</code> an interface on Debian 6.0.5:</p>

<pre><code>user@box:/etc/network$ sudo ifdown eth0 &amp;&amp; sudo ifup eth0
ifdown: interface eth0 not configured
SIOCADDRT: File exists
Failed to bring up eth0.

user@box:/etc/network$ cat interfaces 
auto lo
iface lo inet loopback

allow-hotplug eth0 
allow-hotplug eth1 

auto eth0
iface eth0 inet static
address 10.0.0.1
netmask 255.255.255.0
gateway 10.0.0.254

auto eth1
iface eth1 inet manual
</code></pre>

<p>As requested by marco:</p>

<pre><code>user@box:/etc/network/$ cat run/ifstate 
lo=lo
eth1=eth1
</code></pre>
","<p>Check the contents of the file <code>/run/network/ifstate</code>. <code>ifup</code> and <code>ifdown</code> use this file to note which network interfaces can be brought up and down. Thus, <code>ifup</code> can be easily confused when other networking tools are used to bring up an interface (e.g. <code>ifconfig</code>).</p>

<p>From <a href=""http://www.unix.com/man-page/Linux/8/ifup/"">man ifup</a></p>

<blockquote>
  <p>The program keeps records of whether network interfaces are up or
  down. Under  exceptional  circumstances these records can become
  inconsistent with the real states of the interfaces. For example,
  an interface that was  brought  up  using ifup and later
  deconfigured using <code>ifconfig</code> will still be recorded as up.  To fix
  this you can use the <code>--force</code> option to force <code>ifup</code>  or <code>ifdown</code> to
  run configuration or deconfiguration commands despite what it
  considers the current state of the interface to be.</p>
</blockquote>
","50605"
"rsync without prompt for password","131876","","<p>I need to execute <code>rsync</code>, without it prompting me for password.  </p>

<p>I've seen in <code>rsync</code> manpage that it doesn't allow specifying the password as command line argument.<br>
But I noticed that it allows specifying the password via the variable <code>RSYNC_PASSWORD</code>.</p>

<p>So I've tried exporting the variable, but <code>rsync</code> keeps asking me for password.  </p>

<pre><code>export RSYNC_PASSWORD=""abcdef""
rsync root@1.2.3.4:/abc /def
</code></pre>

<p>What am I doing wrong?</p>

<p><em>Please consider:</em></p>

<ul>
<li>I understand that this is a <strong>bad idea from security aspect</strong></li>
<li>I must use only <code>rsync</code>, can't use other software</li>
<li>I can't use key-based authentication</li>
<li>I've already read many SE question, e.g.:<br>
<a href=""https://stackoverflow.com/questions/3299951/how-to-pass-password-for-rsync-ssh-command"">how-to-pass-password-for-rsync-ssh-command @ stackoverflow.com</a><br>
<a href=""https://superuser.com/questions/325715/rsync-cron-job-with-a-password"">rsync-cron-job-with-a-password @ superuser.com</a><br>
<a href=""https://superuser.com/questions/555799/how-to-setup-rsync-without-password-with-ssh-on-unix-linux"">how-to-setup-rsync-without-password-with-ssh-on-unix-linux @ superuser.com</a>  </li>
</ul>

<p>In other words, I need to have the <code>RSYNC_PASSWORD</code> approach working! :-)</p>
","<p>This password environment variable appears only to be used when using the rsync protocol:</p>

<pre><code>rsync rsync://username@1.2.3.4:/abc /def
</code></pre>

<p>For this to work, you need to run rsync as a daemon as well (<code>--daemon</code> option), which is often done using <code>inetd.conf</code>.</p>

<p>When using this protocol, <code>abc</code> should correspond to a target defined in <code>/etc/rsyncd.conf</code>. The user name should be present in a <code>auth users</code> line for this target, and a password file should be specified with the <code>secrets file</code> option.</p>

<p>It is this secrets file that contains mappings between user names and passwords in the following format:</p>

<pre><code>username:password
</code></pre>

<p>And it is this password that you can specify using the RSYNC_PASSWORD environment variable.</p>
","111531"
"Uploading directories with sftp?","131850","","<p>I'm having some trouble uploading directories(which contain other directories a few levels deep) by sftp. I realize I could work around this by gzipping, but I don't see why that's necessary.</p>

<p>Anyway, I try</p>

<pre><code>sftp&gt; put bin/
Uploading bin/ to /home/earlz/blah/bin
bin/ is not a regular file
sftp&gt; put -r bin/
Uploading bin/ to /home/earlz/blah/bin
Couldn't canonicalise: No such file or directory
Unable to canonicalise path ""/home/earlz/blah/bin""
</code></pre>

<p>I think the last error message is completely stupid. So the directory doesn't exist? Why not create the directory? </p>

<p>Is there anyway around this issue with sftp, or should I just use scp? </p>
","<p><strong>CORRECTED</strong>: I initially claimed wrongly that OpenSSH did not support <code>put -r</code>.  It does, but it does it in a very strange way.  It seems to expect the destination directory to already exist, with the same name as the source directory.</p>

<pre><code>sftp&gt; put -r source
 Uploading source/ to /home/myself/source
 Couldn't canonicalize: No such file or directory
 etc.
sftp&gt; mkdir source
sftp&gt; put -r source
 Uploading source/ to /home/myself/source
 Entering source/
 source/file1
 source/file2
</code></pre>

<p>What's especially strange is that this even applies if you give a different name for the destination:</p>

<pre><code>sftp&gt; put -r source dest
 Uploading source/ to /home/myself/dest
 Couldn't canonicalize: ...
sftp&gt; mkdir dest
sftp&gt; put -r source dest
 Uploading source/ to /home/myself/dest/source
 Couldn't canonicalize: ...
sftp&gt; mkdir dest/source
sftp&gt; put -r source dest
 Uploading source/ to /home/myself/dest/source
 Entering source/
 source/file1
 source/file2
</code></pre>

<p>For a better-implemented recursive <code>put</code>, you could use the PuTTY <code>psftp</code> command line tool instead.  It's in the <code>putty-tools</code> package under Debian (and most likely Ubuntu).</p>

<p>Alternately, Filezilla will do what you want, if you want to use a GUI.</p>
","7008"
"How to display lines 2-4 after each grep result?","131679","","<p>I'm parsing a mailbox file that stores e-mail server reports for unsuccessfully delivered e-mail. I wish to extract bad e-mail addresses, so that I remove them from the system. The log file looks like this:</p>

<pre><code>...some content...
                   The mail system

&lt;slavicatomic118@hotmail.com&gt;: host mx1.hotmail.com[65.54.188.94] said: 550
    Requested action not taken: mailbox unavailable (in reply to RCPT TO
    command)

...some content...
                   The mail system

&lt;oki88@optimumpro.net&gt;: host viking.optimumpro.net[79.101.51.82] said: 550
    Unknown user (in reply to RCPT TO command)

...some content...
                   The mail system

&lt;sigirna_luka@yahoo.com&gt;: host mta5.am0.yahoodns.net[74.6.140.64] said: 554
    delivery error: dd This user doesn't have a yahoo.com account
    (sigirna_luka@yahoo.com) [0] - mta1172.mail.sk1.yahoo.com (in reply to end
    of DATA command)

...etc.
</code></pre>

<p>E-mail address comes 2 lines after a line with ""The mail system"". Using grep like this gives me the ""The mail system"" line and the next two lines:</p>

<pre><code>grep -A 2 ""The mail system"" mbox_file
</code></pre>

<p>However, I don't know how to remove the ""The mail system"" line and the second empty line from this output. I guess I could write PHP/Perl/Python script to do it, but I wonder if this is possible with grep or some other standard tool. I tried to give negative offset to -B parameter:</p>

<pre><code>grep -A 2 -B -2 ""The mail system"" mbox_file
</code></pre>

<p>But grep complains:</p>

<pre><code>grep: -2: invalid context length argument
</code></pre>

<p>Is there a way to do this with grep?</p>
","<p>The simplest way to solve it using <code>grep</code> only, is to pipe one more inverted <code>grep</code> at the end.
For example:</p>

<pre><code>grep -A 4 ""The mail system"" temp.txt | grep -v ""The mail system"" | grep -v '^\d*$'
</code></pre>
","31532"
"Detecting pattern at the end of a line with grep","131548","","<p>If I do:</p>

<pre><code>$ ls -R
.:
4Shared/  Cloud/

./4Shared:
UFAIZLV2R7.part3.rar

./Cloud:
UFAIZLV2R7.part2.rar.part
UFAIZLV2R7.part1.rar.part
UFAIZLV2R7.part4.rar.part
</code></pre>

<p>If I want to list the <code>.rar</code> files only, and I use <strong>grep</strong>, it will show me too the <code>.rar.part</code> files, what is not my wish.<br>
I am solving this using <code>find</code> or <code>ls **/*.rar</code> as told in <a href=""https://unix.stackexchange.com/questions/7169/what-is-the-linux-equivalent-of-dos-dir-s-b-filename"">this thread</a> and they work fine, but I would like to learn if it is possible to do it via <code>grep</code>.  </p>

<p>I have tried (thinking about <code>EOL</code>):</p>

<pre><code>ls -R | grep "".rar\n""
</code></pre>

<p>with no results.<br>
I think that the problem lies in discover if the greping is found <strong>at the end of the line</strong>, but I am not sure.  </p>

<p>Any help out here, please?</p>
","<p>The <code>$</code> anchor matches the end of a line.</p>

<pre><code>ls -R | grep '\.rar$'
</code></pre>

<p>You can also use <code>find</code> for this:</p>

<pre><code>find . -name '*.rar'
</code></pre>
","124463"
"Which is more widely used: chmod 777 or chmod a+rwx","131397","","<p>Out of the two options to change permissions:</p>

<ul>
<li><code>chmod 777 file.txt</code></li>
<li><code>chmod a+rwx file.txt</code></li>
</ul>

<p>I am writing a document to send out to our customers, detailing that they need to change the file permissions of a certain file. I want to detail it as the most common way of changing file permissions.</p>

<p>Currently is says:</p>

<pre><code>- Set permissions on file.txt as per the example below:
    - chmod 777 /tmp/file.txt
</code></pre>

<p>This is just an example, and won't change files to have full permissions for everyone.</p>
","<p>Google gives:</p>

<ul>
<li>1,030,000 results for '<a href=""https://www.google.com/search?q=chmod+777"">chmod 777</a>'</li>
<li>371,000 results for '<a href=""https://www.google.com/search?q=chmod+a%2Brwx"">chmod a+rwx</a>'</li>
</ul>

<p><code>chmod 777</code> is about 3 times more popular.</p>

<p>That said, I prefer using long options in documentation and scripts, because they are self-documenting.  If you are following up your instructions with ""Run <code>ls -l | grep file.txt</code> and verify permissions"", you may want to use <code>chmod a+rwx</code> because that's how ls will display the permissions.</p>
","153553"
"Recovering ext4 superblocks","130765","","<p>Recently, my external hard drive enclosure failed (the hard drive itself powers up in another enclosure). However, as a result, it appears its EXT4 file system is corrupt.</p>

<p>The drive has a single partition and uses a GPT partition table (with the label <code>ears</code>).</p>

<p><code>fdisk -l /dev/sdb</code> shows:</p>

<pre><code>   Device Boot      Start         End      Blocks   Id  System
     /dev/sdb1          1  1953525167   976762583+  ee  GPT
</code></pre>

<p><code>testdisk</code> shows the partition is intact:</p>

<pre><code>1 P MS Data                     2049 1953524952 1953522904 [ears]
</code></pre>

<p>... but the partition fails to mount:</p>

<pre><code>$ sudo mount /dev/sdb1 a
mount: you must specify the filesystem type
$ sudo mount -t ext4 /dev/sdb1 a 
mount: wrong fs type, bad option, bad superblock on /dev/sdb1,
</code></pre>

<p><code>fsck</code> reports an invalid superblock:</p>

<pre><code>$ sudo fsck.ext4 /dev/sdb1            
e2fsck 1.42 (29-Nov-2011)
fsck.ext4: Superblock invalid, trying backup blocks...
fsck.ext4: Bad magic number in super-block while trying to open /dev/sdb1
</code></pre>

<p>and <code>e2fsck</code> reports a similar error:</p>

<pre><code>$ sudo e2fsck /dev/sdb1        
Password: 
e2fsck 1.42 (29-Nov-2011)
e2fsck: Superblock invalid, trying backup blocks...
e2fsck: Bad magic number in super-block while trying to open /dev/sdb1
</code></pre>

<p><code>dumpe2fs</code> also:</p>

<pre><code>$ sudo dumpe2fs /dev/sdb1                      
dumpe2fs 1.42 (29-Nov-2011)
dumpe2fs: Bad magic number in super-block while trying to open /dev/sdb1
</code></pre>

<hr>

<p><code>mke2fs -n</code> (note, <code>-n</code>) returns the superblocks:</p>

<pre><code>$ sudo mke2fs -n /dev/sdb1       
mke2fs 1.42 (29-Nov-2011)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=0 blocks, Stripe width=0 blocks
61054976 inodes, 244190363 blocks
12209518 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=4294967296
7453 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks: 
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
    4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 
    102400000, 214990848
</code></pre>

<p>... but trying ""e2fsck -b [block]"" for each block fails:</p>

<pre><code>$ sudo e2fsck -b 71663616 /dev/sdb1 
e2fsck 1.42 (29-Nov-2011)
e2fsck: Invalid argument while trying to open /dev/sdb1
</code></pre>

<p>However as I understand, these are where the superblocks were when the filesystem was created, which does not necessarily mean they are still intact.</p>

<hr>

<p>I've also ran a <code>testdisk</code> <a href=""https://gist.github.com/1961186"">deep search</a> if anyone can decypher the log. It mentions many entry like:</p>

<pre><code>recover_EXT2: s_block_group_nr=1/7452, s_mnt_count=6/20,
s_blocks_per_group=32768, s_inodes_per_group=8192
recover_EXT2: s_blocksize=4096
recover_EXT2: s_blocks_count 244190363
recover_EXT2: part_size 1953522904
recover_EXT2: ""e2fsck -b 32768 -B 4096 device"" may be needed
</code></pre>

<p>Running e2fsck with those values gives:</p>

<pre><code>e2fsck: Bad magic number in super-block while trying to open /dev/sdb1
</code></pre>

<p>I tried that with all superblocks in the <code>testdisk.log</code></p>

<pre><code>for i in $(grep e2fsck testdisk.log | uniq | cut -d "" "" -f 4); do
   sudo e2fsck -b $i -B 4096 /dev/sdb1
done
</code></pre>

<p>... all with the same <code>e2fsck</code> error message.</p>

<hr>

<p>In my last attempt, I tried different filesystem offsets. For each offset <code>i</code>, where <code>i</code> is one of 31744, 32768, 1048064, 1049088:</p>

<pre><code>$ sudo losetup -v -o $i /dev/loop0 /dev/sdb
</code></pre>

<p>... and running <code>testdisk /dev/loop0</code>, I didn't find anything interesting.</p>

<hr>

<p>I've been fairly exhaustive, but is there <em>any way</em> to recover the file system without resorting to low-level file recovery tools (<code>foremost</code>/<code>photorec</code>)?</p>
","<p>Unfortunately, I was unable to recover the file system and had to resort to lower-level data recovery techniques (nicely summarised in Ubuntu's <a href=""https://help.ubuntu.com/community/DataRecovery"">Data Recovery</a> wiki entry), of which <a href=""http://www.sleuthkit.org/autopsy/desc.php"">Sleuth Kit</a> proved most useful.</p>

<p>Marking as answered for cleanliness' sake.</p>
","56952"
"How to list groups with gid in redhat?","130702","","<p>I first listed the groups using :</p>

<pre><code>groups 
</code></pre>

<p>I added group using </p>

<pre><code>groupadd -g 300 oinstall
groupadd –g 500 dba
</code></pre>

<p>and then when I do </p>

<pre><code>groups 
root bin daemon sys adm disk wheel sfcb
</code></pre>

<p>I am unable to find groups I added. How to list groups with group id?
Also, if I try to add it again it says groups is already present.</p>
","<p>The <code>groups</code> command lists groups that the user is currently a member of, not all the groups available on the system. You can lookup a group by name or gid using the <code>getent</code> command.</p>

<pre><code>getent group oinstall
getent group 500
</code></pre>

<p>To show all the groups, just leave your search query off of the command:</p>

<pre><code>getent group
</code></pre>
","97658"
"Do we have an undo in Linux?","129801","","<p>I renamed a few files in one batch script. Is there a way to undo the changes without having to rename them back?</p>

<p>Does Linux provide some native way of <code>undo</code>ing?</p>
","<p>Linux (like other unices) doesn't natively provide an undo feature. The philosophy is that if it's gone, it's gone. If it was important, it should have been backed up.</p>

<p>There is a fuse filesystem that automatically keeps copies of old versions: <a href=""http://boklm.eu/copyfs/"">copyfs</a>, available in all good distributions. Of course, that can use a lot of resources.</p>

<p>The best way to protect against such accidents is to use a version control system (cvs, bazaar, darcs, git, mercurial, subversion, ...). It takes a little time to learn, but it pays off awesomely in the medium and long term.</p>
","1004"
"How to remove a single line from history?","129712","","<p>I'm working in Mac OSX, so I guess I'm using bash...?</p>

<p>Sometimes I enter something that I don't want to be remembered in the history. How do I remove it?</p>
","<h1>Preventative measures</h1>

<p>If you want to run a command without saving it in history, prepend it with an extra space</p>

<pre><code>prompt$ echo saved
prompt$  echo not saved \
&gt; #     ^ extra space
</code></pre>

<p>For this to work you need either <code>ignorespace</code> or <code>ignoreboth</code> in <code>HISTCONTROL</code>.  For example, run</p>

<pre><code>HISTCONTROL=ignorespace
</code></pre>

<p>To make this setting persistent, put it in your <code>.bashrc</code>.</p>

<h1>Post-mortem clean-up</h1>

<p>If you've already run the command, and want to remove it from history, first use</p>

<pre><code>history
</code></pre>

<p>to display the list of commands in your history.  Find the number next to the one you want to delete (e.g. 1234) and run </p>

<pre><code>history -d 1234
</code></pre>
","49216"
"Keep SSH Sessions running after disconnection","129661","","<p>I sometimes have long running processes that I want to kick off before going home, so I create a SSH session to the server to start the process, but then I want to close my laptop and go home and later, after dinner, I want to check on the process that I started before leaving work. How can I do that with SSH? My understanding is that if you break your SSH connection you will also break your login session on the server, therefore killing the long running process.</p>
","<p>Use <code>nohup</code> to make your process ignore the hangup signal:</p>

<pre><code>$ nohup long-running-process &amp;
$ exit
</code></pre>
","488"
"How to disable SSLv3 in Apache?","129185","","<p>Everybody seems to be talking about the <a href=""https://security.stackexchange.com/questions/70719/ssl3-poodle-vulnerability"">POODLE vulnerability</a> today. And everybody recommends disabling SSLv3 in Apache using the following configuration directive:</p>

<pre><code>SSLProtocol All -SSLv2 -SSLv3
</code></pre>

<p>instead of the default</p>

<pre><code>SSLProtocol All -SSLv2
</code></pre>

<p>I've done that, and no joy – after testing repeatedly with various tools (<a href=""https://www.tinfoilsecurity.com/poodle"" rel=""noreferrer"">here's a fast one</a>), I find that SSLv3 is happily accepted by my server.</p>

<p>Yes, I did restart Apache. Yes, I did a recursive <code>grep</code> on all configuration files, and I don't have any override anywhere. And no, I'm not using some ancient version of Apache:</p>

<pre><code>[root@server ~]# apachectl -v
Server version: Apache/2.2.15 (Unix)
Server built:   Jul 23 2014 14:17:29
</code></pre>

<p>So, what gives? How does one <em>really</em> disable SSLv3 in Apache?</p>
","<p>I had the same problem...
You have to include <code>SSLProtocol all -SSLv2 -SSLv3</code> within every VirtualHost stanza in httpd.conf</p>

<p>The VirtualHost stanzas are generally towards the end of the httpd.conf file. So for example:</p>

<pre><code>...
...
&lt;VirtualHost your.website.example.com:443&gt;
    DocumentRoot /var/www/directory
    ServerName your.website.example.com

    ...
    SSLEngine on
    ...
    SSLProtocol all -SSLv2 -SSLv3
    ...
&lt;/VirtualHost&gt;
</code></pre>

<p>Also check ssl.conf or httpd-ssl.conf or similar because they may be set there, not necessarily in httpd.conf</p>
","162500"
"how to output text to both screen and file inside a shell script?","129148","","<p>Currently I have a shell script which logs messages to a log file like this:</p>

<pre><code>log_file=""/some/dir/log_file.log""
echo ""some text"" &gt;&gt; $log_file
do_some_command
echo ""more text"" &gt;&gt; $log_file
do_other_command
</code></pre>

<p>When executing this script, there is no output to screen, and, since I'm connecting to the server via putty, I have to open another connection and do ""tail -f log_file_path.log"", because I can't terminate the running script and I want to see the output in real time.</p>

<p>Obviously, what I want is that the text messages are printed on screen and into file, but I'd like to do it in one line, not two lines, one of which has no redirection to file.</p>

<p>How to achieve this?</p>
","<p>This works:</p>

<pre><code>command | tee -a ""$log_file""
</code></pre>

<p><code>tee</code> saves input to a file (use <code>-a</code> to append rather than overwrite), and copies the input to standard output as well.</p>
","80708"
"How to measure time of program execution and store that inside a variable","129064","","<p>In order to find out how long certain operations within a Bash (v4+) script take, I would like to parse the output from the <code>time</code> command ""separately"" and (ultimately) capture it within a Bash variable (<code>let VARNAME=...</code>).</p>

<p>Now, I am using <code>time -f '%e' ...</code> (or rather <code>command time -f '%e' ...</code> because of the Bash built-in), but since I already redirect the output of the executed command I'm really lost as to how I would go about to capture the output of the <code>time</code> command. Basically the problem here is <em>to separate the output</em> of <code>time</code> from the output of the executed command(s).</p>

<p>What I want is the <em>functionality</em> of counting the amount of time in seconds (integers) between starting a command and its completion. It doesn't have to be the <code>time</code> command or the respective built-in.</p>

<hr>

<p><em>Edit:</em> given the two useful answers below, I wanted to add two clarifications.</p>

<ol>
<li>I do not want to throw away the output of the executed command, but it will not really matter whether it ends up on stdout or stderr.</li>
<li>I would prefer a direct approach over an indirect one (i.e. catching output directly as opposed to store it in intermediate files).</li>
</ol>

<p>The solution using <code>date</code> so far comes closes to what I want.</p>
","<p>To get the output of <code>time</code> into a var use the following:</p>

<pre><code>usr@srv $ mytime=""$(time ( ls ) 2&gt;&amp;1 1&gt;/dev/null )""
usr@srv $ echo ""$mytime""

real    0m0.006s
user    0m0.001s
sys     0m0.005s
</code></pre>

<p>You can also just ask for a single time type, e.g. utime:</p>

<pre><code>usr@srv $ utime=""$( TIMEFORMAT='%lU';time ( ls ) 2&gt;&amp;1 1&gt;/dev/null )""
usr@srv $ echo ""$utime""
0m0.000s
</code></pre>

<p>To get the time you can also use <code>date +%s.%N</code>, so take it before and after execution and calculate the diff:</p>

<pre><code>START=$(date +%s.%N)
command
END=$(date +%s.%N)
DIFF=$(echo ""$END - $START"" | bc)
# echo $DIFF
</code></pre>

<p>Hope that helps.</p>
","12069"
"How to make log-rotate change take effect","128801","","<p>I followed <a href=""https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/s1-logfiles-locating.html"">this link</a> to change log-rotate configuration for RHEL 6</p>

<p>After I made the change to config file, what should I do to let this take effect?</p>
","<p><code>logrotate</code> uses <code>crontab</code> to work. It's scheduled work, not a daemon, so no need to reload its configuration.<br>
When the <code>crontab</code> executes <code>logrotate</code>, it will use your new config file automatically.<br>
If you need to test your config you can also execute <code>logrotate</code> on your own  with the command:  </p>

<pre><code>logrotate /etc/logrotate.d/your-logrotate-config
</code></pre>

<p><strike>Or as mentioned in comments, identify the <code>logrotate</code> line in the output of the command <code>crontab -l</code> and execute the command line</strike> refer to <a href=""https://unix.stackexchange.com/a/116141/53092"">slm's answer</a> to have a precise cron.daily explanation</p>
","116138"
"Linux: How to find the device driver used for a device?","128631","","<p>If my target has one device connected and many drivers for that device loaded, how can I understand what device is using which driver?</p>
","<p>Just use <code>/sys</code>.</p>

<p>Example. I want to find the driver for my Ethernet card:</p>

<pre><code>$ sudo lspci
...
02:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168B PCI Express Gigabit Ethernet controller (rev 01)
$ find /sys | grep drivers.*02:00
/sys/bus/pci/drivers/r8169/0000:02:00.0
</code></pre>

<p>That is <code>r8169</code>.</p>

<p>First I need to find coordinates of the device using <code>lspci</code>; then I find driver that is used for the devices with these coordinates.</p>
","41819"
"how can I make cron run a job right now, for testing/debugging? without changing the schedule!","128188","","<p>I have a cron job that is scheduled to run everyday, other than changing the schedule, is there any other way to do a test run of the command right now to see if it works as intended?  </p>
","<p>As far as i know there is no way to directly do that as cron has a special purpose - running schedules commands at a specific time. So the best thing is to either to  manually create a crontab entry or write a script which removes and resets the environment.</p>
","42720"
"How can I make a program executable from everywhere","127607","","<p>What should I do if I want to be able to run a given program regardless of my current directory? Should I create a symbolic link to the program in the <code>/bin</code> folder?</p>
","<p>If you just type <code>export PATH=$PATH:&lt;/path/to/file&gt;</code> at the command line it will only last for the length of the session.</p>

<p>If you want to change it permanently add <code>export PATH=$PATH:&lt;/path/to/file&gt;</code> to your ~/.bashrc file (just at the end is fine).</p>
","3820"
"Need to install glibc >= 2.14 on Wheezy","127262","","<p>I am trying to get <a href=""https://github.com/juliemr/protractor"">Protractor</a> working for performing e2e angular testing, but protractor requires <a href=""https://code.google.com/p/selenium/"">Selenium</a> which requires <a href=""https://code.google.com/p/chromedriver/"">ChromeDriver</a> which requires <code>glibc</code> 2.14. My current development box is running Debian Wheezy which comes with <code>glibc</code> 2.13. I have read that switching over to Debian's unstable branch would provide access to <code>glib-2.14</code>, but from what I have heard unstable is pretty...unstable.</p>

<p>Is there any way I can upgrade <code>glibc</code> to 2.14 or 2.15 without the risk of breaking everything?  Or is it possible to switch back from the unstable Debian branch if things start to break?</p>

<pre><code>12:15:22.784 INFO - Executing: [new session: {browserName=chrome}] at URL: /session)
12:15:22.796 INFO - Creating a new session for Capabilities [{browserName=chrome}]
/home/chris/projects/personal/woddy/client/selenium/chromedriver:     /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.15' not found (required by      /home/chris/projects/personal/woddy/client/selenium/chromedriver)
/home/chris/projects/personal/woddy/client/selenium/chromedriver: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.14' not found (required by /home/chris/projects/personal/woddy/client/selenium/chromedriver)
12:15:43.032 WARN - Exception thrown
java.util.concurrent.ExecutionException: org.openqa.selenium.WebDriverException:  java.lang.reflect.InvocationTargetException
</code></pre>
","<p>You don't have to switch to the <em>unstable</em> to get glib >= 2.14. In fact, the testing branch (now stable, or Jessie) has glib-2.17 which you can pick just adding the testing repository and launching:</p>

<pre><code>sudo apt-get install libc6-dev=2.17-7
</code></pre>

<p>or,</p>

<pre><code>sudo apt-get -t testing install libc6-dev
</code></pre>

<p>You can add the switch <code>--dry-run</code> to see what will being installed before hand. You can see the status of the glibc package in the <a href=""https://tracker.debian.org/pkg/eglibc"" rel=""nofollow"">Debian Package Tracker System</a> (Debian renamed eglibc package to simply <a href=""https://tracker.debian.org/pkg/glibc"" rel=""nofollow"">glibc</a> from Jessie onwards).</p>

<p>You can also just wait for Jessie <a href=""https://lists.debian.org/debian-devel-announce/2015/03/msg00016.html"" rel=""nofollow"">release on April 25</a>.</p>
","85510"
"configure: error: C compiler cannot create executables","127250","","<p>I am trying to upgrade apache 2.2.15 to 2.2.27. While running config.nice taken from apache2.2.15/build I am getting following error:</p>

<pre><code>checking whether the C compiler works... no
configure: error: in `/home/vkuser/httpd-2.2.27/srclib/apr':
configure: error: C compiler cannot create executables
</code></pre>

<p>I have tried to search online but no luck. I have also tested out c compiler by running a small test.c script and it runs fine. There were few solution given online like installing 'kernel-devel' package but it did not resolve issue. How can I get this to work? </p>

<p>Following is the config.log generated:</p>

<pre><code>    This file contains any messages produced by compilers while
    running configure, to aid debugging if configure makes a mistake.

    It was created by configure, which was
    generated by GNU Autoconf 2.67.  Invocation command line was

      $ ./configure --prefix=/opt/myapp/apache2.2 --with-mpm=worker --enable-static-support --enable-ssl=static --enable-modules=most --disable-authndbd --disable-authn-dbm --disable-dbd --enable-static-logresolve --enable-static-rotatelogs --enable-proxy=static --enable-proxyconnect=static --enable-proxy-ftp=static --enable-proxy-http=static --enable-rewrite=static --enable-so=static --with-ssl=/opt/myapp/apache2.2/openssl --host=x86_32-unknown-linux-gnu host_alias=x86_32-unknown-linux-gnu CFLAGS=-m32 LDFLAGS=-m32 --with-included-apr

    ## --------- ##
    ## Platform. ##
    ## --------- ##

    hostname = dmcpq-000
    uname -m = x86_64
    uname -r = 2.6.18-348.12.1.el5
    uname -s = Linux
    uname -v = #1 SMP Mon Jul 1 17:54:12 EDT 2013

    /usr/bin/uname -p = unknown
    /bin/uname -X     = unknown

    /bin/arch              = x86_64
    /usr/bin/arch -k       = unknown
    /usr/convex/getsysinfo = unknown
    /usr/bin/hostinfo      = unknown
    /bin/machine           = unknown
    /usr/bin/oslevel       = unknown
    /bin/universe          = unknown

    PATH: /opt/myapp/Entrust/GetAccess/Runtime/Apache22/bin
    PATH: /usr/kerberos/sbin
    PATH: /usr/kerberos/bin
    PATH: /usr/local/sbin
    PATH: /usr/local/bin
    PATH: /sbin
    PATH: /bin
    PATH: /usr/sbin
    PATH: /usr/bin
    PATH: /root/bin


    ## ----------- ##
    ## Core tests. ##
    ## ----------- ##

    configure:2793: checking for chosen layout
    configure:2795: result: Apache
    configure:3598: checking for working mkdir -p
    configure:3614: result: yes
    configure:3629: checking build system type
    configure:3643: result: x86_64-unknown-linux-gnu
    configure:3663: checking host system type
    configure:3676: result: x86_32-unknown-linux-gnu
    configure:3696: checking target system type
    configure:3709: result: x86_32-unknown-linux-gnu

    ## ---------------- ##
    ## Cache variables. ##
    ## ---------------- ##

    ac_cv_build=x86_64-unknown-linux-gnu
    ac_cv_env_CC_set=
    ac_cv_env_CC_value=
    ac_cv_env_CFLAGS_set=set
    ac_cv_env_CFLAGS_value=-m32
    ac_cv_env_CPPFLAGS_set=
    ac_cv_env_CPPFLAGS_value=
    ac_cv_env_CPP_set=
    ac_cv_env_CPP_value=
    ac_cv_env_LDFLAGS_set=set
    ac_cv_env_LDFLAGS_value=-m32
    ac_cv_env_LIBS_set=
    ac_cv_env_LIBS_value=
    ac_cv_env_build_alias_set=
    ac_cv_env_build_alias_value=
    ac_cv_env_host_alias_set=set
    ac_cv_env_host_alias_value=x86_32-unknown-linux-gnu
    ac_cv_env_target_alias_set=
    ac_cv_env_target_alias_value=
    ac_cv_host=x86_32-unknown-linux-gnu
    ac_cv_mkdir_p=yes
    ac_cv_target=x86_32-unknown-linux-gnu

    ## ----------------- ##
    ## Output variables. ##
    ## ----------------- ##

    APACHECTL_ULIMIT=''
    APR_BINDIR=''
    APR_CONFIG=''
    APR_INCLUDEDIR=''
    APR_VERSION=''
    APU_BINDIR=''
    APU_CONFIG=''
    APU_INCLUDEDIR=''
    APU_VERSION=''
    AP_BUILD_SRCLIB_DIRS=''
    AP_CLEAN_SRCLIB_DIRS=''
    AP_LIBS=''
    AWK=''
    BUILTIN_LIBS=''
    CC=''
    CFLAGS='-m32'
    CORE_IMPLIB=''
    CORE_IMPLIB_FILE=''
    CPP=''
    CPPFLAGS=''
    CRYPT_LIBS=''
    CXX=''
    CXXFLAGS=''
    DEFS=''
    DSO_MODULES=''
    ECHO_C=''
    ECHO_N='-n'
    ECHO_T=''
    EGREP=''
    EXEEXT=''
    EXTRA_CFLAGS=''
    EXTRA_CPPFLAGS=''
    EXTRA_CXXFLAGS=''
    EXTRA_INCLUDES=''
    EXTRA_LDFLAGS=''
    EXTRA_LIBS=''
    GREP=''
    HTTPD_LDFLAGS=''
    HTTPD_VERSION=''
    INCLUDES=''
    INSTALL=''
    INSTALL_DSO=''
    INSTALL_PROG_FLAGS=''
    LDFLAGS='-m32'
    LIBOBJS=''
    LIBS=''
    LIBTOOL=''
    LN_S=''
    LTCFLAGS=''
    LTFLAGS=''
    LTLIBOBJS=''
    LT_LDFLAGS=''
    LYNX_PATH=''
    MKDEP=''
    MKINSTALLDIRS=''
    MK_IMPLIB=''
    MODULE_CLEANDIRS=''
    MODULE_DIRS=''
    MOD_ACTIONS_LDADD=''
    MOD_ALIAS_LDADD=''
    MOD_ASIS_LDADD=''
    MOD_AUTHNZ_LDAP_LDADD=''
    MOD_AUTHN_ALIAS_LDADD=''
    MOD_AUTHN_ANON_LDADD=''
    MOD_AUTHN_DBD_LDADD=''
    MOD_AUTHN_DBM_LDADD=''
    MOD_AUTHN_DEFAULT_LDADD=''
    MOD_AUTHN_FILE_LDADD=''
    MOD_AUTHZ_DBM_LDADD=''
    MOD_AUTHZ_DEFAULT_LDADD=''
    MOD_AUTHZ_GROUPFILE_LDADD=''
    MOD_AUTHZ_HOST_LDADD=''
    MOD_AUTHZ_OWNER_LDADD=''
    MOD_AUTHZ_USER_LDADD=''
    MOD_AUTH_BASIC_LDADD=''
    MOD_AUTH_DIGEST_LDADD=''
    MOD_AUTOINDEX_LDADD=''
    MOD_BUCKETEER_LDADD=''
    MOD_CACHE_LDADD=''
    MOD_CASE_FILTER_IN_LDADD=''
    MOD_CASE_FILTER_LDADD=''
    MOD_CERN_META_LDADD=''
    MOD_CGID_LDADD=''
    MOD_CGI_LDADD=''
    MOD_CHARSET_LITE_LDADD=''
    MOD_DAV_FS_LDADD=''
    MOD_DAV_LDADD=''
    MOD_DAV_LOCK_LDADD=''
    MOD_DBD_LDADD=''
    MOD_DEFLATE_LDADD=''
    MOD_DIR_LDADD=''
    MOD_DISK_CACHE_LDADD=''
    MOD_DUMPIO_LDADD=''
    MOD_ECHO_LDADD=''
    MOD_ENV_LDADD=''
    MOD_EXAMPLE_LDADD=''
    MOD_EXPIRES_LDADD=''
    MOD_EXT_FILTER_LDADD=''
    MOD_FILE_CACHE_LDADD=''
    MOD_FILTER_LDADD=''
    MOD_HEADERS_LDADD=''
    MOD_HTTP_LDADD=''
    MOD_IDENT_LDADD=''
    MOD_IMAGEMAP_LDADD=''
    MOD_INCLUDE_LDADD=''
    MOD_INFO_LDADD=''
    MOD_ISAPI_LDADD=''
    MOD_LDAP_LDADD=''
    MOD_LOGIO_LDADD=''
    MOD_LOG_CONFIG_LDADD=''
    MOD_LOG_FORENSIC_LDADD=''
    MOD_MEM_CACHE_LDADD=''
    MOD_MIME_LDADD=''
    MOD_MIME_MAGIC_LDADD=''
    MOD_NEGOTIATION_LDADD=''
    MOD_OPTIONAL_FN_EXPORT_LDADD=''
    MOD_OPTIONAL_FN_IMPORT_LDADD=''
    MOD_OPTIONAL_HOOK_EXPORT_LDADD=''
    MOD_OPTIONAL_HOOK_IMPORT_LDADD=''
    MOD_PROXY_AJP_LDADD=''
    MOD_PROXY_BALANCER_LDADD=''
    MOD_PROXY_CONNECT_LDADD=''
    MOD_PROXY_FTP_LDADD=''
    MOD_PROXY_HTTP_LDADD=''
    MOD_PROXY_LDADD=''
    MOD_PROXY_SCGI_LDADD=''
    MOD_REQTIMEOUT_LDADD=''
    MOD_REWRITE_LDADD=''
    MOD_SETENVIF_LDADD=''
    MOD_SO_LDADD=''
    MOD_SPELING_LDADD=''
    MOD_SSL_LDADD=''
    MOD_STATUS_LDADD=''
    MOD_SUBSTITUTE_LDADD=''
    MOD_SUEXEC_LDADD=''
    MOD_UNIQUE_ID_LDADD=''
    MOD_USERDIR_LDADD=''
    MOD_USERTRACK_LDADD=''
    MOD_VERSION_LDADD=''
    MOD_VHOST_ALIAS_LDADD=''
    MPM_LIB=''
    MPM_NAME=''
    MPM_SUBDIR_NAME=''
    NONPORTABLE_SUPPORT=''
    NOTEST_CFLAGS=''
    NOTEST_CPPFLAGS=''
    NOTEST_CXXFLAGS=''
    NOTEST_LDFLAGS=''
    NOTEST_LIBS=''
    OBJEXT=''
    OS=''
    OS_DIR=''
    OS_SPECIFIC_VARS=''
    PACKAGE_BUGREPORT=''
    PACKAGE_NAME=''
    PACKAGE_STRING=''
    PACKAGE_TARNAME=''
    PACKAGE_URL=''
    PACKAGE_VERSION=''
    PATH_SEPARATOR=':'
    PCRE_CONFIG=''
    PICFLAGS=''
    PILDFLAGS=''
    PKGCONFIG=''
    PORT=''
    POST_SHARED_CMDS=''
    PRE_SHARED_CMDS=''
    RANLIB=''
    RM=''
    RSYNC=''
    SHELL='/bin/sh'
    SHLIBPATH_VAR=''
    SHLTCFLAGS=''
    SH_LDFLAGS=''
    SH_LIBS=''
    SH_LIBTOOL=''
    SSLPORT=''
    SSL_LIBS=''
    UTIL_LDFLAGS=''
    ab_LTFLAGS=''
    abs_srcdir=''
    ac_ct_CC=''
    ap_make_delimiter=''
    ap_make_include=''
    bindir='${exec_prefix}/bin'
    build='x86_64-unknown-linux-gnu'
    build_alias=''
    build_cpu='x86_64'
    build_os='linux-gnu'
    build_vendor='unknown'
    cgidir='${datadir}/cgi-bin'
    checkgid_LTFLAGS=''
    datadir='${prefix}'
    datarootdir='${prefix}/share'
    docdir='${datarootdir}/doc/${PACKAGE}'
    dvidir='${docdir}'
    errordir='${datadir}/error'
    exec_prefix='${prefix}'
    exp_bindir='/opt/myapp/apache2.2/bin'
    exp_cgidir='/opt/myapp/apache2.2/cgi-bin'
    exp_datadir='/opt/myapp/apache2.2'
    exp_errordir='/opt/myapp/apache2.2/error'
    exp_exec_prefix='/opt/myapp/apache2.2'
    exp_htdocsdir='/opt/myapp/apache2.2/htdocs'
    exp_iconsdir='/opt/myapp/apache2.2/icons'
    exp_includedir='/opt/myapp/apache2.2/include'
    exp_installbuilddir='/opt/myapp/apache2.2/build'
    exp_libdir='/opt/myapp/apache2.2/lib'
    exp_libexecdir='/opt/myapp/apache2.2/modules'
    exp_localstatedir='/opt/myapp/apache2.2'
    exp_logfiledir='/opt/myapp/apache2.2/logs'
    exp_mandir='/opt/myapp/apache2.2/man'
    exp_manualdir='/opt/myapp/apache2.2/manual'
    exp_proxycachedir='/opt/myapp/apache2.2/proxy'
    exp_runtimedir='/opt/myapp/apache2.2/logs'
    exp_sbindir='/opt/myapp/apache2.2/bin'
    exp_sysconfdir='/opt/myapp/apache2.2/conf'
    host='x86_32-unknown-linux-gnu'
    host_alias='x86_32-unknown-linux-gnu'
    host_cpu='x86_32'
    host_os='linux-gnu'
    host_vendor='unknown'
    htcacheclean_LTFLAGS=''
    htdbm_LTFLAGS=''
    htdigest_LTFLAGS=''
    htdocsdir='${datadir}/htdocs'
    htmldir='${docdir}'
    htpasswd_LTFLAGS=''
    httxt2dbm_LTFLAGS=''
    iconsdir='${datadir}/icons'
    includedir='${prefix}/include'
    infodir='${datarootdir}/info'
    installbuilddir='${datadir}/build'
    libdir='${exec_prefix}/lib'
    libexecdir='${exec_prefix}/modules'
    localedir='${datarootdir}/locale'
    localstatedir='${prefix}'
    logfiledir='${localstatedir}/logs'
    logresolve_LTFLAGS=''
    mandir='${prefix}/man'
    manualdir='${datadir}/manual'
    nonssl_listen_stmt_1=''
    nonssl_listen_stmt_2=''
    oldincludedir='/usr/include'
    other_targets=''
    pdfdir='${docdir}'
    perlbin=''
    prefix='/opt/myapp/apache2.2'
    progname=''
    program_transform_name='s,x,x,'
    proxycachedir='${localstatedir}/proxy'
    psdir='${docdir}'
    rel_bindir='bin'
    rel_cgidir='cgi-bin'
    rel_datadir=''
    rel_errordir='error'
    rel_exec_prefix=''
    rel_htdocsdir='htdocs'
    rel_iconsdir='icons'
    rel_includedir='include'
    rel_installbuilddir='build'
    rel_libdir='lib'
    rel_libexecdir='modules'
    rel_localstatedir=''
    rel_logfiledir='logs'
    rel_mandir='man'
    rel_manualdir='manual'
    rel_proxycachedir='proxy'
    rel_runtimedir='logs'
    rel_sbindir='bin'
    rel_sysconfdir='conf'
    rotatelogs_LTFLAGS=''
    runtimedir='${localstatedir}/logs'
    sbindir='${exec_prefix}/bin'
    shared_build=''
    sharedstatedir='${prefix}/com'
    sysconfdir='${prefix}/conf'
    target='x86_32-unknown-linux-gnu'
    target_alias=''
    target_cpu='x86_32'
    target_os='linux-gnu'
    target_vendor='unknown'

    configure: exit 1
</code></pre>
","<p>From the output you've given, you are trying to compile a 32-bit build of apache on a 64 bit system.  This is from the intput to configure here:</p>

<pre><code>--host=x86_32-unknown-linux-gnu host_alias=x86_32-unknown-linux-gnu CFLAGS=-m32 LDFLAGS=-m32
</code></pre>

<p>Also see the output lines confirming this:</p>

<pre><code>configure:3629: checking build system type
configure:3643: result: x86_64-unknown-linux-gnu
configure:3663: checking host system type
configure:3676: result: x86_32-unknown-linux-gnu
configure:3696: checking target system type
configure:3709: result: x86_32-unknown-linux-gnu
</code></pre>

<p>Here it is using a 64 bit build system but a 32 bit host/target.  Further down we see:</p>

<pre><code>ac_cv_env_CFLAGS_set=set
ac_cv_env_CFLAGS_value=-m32
</code></pre>

<p>This flag tells gcc to produce 32 bit objects.  Your error that the C compiler cannot produce executable is likely caused by not having a 32 bit toolchain present.</p>

<h3>Testing your ability to compile 32 bit objects</h3>

<p>You can test this by compiling a small C example with the <code>-m32</code> flag.  </p>

<pre><code>// Minimal C example
#include &lt;stdio.h&gt;
int main()
{
   printf(""This works\n"");
   return 0;
}
</code></pre>

<p>Compiling:</p>

<pre><code>gcc -m32 -o m32test m32test.c
</code></pre>

<p>If this command fails, then you have a problem with your compiler being able to build 32 bit objects.  The error messages emitted from the compiler may be helpful in remedying this.  </p>

<h3>Remedies</h3>

<ol>
<li>Build for a 64 bit target (by removing the configure options forcing a 32 bit build), or</li>
<li>Install a 32 bit compiler toolchain</li>
</ol>
","146406"
"List partition labels from the command line","127075","","<p>Is there a command that will list all partitions along with their labels? <code>sudo fdisk -l</code> and <code>sudo parted -l</code> don't show labels by default.</p>

<p>EDIT: (as per comment below) I'm talking about ext2 labels - those that you can set in <code>gparted</code> upon partitioning.</p>

<p>EDIT2: The intent is to list <em>unmounted</em> partitions (so I know which one to mount).</p>
","<p>With udev, You can use</p>

<pre><code>ls -l /dev/disk/by-label
</code></pre>

<p>to show the symlinks by label to at least <em>some</em> partition device nodes.</p>

<p>Not sure what the logic of inclusion is, possibly the existence of a label.</p>
","14166"
"Connect to a Bluetooth device via Terminal","126751","","<p>I'm using Mint 15 w/ Cinnamon.</p>

<p>I bought a set of bluetooth speakers and I'm trying to connect to them via terminal. Via the GUI I can see them normally and I am connected to them. I want to make a small script so every time they are visible I would connect to them automatically. </p>

<p>I am trying to scan them with:</p>

<blockquote>
  <p>hcitool scan</p>
</blockquote>

<p>But I get </p>

<blockquote>
  <p>Scanning...</p>
</blockquote>

<p>and after a few seconds the process dies.</p>

<p>The same thing with <code>hidd --search</code>.</p>

<p>If I run <code>hciconfig scan</code> I get:</p>

<pre><code>hci0:   Type: BR/EDR  Bus: USB
    BD Address: 40:2C:F4:78:E8:69  ACL MTU: 1021:8  SCO MTU: 64:1
    UP RUNNING PSCAN ISCAN 
    RX bytes:130700 acl:22 sco:0 events:18527 errors:0
    TX bytes:31875398 acl:36784 sco:0 commands:75 errors:0
</code></pre>

<p>I suppose that is just saying my bluetooth address and that it is turned on.</p>

<p>As I said already, via the normal User Interface, I can see the speakers and I am connected to them, but through terminal I get nothing.</p>

<p>Actually it is quite funny that <code>hcitool scan</code> isn't finding anything since my speakers are connected and every time I run the command the sound from the speakers breaks for a couple of seconds.</p>
","<p>I managed to do so via <a href=""https://code.google.com/p/bluez-tools/"" rel=""noreferrer"">bluez-tools</a>:</p>

<p><code>sudo apt-get install bluez-tools</code></p>

<p>List of devices to get the MAC address of my device:</p>

<p><code>bt-device -l</code></p>

<p>and successfully connect to it:</p>

<p><code>bt-audio -c 01:02:03:04:05:06</code></p>

<hr />

<p>Keep in mind that the <a href=""https://github.com/khvzak/bluez-tools/issues/13"" rel=""noreferrer""><code>bt-audio</code> feature is being reimplemented for newer versions.</a></p>
","101422"
"How can I know if IP Multicast is enabled","126388","","<p>I have scripts that run IP multicast tests; however, my scripts are failing on a particular linux machine.</p>

<p>I know that I can look at <code>CONFIG_IP_MULTICAST</code> in the kernel configuration file to determine whether the kernel was compiled with this.  However, it would be easier to flag missing requirements in my script if I could look at <code>/proc</code> or <code>sysctl</code> and get the answer.</p>

<p>Is there a way to find if IP Multicast was compiled into the kernel without looking at <code>CONFIG_IP_MULTICAST</code>?</p>
","<p>It seems the most kernels(post v1) do support multicast by default or have CONFIG_IP_MULTICAST enabled while compiling. To check whether the compiled and running kernel subscribes to any multicast group, I would use <code>netstat -g</code>.</p>

<p>It seems the most kernels(post v1) do support multicast by default.</p>

<pre><code>[root@centos module]# grep CONFIG_IP_MULTICAST /usr/src/kernels/2.6.18-274.7.1.el5-i686/.config 
CONFIG_IP_MULTICAST=y
[root@centos module]# netstat -g
IPv6/IPv4 Group Memberships
Interface       RefCnt Group
--------------- ------ ---------------------
lo              1      all-systems.mcast.net
eth0            1      224.0.0.251
eth0            1      all-systems.mcast.net
</code></pre>

<p>I see that eth0 interface on my host thinks its subscribed 224.0.0.251 basic mcast group . Also alternatively, if I ping the group or the network and then all the known multicast enabled host networks are returned to me, I would say multicast is working on the host. 
Can you try <code>netstat -g</code> or <code>cat /proc/net/igmp</code> and see if it returns any groups on your host?</p>
","25874"
"How to remove duplicate lines inside a text file?","126321","","<p>A huge (up to 2 GiB) text file of mine contains about 100 exact duplicates of every line in it (useless in my case, as the file is a CSV-like data table).</p>

<p>What I need is to remove all the repetitions while (preferably, but this can be sacrificed for a significant performance boost) maintaining the original sequence order. In the result each line is to be unique. If there were 100 equal lines (usually the duplicates are spread across the file and won't be neighbours) there is to be only one of the kind left.</p>

<p>I have written a program in Scala (consider it Java if you don't know about Scala) to implement this. But maybe there are faster C-written native tools able to do this faster?</p>
","<p>An <code>awk</code> solution seen on #bash (Freenode):</p>

<pre><code>awk '!seen[$0]++' filename
</code></pre>
","30178"
"How do I know if dd is still working?","126271","","<p>I've not used <code>dd</code> all that much, but so far it's not failed me yet. Right now, I've had a <code>dd</code> going for over 12 hours - I'm writing an image back to the disk it came from - and I'm getting a little worried, as I was able to <code>dd</code> from the disk to the image in about 7 hours. </p>

<p>I'm running OSX 10.6.6 on a MacBook with a Core 2 Duo at 2.1ghz/core with 4gb RAM. I'm reading from a .dmg on a 7200rpm hard drive (the boot drive), and I'm writing to a 7200rpm drive connected over a SATA-to-USB connector. I left the blocksize at default, and the image is about 160gb.</p>

<p>EDIT: And, after 14 hours of pure stress, the <code>dd</code> worked perfectly after all. Next time, though, I'm going to run it through <code>pv</code> and track it with <code>strace</code>. Thanks to everyone for all your help.</p>
","<p>You can send <a href=""http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man1/dd.1.html"" rel=""noreferrer""><code>dd</code></a> a certain signal using the <code>kill</code> command to make it output its current status. The signal is <code>INFO</code> on BSD systems (including OSX) and <code>USR1</code> on Linux. In your case:</p>

<pre><code>kill -INFO $PID
</code></pre>

<p>You can find the process id (<code>$PID</code> above) with the <code>ps</code> command; or see <a href=""https://unix.stackexchange.com/questions/225/pgrep-and-pkill-alternatives-on-mac-os-x"">pgrep and pkill alternatives on mac os x</a> for more convenient methods.</p>

<p>More simply, as <a href=""https://unix.stackexchange.com/users/22308/antoineg"">AntoineG</a> points out in <a href=""https://unix.stackexchange.com/a/59076/977"">his answer</a>, you can type <code>ctrl-shift-T</code> at the shell running dd to send it the <code>INFO</code> signal.</p>

<p>As an example on Linux, you could make all active <code>dd</code> processes output status like this:</p>

<pre><code>pkill -USR1 -x dd
</code></pre>

<p>After outputting its status, <code>dd</code> will continue coping.</p>
","11264"
"Appending a current date from a variable to a filename","125920","","<p>I'm trying to append the current date to the end of a file name like this:</p>

<pre><code>TheFile.log.2012-02-11
</code></pre>

<p>Here is what I have so far:</p>

<pre><code>set today = 'date +%Y'
mkdir -p The_Logs &amp;
find . -name The_Logs -atime -1 -type d -exec mv \{} ""The_Logs_+$today"" \; &amp;
</code></pre>

<p>However all I get is the name of the file, and it appends nothing.  How do I append a current date to a filename?</p>
","<p>More than likely it is your use of <code>set</code>.  That will assign 'today', '=' and the output of the <code>date</code> program to positional parameters (aka command-line arguments).  You want to just use C shell (which you are tagging this as ""bash"", so likely not), you will want to use:</p>

<pre><code>today=`date +%Y-%m-%d.%H:%M:%S` # or whatever pattern you desire
</code></pre>

<p>Notice the lack of spaces around the equal sign.</p>

<p>You also do not want to use <code>&amp;</code> at the end of your statements; which causes the shell to not wait for the command to finish.  Especially when one relies on the next.  The <code>find</code> command could fail because it is started before the <code>mkdir</code>. </p>
","57592"
"How to delete directories based on `find` output?","125845","","<p>I issue the following command to find the .svn directories:</p>

<pre><code>find . -name "".svn""
</code></pre>

<p>That gives me the following results:</p>

<pre><code>./toto/.svn
./toto/titi/.svn
./toto/tata/.svn
</code></pre>

<p>How could I process all these lines with <code>rm -fr</code> in order to delete the directories and their content?</p>
","<p>Find can execute arguments with the <code>-exec</code> option for each match it finds. It is a recommended mechanism because you can handle paths with spaces/newlines and other characters in them correctly. You will have to delete the contents of the directory before you can remove the directory itself, so use <code>-r</code> with the <code>rm</code> command to achieve this.</p>

<p>For your example you can issue:</p>

<pre><code>find . -name "".svn"" -exec rm -r ""{}"" \;
</code></pre>

<p>You can also tell find to just find <em>directories</em> named .svn by adding a <code>-type d</code> check:</p>

<pre><code>find . -name "".svn"" -type d -exec rm -r ""{}"" \;
</code></pre>

<p><strong>Warning</strong> Use <code>rm -r</code> with caution it deletes the folder and all its contents.</p>

<p>If you want to delete just empty directories as well as directories that contain only empty directories, find can do that itself with <code>-delete</code> and <code>-empty</code>:</p>

<pre><code>find . -name "".svn"" -type d -empty -delete
</code></pre>
","89929"
"Specify identity file (id_rsa) with rsync","125667","","<p>I need to make periodic backups of a directory on a remote server which is a virtual machine hosted by a research organisation. They mandate that access to VMs is through ssh keys, which is all good, except that I can't figure out how to point rsync to the ssh key for this server. </p>

<p>Rsync has no problem if the key file is <code>~/.ssh/id_rsa</code>, but when it is something else I get <code>Permission denied (publickey)</code>.</p>

<p>With ssh I can specify the identity file with <code>-i</code>, but rsync appears to have no such option. </p>

<p>I have also tried temporarily moving the key on the local machine to <code>~/.ssh/id_rsa</code>, but that similarly does not work.</p>

<p><strong>tl;dr</strong></p>

<p>Can you specify an identity file with rsync?</p>
","<p>You can specify the exact ssh command via the '-e' option:</p>

<pre><code>rsync -Pav -e ""ssh -i $HOME/.ssh/somekey"" username@hostname:/from/dir/ /to/dir/
</code></pre>

<p>Many ssh users are unfamiliar with their ~/.ssh/config file. You can specify default settings per host via the config file.</p>

<pre><code>Host hostname
    User username
    IdentityFile ~/.ssh/somekey
</code></pre>

<p>In the long run it is best to learn the ~/.ssh/config file.</p>
","127355"
"how do you sort du output by size?","124958","","<p>How do you sort <code>du -sh /dir/*</code> by size? I read one site that said use <code>| sort -n</code> but that's obviously not right. Here's an example that is wrong.</p>

<pre><code>[~]# du -sh /var/* | sort -n
0       /var/mail
1.2M    /var/www
1.8M    /var/tmp
1.9G    /var/named
2.9M    /var/run
4.1G    /var/log
8.0K    /var/account
8.0K    /var/crash
8.0K    /var/cvs
8.0K    /var/games
8.0K    /var/local
8.0K    /var/nis
8.0K    /var/opt
8.0K    /var/preserve
8.0K    /var/racoon
12K     /var/aquota.user
12K     /var/portsentry
16K     /var/ftp
16K     /var/quota.user
20K     /var/yp
24K     /var/db
28K     /var/empty
32K     /var/lock
84K     /var/profiles
224M    /var/netenberg
235M    /var/cpanel
245M    /var/cache
620M    /var/lib
748K    /var/spool
</code></pre>
","<p>If you have GNU coreutils (common in most Linux distributions), you can use <code>du -sh * | sort -h</code>. The <code>-h</code> option tells <code>sort</code> that the input is the human-readable format (number with unit).</p>

<p>This feature <a href=""http://savannah.gnu.org/patch/?func=detailitem&amp;item_id=2565"">was added to GNU Core Utilities 7.5 in Aug 2009</a>.</p>

<blockquote>
  <p><strong>Note:</strong></p>
  
  <p>If you are using Mac OSX, you need install coreutils with
  <code>brew install coreutils</code>， then use <code>gsort</code> as drop-in replacement of <code>sort</code></p>
</blockquote>
","4682"
"Convince apt-get *not* to use IPv6 method","124530","","<p>The ISP I work at is setting up an internal IPv6 network in preparation for eventually connecting to the IPv6 internet. As a result, several of the servers in this network now try to connect to security.debian.org via its IPv6 address by default when running <code>apt-get update</code>, and that results in having to wait for a lengthy timeout whenever I'm downloading updates of any sort.  </p>

<p>Is there a way to tell apt to either prefer IPv4 or ignore IPv6 altogether?</p>
","<p>Add <code>-o Acquire::ForceIPv4=true</code> when running <code>apt-get</code>.</p>

<p>If you want to make the setting persistent just create <em>/etc/apt/apt.conf.d/99force-ipv4</em> and put <code>Acquire::ForceIPv4 ""true"";</code> in it:</p>

<pre><code>echo 'Acquire::ForceIPv4 ""true"";' | sudo tee /etc/apt/apt.conf.d/99force-ipv4
</code></pre>

<p>Config options <code>Acquire::ForceIPv4</code> and <code>Acquire::ForceIPv6</code> were added to version <em>0.9.7.9~exp1</em> (see <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=611891#35"">bug 611891</a>) which is available since Ubuntu Saucy (released in October 2013) and Debian Jessie (released in April 2015).</p>
","100887"
"Execute a command once per line of piped input?","124417","","<p>I want to run a java command once for every match of <code>ls | grep pattern -</code>. In this case, I think I could do <code>find pattern -exec java MyProg '{}' \;</code> but I'm curious about the general case - is there an easy way to say ""run a command once for every line of standard input""? (In fish or bash.)</p>
","<p>That's what <code>xargs</code> does.</p>

<pre><code>... | xargs command
</code></pre>
","7560"
"Best way to mount remote folder","124347","","<p>I have two RasberryPi running debian wheezy and I would like to mount a folder from computer A on computer B.</p>

<p>What is the best (as in most efficient) way to do this?</p>

<p>I can do it via SMB, but that is for windows, I think there must be a better way to share across linux.</p>
","<p>You can use plenty of things, among which, popular options are:</p>

<ul>
<li>NFS</li>
<li>Samba / CIFS</li>
<li>SSHFS</li>
</ul>

<p>By ease-of-setup I think they would have to be put in this order (top: easiest)</p>

<p><strong>SSHFS</strong></p>

<p>Through FUSE, you can mount remote filesystems via ssh. I won't cover how, as Cristopher has already very well explained that. Just note that, in order to mount the file automatically it will need a <a href=""http://www.tjansson.dk/?p=84"">bit more of work</a>.</p>

<p><strong>Samba</strong></p>

<p>It will allow you to use Windows and Unix machines to access the remote folder. If it's not a big deal for you, then you won't probably benefit from it. However, it's easy to automount it on init (just input the apropriate values at <code>/etc/fstab</code>, including <code>username=&lt;your-samba-username&gt;,password=&lt;your-samba-password&gt;</code> in the options column.</p>

<p><strong>NFS</strong></p>

<p>It will let you authenticate just via IP (no usernames thing = faster, only of use inside your <em>non-hostile</em> LAN) or via Kerberos Tickets (too painful for just two Raspberries; but useful in corporate environments).</p>

<p>As it has kernel mode support, it will run faster than sshfs. Besides, as there's <a href=""http://nfs.sourceforge.net/nfs-howto/ar01s06.html"">no encryption performed</a> it will have a better throughput, and in the case of the tiny Raspberry ARM, it may make a difference.</p>

<p>Besides, it's not so painful to setup simply you trust your network. You have automount support in <code>/etc/fstab</code> too, and you don't have to put sensitive data (such as usernames or passwords), and if you have your usernames syncrhronized (same <code>/etc/passwd</code> and <code>/etc/group</code> files) you can use the usual POSIX permissions toolset (<code>chown</code>, <code>chgrp</code> and <code>chmod</code>).</p>
","62696"
"Difference between 2>&-, 2>/dev/null, |&, &>/dev/null and >/dev/null 2>&1","124343","","<p>Just looking for the difference between</p>

<ul>
<li><code>2&gt;&amp;-</code></li>
<li><code>2&gt;/dev/null</code></li>
<li><code>|&amp;</code></li>
<li><code>&amp;&gt;/dev/null</code></li>
<li><code>&gt;/dev/null 2&gt;&amp;1</code></li>
</ul>

<p>and their portability with <code>non-Bourne shells</code> like <code>tcsh</code>, <code>mksh</code>, etc.</p>
","<p>For background:</p>

<ul>
<li>a <strong>number 1</strong> = standard out (i.e. STDOUT)</li>
<li>a <strong>number 2</strong> = standard error (i.e. STDERR)</li>
<li>if a number isn't explicitly given, then <strong>number 1</strong> is assumed by the shell (bash)</li>
</ul>

<hr>

<p>First let's tackle the function of these. For reference see the <a href=""http://www.tldp.org/LDP/abs/html/io-redirection.html"" rel=""noreferrer"">Advanced Bash-Scripting Guide</a>.</p>

<h3>Functions</h3>

<h3><code>2&gt;&amp;-</code></h3>

<p>The general form of this one is <code>M&gt;&amp;-</code>, where <strong>""M""</strong> is a file descriptor number. This will close output for whichever file descriptor is referenced, i.e. <strong>""M""</strong>.</p>

<h3><code>2&gt;/dev/null</code></h3>

<p>The general form of this one is <code>M&gt;/dev/null</code>, where <strong>""M""</strong> is a file descriptor number. This will redirect the file descriptor, <strong>""M""</strong>, to <code>/dev/null</code>.</p>

<h3><code>2&gt;&amp;1</code></h3>

<p>The general form of this one is <code>M&gt;&amp;N</code>, where <strong>""M""</strong> &amp; <strong>""N""</strong> are file descriptor numbers. It combines the output of file descriptors <strong>""M""</strong> and <strong>""N""</strong> into a single stream.</p>

<h3><code>|&amp;</code></h3>

<p>This is just an abbreviation for <code>2&gt;&amp;1 |</code>. It was added in Bash 4.</p>

<h3><code>&amp;&gt;/dev/null</code></h3>

<p>This is just an abbreviation for <code>&gt;/dev/null 2&gt;&amp;1</code>. It redirects file descriptor 2 (STDERR) and descriptor 1 (STDOUT) to <code>/dev/null</code>.</p>

<h3><code>&gt;/dev/null</code></h3>

<p>This is just an abbreviation for <code>1&gt;/dev/null</code>. It redirects file descriptor 1 (STDOUT) to <code>/dev/null</code>.</p>

<h3>Portability to non-bash, tcsh, mksh, etc.</h3>

<p>I've not dealt much with other shells outside of <code>csh</code> and <code>tcsh</code>. My experience with those 2 compared to bash's redirection operators, is that bash is superior in that regard. See the <a href=""http://linux.die.net/man/1/tcsh"" rel=""noreferrer"">tcsh man page</a> for more details.</p>

<p>Of the commands you asked about none are directly supported by csh/tcsh. You'd have to use different syntaxes to construct similar functions.</p>
","70971"
"Find the total size of certain files within a directory branch","124178","","<p>Assume there's an image storage directory, say, <code>./photos/john_doe</code>, within which there are multiple subdirectories, where many certain files reside (say, <code>*.jpg</code>). How can I calculate a summary size of those files below the <code>john_doe</code> branch?</p>

<p>I tried <code>du -hs ./photos/john_doe/*/*.jpg</code>, but this shows individual files only. Also, this tracks only the first nest level of the <code>john_doe</code> directory, like <code>john_doe/june/</code>, but skips <code>john_doe/june/outrageous/</code>.</p>

<p>So, how could I traverse the entire branch, summing up the size of the certain files?</p>
","<pre><code>find ./photos/john_doe -type f -name '*.jpg' -exec du -ch {} + | grep total$
</code></pre>

<p>If more than one invocation of <code>du</code> is required because the file list is very long, multiple totals will be reported and need to be summed.</p>
","41552"
"tar --exclude doesn't exclude. Why?","124132","","<p>I have this very simple line in a bash script which executes successfully (i.e. producing the <code>_data.tar</code> file), except that it <strong>doesn't</strong> exclude the sub-directories it is told exclude via the <code>--exclude</code> option:</p>

<pre><code>/bin/tar -cf /home/_data.tar  --exclude='/data/sub1/*'  --exclude='/data/sub2/*' --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'  /data
</code></pre>

<p>Instead, it produces a <code>_data.tar</code> file that contains everything under /data, including the files in the subdirectories I wanted to exclude.</p>

<p>Any idea why? and how to fix this?</p>

<p><strong>Update</strong> I implemented my observations based on the link provided in the first answer below (top level dir first, no whitespace after last exclude):</p>

<pre><code>/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1/*'  --exclude='/data/sub2/*'  --exclude='/data/sub3/*'  --exclude='/data/sub4/*'  --exclude='/data/sub5/*'
</code></pre>

<p>But that didn't help. All ""excluded"" sub-directories are present in the resulting <code>_data.tar</code> file.</p>

<p>This is puzzling. Whether this is a bug in current tar (GNU tar 1.23, on a CentOS 6.2, Linux 2.6.32)  or ""extreme sensitivity"" of tar to whitespaces and other easy-to-miss typos, I consider this a bug. For now.</p>

<p><strong>This is horrible</strong>: I tried the insight suggested below (no trailing <code>/*</code>) and it still doesn't work in the production script:</p>

<pre><code>/bin/tar -cf /home/_data.tar  /data  --exclude='/data/sub1'  --exclude='/data/sub2'  --exclude='/data/sub3'  --exclude='/data/sub4'
</code></pre>

<p>I can't see any difference between what I tried and what @Richard Perrin tried, except for the quotes and 2 spaces instead of 1. I am going to try this (must wait for the nightly script to run as the directory to be backed up is huge) and report back.</p>

<pre><code>/bin/tar -cf /home/_data.tar  /data --exclude=/data/sub1 --exclude=/data/sub2 --exclude=/data/sub3 --exclude=/data/sub4
</code></pre>

<p>I am beginning to think that all these <code>tar --exclude</code> sensitivities aren't tar's but something in my environment, but then what could that be?</p>

<p><strong>It worked!</strong> The last variation tried (no single-quotes and single-space instead of double-space between the <code>--exclude</code>s) tested working. Weird but accepting.</p>

<p><strong>Unbelievable!</strong> It turns out that an older version of <code>tar</code> (1.15.1) would only exclude if the top-level dir is <strong>last</strong> on the command line. This is the exact opposite of how version 1.23 requires. FYI.</p>
","<p>If you want to exclude an entire directory, your pattern should match that directory, not files within it. Use <code>--exclude=/data/sub1</code> instead of <code>--exclude='/data/sub1/*'</code></p>

<p>Be careful with quoting the patterns to protect them from shell expansion.</p>

<p>See this example, with trouble in the final invocation:</p>

<pre><code>$ for i in 0 1 2; do mkdir -p /tmp/data/sub$i; echo foo &gt; /tmp/data/sub$i/foo; done
$ find /tmp/data
/tmp/data
/tmp/data/sub2
/tmp/data/sub2/foo
/tmp/data/sub0
/tmp/data/sub0/foo
/tmp/data/sub1
/tmp/data/sub1/foo
$ tar -zvcf /tmp/_data.tar /tmp/data --exclude='/tmp/data/sub[1-2]'
tar: Removing leading `/' from member names
/tmp/data/
/tmp/data/sub0/
/tmp/data/sub0/foo
$ tar -zvcf /tmp/_data.tar /tmp/data --exclude=/tmp/data/sub[1-2]
tar: Removing leading `/' from member names
/tmp/data/
/tmp/data/sub0/
/tmp/data/sub0/foo
$ echo tar -zvcf /tmp/_data.tar /tmp/data --exclude=/tmp/data/sub[1-2]
tar -zvcf /tmp/_data.tar /tmp/data --exclude=/tmp/data/sub[1-2]
$ tar -zvcf /tmp/_data.tar /tmp/data --exclude /tmp/data/sub[1-2]
tar: Removing leading `/' from member names
/tmp/data/
/tmp/data/sub2/
/tmp/data/sub2/foo
/tmp/data/sub0/
/tmp/data/sub0/foo
/tmp/data/sub2/
tar: Removing leading `/' from hard link targets
/tmp/data/sub2/foo
$ echo tar -zvcf /tmp/_data.tar /tmp/data --exclude /tmp/data/sub[1-2]
tar -zvcf /tmp/_data.tar /tmp/data --exclude /tmp/data/sub1 /tmp/data/sub2
</code></pre>
","33036"
"Looping through files with spaces in the names?","124026","","<p>I wrote the following script to diff the outputs of two directores with all the same files in them as such:</p>

<pre><code>#!/bin/bash

for file in `find . -name ""*.csv""`  
do
     echo ""file = $file"";
     diff $file /some/other/path/$file;
     read char;
done
</code></pre>

<p>I know there are other ways to achieve this.  Curiously though, this script fails when the files have spaces in them.  How can I deal with this?</p>

<p>Example output of find:</p>

<pre><code>./zQuery - abc - Do Not Prompt for Date.csv
</code></pre>
","<p><strong>Short answer (closest to your answer, but handles spaces)</strong></p>

<pre><code>OIFS=""$IFS""
IFS=$'\n'
for file in `find . -type f -name ""*.csv""`  
do
     echo ""file = $file""
     diff ""$file"" ""/some/other/path/$file""
     read line
done
IFS=""$OIFS""
</code></pre>

<p><strong>Better answer (also handles wildcards and newlines in file names)</strong></p>

<pre><code>find . -type f -name ""*.csv"" -print0 | while IFS= read -r -d '' file; do
    echo ""file = $file""
    diff ""$file"" ""/some/other/path/$file""
    read line &lt;/dev/tty
done
</code></pre>

<p><strong>Best answer (based on <a href=""https://unix.stackexchange.com/questions/9496/looping-through-files-with-spaces-in-the-names/9500#9500"">Gilles' answer</a>)</strong></p>

<pre><code>find . -type f -name '*.csv' -exec sh -c '
  file=""$0""
  echo ""$file""
  diff ""$file"" ""/some/other/path/$file""
  read line &lt;/dev/tty
' {} ';'
</code></pre>

<p>Or even better, to avoid running one <code>sh</code> per file:</p>

<pre><code>find . -type f -name '*.csv' -exec sh -c '
  for file do
    echo ""$file""
    diff ""$file"" ""/some/other/path/$file""
    read line &lt;/dev/tty
  done
' sh {} +
</code></pre>

<hr>

<p><strong>Long answer</strong></p>

<p>You have three problems:</p>

<ol>
<li>By default, the shell splits the output of a command on spaces, tabs, and newlines</li>
<li>Filenames could contain wildcard characters which would get expanded</li>
<li>What if there is a directory whose name ends in <code>*.csv</code>?</li>
</ol>

<p><strong>1. Splitting only on newlines</strong></p>

<p>To figure out what to set <code>file</code> to, the shell has to take the output of <code>find</code> and interpret it somehow, otherwise <code>file</code> would just be the entire output of <code>find</code>.</p>

<p>The shell reads the <code>IFS</code> variable, which is which is set to <code>&lt;space&gt;&lt;tab&gt;&lt;newline&gt;</code> by default.</p>

<p>Then it looks at each character in the output of <code>find</code>.  As soon as it sees any character that's in <code>IFS</code>, it thinks that marks the end of the file name, so it sets <code>file</code> to whatever characters it saw until now and runs the loop.  Then it starts where it left off to get the next file name, and runs the next loop, etc., until it reaches the end of output.</p>

<p>So it's effectively doing this:</p>

<pre><code>for file in ""zquery"" ""-"" ""abc"" ...
</code></pre>

<p>To tell it to only split the input on newlines, you need to do</p>

<pre><code>IFS=$'\n'
</code></pre>

<p>before your <code>for ... find</code> command.</p>

<p>That sets <code>IFS</code> to a single newline, so it only splits on newlines, and not spaces and tabs as well.</p>

<p>If you are using <code>sh</code> or <code>dash</code> instead of <code>ksh93</code>, <code>bash</code> or <code>zsh</code>, you need to write <code>IFS=$'\n'</code> like this instead:</p>

<pre><code>IFS='
'
</code></pre>

<p>That is probably enough to get your script working, but if you're interested to handle some other corner cases properly, read on...</p>

<p><strong>2. Expanding <code>$file</code> without wildcards</strong></p>

<p>Inside the loop where you do</p>

<pre><code>diff $file /some/other/path/$file
</code></pre>

<p>the shell tries to expand <code>$file</code> (again!).</p>

<p>It could contain spaces, but since we already set <code>IFS</code> above, that won't be a problem here.</p>

<p>But it could also contain wildcard characters such as <code>*</code> or <code>?</code>, which would lead to unpredictable behavior.  (Thanks to Gilles for pointing this out.)</p>

<p>To tell the shell not to expand wildcard characters, put the variable inside double quotes, e.g.</p>

<pre><code>diff ""$file"" ""/some/other/path/$file""
</code></pre>

<p>The same problem could also bite us in</p>

<pre><code>for file in `find . -name ""*.csv""`
</code></pre>

<p>For example, if you had these three files</p>

<pre><code>file1.csv
file2.csv
*.csv
</code></pre>

<p>(very unlikely, but still possible)</p>

<p>It would be as if you had run</p>

<pre><code>for file in file1.csv file2.csv *.csv
</code></pre>

<p>which will get expanded to</p>

<pre><code>for file in file1.csv file2.csv *.csv file1.csv file2.csv
</code></pre>

<p>causing <code>file1.csv</code> and <code>file2.csv</code> to be processed twice.</p>

<p>Instead, we have to do</p>

<pre><code>find . -name ""*.csv"" -print | while IFS= read -r file; do
    echo ""file = $file""
    diff ""$file"" ""/some/other/path/$file""
    read line &lt;/dev/tty
done
</code></pre>

<p><code>read</code> reads lines from standard input, splits the line into words according to <code>IFS</code> and stores them in the variable names that you specify.</p>

<p>Here, we're telling it not to split the line into words, and to store the line in <code>$file</code>.</p>

<p>Also note that <code>read line</code> has changed to <code>read line &lt;/dev/tty</code>.</p>

<p>This is because inside the loop, standard input is coming from <code>find</code> via the pipeline.</p>

<p>If we just did <code>read</code>, it would be consuming part or all of a file name, and some files would be skipped.</p>

<p><code>/dev/tty</code> is the terminal where the user is running the script from.  Note that this will cause an error if the script is run via cron, but I assume this is not important in this case.</p>

<p>Then, what if a file name contains newlines?</p>

<p>We can handle that by changing <code>-print</code> to <code>-print0</code> and using <code>read -d ''</code> on the end of a pipeline:</p>

<pre><code>find . -name ""*.csv"" -print0 | while IFS= read -r -d '' file; do
    echo ""file = $file""
    diff ""$file"" ""/some/other/path/$file""
    read char &lt;/dev/tty
done
</code></pre>

<p>This makes <code>find</code> put a null byte at the end of each file name.  Null bytes are the only characters not allowed in file names, so this should handle all possible file names, no matter how weird.</p>

<p>To get the file name on the other side, we use <code>IFS= read -r -d ''</code>.</p>

<p>Where we used <code>read</code> above, we used the default line delimiter of newline, but now, <code>find</code> is using null as the line delimiter. In <code>bash</code>, you can't pass a NUL character in an argument to a command (even builtin ones), but <code>bash</code> understands <code>-d ''</code> as meaning <em>NUL delimited</em>. So we use <code>-d ''</code> to make <code>read</code> use the same line delimiter as <code>find</code>. Note that <code>-d $'\0'</code>, incidentally, works as well, because <code>bash</code> not supporting NUL bytes treats it as the empty string.</p>

<p>To be correct, we also add <code>-r</code>, which says don't handle backslashes in file names specially.  For example, without <code>-r</code>, <code>\&lt;newline&gt;</code> are removed, and <code>\n</code> is converted into <code>n</code>.</p>

<p>A more portable way of writing this that doesn't require <code>bash</code> or <code>zsh</code> or remembering all the above rules about null bytes (again, thanks to Gilles):</p>

<pre><code>find . -name '*.csv' -exec sh -c '
  file=""$0""
  echo ""$file""
  diff ""$file"" ""/some/other/path/$file""
  read char &lt;/dev/tty
' {} ';'
</code></pre>

<p><strong>3. Skipping directories whose names end in *.csv</strong></p>

<pre><code>find . -name ""*.csv""
</code></pre>

<p>will also match directories that are called <code>something.csv</code>.</p>

<p>To avoid this, add <code>-type f</code> to the <code>find</code> command.</p>

<pre><code>find . -type f -name '*.csv' -exec sh -c '
  file=""$0""
  echo ""$file""
  diff ""$file"" ""/some/other/path/$file""
  read line &lt;/dev/tty
' {} ';'
</code></pre>

<p>As <a href=""https://unix.stackexchange.com/users/4667/glenn-jackman"">glenn jackman</a> points out, in both of these examples, the commands to execute for each file are being run in a subshell, so if you change any variables inside the loop, they will be forgotten.</p>

<p>If you need to set variables and have them still set at the end of the loop, you can rewrite it to use process substitution like this:</p>

<pre><code>i=0
while IFS= read -r -d '' file; do
    echo ""file = $file""
    diff ""$file"" ""/some/other/path/$file""
    read line &lt;/dev/tty
    i=$((i+1))
done &lt; &lt;(find . -type f -name '*.csv' -print0)
echo ""$i files processed""
</code></pre>

<p>Note that if you try copying and pasting this at the command line, <code>read line</code> will consume the <code>echo ""$i files processed""</code>, so that command won't get run.</p>

<p>To avoid this, you could remove <code>read line &lt;/dev/tty</code> and send the result to a pager like <code>less</code>.</p>

<hr>

<p><strong>NOTES</strong></p>

<p>I removed the semi-colons (<code>;</code>) inside the loop.  You can put them back if you want, but they are not needed.</p>

<p>These days, <code>$(command)</code> is more common than <code>`command`</code>.  This is mainly because it's easier to write <code>$(command1 $(command2))</code> than <code>`command1 \`command2\``</code>.</p>

<p><code>read char</code> doesn't really read a character.  It reads a whole line so I changed it to <code>read line</code>.</p>
","9499"
"Where is bash's history stored?","123997","","<p>If I run <code>history</code>, I can see my latest executed commands.</p>

<p>But if I do <code>tail -f $HISTFILE</code> or <code>tail -f ~/.bash_history</code>, they do not get listed.</p>

<p>Does the file get locked, is there a temporary location or something similar?</p>
","<p>Bash maintains the list of commands internally in memory while it's running. They are written into <a href=""https://www.gnu.org/software/bash/manual/bashref.html#Bash-History-Facilities""><code>.bash_history</code> on exit</a>:</p>

<blockquote>
  <p>When an interactive shell exits, the last $HISTSIZE lines are copied from the history list to the file named by $HISTFILE</p>
</blockquote>

<p>If you want to force the command history to be written out, you can use the <a href=""https://www.gnu.org/software/bash/manual/bashref.html#index-history""><code>history -a</code></a> command, which will:</p>

<blockquote>
  <p>Append the new history lines (history lines entered since the beginning of the current Bash session) to the history file.</p>
</blockquote>

<p>There is also a <code>-w</code> option:</p>

<blockquote>
  <p>Write out the current history to the history file.</p>
</blockquote>

<p>which may suit you more depending on exactly how you use your history.</p>

<p>If you want to make sure that they're always written immediately, you can put that command into your <code>PROMPT_COMMAND</code> variable:</p>

<pre><code>export PROMPT_COMMAND='history -a'
</code></pre>
","145254"
"What is ""mail"", and how is it navigated?","123786","","<p>The program is located in <code>/usr/bin/mail</code>.  Upon execution, <code>Version 8.1.2 01/15/2001</code> is shown.</p>

<p>Entering <code>list</code> produces:</p>

<pre><code>Commands are:
next, alias, print, type, Type, Print, visual, top, touch, preserve, 
delete, dp, dt, undelete, unset, mail, mbox, pipe, |, more, page, More, 
Page, unread, Unread, !, copy, chdir, cd, save, source, set, shell, 
version, group, write, from, file, folder, folders, ?, z, headers, 
help, =, Reply, Respond, reply, respond, edit, echo, quit, list, xit, 
exit, size, hold, if, else, endif, alternates, ignore, discard, retain, 
saveignore, savediscard, saveretain, core, #, inc, new
</code></pre>

<p>Entering <code>?</code> produces:</p>

<pre><code>Mail Command               Description
-------------------------  --------------------------------------------
t [message list]           type message(s).
n                          goto and type next message.
e [message list]           edit message(s).
f [message list]           give head lines of messages.
d [message list]           delete message(s).
s [message list] &lt;file&gt;    append message(s) to file.
u [message list]           undelete message(s).
R [message list]           reply to message sender(s).
r [message list]           reply to message sender(s) and all recipients.
p [message list]           print message list.
pre [message list]         make messages go back to /var/mail.
m &lt;recipient list&gt;         mail to specific recipient(s).
q                          quit, saving unresolved messages in mbox.
x                          quit, do not remove system mailbox.
h                          print out active message headers.
!                          shell escape.
| [msglist] command        pipe message(s) to shell command.
pi [msglist] command       pipe message(s) to shell command.
cd [directory]             chdir to directory or home if none given
fi &lt;file&gt;                  switch to file (%=system inbox, %user=user's
                           system inbox).  + searches in your folder
                           directory for the file.
set variable[=value]       set Mail variable.
</code></pre>

<p>Entering <code>z</code> shows the end of the list of messages - but that command is not presented in the <code>?</code> help page.</p>

<ol>
<li>What program is this?</li>
<li>Are there tutorials for its use?</li>
<li>What are some common commands and helpful tricks for its use?</li>
<li>How can the message list be navigated (the opposite of <code>z</code>) or refreshed?</li>
</ol>

<p><strong>Clarification</strong>: This question is about the interactive program and not the script-able command - i.e. the result of typing <code>mail</code> with no flags or parameters into a terminal.</p>
","<p><a href=""http://www.johnkerl.org/doc/mail-how-to.html"">This page</a> describes the interactive command in detail, and is in fact a fairly thorough tutorial. Describes commands such as <code>z</code> and <code>z-</code> :</p>

<pre><code>If there is more than a screenful of messages, then z will
show the next screenful, and z- will show the previous screenful.
</code></pre>

<p>Hope that helps.</p>
","26933"
"Sort and count number of occurrence of lines","123583","","<p>I have <code>Apache</code> logfile, <code>access.log</code>, how to count number of line occurrence in that file? for example the result of <code>cut -f 7 -d ' ' | cut -d '?' -f 1 | tr '[:upper:]' '[:lower:]'</code> is</p>

<pre><code>a.php
b.php
a.php
c.php
d.php
b.php
a.php
</code></pre>

<p>the result that I want is:</p>

<pre><code>3 a.php
2 b.php
1 d.php # order doesn't matter
1 c.php 
</code></pre>
","<pre><code>| sort | uniq -c
</code></pre>

<p>As stated in the comments.</p>

<p>Piping the output into <code>sort</code> organises the output into alphabetical/numerical order.</p>

<p>This is a requirement because <code>uniq</code> only matches on repeated lines, ie </p>

<pre><code>a
b
a
</code></pre>

<p>If you use <code>uniq</code> on this text file, it will return the following. This is because the two <code>a</code>s are separated by the <code>b</code> - they are not consecutive lines.</p>

<pre><code>a
b
a
</code></pre>

<p>However if you first sort the data into alphabetical order first like</p>

<pre><code>a
a
b
</code></pre>

<p>Then <code>uniq</code> will remove the repeating lines. The <code>-c</code> option of <code>uniq</code> counts the number of duplicates and provides output in the form:</p>

<pre><code>2 a
1 b
</code></pre>

<p><a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?sort"">http://unixhelp.ed.ac.uk/CGI/man-cgi?sort</a></p>

<p><a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?uniq"">http://unixhelp.ed.ac.uk/CGI/man-cgi?uniq</a></p>
","170044"
"How can I update to a newer version of Git using apt-get?","123269","","<p>I've just set up a new machine with Ubuntu Oneiric 11.10 and then run</p>

<pre><code>apt-get update
apt-get upgrade
apt-get install git
</code></pre>

<p>Now if I run <code>git --version</code> it tells me I have <code>git version 1.7.5.4</code> but on my local machine I have the much newer <code>git version 1.7.9.2</code></p>

<p>I know I can install from source to get the newest version, but I thought that it was a good idea to use the package manager as much as possible to keep everything standardized.</p>

<p>So is it possible to use <code>apt-get</code> to get a newer version of <code>git</code>, and what is the right way to do it?</p>
","<p>You have several options:</p>

<ol>
<li>Either wait until the version you need is present in the repository you use.</li>
<li>Compile your own version and create a <code>deb</code>.</li>
<li>Find a repository that provides the version you need for your version of your distribution(e.g. <a href=""https://launchpad.net/~git-core/+archive/ppa"">Git PPA</a>).</li>
<li>If you don't need any particular feature from the newer version, stay with the old one.</li>
</ol>

<p>If a newer version is available in the repositories you use, then <code>apt-get update &amp;&amp; apt-get upgrade</code> (as root) updates to the latest available version.</p>


","33618"
"How to unzip a multipart (spanned) ZIP on Linux?","123172","","<p>I need to upload a 400mb file to my web server, but I'm limited to 200mb uploads. My host suggested I use a spanned archive, which I've never done on Linux.</p>

<p>I created a test in its own folder, zipping up a PDF into <code>test.zip.001</code>, <code>.002</code>, and <code>.003</code>. How do I go about unzipping it? Do I need to join them first?</p>

<p>Please note that I'd be just as happy using 7z as I am using ZIP formats.  If this makes any difference to the outcome.</p>
","<p>You will need to join them first. You may use the common linux app, <code>cat</code> as in the example below:</p>

<pre><code>cat test.zip* &gt; ~/test.zip
</code></pre>

<p>This will concatenate all of your <code>test.zip.001</code>, <code>test.zip.002</code>, etc files into one larger, test.zip file. Once you have that single file, you may run <code>unzip test.zip</code> </p>

<p><a href=""http://linuxg.net/how-to-create-split-join-and-extract-zip-archives-in-linux"">""How to create, split, join and extract zip archives in Linux""</a> may help.</p>
","40481"
"Hide curl output","122320","","<p>I'm making a curl request where it displays an html output in the console like this </p>

<pre><code>&lt;b&gt;Warning&lt;/b&gt;:  Cannot modify header information - headers already sent by (output started at /home/domain/public_html/wp-content/themes/explicit/functions/ajax.php:87) in &lt;b&gt;/home/domain/public_html/wp-content/themes/explicit/functions/ajax.php&lt;/b&gt; on line &lt;b&gt;149&lt;/b&gt;&lt;br /&gt;......
</code></pre>

<p>etc</p>

<p>I need to hide these outputs when running the CURL requests, tried running the CURL like this</p>

<pre><code>curl -s 'http://example.com'
</code></pre>

<p>But it still displays the output, how can I hide the output? </p>

<p>Thanks</p>
","<p>From <code>man curl</code></p>

<blockquote>
  <p>-s, --silent
                Silent or quiet mode. Don't show progress meter or error messages.  Makes Curl mute. It will still output the data you ask for,
  potentially even to the terminal/stdout
                <strong>unless you redirect it</strong>.</p>
</blockquote>

<p>So if you don't want any output use:</p>

<pre><code>curl -s 'http://example.com' &gt; /dev/null
</code></pre>
","196552"
"How do I know if a partition is ext2, ext3, or ext4?","122117","","<p>I just formatted stuff. One disk I format as ext2. The other I want to format as ext4. I want to test how they perform.</p>

<p>Now, how do I know the kind of file system in a partition?</p>
","<p><strong>How do I tell what sort of data (what data format) is in a file?<br>
→ Use the <a href=""http://en.wikipedia.org/wiki/File_%28command%29""><code>file</code></a> utility.</strong></p>

<p>Here, you want to know the format of data in a device file, so you need to pass the <code>-s</code> flag to tell <code>file</code> not just to say that it's a device file but look at the content. Sometimes you'll need the <code>-L</code> flag as well, if the device file name is a symbolic link. You'll see output like this:</p>

<pre><code># file -sL /dev/sd*
/dev/sda1: Linux rev 1.0 ext4 filesystem data, UUID=63fa0104-4aab-4dc8-a50d-e2c1bf0fb188 (extents) (large files) (huge files)
/dev/sdb1: Linux rev 1.0 ext2 filesystem data, UUID=b3c82023-78e1-4ad4-b6e0-62355b272166
/dev/sdb2: Linux/i386 swap file (new style), version 1 (4K pages), size 4194303 pages, no label, UUID=3f64308c-19db-4da5-a9a0-db4d7defb80f
</code></pre>

<p>Given this sample output, the first disk has one partition and the second disk has two partitions. <code>/dev/sda1</code> is an ext4 filesystem, <code>/dev/sdb1</code> is an ext2 filesystem, and <code>/dev/sdb2</code> is some swap space (about 4GB).</p>

<p>You must run this command as root, because ordinary users may not read disk partitions directly: if needed, add <code>sudo</code> in front.</p>
","60783"
"""not a valid identifier"" when I do ""export $PATH""","122048","","<p>When I run <code>export $PATH</code> in bash, I get the error <code>not a valid identifier</code>. Why?</p>
","<p>Running <code>export $PATH</code> will try to export a variable with a name equal to the <strong>value</strong> of <code>$PATH</code> (after <a href=""http://mywiki.wooledge.org/WordSplitting"">word splitting</a>). That is, it's equivalent to writing something like <code>export /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code>. And since <code>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</code> is not a valid variable name, it fails. <strong>What you want to do is <code>export PATH</code>.</strong></p>

<p><a href=""http://mywiki.wooledge.org/BashGuide/Parameters#Variable_Types""><code>export</code></a> (equivalent to <code>declare -x</code>) in Bash simply makes the variable available to subshells.</p>

<p>To print the value of a variable safely and readably, use <code>printf %q ""$PATH""</code>.</p>
","79662"
"What is the 'working directory' when cron executes a job?","121989","","<p>I have a script that works when I run it from the command line, but when I schedule it with <code>cron</code> I get errors that it cannot find files or commands.  My question is twofold:</p>

<ol>
<li><p>When I schedule a cron job using <code>crontab -e</code>, does it use my user ID as the basis for its permissions?  Or does it use a cron user ID of some sort and its related permissions?</p></li>
<li><p>When a cron job is launched, what is the working directory?  Is it the directory where I specify the script to run, or a different directory?</p></li>
</ol>

<p>Here is my cron job:</p>

<pre><code>15 7 * * * /home/xxxx/Documents/Scripts/email_ip_script.sh
</code></pre>

<p>Here is the actual script:</p>

<pre><code>vIP_ADDR=""`curl automation.whatismyip.com/n09230945.asp`""
echo ""$vIP_ADDR""
sed ""s/IPADDR/$vIP_ADDR/g"" template.txt &gt; emailmsg.txt
ssmtp XXXXX@gmail.com &lt; emailmsg.txt
</code></pre>

<p>Here are the errors I get when I view the <code>mail</code> message produced by <code>cron</code>:</p>

<pre><code>sed: can't read template.txt: No such file or directory
/home/xxxx/Documents/Scripts/email_ip_script.sh: line 15: ssmtp: command not found
</code></pre>

<p>It cannot find the <code>template.txt</code> but it resides in the same directory as the script.  It also cannot run <code>ssmtp</code>, but I can as my user.  What am I missing to get this to work properly? </p>
","<p>Add <code>cd /home/xxxx/Documents/Scripts/</code> if you want your job to run in that directory. There's no reason why cron would change to that particular directory. Cron runs your commands in your home directory.</p>

<p>As for <code>ssmtp</code>, it might not be in your default <code>PATH</code>. Cron's default path is implementation-dependent, so check your man page, but in all likelihood <code>ssmtp</code> is in <code>/usr/sbin</code> which is not in your default <code>PATH</code>, only root's.</p>

<pre><code>PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin
15 7 * * * cd /home/xxxx/Documents/Scripts &amp;&amp; ./email_ip_script.sh
</code></pre>
","38956"
"How to set the resolution in text consoles (troubleshoot when any `vga=...` fails)","121826","","<p>It is a common way to set the resolution of a text consoles (that are usually available by Ctrl-Alt-F1 thru Ctrl-Alt-F6) by using a <code>vga=...</code> kernel parameter.
I'm using Ubuntu 10.04 Lucid, output of <code>uname -a</code> is:</p>

<pre><code>Linux  2.6.32-33-generic #70-Ubuntu SMP Thu Jul 7 21:13:52 UTC 2011 x86_64 GNU/Linux
</code></pre>

<p>To identify modes available i use the <code>sudo hwinfo --framebuffer</code> which reports:</p>

<pre><code>02: None 00.0: 11001 VESA Framebuffer                            
  [Created at bios.464]  
  Unique ID: rdCR.R1b4duaxSqA  
  Hardware Class: framebuffer  
  Model: ""NVIDIA G73 Board - p456h1  ""  
  Vendor: ""NVIDIA Corporation""  
  Device: ""G73 Board - p456h1  ""  
  SubVendor: ""NVIDIA""  
  SubDevice:   
  Revision: ""Chip Rev""  
  Memory Size: 256 MB  
  Memory Range: 0xc0000000-0xcfffffff (rw)  
  Mode 0x0300: 640x400 (+640), 8 bits  
  Mode 0x0301: 640x480 (+640), 8 bits  
  Mode 0x0303: 800x600 (+800), 8 bits  
  Mode 0x0305: 1024x768 (+1024), 8 bits  
  Mode 0x0307: 1280x1024 (+1280), 8 bits  
  Mode 0x030e: 320x200 (+640), 16 bits  
  Mode 0x030f: 320x200 (+1280), 24 bits  
  Mode 0x0311: 640x480 (+1280), 16 bits  
  Mode 0x0312: 640x480 (+2560), 24 bits  
  Mode 0x0314: 800x600 (+1600), 16 bits  
  Mode 0x0315: 800x600 (+3200), 24 bits  
  Mode 0x0317: 1024x768 (+2048), 16 bits  
  Mode 0x0318: 1024x768 (+4096), 24 bits  
  Mode 0x031a: 1280x1024 (+2560), 16 bits  
  Mode 0x031b: 1280x1024 (+5120), 24 bits  
  Mode 0x0330: 320x200 (+320), 8 bits  
  Mode 0x0331: 320x400 (+320), 8 bits  
  Mode 0x0332: 320x400 (+640), 16 bits  
  Mode 0x0333: 320x400 (+1280), 24 bits  
  Mode 0x0334: 320x240 (+320), 8 bits  
  Mode 0x0335: 320x240 (+640), 16 bits  
  Mode 0x0336: 320x240 (+1280), 24 bits  
  Mode 0x033d: 640x400 (+1280), 16 bits  
  Mode 0x033e: 640x400 (+2560), 24 bits  
  Config Status: cfg=new, avail=yes, need=no, active=unknown  
</code></pre>

<p>It looks like many hi-res modes are available, like 0x305, 0x307, 0x317, 0x318, 0x31a, 0x31b (by the way, what does the plus-number means in the list of modes?). However, setting any of these modes in kernel option string, line <code>vga=0x305</code>, results in either pitch black text console, or screen filled by blinking color/bw dots. </p>

<p>What is the 'modern', 'robust' way to set up high resolution in text consoles?</p>
","<p>Newer kernels use <a href=""http://en.wikipedia.org/wiki/Mode-setting"" rel=""nofollow noreferrer"">KMS</a> by default, so you should move away from appending <code>vga=</code> to your grub line as it will conflict with the native resolution of KMS. However, it depends upon the video driver you are using: the proprietary Nvidia driver doesn't support <a href=""https://wiki.archlinux.org/index.php/KMS"" rel=""nofollow noreferrer"">KMS</a>, but you can work around it.</p>

<p>You should be able to get full resolution in the framebuffer by editing your <code>/etc/default/grub</code> and making sure that the <code>GFXMODE</code> is set correctly, and then adding a <code>GFXPAYLOAD</code> entry like so:</p>

<pre><code>GRUB_GFXMODE=1680x1050x24

# Hack to force higher framebuffer resolution
GRUB_GFXPAYLOAD_LINUX=1680x1050</code></pre>

<p>Remember to run <code>sudo update-grub</code> afterwards.</p>
","17032"
"Why is number of open files limited in Linux?","121762","","<p>Right now, I know that:  </p>

<ul>
<li>Find open files limit per process: <code>ulimit -n</code></li>
<li>Count all opened files by all process: <code>lsof | wc -l</code></li>
<li>Get maximum open files count allowd: <code>cat /proc/sys/fs/file-max</code></li>
</ul>

<p>My question here is that: Why there would be a limit of open files in Linux? </p>
","<p>The reason is that the operating system needs memory to manage each open file, and memory is a limited resource - especially on embedded systems.  </p>

<p>As root user you can change the maximum of the open files count per process (via <code>ulimit -n</code>) and per system (e.g. <code>echo 800000 &gt; /proc/sys/fs/file-max</code>).</p>
","36842"
"What could DUP mean when using ping?","121494","","<p>What could DUP mean when using ping?</p>
","<p>DUP means duplicate packet.</p>

<p>From <code>man ping</code>:</p>

<blockquote>
  <p>Duplicate and Damaged Packets</p>
  
  <p>ping will report duplicate and damaged
  packets. Duplicate packets should
  never occur, and seem to be caused by
  inappropriate link-level
  retransmissions. Duplicates may occur
  in many situations and are rarely (if
  ever) a good sign, although the
  presence of low levels of duplicates
  may not always be cause for alarm.</p>
  
  <p>Damaged packets are obviously serious
  cause for alarm and often indicate
  broken hardware somewhere in the ping
  packet's path (in the network or in
  the hosts).</p>
</blockquote>

<p>There are different reasons for this, did you capture your network traffic with an interface in promiscous mode? Sometimes this is the reason for dupplicated packets.</p>
","13256"
"Starting with bash: -lt and -gt arguments","121286","","<p>I'm starting with bash and I found the following:</p>

<pre><code>if test $first -lt $second
then
  echo $first is lower than $second
else
  if test $first -gt $second
  then
    echo $first is higher than $second
  else
    echo $first and $second are equals
  fi
fi
</code></pre>

<p>For reading the script and executing it, I know what it does, but not what -lt and -gt are for.</p>

<p>Can somebody tell me what is the name of that kind of 'tool' and what they(-lt and -gt) do? Thanks!</p>
","<p>It's short for <code>less than</code> and <code>greater than</code>. It's used for integer comparison in bash. You can read more by typing <code>man test</code>:</p>

<pre><code>   ....
   INTEGER1 -gt INTEGER2
          INTEGER1 is greater than INTEGER2
   ....
   INTEGER1 -lt INTEGER2
          INTEGER1 is less than INTEGER2
   ....
</code></pre>
","119548"
"Using while loop to ssh to multiple servers","121183","","<p>I have a file <code>servers.txt</code>, with list of servers:</p>

<pre><code>server1.mydomain.com
server2.mydomain.com
server3.mydomain.com
</code></pre>

<p>when I read the file line by line with <code>while</code> and echo each line, all works as expected. All lines are printed.</p>

<pre><code>$ while read HOST ; do echo $HOST ; done &lt; servers.txt
server1.mydomain.com
server2.mydomain.com
server3.mydomain.com
</code></pre>

<p>However, when I want to ssh to all servers and execute a command, suddenly my <code>while</code> loop stops working:</p>

<pre><code>$ while read HOST ; do ssh $HOST ""uname -a"" ; done &lt; servers.txt
Linux server1 2.6.30.4-1 #1 SMP Wed Aug 12 19:55:12 EDT 2009 i686 GNU/Linux
</code></pre>

<p>This only connects to the first server in the list, not to all of them. I don't understand what is happening here. Can somebody please explain?</p>

<p>This is even stranger, since using <code>for</code> loop works fine:</p>

<pre><code>$ for HOST in $(cat servers.txt ) ; do ssh $HOST ""uname -a"" ; done
Linux server1 2.6.30.4-1 #1 SMP Wed Aug 12 19:55:12 EDT 2009 i686 GNU/Linux
Linux server2 2.6.30.4-1 #1 SMP Wed Aug 12 19:55:12 EDT 2009 i686 GNU/Linux
Linux server3 2.6.30.4-1 #1 SMP Wed Aug 12 19:55:12 EDT 2009 i686 GNU/Linux
</code></pre>

<p>It must be something specific to <code>ssh</code>, because other commands work fine, such as <code>ping</code>:</p>

<pre><code>$ while read HOST ; do ping -c 1 $HOST ; done &lt; servers.txt
</code></pre>
","<p><code>ssh</code> is reading the rest of your standard input.</p>

<pre><code>while read HOST ; do … ; done &lt; servers.txt
</code></pre>

<p><code>read</code> reads from stdin. The <code>&lt;</code> redirects stdin from a file. </p>

<p>Unfortunately, the command you're trying to run also reads stdin, so it winds up eating the rest of your file. You can see it clearly with:</p>

<pre><code>$ while read HOST ; do echo start $HOST end; cat; done &lt; servers.txt 
start server1.mydomain.com end
server2.mydomain.com
server3.mydomain.com
</code></pre>

<p>Notice how <code>cat</code> ate (and echoed) the remaining two lines. (Had read done it as expected, each line would have the ""start"" and ""end"" around the host.)</p>

<p><strong>Why does <code>for</code> work?</strong></p>

<p>Your <code>for</code> line doesn't redirect to stdin. (In fact, it reads the entire contents of the <code>servers.txt</code> file into memory before the first iteration). So <code>ssh</code> continues to read its stdin from the terminal (or possibly nothing, depending on how your script is called). </p>

<h1>Solution</h1>

<p>At least in bash, you can have <code>read</code> use a different file descriptor.</p>

<pre><code>while read -u10 HOST ; do ssh $HOST ""uname -a"" ; done 10&lt; servers.txt
#          ^^^^                                       ^^
</code></pre>

<p>ought to work. <code>10</code> is just an arbitrary file number I picked. 0, 1, and 2 have defined meanings, and typically opening files will start from the first available number (so 3 is next to be used). 10 is thus high enough to stay out of the way, but low enough to be under the limit in some shells. Plus its a nice round number...</p>

<h2>Alternative Solution 1: -n</h2>

<p>As <a href=""https://unix.stackexchange.com/users/29821/mcnisse"">McNisse</a> points out in <a href=""https://unix.stackexchange.com/a/107802/977"">his/her answer</a>, the OpenSSH client has an <code>-n</code> option that'll prevent it from reading stdin. This works well in the particular case of <code>ssh</code>, but of course other commands may lack this—the other solutions work regardless of which command is eating your stdin.</p>

<h2>Alternative Solution 2: second redirect</h2>

<p>You can apparently (as in, I tried it, it works in my version of Bash at least...) do a second redirect, which looks something like this:</p>

<pre><code>while read HOST ; do ssh $HOST ""uname -a"" &lt; /dev/null; done &lt; servers.txt
</code></pre>

<p>You can use this with any command, but it'll be difficult if you actually want terminal input going to the command.</p>
","107801"
"In CentOS, what is the difference between yum update and yum upgrade?","121121","","<p>What is the difference between <code>yum update</code> and <code>yum upgrade</code>, and when should I use one over the other?</p>
","<p><code>yum upgrade</code> forces the removal of obsolete packages, while <code>yum update</code> may or may not also do this. The removal of obsolete packages can be risky, as it may remove packages that you use.  </p>

<p>This makes <code>yum update</code> the safer option.</p>

<p>From <code>man yum</code>:</p>

<blockquote>
  <p>update</p>
  
  <p>If run without any packages, update will update every currently installed package. If one or more packages or package globs are specified, Yum will only update the listed packages. While updating packages, yum will ensure that all dependencies are satisfied. (See Specifying package names for more information) If the packages or globs specified match to packages which are not currently installed then update will not install them. update operates on groups, files, provides and filelists just like the ""install"" command.
  <strong>If the main obsoletes configure option is true (default) or the --obsoletes flag is present yum will include package obsoletes in its calculations - this makes it better for distro-version changes, for example: upgrading from somelinux 8.0 to somelinux 9.</strong></p>
  
  <p>upgrade</p>
  
  <p>Is <strong>the same as the update command with the --obsoletes flag set</strong>. See update for more details.</p>
</blockquote>
","55778"
"How can I list subdirectories recursively?","120982","","<p>The obvious</p>

<pre><code>ls -dR
</code></pre>

<p>does not work.</p>

<p>I am currently using</p>

<pre><code>find /path/ -type d -ls
</code></pre>

<p>but the output is not what I need (plain listing of sub-folders)</p>

<p>Is there a way out?</p>
","<p>Assuming you just want the name of each directory:</p>

<pre><code>find /path/ -type d -print
</code></pre>
","32681"
"How can I look up a username by id in linux?","120976","","<p>The command <code>id</code> can be used to look up a user's <code>uid</code>, for example:</p>

<pre><code>$ id -u ubuntu
1000
</code></pre>

<p>Is there a command to lookup up a username from a <code>uid</code>? I realize this can be done by looking at the <code>/etc/passwd</code> file but I'm asking if there is an existing command to to this, especially if the user executing it is not root.</p>

<p>I'm not looking for the <em>current</em> user's username, i.e. I am not looking for <code>whoami</code> or <code>logname</code>.</p>

<p>This also made me wonder if on shared web hosting this is a security feature, or am I just not understanding something correctly?</p>

<p>For examination, the <code>/etc/passwd</code> file from a shared web host:</p>

<pre><code>root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
sync:x:5:0:sync:/sbin:/bin/sync
shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
halt:x:7:0:halt:/sbin:/sbin/halt
mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
news:x:9:13:news:/etc/news:
uucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin
operator:x:11:0:operator:/root:/sbin/nologin
games:x:12:100:games:/usr/games:/sbin/nologin
gopher:x:13:30:gopher:/var/gopher:/sbin/nologin
ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
nobody:x:99:99:Nobody:/:/sbin/nologin
nscd:x:28:28:NSCD Daemon:/:/sbin/nologin
vcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin
pcap:x:77:77::/var/arpwatch:/sbin/nologin
rpc:x:32:32:Portmapper RPC user:/:/sbin/nologin
mailnull:x:47:47::/var/spool/mqueue:/sbin/nologin
smmsp:x:51:51::/var/spool/mqueue:/sbin/nologin
oprofile:x:16:16:Special user account to be used by OProfile:/home/oprofile:/sbin/nologin
sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
dbus:x:81:81:System message bus:/:/sbin/nologin
avahi:x:70:70:Avahi daemon:/:/sbin/nologin
rpcuser:x:29:29:RPC Service User:/var/lib/nfs:/sbin/nologin
haldaemon:x:68:68:HAL daemon:/:/sbin/nologin
xfs:x:43:43:X Font Server:/etc/X11/fs:/sbin/nologin
avahi-autoipd:x:100:104:avahi-autoipd:/var/lib/avahi-autoipd:/sbin/nologin
named:x:25:25:Named:/var/named:/sbin/nologin
mailman:x:32006:32006::/usr/local/cpanel/3rdparty/mailman/mailman:/usr/local/cpanel/bin/noshell
dovecot:x:97:97:dovecot:/usr/libexec/dovecot:/sbin/nologin
mysql:x:101:105:MySQL server:/var/lib/mysql:/bin/bash
cpaneleximfilter:x:32007:32009::/var/cpanel/userhomes/cpaneleximfilter:/usr/local/cpanel/bin/noshell
nagios:x:102:106:nagios:/var/log/nagios:/bin/sh
ntp:x:38:38::/etc/ntp:/sbin/nologin
myuser:x:1747:1744::/home/myuser:/usr/local/cpanel/bin/jailshell
</code></pre>

<p>And here is a sample directory listing of <code>/tmp/</code></p>

<pre><code>drwx------  3 root     root        1024 Apr 16 02:09 spamd-22217-init/
drwxr-xr-x  2      665      664    1024 Apr  4 00:05 update-cache-44068ab4/
drwxr-xr-x  4      665      664    1024 Apr 17 15:17 update-extraction-44068ab4/
-rw-rw-r--  1      665      664   43801 Apr 17 15:17 variable.zip
-rw-r--r--  1      684      683    4396 Apr 17 07:01 wsdl-13fb96428c0685474db6b425a1d9baec
</code></pre>

<p>We can see <code>root</code> is the owner of some files, and <code>root</code> is also showing up in <code>/etc/passwd</code> , however the other users/groups all show up as numbers.</p>
","<p><code>ls</code> already performs that lookup. You can perform a user information lookup from the command line with <a href=""https://unix.stackexchange.com/questions/36580/how-can-i-look-up-a-username-by-id-in-linux/36582#36582""><code>getent passwd</code></a>.</p>

<p>If <code>ls</code> shows a user ID instead of a user name, it's because there's no user by that name. Filesystems store user IDs, not user names. If you mount a filesystem from another system, or if a file belongs to a now-deleted user, or if you passed a numerical user ID to <code>chown</code>, you can have a file that belongs to a user ID that doesn't have a name.</p>

<p>On a shared host, you may have access to some files that are shared between several virtual machines, each with their user database. This is a bit weird (why share files but not the users that own them?), but it's technically possible.</p>
","36721"
"How to umount a USB drive?","120793","","<p>I want to know how to <code>umount</code> my USB drive via command line. I am using Ubuntu 12.04 LTS 32-bit.</p>
","<p>Suppose your usb drive is mounted to <code>/media/usb</code> then it would be sufficient to do </p>

<pre><code>sudo umount /media/usb
</code></pre>

<p>Suppose the your usb is <code>/dev/sdb1</code> then you could also do</p>

<pre><code>sudo umount /dev/sdb1
</code></pre>

<p>You may also have a look at the anwers of one of my questions, how to umount all attached usb devices with a single command: <a href=""https://unix.stackexchange.com/questions/43594/umount-all-attached-usb-disks-with-a-single-command"">Umount all attached usb disks with a single command</a></p>
","45821"
"Find command: how to ignore case?","120116","","<p>I am looking for file ""WSFY321.c"" in a huge directory hierarchy.<br>
Usually I would use <a href=""http://www.gnu.org/software/findutils/"">GNU find</a>: <code>find . -name ""WSFY321.c""</code><br>
But I do not know the case, it could be uppercase, lowercase, or a mix of both.</p>

<p>What is the easiest way to find this file?<br>
Is there something better than <code>find . | grep -i ""WSFY321.c""</code> ?</p>
","<p>Recent versions of GNU <code>find</code> have an <code>-iname</code> flag, for case-insensitive name searches.</p>

<pre><code>find . -iname ""WSFY321.c""
</code></pre>
","32158"
"How to stop the loop bash script in terminal?","120075","","<p>For example,</p>

<pre><code>#!/bin/bash
while :
do
    sl
done
</code></pre>

<p>How to terminate this bash script?</p>
","<p>The program <code>sl</code> purposely ignores <code>SIGINT</code>, which is what gets sent when you press <kbd>Ctrl+C</kbd>.  So, firstly, you'll need to tell <code>sl</code> not to ignore <code>SIGINT</code> by adding the <code>-e</code> argument.</p>

<p>If you try this, you'll notice that you can stop each individual <code>sl</code>, but they still repeat.  You need to tell <code>bash</code> to exit after <code>SIGINT</code> as well.  You can do this by putting a <code>trap ""exit"" INT</code> before the loop. </p>

<pre><code>#!/bin/bash
trap ""exit"" INT
while :
do
    sl -e
done
</code></pre>
","48432"
"Why use ""nohup &"" rather than ""exec &""","120009","","<p>I know that, <code>nohup</code> being a binary, it can be reached from any shell. But the <code>exec</code> built-in probably exists in every shell.</p>

<p>Is there a reason to prefer one of them, to the other? </p>
","<p>What's better, a fish or a bicycle? <code>nohup</code> and <code>exec</code> do different things.</p>

<p><code>exec</code> replaces the shell with another program. Using <code>exec</code> in a simple background job isn't useful: <code>exec myprogram; more stuff</code> replaces the shell with <code>myprogram</code> and so doesn't run <code>more stuff</code>, unlike <code>myprogram; more stuff</code> which runs <code>more stuff</code> when <code>myprogram</code> terminates; but <code>exec myprogram &amp; more stuff</code> starts <code>myprogram</code> in the background and then runs <code>more stuff</code>, just like <code>myprogram &amp; more stuff</code>.</p>

<p><a href=""http://en.wikipedia.org/wiki/Nohup""><code>nohup</code></a> runs the specificed program with the SIGHUP signal ignored. When a terminal is closed, the kernel sends SIGHUP to the controlling process in that terminal (i.e. the shell). The shell in turn sends SIGHUP to all the jobs running in the background. Running a job with <code>nohup</code> prevents it from being killed in this way if the terminal dies (which happens e.g. if you were logged in remotely and the connection drops, or if you close your terminal emulator).</p>

<p><code>nohup</code> also redirects the program's output to the file <code>nohup.out</code>. This avoids the program dying because it isn't able to write to its output or error output. Note that <code>nohup</code> doesn't redirect the input. To fully disconnect a program from the terminal where you launched it, use</p>

<pre><code>nohup myprogram &lt;/dev/null &gt;myprogram.log 2&gt;&amp;1 &amp;
</code></pre>
","137950"
"Execute remote commands, completely detaching from the ssh connection","118951","","<p>I have 2 computers, <code>localpc</code> and <code>remoteserver</code>. </p>

<p>I need <code>localpc</code> to execute some commands on <code>remoteserver</code>. One of the things it needs to do is start a backup script that runs for a number of hours. I would like the command on <code>localpc</code> to “fire” and then be running totally independent on <code>remoteserver</code>, like <code>localpc</code> was never there in the first place. </p>

<p>This is what I have done so far:</p>

<p><code>remoteserver</code> contains has the script:</p>

<pre><code>/root/backup.sh
</code></pre>

<p><code>localpc</code> is scheduled to run this:</p>

<pre><code>ssh root@remoteserver 'nohup /root/backup.sh' &amp;
</code></pre>

<p>Am I doing this the right way? Is there a better way to do this? Will I run into any trouble doing it this way?</p>
","<p>You should probably use <code>screen</code> on the remote host, to have a real detached command:</p>

<pre><code>ssh root@remoteserver screen -d -m ./script
</code></pre>
","30507"
"Network interface eth0 not up at start on Debian 6","118939","","<p>I have Debian 6.0. In my interfaces file (<code>/etc/network/interfaces</code>) I have the following lines:</p>

<pre><code>auto eth0
iface eth0 inet static
address 192.168.0.8
netmask 255.255.255.0
gateway 192.168.0.1
</code></pre>

<p>Every time I start the computer, <code>eth0</code> is not working.</p>

<p>When I enter <code>ifconfig -a</code>, <code>eth0</code> is not ""up"" and ""running"".</p>

<p>I have to enter:  </p>

<pre><code> ifconfig eth0 up
 /etc/init.d/networking restart
</code></pre>

<p>... and then it works.</p>

<p>How do I have to change the config in order to have a working <code>eth0</code> in the beginning?</p>
","<blockquote>
  <p>How do I have to change the config in order to have a working ""eth0"" <strong>in the beginning</strong>?</p>
</blockquote>

<p>Whenever I hear ""in the beginning"" it reminds of <code>rc.local</code>.</p>

<p>This is not really a direct answer for solving your problem but it seems that you're experiencing some difficulties with the driver.</p>

<p>For a quick fix, why not using <code>rc.local</code>? if your problem gets solved by typing those 2 commands every time you boot into your box then add them at the end of <code>/etc/rc.local</code>.</p>

<p>For an in-depth fix for your problem, first and before everything else do a:</p>

<pre><code>apt-get update &amp;&amp; apt-get upgrade
</code></pre>

<p>I had the same issue in my Backtrack, it solved after update. Apparently the same topic discussed here: <a href=""http://lists.slug.org.au/archives/slug/2004/04/msg00098.html"" rel=""nofollow"">Debian not starting eth0 at boot</a></p>
","48092"
"How to suspend and resume processes","118813","","<p>In the bash terminal I can hit <kbd>Control</kbd>+<kbd>Z</kbd> to suspend any running process... then I can type <code>fg</code> to resume the process.</p>

<p>Is it possible to suspend a process if I only have it's PID? And if so, what command should I use?</p>

<p>I'm looking for something like:</p>

<pre><code>suspend-process $PID_OF_PROCESS
</code></pre>

<p>and then to resume it with</p>

<pre><code>resume-process $PID_OF_PROCESS
</code></pre>
","<p>You can use <code>kill</code> to stop the process.</p>

<p>For a 'polite' stop to the process (prefer this for normal use), send SIGTSTP:</p>

<pre><code>kill -TSTP [pid]
</code></pre>

<p>For a 'hard' stop, send SIGSTOP:</p>

<pre><code>kill -STOP [pid]
</code></pre>

<p>Note that if the process you are trying to stop by PID is in your shell's job table, it may remain visible there, but terminated, until the process is <code>fg</code>'d again.</p>

<p>To resume execution of the process, sent SIGCONT:</p>

<pre><code>kill -CONT [pid]
</code></pre>
","2112"
"Getting new files to inherit group permissions on Linux","118704","","<p>I am having a problem with permissions on a Linux server. I am used to BSD. When a directory is owned by a group the user who owns it isn't in such as www-data, files created in it will be owned by that group. This is important because I want files to be readable by the webserver (which I will not run as root) but so a user can still put new files in the directory. I can't put the users in www-data because then they can read every other users websites. </p>

<p>I want the webserver to read all websites, I want users to be able to change their own.</p>

<p>The permissions are set like this on the folders at the moment....</p>

<pre><code>drwxr-x--- 3 john www-data 4096 Feb 17 21:27 john
</code></pre>

<p>It is standard behavior on BSD for permissions to work this way. How do I get Linux to do this?</p>
","<p>It sounds like you're describing the <a href=""https://en.wikipedia.org/wiki/Setuid#setuid_and_setgid_on_directories"" rel=""noreferrer"">setgid bit</a> functionality where when a directory that has it set, will force any new files created within it to have their group set to the same group that's set on the parent directory.</p>

<h3>Example</h3>

<pre><code>$ whoami
saml

$ groups
saml wheel wireshark
</code></pre>

<p><em>setup a directory with perms + ownerships</em></p>

<pre><code>$ sudo mkdir --mode=u+rwx,g+rs,g-w,o-rwx somedir
$ sudo chown saml.apache somedir
$ ll -d somedir/
drwxr-s---. 2 saml apache 4096 Feb 17 20:10 somedir/
</code></pre>

<p><em>touch a file as saml in this dir</em></p>

<pre><code>$ whoami
saml

$ touch somedir/afile
$ ll somedir/afile 
-rw-rw-r--. 1 saml apache 0 Feb 17 20:11 somedir/afile
</code></pre>

<p>This will give you approximately what it sounds like you want. If you truly want exactly what you've described though, I think you'll need to resort to Access Control Lists functionality to get that (ACLs).</p>

<h3><a href=""http://en.wikipedia.org/wiki/Access_control_list"" rel=""noreferrer"">ACLs</a></h3>

<p>If you want to get a bit more control over the permissions on the files that get created under the directory, <code>somedir</code>, you can add the following ACL rule to set the default permissions like so.</p>

<p><em>before</em></p>

<pre><code>$ ll -d somedir
drwxr-s---. 2 saml apache 4096 Feb 17 20:46 somedir
</code></pre>

<p><em>set permissions</em></p>

<pre><code>$ sudo setfacl -Rdm g:apache:rx somedir
$ ll -d somedir/
drwxr-s---+ 2 saml apache 4096 Feb 17 20:46 somedir/
</code></pre>

<p>Notice the <code>+</code> at the end, that means this directory has ACLs applied to it.</p>

<pre><code>$ getfacl somedir
# file: somedir
# owner: saml
# group: apache
# flags: -s-
user::rwx
group::r-x
other::---
default:user::rwx
default:group::r-x
default:group:apache:r-x
default:mask::r-x
default:other::---
</code></pre>

<p><em>after</em></p>

<pre><code>$ touch somedir/afile
$ ll somedir/afile 
-rw-r-----+ 1 saml apache 0 Feb 17 21:27 somedir/afile
$ 

$ getfacl somedir/afile
# file: somedir/afile
# owner: saml
# group: apache
user::rw-
group::r-x              #effective:r--
group:apache:r-x        #effective:r--
mask::r--
other::---
</code></pre>

<p>Notice with the default permissions (<code>setfacl -Rdm</code>) set so that the permissions are (<code>r-x</code>) by default (<code>g:apache:rx</code>). This forces any new files to only have their <code>r</code> bit enabled.</p>
","115632"
"How do I do a ls and then sort the results by date created?","118282","","<p>In what order are the dated ordered by? Certainly not alphanumeric order.</p>

<p><code>ls -lt</code> sorts by modification time. But I need creation time.</p>
","<p>Most unices do not have a concept of file creation time. You can't make <code>ls</code> print it because the information is not recorded. If you need creation time, use a <a href=""http://en.wikipedia.org/wiki/Revision_control"">version control system</a>: define creation time as the check-in time.</p>

<p>If your unix variant has a creation time, look at its documentation. For example, on <a href=""http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man1/ls.1.html"">Mac OS X</a> (the only example I know of¹), use <code>ls -tU</code>. Windows also stores a creation time, but it's not always exposed to ports of unix utilities, for example Cygwin <code>ls</code> doesn't have an option to show it. The <code>stat</code> utility can show the creation time, called “birth time” in GNU utilities, so under Cygwin you can show files sorted by birth time with <code>stat -c '%W %n' * | sort -k1n</code>.</p>

<p>Note that <strong>the ctime</strong> (<code>ls -lc</code>) <strong>is not the file creation time</strong>, it's the <a href=""http://en.wikipedia.org/wiki/Inode"">inode</a> change time. The inode change time is updated whenever anything about the file changes (contents or metadata) except that the ctime isn't updated when the file is merely read (even if the atime is updated). In particular, the ctime is always more recent than the mtime (file content modification time) unless the mtime has been explicitly set to a date in the future.</p>
","20464"
"Download using wget to a different directory than current directory","117991","","<p>I need to use <code>wget</code> to download a file to the directory <code>/var/cache/foobar/</code> (so, as an example, if I download <code>stackexchange-site-list.txt</code>, it'd be downloaded to <code>/var/cache/foobar/stackexchange-site-list.txt</code>)</p>

<p>Is this possible? <code>curl</code> would also be an option, but I'd prefer to not use <code>curl</code>, since it's not installed by default. </p>
","<p>If you know the name of the file ahead of time, you can use the <code>-O</code> option to wget to tell it where to write the file.</p>

<pre><code>wget -O /var/cache/foobar/stackexchange-site-list.txt http://url.to/stackexchange-site-list.txt
</code></pre>
","23504"
"Command line to return to the GUI after Ctrl-Alt-F1?","117906","","<p>I'm unable to return to the GUI with <code>Ctrl-Alt-F7</code> (or any of the 12 function keys). I have some unsaved work and I don't want to lose them. <strong>Are there any other key combinations that will allow me to switch back?</strong></p>

<p>Here is what I did:</p>

<ol>
<li>I pressed <code>Ctrl-Alt-F1</code> and it showed a text-based login screen as usual</li>
<li>Then I pressed <code>Ctrl-Alt-F7</code> and it showed a screen full of text (I can't remember what they were)</li>
<li>Then I pressed <code>Ctrl-Alt-F8</code> and it showed log messages that resembles <code>/var/log/messages</code>. Some entries are from <code>automount</code>, some from <code>sendmail</code>, and none are errors.</li>
<li>Pressing any of the <code>Ctrl-Alt-Fn</code> combinations now has no effect. The cap-lock and num-lock LED no longer respond to their corresponding keys. I can use the mouse to highlight the text on the screen, but nothing else.</li>
</ol>

<p>Any idea what happened?</p>

<p>I can still login to the system via SSH. GUI applications that I was using (e.g. <code>opera</code>) are still running and consuming tiny amounts of CPU as usual, as reported by <code>top</code>. <strong>Is it possible to switch back to the GUI via the command line?</strong> If possible, I don't want to restart X, because doing so will kill all the GUI applications.</p>

<p>System info:</p>

<pre><code> Red Hat Enterprise Linux Client release 5.7
 Linux 2.6.18-238.12.1.el5 SMP x86_64
 gnome-desktop: 2.16.0-1.fc6
 xorg-x11-server-Xorg: 1.1.1-48.76.el5_7.5
</code></pre>

<hr>

<p>Thanks to Shawn I was able to get back using <code>chvt 9</code>.</p>

<p>Further experiments shows that if I go to the 8th virtual terminal (either by <code>Ctrl-Alt-F8</code> or <code>chvt 8</code>), I will not be able to switch to any other terminals using <code>Ctrl-Alt-Fx</code> keys. Now sure if this is a bug.</p>
","<p><code>chvt</code> allows you to change your virtual terminal.</p>

<p>From <code>man chvt</code>:</p>

<blockquote>
  <p>The command chvt N makes /dev/ttyN the foreground terminal. (The
  corresponding screen is created if it did not exist yet. To get rid of
  unused VTs, use deallocvt(1).) The key combination (Ctrl-)LeftAlt-FN
  (with N in the range 1-12) usually has a similar effect.</p>
</blockquote>
","23364"
"mail: send email with attachment from commandline","117672","","<p>I know how to send an email from command line (script)</p>

<pre><code>echo ""body"" | mail -s ""subject"" my@email.com
</code></pre>

<p>Is it possible to send attachments from commandline (script) as well?</p>

<p>I am using <code>heirloom-mailx</code> on Debian Wheezy.</p>
","<p>The simple way: to use <code>uuencode</code> (part of <code>sharutils</code> package). Any formatting or body text are unavailable. Just a email with attachement and custom subject.</p>

<pre><code>uuencode /path/to/file file_name.ext | mail -s subject my@email.com
</code></pre>

<p>The complex way: to use <code>sendmail</code> and html formatting:</p>

<pre><code>v_mailpart=""$(uuidgen)/$(hostname)""
echo ""To: my@email.com
Subject: subject
Content-Type: multipart/mixed; boundary=\""$v_mailpart\""
MIME-Version: 1.0

This is a multi-part message in MIME format.
--$v_mailpart
Content-Type: text/html
Content-Disposition: inline

&lt;html&gt;&lt;body&gt;Message text itself.&lt;/body&gt;&lt;/html&gt;

--$v_mailpart
Content-Transfer-Encoding: base64
Content-Type: application/octet-stream; name=file_name.ext
Content-Disposition: attachment; filename=file_name.ext

`base64 /path/to/file`
 --$v_mailpart--"" | /usr/sbin/sendmail -t
</code></pre>

<p>in case with several attachments last part may be repeated.</p>
","102095"
"Open a window on a remote X display (why ""Cannot open display"")?","117481","","<p>Once upon a time,</p>

<pre><code>DISPLAY=:0.0 totem /path/to/movie.avi
</code></pre>

<p>after ssh 'ing into my desktop from my laptop would cause totem to play <code>movie.avi</code> on my desktop.</p>

<p>Now it gives the error:</p>

<blockquote>
<pre><code>No protocol specified
Cannot open display:
</code></pre>
</blockquote>

<p>I reinstalled Debian squeeze when it went stable on both computers, and I guess I broke the config. </p>

<p>I've googled on this, and cannot for the life of me figure out what I'm supposed to be doing. </p>

<p>(VLC has an HTTP interface that works, but it isn't as convenient as ssh.)</p>

<p>The same problem arises when I try to run this from a cron job.</p>
","<p><sub><em>(Adapted from <a href=""https://superuser.com/questions/190801/linux-wmctrl-cannot-open-display-when-session-initiated-via-sshscreen#190878"">Linux: wmctrl cannot open display when session initiated via ssh+screen</a>)</em></sub></p>

<h2>DISPLAY and AUTHORITY</h2>

<p>An X program needs two pieces of information in order to connect to an X display.</p>

<ul>
<li><p>It needs the address of the display, which is typically <code>:0</code> when you're logged in locally or <code>:10</code>, <code>:11</code>, etc. when you're logged in remotely (but the number can change depending on how many X connections are active). The address of the display is normally indicated in the <code>DISPLAY</code> environment variable.</p></li>
<li><p>It needs the password for the display. X display passwords are called <em>magic cookies</em>. Magic cookies are not specified directly: they are always stored in X authority files, which are a collection of records of the form “display <code>:42</code> has cookie <code>123456</code>”. The X authority file is normally indicated in the <code>XAUTHORITY</code> environment variable. If <code>$XAUTHORITY</code> is not set, programs use <code>~/.Xauthority</code>.</p></li>
</ul>

<p>You're trying to act on the windows that are displayed on your desktop. If you're the only person using your desktop machine, it's very likely that the display name is <code>:0</code>. Finding the location of the X authority file is harder, because with gdm as set up under Debian squeeze or Ubuntu 10.04, it's in a file with a randomly generated name. (You had no problem before because earlier versions of gdm used the default setting, i.e. cookies stored in <code>~/.Xauthority</code>.)</p>

<h2>Getting the values of the variables</h2>

<p>Here are a few ways to obtain the values of <code>DISPLAY</code> and <code>XAUTHORITY</code>:</p>

<ul>
<li><p>You can systematically start a screen session from your desktop, perhaps automatically in your login scripts (from <code>~/.profile</code>; but do it only if logging in under X: test if <code>DISPLAY</code> is set to a value beginning with <code>:</code> (that should cover all the cases you're likely to encounter)). In <code>~/.profile</code>:</p>

<pre><code>case $DISPLAY in
  :*) screen -S local -d -m;;
esac
</code></pre>

<p>Then, in the ssh session:</p>

<pre><code>screen -d -r local
</code></pre></li>
<li><p>You could also save the values of <code>DISPLAY</code> and <code>XAUTHORITY</code> in a file and recall the values. In <code>~/.profile</code>:</p>

<pre><code>case $DISPLAY in
  :*) export | grep -E '(^| )(DISPLAY|XAUTHORITY)=' &gt;~/.local-display-setup.sh;;
esac
</code></pre>

<p>In the ssh session:</p>

<pre><code>. ~/.local-display-setup.sh
screen
</code></pre></li>
<li><p>You could detect the values of <code>DISPLAY</code> and <code>XAUTHORITY</code> from a running process. This is harder to automate. You have to figure out the PID of a process that's connected to the display you want to work on, then get the environment variables from <code>/proc/$pid/environ</code> (<code>eval export $(&lt;/proc/$pid/environ tr \\0 \\n | grep -E '^(DISPLAY|XAUTHORITY)=')</code>¹).</p></li>
</ul>

<h2>Copying the cookies</h2>

<p>Another approach (following a suggestion by <a href=""https://unix.stackexchange.com/users/3459/arrowmaster"">Arrowmaster</a>) is to not try to obtain the value of <code>$XAUTHORITY</code> in the ssh session, but instead to make the X session copy its cookies into <code>~/.Xauthority</code>. Since the cookies are generated each time you log in, it's not a problem if you keep stale values in <code>~/.Xauthority</code>.</p>

<p>There can be a security issue if your home directory is accessible over NFS or other network file system that allows remote administrators to view its contents. They'd still need to connect to your machine somehow, unless you've enabled X TCP connections (Debian has them off by default). So for most people, this either does not apply (no NFS) or is not a problem (no X TCP connections).</p>

<p>To copy cookies when you log into your desktop X session, add the following lines to <code>~/.xprofile</code> or <code>~/.profile</code> (or some other script that is read when you log in):</p>

<pre><code>case $DISPLAY:$XAUTHORITY in
  :*:?*)
    # DISPLAY is set and points to a local display, and XAUTHORITY is
    # set, so merge the contents of `$XAUTHORITY` into ~/.Xauthority.
    XAUTHORITY=~/.Xauthority xauth merge ""$XAUTHORITY"";;
esac
</code></pre>

<p>¹ <sub> In principle this lacks proper quoting, but in this specific instance <code>$DISPLAY</code> and <code>$XAUTHORITY</code> won't contain any shell metacharacter. </sub></p>
","10126"
"Remove GPT - Default back to MBR","117434","","<p>I keep receiving this error:</p>

<blockquote>
  <p>Warning!! Unsupported GPT (GUID Partition Table) detected. Use GNU Parted</p>
</blockquote>

<p>I want to go back to the normal MBR. I found some advice <a href=""http://lukas.zapletalovi.com/2011/12/how-to-get-rid-of-guid-partition-table.html"">here</a> and did:</p>

<pre><code>parted /dev/sda
mklabel msdos
quit
</code></pre>

<p>But when I get to the <code>mklabel</code> option it spits out a warning that I will lose all data on <code>/dev/sda</code>. Is there a way to get the normal MBR back without formatting the disk?</p>
","<p>That link you posted looks like a very ugly hack type solution.  </p>

<p>However, according to the man page, <code>gdisk</code>, which is used to convert MBR -> GPT, also has an option in the ""recovery &amp; transformation"" menu to convert GPT -> MBR; the <code>g</code> key will:     </p>

<blockquote>
  <p>Convert GPT into MBR and exit. This option converts as many partitions
  as  possible  into  MBR form,  destroys  the  GPT  data  structures,
  saves the new MBR, and exits.  Use this option if you've tried GPT and
  find that MBR works better for you.  Note that this function generates
  up to  four  primary MBR partitions or three primary partitions and as
  many logical partitions as           can be generated. Each logical
  partition requires at least one unallocated  block  immediately 
  before  its first block.</p>
</blockquote>

<p>I'd try that first.</p>
","61143"
"What user should apache and PHP be running as? What permissions should /var/www files have?","117376","","<p>I just spun up an Ubuntu 11.10 box and then ran <code>apt-get install apache2 php5</code> to install apache2 and PHP 5 on the box. Now it is functioning as a ""web server"" and it loads the ""It Works!"" page. Now I'm trying to tighten up security and I have the following questions about linux web servers:</p>

<ol>
<li>Who should apache be running as?</li>
<li>What group(s) should this user be in?</li>
<li>What package(s) can make PHP (and Apache?) run as the owner of the files? (like on shared web hosts) Should I use these packages? Are they easy / feasible to maintain on a small system?</li>
<li>What should the default permissions be for files and folders being served out to the web with apache running as <code>www-data</code>? For apache/php running as the user?</li>
</ol>

<p>I have done the following things in examination of the default setup:</p>

<h3>File Structure</h3>

<p>When I <code>cd /</code> and do a <code>ls -al</code> listing of the contents, I see <code>/var</code>:</p>

<pre><code>drwxr-xr-x 13 root root  4096 2012-02-04 20:47 var/
</code></pre>

<p>If I <code>cd</code> into <code>var</code> and do <code>ls -al</code> I see:</p>

<pre><code>drwxr-xr-x  2 root root  4096 2012-02-04 20:47 www/
</code></pre>

<p>Finally, inside <code>/var/www</code> I see:</p>

<pre><code>drwxr-xr-x  2 root root 4096 2012-02-04 20:47 ./
drwxr-xr-x 13 root root 4096 2012-02-04 20:47 ../
-rw-r--r--  1 root root  177 2012-02-04 20:47 index.html
</code></pre>

<p>My key takeaway is that so far all of these files belong to <code>root:root</code>, files have permissions of 644, and directories have permissions of 755.</p>

<h3>Apache's Permissions</h3>

<p>If I create a file as root in <code>/var/www/test.php</code> with the contents:</p>

<pre><code>&lt;?php echo shell_exec('whoami');
</code></pre>

<p>and load that file into a browser it tells me <code>www-data</code>, which is the same as in the <code>/etc/apache2/envvars</code> file:</p>

<pre><code>export APACHE_RUN_USER=www-data
export APACHE_RUN_GROUP=www-data
</code></pre>

<p>If I do <code>ps aux | grep -i apache</code> I see the following:</p>

<pre><code>root      1916  1.2 104664  7488 Ss   20:47 /usr/sbin/apache2 -k start
www-data  1920  0.8 105144  5436 S    20:47 /usr/sbin/apache2 -k start
www-data  1921  1.0 105144  6312 S    20:47 /usr/sbin/apache2 -k start
www-data  1922  0.7 104688  4624 S    20:47 /usr/sbin/apache2 -k start
www-data  1923  0.7 104688  4624 S    20:47 /usr/sbin/apache2 -k start
www-data  1924  0.7 104688  4624 S    20:47 /usr/sbin/apache2 -k start
www-data  1925  0.7 104688  4624 S    20:47 /usr/sbin/apache2 -k start
</code></pre>

<p>So who is apache running as? It looks like perhaps the first process is as <code>root</code>, maybe from the <code>/etc/init.d/apache</code> script when the system started, and the other ones as <code>www-data</code> spawned from the first. Is that correct?</p>

<p>Next, if I type in <code>groups www-data</code> then I see <code>www-data : www-data</code> - so it looks to only be in the <code>www-data</code> group. I'm guessing this is standard practice as well.</p>

<h3>Shared Hosting and Security</h3>

<p>So if I understand things correctly, if apache is running as <code>www-data</code> and I want apache to be able to read a directory, the <code>x</code> bit needs to be set for the world (other) group (<code>o+x</code>), and that also needs to be set on all parent directories all the way  up the chain (<code>www</code>, <code>var</code>). And if I want apache to be able to read from a file, then the <code>o+r</code> bit needs to be set.</p>

<p>Unfortunately I believe this introduces a security hole for multiple applications and/or multiple users on the same linux box: All web files need to be world-readable, and so they are also accessible by other applications and other users on the system. If one application installed on the system had a security vulnerability that allowed raw, unvalidated user input, which was then executed by PHP, a remote attacker could then browse all the other files on the web system which were world readable. Likewise, if the box had multiple users, and a user knew the path of another user's web files, s/he could then read the file contents (and see sensitive things like database connection strings, etc).</p>

<p>I've heard of two packages, <code>suphp</code> and <code>phpsuexec</code> that deal with allowing users' files to be served out ""as them"" on a shared system. One of the niceties of this is that it allows web applications (like Wordpress) to create and modify files - very helpful for adding themes, plugins, and upgrading software. Of course it is probably more secure to do these things manually, but can a compromise be made perhaps with one of the packages mentioned above? Or by possibly using <code>chown</code> to make the wordpress directory group belong to <code>www-data</code> and set the sticky bit on the group (<code>g+s</code>)?</p>

<p>I have only used these as the end user of a web hosting company, and so I don't know the ins-and-outs of them, and if they are even reasonable to install on a small system, or if there are some other security measures I should use instead, but I thought I would mention them here as they seem like one possible way to address some of my concerns.</p>

<h3>Back to the Questions</h3>

<ol>
<li>Who should apache be running as?</li>
<li>What group(s) should this user be in?</li>
<li>What package(s) can make PHP (and Apache?) run as the owner of the files? (like on shared web hosts) Should I use these packages? Are they easy / feasible to maintain on a small system?</li>
<li>What should the default permissions be for files and folders being served out to the web with apache running as <code>www-data</code>? For apache/php running as the user?</li>
</ol>
","<ol>
<li>not root</li>
<li>not root</li>
<li><a href=""http://httpd.apache.org/docs/current/suexec.html"">SuEXEC</a></li>
<li>Depends. 644 for files and 755 for folders are a safeish default.</li>
</ol>

<p><strong>Don't change ownership of anything to www-data unless you want php to be able to edit the contents of that file/folder</strong></p>

<p>Irrespective of anything else you do: folders need read and execute permissions for the user to find files; files need read permissions for the user to read them. If you get any permissions errors when changing things - you've managed to remove these fundamentally required permissions.</p>

<p>If you are not writing any files via your php application, you can leave files owned by you:you. In this circumstance the world permission (xx4/5) is the one which applies.</p>

<p>If you leave the files as owned by you:you with file permissions of 644 (files) what that would mean is that <em>only</em> you can edit the website files - www-data is not you - so it cannot edit the files.</p>

<p>If you want to restrict access to apache + you and block out all other access <code>chown -R you:www-data *</code>. With file permissions of 640 and folder permissions of 750 you can edit, www-data can read - because then apache reads the group permission (x4/5x).</p>

<p>Restrict to a minimum the paths you allow apache/php to write to - if there's a tmp dir the application needs to write to - allow it to write to <em>that folder only</em> - and for any writable locations if at all possible make sure it's <em>outside</em> the document root or take steps to ensure this writable path is not web-accessible.</p>

<p>Note that ""you"" should <em>not</em> be root. Allowing direct ssh access as root is an indicator of other security lapses (such as <em>not</em> disallowing password login), but that's a whole bunch of questions unto itself.</p>
","30888"
"How do I get sar to show for the previous day?","117246","","<p>on our servers, typing <code>sar</code> show's the system load statistics for today starting at midnight, is it possible to show yesterdays statistics?</p>
","<p>Usually, <a href=""http://sebastien.godard.pagesperso-orange.fr/documentation.html"" rel=""noreferrer"">sysstat</a>, which provides a <code>sar</code> command, keeps logs in <code>/var/log/sysstat/</code> or <code>/var/log/sa/</code> with filenames such as <code>/var/log/sysstat/sadd</code> where <code>dd</code> is a numeric value for the day of the month (starting at 01).  By default, the file from the current day is used; however, you can change the file that is used with the <code>-f</code> command line switch.  Thus for the 3rd of the month you would do something like:</p>

<pre><code>sar -f /var/log/sysstat/sa03
</code></pre>

<p>If you want to restrict the time range, you can use the <code>-s</code> and <code>-e</code> parameters.  If you want to routinely get yesterday's file and can never remember the date and have GNU date you could try</p>

<pre><code>sar -f /var/log/sysstat/sa$(date +%d -d yesterday)
</code></pre>

<p>I highly recommend reading the <a href=""http://linux.die.net/man/1/sar"" rel=""noreferrer"">manual page for sar</a>.</p>
","5417"
"How can I benchmark my HDD?","116939","","<p>I've seen commands to benchmark one's HDD such as this using <code>dd</code>:</p>

<pre><code>$ time sh -c ""dd if=/dev/zero of=ddfile bs=8k count=250000 &amp;&amp; sync""
</code></pre>

<p>Are there better methods to do so than this?</p>
","<p>I usually use <code>hdparm</code> to benchmark my HDD's. You can benchmark both the direct reads and the cached reads. You'll want to run the commands a couple of times to establish an average value.</p>

<h3>Examples</h3>

<p>Here's a direct read.</p>

<pre><code>$ sudo hdparm -t /dev/sda2

/dev/sda2:
 Timing buffered disk reads: 302 MB in  3.00 seconds = 100.58 MB/sec
</code></pre>

<p>And here's a cached read.</p>

<pre><code>$ sudo hdparm -T /dev/sda2

/dev/sda2:
 Timing cached reads:   4636 MB in  2.00 seconds = 2318.89 MB/sec
</code></pre>

<h3>Details</h3>

<pre><code>-t     Perform  timings  of  device reads for benchmark and comparison 
       purposes.  For meaningful results, this operation should be repeated
       2-3 times on an otherwise inactive system (no other active processes) 
       with at least a couple of megabytes of free memory.  This displays  
       the  speed of reading through the buffer cache to the disk without 
       any prior caching of data.  This measurement is an indication of how 
       fast the drive can sustain sequential data reads under Linux, without 
       any filesystem overhead.  To ensure accurate  measurements, the 
       buffer cache is flushed during the processing of -t using the 
       BLKFLSBUF ioctl.

-T     Perform timings of cache reads for benchmark and comparison purposes.
       For meaningful results, this operation should be repeated 2-3
       times on an otherwise inactive system (no other active processes) 
       with at least a couple of megabytes of free memory.  This displays
       the speed of reading directly from the Linux buffer cache without 
       disk access.  This measurement is essentially an indication of the
       throughput of the processor, cache, and memory of the system under 
       test.
</code></pre>

<h3>Using dd</h3>

<p>I too have used <code>dd</code> for this type of testing as well. One modification I would make to the above command is to add this bit to the end of your command, <code>; rm ddfile</code>.</p>

<pre><code>$ time sh -c ""dd if=/dev/zero of=ddfile bs=8k count=250000 &amp;&amp; sync""; rm ddfile
</code></pre>

<p>This will remove the <code>ddfile</code> after the command has completed. <strong>NOTE:</strong> <code>ddfile</code> is a transient file that you don't need to keep, it's the file that <code>dd</code> is writing to (<code>of=ddfile</code>), when it's putting your HDD under load.</p>

<h3>Going beyond</h3>

<p>If you need more rigorous testing of your HDD's you can use <a href=""http://www.coker.com.au/bonnie++/"">Bonnie++</a>.</p>

<h3>References</h3>

<ul>
<li><a href=""https://romanrm.net/dd-benchmark"">How to use 'dd' to benchmark your disk or CPU?</a></li>
<li><a href=""http://www.jamescoyle.net/how-to/599-benchmark-disk-io-with-dd-and-bonnie"">Benchmark disk IO with DD and Bonnie++</a></li>
</ul>
","108839"
"Why doesn't my Bash script recognize aliases?","116842","","<p>In my <code>~/.bashrc</code> file reside two definitions:</p>

<ol>
<li><code>commandA</code>, which is an alias to a longer path</li>
<li><code>commandB</code>, which is an alias to a Bash script</li>
</ol>

<p>I want to process the same file with these two commands, so I wrote the following Bash script:</p>

<hr>

<pre><code>#!/bin/bash

for file in ""$@""
    do
    commandA $file
    commandB $file
done
</code></pre>

<p>Even after logging out of my session and logging back in, Bash prompts me with <code>command not found</code> errors for both commands when I run this script.</p>

<p>What am I doing wrong?</p>
","<p>First of all, as ddeimeke said, aliases by default are not expanded in non-interactive shells.</p>

<p>Second, <code>.bashrc</code> is not read by non-interactive shells unless you set the <code>BASH_ENV</code> environment variable.</p>

<p>But most importantly: don't do that! Please? One day you will move that script somewhere where the necessary aliases are not set and it will break again.</p>

<p>Instead set and use environment variables as shortcuts in your script:</p>

<pre><code>#!/bin/bash

CMDA=/path/to/gizmo
CMDB=/path/to/huzzah.sh

for file in ""$@""
do
    $CMDA ""$file""
    $CMDB ""$file""
done
</code></pre>
","1499"
"How to recover a 'lost' screen session?","116839","","<p>Somehow I managed to close a screen window without screen 'noticing' it, so the session is still flagged as <code>attached</code>. This prevents me from re-attaching to this session. What can I do?</p>

<pre><code>me@iupr-serv8:~$ screen -r
There are several suitable screens on:
    25028.pts-19.XXX-serv8  (01/05/2012 07:15:34 PM)    (Attached)
    24658.pts-19.XXX-serv8  (01/05/2012 07:11:38 PM)    (Detached)
    24509.pts-19.XXX-serv8  (01/05/2012 07:10:00 PM)    (Detached)
    18676.pts-5.XXX-serv8   (01/02/2012 06:55:33 PM)    (Attached)
Type ""screen [-d] -r [pid.]tty.host"" to resume one of them.
me@XXX-serv8:~$ screen -r 25028
There is a screen on:
    25028.pts-19.XXX-serv8  (01/05/2012 07:15:33 PM)    (Attached)
There is no screen to be resumed matching 25028.
</code></pre>

<p><strong>[update]</strong></p>

<p>In the end I found out, that the session was not lost, but the ID of the first session is <code>0</code>. The second session than has the ID <code>1</code>.</p>
","<p>Try detaching it first with <code>screen -d</code>. If that doesn't work, you can try, in increasing order of <em>emphasis</em>,</p>

<pre><code>   -d|-D [pid.tty.host]
        does  not  start screen, but detaches the elsewhere running screen session. It has the
        same effect as typing ""C-a d"" from screen's controlling terminal. -D is the equivalent
        to  the  power  detach key.  If no session can be detached, this option is ignored. In
        combination with the -r/-R option more powerful effects can be achieved:

   -d -r   Reattach a session and if necessary detach it first.

   -d -R   Reattach a session and if necessary detach or even create it first.

   -d -RR  Reattach a session and if necessary detach or create it. Use the first  session  if
           more than one session is available.

   -D -r   Reattach a session. If necessary detach and logout remotely first.

   -D -R   Attach  here and now. In detail this means: If a session is running, then reattach.
           If necessary detach and logout remotely first.  If it was not running create it and
           notify the user. This is the author's favorite.

   -D -RR  Attach here and now. Whatever that means, just do it.
</code></pre>
","28676"
"combine text files column-wise","116581","","<p>I have two text files. The first one has content:</p>

<pre><code>Languages
Recursively enumerable
Regular
</code></pre>

<p>while the second one has content:</p>

<pre><code>Minimal automaton
Turing machine
Finite
</code></pre>

<p>I want to combine them into one file column-wise. So I tried <code>paste 1 2</code> and its output is:</p>

<pre><code>Languages   Minimal automaton
Recursively enumerable  Turing machine
Regular Finite
</code></pre>

<p>However I would like to have the columns aligned well such as</p>

<pre><code>Languages               Minimal automaton
Recursively enumerable  Turing machine
Regular                 Finite
</code></pre>

<p>I was wondering if it would be possible to achieve that without manually handling? </p>

<hr>

<p>Added:</p>

<p>Here is another example, where Bruce method almost nails it, except some slight misalignment about which I wonder why?</p>

<pre><code>$ cat 1
Chomsky hierarchy
Type-0
—

$ cat 2
Grammars
Unrestricted

$ paste 1 2 | pr -t -e20
Chomsky hierarchy   Grammars
Type-0              Unrestricted
—                    (no common name)
</code></pre>
","<p>You just need the <a href=""http://linux.die.net/man/1/column"" rel=""noreferrer""><code>column</code></a> command, and tell it to use tabs to separate columns</p>

<pre><code>paste file1 file2 | column -s $'\t' -t
</code></pre>

<hr>

<p>To address the ""empty cell"" controversy, we just need the <code>-n</code> option to <code>column</code>:</p>

<pre><code>$ paste &lt;(echo foo; echo; echo barbarbar) &lt;(seq 3) | column -s $'\t' -t
foo        1
2
barbarbar  3

$ paste &lt;(echo foo; echo; echo barbarbar) &lt;(seq 3) | column -s $'\t' -tn
foo        1
           2
barbarbar  3
</code></pre>

<p>My column man page indicates <code>-n</code> is a ""Debian GNU/Linux extension."" My Fedora system does not exhibit the empty cell problem: it appears to be derived from BSD and the man page says ""Version 2.23 changed the -s option to be non-greedy""</p>
","16465"
"How to find and delete files older than specific days in unix?","116091","","<p>I have got one folder for log with 7 folders in it. Those seven folders too have subfolders in them and those subfolders have subfolders too. I want to delete all the files older than 15 days in all folders including subfolders without touching folder structrure, that means only files.</p>

<pre><code>mahesh@inl00720:/var/dtpdev/tmp/ &gt; ls
A1  A2  A3  A4  A5  A6  A7

mahesh@inl00720:/var/dtpdev/tmp/A1/ &gt; ls
B1 B2 B3 B4 file1.txt file2.csv
</code></pre>
","<p>You could start by saying <code>find /var/dtpdev/tmp/ -type f -mtime +15</code>.
This will find all files older than 15 days and print their names.
Optionally, you can specify <code>-print</code> at the end of the command,
but that is the default action.
It is advisable to run the above command first,
to see what files are selected.</p>

<p>After you verify that the <code>find</code> command is listing the files
that you want to delete (and no others),
you can add an ""action"" to delete the files.
The typical actions to do this are:</p>

<ol>
<li><p><code>-exec rm -f {} \;</code> (or, equivalently, <code>-exec rm -f {} ';'</code>)<br>
This will run <code>rm -f</code> on each file; e.g.,</p>

<pre><code>rm -f /var/dtpdev/tmp/A1/B1; rm -f /var/dtpdev/tmp/A1/B2; rm -f /var/dtpdev/tmp/A1/B3; …
</code></pre></li>
<li><p><code>-exec rm -f {} +</code><br>
This will run <code>rm -f</code> on many files at once; e.g.,</p>

<pre><code>rm -f /var/dtpdev/tmp/A1/B1 /var/dtpdev/tmp/A1/B2 /var/dtpdev/tmp/A1/B3 …
</code></pre>

<p>so it may be slightly faster than option 1. 
(It may need to run <code>rm -f</code> a few times if you have thousands of files.)</p></li>
<li><code>-delete</code><br>
This tells <code>find</code> itself to delete the files, without running <code>rm</code>.
This may be infinitesimally faster than the <code>-exec</code> variants,
but it will not work on all systems.</li>
</ol>

<p>So, if you use option 2, the whole command would be:</p>

<pre><code>find /var/dtpdev/tmp/ -type f -mtime +15 -exec rm -f {} +
</code></pre>
","155185"
"How to limit network bandwidth?","116022","","<p>We are hosting an application on remote server. We need to test it with a limited network bandwidth (for users with bad Internet access).</p>

<p>Can I limit my internet bandwidth? For instance: 128 KB per second.</p>
","<p>You can throttle the network bandwidth on the interface using the command called <code>tc</code> Man page available at <a href=""http://linux.die.net/man/8/tc"">http://linux.die.net/man/8/tc</a></p>

<p>For a simple script, try <a href=""http://jwalanta.blogspot.com/2009/04/easy-bandwidth-shaping-in-linux.html"">wondershaper</a>.</p>
","28203"
"How To Install Virtualbox Guest Additions On CentOS via Command Line only","115328","","<p>Reading the Virtualbox user manual, I finally got [<a href=""http://www.virtualbox.org/manual/ch04.html#idp11259744"">here</a>], which explains how to install Virtualbox Guest Additions on a Linux guest via Command Line.</p>

<p>But it's not clear enough for me (I just started learning some commands). Can someone put down the exact commands you would use to install Virtualbox Guest Additions via CLI? (which includes finding where virtualbox guest additions has been mounted etc.)</p>
","<p>... finally this worked for me, should also work for anybody else trying to install VirtualBox Guest Additions on a CentOS (x86_64) virtual server in command line mode.</p>

<pre><code># yum update
# yum install dkms gcc make kernel-devel bzip2 binutils patch libgomp glibc-headers glibc-devel kernel-headers
# mkdir -p /media/cdrom
# mount /dev/scd0 /media/cdrom
# sh /media/cdrom/VBoxLinuxAdditions.run
</code></pre>

<p>When the process is complete, reboot the system. That's all.</p>
","18926"
"Syntax error near unexpected token `('","115056","","<p>When I use below code in SSH terminal for CentOS it works fine:</p>

<pre><code>paste &lt;(printf ""%s\n"" ""TOP"")
</code></pre>

<p>But if I place the same line code in a shell script (test.sh) and run shell script from terminal, it throws error as this</p>

<pre><code>./test.sh: line 30: syntax error near unexpected token ('   
./test.sh: line 30:     paste &lt;(printf ""%s\n"" ""TOP"")
</code></pre>

<p>How can I fix this problem?</p>
","<p><a href=""http://mywiki.wooledge.org/ProcessSubstitution"" rel=""noreferrer"">Process substitution</a> is not specified by POSIX, so not all POSIX shell support it, only some shells like <code>bash</code>, <code>zsh</code>, <code>ksh88</code>, <code>ksh93</code> support.</p>

<p>In <code>Centos</code> system, <code>/bin/sh</code> is symlink to <code>/bin/bash</code>. When <code>bash</code> is invoked with name <code>sh</code>, <code>bash</code> enters posix mode (<a href=""https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html"" rel=""noreferrer"">Bash Startup Files - Invoked with name sh</a>). In posix mode, <code>process substitution</code> is not supported, cause syntax error.</p>

<p>Script should work, if you call <code>bash</code> directly <code>bash test.sh</code>. If not, maybe <code>bash</code> has entered posix mode. This can be occur if you start <code>bash</code> with <code>--posix</code> argument or variable <code>POSIXLY_CORRECT</code> is set when <code>bash</code> start:</p>

<pre><code>$ bash --posix test.sh 
test.sh: line 54: syntax error near unexpected token `('
test.sh: line 54: `paste &lt;(printf ""%s\n"" ""TOP"")'

$ POSIXLY_CORRECT=1 bash test.sh 
test.sh: line 54: syntax error near unexpected token `('
test.sh: line 54: `paste &lt;(printf ""%s\n"" ""TOP"")
</code></pre>

<p>Or <code>bash</code> is built with <code>--enable-strict-posix-default</code> option.</p>

<p>Here, you don't need process substitution, you can use standard shell pipes:</p>

<pre><code>printf ""%s\n"" ""TOP"" | paste -
</code></pre>

<p><code>-</code> is the standard way to tell <code>paste</code> to read the data from stdin. With some <code>paste</code> implementations, you can omit it though that's not standard.</p>

<p>Where it would be useful is when pasting the output of more than one command like in:</p>

<pre><code>paste &lt;(cmd1) &lt;(cmd2)
</code></pre>

<p>On systems that support <code>/dev/fd/n</code>, that can be done in <code>sh</code> with:</p>

<pre><code>{ cmd1 4&lt;&amp;- | { cmd2 3&lt;&amp;- | paste /dev/fd/3 -; } 3&lt;&amp;0 &lt;&amp;4 4&lt;&amp;-; } 4&lt;&amp;0
</code></pre>

<p>(it's what <code>&lt;(...)</code> does internally).</p>
","151925"
"Merging folders with mv?","114738","","<p>If I use <code>mv</code> to move a folder called ""folder"" to a directory that already contains ""folder"" will they merge or will it be replaced?</p>
","<p><code>mv</code> cannot merge or overwrite directories, it will fail with the message <em>""mv: cannot move 'a' to 'b': Directory not empty""</em>, even when you're using the <code>--force</code> option.</p>

<hr>

<p>You can work around this using other tools (like <code>rsync</code>, <code>find</code>, or even <code>cp</code>), but you need to carefully consider the implications:</p>

<ul>
<li><code>rsync</code> can merge the contents of one directory into another (ideally with the <code>--remove-source-files</code><sup>1</sup> option to safely delete only those source files that were transferred successfully, and with the usual permission/ownership/time preservation option <code>-a</code> if you wish)<br>
… <strong>but</strong> this is a full copy operation, and can therefore be very disk-intensive.</li>
<li><a href=""https://unix.stackexchange.com/a/155633/28235"">You can use <code>find</code></a> to sequentially recreate the source directory structure at the target, then individually move the actual files<br>
… <strong>but</strong> this has to recurse through the source multiple times and can encounter race conditions (new directories being created at the source during the multi-step process)</li>
<li><a href=""https://unix.stackexchange.com/a/172402/28235""><code>cp</code> can create hard links</a> (simply put, additional pointers to the same existing file), which creates a result very similar to a merging <code>mv</code> (and is very IO-efficient since only pointers are created and no actual data has to be copied)<br>
… <strong>but</strong> this again suffers from a possible race condition (new files at the source being deleted even though they weren't copied in the previous step)</li>
</ul>

<p>Which of these workarounds (if any) is appropriate will very much depend on your specific use case.<br>
As always, think before you execute any of these commands, and have backups.</p>

<hr>

<p><sub>1: Note that <code>rsync --remove-source-files</code> won't delete any directories, so you will have to do something like <code>find -type d -empty -delete</code> afterwards to get rid of the empty source directory tree.</sub></p>
","127713"
"how do I trim leading and trailing whitespace from each line of some output?","114713","","<p>I would like to remove all leading and trailing spaces and tabs from each line in an output.</p>

<p>Is there a simple tool like <code>trim</code> I could pipe my output into?</p>

<p>example file:</p>

<pre><code>test space at back 
 test space at front
TAB at end  
    TAB at front
some empty lines with differing TABS and spaces:





 test space at both ends 
</code></pre>
","<pre><code>awk '{$1=$1;print}'
</code></pre>

<p>or shorter:</p>

<pre><code>awk '{$1=$1};1'
</code></pre>

<p>Would trim leading and trailing space or tab characters<sup>1</sup> <em>and also</em> squeeze sequences of tabs and spaces into a single space.</p>

<p>That works because when you assign something to one of the <em>fields</em>, <code>awk</code> rebuilds the whole record (as printed by <code>print</code>) by joining all fields (<code>$1</code>, ..., <code>$NF</code>) with <code>OFS</code> (space by default).</p>

<p><sub><sup>1</sup>(and possibly other blank characters depending on the locale and the <code>awk</code> implementation)</sub></p>
","205854"
"Bring down and delete bridge interface that's up","114587","","<p>How do I remove a bridge that has an IP address that was brought up manually and isn't in /etc/network/interfaces?</p>

<pre><code>$ ifconfig br100                                                
    br100     Link encap:Ethernet  HWaddr 00:00:00:00:00:00                         
              inet addr:172.16.0.5  Bcast:172.16.0.255  Mask:255.255.255.0
</code></pre>

<p>Can't delete it:</p>

<pre><code># brctl delbr br100
bridge br100 is still up; can't delete it   
</code></pre>

<p>Can't bring it down with ifdown:</p>

<pre><code># ifdown br100                                                  
ifdown: interface br100 not configured     
</code></pre>
","<p>Figured it out:</p>

<pre><code># ip link set br100 down
# brctl delbr br100
</code></pre>
","31765"
"grep lines starting with ""1"" in Ubuntu","114370","","<p>I try to search for lines that start with ""1"" using </p>

<pre><code>ls -1 | grep ^1*
</code></pre>

<p>but it returns lines that do not start with 1. What I am missing here? </p>
","<p>Your regular expression doesn't mean what you think it does. It matches all lines starting (^) with one (1) repeated zero or more (*) times. All strings match that regular expression. <code>grep '^1'</code> does what you want.</p>
","59895"
"Generate random numbers in specific range","114185","","<p>After googling a bit I couldn't find a simple way to use a shell command to generate a random decimal integer number included in a specific range, that is between a minimum and a maximum.</p>

<p>I read about <code>/dev/random</code>, <code>/dev/urandom</code> and <code>$RANDOM</code>, but none of these can do what I need.</p>

<p>Is there another useful command, or a way to use the previous data?</p>
","<p>In the POSIX toolchest, you can use <code>awk</code>:</p>

<pre><code>awk -v min=5 -v max=10 'BEGIN{srand(); print int(min+rand()*(max-min+1))}'
</code></pre>

<p>Do <em>not</em> use that as a source to generate passwords or secret data for instance, as with most <code>awk</code> implementations, the number can easily be guessed based on the time that command was run.</p>

<p>With many <code>awk</code> implementations, that command run twice within the same second will generally give you the same output.</p>
","140756"
"Splitting string by the first occurrence of a delimiter","113874","","<p>I have a string in the next format</p>

<pre><code>id;some text here with possible ; inside
</code></pre>

<p>and want to split it to 2 strings by first occurrence of the <code>;</code>. So, it should be: <code>id</code> and <code>some text here with possible ; inside</code></p>

<p>I know how to split the string (for instance, with <code>cut -d ';' -f1</code>), but it will split to more parts since I have <code>;</code> inside the left part.</p>
","<p><code>cut</code> sounds like a suitable tool for this:</p>

<pre><code>bash-4.2$ s='id;some text here with possible ; inside'

bash-4.2$ id=""$( cut -d ';' -f 1 &lt;&lt;&lt; ""$s"" )""; echo ""$id""
id

bash-4.2$ string=""$( cut -d ';' -f 2- &lt;&lt;&lt; ""$s"" )""; echo ""$string""
some text here with possible ; inside
</code></pre>

<p>But <code>read</code> is even more suitable:</p>

<pre><code>bash-4.2$ IFS=';' read -r id string &lt;&lt;&lt; ""$s""

bash-4.2$ echo ""$id""
id

bash-4.2$ echo ""$string""
some text here with possible ; inside
</code></pre>
","53315"
"How to loop over the lines of a file?","113866","","<p>Say I have this file:</p>

<pre><code>hello
world
hello world
</code></pre>

<p>This program</p>

<pre><code>#!/bin/bash

for i in $(cat $1); do
    echo ""tester: $i""
done
</code></pre>

<p>outputs</p>

<pre><code>tester: hello
tester: world
tester: hello
tester: world
</code></pre>

<p>I'd like to have the <code>for</code> iterate over each line individually ignoring whitespaces though, i.e. the last two lines should be replaced by</p>

<pre><code>tester: hello world
</code></pre>

<p>Using quotes <code>for i in ""$(cat $1)"";</code> results in <code>i</code> being assigned the whole file at once. What should I change?</p>
","<p>With <code>for</code> and <a href=""https://unix.stackexchange.com/questions/16192/what-is-ifs-in-context-of-for-looping"">IFS</a>:</p>

<pre><code>#!/bin/bash

IFS=$'\n'       # make newlines the only separator
set -f          # disable globbing
for i in $(cat &lt; ""$1""); do
  echo ""tester: $i""
done
</code></pre>

<p>Note however that it will skip empty lines as <em>newline</em> being an IFS-white-space character, sequences of it count as 1 and the leading and trailing ones are ignored. With <code>zsh</code> and <code>ksh93</code> (not <code>bash</code>), you can change it to <code>IFS=$'\n\n'</code> for newline not to be treated specially, however note that all <em>trailing</em> newline characters (so that includes trailing empty lines) will always be removed by the command substitution.</p>

<p>Or <a href=""https://unix.stackexchange.com/questions/18886/why-is-while-ifs-read-used-so-often-instead-of-ifs-while-read"">with <code>read</code></a> (no more <code>cat</code>):</p>

<pre><code>#!/bin/bash

while IFS= read -r line; do
  echo ""tester: $line""
done &lt; ""$1""
</code></pre>

<p>There, empty lines are preserved, but note that it would skip the last line if it was not properly delimited by a newline character.</p>
","7012"
"SED: insert text after the last line?","113770","","<p>The <a href=""https://superuser.com/questions/36575/unix-add-a-tag-to-the-beginning-of-each-file/36588#36588"">command</a> inserts a tag to the beginning of a file:</p>

<pre><code>sed -i ""1s/^/&lt;?php /"" file
</code></pre>

<p>How can I insert something to the end of each file with SED?</p>
","<p><code>-e ""\$aTEXTTOEND""</code> is the simplest way</p>
","26639"
"How to fix curl sslv3 alert handshake failure?","113726","","<p>I'm trying to curl HTTPS website in the following way:</p>

<pre><code>$ curl -v https://thepiratebay.se/
</code></pre>

<p>However it fails with the error:</p>

<pre><code>* About to connect() to thepiratebay.se port 443 (#0)
*   Trying 173.245.61.146...
* connected
* Connected to thepiratebay.se (173.245.61.146) port 443 (#0)
* SSLv3, TLS handshake, Client hello (1):
* SSLv3, TLS alert, Server hello (2):
* error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure
* Closing connection #0
curl: (35) error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure
</code></pre>

<p>Using <code>-k</code>/<code>--insecure</code> or adding <code>insecure</code> to my <code>~/.curlrc</code> doesn't make any difference.</p>

<p><strong>How do I ignore or force the certificate using <code>curl</code> command line?</strong></p>

<hr>

<p>When using <code>wget</code> seems to work fine. Also works when testing with <code>openssl</code> as below:</p>

<pre><code>$ openssl s_client -connect thepiratebay.se:443
CONNECTED(00000003)
SSL handshake has read 2651 bytes and written 456 bytes
New, TLSv1/SSLv3, Cipher is AES128-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1
    Cipher    : AES128-SHA
</code></pre>

<hr>

<p>I've:</p>

<pre><code>$ curl --version
curl 7.28.1 (x86_64-apple-darwin10.8.0) libcurl/7.28.1 OpenSSL/0.9.8| zlib/1.2.5 libidn/1.17
Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp smtp smtps telnet tftp 
Features: IDN IPv6 Largefile NTLM NTLM_WB SSL libz 
</code></pre>
","<p>Some sites disable support for SSL 3.0 (possible because of many exploits/vulnerabilities), so it's possible to force specific SSL version by either <code>-2</code>/<code>--sslv2</code> or <code>-3</code>/<code>--sslv3</code>.
Also <code>-L</code> is worth a try if requested page has moved to a different location.</p>

<p>In my case it was a <code>curl</code> bug (<a href=""https://bugs.launchpad.net/ubuntu/+source/openssl/+bug/861137"" rel=""nofollow noreferrer"">found in OpenSSL</a>), so <code>curl</code> needed to be upgraded to the latest version (>7.40) and it worked fine.</p>

<p>See also:</p>

<ul>
<li><a href=""http://blog.techstacks.com/2010/03/3-common-causes-of-unknown-ssl-protocol-errors-with-curl.html"" rel=""nofollow noreferrer"">3 Common Causes of Unknown SSL Protocol Errors with cURL</a></li>
<li><a href=""https://stackoverflow.com/questions/11679961/error-when-installing-meteor-in-ubuntu"">Error when Installing Meteor</a> at SO</li>
<li><a href=""https://lists.ubuntu.com/archives/foundations-bugs/2013-July/155861.html"" rel=""nofollow noreferrer"">[Bug 861137] Re: Openssl TLS errors while connecting to SSLv3 sites</a></li>
</ul>
","192953"
"Getting 256 colors to work in tmux","113589","","<p>I have 256 colors working just fine in <code>konsole,</code>. I thought I'd give <code>tmux</code> a try because, unlike screen, it seems to support <code>vi</code> mode.  However I find that the colors of my prompt show up and this is most likely because I have a 256 color mode prompt. What do I need to do to get <code>tmux</code> to recognize all 256 colors?</p>
","<p>The <a href=""https://github.com/tmux/tmux/wiki/FAQ"" rel=""noreferrer"">Tmux FAQ</a> explicitly advises against setting TERM to anything other than screen or screen-256color in your shell init file, so don't do it!</p>

<p>Here's what I use:</p>

<pre><code>~$ which tmux
tmux: aliased to TERM=xterm-256color tmux
</code></pre>

<p>and in in my .tmux.conf:</p>

<pre><code>set -g default-terminal ""screen-256color""
</code></pre>

<p>Aliasing <code>tmux</code> to ""<code>tmux -2</code>"" should also do the trick.</p>
","1098"
"How to list all running daemons?","113387","","<p>From my <a href=""https://unix.stackexchange.com/questions/175008/can-process-id-and-session-id-of-a-daemon-differ"">question</a>, it was clear that I cannot easily decide the features of a daemon.I have read in different articles and from different forums that <code>service --status-all</code> command can be used to list all the daemons in my system. But I do not think that the command is listing all daemons because <code>NetworkManager</code>,a daemon which is currently running in my <code>Ubuntu 14.04</code> system, is not listed by the command. Is there some command to list the running daemons or else is there some way to find the daemons from the <code>filesystem</code> itself?</p>
","<p>The notion of <em>daemon</em> is attached to <em>processes</em>, not <em>files</em>. For this reason, there is no sense in ""finding daemons on the filesystem"". Just to make the notion a little clearer : a program is an executable file (visible in the output of <code>ls</code>) ; a process is an instance of that program (visible in the output of <code>ps</code>).</p>

<p>Now, if we use the information that I gave in <a href=""https://unix.stackexchange.com/questions/175008/can-process-id-and-session-id-of-a-daemon-differ"">my answer</a>, we could find running daemons by <em>searching for processes which run without a controlling terminal  attached to them</em>. This can be done quite easily with <code>ps</code>:</p>

<pre><code>$ ps -eo 'tty,pid,comm' | grep ^?
</code></pre>

<p><em>The <code>tty</code> output field contains ""?"" when the process has no controlling terminal.</em></p>

<p>The big problem here comes when your system runs a graphical environment. Since GUI programs (i.e. Chromium) are not attached to a terminal, they also appear in the output. On a standard system, where root does not run graphical programs, you could simply restrict the previous list to root's processes. This can be achieved using <code>ps</code>' <code>-U</code> switch.</p>

<pre><code>$ ps -U0 -o 'tty,pid,comm' | grep ^?
</code></pre>

<p>Yet, two problems arise here:</p>

<ul>
<li>If root is running graphical programs, they will show up.</li>
<li>Daemons running without root privileges won't. Note that daemons which start at boot time are usually running as root.</li>
</ul>

<p>Basically, we would like to <em>display all programs without a controlling terminal, but not GUI programs</em>. Luckily for us, there is a program to list GUI processes : <code>xlsclients</code>! <a href=""https://unix.stackexchange.com/a/147086/41892"">This answer from slm</a> tells us how to use it to list all GUI programs, but we'll have to reverse it, since we want to exclude them. This can be done using the <code>--deselect</code> switch.</p>

<p>First, we'll build a list of all GUI programs for which we have running processes. From the answer I just linked, this is done using...</p>

<pre><code>$ xlsclients | cut -d' ' -f3 | paste - -s -d ','
</code></pre>

<p>Now, <code>ps</code> has a <code>-C</code> switch which allows us to select by command name. We just got our command list, so let's inject it into the <code>ps</code> command line. Note that I'm using <code>--deselect</code> afterwards to reverse my selection.</p>

<pre><code>$ ps -C ""$(xlsclients | cut -d' ' -f3 | paste - -s -d ',')"" --deselect
</code></pre>

<p>Now, we have a list of all non-GUI processes. Let's not forget our ""no TTY attached"" rule. For this, I'll add <code>-o tty,args</code> to the previous line in order to output the <code>tty</code> of each process (and its full command line) :</p>

<pre><code>$ ps -C ""$(xlsclients | cut -d' ' -f3 | paste - -s -d ',')"" --deselect -o tty,args | grep ^?
</code></pre>

<p>The final <code>grep</code> captures all lines which begin with ""?"", that is, all processes without a controlling tty. And there you go! This final line gives you all non-GUI processes running without a controlling terminal. Note that you could still improve it, for instance, by <a href=""https://unix.stackexchange.com/a/78585/41892"">excluding kernel threads</a> (which aren't processes)...</p>

<pre><code>$ ps -C ""$(xlsclients | cut -d' ' -f3 | paste - -s -d ',')"" --ppid 2 --pid 2 --deselect -o tty,args | grep ^?
</code></pre>

<p>... or by adding a few columns of information for you to read:</p>

<pre><code>$ ps -C ""$(xlsclients | cut -d' ' -f3 | paste - -s -d ',')"" --ppid 2 --pid 2 --deselect -o tty,uid,pid,ppid,args | grep ^?
</code></pre>
","175396"
"How can I edit multiple files in VIM?","113020","","<p>I know I can open multiple files with <code>vim</code> by doing something like <code>vim 2011-12*.log</code>, but how can I switch between files and close the files one at a time? Also, how can I tell the file name of the current file that I'm editing?</p>
","<p>First of all, in vim you can enter <kbd>:</kbd> (colon) and then help <code>help</code>, ala <code>:help</code> for a list of self help topics, including a short tutorial. Within the list of topics move your cursor over the topic of interest and then press <kbd>ctrl</kbd><kbd>]</kbd> and that topic will be opened.</p>

<p>A good place for you to start would be the topic</p>

<pre><code>|usr_07.txt|  Editing more than one file
</code></pre>

<p>Ok, on to your answer.</p>

<p>After starting vim with a list of files, you can move to the next file by entering <code>:next</code> or <code>:n</code> for short.
 <code>:wnext</code> is short for write current changes and then move to next file.</p>

<p>There's also an analogous <code>:previous</code>, <code>:wprevious</code> and <code>:Next</code>. (Note that <code>:p</code> is shorthand for <code>:print</code>. The shorthand for <code>:previous</code> is <code>:prev</code> or <code>:N</code>.)</p>

<p>To see where you are in the file list enter <code>:args</code> and the file currently being edited will appear in <code>[]</code> (brackets).  </p>

<p>Example:  </p>

<pre><code>vim foo.txt bar.txt
:args
</code></pre>

<p>result:  </p>

<pre><code>[foo.txt] bar.txt
</code></pre>
","27590"
"What are the differences between most, more and less?","112717","","<p>I'm now using Arch Linux, and find a command <code>most</code> works like <code>more</code> and <code>less</code>. To understand the differences between them is a confusing problem. The question <a href=""https://unix.stackexchange.com/questions/604/isnt-less-just-more"">Isn&#39;t less just more?</a> mentions the differences between <code>less</code> and <code>more</code>. Do you know the differences on color performance, shortcuts and ability moving forward and backward?</p>
","<p><strong>more</strong></p>

<p><code>more</code> is an old utility. When the text passed to it is too large to fit on one screen, it pages it. You can scroll down but not up.</p>

<p>Some systems hardlink <code>more</code> to <code>less</code>, providing users with a strange hybrid of the two programs that looks like <code>more</code> and quits at the end of the file like <code>more</code> but has some <code>less</code> features such as backwards scrolling. This is a result of <code>less</code>'s <code>more</code> compatibility mode. You can enable this compatibility mode temporarily with <code>LESS_IS_MORE=1 less ...</code>.</p>

<p><code>more</code> passes raw escape sequences by default. Escape sequences tell your terminal which colors to display.</p>

<p><strong>less</strong></p>

<p><code>less</code> was written by a man who was fed up with <code>more</code>'s inability to scroll backwards through a file. He turned <code>less</code> into an open source project and over time, various individuals added new features to it. <code>less</code> is massive now. That's why some small embedded systems have <code>more</code> but not <code>less</code>. For comparison, <code>less</code>'s source is over 27000 lines long. <code>more</code> implementations are generally only a little over 2000 lines long.</p>

<p>In order to get <code>less</code> to pass raw escape sequences, you have to pass it the <code>-r</code> flag. You can also tell it to only pass ANSI escape characters by passing it the <code>-R</code> flag.</p>

<p><strong>most</strong></p>

<p><code>most</code> is supposed to be more than <code>less</code>. It can display multiple files at a time. By default, it truncates long lines instead of wrapping them and provides a left/right scrolling mechanism. <a href=""http://www.jedsoft.org/most"">most's website</a> has no information about <code>most</code>'s features. Its manpage indicates that it is missing at least a few <code>less</code> features such as log-file writing (you can use <code>tee</code> for this though) and external command running.</p>

<p>By default, <code>most</code> uses strange non-vi-like keybindings. <code>man most | grep '\&lt;vi.?\&gt;'</code> doesn't return anything so it may be impossible to put <code>most</code> into a vi-like mode.</p>

<p><code>most</code> has the ability to decompress gunzip-compressed files before reading. Its status bar has more information than <code>less</code>'s.</p>

<p><code>most</code> passes raw escape sequences by default.</p>
","81131"
"Correctly determining memory usage in Linux","112695","","<p>I'm a bit confused on some of the results I am seeing from <strong>ps</strong> and <strong>free</strong>.</p>

<p>On my server, this is the result of <code>free -m</code></p>

<pre><code>[root@server ~]# free -m
             total       used       free     shared    buffers     cached
Mem:          2048       2033         14          0         73       1398
-/+ buffers/cache:        561       1486
Swap:         2047         11       2036
</code></pre>

<p>My understanding of how Linux manages memory, is that it will store disk usage in RAM, so that each subsequent access is quicker. I believe this is indicated by the ""cached"" columns. Additionally, various buffers are stored in RAM, indicated in the ""buffers"" column.</p>

<p>So if I understand correctly, the ""actual"" usage is supposed to be the ""used"" value of ""-/+ buffers/cache"", or 561 in this case.</p>

<p>So assuming all of that is correct, the part that throws me is the results of <code>ps aux</code>.</p>

<p>My understanding of the <code>ps</code> results, is that the 6th column (RSS), represents the size in kilobytes the process uses for memory.</p>

<p>So when I run this command:</p>

<pre><code>[root@server ~]# ps aux | awk '{sum+=$6} END {print sum / 1024}'
1475.52
</code></pre>

<p>Shouldn't the result be the ""used"" column of ""-/+ buffers/cache"" from <code>free -m</code>?</p>

<p>So, how can I properly determine the memory usage of a process in Linux? Apparently my logic is flawed.</p>
","<p>This <strong>exact</strong> same question was asked on <a href=""https://serverfault.com/questions/372430/ubuntu-linux-process-swap-memory-and-memory-usage/372594"">serverfault</a> just the other day :-)</p>

<p>The linux virtual memory system isn't quite so simple. You can't just add up all the RSS fields and get the value reported <code>used</code> by <code>free</code>. There are many reasons for this, but I'll hit a couple of the biggest ones.</p>

<ul>
<li><p>When a process forks, both the parent and the child will show with the same RSS. However linux employs <code>copy-on-write</code> so that both processes are really using the same memory. Only when one of the processes modifies the memory will it actually be duplicated. So this will cause the <code>free</code> number to be smaller than the <code>top</code> RSS sum.</p></li>
<li><p>The RSS value doesn't include shared memory. Because shared memory isn't owned by any one process, <code>top</code> doesn't include it in RSS. So this will cause the <code>free</code> number to be larger than the <code>top</code> RSS sum.</p></li>
</ul>
","34867"
"How do I repeat the last command without using the arrow keys?","112669","","<p>I know I can use <kbd>Up</kbd> to iterate through previous commands. Running the <em>last</em> command simply involves <kbd>Up</kbd> + <kbd>Enter</kbd>. However, I was thinking of buying the <a href=""http://en.wikipedia.org/wiki/Happy_Hacking_Keyboard"">Happy Hacking Keyboard</a> as I spend a lot of time in <code>vim</code>.</p>

<p>This keyboard has no arrow keys, and the only way I know how to get this kind of behaviour is by pressing <kbd>Ctrl</kbd> + <kbd>R</kbd> and beginning to repeat my previous command.</p>

<p>Is there an easy way to emulate <kbd>Up</kbd> + <kbd>Enter</kbd> in an UNIX terminal without the arrow keys?</p>
","<p>With <code>csh</code> or any shell implementing <code>csh</code>-like history substitution (<code>tcsh</code>, <code>bash</code>, <code>zsh</code>):</p>

<pre><code>!!
</code></pre>

<p>Then <kbd>Enter</kbd>.</p>

<hr>

<p>Or <a href=""http://www.thegeekstuff.com/2008/08/15-examples-to-master-linux-command-line-history/"">alternatively</a>:</p>

<pre><code>!-1
</code></pre>

<p>Then <kbd>Enter</kbd>.</p>

<hr>

<p>Or <kbd>Ctrl</kbd>+<kbd>P</kbd>, <kbd>Enter</kbd></p>

<hr>

<h2>Magic space</h2>

<p>Also, note that <code>!!</code> and <code>!-1</code> will not auto-expand for you, until you execute them (when it might be too late).</p>

<p>If using <code>bash</code>, you can put <code>bind Space:magic-space</code> into <code>~/.bashrc</code>, then pressing <kbd>Space</kbd> after the command will auto-expand them inline, allowing you to inspect them before execution. This is particularly useful for history expansion from a command run a while ago, e.g. <code>!echo</code> will pull the last command run starting with <code>echo</code>. With magic space, you get to preview the command before it's run.</p>

<p>That's the equivalent of doing <code>bindkey ' ' magic-space</code> in <code>tcsh</code> or <code>zsh</code>.</p>
","147572"
"How do I switch from an unknown shell to bash?","112599","","<p>I was surprised that I didn't find this question already on the site. So, today <code>$</code> came up after I logged in as a new user. This was unexpected because my main user's prompt starts with <code>username@computername:~$</code>.</p>

<p>So, how do I switch from this other shell to bash?</p>
","<p>Assuming the unknown shell supports running an absolute command, you could try: <code>/bin/bash</code></p>

<p>To change the default shell, I would use <code>chsh(1)</code>. Sample usage: <code>chsh -s /bin/bash your_user</code></p>
","1375"
"read only root filesystem","112461","","<p>Somehow my Debian went to read only in root file system. I have no idea how this could have happened.<br>
For example when I am in <code>/root</code> folder and type command <code>nano</code> and after that press <kbd>Tab</kbd> to list possible file in that folder I get the message:</p>

<pre><code>root@debian:~# nano -bash: cannot create temp file for here-document: Read-only file system
</code></pre>

<p>The same for the <code>cd</code> command when I type <code>cd /home</code> and press <kbd>Tab</kbd> to list paths I have this:</p>

<pre><code>root@debian:~# cd /home -bash: cannot create temp file for here-document: Read-only file system
</code></pre>

<p>I also have problems with software like <code>apt</code> and others. Can't even apt-get update. I have a lot of errors like this:</p>

<pre><code>Err http ://ftp.de.debian.org wheezy-updates/main Sources
406  Not Acceptable
W: Not using locking for read only lock file /var/lib/apt/lists/lock
W: Failed to fetch http ://ftp.de.debian.org/debian/dists/wheezy/Release  rename failed, Read-only file system (/var/lib/apt/lists/ftp.de.debian.org_debian_dists_wheezy_Release -&gt; /var/lib/apt/lists/ftp.de.debian.org_debian_dists_wheezy_Release).
W: Failed to fetch http ://security.debian.org/dists/wheezy/updates/main/source/Sources  404  Not Found
W: Failed to fetch http ://security.debian.org/dists/wheezy/updates/main/binary-amd64/Packages  404  Not Found
W: Failed to fetch http ://ftp.de.debian.org/debian/dists/wheezy-updates/main/source/Sources  406  Not Acceptable
E: Some index files failed to download. They have been ignored, or old ones used instead.
W: Not using locking for read only lock file /var/lib/dpkg/lock
</code></pre>

<p>I have a lot of problems in the system.
Is it possible to fix that? How can I check what happened? What should I look for in the logs?</p>

<p>I know it could be because of the line in <code>/etc/fstab</code> file:</p>

<pre><code>/dev/mapper/debian-root /               ext4    errors=remount-ro 0       1
</code></pre>

<p>but what is the problem? I can't find nothing or maybe I don't know where to look.</p>

<p>Edit:</p>

<p>I did search messages logs and found only this:</p>

<pre><code>kernel: [    5.709326] EXT4-fs (dm-0): re-mounted. Opts: (null)
kernel: [    5.977131] EXT4-fs (dm-0): re-mounted. Opts: errors=remount-ro
kernel: [    7.174856] EXT4-fs (dm-2): mounted filesystem with ordered data mode. Opts: (null)
</code></pre>

<p>I guess it's correct, because I have the same entries on other debian machines.</p>

<p>I found something in dmesg (I cut that output a bit because was a lot standard ext4 things)</p>

<pre><code>root@gs3-svn:/# dmesg |grep ext4
EXT4-fs error (device dm-0) in ext4_reserve_inode_write:4507: Journal has aborted
EXT4-fs error (device dm-0) in ext4_reserve_inode_write:4507: Journal has aborted
EXT4-fs error (device dm-0) in ext4_dirty_inode:4634: Journal has aborted
EXT4-fs error (device dm-0): ext4_discard_preallocations:3894: comm rsyslogd: Error loading buddy information for 1
EXT4-fs warning (device dm-0): ext4_end_bio:250: I/O error -5 writing to inode 133130 (offset 132726784 size 8192 starting block 159380)
EXT4-fs error (device dm-0): ext4_journal_start_sb:327: Detected aborted journal
</code></pre>

<p>5 errors and 1 warning. Any ideas? Is it safe to use mount -o remount,rw / ?</p>
","<p>The default behaviour for most Linux file systems is to safeguard your data. When the kernel detects an error in the storage subsystem it will make the filesystem read-only to prevent (further) data corruption. </p>

<p>You can tune this somewhat with the mount option <code>errors={continue|remount-ro|panic}</code> which are documented in the system manual (<code>man mount</code>).</p>

<p>When your root file-system encounters such an error, most of the time the error won't be recorded in your log-files, as they will now be read-only too. Fortunately since it is a kernel action the original error message is recorded in memory first, in the kernel ring buffer. Unless already flushed from memory you can display the contents of the ring buffer with the <code>dmesg</code> command. . </p>

<p>Most real hard disks support <a href=""http://en.wikipedia.org/wiki/S.M.A.R.T."">SMART</a> and you can use <code>smartctl</code> to try and diagnose the disk health. </p>

<p>Depending on the error messages, you could decide it is still safe to use file-system and return it read-write condition with <code>mount -o remount,rw /</code></p>

<p>In general though, disk errors are a precursor to complete disk failure. Now is the time to create a back-up of your data or to confirm the status of your existing back-ups. </p>
","145919"
"#!/bin/bash - no such file or directory","111599","","<p>I've created a bash script but when I try to execute it, I get </p>

<pre><code>#!/bin/bash no such file or directory
</code></pre>

<p>I need to run the command: <code>bash script.sh</code> for it to work.</p>

<p>How can I fix this?</p>
","<p>Usually, this kind of message is due to an extra carriage return at the end of the first line.
Run </p>

<pre><code>$ head -1 yourscript | od -c
</code></pre>

<p>and see how it ends.  </p>

<p>This is wrong:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \r  \n
</code></pre>

<p>This is correct:</p>

<pre><code>0000000   #   !   /   b   i   n   /   b   a   s   h  \n
</code></pre>

<p>Use <code>dos2unix</code> (or <code>sed</code>, <code>tr</code>, <code>awk</code>, <code>perl</code>, <code>python</code>…) to fix your script if this is the issue.</p>
","27067"
"How to find the path of a file based on its name?","111527","","<p>I am trying to find <code>settings.xml</code> file in my Ubuntu machine. I have no clue where it is, and which directory it is in.</p>

<p>I tried using this - </p>

<pre><code>ls -R | grep settings.xml
</code></pre>

<p>But it doesn't show me the full path where it is.. Is there any other command which I need to try that can give me the full path?</p>
","<p>For fast search (but not definitive):</p>

<pre><code>locate -br '^settings.xml$'
</code></pre>

<p>From <code>man locate</code>:</p>

<pre><code>   locate  reads  one or more databases prepared by updatedb(8) and writes
   file names matching at least one of the PATTERNs  to  standard  output,
   one per line.

   -b, --basename
          Match  only  the base name against the specified patterns.  This
          is the opposite of --wholename.
   -r, --regexp REGEXP
          Search for a basic regexp REGEXP.  No PATTERNs  are  allowed  if
          this  option  is used, but this option can be specified multiple
          times.
</code></pre>

<p>The <code>^</code> and <code>$</code> ensure that only files whose name is <code>settings.xml</code> and not files whose names <em>contain</em> <code>settings.xml</code> will be printed.</p>

<p>You may need for the first time to run: <code>updatedb</code> (as <code>root</code>) to update/build the database of <code>locate</code>.</p>
","114653"
"Using sftp to Transfer a Directory?","111413","","<p>When I try to use sftp to transfer a directory containing files, I get an error message:</p>

<pre><code>skipping non-regular file directory_name
</code></pre>

<p>The directory contains a couple of files and two subdirectories.</p>

<p>What am I doing wrong?</p>
","<p><code>sftp</code>, like <code>cp</code> and <code>scp</code>, requires that when you copy a folder (and its contents, obviously), you have to explicitly tell it you want to transfer the folder recursively with the <code>-r</code> option.</p>

<p>So, add <code>-r</code> to the command.</p>
","26938"
"Installing a .deb package on Arch - Is it possible?","111159","","<p>The problem is simple - I have a .deb package and I want to install it on my Arch Linux. Is this possible? If yes, how?</p>
","<p>Is it possible? Yes. Is it a good idea? That depends. You would only really need to do this if the application only exists as a <code>.deb</code> package. It is much more likely that you can just grab the upstream source and write a simple <a href=""https://wiki.archlinux.org/index.php/Pkgbuild"">PKGBUILD</a> to install it with pacman.</p>

<p>You should also search <a href=""https://wiki.archlinux.org/index.php/AUR"">the AUR</a> to ensure that someone hasn't done this already.</p>
","83544"
"How to list processes locking file?","110747","","<p>Using <code>flock</code>, several processes can have a shared lock at the same time, or be waiting to acquire a write lock. How do I get a list of these processes?</p>

<p>That is, for a given file X, ideally to find the process id of each process which either holds, or is waiting for, a lock on the file. It would be a very good start though just to get a count of the number of processes waiting for a lock.</p>
","<p><code>lslocks</code>, from the <a href=""https://www.kernel.org/pub/linux/utils/util-linux/"" rel=""nofollow noreferrer"">util-linux package</a>, does exactly this.</p>

<p>In the <code>MODE</code> column, processes waiting for a lock will be marked with a <code>*</code>.</p>
","85997"
"How can I find broken symlinks","110370","","<p>Is there a way to find all symbolic links that don't point anywere?  </p>

<p><code>find ./ -type l</code> </p>

<p>will give me all symbolic links, but makes no distinction between links that go somewhere and links that don't.  </p>

<p>I'm currently doing: </p>

<p><code>find ./ -type l -exec file {} \; |grep broken</code></p>

<p>But I'm wondering what alternate solutions exist. </p>
","<p>I'd strongly suggest <strong>not</strong> to use <code>find -L</code>  for the task (see below for explanation). Here are some other ways to do this:</p>

<ul>
<li><p>If you want to use a ""pure <code>find</code>"" method, it should rather look like this:</p>

<pre><code>find . -xtype l
</code></pre>

<p>(<strong><code>xtype</code></strong> is a test performed on a dereferenced link) This may not be available in all versions of <code>find</code>, though. But there are other options as well;</p></li>
<li><p>You can also exec <code>test -e</code> from within the <code>find</code> command:</p>

<pre><code>find . -type l ! -exec test -e {} \; -print
</code></pre></li>
<li><p>Even some <code>grep</code> trick could be better (i.e. <em>safer</em>) than <code>find -L</code>, but not exactly such as presented in the question (which greps in entire output lines, including filenames): </p>

<pre><code> find . -type l -exec sh -c ""file -b {} | grep -q ^broken"" \; -print
</code></pre></li>
</ul>

<p>The <code>find -L</code> trick quoted <a href=""https://unix.stackexchange.com/a/34253/9382"">by solo</a> from <a href=""http://www.commandlinefu.com/commands/view/8260/find-broken-symlinks"" rel=""noreferrer"">commandlinefu</a> looks nice and hacky, but it has one very <strong>dangerous pitfall</strong>: All the symlinks are followed. Consider directory with the contents presented below:</p>

<pre><code>$ ls -l
total 0
lrwxrwxrwx 1 michal users  6 May 15 08:12 link_1 -&gt; nonexistent1
lrwxrwxrwx 1 michal users  6 May 15 08:13 link_2 -&gt; nonexistent2
lrwxrwxrwx 1 michal users  6 May 15 08:13 link_3 -&gt; nonexistent3
lrwxrwxrwx 1 michal users  6 May 15 08:13 link_4 -&gt; nonexistent4
lrwxrwxrwx 1 michal users 11 May 15 08:20 link_out -&gt; /usr/share/
</code></pre>

<p>If you run <code>find -L . -type l</code> in that directory, all <code>/usr/share/</code> would be searched as well (and that can take really long)<sup>1</sup>. <strong>For a <code>find</code> command that is ""immune to outgoing links"", don't use <code>-L</code></strong>. </p>

<hr>

<p><sup>1</sup> This may look like a minor inconvenience (the command will ""just"" take long to traverse all <code>/usr/share</code>) - but can have more severe consequences. For instance, consider chroot environments: They can exist in some subdirectory of the main filesystem and contain symlinks to absolute locations. Those links could seem to be broken for the ""outside"" system, because they only point to proper places once you've entered the chroot. I also recall that some bootloader used symlinks under <code>/boot</code> that only made sense in an initial boot phase, when the boot partition was mounted as <code>/</code>.</p>

<p>So if you use a <code>find -L</code> command to find and then delete broken symlinks from some harmless-looking directory, you might even break your system...</p>
","38691"
"Using awk to sum the values of a column, based on the values of another column","110348","","<p>I am trying to sum certain numbers in a column using <code>awk</code>. I would like to sum just column 3 of the ""smiths"" to get a total of 212. I can sum the whole column using <code>awk</code> but not just the ""smiths"". I have:</p>

<pre><code>awk 'BEGIN {FS = ""|""} ; {sum+=$3} END {print sum}' filename.txt
</code></pre>

<p>Also I am using putty. Thank you for any help.</p>

<pre><code>smiths|Login|2
olivert|Login|10
denniss|Payroll|100
smiths|Time|200
smiths|Logout|10
</code></pre>
","<pre><code>awk -F '|' '$1 ~ /smiths/ {sum += $3} END {print sum}' inputfilename
</code></pre>

<ul>
<li>The <code>-F</code> flag sets the field separator; I put it in single quotes because it is a special shell character.  </li>
<li>Then <code>$1 ~ /smiths/</code> applies the following {code block} only to lines where the first field matches the regex <code>/smiths/</code>.  </li>
<li>The rest is the same as your code.</li>
</ul>

<p>Note that since you're not really using a regex here, just a specific value, you could just as easily use:</p>

<pre><code>awk -F '|' '$1 == ""smiths"" {sum += $3} END {print sum}' inputfilename
</code></pre>

<p>Which checks string equality.  This is equivalent to using the regex <code>/^smiths$/</code>, as mentioned in another answer, which includes the <code>^</code> anchor to only match the start of the string (the start of field 1) and the <code>$</code> anchor to only match the end of the string.  Not sure how familiar you are with regexes.  They are very powerful, but for this case you could use a string equality check just as easily.</p>
","242949"
"How to get full path of original file of a soft symbolic link?","110331","","<p>I want to <code>scp</code> a file to a server. The file is a symbolic link, and actually what I want to do is copying the source file. Anyway I don't want to track source file's path manually, because it can be replaced. If I can get source file's absolute path, I can <code>sep</code> with it. How can I get the path?</p>
","<p>Try this line:</p>

<pre><code>readlink -f `which command`
</code></pre>

<p>If <code>command</code> is in your <code>$PATH</code> variable , otherwise you need to specify the path you know.</p>
","22132"
"How to copy the partition layout of a whole disk using standard tools","109816","","<p>I want to take a backup of the whole partition <em>layout</em> of a hard drive, including logical drives, so that I can restore that layout to another disk. I do <strong>not</strong> want to copy the contents of the partitions, only the layout. For the primary and extended partitions, it's easy:</p>

<pre><code>dd if=/dev/sda of=partitiontable.bin bs=1 skip=446 count=64 # backup
dd if=partitiontable.bin of=/dev/sda bs=1 seek=446 count=64 # restore
</code></pre>

<p>But when it comes to the layout of the logical partitions, I wonder if there exists among the standard tools a similar way of saving the layout? I guess the main problem is finding the offsets to the locations of the EBRs, because with that, <code>dd</code> will do the rest. Keep in mind I need to be able to put everything back to a (possibly) blank disk and thereby restore the same layout. Using partitioning tools like <code>fdisk</code> or <code>parted</code> is fine, but I must be able to automate their use (scripting) and they should not depend on any X-related packages -- command line only.</p>

<p>My backup plan is doing it manually in a little python script using the struct module, but I rather hoped there was an easier way.</p>
","<p>You can use sfdisk for this task.</p>

<p>Save:</p>

<pre><code>sfdisk -d /dev/sda &gt; part_table
</code></pre>

<p>Restore:</p>

<pre><code>sfdisk /dev/sda &lt; part_table
</code></pre>

<p>For GPT partition tables, this requires <code>sfdisk</code> from util-linux 2.26 or later.  It <a href=""https://git.kernel.org/cgit/utils/util-linux/util-linux.git/commit/?id=1881390de25df8587b8fc281c451796f7d032dd3"">was re-written from scratch on top of libfdisk</a>.</p>

<p>This copies the UUIDs unchanged, rather than generating new ones.  So the new disk is a clone of the original, not just another disk with the same layout.  Note that Linux's <code>/dev/disk/by-uuid/</code> looks at filesystem UUIDs, though, not UUIDs in the partition table.  <code>sfdisk</code> will generate new UUIDs if you edit out the UUIDs from the dump (per-partition and the UUID for the partition table itself near the start of the file).</p>
","12988"
"What's the difference of dmesg output and /var/log/messages?","109733","","<p>AFAIK <code>dmesg</code> shows information about kernel and kernel modules, and <code>/var/log/messages</code> also shows information produced by kernel and modules. </p>

<p>So what's the difference? Does <code>/var/log/messages</code> <a href=""http://en.wikipedia.org/wiki/Subset"">⊂</a> output of <code>dmesg</code>?</p>

<p>More Info that may be helpful:<br>
- There is a <a href=""http://www.mjmwired.net/kernel/Documentation/trace/ring-buffer-design.txt"">kernel ring buffer</a>, which I think is the very and only place to store kernel log data.<br>
- Article ""<a href=""http://www.ibm.com/developerworks/linux/library/l-kernel-logging-apis/index.html"">Kernel logging: APIs and implementation</a>"" on IBM DeveloperWorks described APIs and the bird-view picture.</p>
","<p><code>dmesg</code> prints the contents of the ring buffer.  This information is also sent in real time to <code>syslogd</code> or <code>klogd</code>, when they are running, and ends up in <code>/var/log/messages</code>; when <code>dmesg</code> is most useful is in capturing boot-time messages from before <code>syslogd</code> and/or <code>klogd</code> started, so that they will be properly logged.</p>
","35853"
"Why is it better to use ""#!/usr/bin/env NAME"" instead of ""#!/path/to/NAME"" as my shebang?","109550","","<p>I notice that some scripts which I have acquired from others have the shebang <code>#!/path/to/NAME</code> while others (using the same tool, NAME) have the shebang <code>#!/usr/bin/env NAME</code>.</p>

<p>Both seem to work properly.  In tutorials (on Python, for example), there seems to be a suggestion that the latter shebang is better.  But, I don't quite understand why this is so.</p>

<p>I realize that, in order to use the latter shebang, NAME must be in the PATH whereas the first shebang does not have this restriction.</p>

<p>Also, it appears (to me) that the first would be the better shebang, since it specifies precisely where NAME is located.  So, in this case, if there are multiple versions of NAME (e.g., /usr/bin/NAME, /usr/local/bin/NAME), the first case specifies which to use.</p>

<p>My question is why is the first shebang preferred to the second one?</p>
","<p>It isn't necessarily better.</p>

<p>The advantage of <code>#!/usr/bin/env python</code> is that it will use whatever <code>python</code> executable appears first in the user's <code>$PATH</code>.</p>

<p>The <em>disadvantage</em> of <code>#!/usr/bin/env python</code> is that it will use whatever <code>python</code> executable appears first in the user's <code>$PATH</code>.</p>

<p>That means that the script could behave differently depending on who runs it.  For one user, it might use the <code>/usr/bin/python</code> that was installed with the OS.  For another, it might use an experimental <code>/home/phred/bin/python</code> that doesn't quite work correctly.</p>

<p>And if <code>python</code> is only installed in <code>/usr/local/bin</code>, a user who doesn't have <code>/usr/local/bin</code> in <code>$PATH</code> won't even be able to run the script.  (That's probably not too likely on modern systems, but it could easily happen for a more obscure interpreter.)</p>

<p>By specifying <code>#!/usr/bin/python</code> you specify exactly which interpreter will be used to run the script <em>on a particular system</em>.</p>

<p>Another potential problem is that the <code>#!/usr/bin/env</code> trick <a href=""http://www.in-ulm.de/~mascheck/various/shebang/#splitting"" rel=""noreferrer"">doesn't let you pass arguments to the intrepreter</a> (other than the name of the script, which is passed implicitly).  This <em>usually</em> isn't an issue, but it can be.  Many Perl scripts are written with <code>#!/usr/bin/perl -w</code>, but <code>use warnings;</code> is the recommended replacement these days.  Csh scripts should use <code>#!/bin/csh -f</code> -- but csh scripts are <a href=""http://www.faqs.org/faqs/unix-faq/shell/csh-whynot/"" rel=""noreferrer"">not recommended</a> in the first place.  But there could be other examples.</p>

<p>I have a number of Perl scripts in a personal source control system that I install when I set up an account on a new system.  I use an installer script that modifies the <code>#!</code> line of each script as it installs it in my <code>$HOME/bin</code>.  (I haven't had to use anything other than <code>#!/usr/bin/perl</code> lately; it goes back to times when Perl often wasn't installed by default.)</p>

<p>A minor point: the <code>#!/usr/bin/env</code> trick is arguably an abuse of the <code>env</code> command, which was originally intended (as the name implies) to invoke a command with an altered environment.  Furthermore, some older systems (including SunOS 4, if I recall correctly) didn't have the <code>env</code> command in <code>/usr/bin</code>.  Neither of these is likely to be a significant concern.  <code>env</code> does work this way, a lot of scripts do use the <code>#!/usr/bin/env</code> trick, and OS providers aren't likely to do anything to break it.  It <em>might</em> be an issue if you want your script to run on a really old system, but then you're likely to need to modify it anyway.</p>

<p>Another possible issue, (thanks to Sopalajo de Arrierez for pointing it out in comments) is that cron jobs run with a restricted environment. In particular, <code>$PATH</code> is typically something like <code>/usr/bin:/bin</code>. So if the directory containing the interpreter doesn't happen to be in one of those directories, even if it's in your default <code>$PATH</code> in a user shell, then the <code>/usr/bin/env</code> trick isn't going to work. You can specify the exact path, or you can add a line to your crontab to set <code>$PATH</code> (<code>man 5 crontab</code> for details).</p>
","29620"
"Difference between Login Shell and Non-Login Shell?","109326","","<p>I understand the basic difference between an interactive shell and a non-interactive shell.  But what exactly differentiates a login shell from a non-login shell?</p>

<p>Can you give examples for uses of a <strong><em>non-login interactive</em></strong> shell?</p>
","<p>A login shell is the first process that executes under your user ID when you log in for an interactive session. The login process tells the shell to behave as a login shell with a convention: passing argument 0, which is normally the name of the shell executable, with a <code>-</code> character prepended (e.g. <code>-bash</code> whereas it would normally be <code>bash</code>. Login shells typically read a file that does things like setting environment variables: <code>/etc/profile</code> and <code>~/.profile</code> for the traditional Bourne shell, <code>~/.bash_profile</code> additionally for bash<sup>†</sup>, <code>/etc/zprofile</code> and <code>~/.zprofile</code> for zsh<sup>†</sup>, <code>/etc/csh.login</code> and <code>~/.login</code> for csh, etc.</p>

<p>When you log in on a text console, or through SSH, or with <code>su -</code>, you get an <strong>interactive login</strong> shell. When you log in in graphical mode (on an <a href=""http://en.wikipedia.org/wiki/X_display_manager_%28program_type%29"" rel=""noreferrer"">X display manager</a>), you don't get a login shell, instead you get a session manager or a window manager.</p>

<p>It's rare to run a <strong>non-interactive login</strong> shell, but some X settings do that when you log in with a display manager, so as to arrange to read the profile files. Other settings (this depends on the distribution and on the display manager) read <code>/etc/profile</code> and <code>~/.profile</code> explicitly, or don't read them. Another way to get a non-interactive login shell is to log in remotely with a command passed through standard input which is not a terminal, e.g. <code>ssh example.com &lt;my-script-which-is-stored-locally</code> (as opposed to <code>ssh example.com my-script-which-is-on-the-remote-machine</code>, which runs a non-interactive, non-login shell).</p>

<p>When you start a shell in a terminal in an existing session (screen, X terminal, Emacs terminal buffer, a shell inside another, etc.), you get an <strong>interactive, non-login</strong> shell. That shell might read a shell configuration file (<code>~/.bashrc</code> for bash invoked as <code>bash</code>, <code>/etc/zshrc</code> and <code>~/.zshrc</code> for zsh, <code>/etc/csh.cshrc</code> and <code>~/.cshrc</code> for csh, the file indicated by the <code>ENV</code> variable for POSIX/XSI-compliant shells such as dash, ksh, and bash when invoked as <code>sh</code>, <code>$ENV</code> if set and <code>~/.mkshrc</code> for mksh, etc.).</p>

<p>When a shell runs a script or a command passed on its command line, it's a <strong>non-interactive, non-login</strong> shell. Such shells run all the time: it's very common that when a program calls another program, it really runs a tiny script in a shell to invoke that other program. Some shells read a startup file in this case (bash runs the file indicated by the <code>BASH_ENV</code> variable, zsh runs <code>/etc/zshenv</code> and <code>~/.zshenv</code>), but this is risky: the shell can be invoked in all sorts of contexts, and there's hardly anything you can do that might not break something.</p>

<p><sup>†</sup> I'm simplifying a little, see the manual for the gory details.   </p>
","46856"
"Ctrl-s hang terminal emulator?","109246","","<p>I came across a sentence in vimdoc:</p>

<pre><code>Note: CTRL-S does not work on all terminals and might block
                further input, use CTRL-Q to get going again.
</code></pre>

<p>and this key indeed hangs my vim. I was thinking that it was the fault of vim,
since there was no problem when I use <code>C-s</code>/<code>C-x C-s</code> in emacs nox. However
just now when I was reading a manpage and pressed the <code>Ctrl-s</code>, it hangs <code>man</code>
as well(I am setting <code>less</code> as the PAGER).</p>

<p>So can someone tell me what's happening?</p>

<p>The terminal emulators are <code>xterm</code> and <code>lxterminal</code>, and <code>tty</code> also
has this problem. And a <kbd>Ctrl</kbd>+<kbd>q</kbd> puts the process right again in all the cases.</p>
","<p>This feature is called Software Flow Control (XON/XOFF flow control)</p>

<p>When one end of the data link (in this case the terminal emulator) can't receive any more data (because the buffer is full or nearing full or the user sends <code>C-s</code>) it will send an ""XOFF"" to tell the sending end of the data link to pause until the ""XON"" signal is received.  </p>

<p>What is happening under the hood is the ""XOFF"" is telling the TTY driver in the kernel to put the process that is sending data into a sleep state (like pausing a movie) until the TTY driver is sent an ""XON"" to tell the kernel to resume the process as if it were never stopped in the first place.</p>

<p><code>C-s</code> enables terminal scroll lock. Which prevents your terminal from scrolling (By sending an ""XOFF"" signal to pause the output of the software).</p>

<p><code>C-q</code> disables the scroll lock. Resuming terminal scrolling (By sending an ""XON"" signal to resume the output of the software).</p>

<p>This feature is legacy (back from the 80's when terminals were very slow and did not allow scrolling) and is enabled by default.</p>

<p>To disable this feature you need the following in either <code>~/.bash_profile</code> or <code>~/.bashrc</code>:</p>

<pre><code>stty -ixon
</code></pre>
","72092"
"How to remove all default gateways","108773","","<p>I'm running a custom built Linux machine, so <strong>not all Linux commands are available</strong>.<br>
I execute network related commands, so I need to <strong>set a default gateway</strong> right before I run my command, then remove that gateway immediately afterward.  </p>

<p>To do that I run all my commands in one line:</p>

<pre><code>/sbin/route add default gw 10.10.10.10;my command;/sbin/route del default gw 10.10.10.10;
</code></pre>

<p>The problem is, for some reason I once found 2 default gateways on the same machine which caused all my commands to fail because even if I set my default gateway before running my test, it is still messed up and can't run my test.</p>

<p>So is there a way to <strong>remove ALL default gateways in one command</strong> ? I have a large amount of machines that are increasing and it won't be practical to plant a script on every machine. I need a command as <strong>simple</strong> as the following:</p>

<pre><code>/sbin/route del all default;set my default gw;mycommand;/sbin/route del all default;
</code></pre>

<p>All I have found so far is a command to remove <strong>a</strong> default gateway but not all of them
<code>/sbin/route del default</code> which won't work for me.</p>

<p><code>/sbin/route</code> help displays the following</p>

<pre><code>/sbin/route --help

Usage: route [{add|del|delete}]

Edit the kernel's routing tables

Options:
        -n      Don't resolve names
        -e      Display other/more information
        -A inet Select address family
</code></pre>
","<p>All the answers are great but I resolved this problem using a different approach, I used the command to add only one default gateway, but fail if there is already one. And thus eventually remove the wrong gateway at the end of the command. This should work on the second time isa.</p>

<p><code>ip route add default via my-gateway</code>
<code>ip route del default</code></p>
","63662"
"How big is the pipe buffer?","108564","","<p>As a comment in <a href=""https://unix.stackexchange.com/questions/11937/im-confused-as-to-why-true-in-a-makefile-has-the-same-effect-as-true/11938#11938"">I&#39;m confused as to why &quot;| true&quot; in a makefile has the same effect as &quot;|| true&quot;</a> user <a href=""https://unix.stackexchange.com/users/2421/cjm"">cjm</a> wrote:</p>

<blockquote>
  <p>Another reason to avoid | true is that if the command produced enough output to fill up the pipe buffer, it would block waiting for true to read it.</p>
</blockquote>

<p>Do we have some way of finding out what the size of the pipe buffer is?</p>
","<p>The capacity of a pipe buffer varies across systems (and can even vary on the same system). I am not sure there is a quick, easy, and cross platform way to just lookup the capacity of a pipe.</p>

<p>Mac OS X, for example, uses a capacity of 16384 bytes by default, but can switch to 65336 byte capacities if large write are made to the pipe, or will switch to a capacity of a single system page if too much kernel memory is already being used by pipe buffers (see <a href=""http://www.opensource.apple.com/source/xnu/xnu-1504.9.37/bsd/sys/pipe.h""><code>xnu/bsd/sys/pipe.h</code></a>, and <a href=""http://www.opensource.apple.com/source/xnu/xnu-1504.9.37/bsd/kern/sys_pipe.c""><code>xnu/bsd/kern/sys_pipe.c</code></a>; since these are from FreeBSD, the same behavior may happen there, too).</p>

<p>One Linux <a href=""http://linux.die.net/man/7/pipe""><em>pipe(7)</em> man page</a> says that pipe capacity is 65536 bytes since Linux 2.6.11 and a single system page prior to that (e.g. 4096 bytes on (32-bit) x86 systems). The code (<a href=""http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=include/linux/pipe_fs_i.h;h=77257c92155aa4efb6ea68d9c99298426aedc60b;hb=HEAD""><code>include/linux/pipe_fs_i.h</code></a>, and <a href=""http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=fs/pipe.c;h=da42f7db50de42640a7fa56df21e21bbb48cf588;hb=HEAD""><code>fs/pipe.c</code></a>) seems to use 16 system pages (i.e. 64 KiB if a system page is 4 KiB), but the buffer for each pipe can be adjusted via a <em>fcntl</em> on the pipe (up to a maximum capacity which defaults to 1048576 bytes, but can be changed via <code>/proc/sys/fs/pipe-max-size</code>)).</p>

<hr>

<p>Here is a little <em>bash</em>/<em>perl</em> combination that I used to test the pipe capacity on my system:</p>

<pre><code>#!/bin/bash
test $# -ge 1 || { echo ""usage: $0 write-size [wait-time]""; exit 1; }
test $# -ge 2 || set -- ""$@"" 1
bytes_written=$(
{
    exec 3&gt;&amp;1
    {
        perl -e '
            $size = $ARGV[0];
            $block = q(a) x $size;
            $num_written = 0;
            sub report { print STDERR $num_written * $size, qq(\n); }
            report; while (defined syswrite STDOUT, $block) {
                $num_written++; report;
            }
        ' ""$1"" 2&gt;&amp;3
    } | (sleep ""$2""; exec 0&lt;&amp;-);
} | tail -1
)
printf ""write size: %10d; bytes successfully before error: %d\n"" \
    ""$1"" ""$bytes_written""
</code></pre>

<p>Here is what I found running it with various write sizes on a Mac OS X 10.6.7 system (note the change for writes larger than 16KiB):</p>

<pre><code>% /bin/bash -c 'for p in {0..18}; do /tmp/ts.sh $((2 ** $p)) 0.5; done'
write size:          1; bytes successfully before error: 16384
write size:          2; bytes successfully before error: 16384
write size:          4; bytes successfully before error: 16384
write size:          8; bytes successfully before error: 16384
write size:         16; bytes successfully before error: 16384
write size:         32; bytes successfully before error: 16384
write size:         64; bytes successfully before error: 16384
write size:        128; bytes successfully before error: 16384
write size:        256; bytes successfully before error: 16384
write size:        512; bytes successfully before error: 16384
write size:       1024; bytes successfully before error: 16384
write size:       2048; bytes successfully before error: 16384
write size:       4096; bytes successfully before error: 16384
write size:       8192; bytes successfully before error: 16384
write size:      16384; bytes successfully before error: 16384
write size:      32768; bytes successfully before error: 65536
write size:      65536; bytes successfully before error: 65536
write size:     131072; bytes successfully before error: 0
write size:     262144; bytes successfully before error: 0
</code></pre>

<p>The same script on Linux 3.19:</p>

<pre><code>/bin/bash -c 'for p in {0..18}; do /tmp/ts.sh $((2 ** $p)) 0.5; done'
write size:          1; bytes successfully before error: 65536
write size:          2; bytes successfully before error: 65536
write size:          4; bytes successfully before error: 65536
write size:          8; bytes successfully before error: 65536
write size:         16; bytes successfully before error: 65536
write size:         32; bytes successfully before error: 65536
write size:         64; bytes successfully before error: 65536
write size:        128; bytes successfully before error: 65536
write size:        256; bytes successfully before error: 65536
write size:        512; bytes successfully before error: 65536
write size:       1024; bytes successfully before error: 65536
write size:       2048; bytes successfully before error: 65536
write size:       4096; bytes successfully before error: 65536
write size:       8192; bytes successfully before error: 65536
write size:      16384; bytes successfully before error: 65536
write size:      32768; bytes successfully before error: 65536
write size:      65536; bytes successfully before error: 65536
write size:     131072; bytes successfully before error: 0
write size:     262144; bytes successfully before error: 0
</code></pre>

<hr>

<p>Note: The <code>PIPE_BUF</code> value defined in the C header files (and the <em>pathconf</em> value for <code>_PC_PIPE_BUF</code>), does not specify the capacity of pipes, but the maximum number of bytes that can be written atomically (see <a href=""http://pubs.opengroup.org/onlinepubs/009695399/functions/write.html#tag_03_866"">POSIX <em>write(2)</em></a>).</p>

<p>Quote from <a href=""http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=blob;f=include/linux/pipe_fs_i.h;h=77257c92155aa4efb6ea68d9c99298426aedc60b;hb=HEAD#l134""><code>include/linux/pipe_fs_i.h</code></a>:</p>

<pre><code>/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual
   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */
</code></pre>
","11954"
"Extract a part of one line from a file with sed","108453","","<p>I want to read one part of one line from a file.
For example:</p>

<blockquote>
  <p>POP3_SERVER_NAME = localhost</p>
</blockquote>

<p>I want to return only <code>localhost</code>, using sed.</p>

<p>This text is on the third line. I do this to extract the line:</p>

<pre><code>sed -n '3p' installation.sh
</code></pre>

<p>How do I extract only the <code>localhost</code> part?</p>
","<p>awk might be a better tool here.</p>

<pre><code>$ cat test.dat
LINE 1
LINE 2
POP3_SERVER_NAME = localhost
</code></pre>

<p>Search for lines that contain ""POP3_SERVER_NAME""; print the last field. This doesn't depend on POP3_SERVER_NAME always being on line 3, which is probably a Good Thing.</p>

<pre><code>$ awk '/POP3_SERVER_NAME/{print $NF}' test.dat
localhost
</code></pre>

<p>Depending on your application, you might need to make the regular expression more stringent. For example, you might want to match only that line that <em>starts</em> with POP3_SERVER_NAME.</p>

<pre><code>$ awk '/^POP3_SERVER_NAME/{print $NF}' test.dat
localhost
</code></pre>

<p>Using sed is a little less intuitive. (Thanks, I'm aware of the irony.) Address the line that contains POP3_SERVER_NAME anywhere. Substitute an empty string for all the text from the beginning of the line to the optional space following ""="". Then print.</p>

<pre><code>sed -n -e '/POP3_SERVER_NAME/ s/.*\= *//p' test.dat
</code></pre>
","84957"
"How to uppercase the command line argument?","108371","","<p>I searched SO and found that to uppercase a string following would work</p>

<pre><code>str=""Some string""
echo ${str^^}
</code></pre>

<p>But I tried to do a similar thing on a command-line argument, which gave me the following error</p>

<h3>Tried</h3>

<pre><code>#!/bin/bash
             ## Output
echo ${1^^}  ## line 3: ${1^^}: bad substitution
echo {$1^^}  ## No error, but output was still smaller case i.e. no effect
</code></pre>

<p>How could we do this?</p>
","<p>The syntax <code>str^^</code> which you are trying is available from Bash 4.0 and above. Perhaps yours is an older version:</p>

<p>Try this:</p>

<pre><code>str=""Some string""
echo $str | awk '{print toupper($0)}'
</code></pre>
","51987"
"Kill many instances of a running process with one command","108242","","<p>Suppose I have a thousand or more instances of any process (for example, <code>vi</code>) running. How do I kill them all in one single shot/one line command/one command?</p>
","<p>What's wrong with the good old,</p>

<pre><code>for pid in $(ps -ef | grep ""some search"" | awk '{print $2}'); do kill -9 $pid; done
</code></pre>

<p>There are ways to make that more efficient,</p>

<pre><code>for pid in $(ps -ef | awk '/some search/ {print $2}'); do kill -9 $pid; done
</code></pre>

<p>and other variations, but at the basic level, it's always worked for me.</p>
","50573"
"Remove unused packages","108183","","<p>I have installed some rpm package on my Fedora 17. Some packages had a lot of dependencies.
I have removed some packages but I forgot remove unused dependencies with yum remove.</p>

<p>How can I do that now?</p>
","<p>It's not easy.  How do you differentiate between ""a file that was required by something I have since removed"" from ""a file that is not required by anything else that I really want""?  </p>

<p>You can use the <code>package-cleanup</code> command from the <code>yum-utils</code> package to list ""leaf nodes"" in your package dependency graph.  These are packages that can be removed without affecting anything else:</p>

<pre><code>$ package-cleanup --leaves
</code></pre>

<p>This will produce a list of ""libraries"" on which nothing else depends.  In most cases you can safely remove these packages.  If you add <code>--all</code> to the command line:</p>

<pre><code>$ package-cleanup --leaves --all
</code></pre>

<p>You'll get packages that aren't considered libraries, also, but this list is going to be so long that it probably won't be useful.</p>
","40180"
"Writing basic systemd service files","108062","","<p>I am developing a Nodejs application that the user interacts with via HTTP on localhost. There are practically no parameters and the daemon has virtually no dependencies and it just needs to be up by log-in time.</p>

<p>I would like to follow the idioms on each platform for start-up scripts, and that means Upstart on Ubuntu and systemd on Fedora.</p>

<ul>
<li><strong>Are there any good tutorials for writing systemd system files?</strong></li>
<li><strong>Are there any 'best practices' to be aware of?</strong></li>
</ul>

<p>I have found these resources:</p>

<ul>
<li><a href=""http://fedoraproject.org/wiki/Features/systemd"">Fedora wiki page about systemd</a></li>
<li><a href=""http://patrakov.blogspot.com/2011/01/writing-systemd-service-files.html"">Blog about writing systemd system files</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Systemd"">Systemd on Wikipedia</a></li>
</ul>

<p>I am mostly looking for an API of sorts as a reference, as well as a basic format to follow.</p>
","<p>I would start with the <a href=""http://0pointer.de/public/systemd-man/"">Systemd manual pages</a>. They represent a comprehensive resource of the system and services.</p>

<p>There is also the <a href=""http://www.freedesktop.org/wiki/Software/systemd/FrequentlyAskedQuestions"">freedesktop Systemd FAQs</a>. </p>
","15350"
"How to clear memory cache in Linux","108051","","<p><img src=""https://i.stack.imgur.com/auNr7.png"" alt=""TOP""></p>

<p>Is there any command that by using I can clean the cache in RHEL?</p>

<p>I used this command:</p>

<pre><code>sync; echo 3 &gt; /proc/sys/vm/drop_caches
</code></pre>

<p>but it didn't work.</p>
","<p>Try  <code>sync; echo 1 &gt; /proc/sys/vm/drop_caches</code>.</p>
","58564"
"How do I add a user in SVN?","107733","","<p>UPDATE #3:
I think I found it.<br>
/etc/httpd/conf.d/subversion.conf </p>

<pre><code>&lt;Location /repos&gt;
        DAV svn
        SVNPath /var/www/svn/repos
        AuthType Basic
        AuthName ""Subversion repos""
        AuthUserFile /etc/svn-auth-conf
        Require valid-user
&lt;/Location&gt;
</code></pre>

<p>svn-auth-conf is in the format of </p>

<pre><code>user1:$apr1$randome letters ... $random letters and numbers
</code></pre>

<p>I feel like I used some command line tool to add the users but I can't remember what.</p>

<p>UPDATE #2:
locate http.conf gives these results</p>

<pre><code>/usr/share/logwatch/default.conf/logfiles/http.conf
/usr/share/logwatch/default.conf/services/http.conf
</code></pre>

<p>I don't see anything in either of them that looked related to subversion.  I can post the files here if it will help.</p>

<p>original question:</p>

<p>A few years ago, I installed and setup subversion.  I don't remember how I added a user.  I need to add another now, but when I checked the <code>passwd</code> file there were no users defined, nor was there anything set in the <code>svnserve.conf</code> file -- both had everything commented out.  That is the only way I can find to add users via google.  So I tried it:</p>

<h3>passwd</h3>

<pre><code>user1 = password1
user2 = password2
</code></pre>

<h3>svnserve.conf</h3>

<pre><code>anon-access = none
auth-access = write
pasword-db = passwd
authz-db - authz
realm = repos
</code></pre>

<p>but I can still only access it with <code>user1</code>, even though <code>user1</code> is not a user on the linux server.  Is there some other way to add users or do I have something wrong in my configuration?</p>

<p>update: </p>

<p>I've added the following line to my svnserve.conf file</p>

<pre><code>authz-db = authz
</code></pre>

<p>then my authz file looks like this</p>

<p>authz
    [groups]
    devs = user1,user2</p>

<pre><code>[repos:/]
devs = rw
</code></pre>

<p>still doesn't work.  the password for user1 is different in the passwd than I use to login.  So i think that subversion is not using that for authentication at all.  especially since the file was empty before and I was still able to login.  is there any way to make sure it uses the svnserve,passwd,authz files for authentication?  is there another config file somewhere?</p>
","<p>Subversion authentication typically has two parts. The first is a password repository of some kind. The second is an access control list. The password list is used to authenticate users and check their passwords, but you also have to set which users have access to which resources.</p>

<p>There are several password repository types. I am not familier with the <em>passwd</em> type, I use the <em>htpasswd</em> type that uses the same user/password files that apache does for user authentication. I add users to this file with <code>htpasswd .htpasswd username</code>. It looks like you may have done this step correctly for your password type.</p>

<p>The second part is the ACL. Subversion should have a file called <code>access</code> somewhere with sections for each of your repositories. You will need to add a line under the repository you want to access with <code>username = rw</code> to give that user read/write access to that repository.</p>
","15933"
"How do I count the number of occurrences of a word in a text file with the command line?","107400","","<p>I have a large JSON file that is on one line, and I want to use the command line to be able to count the number of occurrences of a word in the file. How can I do that?</p>
","<pre><code>$ tr ' ' '\n' &lt; FILE | grep WORD | wc -l
</code></pre>

<p>Where <code>tr</code> replaces spaces with newlines, <code>grep</code> filters all resulting lines matching WORD and <code>wc</code> counts the remaining ones.</p>

<p>One can even save the <code>wc</code> part using the <code>-c</code> option of grep:</p>

<pre><code>$ tr ' ' '\n' &lt; FILE | grep -c WORD
</code></pre>

<p>The <code>-c</code> option is defined by POSIX.</p>

<p>If it is not guaranteed that there are spaces between the words, you have to use some other character (as delimiter) to replace. For example alternative tr parts are</p>

<pre><code>tr '""' '\n'
</code></pre>

<p>or</p>

<pre><code>tr ""'"" '\n'
</code></pre>

<p>if you want to replace double or single quotes.</p>

<p>In case you need to count WORD but not prefixWORD, WORDsuffix or  prefixWORDsuffix, you can enclose the WORD pattern in word-begin/end markers:</p>

<pre><code>grep -c '\&lt;WORD\&gt;'
</code></pre>
","2245"
"Connecting Linux Mint to WiFi network","107328","","<p>I just installed Linux Mint 17 (MATE) on an old laptop and everything works amazing, however I can't seem to get it to connect to my WiFi network. All my other computers can get access, plus, before when the laptop has Windows XP, it could also find and connect. Is there a way to check if it's even detecting the correct network? If so, how would I set up a proper connection to the network?</p>

<p>There is nothing wrong with my network nor the laptop, so it must be Mint's fault.</p>

<p><strong>Edit</strong>:
Output of <code>iwconfig</code>:</p>

<pre><code>lo       no wireless extensions.

eth0     no wireless extensions.
</code></pre>

<p>Output of <code>lspci -nn | grep 0280</code>:</p>

<pre><code>02:04.0 Network controller [0280]: Broadcom Corporation BCM4318 [AirForce One 54g] 802.11g Wireless LAN Controller [14e4:4318] (rev 02)
</code></pre>
","<p>This answer assumes that you can connect your machine to the network using a cable and so get internet access. If that assumption is wrong, let me know and I'll modify this. </p>

<p>You need to install the driver for your wireless card. The <a href=""http://wireless.kernel.org/en/users/Drivers/b43#supported"">driver support table</a> of the Linux Wireless page lists it as supported so you should be able to get everything working by simply running:</p>

<pre><code>sudo apt-get install firmware-b43-installer
</code></pre>

<p>If this does not work leave me a comment, you might need to tweak it a bit. </p>

<p>Further reading:</p>

<ul>
<li><a href=""http://forums.linuxmint.com/viewtopic.php?f=194&amp;t=139947&amp;start=20"">http://forums.linuxmint.com/viewtopic.php?f=194&amp;t=139947&amp;start=20</a></li>
<li><a href=""https://help.ubuntu.com/community/WifiDocs/Driver/bcm43xx"">https://help.ubuntu.com/community/WifiDocs/Driver/bcm43xx</a></li>
</ul>
","156216"
"How can I tell what version of apache I'm running?","107192","","<p>I know about the phpinfo() way but is there any other way? I'm using CentOS and I can't find the httpd executable to run httpd -v.</p>
","<p>Either <code>rpm -q httpd</code> or <code>/usr/sbin/httpd -v</code> should work.</p>
","6793"
"Multiline shell script comments - how does this work?","107079","","<p>Recently, I stumbled upon a multiline comment type I have never seen before - here is a script example:</p>

<pre><code>echo a
#
: aaa 
: ddd 
#
echo b
</code></pre>

<p>This seems to work, hell, even vim syntax-highlights it. What is this style of commenting called and how to I find more info about it?</p>
","<p>That is not a multi-line comment.  <code>#</code> is a single line comment. 
<a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#colon""><code>:</code> (colon)</a> is not a comment at all, but rather a shell built-in command that is basically a <a href=""http://en.wikipedia.org/wiki/Nop"">NOP</a>, a null operation that does nothing except return true, like <code>true</code> (and thus setting <code>$?</code> to 0 as a side effect).  However since it is a command, it can accept arguments, and since it ignores its arguments, in most cases it superficially acts like a comment.  The main problem with this kludge is the arguments are still expanded, leading to a host of unintended consequences.  The arguments are still affected by syntax errors, redirections are still performed so <code>: &gt; file</code> will truncate <code>file</code>, and <code>: $(dangerous command)</code> substitutions will still run.</p>

<p>The least surprising completely safe way to insert comments in shell scripts is with <code>#</code>.  Stick to that even for multi-line comments.  <em>Never</em> attempt to (ab)use <code>:</code> for comments.  There is no dedicated multi-line comment mechanism in shell that is analogous to the slash-star <code>/* */</code> form in <code>C</code>-like languages.</p>

<hr>

<p>For the sake of completeness, but not because it is recommended practice, I will mention that it is possible to use <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_07_04"">here-documents</a> to do multi-line ""comments"":</p>

<pre><code>: &lt;&lt;'end_long_comment'
This is an abuse of the null command ':' and the here-document syntax
to achieve a ""multi-line comment"".  According to the POSIX spec linked 
above, if any character in the delimiter word (""end_long_comment"" in 
this case) above is quoted, the here-document will not be expanded in 
any way.  This is **critical**, as failing to quote the ""end_long_comment"" 
will result in the problems with unintended expansions described above. 
All of this text in this here-doc goes to the standard input of :, which 
does nothing with it, hence the effect is like a comment.  There is very 
little point to doing this besides throwing people off.  Just use '#'.
end_long_comment
</code></pre>
","37414"
"Where are my sshd logs?","106976","","<p>I can't find my sshd logs in the standard places.</p>

<p>What I've tried:</p>

<ul>
<li>Not in <code>/var/log/auth.log</code></li>
<li>Not in <code>/var/log/secure</code></li>
<li>Did a system search for <code>'auth.log'</code> and found nothing</li>
<li>I've set <code>/etc/ssh/sshd_config</code> to explicitly use <code>SyslogFacility AUTH</code> and <code>LogLevel INFO</code> and restarted sshd and still can't find them.</li>
</ul>

<p>I'm using OpenSSH 6.5p1-2 on Arch Linux.</p>
","<p>I have found the output of sshd and other core services in 'journalctl'.</p>

<p>See more at the Arch Wiki entry for systemd:</p>

<p><a href=""https://wiki.archlinux.org/index.php/systemd#Journal"">https://wiki.archlinux.org/index.php/systemd#Journal</a></p>
","114191"
"Concatenating two variables with an underscore","106976","","<p>I need to concatenate two variables to create a filename that has an underscore.
Lets call my variables <code>$FILENAME</code> and <code>$EXTENSION</code> where filename is read from a file.</p>

<pre><code>FILENAME=Hello
EXTENSION=WORLD.txt
</code></pre>

<p>Now...</p>

<p>I have tried the following without success:</p>

<pre><code>NAME=${FILENAME}_$EXTENSION
NAME=${FILENAME}'_'$EXTENSION
NAME=$FILENAME\\_$EXTENSION
</code></pre>

<p>I always get some kind of weird output. Usually the underscore first.</p>

<p>I need it to be</p>

<pre><code>echo $NAME
Hello_WORLD.txt
</code></pre>
","<p>You can use something like this:</p>

<pre><code>NAME=$(echo ${FILENAME}_${EXTENSION})
</code></pre>

<p>This works as well:</p>

<pre><code>NAME=${FILENAME}_${EXTENSION}
</code></pre>
","88453"
"Vi vs vim, or, is there any reason why I would ever want to use vi?","106853","","<p>I know a bit about *NIX text editors (currently migrating from <code>nano</code> to <code>vim</code>), and, after looking around a bit on the Unix &amp; Linux SE, have noticed that <code>vi</code> is used instead of 'vim' in a fair number of questions.  I know that 'vim' stands for 'Vi IMproved', and, with that in mind, am wondering why anyone would rather use vi instead of vim.  Does vi have any significant advantage over vim?</p>

<p>Edit: I think that my question is being misinterpreted.  I know that vim is, for the most part, significantly more powerful and feature-complete then vi is.  What I want to know is if there are any possible cases where vi has an advantage over vim, such as less memory use, prevalence on *nix systems, etc.</p>
","<p><code>vi</code> is (also) a standard. There are plenty of implementations and <code>vim</code> is likely the most popular at least on Linux.</p>

<p>While many traditional Unix compliant OSes provide <code>vi</code> implementations very close to the standard, <code>vim</code> has added a lot of extra features that make it a double-edged sword.</p>

<p>Of course, these extensions are usually designed to ease the editing process and provide useful features and functionalities. However, once you are used to some of them (not the cosmetic ones like syntax coloring but those that change the editor's behavior) you can easily forget they are specific; and using a different implementation, including the ones based on the original BSD code can be very frustrating. The opposite is also true.</p>

<p>This is quite similar to the issue that happens with scripts using non POSIX bashisms faced to more orthodox shell implementations like <code>dash</code> or <code>ksh</code>.</p>
","61815"
"Create partition aligned using parted","106695","","<p>I'm partitioning a non-SSD hard disk with <em>parted</em> because I want a GPT partition table.  </p>

<pre><code>parted /dev/sda mklabel gpt
</code></pre>

<p>Now, I'm trying to create the partitions correctly aligned so I use the following command to know where the first sector begins:</p>

<pre><code>parted /dev/sda unit s p free

Disk /dev/sda: 488397168s
Sector size (logical/physical): 512B/512B
Partition Table: gpt

Number  Start  End         Size        File system  Name      Flags
        34s    488397134s  488397101s  Free Space
</code></pre>

<p>We can see that it starts in sector 34 (that's the default when this partition table is used).</p>

<p>So, to create the first partition I tried:</p>

<pre><code>parted /dev/sda mkpart primary 63s 127s
</code></pre>

<p>to align it on sector 64 since it's a multiple of 8 but it shows:</p>

<p><strong>Warning: The resulting partition is not properly aligned for best performance.</strong></p>

<p>The logical and physical sector sizes in my hard disk are both 512 bytes:</p>

<pre><code>cat /sys/block/sda/queue/physical_block_size
512

cat /sys/block/sda/queue/logical_block_size 
512
</code></pre>

<p>How do I create partitions correctly aligned? What am I doing wrong?</p>
","<p>In order to align partition with <code>parted</code> you can use <code>--align</code> option. Valid alignment types are:</p>

<ul>
<li><strong>none</strong>  - Use the minimum alignment allowed by the disk type.</li>
<li><strong>cylinder</strong> - Align partitions to cylinders.</li>
<li><strong>minimal</strong> - Use minimum alignment as given by the disk topology information. This and the opt value will use layout information provided by the disk to align the logical partition table addresses  to  actual  physical  blocks  on  the disks. The min value is the minimum alignment needed to align the partition properly to physical blocks, which avoids performance degradation.</li>
<li><strong>optimal</strong> Use optimum alignment as given by the disk topology information. This aligns to a multiple of the physical block size in a way that guarantees optimal performance.</li>
</ul>

<p>Other useful tip is that you can set the size with percentages to get it aligned. Start at 0% and end at 100%. For example:</p>

<p><code>parted -a optimal /dev/sda mkpart primary 0% 4096MB</code></p>
","49274"
"How to list keys added to ssh-agent with ssh-add?","106681","","<p>How and where can I check what keys have been added with <code>ssh-add</code> to my <code>ssh-agent</code> ?</p>
","<p>Use the <code>-l</code> option to <code>ssh-add</code> to list them by fingerprint.</p>

<pre><code>$ ssh-add -l
2048 72:...:eb /home/gert/.ssh/mykey (RSA)
</code></pre>

<p>Or with <code>-L</code> to get the full key in OpenSSH format.</p>

<pre><code>$ ssh-add -L
ssh-rsa AAAAB3NzaC1yc[...]B63SQ== /home/gert/.ssh/id_rsa
</code></pre>

<p>The latter format is the same as you would put them in a <code>~/.ssh/authorized_keys</code> file.</p>
","58977"
"Any reason NOT to run Linux in a VM all the time?","106638","","<p>I've switched to using Arch Linux for most of my day to day work and don't need Windows for anything but gaming and the couple of apps that aren't ported to Linux like OneNote. My Linux distribution is hosted in VirtualBox with Windows as host, and I quite like it that way, snapshots are incredibly useful.</p>

<p>Let's say I were to pretty much never care about the Windows host and spend 95% of the time in the guest, what would I be missing out on?</p>

<p>Are there serious downsides?</p>

<p>Is performance severely affected and will installing straight onto the machine make my life much more amazing?</p>
","<p>Assuming you can get everything working, and you don't want to do resource intensive tasks such as playing games or doing large compiles, then I think you'll be fine.</p>

<p>There's some basic issues you will probably encounter:</p>

<ul>
<li>guest time incorrect</li>
<li>guest screen size or color depth incorrect</li>
<li>can't access USB devices (printers, phones, etc.)</li>
</ul>

<p>To fix this, you should install <a href=""http://www.virtualbox.org/manual/ch04.html"" rel=""noreferrer"">VirtualBox guest additions</a>.  See the <a href=""https://wiki.archlinux.org/index.php/VirtualBox#Arch_Linux_guests"" rel=""noreferrer"">VirtualBox Arch Linux guests guide</a> for details.</p>

<p>To get some extra features, such as USB 2.0 and Intel PXE support, you can also install the <a href=""https://www.virtualbox.org/manual/ch01.html#intro-installing"" rel=""noreferrer"">VirtualBox extension pack</a>.</p>

<p>After that, there's a few issues you should know about:</p>

<ul>
<li><a href=""https://www.virtualbox.org/ticket/8873"" rel=""noreferrer"">can't use USB 3.0</a></li>
<li><a href=""https://www.virtualbox.org/ticket/721"" rel=""noreferrer"">can't use IEEE1394/""FireWire""</a></li>
<li><a href=""https://www.virtualbox.org/ticket/2720"" rel=""noreferrer"">can't use seamless mode in combination with dual-head</a></li>
<li><a href=""https://www.virtualbox.org/ticket/3135"" rel=""noreferrer"">time gets out of sync on 64-bit guests</a></li>
</ul>

<p>Obviously your Linux VM will be affected if your Windows system crashes too.  Issues I've had happen recently:</p>

<ul>
<li>Windows host crashes due to driver bug (blue screen)</li>
<li>Windows host reboots due to security update</li>
</ul>

<p>When running a virtual machine <a href=""http://www.hanselman.com/blog/VMPerformanceChecklistBeforeYouComplainThatYourVirtualMachineIsSlow.aspx"" rel=""noreferrer"">the biggest performance hit will be to your disk I/O</a>.  If at all possible, <a href=""http://www.codinghorror.com/blog/2006/10/the-single-most-important-virtual-machine-performance-tip.html"" rel=""noreferrer"">put your VM on a separate disk</a> and/or <a href=""https://superuser.com/questions/291056/virtual-machines-and-ssds"">use a solid-state drive</a>.  Using a <a href=""http://www.virtualbox.org/manual/ch05.html#harddiskcontrollers"" rel=""noreferrer"">virtual SATA drive</a> instead of a virtual IDE drive can help too.</p>
","38384"
"Kali Linux: On install, designated root password cannot login - incorrect password","106565","","<p>I found an old laptop lying around and I figured I would install Kali Linux on it to learn more about penetration testing and to try to break into my servers for practice and learning more about creating a secure network.</p>

<p>I used unetbootin to install the Kali Linux ISO I downloaded via the torrent from their site, and the install to my flash drive went successfully.</p>

<p>On the laptop I was wanting to put Kali on, I went into the BIOS and ordered the USB to be the priority to boot from. After that, I booted live into Kali and installed the operating system from there. It loaded up the graphical install page from Kali and the install seemed to go successfully.</p>

<p>However, once the installation was successful, it would take me to the login page and prompt me for the root password. I would type in the root password I had used during the initial setup, yet it gives me the <code>Incorrect password</code> prompt, even though I am 100% sure I was using the password I had installed at the start.</p>

<p>To confirm this, I tried reinstalling from the live boot multiple times, using the same password from the first attempt and even just doing ""password"" for testing purposes. Neither one worked.</p>

<p>As a final measure, I opened up the GParted Partition Editor from the Kali live boot and deleted all of the partitions on my laptop's hard drive. From there, I went through the Kali installation process again successfully, only to encounter the EXACT same issue as before when trying to log in after installation.</p>

<p>Does anyone know what is going on? I am clueless as to what I could do to be able to log into my Kali installation... thanks.</p>
","<p>Try this in login window:</p>

<pre><code>Username: root
Password: password
</code></pre>

<p>the ""root"" is exactly word ""root"", means root user</p>

<p>the ""password"" is the password you created during installation.</p>

<p>From: <a href=""https://forums.kali.org/showthread.php?18428-Username-PW-login-failure-to-Kali-Linux"" rel=""noreferrer"">https://forums.kali.org/showthread.php?18428-Username-PW-login-failure-to-Kali-Linux</a></p>
","257177"
"Remove line containing certain string and the following line","106533","","<p>I use this</p>

<p><code>cat foo.txt | sed '/bar/d'</code></p>

<p>to remove lines containing the string <code>bar</code> in the file.</p>

<p>I would like however to remove those lines <em>and the line directly after it</em>. Preferably in <code>sed</code>, <code>awk</code> or other tool that's available in MinGW32.</p>

<p>It's a kind of reverse of what I can get in <code>grep</code> with <code>-A</code> and <code>-B</code> to print matching lines as well as lines before/after the matched line.</p>

<p>Is there any easy way to achieve it?</p>
","<p>If you have GNU sed (so non-embedded Linux or Cygwin):</p>

<pre><code>sed '/bar/,+1 d'
</code></pre>

<p>If you have <code>bar</code> on two consecutive lines, this will delete the second line without analyzing it. For example, if you have a 3-line file <code>bar</code>/<code>bar</code>/<code>foo</code>, the <code>foo</code> line will stay.</p>
","56166"
"How to automatically start and shut down VirtualBox machines?","106445","","<p>I need to run a software system that is intended to be installed as an appliance on a dedicated machine. In order to save energy, I plan to run the system on a VirtualBox VM instead.</p>

<p>The host is a standard Linux box with a SysV-Init system, the guest is a heavily modified Linux and I would prefer not to have to alter it further. VirtualBox is used in the OSE version.</p>

<p>I have already figured out how to start the VM when the host boots (<em>Edit:</em> this is done, as Nikhil mentioned below, through the command <code>VBoxManager startvm</code>), but how can I gracefully shut down the VM? Any script running on the host would need to wait until the guest has fully shut down. </p>

<p>Can anyone suggest how, for example, a service file doing this would have to look?</p>
","<p>Have you tried <code>acpipowerbutton</code> from this command set?</p>

<pre><code>VBoxManage controlvm        &lt;uuid&gt;|&lt;name&gt;
                            pause|resume|reset|poweroff|savestate|
                            acpipowerbutton|acpisleepbutton|
</code></pre>

<p>Edit after reading the comments:</p>

<p>You can use <code>acpid</code> or other acpi utilities to make it graceful. Also, can you provide more information about how do you shutdown the machine at the moment? </p>

<p>Plain <code>shutdown</code> wouldn't wait for unfinished jobs, a time delay may be too long.</p>

<p>I assume you aren't using a window manager so try <a href=""http://vboxtool.sourceforge.net/"">this</a> tool.</p>

<p>Just seen <a href=""http://www.glump.net/howto/virtualbox_as_a_service"">this daemon</a>. You might find it useful.</p>
","29384"
"Difference between Kali Linux 2.0 mini and Light","106440","","<p>Recently <a href=""https://www.kali.org/downloads/"">Kali Linux 2.0</a> is released and they release Normal,Minimal and Light ISO installations. Minimal Kali Linux installations has necessary software packages and some other motivations.<a href=""https://unix.stackexchange.com/questions/107110/what-is-the-difference-between-kali-linux-minimal-and-normal"">Some of them are listed here</a>. But what is Light installation? </p>
","<p>The <a href=""http://docs.kali.org/installation/kali-linux-network-mini-iso-install"">mini ISO</a> only provides the minimum necessary to boot Kali and allow installation over the Internet; that's why it's so small. The normal ISO provides the full Kali 2.0 setup using GNOME 3, which now requires 768MB of memory. The <a href=""https://www.kali.org/releases/kali-linux-20-released/"">light ISO</a> provides a Kali 2.0 setup using XFCE, and a smaller selection of tools (Iceweasel, OpenSSH, NMap, NCrack, SQLMap and AirCrack-NG; see <a href=""http://git.kali.org/gitweb/?p=live-build-config.git;a=blob;f=kali-config/variant-light/package-lists/kali.list.chroot;h=5dfc12ba6c75f3e6bfd0983da6ac702372f06c8e;hb=HEAD""><code>variant-light/package-lists/kali.list.chroot</code> in the live build configuration</a> for details).</p>
","222616"
"Transfer files using scp: permission denied","106303","","<p>I try to transfer files from remote computer using <code>ssh</code> to my computer : </p>

<pre><code>scp My_file.txt user_id@server:/Home</code></pre>

<p>This should put My_file.txt in the home folder on my own computer, right? 
I get </p>

<blockquote>scp/Home: permission denied</blockquote> 

<p>Also when I try: <code>...@server:/Desktop</code>, in order to copy the files from the remote computer to my desktop. </p>

<p>What am I doing wrong? </p>
","<p>Your commands are trying to put the new Document to the <strong>root</strong> (<code>/</code>) of your machine. What you want to do is to transfer them to your <strong>home</strong> directory (since you have no permissions to write to <code>/</code>). If path to your home is something like <code>/home/erez</code> try the following:</p>

<pre><code>scp My_file.txt user_id@server:/home/erez/
</code></pre>

<p>You can substitute the path to your home directory with the shortcut <code>~/</code>, so the following will have the same effect:</p>

<pre><code>scp My_file.txt user_id@server:~/
</code></pre>

<p>You can even leave out the path altogether on the remote side; this means your home directory.</p>

<pre><code>scp My_file.txt user_id@server:
</code></pre>

<p>That is, to copy the file to your desktop you might want to transfer it to <code>/home/erez/Desktop/</code>:</p>

<pre><code>scp My_file.txt user_id@server:/home/erez/Desktop/
</code></pre>

<p>or using the shortcut:</p>

<pre><code>scp My_file.txt user_id@server:~/Desktop/
</code></pre>

<p>or using a relative path on the remote side, which is interpreted relative to your home directory:</p>

<pre><code>scp My_file.txt user_id@server:Desktop/
</code></pre>

<hr>

<h2>Edit:</h2>

<p>As @ckhan already mentioned, you also have to swap the arguments, it has to be </p>

<pre><code>scp FROM TO
</code></pre>

<p>So if you want to copy the file <code>My_file.txt</code> from the server <code>user_id@server</code> to your desktop you should try the following:</p>

<pre><code>scp user_id@server:/path/to/My_file.txt ~/Desktop/
</code></pre>

<p>If the file <code>My_file.txt</code> is located in your home directory on the server you may again use the shortcut:</p>

<pre><code>scp user_id@server:~/My_file.txt ~/Desktop/
</code></pre>
","47911"
"What is the difference between ""sort -u"" and ""sort | uniq""?","105878","","<p>Everywhere I see someone needing to get a sorted, unique list, they always pipe to <code>sort | uniq</code>. I've never seen any examples where someone uses <code>sort -u</code> instead. Why not? What's the difference, and why is it better to use uniq than the unique flag to sort?</p>
","<p><code>sort | uniq</code> existed before <code>sort -u</code>, and is compatible with a wider range of systems, although almost all modern systems do support <code>-u</code> -- it's POSIX. It's mostly a throwback to the days when <code>sort -u</code> didn't exist (and people don't tend to change their methods if the way that they know continues to work, just look at <code>ifconfig</code> vs. <code>ip</code> adoption).</p>

<p>The two were likely merged because removing duplicates within a file requires sorting (at least, in the standard case), and is an extremely common use case of sort. It is also faster internally as a result of being able to do both operations at the same time (and due to the fact that it doesn't require IPC between <code>uniq</code> and <code>sort</code>). Especially if the file is big, <code>sort -u</code> will likely use fewer intermediate files to sort the data.</p>

<p>On my system I consistently get results like this:</p>

<pre><code>$ dd if=/dev/urandom of=/dev/shm/file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB) copied, 8.95208 s, 11.7 MB/s
$ time sort -u /dev/shm/file &gt;/dev/null

real        0m0.500s
user        0m0.767s
sys         0m0.167s
$ time sort /dev/shm/file | uniq &gt;/dev/null

real        0m0.772s
user        0m1.137s
sys         0m0.273s
</code></pre>

<p>It also doesn't mask the return code of <code>sort</code>, which may be important (in modern shells there are ways to get this, for example, <code>bash</code>'s <code>$PIPESTATUS</code> array, but this wasn't always true).</p>
","76050"
"Centos 7: failed to bring up/down networking: configure interface for a trunk interface","105857","","<p>The switch configured on the server (Centos 7) is configured as trunk for VLAN#115,2014.
I have loaded </p>

<pre><code># lsmod | grep 8021q
# modprobe 8021q
</code></pre>

<p>I would like to configure an IP address on the server using the VLAN#115
Performing the following configuration:</p>

<p><strong>ifcfg-em1</strong></p>

<pre><code>TYPE=Ethernet
BOOTPROTO=none
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=em1
UUID=c0c4d851-d762-4301-8c20-d6128aee5261
DEVICE=em1
ONBOOT=yes
</code></pre>

<p><strong>ifcfg-em1.115</strong></p>

<pre><code>TYPE=Ethernet
BOOTPROTO=none
IPADDR=172.31.141.242
PREFIX=24
GATEWAY=172.31.141.1
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=em1.115
UUID=c0c4d851-d762-4301-8c20-d6128aee5261
DEVICE=em1.115
VLAN=yes
ONBOOT=yes
</code></pre>

<p>I ended up being not able to restart the network service.
The error message appearing is : </p>

<pre><code>Failed to start LSB: Bring up/down networking.
</code></pre>

<p>Could you please advise ? What am doing wrong ?</p>
","<p>it seems that disabling NetworkManager did the trick :)</p>

<pre><code>systemctl stop NetworkManager
systemctl disable NetworkManager
</code></pre>
","220961"
"How to assign a string value to a variable over multiple lines while indented?","105711","","<p>The issue:</p>

<ol>
<li>I need to assign a variable a value that is decently long.</li>
<li>All the lines of my script must be under a certain number of columns.</li>
</ol>

<p>So, I am trying to assign it using more than one line.</p>

<p>It's simple to do without indents:</p>

<pre><code>VAR=""This displays without \
any issues.""
echo ""${VAR}""
</code></pre>

<p>Result:</p>

<pre><code>This displays without any issues.
</code></pre>

<p>However with indents:</p>

<pre><code>    VAR=""This displays with \
    extra spaces.""
    echo ""${VAR}""
</code></pre>

<p>Result:</p>

<pre><code>This displays with      extra spaces.
</code></pre>

<p>How can I elegantly assign it without these spaces?</p>
","<p>Here the issue is that you are surrounding the variable with double quotes (""""). Remove it and things will work fine.</p>

<pre><code>    VAR=""This displays with \
    extra spaces.""
    echo ${VAR}
</code></pre>

<p>Output</p>

<pre><code> This displays with extra spaces.
</code></pre>

<p>Here the issue is that double quoting a variable preserves all white space characters. This can be used in case if you explicitly need it.</p>

<p>For example,</p>

<pre><code>$ echo ""Hello     World    ........ ...            ....""
</code></pre>

<p>will print</p>

<pre><code>Hello     World    ........ ...            ....
</code></pre>

<p>And on removing quotes, its different</p>

<pre><code>$ echo Hello     World    ........ ...            ....
Hello World ........ ... ....
</code></pre>

<p>Here the Bash removes extra spaces in the text because in the first case the entire text is taken as a ""single"" argument and thus preserving extra spaces.
But in the second case <code>echo</code> command receives the text as 5 arguments.</p>

<p>Quoting a variable will also be helpful while passing arguments to commands.</p>

<p>In the below command, <code>echo</code> only gets single argument as <code>""Hello World""</code></p>

<pre><code>$ variable=""Hello World""
$ echo ""$variable""
</code></pre>

<p>But in case of the below scenario <code>echo</code> gets two arguments as <code>Hello</code> and <code>World</code></p>

<pre><code>$ variable=""Hello World""
$ echo $variable
</code></pre>
","163941"
"Can I change root's email address or forward it to an external address?","105652","","<p>I'm getting a lot of mail in my <code>root</code> user's mail account. This appears to be mostly reports and errors from things like <code>cron</code> scripts. I'm trying to work though and solve these things, possibly even have them be piped to some sort of ""dashboard"" - but until then how can I have these messages go to my personal e-mail account instead?</p>
","<p>Any user, including root, can forward their local email by putting the forwarding address in a file called <a href=""http://www.freebsd.org/cgi/man.cgi?query=forward&amp;sektion=5""><code>~/.forward</code></a>. You can have multiple addresses there, all on one line and separated by comma. If you want both local delivery and forwarding, put <code>root@localhost</code> as one of the addresses.</p>

<p>The system administrator can define email aliases in the file <a href=""http://www.freebsd.org/cgi/man.cgi?query=aliases&amp;sektion=5""><code>/etc/aliases</code></a>. This file contains lines like <code>root: cwd@mailhost.example.com, /root/mailbox</code>; the effect is the same as having <code>cwd@mailhost.example.com, /root/mailbox</code> in <code>~root/.forward</code>. You may need to run a program such as <code>newaliases</code> after changing <code>/etc/aliases</code>.</p>

<p>Note that the workings of <code>.forward</code> and <code>/etc/aliases</code> depend on your <a href=""http://en.wikipedia.org/wiki/Message_transfer_agent"">MTA</a>. Most MTAs implement the main features provided by the traditional sendmail, but check your MTA's documentation.</p>
","26670"
"What does ampersand mean at the end of a shell script line?","105559","","<pre><code>sh sys-snap.sh &amp;
</code></pre>

<p>What is <code>sh</code>?
What is <code>sys-snap.sh</code>?
Why I should put <code>&amp;</code> at the end of the line?
Can anyone explain the syntax?</p>

<p>Without the <code>&amp;</code> the script won't go back to the prompt till I press <kbd>Ctrl</kbd>+<kbd>C</kbd>.
With <code>&amp;</code> I can press enter and it works.</p>
","<p><code>sh</code> is the default <a href=""http://en.wikipedia.org/wiki/Bourne_shell"">Bourne-compatible shell</a> (usually bash or dash)</p>

<p><code>sys-snap.sh</code> is a shell script, which contains commands that <code>sh</code> executes.
As you do not post its content, I can only guess from its name, what it does.
I can find a script related to CPanel with the same file name, that make a log file with all current processes, current memory usage, database status etc.
If the script starts with a <a href=""http://en.wikipedia.org/wiki/Shebang_%28Unix%29"">shebang line</a> (<code>#!/bin/sh</code> or similar), you can make it executable with <code>chmod +x sys-snap.sh</code> and start it directly by using <code>./sys-snap.sh</code> if it is in the current directory.</p>

<p>With <code>&amp;</code> the process starts in the background, so you can continue to use the shell and do not have to wait until the script is finished.
If you forget it, you can stop the current running process with <code>Ctrl-Z</code> and continue it in the background with  <code>bg</code> (or in the foreground with <code>fg</code>).
For more information, see job control</p>
","86249"
"How can I prevent 'grep' from showing up in ps results?","105482","","<p>When I  search for some process that doesn't exist, e.g.</p>

<pre><code>$ ps aux | grep fnord                          
wayne    15745  0.0  0.0  13580   928 pts/6    S+   03:58   0:00 grep fnord
</code></pre>

<p>Obviously I don't care about grep - that makes as much sense as searching for the <code>ps</code> process!</p>

<p>How can I prevent grep from showing up in the results?</p>
","<p>Turns out there's a solution found in <a href=""http://www.ibm.com/developerworks/library/l-keyc3/#code10"">keychain</a>.</p>

<pre><code>$ ps aux | grep ""[f]nord""
</code></pre>

<p>By putting the brackets around the letter and quotes around the string you search for the regex, which says, ""Find the character 'f' followed by 'nord'.""</p>

<p>But since you put the brackets in the pattern 'f' is now followed by ']', so <code>grep</code> won't show up in the results list. Neato!</p>
","74186"
"How to install tarball packages on a Debian based distribution?","105239","","<p>I would like to know how to install .tar.bz and .tar.bz2 packages on Debian, please give me a complete explanation.</p>
","<p>Firstly, according to the <a href=""http://www.pathname.com/fhs/pub/fhs-2.3.html"" rel=""nofollow noreferrer"">File System Hierarchy Standards</a>, the location of this installed package should be <code>/opt</code> if it is a binary install and <code>/usr/local</code> if it's a from source install.</p>

<h2>Pure binaries</h2>

<p>These are ready to use binaries. Normally they just need to be extracted to be installed. A binary package is going to be easy:</p>

<ul>
<li><code>sudo tar --directory=/opt -xvf &lt;file&gt;.tar.[bz2|gz]</code></li>
<li>add the directory to your path: <code>export PATH=$PATH:/opt/[package_name]/bin</code> </li>
</ul>

<p>and you are done.</p>

<h2>From sources</h2>

<p>A source package is going to be more troublesome (by far) and through they can roughly be processed with the method below, <strong>each package is different</strong>:</p>

<ul>
<li>download the package to <code>/usr/local/src</code></li>
<li><code>tar xf &lt;file&gt;.tar.[bz2|gz]</code></li>
<li><code>cd &lt;package name&gt;</code></li>
<li>read the <code>README</code> file (this almost certainly exists).</li>
<li>most Open Source projects use autoconf/automake, the instructions should be in the <code>README</code>. Probably this step will go: <code>./configure &amp;&amp; make &amp;&amp; make install</code> (run the commands separately for sanity if something goes wrong though).</li>
</ul>

<p><strong>If there's any problems in the install then you'll have to ask specific questions.</strong> You might have problems of incorrect versions of libraries or missing dependencies. There's a reason that Debian packages everything up for you. And there is a reason Debian stable runs old packages - finding all the corner cases of installing packages on more than a dozen different architectures and countless different hardware/systems configurations is difficult. When you install something on your own you might run into one of these problems!</p>
","53618"
"Is there a command to list all open displays on a machine?","105231","","<p>When SSH'd locally into my computer (don't ask, it's a workaround), I can't start graphical applications without running:</p>

<pre><code>export DISPLAY=:0.0
</code></pre>

<p>If I run this first and then run a graphical application, things work out. If not, it doesn't work, there's no display to attach to. </p>

<p>Is there a command for listing all available displays (ie: all possible values) on a machine? </p>
","<p>If you want the X connection forwarded over SSH, you need to enable it on both the server side and the client side. (Depending on the distribution, it may be enabled or disabled by default.) On the server side, make sure that you have <code>X11Forwarding yes</code> in <a href=""http://www.freebsd.org/cgi/man.cgi?query=sshd_config&amp;sektion=5"" rel=""noreferrer""><code>/etc/sshd_config</code></a> (or <code>/etc/ssh/sshd_config</code> or wherever the configuration file is). On the client side, pass the <code>-X</code> option to the <a href=""http://www.freebsd.org/cgi/man.cgi?query=ssh&amp;sektion=1"" rel=""noreferrer""><code>ssh</code> command</a>, or put <code>ForwardX11</code> in your <a href=""http://www.freebsd.org/cgi/man.cgi?query=ssh_config&amp;sektion=5"" rel=""noreferrer""><code>~/.ssh/config</code></a>.</p>

<p>If you run <code>ssh -X localhost</code>, you should see that <code>$DISPLAY</code> is (probably) <code>localhost:10.0</code>. Contrast with <code>:0.0</code>, which is the value when you're not connected over SSH. (The <code>.0</code> part may be omitted; it's a screen number, but multiple screens are rarely used.) There are two forms of X displays that you're likely to ever encounter:</p>

<ul>
<li>Local displays, with nothing before the <code>:</code>.</li>
<li>TCP displays, with a host name before the <code>:</code>.</li>
</ul>

<p>With <code>ssh -X localhost</code>, you can access the X server through both displays, but the applications will use a different method: <code>:NUMBER</code> accesses the server via local sockets and shared memory, whereas <code>HOSTNAME:NUMBER</code> accesses the server over TCP, which is slower and disables some extensions.</p>

<p>Note that you need a form of authorization to access an X server, called a cookie and normally stored behind the scenes in the file <code>~/.Xauthority</code>. If you're using ssh to access a different user account, or if your distribution puts the cookies in a different file, you may find that <code>DISPLAY=:0</code> doesn't work within the SSH session (but <code>ssh -X</code> will, if it's enabled in the server; you never need to mess with <code>XAUTHORITY</code> when doing <code>ssh -X</code>). If that's a problem, you need to <a href=""https://unix.stackexchange.com/questions/10121/ssh-display-variable/10126#10126"">set the <code>XAUTHORITY</code> environment variable</a> or <a href=""https://unix.stackexchange.com/questions/1596/can-i-launch-a-graphical-program-on-another-users-desktop-as-root/1600#1600"">obtain the other user's cookies</a>.</p>

<p>To answer your actual question:</p>

<ul>
<li><p>Local displays correspond to a socket in <code>/tmp/.X11-unix</code>.</p>

<pre><code>ls /tmp/.X11-unix
cd /tmp/.X11-unix &amp;&amp; for x in X*; do echo "":${x#X}""; done
</code></pre></li>
<li><p>Remote displays correspond to open TCP ports above 6000; accessing display number N on machine M is done by connecting to TCP port 6000+N on machine M. From machine M itself:</p>

<pre><code>netstat -lnt
netstat -lnt | awk '
  sub(/.*:/,"""",$4) &amp;&amp; $4 &gt;= 6000 &amp;&amp; $4 &lt; 6100 {
    print ($1 == ""tcp6"" ? ""ip6-localhost:"" : ""localhost:"") ($4 - 6000)
  }'
</code></pre>

<p>(The rest of this bullet point is of academic interest only.)</p>

<p>From another machine, you can use <code>nmap -p 6000-6099 host_name</code> to probe open TCP ports in the usual range. It's rare nowadays to have X servers listening on a TCP socket, especially outside the loopback interface.</p>

<p>Strictly speaking, another application could be using a port in the range usually used by X servers. You can tell whether an X server is listening by checking which program has the port open.</p>

<pre><code>lsof -i -n | awk '$9 ~ /:60[0-9][0-9]$/ {print}'
</code></pre>

<p>If that shows something ambiguous like <code>sshd</code>, there's no way to know for sure whether it's an X server or a coincidence.</p></li>
</ul>
","17278"
"Running application ends with ""Segmentation Fault""","104824","","<p>I have a command line application that when run does not do what it is supposed to do and at a certain point leaves the message:</p>

<pre><code>Segmentation fault
</code></pre>

<p>What does this mean?  What should I do?</p>
","<p>A <a href=""http://en.wikipedia.org/wiki/Segmentation_fault"">segmentation fault</a> is the result of a memory access violation.  The program has referred to a memory address outside of what was allocated to it, and the OS kernel responds by killing the program with SIGSEGV.</p>

<p>This is a mistake, since there is no point in trying to access inaccessible memory (it cannot be done).  Mistakes of this sort are easy to make, however, particularly in languages such as C and C++ (which account for a lot of common applications).  It indicates a bug in either the program itself or a library it links to.  If you wish to report the bug (do -- this helps), it is a good idea to include a <em>backtrace</em> of the events that led up to the seg fault.</p>

<p>To do this, you can run the program inside <code>gdb</code> (the GNU debugger), which should be available from any linux distro if it is not installed already (the package will just be called ""gdb"").  If the broken application is called ""brokenapp"":</p>

<pre><code>gdb brokenapp
</code></pre>

<p>A paragraph about copyright and licensing will appear, and at the end a prompt with the cursor:</p>

<pre><code>(gdb) _ 
</code></pre>

<p>Type <code>run</code> and hit enter.  If you need to supply arguments (e.g. <code>-x --foo=bar whatever</code>) append those (<code>run -x --foo=bar whatever</code>).  The program will do what it does, you will see the output and if you need to interact you can (note you can run any sort of program, including a GUI one, inside gdb).  At the point where it usually segfaults you will see:</p>

<pre><code>Program received signal SIGSEGV, Segmentation fault.
0x00000000006031c9 in ?? ()
(gdb) _
</code></pre>

<p>The second line of output here is just an example.  Now type <code>bt</code> (for ""backtrace"") and hit enter.  You'll see something like this, although it may be much longer:</p>

<pre><code>(gdb) bt
#0  0x00000000006031c9 in ?? ()
#1  0x000000000040157f in mishap::what() const ()
#2  0x0000000000401377 in main ()
</code></pre>

<p>If it is longer, you'll only get a screenful at a time and there will be a <code>--More--</code> message.  Keep hitting enter until it's done. You can now <code>quit</code>, the output will remain in your terminal.  Copy everything from <code>Program received signal SIGSEGV</code> onward into a text file, and file a bug report with the application's bug tracker; you can find these online by searching, e.g. ""brokenapp bug report"" -- you will probably have to register so a reply can be sent to you by email.  Include your description of the problem, any arguments you supplied to <code>run</code>, etc., and a copy of the backtrace (if it is very long, there may be a means to attach a text file in the bug tracker interface). Also include the version, if you know what it is (<code>brokenapp --version</code> may work, or the man page may indicate how to get this), and which distribution you are using.</p>

<p>Someone will hopefully get back to you in not too long.  Filing bugs is a usually appreciated.</p>
","132193"
"How to fill 90% of the free memory?","104811","","<p>I want to do some <a href=""http://www.allthingsquality.com/p/testing-terms-glossary.html"">low-resources testing</a> and for that I need to have 90% of the free memory full.</p>

<p>How can I do this on a <code>*nix</code> system?</p>
","<p><a href=""http://people.seas.harvard.edu/~apw/stress/"">stress</a> is a workload generator that simulates cpu/mem/io/hdd stress on POSIX systems.  This call should do the trick on Linux:</p>

<pre><code>stress --vm-bytes $(awk '/MemFree/{printf ""%d\n"", $2 * 0.9;}' &lt; /proc/meminfo)k --vm-keep -m 1
</code></pre>

<p>Adapt the <code>/proc/meminfo</code> call with <code>free(1)</code>/<code>vm_stat(1)</code>/etc. if you need it portable.</p>
","99435"
"How to set the permission drwxr-xr-x to other folders?","104808","","<p>I have few directores inside a folder like below - </p>

<pre><code>teckapp@machineA:/opt/keeper$ ls -ltrh
total 8.0K
drwxr-xr-x 10 teckapp cloudmgr 4.0K Feb  9 10:22 keeper-3.4.6
drwxr-xr-x  3 teckapp cloudmgr   4.0K Feb 12 01:44 data
</code></pre>

<p>I have some other folder as well in some other machines for which I need to change the permission to the above one like this <code>drwxr-xr-x</code>.</p>

<p>Meaning how can I change any folder permissions to <code>drwxr-xr-x</code>? I know I need to use <code>chmod</code> command with this but what should be the value with chown that I should use for this?</p>
","<p>To apply those permissions to a directory:</p>

<p><code>chmod 755 directory_name</code></p>

<p>To apply to all directories inside the current directory:</p>

<p><code>chmod 755 */</code></p>

<p>If you want to modify all directories and subdirectories, you'll need to combine <em>find</em> with <em>chmod</em>:</p>

<p><code>find . -type d -exec chmod 755 {} +</code></p>
","184426"
"Using ip addr instead of ifconfig reports ""RTNETLINK answers: File exists"" on Debian","104708","","<p>I have a Debian system working as a wireless router with <code>eth0</code> and <code>wlan0</code>. Now I added an additional network manually on <code>eth1</code> with <code>ifconfig</code>:</p>

<pre>
alix:~# ifconfig eth1 192.168.0.2 netmask 255.255.255.0
alix:~# netstat -rn
Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
0.0.0.0         192.168.2.1     0.0.0.0         UG        0 0          0 eth0
192.168.0.0     0.0.0.0         255.255.255.0   U         0 0          0 eth1
192.168.2.0     0.0.0.0         255.255.255.0   U         0 0          0 eth0
192.168.3.0     0.0.0.0         255.255.255.0   U         0 0          0 wlan0
alix:~# ping 192.168.0.254
PING 192.168.0.254 (192.168.0.254) 56(84) bytes of data.
64 bytes from 192.168.0.254: icmp_req=1 ttl=64 time=0.537 ms
64 bytes from 192.168.0.254: icmp_req=2 ttl=64 time=0.199 ms
64 bytes from 192.168.0.254: icmp_req=3 ttl=64 time=0.188 ms
^C
--- 192.168.0.254 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2005ms
rtt min/avg/max/mdev = 0.188/0.308/0.537/0.161 ms
</pre>

<p>Everything works fine as you can see.</p>

<p>Now I would like to make the configuration permanent. Therefor I added the following section to <code>/etc/network/interfaces</code>:</p>

<pre>
alix:~# sed -n '/iface eth1/,/^$/p' /etc/network/interfaces
iface eth1 inet static
  address 192.168.0.2
  netmask 255.255.255.0
</pre>

<p>But when I try to start the network I get the following error:</p>

<pre>
alix:~# ifconfig eth1 down
alix:~# ifup -v eth1
Configuring interface eth1=eth1 (inet)
run-parts --verbose /etc/network/if-pre-up.d
run-parts: executing /etc/network/if-pre-up.d/hostapd
ip addr add 192.168.0.2/255.255.255.0 broadcast 192.168.0.255     dev eth1 label eth1
RTNETLINK answers: File exists
Failed to bring up eth1.
</pre>

<p>When I run the <code>ip</code> command manually I get the same error:</p>

<pre>
alix:~# ip addr add 192.168.0.2/255.255.255.0 broadcast 192.168.0.255     dev eth1 label eth1
RTNETLINK answers: File exists
</pre>

<p>What is wrong with the command? And how can I tell Debian to do the right thing?</p>
","<p>Sorry but I got it myself. I had to flush the device before bringing it up:</p>

<pre>
alix:~# ip addr flush dev eth1
</pre>

<p>Clearing manually set interface configuration information like this is <a href=""https://help.ubuntu.com/lts/serverguide/network-configuration.html#ip-addressing"" rel=""nofollow noreferrer"">mentioned in the <em>Ubuntu Server Guide</em></a>.</p>
","100593"
"Replace multiple spaces with one using 'tr' only","104627","","<p>I have a file, <code>f1.txt</code>:</p>

<pre><code>ID     Name
1      a
2         b
3   g
6            f
</code></pre>

<p>The number of spaces is <em>not</em> fixed. What is the best way to replace all the white spaces with one space using only <code>tr</code>?</p>

<p>This is what I have so far:</p>

<pre><code>cat f1.txt | tr -d "" ""
</code></pre>

<p>But the output is:</p>

<pre><code>IDName
1a
2b
3g
6f
</code></pre>

<p>But I want it to look like this:</p>

<pre><code>ID Name
1 a
2 b
3 g
6 f
</code></pre>

<p>Please try and avoid <code>sed</code>.</p>
","<p>With <code>tr</code>, use the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/tr.html#tag_20_132_04""><code>s</code>queeze</a> repeat option:</p>

<pre><code>$ tr -s "" "" &lt; file
ID Name
1 a
2 b
3 g
6 f
</code></pre>

<p>Or you can use an <code>awk</code> solution:</p>

<pre><code>$ awk '{$2=$2};1' file
ID Name
1 a
2 b
3 g
6 f
</code></pre>

<p>When you change a field in record, <code>awk</code> rebuild <code>$0</code>, takes all field and concat them together, separated by <code>OFS</code>, which is a space by default.</p>

<p>That will squeeze sequences of space and tabs (and possibly other blank characters depending on the locale and implementation of <code>awk</code>) into one space, but also remove the leading and trailing blanks off each line.</p>
","145981"
"How to add arithmetic variables in a script","104580","","<p>I want to accumulate the line size of a number of files contained in a folder. I have written the following script:</p>

<pre><code>let a=0
let num=0
for i in folder/*
do
        num=`cat $i | wc -l`
        a=$a+$num
done
echo $a
</code></pre>

<p>What i am geting at the end of the script is 123+234+432+... and not the result of the arithmetic operation of addition.</p>
","<p>Your arithmetic evaluation syntax is wrong. Use any of the following (the first is extremely portable but slow, the second is POSIX and portable except to the Bourne shell and earlier versions of the Almquist shell, the last three require <code>ksh</code>, <code>bash</code> or <code>zsh</code>):</p>

<pre><code>a=`expr ""$a"" + ""$num""`

a=$(($a+$num))

((a=a+num))

let a=a+num

((a+=num))
</code></pre>

<p>Or you can just skip the entire <code>for</code> loop and just do:</p>

<pre><code>wc -l folder/*
</code></pre>

<p>Or, if you only want the total:</p>

<pre><code>cat folder/* | wc -l
</code></pre>

<p>Or with zsh and its <code>mult_ios</code> option:</p>

<pre><code>wc -l &lt; folder/*
</code></pre>
","55071"
"How can I delete all files with a particular extension in a particular folder?","104392","","<p>If I set the current/working directory (navigating to it using <code>cd</code>) to some particular directory and then type:</p>

<pre><code>rm *.xvg
</code></pre>

<p>What will this command do?  Is it true that the above command will <strong>only</strong> delete files with the extension <code>.xvg</code> <strong>only</strong> in the working directory?</p>

<p>I was nervous about trying this before asking, because I want to be absolutely sure that the above command will only delete <code>.xvg</code> files <strong><em>LOCATED IN THE WORKING DIRECTORY</em></strong>.</p>
","<p>Yes, <code>rm *.xvg</code> will <em>only</em> delete the files with the specified extension in your <em>current</em> directory.</p>

<p>A <em>good</em> way to make sure you are indeed in the directory you want delete your files is to use the <code>pwd</code> command which will display your current directory and then do an <code>ls</code> to verify you find the files you are expecting.</p>

<p>If you are bit apprehensive about issuing the <code>rm</code> command, there are 2 things you can do:</p>

<ol>
<li><p>type <code>ls *.xvg</code> to see a list of what files would be affected by this command.</p></li>
<li><p>Unless you have a <em>lot</em> of files, you could always also use the <code>-i</code> command line switch for <code>rm</code> (also exists for <code>cp</code> and <code>mv</code>). Using <code>rm -i *.xvg</code> would prompt you for each individual file if it was ok to delete it, so you could be sure nothing you didn't expect was getting deleted. (This will be tedious if you have a lot of files though :)</p></li>
</ol>
","42021"
"List ports a process PID is listening on (preferably using iproute2 tools)?","104019","","<p>I'm looking to list all ports a PID is currently listening on. </p>

<p>How would you recommend I get this kind of data about a process?</p>
","<p>You can use <code>ss</code> from the iproute2 package (which is similar to <code>netstat</code>):</p>

<pre><code>ss -l -p -n | grep "",1234,""
</code></pre>

<p>Replace 1234 with the PID of the program.</p>
","157826"
"Checking if HyperThreading is enabled or not?","103815","","<p>How can I check if hyperthreading is enabled on a Linux machine, using a perl script to check for it?</p>

<p>I'm trying the following way:</p>

<pre><code>dmidecode -t processor | grep HTT
</code></pre>

<p>Let me know if I'm on right track.</p>
","<p><em>Notes added on July 8, 2014:</em> As <a href=""https://unix.stackexchange.com/users/274/riccardo-murri"">Riccardo Murri</a> pointed out, my answer below only shows whether the processor reports to support hyperthreading. Generally, *nix O/S are configured to enable hyperthreading if supported. However, to actually check this programmatically see for instance <a href=""https://unix.stackexchange.com/a/33509/13746"">Nils' answer</a>!</p>

<p>---- Original answer from March 25, 2012:</p>

<p>You are indeed on the right track :) with</p>

<pre><code>dmidecode -t processor | grep HTT
</code></pre>

<p>On Linux, I generally just look for ""ht"" on the ""flags"" line of <code>/proc/cpuinfo</code>. See for instance</p>

<pre><code>grep '^flags\b' /proc/cpuinfo | tail -1
</code></pre>

<p>or if you want to include the ""ht"" in the pattern</p>

<pre><code>grep -o '^flags\b.*: .*\bht\b' /proc/cpuinfo | tail -1
</code></pre>

<p>(<code>\b</code> matches the word boundaries and helps avoid false positives in cases where ""ht"" is part of another flag.)</p>
","33454"
"bash - replace space with new line","103717","","<p>How can I replace spaces with new lines on an input like:</p>

<p><code>/path/to/file /path/to/file2 /path/to/file3 /path/to/file4 /path/to/file5</code> etc...</p>

<p>To obtain the following:</p>

<pre><code>/path/to/file
/path/to/file2
/path/to/file3
/path/to/file4
/path/to/file5
</code></pre>

<h3>Note</h3>

<p>I'm posting this question to help other users, it was not easy to find a useful answer on UNIX SE until I started to type this question. After that I found the following:</p>

<h3>Related question</h3>

<p><a href=""https://unix.stackexchange.com/questions/9647/how-can-i-find-and-replace-with-a-new-line"">How can I find and replace with a new line?</a></p>
","<p><strong>Use the <code>tr</code> command</strong></p>

<pre><code>echo ""/path/to/file /path/to/file2 /path/to/file3 /path/to/file4 /path/to/file5""\
| tr "" "" ""\n""
</code></pre>

<p>Found on <a href=""http://www.unix.com/shell-programming-scripting/67831-replace-space-new-line.html"">http://www.unix.com/shell-programming-scripting/67831-replace-space-new-line.html</a></p>
","105570"
"Is there a way to get the min, max, median, and average of a list of numbers in a single command?","103580","","<p>I have a list of numbers in a file, one per line.  How can I get the minimum, maximum, <em>median</em> and <em>average</em> values?  I want to use the results in a bash script.  </p>

<p>Although my immediate situation is for integers, a solution for floating-point numbers would be useful down the line, but a simple integer method is fine.</p>
","<p>You can use the <a href=""http://en.wikipedia.org/wiki/R_%28programming_language%29"">R programming language</a>.</p>

<p>Here is a quick and dirty R script:</p>

<pre><code>#! /usr/bin/env Rscript
d&lt;-scan(""stdin"", quiet=TRUE)
cat(min(d), max(d), median(d), mean(d), sep=""\n"")
</code></pre>

<p>Note the <code>""stdin""</code> in <code>scan</code> which is a special filename to read from standard input (that means from pipes or redirections).</p>

<p>Now you can redirect your data over stdin to the R script:</p>

<pre><code>$ cat datafile
1
2
4
$ ./mmmm.r &lt; datafile
1
4
2
2.333333
</code></pre>

<p>Also works for floating points:</p>

<pre><code>$ cat datafile2
1.1
2.2
4.4
$ ./mmmm.r &lt; datafile2
1.1
4.4
2.2
2.566667
</code></pre>

<p>If you don't want to write an R script file you can invoke a true one-liner (with linebreak only for readability) in the command line using <code>Rscript</code>:</p>

<pre><code>$ Rscript -e 'd&lt;-scan(""stdin"", quiet=TRUE)' \
          -e 'cat(min(d), max(d), median(d), mean(d), sep=""\n"")' &lt; datafile
1
4
2
2.333333
</code></pre>

<p>Read the fine R manuals at <a href=""http://cran.r-project.org/manuals.html"">http://cran.r-project.org/manuals.html</a>.</p>

<p>Unfortunately the full reference is only available in PDF. Another way to read the reference is by typing <code>?topicname</code> in the prompt of an interactive R session.</p>

<hr>

<p>For completeness: there is an R command which outputs all the values you want and more. Unfortunately in a human friendly format which is hard to parse programmatically.</p>

<pre><code>&gt; summary(c(1,2,4))
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.000   1.500   2.000   2.333   3.000   4.000 
</code></pre>
","13775"
"How to add a repository on Fedora?","103552","","<p>With one repository I did it like this (<a href=""https://unix.stackexchange.com/questions/3063/how-do-i-run-a-command-as-the-system-administrator-root"">as root</a>):</p>

<pre><code># cd /etc/yum.repos
# wget https://some.repo.example.org/foo/bar/Fedora_14/foo_bar.repo
# grep enabled foo_bar.repo
enabled=1
</code></pre>

<p>Is this the recommended way to add a package repository under Fedora (>= 14)?</p>

<p>Is there some policy/standard which specifies that each proper Fedora package repository should (or must) contain such a config file (i.e. such a <code>.repo</code> file)?</p>

<p>(basically just for the reason that a user or some tool can copy it to the local <code>/etc/yum.repos</code> directory?)</p>
","<p>I googled a bit around with 'fedora add repository' and got some outdated and not very helpful links. Because of the noise I missed this link:</p>

<p><a href=""http://docs.fedoraproject.org/en-US/Fedora_Core/3/html/Software_Management_Guide/sn-using-repositories.html"">http://docs.fedoraproject.org/en-US/Fedora_Core/3/html/Software_Management_Guide/sn-using-repositories.html</a></p>

<p>Which is kind of outdated as well, but it gives me the hint to look for an updated version of the software management guide:</p>

<p><a href=""http://docs.fedoraproject.org/en-US/Fedora/14/html/Software_Management_Guide/ch04s08.html#id865515"">Add New Repositories</a></p>

<p>(which also mentions the wget method I used to add a .repo file)</p>

<p>I am a bit surprised that the official and as it seems quite extensive fedora documentation is not higher scored in the google results.</p>
","6829"
"What causes this green background in ls output?","102957","","<p><img src=""https://i.stack.imgur.com/rQomR.png"" alt=""screencap of ls output on linux machine""></p>

<p>There are two directories shown by 'ls'.  Normally directories anywhere are blue on black background.  But the first one is blue on green and impossible to read.  Why is this?  How to make it blue on black, or at least something light on something dark?</p>

<p>This is on Ubuntu 12.04, using bash in Gnome Terminal.  In Konsole, the blue is slightly darker, and possible to read, though could be way better.</p>
","<p>Apart from coloring files based on their type (turquoise for audio files, bright red for Archives and compressed files, and purple for images and videos), <code>ls</code> also colors files and directories based on their attributes:</p>

<ul>
<li>Black text with green background indicates that a directory is writable by others apart from the owning user and group, and has the sticky bit set (<code>o+w, +t</code>).</li>
<li>Blue text with green background indicates that a directory is writable by others apart from the owning user and group, and has <em>not</em> the sticky bit set (<code>o+w, -t</code>).</li>
</ul>

<p><a href=""https://askubuntu.com/questions/17299/what-do-the-different-colors-mean-in-the-terminal"" title=""https://askubuntu.com/questions/17299/what-do-the-different-colors-mean-in-the-terminal"">Stephano Palazzo</a> over at <a href=""http://askubuntu.com"" title=""Ask Ubuntu"">Ask Ubuntu</a> has made this very instructive picture over the different attribute colors:</p>

<p><img src=""https://i.stack.imgur.com/gnT32.png"" alt=""What the different colors mean in the terminal""></p>

<p>As <a href=""https://unix.stackexchange.com/a/94508/43779"" title=""What causes this green background in ls output?"">terdon</a> pointed out, the color settings can be modified via <a href=""http://linux.die.net/man/1/dircolors"" rel=""noreferrer"" title=""Manual page of dircolors""><code>dircolors</code></a>. A list of the different coloring settings can be accessed with <code>dircolors --print-database</code>.</p>

<p>Each line of output, such as <code>BLK 40;33;01</code>, is of the form:</p>

<pre><code>[TARGET] [TEXT_STYLE];[FOREGROUND_COLOR];[BACKGROUND_COLOR]
</code></pre>

<ul>
<li><p><code>TARGET</code> indicates the target for the coloring rule</p></li>
<li><p><code>TEXT_STYLE</code> indicates the text style:</p>

<ul>
<li><code>00</code> = none</li>
<li><code>01</code> = bold</li>
<li><code>04</code> = underscore</li>
<li><code>05</code> = blink</li>
<li><code>07</code> = reverse, </li>
<li><code>08</code> = concealed</li>
</ul></li>
<li><p><code>FOREGROUND_COLOR</code> indicates the foreground color:</p>

<ul>
<li><code>30</code> = black</li>
<li><code>31</code> = red</li>
<li><code>32</code> = green</li>
<li><code>33</code> = yellow</li>
<li><code>34</code> = blue,</li>
<li><code>35</code> = magenta</li>
<li><code>36</code> = cyan</li>
<li><code>37</code> = white</li>
</ul></li>
<li><p><code>BACKGROUND_COLOR</code> indicates the background colors, the color codes are the same as for the foreground</p></li>
</ul>

<p>Fields may be omitted starting from the right, so for instance <code>.tar 01;31</code> means bold and red.</p>
","94505"
"How to change to normal user in the command line when logged in as the root user?","102760","","<p>I log into an system as root via ssh.</p>

<p>How do I become the normal user or another user in the command-line?</p>
","<p><a href=""http://www.penguintutor.com/linux/useradmin-reference"" rel=""nofollow"">penguintutor.com/linux/useradmin-reference</a></p>

<p>Excerpt:</p>

<blockquote>
  <p><code>su</code> (Switch User)</p>
  
  <p>One of the features of Linux is the ability to change userid when
  logged into a system. 
  <strong>This command su is sometimes referred to as
  superuser</strong>
  , however this is not completely correct. In the early days
  of UNIX it was only possible to change to the <strong>root user</strong>, which made
  for the superuser command however it is now possible to 
  <strong>change to any
  user using the su command</strong>. <strong>It is more correct to refer to the command as the switch user command</strong>.</p>
  
  <p>The switch user command <code>su</code> <strong>is used to change between different
  users on a system, without having to logout</strong>. The most common use is
  to to change to the root user, but it can be used to switch to any
  user depending upon the users settings. To switch to a different user
  other than root, then the username is used as the last option on the
  command.</p>
  
  <p>It is also possible to change to another user by putting the username
  after the <code>su</code> command. There are two ways of switching users. By
  putting a '<strong>-</strong>' after the command will cause the users profile to be
  read and variables to be set. Without the '<strong>-</strong>' the previous users
  settings will still remain.</p>
  
  <p>To use the new users profile and variables</p>

<pre><code>su - username
</code></pre>
  
  <p>To continue with the current profile and variables</p>

<pre><code>su username
</code></pre>
  
  <p>you can then return to the previous user by entering <code>exit</code>.</p>
</blockquote>
","157974"
"How can I enable access to USB devices within VirtualBox guests?","102756","","<p>I cannot see any USB devices within my VirtualBox guest VMs from my host. How do I enable access for my guest VMs?</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/j71NC.png"" alt=""ss of missing USB devices""></p>
","<p>In order to enable access to these devices you'll need to add your username to the group <code>vboxusers</code>.</p>

<pre><code>$ sudo usermod -a -G vboxusers &lt;username&gt;
</code></pre>

<h3>Example</h3>

<pre><code>$ sudo usermod -a -G vboxusers saml
</code></pre>

<p>You can confirm the change afterwards:</p>

<pre><code>$ groups saml
saml : saml wheel vboxusers wireshark
</code></pre>

<p>After doing the above you'll want to logout and log back in, so that for the newly added group to get picked up by your user account. Then from the VirtualBox GUI you'll be able to right click on the USB icon in the lower right group of icons, and select whatever USB devices you want to give control over to your running guest VM.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/95DaM.png"" alt=""ss of VB""></p>

<h3>Detecting USB devices</h3>

<p>You can use VirtualBox's little known command line tool VBoxManage to list out the USB devices that are accessible. This is a good way to also confirm that the group addition made above to your username are being picked up correctly.</p>

<h3>Example</h3>

<p><em>without group</em></p>

<pre><code>$ VBoxManage list usbhost
Host USB Devices:

&lt;none&gt;
</code></pre>

<p><em>with group</em></p>

<pre><code>$ VBoxManage list usbhost | head -19
Host USB Devices:

UUID:               abcd1234-123a-2345-b1e0-8a0b1c1f2511
VendorId:           0x046d (046D)
ProductId:          0x0809 (0809)
Revision:           0.9 (0009)
SerialNumber:       ABC34567
Address:            sysfs:/sys/devices/pci0000:00/0000:00:12.2/usb1/1-4//device:/dev/vboxusb/001/004
Current State:      Busy

UUID:               d2abc46d-123-1234-b8c3-691a7ca551ce
VendorId:           0x046d (046D)
ProductId:          0xc504 (C504)
Revision:           19.16 (1916)
Manufacturer:       Logitech
Product:            USB Receiver
Address:            sysfs:/sys/devices/pci0000:00/0000:00:12.0/usb3/3-3//device:/dev/vboxusb/003/003
Current State:      Busy
...
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""http://blog.vodkamelone.de/archives/160-VirtualBox-USB-support-on-Fedora.-The-right-way..html"" rel=""noreferrer"">VirtualBox USB support on Fedora. The right way.</a></li>
<li><a href=""https://help.ubuntu.com/community/VirtualBox/USB"" rel=""noreferrer"">Set up USB for Virtualbox</a></li>
<li><a href=""https://www.virtualbox.org/manual/ch03.html#idp54850224"" rel=""noreferrer"">3.10.1. USB settings - VirtualBox documentation</a></li>
</ul>
","129306"
"last time file opened","102625","","<p>Is it possible to get the time when file was opened last time and sort all files in a directory by those times?</p>
","<p>This depends on exactly what you mean by ""opened"", but in general, yes. There are three timestamps normally recorded:</p>

<ul>
<li><code>mtime</code> — updated when the file contents change. This is the ""default"" file time in most cases.</li>
<li><code>ctime</code> — updated when the file <em>or</em> its metadata (owner, permissions) change</li>
<li><code>atime</code> — updated when the file is read</li>
</ul>

<p>So, generally, what you want to see is the <code>atime</code> of a file. You can get that with <code>stat</code> or with <code>ls</code>. You can use <code>ls -lu</code> to do this, although I prefer to use <code>ls -l --time=atime</code> (which should be supported in almost all modern Linux distributions) because I don't use it often, and when I do I can remember it better. And to <em>sort</em> by time, add the <code>-t</code> flag to ls. So there you go.</p>

<p>There is a big caveat, though. Updating the atime every time a file is read causes a lot of usually-unnecessary IO, slowing everything down. So, most Linux distributions now default to the <code>noatime</code> filesystem mount option, which basically kills atimes, or else <code>relatime</code>, which only updates atimes once a limit has passed (normally once per day) or if the file was actually modified since the previous read. You can find if these options are active by running the <code>mount</code> command.</p>

<p>Also, note that access times are by inode, not by filename, so if you have hardlinks, reading from one will update all names that refer to the same file.</p>

<p>And, be aware that <em>c</em> is not ""creation""; creation isn't tracked by Unix/Linux filesystems, which seems strange but actually makes sense because the filesystem has no way of knowing if it is the original — maybe the file was created forty years ago and copied here. And, in fact, many file editors work by making copies over the original. If you need that information, it's best to use a version control system like <code>git</code>.</p>
","8842"
"Set a network range in the no_proxy environment variable","102599","","<p>I'm in a network using a proxy. I've got machines using lots of scripts here and there accessing each other over HTTP.</p>

<ul>
<li>The network is 10.0.0.0/8.</li>
<li><p>My proxy is 10.1.1.1:81, so I set it up accordingly:</p>

<pre><code>export http_proxy=http://10.1.1.1:81/
</code></pre></li>
<li><p>I want to exclude my own range to be accessed with the proxy. I tried any combination available.</p>

<pre><code>export no_proxy='10.*'
export no_proxy='10.*.*.*'
export no_proxy='10.0.0.0/8'
</code></pre></li>
</ul>

<p>None of the above work!</p>

<p>I'm testing with <code>wget</code> and it always tries to query the proxy, whatever IP address I want to connect to.</p>

<ul>
<li>Since lots of scripts lie everywhere in all systems the <code>--no-proxy</code> option is actually not an option. I want to set it system wide.</li>
</ul>
","<p>You're looking at it the wrong way. The <code>no_proxy</code> environment variable lists the domain suffixes, not the prefixes. From <a href=""http://www.gnu.org/software/wget/manual/html_node/Proxies.html"">the documentation</a>:</p>

<blockquote>
  <p><code>no_proxy</code>: This variable should contain a comma-separated list of domain extensions proxy should <em>not</em> be used for.</p>
</blockquote>

<p>So for IPs, you have two options:</p>

<p>1) Add each IP in full:</p>

<pre><code>printf -v no_proxy '%s,' 10.1.{1..255}.{1..255};
export no_proxy=""${no_proxy%,}"";
</code></pre>

<p>2) Rename <code>wget</code> to <code>wget-original</code> and write a wrapper script (called <code>wget</code>) that looks up the IP for the given URL's host, and determines if it should use the proxy or not:</p>

<pre><code>#!/bin/bash
ip='';
for arg; do
   # parse arg; if it's a URL, determine the IP address
done;
if [[ ""$ip"" =~ ^10\.1\. ]]; then
   wget-original --no-proxy ""$@"";
else
   wget-original ""$@"";
fi;
</code></pre>
","23478"
"Difference between ls -l and ll?","101860","","<p>I'm relatively new to programming as a whole and some tutorials have been telling me to use <code>ls -l</code> to look at files in a directory and others have been saying <code>ll</code>.  I know that <code>ls</code> is a short list, but is there a difference between the other two?</p>
","<p>On many systems, <code>ll</code> is an alias of <code>ls -l</code>:</p>

<pre><code>$ type ll
ll is aliased to `ls -l'
</code></pre>

<p>They are the same.</p>
","137705"
"How can I increase the number of inodes in an ext4 filesystem?","101790","","<p>I had a problem (new to me) last week. I have a ext4 (Fedora 15) filesystem. The application that runs on the server suddenly stopped. I couldn't find the problem at first look. </p>

<p><code>df</code> showed 50% available space. After searching for about an hour I saw a forum post where the guy used <code>df -i</code>. The option looks for inodes usage. The system was out of inodes, a simple problem that I didn't realize. The partition had only 3.2M inodes.</p>

<p>Now, my questions are: Can I make the system have more inodes? Should/can it be set when formatting the disk? With the 3.2M inodes, how many files could I have?</p>
","<p>It seems that you have a lot more files than normal expectation.</p>

<p>I don't know whether there is a solution to change the inode table size dynamically.  I'm afraid that you need to back-up your data, and create new filesystem, and restore your data.</p>

<p>To create new filesystem with such a huge inode table, you need to use '-N' option of mke2fs(8).</p>

<p>I'd recommend to use '-n' option first (which does not create the fs, but display the use-ful information) so that you could get the estimated number of inodes.  Then if you need to, use '-N' to create your filesystem with a specific inode numbers.</p>
","26600"
"What does etc stand for?","101733","","<p>What does the ""etc"" folder in the root directory stand for? I think knowing this will help me remember where certain files are located.</p>

<p><em>Update</em>: Might be useful for others, the folder is used for ""Host specific configuration files"" - <a href=""http://www.pathname.com/fhs/pub/fhs-2.3.pdf"">reference</a>.</p>
","<p><a href=""https://ask.slashdot.org/story/07/03/03/028258/define---etc"" rel=""noreferrer"">Define - /etc?</a> has some good history.</p>

<p>You can find references to ""et cetera"" in old Bell Labs UNIX manuals and so on – nowadays it's used only for system configuration, but it used to be where all the stuff that didn't fit into other directories went.</p>
","5669"
"How do I know a specified user's permissions on Linux with root access?","101655","","<p>I have root access to my local server. And some days ago my colleague created a user on that server, giving me the uesrname and password. But the user has a minimized permissions. Like the user can't even create a file under its own home directory.</p>

<p>Is there any concept about ""the permissions of a user""? If there is, how do I check/modify it?</p>
","<p>It may be the case that your colleague, while creating the account, created the home directory ""by hand"" which resulted in it being owned by <code>root</code>. Try running the following as <code>root</code>:</p>

<pre><code>chown -R username ~username
chgrp -R $(id -gn username) ~username
</code></pre>

<p>Where <code>username</code> is the name of the problematic account.</p>

<p><strong>Edit</strong></p>

<p>If this turns out to be your problem, to avoid this happening in the future, you want to add the <code>-m</code> switch to the <code>useradd</code> command line used to create the user account. This ensures that the user's selected home directory is created if it doesn't exist. This creates the home directory with the ""right"" ownership and permissions so you don't face this kind of issue.</p>

<p><strong>Edit 2</strong></p>

<p>The <code>chgrp</code> command added above will change group ownership of the entire home directory of <code>username</code> to <code>username</code>'s primary group. Depending on your environment, this may not be exactly what you want and you'll possibly need to change group ownership of specific sub-directories inside the home-directory ""manually"", thereby setting different group ownership for different sub-directories. This is usually not the case for personal computers, but since you mentioned ""a colleague"", I'm assuming we're talking about a networked office environment, in which case group ownership is important for shared directories.</p>
","87136"
"How can I monitor serial port traffic?","101623","","<p>Is there any port monitoring tool to watch the packets written on the port? I especially want to check if my program written in Java works so I need some kind of tool to see if my little application is writing the messages to the port. How do I do this?</p>
","<p>I found projects called <a href=""http://freshmeat.net/projects/linuxserialsniffer/"">Linux Serial Sniffer</a>, <a href=""http://jpnevulator.snarl.nl/"">jpnevulator</a>, and <a href=""http://www.rolf-schroedter.de/moni/"">Moni</a>. The first two look like they do exactly what you want. The last one calls itself a monitor, but it actually looks like a standard serial communication program.</p>
","12384"
"Setting /proc/sys/vm/drop_caches to clear cache","101533","","<p>As part of doing some cold cache timings, I'm trying to free the OS cache. The <a href=""http://www.kernel.org/doc/Documentation/sysctl/vm.txt"">kernel documentation</a> says:</p>

<pre><code>drop_caches

Writing to this will cause the kernel to drop clean caches, dentries and
inodes from memory, causing that memory to become free.

To free pagecache:
    echo 1 &gt; /proc/sys/vm/drop_caches
To free dentries and inodes:
    echo 2 &gt; /proc/sys/vm/drop_caches
To free pagecache, dentries and inodes:
    echo 3 &gt; /proc/sys/vm/drop_caches

As this is a non-destructive operation and dirty objects are not freeable, the
user should run `sync' first.
</code></pre>

<p>I'm a bit sketchy about the details. Running</p>

<pre><code>echo 3 &gt; /proc/sys/vm/drop_caches
</code></pre>

<p>frees pagecache, dentries and inodes. Ok.</p>

<p>So, if I want the system to start caching normally again, do I need to reset it to 0 first? My system has the value currently set to 0, which I assume is the default. Or will it reset on its own? I see at least two possibilities here, and I'm not sure which one is true:</p>

<ol>
<li><p><code>echo 3 &gt; /proc/sys/vm/drop_caches</code> frees pagecache, dentries and inodes. The system then immediately starts caching again. I'm not sure what I would expect the value in <code>/proc/sys/vm/drop_caches</code> to do if this is the case. Go back to 0 almost immediately?</p></li>
<li><p>If <code>/proc/sys/vm/drop_caches</code> is set to 3, the system does not do any memory caching till it is reset to 0.</p></li>
</ol>

<p>Which case is true?</p>
","<p>It isn't sticky - you just write to the file to make it drop the caches and then it immediately starts caching again.</p>

<p>Basically when you write to that file you aren't really changing a setting, you are issuing a command to the kernel. The kernel acts on that command (by dropping the caches) then carries on as before.</p>
","17943"
"How to shutdown Linux at a specific datetime from terminal?","101517","","<p>It seems I can shutdown using <code>sudo shutdown</code> by specifying a time or minutes.</p>

<p>Is there a way to specify datetime for shutdown?</p>
","<p>You can do this directly from the <code>shutdown command</code>, see <code>man shutdown</code>:</p>

<pre><code>SYNOPSIS
   /sbin/shutdown [-akrhPHfFnc] [-t sec] time [warning message]

[...]

   time   When to shutdown.
</code></pre>

<p>So, for example:</p>

<pre><code>shutdown -h 21:45
</code></pre>

<p>That will run <code>shutdown -h</code> at 21:45. </p>

<hr>

<p>For commands that don't offer this functionality, you can try one of: </p>

<h3>A. Using at</h3>

<p>The <a href=""http://linux.about.com/library/cmd/blcmdl1_at.htm"" rel=""nofollow noreferrer""><code>at</code></a> daemon is designed for precisely this. Depending on your OS, you may need to install it. On Debian based systems, this can be done with:</p>

<pre><code>sudo apt-get install at
</code></pre>

<p>There are three ways of giving a command to <code>at</code>:</p>

<ol>
<li><p>Pipe it:</p>

<pre><code>$ echo ""ls &gt; a.txt"" | at now + 1 min
warning: commands will be executed using /bin/sh
job 3 at Thu Apr  4 20:16:00 2013
</code></pre></li>
<li><p>Save the command you want to run in a text file, and then pass that file to <code>at</code>:</p>

<pre><code>$ echo ""ls &gt; a.txt"" &gt; cmd.txt
$ at now + 1 min &lt; cmd.txt
warning: commands will be executed using /bin/sh
job 3 at Thu Apr  4 20:16:00 2013
</code></pre></li>
<li><p>You can also pass <code>at</code> commands from STDIN:</p>

<pre><code>$ at now + 1 min
warning: commands will be executed using /bin/sh
at&gt; ls
</code></pre>

<p>Then, press <kbd>Ctrl</kbd><kbd>D</kbd> to exit the <code>at</code> shell. The <code>ls</code> command will be run in one minute. </p></li>
</ol>

<p>You can give very precise times in the format of <code>[[CC]YY]MMDDhhmm[.ss]</code>, as in </p>

<pre><code>$ at -t 201403142134.12 &lt; script.sh
</code></pre>

<p>This will run the script <code>script.sh</code> at 21:34 and 12 seconds on the 14th of March 2014.</p>

<h3>B. Using cron (though this not a good idea for shutdown)</h3>

<p>The other approach is using the <code>cron</code> scheduler which is designed to perform tasks at specific times. It is usually used for tasks that will be repeated but you can also give a specific time. Each user has their own ""crontabs"" which control what jobs are executed and when. The general format of a crontab is:</p>

<pre><code>*     *     *     *     *  command to be executed
-     -     -     -     -
|     |     |     |     |
|     |     |     |     +----- day of week (0 - 6) (Sunday=0)
|     |     |     +------- month (1 - 12)
|     |     +--------- day of month (1 - 31)
|     +----------- hour (0 - 23)
+------------- min (0 - 59)
</code></pre>

<p>So, for example, this will run <code>ls</code> every day at 14:04:</p>

<pre><code>04 14 * * * ls
</code></pre>

<p>To set up a cronjob for a specific date:</p>

<ol>
<li><p>Create a new crontab by running <code>crontab -e</code>. This will bring up a window of your favorite text editor.</p></li>
<li><p>Add this line to the file that just opened. This particular example will run at 14:34 on the 15th of March 2014 if that day is a Friday (so, OK, it might run more than once):</p>

<pre><code>34 14 15 5  /path/to/command        
</code></pre></li>
<li><p>Save the file and exit the editor.</p></li>
</ol>

<p><a href=""https://stackoverflow.com/a/5473841/1081936"">This</a> SO answer suggests a way to have it run only once but I have never used it so I can't vouch for it.</p>
","120509"
"Determining cause of Linux kernel panic","101450","","<p>I'm running an Ubuntu 12.04 derivative (amd64) and I've been having really strange issues recently. Out of the blue, seemingly, X will freeze completely for a while (1-3 minutes?) and then the system will reboot. This system is overclocked, but very stable as verified in Windows, which leads me to believe I'm having a kernel panic or an issue with one of my modules. Even in Linux, I can run LINPACK and won't see a crash despite putting ridiculous load on the CPU. Crashes seem to happen at random times, even when the machine is sitting idle.</p>

<p>How can I debug what's crashing the system?</p>

<p>On a hunch that it might be the proprietary NVIDIA driver, I reverted all the way down to the stable version of the driver, version 304 and I still experience the crash. </p>

<p>Can anyone walk me through a good debugging procedure for after a crash? I'd be more than happy to boot into a thumb drive and post all of my post-crash configuration files, I'm just not sure what they would be. How can I find out what's crashing my system?</p>

<p>Here are a bunch of logs, the usual culprits.</p>

<p><strong>.xsession-errors</strong>: <a href=""http://pastebin.com/EEDtVkVm"">http://pastebin.com/EEDtVkVm</a></p>

<p><strong>/var/log/Xorg.0.log</strong>: <a href=""http://pastebin.com/ftsG5VAn"">http://pastebin.com/ftsG5VAn</a></p>

<p><strong>/var/log/kern.log</strong>: <a href=""http://pastebin.com/Hsy7jcHZ"">http://pastebin.com/Hsy7jcHZ</a></p>

<p><strong>/var/log/syslog</strong>: <a href=""http://pastebin.com/9Fkp3FMz"">http://pastebin.com/9Fkp3FMz</a></p>

<p>I can't even seem to find a record of the crash at all. </p>

<p>Triggering the crash is not so simple, it seem to happen when the GPU is trying to draw multiple things at once. If I put on a YouTube video in full screen and let it repeat for a while or scroll through a ton of GIFs and a Skype notification pops up, sometimes it'll crash. Totally scratching my head on this one. </p>

<p>The CPU is overclocked to 4.8GHz, but it's completely stable and has survived huge LINPACK runs and 9 hours of Prime95 yesterday without a single crash. </p>

<h2>Update</h2>

<p>I've installed <code>kdump</code>, <code>crash</code>, and <code>linux-crashdump</code>, as well as the kernel debug symbols for my kernel version 3.2.0-35. When I run <code>apport-unpack</code> on the crashed kernel file and then <code>crash</code> on the <code>VmCore</code> crash dump, here's what I see:</p>

<pre><code>      KERNEL: /usr/lib/debug/boot/vmlinux-3.2.0-35-generic
    DUMPFILE: Downloads/crash/VmCore
        CPUS: 8
        DATE: Thu Jan 10 16:05:55 2013
      UPTIME: 00:26:04
LOAD AVERAGE: 2.20, 0.84, 0.49
       TASKS: 614
    NODENAME: mightymoose
     RELEASE: 3.2.0-35-generic
     VERSION: #55-Ubuntu SMP Wed Dec 5 17:42:16 UTC 2012
     MACHINE: x86_64  (3499 Mhz)
      MEMORY: 8 GB
       PANIC: ""[ 1561.519960] Kernel panic - not syncing: Fatal Machine check""
         PID: 0
     COMMAND: ""swapper/5""
        TASK: ffff880211251700  (1 of 8)  [THREAD_INFO: ffff880211260000]
         CPU: 5
       STATE: TASK_RUNNING (PANIC)
</code></pre>

<p>When I run <code>log</code> from the <code>crash</code> utility, I see this at the bottom of the log:</p>

<pre><code>[ 1561.519943] [Hardware Error]: CPU 4: Machine Check Exception: 5 Bank 3: be00000000800400
[ 1561.519946] [Hardware Error]: RIP !INEXACT! 33:&lt;00007fe99ae93e54&gt; 
[ 1561.519948] [Hardware Error]: TSC 539b174dead ADDR 3fe98d264ebd MISC 1 
[ 1561.519950] [Hardware Error]: PROCESSOR 0:206a7 TIME 1357862746 SOCKET 0 APIC 1 microcode 28
[ 1561.519951] [Hardware Error]: Run the above through 'mcelog --ascii'
[ 1561.519953] [Hardware Error]: CPU 0: Machine Check Exception: 4 Bank 3: be00000000800400
[ 1561.519955] [Hardware Error]: TSC 539b174de9d ADDR 3fe98d264ebd MISC 1 
[ 1561.519957] [Hardware Error]: PROCESSOR 0:206a7 TIME 1357862746 SOCKET 0 APIC 0 microcode 28
[ 1561.519958] [Hardware Error]: Run the above through 'mcelog --ascii'
[ 1561.519959] [Hardware Error]: Machine check: Processor context corrupt
[ 1561.519960] Kernel panic - not syncing: Fatal Machine check
[ 1561.519962] Pid: 0, comm: swapper/5 Tainted: P   M     C O 3.2.0-35-generic #55-Ubuntu
[ 1561.519963] Call Trace:
[ 1561.519964]  &lt;#MC&gt;  [&lt;ffffffff81644340&gt;] panic+0x91/0x1a4
[ 1561.519971]  [&lt;ffffffff8102abeb&gt;] mce_panic.part.14+0x18b/0x1c0
[ 1561.519973]  [&lt;ffffffff8102ac80&gt;] mce_panic+0x60/0xb0
[ 1561.519975]  [&lt;ffffffff8102aec4&gt;] mce_reign+0x1f4/0x200
[ 1561.519977]  [&lt;ffffffff8102b175&gt;] mce_end+0xf5/0x100
[ 1561.519979]  [&lt;ffffffff8102b92c&gt;] do_machine_check+0x3fc/0x600
[ 1561.519982]  [&lt;ffffffff8136d48f&gt;] ? intel_idle+0xbf/0x150
[ 1561.519984]  [&lt;ffffffff8165d78c&gt;] machine_check+0x1c/0x30
[ 1561.519986]  [&lt;ffffffff8136d48f&gt;] ? intel_idle+0xbf/0x150
[ 1561.519987]  &lt;&lt;EOE&gt;&gt;  [&lt;ffffffff81509697&gt;] ? menu_select+0xe7/0x2c0
[ 1561.519991]  [&lt;ffffffff815082d1&gt;] cpuidle_idle_call+0xc1/0x280
[ 1561.519994]  [&lt;ffffffff8101322a&gt;] cpu_idle+0xca/0x120
[ 1561.519996]  [&lt;ffffffff8163aa9a&gt;] start_secondary+0xd9/0xdb
</code></pre>

<p><code>bt</code> outputs the backtrace:</p>

<pre><code>PID: 0      TASK: ffff880211251700  CPU: 5   COMMAND: ""swapper/5""
 #0 [ffff88021ed4aba0] machine_kexec at ffffffff8103947a
 #1 [ffff88021ed4ac10] crash_kexec at ffffffff810b52c8
 #2 [ffff88021ed4ace0] panic at ffffffff81644347
 #3 [ffff88021ed4ad60] mce_panic.part.14 at ffffffff8102abeb
 #4 [ffff88021ed4adb0] mce_panic at ffffffff8102ac80
 #5 [ffff88021ed4ade0] mce_reign at ffffffff8102aec4
 #6 [ffff88021ed4ae40] mce_end at ffffffff8102b175
 #7 [ffff88021ed4ae70] do_machine_check at ffffffff8102b92c
 #8 [ffff88021ed4af50] machine_check at ffffffff8165d78c
    [exception RIP: intel_idle+191]
    RIP: ffffffff8136d48f  RSP: ffff880211261e38  RFLAGS: 00000046
    RAX: 0000000000000020  RBX: 0000000000000008  RCX: 0000000000000001
    RDX: 0000000000000000  RSI: ffff880211261fd8  RDI: ffffffff81c12f00
    RBP: ffff880211261e98   R8: 00000000fffffffc   R9: 0000000000000f9f
    R10: 0000000000001e95  R11: 0000000000000000  R12: 0000000000000003
    R13: ffff88021ed5ac70  R14: 0000000000000020  R15: 12d818fb42cfe42b
    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018
--- &lt;MCE exception stack&gt; ---
 #9 [ffff880211261e38] intel_idle at ffffffff8136d48f
#10 [ffff880211261ea0] cpuidle_idle_call at ffffffff815082d1
#11 [ffff880211261f00] cpu_idle at ffffffff8101322a
</code></pre>

<p>Any ideas?</p>
","<p>I have two suggestions to start.</p>

<p>The first you're not going to like. No matter how stable you think your overclocked system is, it would be my first suspect.  And any developer you report the problem to will say the same thing. Your stable test workload isn't necessarily using the same instructions, stressing the memory subsystem as much, whatever. Stop overclocking. If you want people to believe the problem's not overclocking, then make it happen when not overclocking so you can get a clean bug report. This will make a huge difference in how much effort other people will invest in solving this problem. Having bug-free software is a point of pride, but reports from people with particularly questionable hardware setups are frustrating time-sinks that probably don't involve a real bug at all.</p>

<p>The second is to get the oops data, which as you've noticed doesn't go to any of the places you've mentioned. If the crash only happens while running X11, I think local console is pretty much out (it's a pain anyway), so you need to do this over a serial console, over the network, or by saving to local disk (which is trickier than it may sound because you don't want an untrustworthy kernel to corrupt your filesystem). Here are some ways to do this:</p>

<ul>
<li>use <a href=""http://linux.die.net/man/8/netdump"" rel=""nofollow noreferrer"">netdump</a> to save to a server over the network. I haven't done this in years, so I'm not sure this software is still around and working with modern kernels, but it's easy enough that it's worth a shot.</li>
<li>boot using a <a href=""http://www.kernel.org/doc/Documentation/serial-console.txt"" rel=""nofollow noreferrer"">serial console</a>; you'll need a serial port free on both machines (whether an old-school one or a USB serial adapter) and a null modem cable; you'd configure the other machine to save the output.</li>
<li><a href=""http://www.kernel.org/doc/Documentation/kdump/kdump.txt"" rel=""nofollow noreferrer"">kdump</a> seems to be what the cool kids use nowadays, and seems quite flexible, although it wouldn't be my preference because it looks complex to set up. In short, it involves booting a different kernel that can do anything and inspect the former kernel's memory contents, but you have to essentially build the whole process and I don't see a lot of canned options out there. <strong>Update:</strong> There are some nice distro things, actually; on Ubuntu, <a href=""https://help.ubuntu.com/lts/serverguide/kernel-crash-dump.html"" rel=""nofollow noreferrer"">linux-crashdump</a></li>
</ul>

<p>Once you get the debug info, there's a tool called <a href=""http://linuxcommand.org/man_pages/ksymoops8.html"" rel=""nofollow noreferrer"">ksymoops</a> that you can use to turn the addresses into symbol names and start getting an idea how your kernel crashed. And if the symbolized dump doesn't mean anything to you, at least this is something helpful to report here or perhaps on your Linux distribution's mailing list / bug tracker.</p>

<hr>

<p>From <code>crash</code> on your crashdump, you can try typing <code>log</code> and <code>bt</code> to get a bit more information (things logged during the panic and a stack backtrace). Your <code>Fatal Machine check</code> seems to be coming from <a href=""http://lxr.linux.no/linux+v3.7.1/arch/x86/kernel/cpu/mcheck/mce.c#L768"" rel=""nofollow noreferrer"">here</a>, though. From skimming the code, your processor has reported a <a href=""http://en.wikipedia.org/wiki/Machine_Check_Exception"" rel=""nofollow noreferrer"">Machine Check Exception</a> - a hardware problem. Again, my first bet would be due to overclocking. It seems like there might be a more specific message in the <code>log</code> output which could tell you more.</p>

<p>Also from that code, it looks like if you boot with the <code>mce=3</code> kernel parameter, it will stop crashing...but I wouldn't really recommend this except as a diagnostic step. If the Linux kernel thinks this error is worth crashing over, it's probably right.</p>
","60928"
"How do you move all files (including hidden) from one directory to another?","101360","","<p>How do I move all files in a directory (including the hidden ones) to another directory?</p>

<p>For example, if I have a folder ""Foo"" with the files "".hidden"" and ""notHidden"" inside, how do I move both files to a directory named ""Bar""?  The following does not work, as the "".hidden"" file stays in ""Foo"".</p>

<pre><code>mv Foo/* Bar/
</code></pre>

<p>Try it yourself.</p>

<pre><code>mkdir Foo
mkdir Bar
touch Foo/.hidden
touch Foo/notHidden
mv Foo/* Bar/
</code></pre>
","<h3>Zsh</h3>

<pre><code>mv Foo/*(DN) Bar/
</code></pre>

<p>or</p>

<pre><code>setopt -s glob_dots
mv Foo/*(N) Bar/
</code></pre>

<p>(Leave out the <code>(N)</code> if you know the directory is not empty.)</p>

<h3>Bash</h3>

<pre><code>shopt -s dotglob nullglob
mv Foo/* Bar/
</code></pre>

<h3>Ksh93</h3>

<p>If you know the directory is not empty:</p>

<pre><code>FIGNORE='.?(.)'
mv Foo/* Bar/
</code></pre>

<h3>Standard (POSIX) sh</h3>

<pre><code>for x in Foo/* Foo/.[!.]* Foo/..?*; do
  if [ -e ""$x"" ]; then mv -- ""$x"" Bar/; fi
done
</code></pre>

<p>If you're willing to let the <code>mv</code> command return an error status even though it succeeded, it's a lot simpler:</p>

<pre><code>mv Foo/* Foo/.[!.]* Foo/..?* Bar/
</code></pre>

<h3>GNU find and GNU mv</h3>

<pre><code>find Foo/ -mindepth 1 -maxdepth 1 -exec mv -t Bar/ -- {} +
</code></pre>

<h3>Standard find</h3>

<p>If you don't mind changing to the source directory:</p>

<pre><code>cd Foo/ &amp;&amp;
find . -name . -o -exec sh -c 'mv -- ""$@"" ""$0""' ../Bar/ {} + -type d -prune
</code></pre>

<hr>

<p>Here's more detail about controlling whether dot files are matched in bash, ksh93 and zsh.</p>

<h3>Bash</h3>

<p>Set the <a href=""http://www.gnu.org/software/bash/manual/bash.html#The-Shopt-Builtin""><code>dotglob</code> option</a>.</p>

<pre><code>$ echo *
none zero
$ shopt -s dotglob
$ echo *
..two .one none zero
</code></pre>

<p>There's also the more flexible <a href=""http://www.gnu.org/software/bash/manual/bash.html#index-GLOBIGNORE-194""><code>GLOBIGNORE</code> variable</a>, which you can set to a colon-separated list of wildcard patterns to ignore. If unset (the default setting), the shell behaves as if the value was empty if <code>dotglob</code> is set, and as if the value was <code>.*</code> if the option is unset. See <a href=""http://www.gnu.org/software/bash/manual/bash.html#Filename-Expansion"">Filename Expansion</a> in the manual. The pervasive directories <code>.</code> and <code>..</code> are always omitted, unless the <code>.</code> is matched explicitly by the pattern.</p>

<pre><code>$ GLOBIGNORE='n*'
$ echo *
..two .one zero
$ echo .*
..two .one
$ unset GLOBIGNORE
$ echo .*
. .. ..two .one
$ GLOBIGNORE=.:..
$ echo .*
..two .one
</code></pre>

<h3>Ksh93</h3>

<p>Set the <a href=""http://www2.research.att.com/sw/download/man/man1/ksh.html#Parameter%20Expansion""><code>FIGNORE</code> variable</a>. If unset (the default setting), the shell behaves as if the value was <code>.*</code>. To ignore <code>.</code> and <code>..</code>, they must be matched explicitly (the manual in ksh 93s+ 2008-01-31 states that <code>.</code> and <code>..</code> are always ignored, but this does not correctly describe the actual behavior).</p>

<pre><code>$ echo *
none zero
$ FIGNORE='@(.|..)'
$ echo *
..two .one none zero
$ FIGNORE='n*'
$ echo *
. .. ..two .one zero
</code></pre>

<p>You can include dot files in a <a href=""http://www2.research.att.com/sw/download/man/man1/ksh.html#File%20Name%20Generation"">pattern</a> by matching them explicitly.</p>

<pre><code>$ unset FIGNORE
$ echo @(*|.[^.]*|..?*)
..two .one none zero
</code></pre>

<p>To have the expansion come out empty if the directory is empty, use the <code>N</code> pattern matching option: <code>~(N)@(*|.[^.]*|..?*)</code> or <code>~(N:*|.[^.]*|..?*)</code>.</p>

<h3>Zsh</h3>

<p>Set the <a href=""http://zsh.sourceforge.net/Doc/Release/Options.html#index-DOT_005fGLOB""><code>dot_glob</code> option</a>.</p>

<pre><code>% echo *
none zero
% setopt dot_glob
% echo *
..two .one none zero
</code></pre>

<p><code>.</code> and <code>..</code> are never matched, even if the pattern matches the leading <code>.</code> explicitly.</p>

<pre><code>% echo .*
..two .one
</code></pre>

<p>You can include dot files in a specific pattern with the <code>D</code> <a href=""http://zsh.sourceforge.net/Doc/Release/Expansion.html#Glob-Qualifiers"">glob qualifier</a>.</p>

<pre><code>% echo *(D)
..two .one none zero
</code></pre>

<p>Add the <code>N</code> glob qualifier to make the expansion come out empty in an empty directory: <code>*(DN)</code>.</p>

<hr>

<p>Note: you may get filename expansion results in different orders
(e.g., <code>none</code> followed by <code>.one</code> followed by <code>..two</code>)
based on your settings of the <code>LC_COLLATE</code>, <code>LC_ALL</code>, and <code>LANG</code> variables.</p>
","6397"
"Command to list assigned dhcp addresses","101325","","<p>Is there a command I can use to ask the dhcpd server which addresses have been assigned? </p>
","<p>No, you can only get this information server side from the DHCP server. This information is contained in the DHCP server's .lease file: <code>/var/lib/dhcpd/dhcpd.leases</code>, if you're using ISC's DHCP server.</p>

<h3>Example</h3>

<pre><code>$ more /var/lib/dhcpd/dhcpd.leases
# All times in this file are in UTC (GMT), not your local timezone.   This is
# not a bug, so please don't ask about it.   There is no portable way to
# store leases in the local timezone, so please don't request this as a
# feature.   If this is inconvenient or confusing to you, we sincerely
# apologize.   Seriously, though - don't ask.
# The format of this file is documented in the dhcpd.leases(5) manual page.
# This lease file was written by isc-dhcp-V3.0.5-RedHat

lease 192.168.1.100 {
  starts 4 2011/09/22 20:27:28;
  ends 1 2011/09/26 20:27:28;
  tstp 1 2011/09/26 20:27:28;
  binding state free;
  hardware ethernet 00:1b:77:93:a1:69;
  uid ""\001\000\033w\223\241i"";
}
...
...
</code></pre>
","91800"
"SSH key-based authentication: known_hosts vs authorized_keys","101194","","<p>I read about setting up ssh keys in Linux and have some questions. Correct me if I'm wrong…</p>

<p>Let's say host tr-lgto wants to connect to host tr-mdm using ssh. If we want to be sure that it's the real tr-mdm, we generate a pair of keys on tr-mdm and we add the public key to <code>known_hosts</code> on tr-lgto.
If tr-mdm wants to check that it's the real tr-lgto, then tr-lgto has to generate a keypair and add the public key to <code>authorized_keys</code> on tr-mdm.</p>

<p><strong>Question 1</strong>: There is no <em>user</em> field in file known_hosts, just IP addresses and hostnames. tr-mdm might have a lot of users, each with their own <code>.ssh</code> folder. Should we add the public key to each of the <code>known_hosts</code> files?</p>

<p><strong>Question 2</strong>: I found that <code>ssh-keyscan -t rsa tr-mdm</code> will return the public key of tr-mdm. How do I know what user this key belongs to? Moreover, the public key in <code>/root/.ssh/</code> is different from what that command returns. How can this be?</p>
","<p>You're mixing up the authentication of the server machine to the client machine, and the authentication of the user to the server machine.</p>

<h3>Server authentication</h3>

<p>One of the first things that happens when the SSH connection is being established is that the server sends its public key to the client, and proves (thanks to <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"" rel=""noreferrer"">public-key cryptography</a>) to the client that it knows the associated private key. This authenticates the server: if this part of the protocol is successful, the client knows that the server is who it pretends it is. </p>

<p>The client may check that the server is a known one, and not some rogue server trying to pass off as the right one. SSH provides only a simple mechanism to verify the server's legitimacy: it remembers servers you've already connected to, in the <code>~/.ssh/known_hosts</code> file on the client machine (there's also a system-wide file <code>/etc/ssh/known_hosts</code>). The first time you connect to a server, you need to check by some other means that the public key presented by the server is really the public key of the server you wanted to connect to. If you have the public key of the server you're about to connect to, you can add it to <code>~/.ssh/known_hosts</code> on the client manually.</p>

<p>Authenticating the server has to be done before you send any confidential data to it. In particular, if the user authentication involves a password, the password must not be sent to an unauthenticated server.</p>

<h3>User authentication</h3>

<p>The server only lets a remote user log in if that user can prove that they have the right to access that account. Depending on the server's configuration and the user's choice, the user may present one of several forms of credentials (the list below is not exhaustive).</p>

<ul>
<li>The user may present the password for the account that he is trying to log into; the server then verifies that the password is correct.</li>
<li>The user may present a public key and prove that he possesses the private key associated with that public key. This is exactly the same method that is used to authenticate the server, but now the user is trying to prove their identity and the server is verifying them. The login attempt is accepted if the user proves that he knows the private key and the public key is in the account's authorization list (<code>~/.ssh/authorized_keys</code> on the server).</li>
<li>Another type of method involves delegating part of the work of authenticating the user to the client machine. This happens in controlled environments such as enterprises, when many machines share the same accounts. The server authenticates the client machine by the same mechanism that is used the other way round, then relies on the client to authenticate the user.</li>
</ul>
","42695"
"git pull from remote but no such ref was fetched?","101075","","<p>I have a git mirror on my disk and when I want to update my repo with git pull it gives me error message:</p>

<pre><code> Your configuration specifies to merge with the ref '3.5/master' from the remote, but no such ref was fetched.
</code></pre>

<p>It also gives me: </p>

<pre><code>  1ce6dac..a5ab7de  3.4/bfq    -&gt; origin/3.4/bfq
  fa52ab1..f5d387e  3.4/master -&gt; origin/3.4/master
  398cc33..1c3000a  3.4/upstream-updates -&gt; origin/3.4/upstream-updates
  d01630e..6b612f7  3.7/master -&gt; origin/3.7/master
  491e78a..f49f47f  3.7/misc   -&gt; origin/3.7/misc
  5b7be63..356d8c6  3.7/upstream-updates -&gt; origin/3.7/upstream-updates
  636753a..027c1f3  3.8/master -&gt; origin/3.8/master
  b8e524c..cfcf7b5  3.8/misc   -&gt; origin/3.8/misc
  * [neuer Zweig]     3.8/upstream-updates -&gt; origin/3.8/upstream-updates
</code></pre>

<p>When I run make menuconfig it gives me Linux version 3.5.7? What does this mean? How can I update my repo?</p>
","<p>Check the branch you are on (<code>git branch</code>), check the configuration for that branch (in <code>.../.git/config</code>), you probably are on the wrong branch or your configuration for it tells to merge with a (now?) non-existent remote branch.</p>
","66868"
"How to clear journalctl","101058","","<p>I couldn't find in google any safe way to clear systemd journal. Do anyone know any safe and reliable way to do so?</p>

<p>Let's say I was experimenting with something and my logs got cluttered with various error messages. Moreover I'm displaying my journal on my desktop by using Conky. I really don't want to see those errors as they remind me an awful day I was fixing this stuff, I want to feel like a fresh man after this horror. I think everyone will agree that this is a valid reason to clear the logs :P .</p>
","<p>The self maintenance method is to vacuum the logs by size or time.</p>

<p>Retain only the past two days:</p>

<pre><code>journalctl --vacuum-time=2d
</code></pre>

<p>Retain only the past 500 MB:</p>

<pre><code>journalctl --vacuum-size=500M
</code></pre>

<p><code>man journalctl</code> for more information.</p>
","194058"
"How to create a bootable Debian USB drive using Windows","100778","","<p>I am running Windows 10 and am starting to learn how to boot from USB devices. </p>

<p>I have a 16GB USB (USB 3.0) drive and I want to do the following:</p>

<ol>
<li>Make the 16GB USB drive run Debian Linux.</li>
<li>Keep Windows 10 on my C: drive.</li>
<li>Not partition my hard drive or set up a dual boot.</li>
<li>Run the OS from my USB drive.</li>
<li>Let all of my files and programs be saved to the USB (so I don't think that a live OS would be suitable). It should work as though it was a dual boot as in the way files are saved.</li>
<li>Make it work on any computer it is plugged in to (assuming the BIOS is compatible).</li>
</ol>

<p>I already know how to boot from a USB in my BIOS but I am unsure as to where to get an ISO file and how to install it to the USB.</p>
","<p><strong><em>To create a bootable USB, you can follow the steps below:</em></strong></p>

<hr>

<p><strong>STEP 1</strong></p>

<p>Go to the website of the OS you wish to install, and find an iso image to download. In your case, since you want to run a Debian OS, here is a link to its iso options: <a href=""https://www.debian.org/distrib/netinst"">https://www.debian.org/distrib/netinst</a></p>

<p>Choose an iso image from the options, and click on it. This should automatically start the image download. While file is downloading, go to second step.</p>

<hr>

<p><strong>STEP 2</strong></p>

<p>Get a utility program to format and create bootable USB flash drives. Some have already been suggested, so I will just link you to my favourite: <a href=""https://rufus.akeo.ie/"">https://rufus.akeo.ie/</a></p>

<p>Download the utility and go to third step.</p>

<hr>

<p><strong>STEP 3</strong></p>

<p>By this stage, if your iso image has not yet finished downloading, then wait until it does. </p>

<p>Now that you have both the utility and the iso image downloaded:</p>

<ol>
<li>Plug in your USB drive</li>
<li>Open Rufus (to write your USB)</li>
<li>Select the iso image you just downloaded to write on the USB, and fill out the other options accordingly (eg. selecting your USB drive etc)</li>
<li>Click on the option for starting the write process (with Rufus, it is the ""Start"" button)</li>
</ol>

<p>Once Rufus finishes, simply reboot, booting from your USB, which should start up your Debian OS.</p>
","263636"
"How to scroll in a terminal using keyboard?","100390","","<p>How can I scroll in bash using only the keyboard? If it's not possible in bash, are there any other shells that support this?</p>
","<p>In ""terminal"" (not a graphic emulator like <code>gterm</code>),
<kbd>Shift</kbd>+<kbd>PageUp</kbd> and <kbd>Shift</kbd>+<kbd>PageDown</kbd> work.</p>
","60387"
"show gateway IP address when performing ifconfig command","100006","","<p>Currently, when using the <code>ifconfig</code> command, the following IP addresses are shown:
own IP, broadcast and mask. </p>

<p>Is there a way to show the related gateway IP address as well (on the same screen with all the others, not by using 'route' command)?</p>
","<p>You can with the <code>ip</code> command, and given that <code>ifconfig</code> is in the process of being deprecated by most distributions it's now the preferred tool.  An example:</p>

<pre><code>$ ip route show
212.13.197.0/28 dev eth0  proto kernel  scope link  src 212.13.197.13
default via 212.13.197.1 dev eth0
</code></pre>
","23386"
"How to list files without directories, and filter by name (ls options)","99975","","<p>I have a directory called <code>uploads</code>. It contains a bunch of files, plus a few subdirectories which in turn contain files. </p>

<p>Is there a way I can (in one step) do the following:</p>

<ol>
<li><p>List ONLY the files in the root uploads directory -- I do not want to see the subfolder names or their contents;</p>

<p><strong>and</strong></p></li>
<li><p>Do NOT list any files that start with <code>t_</code></p></li>
</ol>

<p>I know about the <code>-d</code> flag, but it doesn't get me quite what I want.</p>
","<p>This sounds like a job for <code>find</code>.</p>

<ul>
<li>Use <code>-maxdepth</code> to only return the current directory, not recursivly search inside subfolders</li>
<li>Use <code>-type f</code> to only return files and not directories or device nodes or whatever else</li>
<li>Use a combination if <code>-not</code> and <code>-name</code> to avoid the files with names you don't want</li>
</ul>

<p>It might come together like this:</p>

<pre><code>find /path/to/uploads -maxdepth 1 -type f -not -name 't_*'
</code></pre>
","15951"
"How to grep lines which does not begin with ""#"" or "";""?","99953","","<p>I want to grep <code>smb.conf</code> and see only lines which are not commented.</p>
","<pre><code>grep ""^[^#;]"" smb.conf
</code></pre>

<p>The first <code>^</code> refers to the beginning of the line, so lines with comments starting after the first character will not be excluded.  <code>[^#;]</code> means any character which is not <code>#</code> or <code>;</code>.</p>

<p>In other words, it reports lines that start with any character other than <code>#</code> and <code>;</code>. It's not the same as reporting the lines that don't start with <code>#</code> and <code>;</code> (for which you'd use <code>grep -v '^[#;]'</code>) in that it also excludes <em>empty</em> lines, but that's probably preferable in this case as I doubt you care about empty lines.</p>

<p>If you wanted to ignore leading blank characters, you could change it to:</p>

<pre><code>grep '^[[:blank:]]*[^[:blank:]#;]' smb.conf
</code></pre>

<p>or</p>

<pre><code>grep -vxE '[[:blank:]]*([#;].*)?' smb.conf
</code></pre>

<p>Or</p>

<pre><code>awk '$1 ~ /^[^;#]/' smb.conf
</code></pre>
","60995"
"""E: Sub-process /usr/bin/dpkg returned an error code (1) "" What does this mean?","99727","","<p>I've seen this message several times whenever someone has a problem installing, upgrading or removing some piece of software, but I wonder, <strong>what does it mean</strong>, and more importantly, is possible to solve it?</p>

<pre><code>(Reading database ... 81657 files and directories currently installed.)
 Removing mongodb-10gen ...
 arg: remove
 invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found.
 dpkg: error processing mongodb-10gen (--remove):
  subprocess installed pre-removal script returned error exit status 100
 invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found.
 dpkg: error while cleaning up:
  subprocess installed post-installation script returned error exit status 100
 Errors were encountered while processing:
  mongodb-10gen
 E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>

<p><em>(the above is just an example, not my actual problem)</em></p>
","<p>That message is generic. It just means that the <code>dpkg</code> instance called by <code>apt</code>/<code>apt-get</code> failed for some reason. It doesn't explain why, how, or give hints how to solve it. As a diagnostic message, it is not useful.</p>

<p>You need to read the lines before the message (sometimes quite a number of them) to find the real error that prevents you from completing the installation.</p>

<h2>Yeah, but how do I solve it?</h2>

<p>There is no single way to solve it. There are so many reasons why this can happen that it's futile to attempt to list them all in a single post. Each and every circumstance is almost unique to that package/environment.</p>

<p>But, there's redemption. The fact that you see this message means that probably there is more relevant information in the lines before the message. For illustrative purposes I will use a example:</p>

<pre><code>(Reading database ... 81657 files and directories currently installed.)
 Removing mongodb-10gen ...
 arg: remove
 invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found.
 dpkg: error processing mongodb-10gen (--remove):
  subprocess installed pre-removal script returned error exit status 100
 invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found.
 dpkg: error while cleaning up:
  subprocess installed post-installation script returned error exit status 100
 Errors were encountered while processing:
  mongodb-10gen
 E: Sub-process /usr/bin/dpkg returned an error code (1)
</code></pre>

<p>Now, to find the problem, you need to read backwards:</p>

<ul>
<li><code>E: Sub-process /usr/bin/dpkg returned an error code (1)</code> doesn't tell me anything useful. So moving on.</li>
<li><code>Errors were encountered while processing: mongodb-10gen</code> just tells me what package have problems. Is useful but not enough.</li>
<li><code>subprocess installed post-installation script returned error exit status 100</code>: this tells me that the script that failed was the <code>postinst</code>, the one executed in post-installation. This will come handy in some situations, but not in this one.</li>
<li><code>dpkg: error while cleaning up:</code> nothing useful here.</li>
<li><p><code>invoke-rc.d: unknown initscript, /etc/init.d/mongodb not found.</code> BINGO! This tells us that <code>invoke-rc.d</code>, a binary that controls the init script in most Debian-like system, failed. It failed because it couldn't find the <code>/etc/init.d/mongodb</code> script. This is bad. We need to create it or copy from somewhere else so it starts working again. Reinstalling the package is also normally an option for <code>file not found</code> errors.</p>

<p>In this case, reporting a bug is not necessary because is probable that we were the ones that removed the script, but if you are completely sure you didn't touch the file (a <code>debsums -sla</code> should confirm it) then report a bug.</p></li>
</ul>

<p>So, what exactly do you need to get help? Ideally, the complete output of the problem. It's also helpful to include the output of <code>sudo dpkg -C</code> and <code>sudo apt-get check</code>, and the output of <code>apt-cache policy package1 package2...</code> where ""package1 package2 ..."" includes all the packages with problems.</p>
","139587"
"Increasing Screen Size/Resolution on a VirtualBox Instance of Centos","99675","","<p>I have VirtualBox instance of Centos 5. The screen size is quite small (800*600) and I'd like to increase it to 1280*1080. Under the Gnome preferences for ""Screen Resolution"" I only get the option for 600*800 or 640*480.</p>

<p>I've tried editing my xorg.conf (based on this tutorial <a href=""http://paulsiu.wordpress.com/2008/09/08/creating-and-managing-centos-virtual-machine-under-virtualbox/"">http://paulsiu.wordpress.com/2008/09/08/creating-and-managing-centos-virtual-machine-under-virtualbox/</a>) but it doesn't seem to have made a difference. Here is a snippet from the edited section:</p>

<pre><code>Section ""Screen""
    Identifier ""Screen0""
    Device     ""Card0""
    Monitor    ""Monitor0""
    DefaultDepth     24
    SubSection ""Display""
        Viewport   0 0
        Depth     24
        Modes   ""1280x800""
    EndSubSection
EndSection
</code></pre>

<p>Does anyone know how to do this?</p>
","<p>A maximum resolution of 800x600 suggests that your X server inside the virtual machine is using the <a href=""http://en.wikipedia.org/wiki/SVGA"">SVGA</a> driver. SVGA is the highest resolution for which there is standard support; beyond that, you need a driver.</p>

<p>VirtualBox <a href=""http://www.virtualbox.org/manual/ch03.html#idp10997712"">emulates</a> a graphics adapter that is specific to VirtualBox, it does not emulate a previously existing hardware component like most other subsystems. The <a href=""http://www.virtualbox.org/manual/ch04.html#idp11274368"">guest additions</a> include a driver for that adapter. Insert the guest additions CD from the VirtualBox device menu, then run the installation program. Log out, restart the X server (send <code>Ctrl+Alt+Backspace</code> from the VirtualBox menu), and you should have a screen resolution that matches your VirtualBox window. If you find that you still need manual tweaking of your <code>xorg.conf</code>, the <a href=""http://www.virtualbox.org/manual/ch09.html#guestxorgsetup"">manual</a> has some pointers.</p>

<p>There's a limit to how high you can get, due to the amount of memory you've allocated to the graphics adapter in the VirtualBox configuration. 8MB will give you up to 1600x1200 in 32 colors. Going beyond that is mostly useful if you use 3D.</p>
","25238"
"Good detailed explanation of /etc/network/interfaces syntax?","99447","","<p>I understood the very basic concept of how to use <code>/etc/network/interfaces</code>, but all I find online are examples, example after example, which I can copy-paste from. What I miss is an explanation of the syntax, an explanation of the meaning of the commands and which order the commands require. I want to understand, because most of the time copy-paste is not enough, because I'm not working on a fresh machine, so I can't just overwrite existing configurations because it would break a lot of stuff. <code>man interfaces</code> was not very helpful since it is written very complicated.</p>

<p>Example questions I have: what does <code>inet</code> in an <code>iface</code> line mean exactly (I could not even find it in the manpage), what does <code>manual</code> in an <code>iface</code> line mean exactly (many examples use it, but according to manpage it needs an extra config file then, which the examples don't present), when do I use or need them? When not? When I create a bridge, what exactly happens to the interfaces?</p>
","<p>Well, let’s separate it into pieces, to make it easier to understand <code>/etc/network/interfaces</code>:</p>

<p><strong><a href=""http://en.wikipedia.org/wiki/Link_Layer"" rel=""nofollow noreferrer"">Link layer</a> options (and generally the first of each interface stanza):</strong></p>

<p><code>auto <em>interface</em></code> – Start the interface(s) at boot. That’s why the <code>lo</code> interface uses this kind of linking configuration.</p>

<p><code>allow-auto <em>interface</em></code> – Same as <code>auto</code></p>

<p><code>allow-hotplug <em>interface</em></code> – Start the interface when a ""hotplug"" event is detected. In the real world, this is used in the same situations as <code>auto</code> but the difference is that it will wait for an event like ""being detected by udev hotplug api"" or ""cable linked"". See ""<a href=""https://unix.stackexchange.com/q/192671/34720"">Related Stuff(hotplug)</a>"" for additional info.</p>

<p>These options are pretty much ""layer 2"" options, setting up link states on interfaces, and are not related with ""layer 3"" (routing and addressing). As an example you could have a link aggregation where the bond0 interface needs to be up whatever the link state is, and its members could be up after a link state event:</p>

<pre><code>auto bond0
iface bond0 inet manual
        down ip link set $IFACE down
        post-down rmmod bonding
        pre-up modprobe bonding mode=4 miimon=200
        up ip link set $IFACE up mtu 9000
        up udevadm trigger

allow-hotplug eth0
iface eth0 inet manual
        up ifenslave bond0 $IFACE
        down ifenslave -d bond0 $IFACE 2&gt; /dev/null

allow-hotplug eth1
iface eth1 inet manual
        up ifenslave bond0 $IFACE
        down ifenslave -d bond0 $IFACE 2&gt; /dev/null
</code></pre>

<p>So, this way I create a <a href=""http://en.wikipedia.org/wiki/Link_Aggregation"" rel=""nofollow noreferrer"">link aggregation</a> and the interfaces will be added to it and removed on cable link states.</p>

<p><strong><a href=""http://en.wikipedia.org/wiki/Network_layer"" rel=""nofollow noreferrer"">Layer 3</a> related options and up:</strong></p>

<p>All options below are a suffix to a defined interface (<code>iface &lt;Interface_family&gt;</code>). Basically the <code>iface eth0</code> creates a stanza called <code>eth0</code> on an Ethernet device. <code>iface ppp0</code> should create a <a href=""http://en.wikipedia.org/wiki/Point-to-Point_Protocol"" rel=""nofollow noreferrer"">point-to-point</a> interface, and it could have different ways to acquire addresses like <code>inet wvdial</code> that will forward the configuration of this interface to <code>wvdialconf</code> script. The tuple <code>inet</code>/<code>inet6</code> + <code>option</code> will define the version of the <a href=""http://en.wikipedia.org/wiki/Internet_Protocol"" rel=""nofollow noreferrer"">IP protocol</a> that will be used and the way this address will be configured (<code>static</code>, <code>dhcp</code>, <code>scripts</code>...).
The <a href=""https://www.debian.org/doc/manuals/debian-reference/ch05.en.html"" rel=""nofollow noreferrer"">online Debian manuals</a> will give you more details about this.</p>

<p>Options on Ethernet interfaces:</p>

<p><code>inet static</code> – Defines a static IP address.</p>

<p><code>inet manual</code> – Does not define an IP address for an interface. Generally used by interfaces that are bridge or aggregation members, interfaces that need to operate in promiscuous mode (<em>e.g. port mirroring or network TAPs</em>), or have a VLAN device configured on them.</p>

<p><code>inet dhcp</code> – Acquire IP address through DHCP protocol.</p>

<p><code>inet6 static</code> – Defines a static IPv6 address.</p>

<p>Example:</p>

<pre><code># Eth0
auto eth0
iface eth0 inet manual
    pre-up modprobe 8021q
    pre-up ifconfig eth0 up
    post-down ifconfig eth0 down

# Vlan Interface
auto vlan00
iface vlan00 inet static
        address 10.0.0.1
        netmask 255.255.255.0
        gateway 10.0.0.254
        vlan-raw-device eth0
        ip_rp_filter 0
</code></pre>

<p>This example will bring <code>eth0</code> up, and create a <a href=""http://en.wikipedia.org/wiki/802.1q"" rel=""nofollow noreferrer"">VLAN interface</a> called <code>vlan100</code> that will process the tag number 100 on an Ethernet frame.</p>

<p><strong>Common options inside an interface stanza(layer 2 and 3):</strong></p>

<p><code>address</code> – IP address for a static IP configured interface</p>

<p><code>netmask</code> – netmask</p>

<p><code>gateway</code> – The default gateway of a server. Be careful to use only one of this guy.</p>

<p><code>vlan-raw-device</code> – On a VLAN interface, defines its ""father"".</p>

<p><code>bridge_ports</code> – On a bridge interface, define its members.</p>

<p><code>down</code> – Use the following command to down the interface instead of <code>ifdown</code>.</p>

<p><code>post-down</code> – Actions taken right after the interface is down.</p>

<p><code>pre-up</code> – Actions before the interface is up.</p>

<p><code>up</code> – Use the following command to up the interface instead of <code>ifup</code>. It is up to your imagination to use any option available on <code>iputils</code>. As an example we could use <code>up ip link set $IFACE up mtu 9000</code> to enable <a href=""https://en.wikipedia.org/wiki/Jumbo_frame"" rel=""nofollow noreferrer"">jumbo frames</a> during the <code>up</code> operation(instead of using the <code>mtu</code> option itself). You can also call any other software like <code>up sleep 5; mii-tool -F 100baseTx-FD $IFACE</code> to force 100Mbps Full duplex 5 seconds after the interface is up.</p>

<p><code>hwaddress ether 00:00:00:00:00:00</code> - Change the mac address of the interface instead of using the one that is hardcoded into rom, or generated by algorithms</p>

<p><code>dns-nameservers</code> – IP addresses of nameservers. Requires the <code>resolvconf</code> package. It’s a way to concentrate all the information in <code>/etc/network/interfaces</code> instead of using <code>/etc/resolv.conf</code> for DNS-related configurations.
Do not edit the <code>resolv.conf</code> configuration file manually
as it will be dynamically changed by programs in the system.</p>

<p><code>dns-search example.net</code> – Append example.net as domain to queries of host, creating the FQDN.  Option <code>domain</code> of <code>/etc/resolv.conf</code></p>

<p><code>wpa-ssid</code> – Wireless: Set a wireless WPA SSID.</p>

<p><code>mtu</code> - MTU size. <code>mtu 9000</code> = Jumbo Frame.</p>

<p><code>wpa-psk</code> – Wireless: Set a hexadecimal encoded PSK for your SSID.</p>

<p><code>ip_rp_filter 1</code> - <a href=""http://www.slashroot.in/linux-kernel-rpfilter-settings-reverse-path-filtering"" rel=""nofollow noreferrer"">Reverse path filter</a> enabled. Useful in situations where you have 2 routes to a host, and this will force the packet to come back from where it came(same interface, using its routes). Example: You are connected on your lan(<code>192.168.1.1/24</code>) and you have a dlna server with one interface on your lan(<code>192.168.1.10/24</code>) and other interface on dmz to execute administrative tasks(<code>172.16.1.1/24</code>). During a ssh session from your computer to dlna dmz ip, the information needs to come back to you, but will hang forever because your dlna server will try to deliver the response directly through it's lan interface. With rp_filter enabled, it will ensure that the connection will come back from where it came from. More information <a href=""https://access.redhat.com/solutions/53031"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Some of those options are not optional. Debian will warn you if you put an IP address on an interface without a netmask, for example.</p>

<p>You can find more good examples of network configuration <a href=""https://wiki.debian.org/NetworkConfiguration"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong>Related Stuff</strong>:</p>

<p>Links that have information related to <code>/etc/network/interfaces</code> network configuration file:</p>

<ul>
<li><a href=""https://ubuntuforums.org/showthread.php?t=318539"" rel=""nofollow noreferrer"">HOWTO: Wireless Security - WPA1, WPA2, LEAP, etc</a>. </li>
<li><a href=""https://unix.stackexchange.com/q/255484/34720"">How can I bridge two interfaces with ip/iproute2?</a>.</li>
<li><a href=""https://unix.stackexchange.com/q/192671/34720"">What is a hotplug event from the interface?</a></li>
</ul>
","128662"
"How to disable auto suspend when I close laptop lid?","99434","","<p>I'm using archlinux. It never auto-suspend before a recent system upgrade(maybe I updated the kernel?).</p>

<p>I think it is related to <code>laptop-mode</code> or <code>acpid</code>, so I stop them:</p>

<pre><code>/etc/rc.d/laptop-mode stop
/etc/rc.d/acpid stop
</code></pre>

<p>I also edit <code>/etc/laptop-mode/laptop-mode.conf</code>:</p>

<pre><code>ENABLE_LAPTOP_MODE_TOOLS=0
</code></pre>

<p>Then I edit <code>/etc/acpi/actions/lm_lid.sh</code>, commented out the last line:</p>

<pre><code># /usr/sbin/laptop_mode auto
</code></pre>

<p>But all of above don't work. Following lines were found in <code>/var/log/kernel.log</code>(unrelated lines omitted):</p>

<pre><code>Oct 23 15:29:20 localhost kernel: [18617.549098] PM: Syncing filesystems ... done.
Oct 23 15:29:20 localhost kernel: [18618.001898] PM: Preparing system for mem sleep
Oct 23 15:29:30 localhost kernel: [18618.039565] Freezing user space processes ... (elapsed 0.01 seconds) done.
Oct 23 15:29:30 localhost kernel: [18618.052596] Freezing remaining freezable tasks ... (elapsed 0.01 seconds) done.
Oct 23 15:29:30 localhost kernel: [18618.065999] PM: Entering mem sleep
Oct 23 15:29:30 localhost kernel: [18618.066167] Suspending console(s) (use no_console_suspend to debug)
Oct 23 15:29:30 localhost kernel: [18618.097917] sd 0:0:0:0: [sda] Synchronizing SCSI cache
Oct 23 15:29:30 localhost kernel: [18618.098103] sd 0:0:0:0: [sda] Stopping disk
Oct 23 15:29:30 localhost kernel: [18618.270537] snd_hda_intel 0000:00:14.2: power state changed by ACPI to D3hot
Oct 23 15:29:30 localhost kernel: [18619.274374] PM: suspend of devices complete after 1196.192 msecs
Oct 23 15:29:30 localhost kernel: [18619.274691] PM: late suspend of devices complete after 0.313 msecs
Oct 23 15:29:30 localhost kernel: [18619.440877] ohci_hcd 0000:00:14.5: wake-up capability enabled by ACPI
Oct 23 15:29:30 localhost kernel: [18619.642144] ACPI: Waking up from system sleep state S3
Oct 23 15:29:30 localhost kernel: [18620.049424] PM: noirq resume of devices complete after 333.503 msecs
Oct 23 15:29:30 localhost kernel: [18620.049852] PM: early resume of devices complete after 0.334 msecs
Oct 23 15:29:30 localhost kernel: [18622.418605] PM: resume of devices complete after 2371.906 msecs
Oct 23 15:29:30 localhost kernel: [18622.419018] PM: Finishing wakeup.
Oct 23 15:29:30 localhost kernel: [18622.419019] Restarting tasks ... done.
Oct 23 15:29:30 localhost kernel: [18622.464752] video LNXVIDEO:01: Restoring backlight state
</code></pre>

<p>I think this is not caused by <code>pm-susend</code>, because <code>/var/log/pm-suspend.log</code> don't log anything.</p>

<p>I don't want my laptop go to sleep when I close the lid. How to do it?</p>

<p>Kernel version: 3.6.2-1-ARCH</p>
","<p>Edit <code>/etc/systemd/logind.conf</code> and make sure you have,</p>

<pre><code>HandleLidSwitch=ignore
</code></pre>

<p>which will make it ignore the lid being closed.  (You may need to also undo the other changes you've made).</p>

<p>Then, you'll want to reload <code>logind.conf</code> to make your changes go into effect (thanks to Ehtesh Choudhury for pointing this out in the comments):</p>

<pre><code>systemctl restart systemd-logind
</code></pre>

<p>Full details over at the <a href=""https://wiki.archlinux.org/index.php/Power_management"">archlinux Wiki</a>.</p>

<p>The man page for logind.conf also has the relevant information,</p>

<pre><code>   HandlePowerKey=, HandleSuspendKey=, HandleHibernateKey=,
   HandleLidSwitch=
       Controls whether logind shall handle the system power and sleep
       keys and the lid switch to trigger actions such as system power-off
       or suspend. Can be one of ignore, poweroff, reboot, halt, kexec,
       suspend, hibernate, hybrid-sleep and lock. If ignore logind will
       never handle these keys. If lock all running sessions will be
       screen locked. Otherwise the specified action will be taken in the
       respective event. Only input devices with the power-switch udev tag
       will be watched for key/lid switch events.  HandlePowerKey=
       defaults to poweroff.  HandleSuspendKey= and HandleLidSwitch=
       default to suspend.  HandleHibernateKey= defaults to hibernate.
</code></pre>
","52645"
"disable transparent hugepages","98725","","<p>We are installing SAP HANA in a <strong>RAID</strong> machine. As part of the installation step, it is mentioned that,</p>

<pre><code> To disable the usage of transparent hugepages set the kernel settings 
 at runtime with echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled 
</code></pre>

<p>So instead of runtime, if I wanted to make this a permanent change, should I add the above line inside <code>/proc/vmstat</code> file?</p>
","<p>To make options such as this permanent you'll typically add them to the file <code>/etc/sysctl.conf</code>. You can see a full list of the options available using this command:</p>

<pre><code>$ sysctl -a
</code></pre>

<h3>Example</h3>

<pre><code>$ sudo sysctl -a | head -5
kernel.sched_child_runs_first = 0
kernel.sched_min_granularity_ns = 6000000
kernel.sched_latency_ns = 18000000
kernel.sched_wakeup_granularity_ns = 3000000
kernel.sched_shares_ratelimit = 750000
</code></pre>

<p>You can look for <code>hugepage</code> in the output like so:</p>

<pre><code>$ sudo sysctl -a | grep hugepage
vm.nr_hugepages = 0
vm.nr_hugepages_mempolicy = 0
vm.hugepages_treat_as_movable = 0
vm.nr_overcommit_hugepages = 0
</code></pre>

<h3>It's not there?</h3>

<p>However looking through the output I did not see <code>transparent_hugepage</code>. Googling a bit more I did come across this Oracle page which discusses this very topic. The page is titled: <a href=""http://www.oracle-base.com/articles/linux/configuring-huge-pages-for-oracle-on-linux-64.php"">Configuring HugePages for Oracle on Linux (x86-64)</a>.</p>

<p>Specifically on that page they mention how to <a href=""http://www.oracle-base.com/articles/linux/configuring-huge-pages-for-oracle-on-linux-64.php#disabling-transparent-hugepages"">disable the hugepage feature</a>.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>The preferred method to disable Transparent HugePages is to add ""transparent_hugepage=never"" to the kernel boot line in the ""/etc/grub.conf"" file.</p>

<pre><code>   title Oracle Linux Server (2.6.39-400.24.1.el6uek.x86_64)
            root (hd0,0)
            kernel /vmlinuz-2.6.39-400.24.1.el6uek.x86_64 ro root=/dev/mapper/vg_ol6112-lv_root rd_NO_LUKS  KEYBOARDTYPE=pc KEYTABLE=uk
    LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16  rd_NO_DM rd_LVM_LV=vg_ol6112/lv_swap rd_LVM_LV=vg_ol6112/lv_root rhgb quiet numa=off
    transparent_hugepage=never
            initrd /initramfs-2.6.39-400.24.1.el6uek.x86_64.img
</code></pre>
  
  <p>The server must be rebooted for this to take effect.</p>
</blockquote>

<p>Alternatively you can add the command to your <code>/etc/rc.local</code> file.</p>

<pre><code>if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
   echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
fi
if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag
fi
</code></pre>

<p>I think I would go with the 2nd option, since the first will be at risk of getting unset when you upgrade from one kernel to the next.</p>

<p>You can confirm that it worked with the following command after rebooting:</p>

<pre><code>$ cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]
</code></pre>
","99172"
"Creating an user without a password","98468","","<p>I'm trying to create user without password like this:</p>

<pre><code>sudo adduser \
   --system \
   --shell /bin/bash \
   --gecos ‘User for managing of git version control’ \
   --group \
   --disabled-password \
   --home /home/git \
   git
</code></pre>

<p>It's created fine. But when I try to login under the git user I'm getting the password entering:</p>

<pre><code>su git
Password:...
</code></pre>

<p>When I leave it empty I get an error:</p>

<pre><code>su: Authentication failed
</code></pre>

<p>What's wrong?  </p>
","<p>You've created a user with a “disabled password”, meaning that there is no password that will let you log in as this used. This is different from creating a user that anyone can log in as without supplying a password, which is achieved by specifying an empty password and is very rarely useful.</p>

<p>In order to execute commands as such “system” users who don't log in normally, you need to hop via the root account:</p>

<pre><code>su -c 'su git -c ""git init""'
</code></pre>

<p>or</p>

<pre><code>sudo -u git git init
</code></pre>

<p>If you want certain users to be able to run commands as the <code>git</code> user without letting them run commands as root, set up sudo (run <code>visudo</code> as root and add a line like <code>%gitters ALL = (git) ALL</code>).</p>
","56847"
"How can I send stdout to multiple commands?","98410","","<p>There are some commands which filter or act on input, and then pass it along as output, I think usually to <code>stdout</code> - but some commands will just take the <code>stdin</code> and do whatever they do with it, and output nothing.</p>

<p>I'm most familiar with OS X and so there are two that come to mind immediately are <code>pbcopy</code> and <code>pbpaste</code>- which are means of accessing the system clipboard.</p>

<p>Anyhow, I know that if I want to take stdout and spit the output to go to both <code>stdout</code> and a file then I can use the <code>tee</code> command. And I know a little about <code>xargs</code>, but I don't think that's what I'm looking for.</p>

<p>I want to know how I can split <code>stdout</code> to go between two (or more) commands. For example:</p>

<pre><code>cat file.txt | stdout-split -c1 pbcopy -c2 grep -i errors
</code></pre>

<p>There is probably a better example than that one, but I really am interested in knowing how I can send stdout to a command that does not relay it and while keeping <code>stdout</code> from being ""muted"" - I'm not asking about how to <code>cat</code> a file and <code>grep</code> part of it and copy it to the clipboard - the specific commands are not that important.</p>

<p>Also - I'm not asking how to send this to a file and <code>stdout</code> - this may be a ""duplicate"" question (sorry) but I did some looking and could only find similar ones that were asking about how to split between stdout and a file - and the answers to those questions seemed to be <code>tee</code>, which I don't think will work for me.</p>

<p>Finally, you may ask ""why not just make pbcopy the last thing in the pipe chain?"" and my response is 1) what if I want to use it and still see the output in the console? 2) what if I want to use two commands which do not output <code>stdout</code> after they process the input?</p>

<p>Oh, and one more thing - I realize I could use <code>tee</code> and a named pipe (<code>mkfifo</code>) but I was hoping for a way this could be done inline, concisely, without a prior setup :)</p>
","<p>You can use <code>tee</code> and process substitution for this:</p>

<pre><code>cat file.txt | tee &gt;(pbcopy) | grep errors
</code></pre>

<p>This will send all the output of <code>cat file.txt</code> to <code>pbcopy</code>, and you'll only get the result of <code>grep</code> on your console.</p>

<p>You can put multiple processes in the <code>tee</code> part:</p>

<pre><code>cat file.txt | tee &gt;(pbcopy) &gt;(do_stuff) &gt;(do_more_stuff) | grep errors
</code></pre>
","28519"
"How to make putty to not break my session after some time?","98318","","<p>Each time <code>putty</code> is closing the session after some time if it is idle.</p>

<p>There is no <code>time</code> parameter on putty, so how can I keep my <code>putty</code> ssh session always Alive?</p>
","<p>Another thing to check is if your system is setting the environment variable TMOUT. To check this you can just do:</p>

<p><code>env | grep TMOUT</code></p>

<p>or</p>

<p><code>echo $TMOUT</code></p>

<p>If it is set, you could change it or unset it. To change the value:</p>

<p><code>export TMOUT=3600</code></p>

<p>Where the number is the number of seconds until you get logged out. Otherwise unset it to turn off the feature:</p>

<p><code>unset TMOUT</code></p>

<p>Note, it may be that your system administrator has set this for security reasons. So if you are not the system administrator you may want to check this before changing anything yourself.</p>
","20215"
"Rename multiples files using Bash scripting","98064","","<p>I want to rename multiple files in the same directory using Bash scripting. Names of the files are as follows:</p>

<pre><code>file2602201409853.p
file0901201437404.p  
file0901201438761.p  
file1003201410069.p  
file2602201410180.p
</code></pre>

<p>I want to rename to the following format:</p>

<pre><code>file2503201409853.p
file2503201437404.p  
file2503201438761.p  
file2503201410069.p  
file2503201410180.p
</code></pre>

<p>I was reading about the rename command, and try to do it this way, but it does nothing, I think I have questions about the syntax. Then I read that you can make a loop using the <a href=""http://linux.die.net/man/1/mv"">mv</a> command as follows:</p>

<pre><code>for file in cmpsms*2014*.p; do
    mv ""$file"" ""${file/cmpsms*2014*.p/cmpsms25032014*.p}""
done
</code></pre>

<p>But I can not rename the files. What am I doing wrong?</p>
","<p>You were right to consider rename first. The syntax is a little strange if you're not used to regexes but it's by far the quickest/shortest route once you know what you're doing:</p>

<pre><code>rename 's/\d{4}/2503/' file*
</code></pre>

<p>That simply matches the first 4 numbers and swaps them for the ones you specified.</p>

<p>And a test harness (<code>-vn</code> means be verbose but don't <em>do</em> anything) using your filenames:</p>

<pre><code>$ rename 's/\d{4}/2503/' file* -vn
file0901201437404.p renamed as file2503201437404.p
file0901201438761.p renamed as file2503201438761.p
file1003201410069.p renamed as file2503201410069.p
file2602201409853.p renamed as file2503201409853.p
file2602201410180.p renamed as file2503201410180.p
</code></pre>
","121621"
"How to insert text before the first line of a file?","97984","","<p>I've been looking around <code>sed</code> command to add text into a file in a specific line.
This works adding text after line 1:</p>

<pre><code>sed '1 a\
</code></pre>

<p>But I want to add it <em>before</em> line 1. It would be:</p>

<pre><code>sed '0 a\
</code></pre>

<p>but I get this error: <code>invalid usage of line address 0</code>.</p>

<p>Any suggestion?</p>
","<p>Use sed's insert (i) option which will insert the text in the preceding line.</p>

<pre><code>sed '1 i\
</code></pre>

<p><strong>Question author's update:</strong></p>

<p>To make it edit the file in place - with <code>gnu sed</code> - I had to add the <code>-i</code> option:</p>

<pre><code>sed -i '1 i\anything' file
</code></pre>

<p>Also syntax</p>

<pre><code>sed  -i '1i text' filename
</code></pre>
","99351"
"what is the CentOS equivalent of /var/log/syslog (on Ubuntu)?","97893","","<p>On ubuntu this file exists: <code>/var/log/syslog</code>. </p>

<p>However the same file does not appear on CentOS Distributions. What is the equivalent file on CentOS?</p>
","<p>Red Hat family distributions (including CentOS and Fedora) use <code>/var/log/messages</code> and <code>/var/log/secure</code> where Debian-family distributions use <code>/var/log/syslog</code> and <code>/var/log/auth.log</code>.</p>

<p>Note that in newer Fedora (or RHEL/CentOS 7 if someone has gone out of their way to configure it this way), you may have no traditional syslog daemon running. In that case, the same data can be shown with <code>journalctl</code> (which defaults to producing text output in the syslog format). </p>
","88745"
"How can I set ""vi"" as my default editor in UNIX?","97890","","<p>I believe I can do something like <code>export EDITOR=vi</code>, but I'm not sure what exactly to enter, and where. </p>

<p>How can I set ""vi"" as my default editor?</p>
","<p>You should add it to your shell’s configuration file. For Bash, this is <code>~/.bashrc</code> or <code>~/.bash_profile</code>. You should also set <code>$VISUAL</code>, as some programs (correctly) use that instead of <code>$EDITOR</code> (see <a href=""https://unix.stackexchange.com/questions/4859/visual-vs-editor-whats-the-difference""><code>VISUAL</code> vs. <code>EDITOR</code></a>). Additionally, unless you know why, you should set it to <code>vim</code> instead of <code>vi</code>.</p>

<p>TL;DR, add the following to your shell configuration (probably <code>~/.bashrc</code>):</p>

<pre><code>export VISUAL=vim
export EDITOR=""$VISUAL""
</code></pre>
","73486"
"How to install dig on CentOS 6?","97887","","<p>I can't find the dig command on my new CentOS 6 installation. I've tried <code>yum install dig</code> but it say it cannot find the package. </p>

<p>How do I install dig on CentOS 6?</p>
","<p>The <strong>dig</strong> command is a part of the BIND utilities so you need to install them. To install the BIND utilities, type the following:</p>

<pre><code>$ yum install bind-utils
</code></pre>
","121875"
"What is the real difference between ""apt-get"" and ""aptitude""? (How about ""wajig""?)","97750","","<p>I know that both <code>apt-get</code> and <code>aptitude</code> are command line package management interfaces on Debian derived Linux, with different options, but I'm still somewhat confused.  Under the hood, aren't they using the same APT system?  </p>

<p>Why does Debian maintain these parallel tools?  (Bonus question: what on earth is <a href=""http://manpages.ubuntu.com/manpages/maverick/man1/wajig.1.html"">wajig</a>?)</p>
","<p>The most obvious difference is that <code>aptitude</code> provides a terminal menu interface (much like Synaptic in a terminal), whereas <code>apt-get</code> does not.</p>

<p>Considering only the command-line interfaces of each, they are quite similar, and for the most part, it really doesn't matter which one you use. Recent versions of both will track which packages were manually installed, and which were installed as dependencies (and therefore eligible for automatic removal). In fact, I believe that even more recently, the two tools were updated to actually share the same database of manually vs automatically installed packages, so cases where you install something with apt-get and then aptitude wants to uninstall it are mostly a thing of the past. </p>

<p>There are a few minor differences:</p>

<ul>
<li>aptitude will automatically remove eligible packages, whereas apt-get requires a separate command to do so</li>
<li>The commands for <em>upgrade</em> vs. <em>dist-upgrade</em> have been renamed in aptitude to the probably more accurate names <em>safe-upgrade</em> and <em>full-upgrade</em>, respectively.</li>
<li>aptitude actually performs the functions of not just apt-get, but also some of its companion tools, such as apt-cache and apt-mark.</li>
<li>aptitude has a slightly different query syntax for searching (compared to apt-cache)</li>
<li>aptitude has the <em>why</em> and <em>why-not</em> commands to tell you which <em>manually installed</em> packages are preventing an action that you might want to take.</li>
<li>If the actions (installing, removing, updating packages) that you want to take cause conflicts, aptitude can suggest several potential resolutions. apt-get will just say ""I'm sorry Dave, I can't allow you to do that.""</li>
</ul>

<p>There are other small differences, but those are the most important ones that I can think of.</p>

<p>In short, aptitude more properly belongs in the category with Synaptic and other higher-level package manager frontends. It just happens to also have a command-line interface that resembles apt-get.</p>

<h2>Bonus Round: What is wajig?</h2>

<p>Remember how I mentioned those ""companion"" tools like apt-cache and apt-mark? Well, there's a bunch of them, and if you use them a lot, you might not remember which ones provide which commands. wajig is one solution to that problem. It is essentially a dispatcher, a wrapper around all of those tools. It also applies sudo when necessary. When you say <code>wajig install foo</code>, wajig says ""Ok, <code>install</code> is provided by <code>apt-get</code> and requires admin privileges,"" and it runs <code>sudo apt-get install foo</code>. When you say <code>wajig search foo</code>, wajig says ""Ok, <code>search</code> is provided by <code>apt-cache</code> and does not require admin privileges,"" and it runs <code>apt-cache search foo</code>. If you use wajig instead of apt-get, apt-mark, apt-cache and others, then you'll never have this problem:</p>

<pre><code>$ apt-get search foo
E: Invalid operation search
</code></pre>

<p>If you want to know what wajig is doing behind the scenes, which tools it is using to implement a particular command, it has <code>--simulate</code> and <code>--teaching</code> modes.</p>

<p>Two wajig commands that I often use are <code>wajig listfiles foo</code> and <code>wajig whichpkg /usr/bin/foo</code>.</p>
","935"
"How can I have `date` output the time from a different timezone?","97696","","<p>I have a server running with the timezone set to <code>UTC</code>. It seemed like that was generally a good practice (please correct me if I'm wrong).</p>

<p>Anyhow, one of the servers I connect to, in order to <code>scp</code> files, is running on <code>EDT</code> and stores files that I need to copy in the format <code>/path/to/filename/data20120913</code></p>

<p>I looked at trying to <code>rsync</code> files using something like find's <code>-mtime -1</code> flag for files modified in the last day, but I didn't have any luck.</p>

<p>I don't mind just using <code>scp</code> to copy the current day's file, but as of right now there is a 4-hour window where running <code>date +%Y%m%d</code> will give a different day on each server and that bugs me a little.</p>

<p>Looking through <code>man date</code> I see that I can have the time output as <code>UTC</code>, but I don't see a way to have it output as another timezone like <code>EDT</code></p>

<p>I suppose I could also use something like the <code>GNU</code> date extension <code>date -d 20100909 +%s</code> to get the date in seconds from the epoch, apply a manual <code>4 * 60 * 60</code> second calculation, and see about rendering that as a date - but then when daylight time kicks in it will still be an hour off.</p>

<p>Is there a simpler way to output the date in a <code>YYYYMMDD</code> format for <code>EDT</code> on a server that is set to <code>UTC</code> ?</p>
","<p>You can set a timezone for the duration of the query, thusly:</p>

<pre><code>TZ=America/New_York date
</code></pre>

<p>Note the whitespace between the <code>TZ</code> setting and the <code>date</code> command.  In Bourne-like and <code>rc</code>-like shell, that sets the <code>TZ</code> variable only for the command line. In other shells (<code>csh</code>, <code>tcsh</code>, <code>fish</code>), you can always use the <code>env</code> command instead:</p>

<pre><code>env TZ=America/New_York date
</code></pre>

<p><strong>tl;dr</strong></p>

<p>On Linux systems. timezones are defined in files in the <code>/usr/share/zoneinfo</code> directory. This structure is often referred to as the ""Olson database"" to honor its founding contributor.</p>

<p>The rules for each timezone are defined as text file lines which are then compiled into a binary file.  The lines so compiled, define the zone name; a range of data and time during which the zone applies; an offset from UTC for the standard time; and the notation for defining how transition to-and-from daylight saving time occurs, if applicable.</p>

<p>For example, the directory ""America"" contains the requisite information for New York in the file <code>America/New_York</code> as used, above.</p>

<p>Beware that the specification of a non-existent zone (file name) is silently ignored and UTC times are reported.  For example, this reports an incorrect time:</p>

<pre><code>TZ=""America/New York"" date ### WRONG ###
</code></pre>

<p>The Single UNIX Specification, version-3, known as SUSv3 or POSIX-2001, notes that for portability, the character string that identifies the timezone description should begin with a colon character.  Thus, we can also write:</p>

<pre><code>TZ="":America/New_York"" date
TZ="":America/Los_Angeles"" date
</code></pre>

<p>As an alternative method to the specification of timezones using a pathname to a description file, SUSv3 describes the POSIX model.  In this format, a string is defined as:</p>

<pre><code>std offset [dst[offset][,start-date[/time],end-date[/time]]]
</code></pre>

<p>where <code>std</code> is the standard component name and <code>dst</code> is the daylight saving one.  Each name consists of three or more characters. The <code>offset</code> is positive for timezones west of the prime meridian and negative for those east of the meridian.  The offset is added to the local time to obtain UTC (formerly known as GMT).  The <code>start</code> and <code>end</code> time fields indicate when the standard/daylight transitions occur.</p>

<p>For example, in the Eastern United States, standard time is 5-hours earlier than UTC, and we can specify <code>EST5EDT</code> in lieu of <code>America/New_York</code>.  These alternatives are not always recognized, however, especially for zones outside of the United States and are best avoided.</p>

<p>HP-UX (an SUSv3 compliant UNIX) uses textual rules in <code>/usr/lib/tztab</code> and the POSIX names like EST5EDT, CST6CDT, MST7MDT, PST8PDT.  The file includes all of the historical rules for each time zone, akin to the Olson database.</p>

<p><strong>NOTE:</strong> You should be able to find all of the timezones by inspecting the following directory: <code>/usr/share/zoneinfo</code>.</p>
","48104"
"How to make Fedora user a sudoer?","97643","","<p>I want to give a Fedora user <code>sudo</code> privileges. How do I do that?</p>
","<p>Add the user to the <code>wheel</code> group:</p>

<pre><code>gpasswd wheel -a username
</code></pre>

<p>I use <a href=""http://man7.org/linux/man-pages/man1/gpasswd.1.html"" rel=""noreferrer"">gpasswd</a> because not all versions of <code>usermod</code> have an easy way to add the user to a group without changing <em>all</em> the users' groups. However, on any recent Fedora, <code>usermod username -a -G wheel</code> should have the same effect. You could also use the <code>system-config-users</code> GUI, of course.</p>

<p>If you are using Fedora 14 or earlier, use <code>visudo</code> to edit the <code>sudoers</code> file, removing the <code>#</code> from this line:</p>

<pre><code>%wheel  ALL=(ALL)       ALL
</code></pre>

<p>This is the default in the sudoers file on Fedora 15 and newer, so adding the user to <code>wheel</code> is all you need to do. Note that this won't take effect immediately; the easiest thing to do is log out and in again.</p>

<p>See also <a href=""https://serverfault.com/questions/205598/how-to-tweak-gnome-user-elevation-in-rhel-centos/205610#205610"">this question and answer over on Server Fault</a> for information on granting sudo-like ""auth as self"" behavior to <code>wheel</code> group members for graphical apps which use consolehelper or PackageKit.</p>
","4458"
"How do I delete a file whose name begins with ""-"" (hyphen a.k.a. dash or minus)?","97417","","<p>How do you remove a file whose filename begins with a dash (hyphen or minus) <code>-</code>?  I'm ssh'd into a remote OSX server and I have this file in my directory:</p>

<pre><code>tohru:~ $ ls -l
total 8
-rw-r--r--    1 me  staff  1352 Aug 18 14:33 --help
...
</code></pre>

<p>How in the world can I delete <code>--help</code> from a CLI?  This issue is something that I come across in different forms on occasion, these files are easy to create, but hard to get rid of.</p>

<p>I have tried using backslash</p>

<pre><code>rm \-\-help
</code></pre>

<p>I have tried quotes</p>

<pre><code>rm ""--help""
</code></pre>

<p>How do I prevent the minus (dash or hyphen) character to be interpreted as an option?</p>
","<p>Use ""--"" to make rm stop parsing command line options, like this:</p>

<pre><code>rm -- --help
</code></pre>
","1520"
"How to colorize output of git?","97172","","<p>Is there a way to color output for git (or any command)?</p>

<p>Consider:</p>

<pre class=""lang-none prettyprint-override""><code>baller@Laptop:~/rails/spunky-monkey$ git status
# On branch new-message-types
# Changes not staged for commit:
#   (use ""git add &lt;file&gt;..."" to update what will be committed)
#   (use ""git checkout -- &lt;file&gt;..."" to discard changes in working directory)
#
#       modified:   app/models/message_type.rb
#
no changes added to commit (use ""git add"" and/or ""git commit -a"")
baller@Laptop:~/rails/spunky-monkey$ git add app/models
</code></pre>

<p>And</p>

<pre class=""lang-none prettyprint-override""><code>baller@Laptop:~/rails/spunky-monkey$ git status
# On branch new-message-types
# Changes to be committed:
#   (use ""git reset HEAD &lt;file&gt;..."" to unstage)
#
#       modified:   app/models/message_type.rb
#
</code></pre>

<p>The output looks the same, but the information is totally different: the file has gone from unstaged to staged for commit.</p>

<p>Is there a way to colorize the output? For example, files that are unstaged are red, staged are green? </p>

<p>Or even <code>Changes not staged for commit:</code> to red and <code># Changes to be committed:</code> to green?</p>

<p>Working in Ubuntu.</p>

<p>EDIT: Googling found this answer which works great: <code>git config --global --add color.ui true</code>.</p>

<p>However, is there any more general solution for adding color to a command output?</p>
","<p>You can create a section <code>[color]</code> in your <code>~/.gitconfig</code> with e.g. the following content</p>

<pre><code>[color]
  diff = auto
  status = auto
  branch = auto
  interactive = auto
  ui = true
  pager = true
</code></pre>

<p>You can also fine control what you want to have coloured in what way, e.g.</p>

<pre><code>[color ""status""]
  added = green
  changed = red bold
  untracked = magenta bold

[color ""branch""]
  remote = yellow
</code></pre>

<p>I hope this gets you started. And of course, you need a terminal which supports colour.</p>
","44283"
"Why is USB not working in Linux when it works in UEFI/BIOS?","97170","","<p>For background I have just built a new machine with modern hardware including:</p>

<ul>
<li>AMD FX-8350</li>
<li>Gigabyte GA-990FXA-UD3 motherboard</li>
<li>16GB RAM</li>
<li>NVidia GTX 650 Ti</li>
<li>Kingston SSD</li>
</ul>

<p>Given that, I tried to install various versions of Linux on the SSD and was met with failure almost every time.  I tried installing Arch, Debian stable, Debian sid, and Ubuntu 12.10 from a USB thumb drive but while the BIOS saw the USB drive and started to boot from it, as soon as the OS attempted to enumerate the USB devices I lost all USB functionality (including the boot device).</p>

<p>Eventually I burned a DVD and installed Ubuntu 12.10 onto the SSD.  It should be noted that my USB keyboard (and mouse) work fine while in the American Megatrends UEFI/BIOS.  Even when I'm in the pre-installation menus on the Live Ubuntu DVD the keyboard works fine.</p>

<p>As soon as Linux is booted (either Live DVD or from the SSD) I lose all USB functionality and can only navigate the OS using a PS/2 keyboard.</p>

<p>What I see in the dmesg/syslog is a few lines about ""<code>failed to load microcode amd_ucode/microcode_amd_fam15h.bin</code>"" and I can see USB devices failing to initialize.</p>

<p>If I do an <code>lsusb</code> I can see all the USB host controllers but none of the devices.  Doing an <code>lspci</code> shows me all the hardware I'd expect.  And doing an <code>lsmod</code> I do not see any usb modules loaded (<code>usb_ehci</code> for example).</p>

<p>I tried passing <code>noapic</code> to the kernel boot string and it had no effect on this problem.</p>

<p>The motherboard supports USB 3.0 but all the devices I have plugged into normal USB 2.0 ports.</p>

<p>I'm rather baffled at what could be killing/preventing USB (and my on-board network card) from working <strong>in Linux</strong>.  There doesn't seem to be any problem with any of these devices working in BIOS and I do not have a Windows installation available to test and see if it works.</p>

<p>I've already RMA'd the motherboard once but the second one has exactly the same behavior so I think I can safely rule out hardware failure (since the behavior is identical, I don't think the odd of me getting two identically defective boards are greater than the odds of this being a Linux problem).</p>

<p>What else can I try to get USB (and ideally my network, but we'll stick to USB for now) working?</p>

<p><strong>Edit #1:</strong></p>

<p>Since I have no networking I can only relate interesting bits from <code>dmesg</code> here.</p>

<p>Of interest in <code>dmesg</code> I can see I have 11 USB host controllers (OHCI, EHCI, and xHCI).  It detects my USB devices and then fails immediately as follows:</p>

<pre><code>usb 3-1: new high-speed USB device number 2 using ehci_hcd
usb 3-1: device descriptor read/64, error -32
</code></pre>

<p>That repeats several times incrementing the number and trying other USB Host controllers until it falls back to OHCI controllers which also fail but have an additional message:</p>

<p><code>usb 8-1: device not accepting address 4, error -32</code></p>

<p>I think my networking problems have to do with the fact that I don't have IPv6 enabled on my router and that seems to be a problem</p>

<p><code>eth1: no IPv6 routers present</code></p>

<p><strong>Edit #2:</strong></p>

<p><code>lspci -vvv</code> shows that my network adapters (both onboard and expansion) are Realtek Semiconductor (no surprise); RTL8111/8168B and RTL8169/8110 respectively.  My USB controllers are Etron Technology EJ168 (xHCI) and AMD nee ATI SB7x0/SB8x0/SB9x0 (EHCI &amp; OHCI)</p>

<p>Now running Debian wheezy <code>modprobe</code> shows <code>usb_common</code>, <code>usbcore</code>, <code>xhci_hcd</code>, <code>ehci_hcd</code>, and <code>ohci_hcd</code> all loaded and functioning.</p>
","<p>I found the answer from this thread (<a href=""http://ubuntuforums.org/showthread.php?t=2114055"" rel=""noreferrer"">http://ubuntuforums.org/showthread.php?t=2114055</a>) over at ubuntuforums.org.</p>

<p>It seems with newer Gigabyte mainboards (at least) there is a BIOS option called <code>IOMMU Controller</code> that is disabled by default and gives no clue or indication as to what it is for.</p>

<p>Enabling this setting and rebooting ""magically"" restores all my USB and networking problems in a 64-bit Linux OS (doesn't matter which one).</p>

<p>I am rather shocked and elated that it was such a long search for such a simple fix.</p>

<p>Thanks everyone for your help and suggestions.  Hopefully others will find this helpful.</p>

<p><strong>Update:</strong> I'd just like to add that my current BIOS settings also include enabling XHCI Handoff and EHCI Handoff in addition to IOMMU Controller.  Others have mentioned this as well and enabling those two handoffs also allows my USB 3.0 ports to function as expected.</p>
","72698"
"Rsync filter: copying one pattern only","97104","","<p>I am trying to create a directory that will house all and only my PDFs compiled from LaTeX. I like keeping each project in a separate folder, all housed in a big folder called <code>LaTeX</code>. So I tried running:</p>

<pre><code>rsync -avn *.pdf ~/LaTeX/ ~/Output/
</code></pre>

<p>which should find all the pdfs in <code>~/LaTeX/</code> and transfer them to the output folder. This doesn't work. It tells me it's found no matches for ""<code>*.pdf</code>"". If I leave out this filter, the command lists all the files in all the project folders under LaTeX. So it's a problem with the *.pdf filter. I tried replacing <code>~/</code> with the full path to my home directory, but that didn't have an effect.</p>

<p>I'm, using zsh. I tried doing the same thing in bash and even <em>with</em> the filter that listed every single file in every subdirectory... What's going on here?</p>

<p>Why isn't rsync understanding my pdf only filter?</p>

<hr>

<p>OK. So update: No I'm trying</p>

<pre><code>rsync -avn --include=""*/"" --include=""*.pdf"" LaTeX/ Output/
</code></pre>

<p>And this gives me the whole file list. I guess because everything matches the first pattern...</p>
","<p>Rsync copies the source(s) to the destination. If you pass <code>*.pdf</code> as sources, the shell expands this to the list of files with the <code>.pdf</code> extension in the current directory. No recursive traversal happens because you didn't pass any directory as a source.</p>

<p>So you need to run <code>rsync -a ~/LaTeX/ ~/Output/</code>, but with a filter to tell rsync to copy <code>.pdf</code> files only. Rsync's filter rules can seem daunting when you read the manual, but you can construct many examples with just a few simple rules.</p>

<ul>
<li><p>Inclusions and exclusions:</p>

<ul>
<li>Excluding files by name or by location is easy: <code>--exclude=*~</code>, <code>--exclude=/some/relative/location</code> (relative to the source argument, e.g. this excludes <code>~/LaTeX/some/relative/location</code>).</li>
<li>If you only want to match a few files or locations, include them, <em>include every directory leading to them</em> (for example with <code>--include=*/</code>), then exclude the rest with <code>--exclude='*'</code>. This is because:</li>
<li>If you exclude a directory, this excludes everything below it. The excluded files won't be considered at all.</li>
<li>If you include a directory, this doesn't automatically include its contents. In recent versions, <code>--include='directory/***'</code> will do that.</li>
<li>For each file, the first matching rule applies (and anything never matched is included).</li>
</ul></li>
<li><p>Patterns:</p>

<ul>
<li>If a pattern doesn't contain a <code>/</code>, it applies to the file name sans directory.</li>
<li>If a pattern ends with <code>/</code>, it applies to directories only.</li>
<li>If a pattern starts with <code>/</code>, it applies to the whole path from the directory that was passed as an argument to <code>rsync</code>.</li>
<li><code>*</code> any substring of a single directory component (i.e. never matches <code>/</code>); <code>**</code> matches any path substring.</li>
</ul></li>
<li><p>If a source argument ends with a <code>/</code>, its contents are copied (<code>rsync -r a/ b</code> creates <code>b/foo</code> for every <code>a/foo</code>). Otherwise the directory itself is copied (<code>rsync -r a b</code> creates <code>b/a</code>).</p></li>
</ul>

<hr>

<p>Thus here we need to include <code>*.pdf</code>, include directories containing them, and exclude everything else.</p>

<pre><code>rsync -a --include='*.pdf' --include='*/' --exclude='*' ~/LaTeX/ ~/Output/
</code></pre>

<p>Note that this copies all directories, even the ones that contain no matching file or subdirectory containing one. This can be avoided with the <code>--prune-empty-dirs</code> option (it's not a universal solution since you then can't copy a directory even by matching it explicitly, but that's a rare requirement).</p>

<pre><code>rsync -am --include='*.pdf' --include='*/' --exclude='*' ~/LaTeX/ ~/Output/
</code></pre>
","2503"
"How to install program locally without sudo privileges?","97039","","<p>Assume I have ssh access to some Ubuntu server as user and I need some not system tools to be installed for convenience (mc, rtorrent, mcedit). I do not want to bother admins for these small programs. </p>

<p>Is there a way to install them (make them run) without using something like <code>sudo apt-get install</code>?</p>
","<ol>
<li><p>Compile and install into <code>~/bin</code> (and edit your <code>.bashrc</code> to set the <code>PATH</code> to include it).  libraries can similarly be compiled and installed into <code>~/lib</code> (set <code>LD_LIBRARY_PATH</code> to point to it), and development headers can be installed into e.g. <code>~/includes</code>.</p></li>
<li><p>Depending on the specific details of the programs you want to install and the libraries they depend upon, you can download the .deb files and use '<code>dpkg-deb -x</code>' to extract them underneath your home directory.  You will then have a lot of ""fun"" setting the <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, and other <code>variables</code>.  The more complex the program or app you're installing the more fun you'll be up for :)</p>

<p>You will, of course, not be able to install <code>setuid</code> binaries this way - they'll install but (since you don't have permission to chown them to root or set the <code>setuid</code> bit on them) they'll just be normal binaries owned by you.</p>

<p>Similarly, daemons and system services that expect to be running as a certain <code>UID</code> or have the ability to change uid, or expect files to be in <code>/etc</code> rather <code>~/etc</code> and so on aren't likely to work well, if at all.</p></li>
<li><p>Most sysadmins would consider <code>mc</code> and <code>mcedit</code> to be ""mostly harmless"", innocuous programs.</p>

<p>Very few, however, would consider installing a torrent client to be harmless, especially if they have to pay for bandwidth or end up being legally liable.  Most sysadmins would probably not be entirely happy for end-users to be installing such software without permission.  They may say ""sure, go ahead, knock yourself out"" or they may not...but you should ask about anything that may cause problems for the owners/administrators of the machine.</p></li>
</ol>
","42583"
"How does systemd use /etc/init.d scripts?","96891","","<p>I just switched to debian jessie, and most things run okay, including my graphical display manager <code>wdm</code>.</p>

<p>The thing is, I just don't understand how this works. Obviously my <code>/etc/init.d/wdm</code> script is called, because when I put an early <code>exit</code> in there, wdm is not not started. But when I alternatively rename the  /etc/rc3.d directory (my default runlevel used to be 3), then wdm is still started.</p>

<p>I could not find out how systemd finds this script and I do not understand what it does to all the other init.d scripts.</p>

<ul>
<li>When and how does systemd run init.d scrips?</li>
<li>In the long run, should I get rid of all init.d scripts?</li>
</ul>
","<p>chaos' answer is what some documentation says.  But it's not what systemd actually does.  (It's not what System V <code>rc</code> did, either.  Linux System V <code>rc</code> most definitely <em>did not</em> ignore LSB headers, which <code>insserv</code> used to calculate static orderings, for starters.)  The Freedesktop documentation, such as that ""Incompatibilities"" page, is in fact wrong, on these and other points.  (The <code>HOME</code> environment variable in fact <em>is</em> often set, for example.  This went wholly undocumented anywhere for a long time.  It's now documented in the manual, at least, but that Freedesktop WWW page still hasn't been corrected.)</p>

<p>The native service format for systemd is the <em>service unit</em>.  systemd's service management proper operates <em>solely</em> in terms of those, which it reads from one of nine directories where (system-wide) <code>.service</code> files can live.  <code>/etc/systemd/system</code>, <code>/run/systemd/system</code>, <code>/usr/local/lib/systemd/system</code>, and <code>/usr/lib/systemd/system</code> are four of those directories.</p>

<p>The compatibility with System V <code>rc</code> scripts is achieved with a conversion program, named <code>systemd-sysv-generator</code>.  This program is listed in the <code>/usr/lib/systemd/system-generators/</code> directory and is thus run automatically by systemd early in the bootstrap process at every boot, and again every time that systemd is instructed to re-load its configuration later on.</p>

<p>This program is a <em>generator</em>, a type of ancillary utility whose job is to create service unit files on the fly, in a tmpfs where three more of those nine directories (which are intended to be used only by generators) are located.  <code>systemd-sysv-generator</code> generates the service units that run the System V <code>rc</code> scripts from <code>/etc/init.d</code>, if it doesn't find a native systemd service unit by that name already existing in the other six locations.  </p>

<p>systemd service management only knows about service units.  These automatically (re-)generated service units are written to invoke the system 5 <code>rc</code> scripts.  They have, amongst other things: <pre>[Unit]
SourcePath=/etc/init.d/wibble
[Service]
ExecStart=/etc/init.d/wibble start
ExecStop=/etc/init.d/wibble stop</pre></p>

<p>Received wisdom is that the System V <code>rc</code> scripts must have an LSB header, and are run in parallel without honouring the priorities imposed by the <code>/etc/rc?.d/</code> system.  This is incorrect on all points.</p>

<p>In fact, they don't need to have an LSB header, and if they do not <code>systemd-sysv-generator</code> can recognize the more limited old RedHat comment headers (<code>description:</code>, <code>pidfile:</code>, and so forth).  Moreover, in the absence of an LSB header it will fall back to the contents of the <code>/etc/rc?.d</code> symbolic link farms, reading the priorities encoded into the link names and constructing a before/after ordering from them, serializing the services.  Not only are LSB headers not a requirement, and not only do they themselves encode before/after orderings that serialize things to an extent, the fallback behaviour in their complete absence is actually significantly non-parallelized operation.</p>

<p>The reason that <code>/etc/rc3.d</code> didn't appear to matter is that you probably had that script enabled via another <code>/etc/rc?.d/</code> directory.  <code>systemd-sysv-generator</code> translates being listed in any of <code>/etc/rc2.d/</code>, <code>/etc/rc3.d/</code>, and <code>/etc/rc4.d/</code> into a native <code>Wanted-By</code> relationship to systemd's <code>multi-user.target</code>.  Run levels are ""obsolete"" in the systemd world, and you can forget about them.</p>

<h1>Further reading</h1>

<ul>
<li><a href=""http://www.freedesktop.org/software/systemd/man/systemd-sysv-generator.html"" rel=""noreferrer""><em>systemd-sysv-generator</em></a>. systemd manual pages.  Freedesktop.org.</li>
<li><a href=""http://www.freedesktop.org/software/systemd/man/systemd.exec#Environment%20variables%20in%20spawned%20processes"" rel=""noreferrer"">""Environment variables in spawned processes""</a>. <em><code>systemd.exec</code></em>.  systemd manual pages.  Freedesktop.org.</li>
<li><a href=""https://unix.stackexchange.com/a/204075/5132"">https://unix.stackexchange.com/a/204075/5132</a></li>
<li><a href=""https://unix.stackexchange.com/a/196014/5132"">https://unix.stackexchange.com/a/196014/5132</a></li>
<li><a href=""https://unix.stackexchange.com/a/332797/5132"">https://unix.stackexchange.com/a/332797/5132</a></li>
</ul>
","233581"
"exit out of all SSH connections in one command and close PuTTY","96806","","<p>Is there a way to back out of all SSH connections and close <a href=""http://www.chiark.greenend.org.uk/~sgtatham/putty/"">PuTTY</a> in ""one shot""?  I work in Windows 7 and use PuTTY to SSH to various Linux hosts.</p>

<p>An example of the way I find myself working:</p>

<pre><code>SSH to host1 with PuTTY...
banjer@host1:~&gt; #...doin some work...ooh! need to go check something on host8...
banjer@host1:~&gt; ssh host8
banjer@host8:~&gt; #...doin some work...OK time for lunch. lets close putty...
banjer@host8:~&gt; exit
banjer@host1:~&gt; exit
Putty closes.
</code></pre>

<p>Per above, any way to get from host8 to closing PuTTY in one shot?  Sometimes I find myself up to 5 or 10 hosts deep.  I realize I can click the X to close the PuTTY window, but I like to make sure my SSH connections get closed properly by using the exit command.  I also realize I'm asking for tips on how to increase laziness.  I'll just write it off as ""how can I be more efficient"".</p>
","<p>Try using the <code>ssh</code> connection termination escape sequence.</p>

<p>In the <code>ssh</code> session, enter <code>~.</code> (tilde dot). You won't see the characters when you type them, but the session will terminate immediately.</p>

<pre><code>$ ~.
$ Connection to me.myhost.com closed.  
</code></pre>

<p>From <code>man 1 ssh</code></p>

<pre><code>The supported escapes (assuming the default ‘~’) are:
 ~.      Disconnect.
 ~^Z     Background ssh.
 ~#      List forwarded connections.
 ~&amp;      Background ssh at logout when waiting for forwarded 
         connection / X11 sessions to terminate.
 ~?      Display a list of escape characters.
 ~B      Send a BREAK to the remote system (only useful for SSH protocol
         version 2 and if the peer supports it).
 ~C      Open command line.  Currently this allows the addition of port 
         forwardings using the -L, -R and -D options (see above). It also
         allows the cancellation of existing remote port-forwardings using 
         -KR[bind_address:]port.  !command allows the user to execute a 
         local command if the PermitLocalCommand option is enabled in
         ssh_config(5).  Basic help is available, using the -h option.
 ~R      Request rekeying of the connection (only useful for SSH protocol 
         version 2 and if the peer supports it).
</code></pre>
","41683"
"How to use command line to change volume?","96414","","<p>I am trying to control the volume using my programming script. How can I do the following in Fedora 15, Ubuntu linux?</p>

<ol>
<li>Mute/ Unmute</li>
<li>Volume up and volume down</li>
</ol>

<p>Note: Please note that I use a web USB microphone/speaker and also Analogue microphone/speaker. I want to apply to all to be sure.</p>
","<p>You can use <a href=""http://tldp.org/HOWTO/Alsa-sound-6.html"">amixer</a>. It's in the alsa-utils package on ubuntu/debian.</p>

<p>Run it without parameters to get an overview about your devices.</p>

<pre><code>amixer
</code></pre>

<p>Then use the set command to set the volumn.
For example to set the master channel to 50%:</p>

<pre><code>amixer set Master 50%
</code></pre>

<p>Note the ""%"" sign, without it it will treat the value as 0 - 65536 level.</p>
","21090"
"How to check progress of running cp?","96111","","<p>Is it possible to check the progress of running cp process? Some processes respond to various KILL signals so that you can check what is their status. I know that I can run cp with parameter -v but what if forgot to do that, cp is running for a very long time and I want to know which file is being copied, or how many were already copied.</p>
","<p>Yes, by running stat on target file and local file, and get a file size,</p>

<p>i.e <code>stat -c ""%s"" /bin/ls</code> </p>

<p>And you get the percentage of data copied by comparing the two value, that's it</p>

<p>In a very basic implementation that will look like this:</p>

<pre class=""lang-sh prettyprint-override""><code>function cpstat()
{
  local pid=""${1:-$(pgrep -xn cp)}"" src dst
  [[ ""$pid"" ]] || return
  while [[ -f ""/proc/$pid/fd/3"" ]]; do
    read src dst &lt; &lt;(stat -L --printf '%s ' ""/proc/$pid/fd/""{3,4})
    (( src )) || break
    printf 'cp %d%%\r' $((dst*100/src))
    sleep 1
  done
  echo
}
</code></pre>
","66800"
"Can I change the font of terminal?","95924","","<p>I have a Unix-like OS installed without desktop. Is it possible to change the font of the terminal?</p>
","<p>If you use the Linux console, the best way I found is:</p>

<p>in</p>

<p><code>/etc/default/console-setup</code></p>

<p>put, for example</p>

<pre><code>CHARMAP=""UTF-8""
CODESET=""Lat7""
FONTFACE=""Terminus""
FONTSIZE=""28x14""
</code></pre>

<p>Another way is to use <a href=""http://www.kbd-project.org/manpages/man8/setfont.8.html"" rel=""nofollow noreferrer""><code>setfont</code></a> from the <a href=""http://www.kbd-project.org/"" rel=""nofollow noreferrer""><code>kbd</code></a> package:</p>

<p><code>setfont /usr/share/consolefonts/Lat7-Terminus28x14.psf</code></p>

<p>This works for my Debian; it may be different for you.</p>

<p>In Debian, you can also run <code>dpkg-reconfigure -plow console-setup</code> to be prompted for the various console settings and pick them from menus.</p>

<p><strong>Edit</strong> - I put together a small <a href=""http://user.it.uu.se/~embe8573/cols/www/index.html"" rel=""nofollow noreferrer"">page</a> how to setup the font <strong>colors</strong>. The section that is relevant for this post has the header ""the Linux VTs"" (= ttys, or ""console"").</p>
","49823"
"How to run grep with multiple AND patterns?","95908","","<p>I would like to get the multi pattern match with implicit <strong>AND</strong> between patterns, i.e. equivalent to running several greps in a sequence:</p>

<pre><code>grep pattern1 | grep pattern2 | ...
</code></pre>

<p>So how to convert it to something like?</p>

<pre><code>grep pattern1 &amp; pattern2 &amp; pattern3
</code></pre>

<p><em>I would like to use single grep because I am building arguments dynamically, so everything has to fit in one string. Using filter is system feature, not grep, so it is not an argument for it.</em></p>

<hr>

<p>Don't confuse this question with:</p>

<pre><code>grep ""pattern1\|pattern2\|...""
</code></pre>

<p>This is an <strong>OR</strong> multi pattern match.</p>
","<p><code>agrep</code> can do it with this syntax:</p>

<pre><code>agrep 'pattern1;pattern2'
</code></pre>

<p>With GNU <code>grep</code>, when built with PCRE support, you can do:</p>

<pre><code>grep -P '^(?=.*pattern1)(?=.*pattern2)'
</code></pre>

<p>With <a href=""https://github.com/att/ast/blob/master/src/cmd/re/grep.c"">ast <code>grep</code></a>:</p>

<pre><code>grep -X '.*pattern1.*&amp;.*pattern2.*'
</code></pre>

<p>(adding <code>.*</code>s as <code>&lt;x&gt;&amp;&lt;y&gt;</code> matches strings that match both <code>&lt;x&gt;</code> and <code>&lt;y&gt;</code> <em>exactly</em>, <code>a&amp;b</code> would never match as there's no such string that can <em>be</em> both <code>a</code> and <code>b</code> at the same time).</p>

<p>If the patterns don't overlap, you may also be able to do:</p>

<pre><code>grep -e 'pattern1.*pattern2' -e 'pattern2.*pattern1'
</code></pre>

<p>The best portable way is probably with <code>awk</code> as already mentioned:</p>

<pre><code>awk '/pattern1/ &amp;&amp; /pattern2/'
</code></pre>

<p>With <code>sed</code>:</p>

<pre><code>sed -e '/pattern1/!d' -e '/pattern2/!d'
</code></pre>

<p>Please beware that all those will have different regular expression syntax.</p>
","55391"
"Change gid of a specific group","95584","","<p>I'd like to change group id of a specific group. There are so may solution for changing the gid of a file or directories. But that's not what I want. Is there a way to do that?</p>
","<p>The GID is the primary identifier of the group. As far as the system is concerned, a different GID is a different group. So to change the GID, you're going to have to modify all the places where that GID is used. </p>

<p>You should avoid treating the GID as significant and use group names instead; you can change the name of a group with a single command (on Linux: <code>groupmod -n NEW_GROUP_NAME OLD_GROUP_NAME</code>). </p>

<p>However, if you do really want to change the GID, this is how:</p>

<ul>
<li>First, you may need to log out users in the group and kill processes who have that group as their effective, real or saved group.</li>
<li>Change the entry in the group database. On Linux, run <code>groupmod -g NEWGID GROUPNAME</code>. On other systems, use that system's administration tool, or <code>vigr</code> if available, or edit <code>/etc/group</code> as applicable.</li>
<li><p>Change the group of all the files on your system that belong to the old group.</p>

<pre><code>find / -gid OLDGID ! -type l -exec chgrp NEWGID {} \;
</code></pre></li>
<li><p>chgrp clears suid and sgid flags, restore those.</p></li>
<li>If you have any archive that uses the old GID, rebuild it.</li>
<li>If you have any configuration file or script that references the old GID, update it.</li>
<li>Restart all processes that must use the new GID.</li>
</ul>
","33874"
"Is there a tool to get the lines in one file that are not in another?","95068","","<p>Is there any tool that can get lines which file A contains, but file B doesn't? I could make a little simple script with, e.g, perl, but if something like that already exists, I'll save my time from now on.</p>
","<p>Yes. The standard <code>grep</code> tool for searching files for text strings can be used to subtract all the lines in one file from another.</p>

<pre><code>grep -F -x -v -f fileB fileA
</code></pre>

<p>This works by using each line in fileB as a pattern (<code>-f fileB</code>) and treating it as a plain string to match (not a regular regex) (<code>-F</code>). You force the match to happen on the whole line (<code>-x</code>) and print out only the lines that don't match (<code>-v</code>).  Therefore you are printing out the lines in fileA that don't contain the same data as any line in fileB.</p>

<p>The downside of this solution is that it doesn't take line order into account and if your input has duplicate lines in different places you might not get what you expect. The solution to that is to use a real comparison tool such as <code>diff</code>. You could do this by creating a diff file with the context value at 100% of the lines in the file, then parsing it for just the lines that would be removed if converting file A to file B. <sub>(Note this command also removes the diff formatting after it gets the right lines.)</sub></p>

<pre><code>diff -U $(wc -l &lt; fileA) fileA fileB | sed -n 's/^-//p' &gt; fileC
</code></pre>
","28159"
"Check if folder is a mounted remote filesystem","95011","","<p>What is the best way (reliable, portable, etc.) to check if a given folder is on a mounted remote (nfs) filesystem within a shell script?</p>

<p>I am looking for a command that would look like: </p>

<pre><code>chk-remote-mountpoint /my/path/to/folder 
</code></pre>
","<p>As <em>Stephane</em> says ""there is no universal Unix answer to that"". </p>

<p>The best solution I have found to my question: </p>

<pre><code>df -P -T /my/path/to/folder | tail -n +2 | awk '{print $2}'
</code></pre>

<p>will return the filesystem type, for example: <code>nfs</code> or <code>ext3</code>. </p>

<p>The <code>-T</code> option <strong><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/df.html"">is not</a> standard</strong>, so it may not work on other Unix/Linux systems... </p>

<p>According to <em>Gilles</em>' comment below: ""This works on any non-embedded Linux, but not on BusyBox, *BSD, etc.""</p>
","72224"
"What is `^M` and how do I get rid of it?","94985","","<p>When I open the file in vim I am seeing strange <code>^M</code> characters.</p>

<p>Unfortunately, the world's favorite search engine does not do well with special characters in queries, so I'm asking here:</p>

<ul>
<li>What is this <code>^M</code>?</li>
<li>How could it have got there?</li>
<li>How do I get rid of it?</li>
</ul>
","<p>The <code>^M</code> is a carriage-return character.  If you see this, you're probably looking at a file that originated in the DOS/Windows world, where an end-of-line is marked by a carriage return/newline pair, whereas in the Unix world, end-of-line is marked by a single newline.</p>

<p>Read <a href=""http://peterbenjamin.com/seminars/crossplatform/texteol.html"">this article</a> for more detail, and also the Wikipedia entry for <a href=""http://en.wikipedia.org/wiki/Newline"">newline</a>.</p>

<p><a href=""http://vim.wikia.com/wiki/File_format"">This article</a> discusses how to set up vim to transparently edit files with different end-of-line markers.</p>

<p>If you have a file with <code>^M</code> at the end of some lines and you want to get rid of them, use this in Vim:</p>

<pre><code>:s/^M$//
</code></pre>

<p>(Press <kbd>Ctrl</kbd>+<kbd>V</kbd> <kbd>Ctrl</kbd>+<kbd>M</kbd> to insert that <code>^M</code>.)</p>
","32003"
"Storing output of command in shell variable","94787","","<p>I have an operation using <code>cut</code> that I would like to assign result to a variable</p>

<pre><code>var4=echo ztemp.xml |cut -f1 -d '.'
</code></pre>

<p>I get the error:</p>

<blockquote>
  <p>ztemp.xml is not a command</p>
</blockquote>

<p>The value of <code>var4</code> never gets assigned; I'm trying to assign it the output of:</p>

<pre><code>echo ztemp.xml | cut -f1 -d '.'
</code></pre>

<p>How can I do that?</p>
","<p>You'll want to modify your assignment to read:</p>

<pre><code>var4=""$(echo ztemp.xml | cut -f1 -d '.')""
</code></pre>

<p>The <code>$(…)</code> construct is known as <a href=""http://en.wikipedia.org/wiki/Command_substitution"">command susbtitution</a>.</p>
","4570"
"grep on a variable","94437","","<p>Let's say I have a variable</p>

<pre><code>line=""This is where we select from a table.""
</code></pre>

<p>now I want to grep how many times does select occur in the sentence. </p>

<pre><code>grep -ci ""select"" $line
</code></pre>

<p>I tried that, but it did not work. I also tried </p>

<pre><code>grep -ci ""select"" ""$line""
</code></pre>

<p>It still doesn't work.  I get the following error. </p>

<pre><code>grep: This is where we select from a table.: No such file or directory
</code></pre>
","<p>Have <code>grep</code> read on its standard input. There you go, using <a href=""http://linuxcommand.org/lts0060.php"" rel=""noreferrer"">a pipe</a>...</p>

<pre><code>$ echo ""$line"" | grep select
</code></pre>

<p>... or <a href=""http://linux.die.net/abs-guide/x15683.html"" rel=""noreferrer"">a here string</a>...</p>

<pre><code>$ grep select &lt;&lt;&lt; ""$line""
</code></pre>

<p>Also, you might want to replace spaces by newlines before grepping :</p>

<pre><code>$ echo ""$line"" | tr ' ' '\n' | grep select
</code></pre>

<p>... or you could ask <code>grep</code> to print the match only:</p>

<pre><code>$ echo ""$line"" | grep -o select
</code></pre>

<p>This will allow you to get rid of the rest of the line when there's a match.</p>

<p><em>Edit:</em> Oops, read a little too fast, thanks <a href=""https://unix.stackexchange.com/users/12779/marco"">Marco</a>. In order to count the occurences, just pipe any of these to <a href=""http://linux.die.net/man/1/wc"" rel=""noreferrer""><code>wc(1)</code></a> ;)</p>

<p><em>Another edit made after <a href=""https://unix.stackexchange.com/users/12380/izkata"">lzkata</a>'s comment, quoting <code>$line</code> when using <code>echo</code>.</em></p>
","163814"
"How to check the Passive and Active FTP","94374","","<p>How do I check which FTP (Passive or Active) is running?</p>

<p>By default, passive FTP is running in linux, but how do I check?</p>
","<p>I found the answer as below.</p>

<p>in passive mode we can run <code>ls</code> command but in active mode we have to manually disable passive mode by typing <code>passive</code> command then it will accept <code>ls</code> command otherwise it's gives 550 permission denied error . see below (pasv_enable=NO in vsftpd.conf)</p>

<pre><code>ftp&gt; passive
Passive mode on.
ftp&gt; ls
550 Permission denied.
Passive mode refused.
ftp&gt; passive
Passive mode off.
ftp&gt; ls
200 PORT command successful. Consider using PASV.
150 Here comes the directory listing.
-rw-rw-r--    1 503      503             0 Jan 11  2013 files1
-rw-rw-r--    1 503      503             0 Jan 11  2013 files10
-rw-rw-r--    1 503      503             0 Jan 11  2013 files2
-rw-rw-r--    1 503      503             0 Jan 11  2013 files3
-rw-rw-r--    1 503      503             0 Jan 11  2013 files4
-rw-rw-r--    1 503      503             0 Jan 11  2013 files5
-rw-rw-r--    1 503      503             0 Jan 11  2013 files6
-rw-rw-r--    1 503      503             0 Jan 11  2013 files7
-rw-rw-r--    1 503      503             0 Jan 11  2013 files8
-rw-rw-r--    1 503      503             0 Jan 11  2013 files9
-rw-r--r--    1 0        0           10240 Jan 11  2013 test.tar
226 Directory send OK.
ftp&gt; ls
200 PORT command successful. Consider using PASV.
150 Here comes the directory listing.
-rw-rw-r--    1 503      503             0 Jan 11  2013 files1
-rw-rw-r--    1 503      503             0 Jan 11  2013 files10
-rw-rw-r--    1 503      503             0 Jan 11  2013 files2
-rw-rw-r--    1 503      503             0 Jan 11  2013 files3
-rw-rw-r--    1 503      503             0 Jan 11  2013 files4
-rw-rw-r--    1 503      503             0 Jan 11  2013 files5
-rw-rw-r--    1 503      503             0 Jan 11  2013 files6
-rw-rw-r--    1 503      503             0 Jan 11  2013 files7
-rw-rw-r--    1 503      503             0 Jan 11  2013 files8
-rw-rw-r--    1 503      503             0 Jan 11  2013 files9
-rw-r--r--    1 0        0           10240 Jan 11  2013 test.tar
226 Directory send OK.
</code></pre>

<p><code>ls</code> listing that we asked for on the server comes back over the port 20 on the server to a high port connection on the client. No use of port 21 on the server is made to send back the results of the ls command on the server.</p>

<p>above is extracted from ""http://www.markus-gattol.name/ws/vsftpd.html"" </p>
","58845"
"Convert file contents to lower case","94144","","<p>I have <code>temp</code> file with some lower-case and upper-case contents.</p>

<h2>Input</h2>

<p>Contents of my <code>temp</code> file:</p>

<pre class=""lang-none prettyprint-override""><code>hi
Jigar
GANDHI
jiga
</code></pre>

<p>I want to convert all <strong>upper to lower</strong>.</p>

<h2>Command</h2>

<p>I tried the following command:</p>

<pre><code>sed -e ""s/[A-Z]/[a-z]/g"" temp
</code></pre>

<p>but got wrong output.</p>

<h2>Output</h2>

<p>I want it as:</p>

<pre><code>hi
jigar
gandhi
jiga
</code></pre>

<p>What needs to be in the <em>substitute</em> part of argument for <code>sed</code>?</p>
","<p>If your input only contains ASCII characters, you could use <code>tr</code> like:</p>

<pre><code>tr A-Z a-z &lt; input 
</code></pre>

<p>or (less easy to remember and type IMO, but not limited to ASCII latin letters):</p>

<pre><code>tr '[:upper:]' '[:lower:]' &lt; input
</code></pre>

<p>if you have to use <code>sed</code>:</p>

<pre><code>sed 's/.*/\L\1/g' &lt; input
</code></pre>

<p>(here assuming the GNU implementation).</p>
","171604"
"How do I set the password of a new user after the account has already been created?","94079","","<p>I used the 'useradd' command to create a new account, but I did so without specifying the password.  Now, when the user tries to log in, it asks him for a password.  If I didn't set it up initially, how do I set the password now?</p>
","<p>Easiest way to do this from the command line is to use the <code>passwd</code> command with root privileges.</p>

<p><code>passwd username</code></p>

<p>From <code>man 1 passwd</code></p>

<pre><code>NAME
       passwd - update user's authentication token
SYNOPSIS
       passwd  [-k]  [-l]  [-u [-f]] [-d] [-n mindays] [-x maxdays]
       [-w warndays] [-i inactivedays] [-S] [--stdin] [username]
DESCRIPTION
       The passwd utility is used to update user's authentication token(s).
</code></pre>

<p>After you set the user password, you can force the user to change it on next login using the <code>chage</code> command (also with root privileges) which expires the password.</p>

<p><code>chage -d 0 username</code></p>

<p>When the user successfully authenticates with the password you set, the user will automatically be prompted to change it. After a successful password change, the user will be disconnected, forcing re-authentication with the new password.</p>

<p>See <code>man 1 chage</code> for more information on password expiry.</p>
","42400"
"Linux: Difference between /dev/console , /dev/tty and /dev/tty0","94040","","<p>On a Linux system, what is the difference between <code>/dev/console</code>, <code>/dev/tty</code> and <code>/dev/tty0</code>?</p>

<p>What is their respective use and how do they compare?</p>
","<p>From the <a href=""http://www.kernel.org/doc/Documentation/devices.txt"">documentation</a>:</p>

<pre><code>/dev/tty        Current TTY device
/dev/console    System console
/dev/tty0       Current virtual console
</code></pre>

<p>In the good old days <code>/dev/console</code> was System Administrator console. And TTYs were users' serial devices attached to a server.
Now <code>/dev/console</code> and <code>/dev/tty0</code> represent current display and usually are the same. You can override it for example by adding <code>console=ttyS0</code> to <code>grub.conf</code>. After that your <code>/dev/tty0</code> is a monitor and <code>/dev/console</code> is <code>/dev/ttyS0</code>.</p>

<p>An exercise to show the difference between <code>/dev/tty</code> and <code>/dev/tty0</code>:</p>

<p>Switch to the 2nd console by pressing <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>F2</kbd>. Login as <code>root</code>. Type <code>sleep 5; echo tty0 &gt; /dev/tty0</code>. Press <kbd>Enter</kbd> and switch to the 3rd console by pressing <kbd>Alt</kbd>+<kbd>F3</kbd>.
Now switch back to the 2nd console by pressing <kbd>Alt</kbd>+<kbd>F2</kbd>. Type <code>sleep 5; echo tty &gt; /dev/tty</code>, press <kbd>Enter</kbd> and switch to the 3rd console.</p>

<p>You can see that <code>tty</code> is the console where process starts, and <code>tty0</code> is a always current console.</p>
","60649"
"How to install CentOS 6 via USB mass storage device?","93762","","<p>I want to install CentOS 6.2 on a laptop (Thinkpad R40) which comes without CD/DVD-drive but with USB 2.0 ports.</p>

<p>It seems that CentOS does not provide ready-to-use dd-able USB images for installation.</p>

<p>Thus my question: How to install CentOS via a USB device (e.g. a 16 GB USB flash drive)?</p>

<p>Regarding using different available iso-images as base: the laptop has net-access - but I want to make sure that the CentOS installer is not loading unchecked packages from the net during installation - perhaps a netinstall image does not check cryptographically signed packages during installation (<a href=""https://unix.stackexchange.com/questions/3817/are-packages-cryptographically-signed-in-fedora-14"">as with the Fedora 14 installer</a>).</p>

<p>There is a <a href=""http://wiki.centos.org/HowTos/InstallFromUSBkey"" rel=""nofollow noreferrer"">CentOS InstallFromUSBkey</a> which just provides outdated, cryptic and wrong information.</p>

<p>Especially, the instructions for CentOS 6 are missing details and contain errors (10 MB for the first partition is not enough, <code>syslinux device</code> fails and what are they talking about grub?)</p>
","<p>Following method works with CentOS 6.2:</p>

<p>Requirements: USB flash drive (at least 4 GB, I used a 16 GB one)</p>

<p>Download an ISO image from a mirror - I chose the full 1st DVD image to avoid a network install (because it is not clear if the cryptographic package signatures are checked by the installer or not), e.g.:</p>

<pre><code>$ wget http://ftp.uni-bayreuth.de/linux/CentOS/6.2/isos/i386/CentOS-6.2-i386-bin-DVD1.iso
$ md5sum CentOS-6.2-i386-bin-DVD1.iso
</code></pre>

<p>Check the md5sum against a <code>md5sum.txt</code> file from another mirror (and check <code>md5sum.txt</code> against <code>md5sum.txt.asc</code> via <code>gpg</code>).</p>

<p>Partition your flash drive (say it is <code>/dev/sdb</code>), i.e. delete all partitions, create just one, set the boot-flag and perhaps the FS-type:</p>

<pre><code># dd if=/dev/zero of=/dev/sdb bs=512 count=1
# fdisk /dev/sdb
&gt; n
&gt; p
&gt; 1
(defaults)
&gt; a
&gt; 1
(toggles boot flag)
&gt; t
&gt; c
(filesystem type, default is 83, probably no need to change it)
&gt; w
(write the new table)
</code></pre>

<p>Create a filesystem of type VFAT:</p>

<pre><code># mkfs.vfat /dev/sdb1
</code></pre>

<p>Fetch the Fedora-LiveCD tools:</p>

<pre><code>$ git clone git://git.fedorahosted.org/livecd
</code></pre>

<p>(We need <code>livecd/tools/livecd-iso-to-disk.sh</code> - it also supports <strong>non</strong>-livecd ISO-images as source!)</p>

<p>Install some packages needed by the script, e.g. under a Debian-like system:</p>

<pre><code># apt-get install isomd5sum syslinux extlinux
</code></pre>

<p>Execute the script:</p>

<pre><code># bash livecd-iso-to-disk.sh CentOS-6.2-i386-bin-DVD1.iso /dev/sdb1
</code></pre>

<p>Test the device:</p>

<pre><code>$ qemu -hda /dev/sdb -m 256 -vga std
</code></pre>

<p>For this to work you user (temporarily) needs <code>rw</code> permissions on <code>/dev/sdb</code>.</p>

<p>PS: As a side node, RHEL 6 has dropped support for non PAE hardware - i.e. the kernel does not run on such an old system like a Thinkpad R40 (which is Centrino based).</p>
","29158"
"Trying to sort on two fields, second then first","93687","","<p>I am trying to sort on multiple columns. The results are not as expected.</p>

<p>Here's my data (people.txt):</p>

<pre><code>Simon Strange 62
Pete Brown 37
Mark Brown 46
Stefan Heinz 52
Tony Bedford 50
John Strange 51
Fred Bloggs 22
James Bedford 21
Emily Bedford 18
Ana Villamor 44
Alice Villamor 50
Francis Chepstow 56
</code></pre>

<p>The following works correctly:</p>

<pre><code>bash-3.2$ sort -k2 -k3 &lt;people.txt                                                                                                                    
Emily Bedford 18                                                                                                                                      
James Bedford 21                                                                                                                                      
Tony Bedford 50                                                                                                                                       
Fred Bloggs 22                                                                                                                                        
Pete Brown 37                                                                                                                                         
Mark Brown 46                                                                                                                                         
Francis Chepstow 56                                                                                                                                   
Stefan Heinz 52                                                                                                                                       
John Strange 51                                                                                                                                       
Simon Strange 62                                                                                                                                      
Ana Villamor 44                                                                                                                                       
Alice Villamor 50
</code></pre>

<p>But, the following does not work as expected:</p>

<pre><code>bash-3.2$ sort -k2 -k1 &lt;people.txt                                        
Emily Bedford 18                                                                                                                                      
James Bedford 21                                                                                                                                      
Tony Bedford 50                                                                                                                                       
Fred Bloggs 22                                                                                                                                        
Pete Brown 37                                                                                                                                         
Mark Brown 46                                                                                                                                         
Francis Chepstow 56                                                                                                                                   
Stefan Heinz 52                                                                                                                                       
John Strange 51                                                                                                                                       
Simon Strange 62                                                                                                                                      
Ana Villamor 44                                                                                                                                       
Alice Villamor 50
</code></pre>

<p>I was trying to sort by surname and then by first name, but you will see the Villamors are not in the correct order. I was hoping to sort by surname, and then when surnames matched, to sort by first name.</p>

<p>It seems there is something about how this should work I don't understand. I could do this another way of course (using awk), but I want to understand sort.</p>

<p>I am using the standard Bash shell on Mac OS X.</p>
","<p>A key specification like <code>-k2</code> means to take all the fields from 2 to the end of the line into account. So <code>Villamor 44</code> ends up before <code>Villamor 50</code>. Since these two are not equal, the first comparison in <code>sort -k2 -k1</code> is enough to discriminate these two lines, and the second sort key <code>-k1</code> is not invoked. If the two Villamors had had the same age, <code>-k1</code> would have caused them to be sorted by first name.</p>

<p>To sort by a single column, use <code>-k2,2</code> as the key specification. This means to use the fields from #2 to #2, i.e. only the second field.</p>

<p><code>sort -k2 -k3 &lt;people.txt</code> is redundant: it's equivalent to <code>sort -k2 &lt;people.txt</code>. To sort by last names, then first names, then age, run</p>

<pre><code>sort -k2,2 -k1,1 &lt;people.txt
</code></pre>

<p>or equivalently <code>sort -k2,2 -k1 &lt;people.txt</code> since there are only these three fields and the separators are the same. In fact, you will get the same effect from <code>sort -k2,2 &lt;people.txt</code>, because <code>sort</code> uses the whole line as a last resort when all the keys in a subset of lines are identical.</p>

<p>Also note that the default field separator is the transition between a non-blank and a blank, so the keys will include the leading blanks (in your example, for the first line, the first key will be <code>""Emily""</code>, but the second key <code>"" Bedford""</code>. Add the <code>-b</code> option to strip those blanks:</p>

<pre><code>sort -b -k2,2 -k1,1
</code></pre>

<p>It can also be done on a per-key basis by adding the <code>b</code> flag at the end of the key start specification:</p>

<pre><code>sort -k2b,2 -k1,1 &lt;people.txt
</code></pre>

<p>But something to bear in mind: as soon as you add one such flag to the key specification, the global flags (like <code>-n</code>, <code>-r</code>...) no longer apply to them so it's better to avoid mixing per-key flags and global flags.</p>
","52819"
"How to escape quotes in shell?","93628","","<p>I'm having trouble with escaping characters in bash. I'd like to escape single and double quotes while running a command under a different user. For the purposes of this question let's say I want to echo the following on the screen:</p>

<pre><code>'single quote phrase' ""double quote phrase""
</code></pre>

<p>How can I escape all the special chars, if I also need to switch to a different user:</p>

<pre><code>sudo su USER -c ""echo \""'single quote phrase' \""double quote phrase\""\""""
</code></pre>

<p>Of course, this doesn't produce the right result.</p>
","<p>You can use the following string literal syntax:</p>

<pre><code>&gt; echo $'\'single quote phrase\' ""double quote phrase""'
'single quote phrase' ""double quote phrase""
</code></pre>

<h3>From <code>man bash</code></h3>

<blockquote>
  <p>Words of the form $'string' are treated specially.  The word expands
  to string, with backslash-escaped characters replaced as specified  by
  the
         ANSI C standard.  Backslash escape sequences, if present, are decoded as follows:</p>

<pre><code>          \a     alert (bell)
          \b     backspace
          \e
          \E     an escape character
          \f     form feed
          \n     new line
          \r     carriage return
          \t     horizontal tab
          \v     vertical tab
          \\     backslash
          \'     single quote
          \""     double quote
          \nnn   the eight-bit character whose value is the octal value nnn (one to three digits)
          \xHH   the eight-bit character whose value is the hexadecimal value HH (one or two hex digits)
          \cx    a control-x character
</code></pre>
</blockquote>
","30904"
"How to split window vertically in GNU Screen?","93498","","<p>I saw some body split their window to 2x2, I just want to know how to do that? I know the 'split' command in Screen can only split the window horizontally.</p>
","<p>GNU Screen &lt;4.01 may not support vertical split without a patch.<br>
GNU Screen >4.01 supports vertical splitting.</p>

<p>The <a href=""http://fungi.yuggoth.org/vsp4s/"" rel=""nofollow noreferrer"">Patch</a> is licensed under GPLv2. Some people say that the vertical split in GNU <code>screen</code> makes the application slow but I haven't tested. I use <a href=""https://tmux.github.io/"" rel=""nofollow noreferrer""><code>tmux</code></a> (<em>terminal multiplexer</em>)</p>
","26786"
"What is a Superblock, Inode, Dentry and a File?","93485","","<p>From the article <a href=""http://web.archive.org/web/20150505112327/http://www.ibm.com/developerworks/linux/library/l-linux-filesystem/"">Anatomy of the Linux file system</a> by M. Tim Jones, I read that Linux views all the file systems from the perspective of a common set of objects and these objects are <strong><em>superblock</em></strong>, <strong><em>inode</em></strong>, <strong><em>dentry</em></strong> and <strong><em>file</em></strong>. Even though the rest of the paragraph explains the above, I was not that comfortable with that explanation.</p>

<p>Could somebody explain to me these terms?</p>
","<p>First and foremost, and I realize that it was not one of the terms from your question, you must understand <em>metadata</em>. Succinctly, and stolen from Wikipedia, metadata is data about data. That is to say that metadata contains information about a piece of data. For example, if I own a car then I have a set of information about the car but which is not part of the car itself. Information such as the registration number, make, model, year of manufacture, insurance information, and so on. All of that information is collectively referred to as the metadata. In Linux and UNIX file systems metadata exists at multiple levels of organization as you will see.</p>

<p>The <strong>superblock</strong> is essentially file system metadata and defines the file system type, size, status, and information about other metadata structures (metadata of metadata). The superblock is very critical to the file system and therefore is stored in multiple redundant copies for each file system. The superblock is a very ""high level"" metadata structure for the file system. For example, if the superblock of a partition, /var, becomes corrupt then the file system in question (/var) cannot be mounted by the operating system. Commonly in this event, you need to run <code>fsck</code> which will automatically select an alternate, backup copy of the superblock and attempt to recover the file system. The backup copies themselves are stored in block groups spread through the file system with the first stored at a 1 block offset from the start of the partition. This is important in the event that a manual recovery is necessary. You may view information about superblock backups with the command <code>dumpe2fs /dev/foo | grep -i superblock</code> which is useful in the event of a manual recovery attempt. Let us suppose that the dumpe2fs command outputs the line <code>Backup superblock at 163840, Group descriptors at 163841-163841</code>. We can use this information, and additional knowledge about the file system structure, to attempt to use this superblock backup: <code>/sbin/fsck.ext3 -b 163840 -B 1024 /dev/foo</code>. Please note that I have assumed a block size of 1024 bytes for this example.</p>

<p>An <strong>inode</strong> exists in, or on, a file system and represents metadata about a file. For clarity, all objects in a Linux or UNIX system are files; actual files, directories, devices, and so on. Please note that, among the metadata contained in an inode, there is no file name as humans think of it, this will be important later. An inode contains essentially information about ownership (user, group), access mode (read, write, execute permissions) and file type.</p>

<p>A <strong>dentry</strong> is the glue that holds inodes and files together by relating inode numbers to file names. Dentries also play a role in directory caching which, ideally, keeps the most frequently used files on-hand for faster access. File system traversal is another aspect of the dentry as it maintains a relationship between directories and their files.</p>

<p>A <strong>file</strong>, in addition to being what humans typically think of when presented with the word, is really just a block of logically related arbitrary data. Comparatively very dull considering all of the work done (above) to keep track of them.</p>

<p>I fully realize that a few sentences do not provide a full explanation of any of these concepts so please feel free to ask for additional details when and where necessary.</p>
","4403"
"How do I count all the files recursively through directories","93438","","<p>I want to see how many files are in subdirectories to find out where all the inode usage is on the system. Kind of like I would do this for space usage</p>

<pre><code>du -sh /*
</code></pre>

<p>which will give me the space used in the directories off of root, but in this case I want the number of files, not the size.</p>
","<pre><code>find -maxdepth 1 -type d | while read -r dir; do printf ""%s:\t"" ""$dir""; find ""$dir"" -type f | wc -l; done
</code></pre>

<p>Thanks to Gilles and xenoterracide for safety/compatibility fixes.</p>

<p>The first part: <code>find -maxdepth 1 -type d</code> will return a list of all directories in the current working directory. This is piped to...</p>

<p>The second part: <code>while read -r dir; do</code> begins a while loop - as long as the pipe coming into the while is open (which is until the entire list of directories is sent), the read command will place the next line into the variable ""dir"". Then it continues...</p>

<p>The third part: <code>printf ""%s:\t"" ""$dir"";</code> will print the string in ""$dir"" (which is holding one of the directory names) followed by a tab.</p>

<p>The fourth part: <code>find ""$dir -f file""</code> makes a list of all the files inside the directory name held in ""$dir"". This list is sent to..</p>

<p>The fifth part: <code>wc -l;</code> counts the number of lines that are sent into its standard input.</p>

<p>The final part: <code>done</code> simply ends the while loop.</p>

<p>So we get a list of all the directories in the current directory. For each of those directories, we generate a list of all the files in it so that we can count them all using <code>wc -l</code>. The result will look like:</p>

<pre><code>./dir1: 234
./dir2: 11
./dir3: 2199
...
</code></pre>
","4176"
"What does ""--"" (double-dash) mean? (also known as ""bare double dash"")","93286","","<p>I have seen <code>--</code> used in the <code>compgen</code> command.</p>

<p>For example:</p>

<pre><code>compgen -W ""foo bar baz"" -- b
</code></pre>

<p>What is the meaning of the <code>--</code> in there?</p>
","<p>More precisely, a double dash (<code>--</code>) is used in bash built-in commands and many other commands to signify the end of command options, after which only positional parameters are accepted.</p>

<p>Example use: lets say you want to grep a file for the string <code>-v</code> - normally <code>-v</code> will be considered the option to reverse the matching meaning (only show lines that do not match), but with <code>--</code> you can grep for string <code>-v</code> like this:</p>

<pre><code>grep -- -v file
</code></pre>
","11382"
"unexpected EOF while looking for matching `""' - bash script","93168","","<p>I just wrote a bash script (im a Newbie to shell-scripting ;) ) and always getting this EOF-Error.
So here is my script (only works on OS X):</p>

<pre><code>#!/bin/bash

#DEFINITIONS BEGIN
en_sq() {
    echo -e ""Enabling smart quotes...""
    defaults write NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool true
    status=$(defaults read NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool)
            if [ ""$status"" = ""1"" ]
                then
                    echo -e ""Success! Smart quotes are now enabled.""
                    SUCCESS=""TRUE""
            else
                echo -e ""Sorry, an error occured. Try again.""
            fi
}
di_sq() {
    echo -e ""Disabling smart quotes...""
    defaults write NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool false
    status=$(defaults read NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool)
            if [ ""$status"" = ""0"" ]
                then
                    echo -e ""Success! Smart quotes are now disabled.""
                    SUCCESS=""TRUE""
            else
                echo -e ""Sorry, an error occured. Try again.""
            fi
}
en_sd() {
    echo -e ""Enabling smart dashes...""
    defaults write NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool true
    status=$(defaults read NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool)
            if [ ""$status"" = ""1"" ]
                then
                    echo -e ""Success! Smart dashes are now enabled.""
                    SUCCESS=""TRUE""
            else
                echo -e ""Sorry, an error occured. Try again.""
            fi
}
di_sd() {
    echo -e ""Enabling smart dashes...""
    defaults write NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool false
    status=$(defaults read NSGlobalDomain NSAutomaticDashSubstitutionEnabled -bool)
            if [ ""$status"" = ""0"" ]
                then
                    echo -e ""Success! Smart dashes are now disabled.""
                    SUCCESS=""TRUE""
            else
                echo -e ""Sorry, an error occured. Try again.""
            fi
}
#DEFINITIONS END
#---------------

#BEGIN OF CODE with properties
#This is only terminated if the user entered properties (eg ./sqd.sh 1 1)
if [ ""$1"" = ""1"" ]
    then
        en_sq
    elif [ ""$1"" = ""0"" ]
        then
            di_sq
fi

if [ ""$2"" = ""1"" ]
    then
        en_sd
        #exit 0 if both, $1 and $2 are correct entered and processed.
        exit 0
    elif [ ""$1"" = ""0"" ]
        then
            di_sd
            #exit 0 if both, $1 and $2 are correct entered and processed.
            exit 0
fi
#END OF CODE with properties
#---------------------------


#BEGIN OF CODE without properties
#This is terminated if the user didn't enter two properties
echo -e ""\n\n\n\n\nINFO: You can use this command as following: $0 x y, while x and y can be either 0 for false or 1 for true.""
echo -e ""x is for the smart quotes, y for the smart dashes.""
sleep 1
echo -e "" \n Reading preferences...\n""
status=$(defaults read NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool)
if [ ""$status"" = ""1"" ]
    then
        echo -e ""Smart quotes are enabled.""
    elif [ ""$status"" = ""0"" ]
    then
        echo -e ""Smart quotes are disabled.""

    else
        echo -e ""Sorry, an error occured. You have to run this on OS X""""
fi

status=$(defaults read NSGlobalDomain NSAutomaticQuoteSubstitutionEnabled -bool)
if [ ""$status"" = ""1"" ]
    then
        echo -e ""Smart dashes are enabled.""
    elif [ ""$status"" = ""0"" ]
    then
        echo -e ""Smart dashes are disabled.""

    else
        echo -e ""Sorry, an error occured. You have to run this on OS X!""
fi

sleep 3
echo -e ""\n\n You can now enable or disable smart quotes.""

until [ ""$SUCCESS"" = ""TRUE"" ]
do
echo -e ""Enter e for enable or d for disable:""
read sq

if [ ""$sq"" = ""e"" ]
    then
        en_sq
    elif [ ""$sq"" = ""d"" ]
        then
            di_sq
    else
        echo -e ""\n\n ERROR! Please enter e for enable or d for disable!""
fi
done
SUCCESS=""FALSE""

echo -e ""\n\n You can now enable or disable smart dashes.""

until [ ""$SUCCESS"" = ""TRUE"" ]
do
echo -e ""Enter e for enable or d for disable:""
read sq

if [ ""$sd"" = ""e"" ]
    then
        en_sd
    elif [ ""$sd"" = ""d"" ]
        then
            di_sd
    else
        echo -e ""\n\n ERROR! Please enter e for enable or d for disable!""
fi
done
</code></pre>

<p>And here is my error:</p>

<pre><code>./coding.sh: line 144: unexpected EOF while looking for matching `""'
./coding.sh: line 147: syntax error: unexpected end of file
</code></pre>
","<p>You can see your problem if you just look at your question. Note how the syntax highlighting is screwed up after line 95:</p>

<pre><code>echo -e ""Sorry, an error occurred. You have to run this on OS X""""
</code></pre>

<p>As the error message tells you, you have an unmatched <code>""</code>. Just remove the extra <code>""</code> from the line above and you should be fine:</p>

<pre><code>echo -e ""Sorry, an error occurred. You have to run this on OS X""
</code></pre>
","154430"
"How to read/write to tty* device?","93138","","<p>I have a device that sends information over USB to my computer. Arch Linux sets up this device by creating a file named <code>ttyUSB0</code> in <code>/dev/</code>. I have been using <code>GTKterm</code> to receive this incoming information and display it in an emulated terminal window.</p>

<p>My question is, how exactly does <code>GTKterm</code> read/write to this <code>ttyUSB0</code> file, and where might I start learning how to implement similar functionality? That is, in the most basic form, how might I write a character to <code>ttyUSB0</code>, or, in contrast, receive a byte and write it to a file?</p>
","<p>TTYs are files that you can use just like any other. You can open them with the standard file-opening tools of your language and read or write from them. They have some special behaviour that's different to ""ordinary"" files, but the basics are the same. I'll cover some of the special cases at the end, but first, an experiment.</p>

<p>One interesting thing you can do straight from a regular terminal. Run <code>tty</code> and it will print a line like:</p>

<pre><code>/dev/pts/2
</code></pre>

<p>That's the TTY device your terminal is running in. You can write something to that terminal:</p>

<pre><code>$ echo Hello &gt; /dev/pts/2
Hello
$
</code></pre>

<p>You can even read from it:</p>

<pre><code>$ read X &lt; /dev/pts/2
hello
$ echo $X
hello
$
</code></pre>

<p>(<code>read X</code> is sh's ""read a line from standard input into variable X"" command; the &lt; is to use /dev/pts/2 as standard input for the read command; the first ""hello"" I typed, and the second was printed out).</p>

<p>If you open up another shell, say by using <code>screen</code> or <code>xterm</code>, you can run run <code>echo spooky &gt; /dev/pts/2</code> in that shell to make the text appear on your original terminal, and the same for the other commands. All of this is just your shell opening a file without knowing it's a TTY.</p>

<hr>

<p>Here is a very simple C program that does just what you asked, and writes a single character to /dev/pts/3, then reads a single byte back from it:</p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;unistd.h&gt;
#include &lt;fcntl.h&gt;    
int main() {
    char byte;
    int fd = open(""/dev/pts/3"", O_RDWR);
    write(fd, ""X"", 1);
    ssize_t size = read(fd, &amp;byte, 1);
    printf(""Read byte %c\n"", byte);
    return 0;
}
</code></pre>

<p>A real TTY device that's attached to a shell or terminal emulator will have interesting behaviour there, but you should get something back.</p>

<hr>

<p>To access a terminal you need to have permission to use it. Those are just the standard file permissions you see with <code>ls -l</code> and set with <code>chmod</code>: you need read permission to open the file and read it, and write permission to write into it. The TTYs that back your terminal will be owned by you, but another user's TTY won't, and TTYs for USB devices may or may not be, depending on your configuration. You can change the permissions in the same way as always.</p>

<p>As far as writing a program to work with it goes, you don't need to do much special. You can see in the example that one thing you <strong>don't</strong> need to do is close the file every time to have your data read by the other end: the TTY files act like pipelines, just pushing data in both directions as it comes in. When I wrote text to the TTY it appeared immediately, and when I read from it afterwards there wasn't anything waiting for me already. It's <strong>not</strong> like writing to a regular file where the data gets saved on disk - it gets passed on immediately to the other side, or stored in memory until someone reads it.</p>

<p>You may want to use the <a href=""http://linux.die.net/man/3/select"" rel=""noreferrer"">select</a> function so that you can do other things while you wait for the device to say something, but if you're happy to just wait for data to come through you can just use blocking reads and let the OS do the lifting.</p>

<p>One thing to keep in mind is that there is can be limited buffer size in the kernel, and if you write a lot of data at once you may end up blocking without meaning to. If that's likely to be a problem, use <a href=""http://linux.die.net/man/3/open"" rel=""noreferrer"">non-blocking IO</a> with <code>open(""/dev/..."", O_RDWR | O_NONBLOCK)</code>. The principle will be the same either way.</p>
","138390"
"How can I start the python SimpleHTTPServer on port 80?","93109","","<p>I am using this command on Ubuntu but it is starting on port 8080 and I don't have another server running so I'd like it to start on port 80. I saw ways that you could set up a bash script to do something like this, but isn't there a command line flag or something simpler to specify the port?</p>

<pre><code>python -m SimpleHTTPServer
</code></pre>
","<p><code>sudo python -m SimpleHTTPServer 80</code></p>

<p>for <strong>python 3.x</strong> version, you may need :</p>

<p><code>sudo python -m http.server 80</code></p>

<p>Ports below 1024 require root privileges. </p>

<p>As George added in a comment, running this command as root is not a good idea - <strong>it opens up all kinds of security vulnerabilities.</strong></p>

<p>However, it answers the question.</p>
","24599"
"What is the difference between Docker, LXD, and LXC","92974","","<p>What is the difference between Docker, LXD,  and LXC. Do they offer the same services or different.</p>
","<p>No, LXC, Docker, and LXD, are not quite the same. In short:</p>

<h2>LXC</h2>

<blockquote>
  <p>LinuX Containers (LXC) is an operating system-level virtualization method for running multiple isolated Linux systems (containers) on a single control host (LXC host)</p>
</blockquote>

<p><a href=""https://wiki.archlinux.org/index.php/Linux_Containers"" rel=""noreferrer"">https://wiki.archlinux.org/index.php/Linux_Containers</a></p>

<blockquote>
  <p>low level ...</p>
</blockquote>

<p><a href=""https://linuxcontainers.org/"" rel=""noreferrer"">https://linuxcontainers.org/</a></p>

<h2>Docker</h2>

<ul>
<li>by Docker, Inc</li>
<li>a container system making use of LXC containers</li>
<li>so you can: <code>Build, Ship, and Run Any App, Anywhere</code> <a href=""http://www.docker.com"" rel=""noreferrer"">http://www.docker.com</a></li>
</ul>

<h2>LXD</h2>

<ul>
<li><a href=""https://linuxcontainers.org/lxd/"" rel=""noreferrer"">by Canonical, Ltd</a></li>
<li>a <a href=""http://www.serverwatch.com/server-trends/docker-not-the-only-container-option-in-2015.html"" rel=""noreferrer"">container system making use of LXC containers</a></li>
<li>so that you can: <code>run LXD on Ubuntu and spin up instances of RHEL, CentOS, SUSE, Debian, Ubuntu and just about any other Linux too, instantly, ...</code> <a href=""http://www.zdnet.com/article/ubuntu-lxd-not-a-docker-replacement-a-docker-enhancement/"" rel=""noreferrer"">http://www.zdnet.com/article/ubuntu-lxd-not-a-docker-replacement-a-docker-enhancement/</a></li>
</ul>

<h2>Docker vs LXD</h2>

<ul>
<li>Docker specializes in deploying apps</li>
<li>LXD specializes in deploying (Linux) Virtual Machines</li>
</ul>

<p><a href=""https://i.stack.imgur.com/i1M6l.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/i1M6l.jpg"" alt=""Infographic of Docker vs LXD""></a></p>

<p>Source: <a href=""http://linux.softpedia.com/blog/infographic-lxd-machine-containers-from-ubuntu-linux-492602.shtml"" rel=""noreferrer"">http://linux.softpedia.com/blog/infographic-lxd-machine-containers-from-ubuntu-linux-492602.shtml</a></p>

<p>Originally: <a href=""https://insights.ubuntu.com/2015/09/23/infographic-lxd-machine-containers-from-ubuntu/"" rel=""noreferrer"">https://insights.ubuntu.com/2015/09/23/infographic-lxd-machine-containers-from-ubuntu/</a></p>

<h2>Minor technical note</h2>

<ul>
<li>installing LXD includes a command line program coincidentally named <code>lxc</code> <a href=""http://blog.scottlowe.org/2015/05/06/quick-intro-lxd/"" rel=""noreferrer"">http://blog.scottlowe.org/2015/05/06/quick-intro-lxd/</a></li>
</ul>
","254977"
"How do I read from /proc/$pid/mem under Linux?","92900","","<p>The <a href=""http://www.kernel.org/doc/man-pages/online/pages/man5/proc.5.html"">Linux <code>proc(5)</code> man page</a> tells me that <code>/proc/$pid/mem</code> “can be used to access the pages of a process's memory”. But a straightforward attempt to use it only gives me</p>

<pre><code>$ cat /proc/$$/mem /proc/self/mem
cat: /proc/3065/mem: No such process
cat: /proc/self/mem: Input/output error
</code></pre>

<p>Why isn't <code>cat</code> able to print its own memory (<code>/proc/self/mem</code>)? And what is this strange “no such process” error when I try to print the shell's memory (<code>/proc/$$/mem</code>, obviously the process exists)? How can I read from <code>/proc/$pid/mem</code>, then?</p>
","<h3><code>/proc/$pid/maps</code></h3>

<p><code>/proc/$pid/mem</code> shows the contents of $pid's memory mapped the same way as in the process, i.e., the byte at offset <em>x</em> in the pseudo-file is the same as the byte at address <em>x</em> in the process. If an address is unmapped in the process, reading from the corresponding offset in the file returns <code>EIO</code> (Input/output error). For example, since the first page in a process is never mapped (so that dereferencing a <code>NULL</code> pointer fails cleanly rather than unintendedly accessing actual memory), reading the first byte of <code>/proc/$pid/mem</code> always yield an I/O error.</p>

<p>The way to find out what parts of the process memory are mapped is to read <code>/proc/$pid/maps</code>. This file contains one line per mapped region, looking like this:</p>

<pre><code>08048000-08054000 r-xp 00000000 08:01 828061     /bin/cat
08c9b000-08cbc000 rw-p 00000000 00:00 0          [heap]
</code></pre>

<p>The first two numbers are the boundaries of the region (addresses of the first byte and the byte after last, in hexa). The next column contain the permissions, then there's some information about the file (offset, device, inode and name) if this is a file mapping. See the <a href=""http://www.kernel.org/doc/man-pages/online/pages/man5/proc.5.html"" rel=""noreferrer""><code>proc(5)</code></a> man page or <a href=""https://stackoverflow.com/questions/1401359/understanding-linux-proc-id-maps"">Understanding Linux /proc/id/maps</a> for more information.</p>

<p>Here's a proof-of-concept script that dumps the contents of its own memory.</p>

<pre><code>#! /usr/bin/env python
import re
maps_file = open(""/proc/self/maps"", 'r')
mem_file = open(""/proc/self/mem"", 'r', 0)
for line in maps_file.readlines():  # for each mapped region
    m = re.match(r'([0-9A-Fa-f]+)-([0-9A-Fa-f]+) ([-r])', line)
    if m.group(3) == 'r':  # if this is a readable region
        start = int(m.group(1), 16)
        end = int(m.group(2), 16)
        mem_file.seek(start)  # seek to region start
        chunk = mem_file.read(end - start)  # read region contents
        print chunk,  # dump contents to standard output
maps_file.close()
mem_file.close()
</code></pre>

<hr>

<h3><code>/proc/$pid/mem</code></h3>

<p>If you try to read from the <code>mem</code> pseudo-file of another process, it doesn't work: you get an <code>ESRCH</code> (No such process) error.</p>

<p>The permissions on <code>/proc/$pid/mem</code> (<code>r--------</code>) are more liberal than what should be the case. For example, you shouldn't be able to read a setuid process's memory. Furthermore, trying to read a process's memory while the process is modifying it could give the reader an inconsistent view of the memory, and worse, there were race conditions that could trace older versions of the Linux kernel (according to <a href=""http://lkml.indiana.edu/hypermail/linux/kernel/0505.0/0858.html"" rel=""noreferrer"">this lkml thread</a>, though I don't know the details). So additional checks are needed:</p>

<ul>
<li>The process that wants to read from <code>/proc/$pid/mem</code> must attach to the process using <a href=""http://www.kernel.org/doc/man-pages/online/pages/man2/ptrace.2.html"" rel=""noreferrer""><code>ptrace</code></a> with the <code>PTRACE_ATTACH</code> flag. This is what debuggers do when they start debugging a process; it's also what <a href=""http://linux.die.net/man/1/strace"" rel=""noreferrer""><code>strace</code></a> does to a process's system calls. Once the reader has finished reading from <code>/proc/$pid/mem</code>, it should detach by calling <code>ptrace</code> with the <code>PTRACE_DETACH</code> flag.</li>
<li>The observed process must not be running. Normally calling <code>ptrace(PTRACE_ATTACH, …)</code> will stop the target process (it sends a <code>STOP</code> signal), but there is a race condition (signal delivery is asynchronous), so the tracer should call <code>wait</code> (as documented in <a href=""http://www.kernel.org/doc/man-pages/online/pages/man2/ptrace.2.html"" rel=""noreferrer""><code>ptrace(2)</code></a>).</li>
</ul>

<p>A process running as root can read any process's memory, without needing to call <code>ptrace</code>, but the observed process must be stopped, or the read will still return <code>ESRCH</code>.</p>

<p>In the Linux kernel source, the code providing per-process entries in <code>/proc</code> is in <a href=""http://lxr.linux.no/#linux+v2.6.37/fs/proc/base.c"" rel=""noreferrer""><code>fs/proc/base.c</code></a>, and the function to read from <code>/proc/$pid/mem</code> is <a href=""http://lxr.linux.no/linux+*/fs/proc/base.c#L779"" rel=""noreferrer""><code>mem_read</code></a>. The additional check is performed by <a href=""http://lxr.linux.no/#linux+v2.6.37/fs/proc/base.c#L197"" rel=""noreferrer""><code>check_mem_permission</code></a>.</p>

<p>Here's some sample C code to attach to a process and read a chunk its of <code>mem</code> file (error checking omitted):</p>

<pre><code>sprintf(mem_file_name, ""/proc/%d/mem"", pid);
mem_fd = open(mem_file_name, O_RDONLY);
ptrace(PTRACE_ATTACH, pid, NULL, NULL);
waitpid(pid, NULL, 0);
lseek(mem_fd, offset, SEEK_SET);
read(mem_fd, buf, _SC_PAGE_SIZE);
ptrace(PTRACE_DETACH, pid, NULL, NULL);
</code></pre>

<p><a href=""https://unix.stackexchange.com/questions/6267/how-to-unswap-my-desktop/6271#6271"">I've already posted a proof-of-concept script for dumping <code>/proc/$pid/mem</code> on another thread</a>.</p>
","6302"
"Comparing two strings in Bash","92305","","<p>I have the following <code>if</code> block in my bash script:</p>

<pre><code>if [ ${PACKAGENAME} -eq kakadu-v6_4-00902C ]; then
  echo ""successfully entered if block!!""
fi
</code></pre>

<p>The script execution is not entering my <code>if</code> block even though <code>$PACKAGENAME</code> is equal to <code>kakadu-v6_4-00902C</code>. What am I doing wrong?</p>
","<p><code>-eq</code> is an arithmetic operator, which compares two numbers.</p>

<p>Use <code>=</code> (portable/standard <code>sh</code>), <code>=~</code> or <code>==</code> instead.</p>

<p>Also use quotes, because if <code>${PACKAGENAME}</code> contains a whitespace or wildcard character, then it will be split into multiple arguments, which causes to make <code>[</code> see more arguments than desired. See <a href=""http://mywiki.wooledge.org/BashPitfalls#A.5B_.24foo_.3D_.22bar.22_.5D"">here</a> a list of common bash pitfalls.</p>

<pre><code>if [ ""${PACKAGENAME}"" = 'kakadu-v6_4-00902C' ]; then
    echo ""successfully entered if block!!""
fi
</code></pre>

<p>See <code>man bash</code>, search (<kbd>/</kbd>) for <a href=""https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html""><code>CONDITIONAL EXPRESSIONS</code></a>.</p>
","145161"
"How to import secret gpg key (copied from one machine to another)?","92284","","<p>I'm trying to copy my gpg key from one machine to another.</p>

<p>I do:</p>

<pre><code>gpg --export ${ID} &gt; public.key
gpg --export-secret-key ${ID} &gt; private.key
</code></pre>

<p>Move files to new machine, and then:</p>

<pre><code>gpg --import public.key
gpg: nyckel [ID]: public key [Name, e-mail] was imported
gpg: Total number of treated keys: 1
gpg:                 imported: 1  (RSA: 1)

gpg --allow-secret-key-import private.key
sec  [?]/[ID] [Creation date] [Name, e-mail]
ssb  [?]/[SUB-ID] [Creation date]
</code></pre>

<p>All looks good to me, but then:</p>

<pre><code>$ gpg -d [file].gpg
gpg: encrypted with 4096-bit RSA-key, id [SUB-ID], created [Creation date]
  [Name, e-mail]
gpg: decryption failed: secret key not accessible
</code></pre>

<p>So the error message says that the file has been encrypted with [SUB-ID], which the secret key import appears to say it has imported. (The [SUB-ID] in both messages is the same).</p>

<p>So I'm clearly doing something wrong, but I don't know what.</p>
","<p>You need to add <code>--import</code> to the command line to import the private key. You need not use the <code>--allow-secret-key-import</code> flag. According to the man page: ""This is an obsolete option and is not used anywhere.""</p>

<pre><code>gpg --import private.key
</code></pre>
","184952"
"What are the pros/cons of deb vs. rpm?","92235","","<p>For whatever reasons, I've always used RPM based distributions (Fedora, Centos and currently openSUSE). I have often heard it stated that deb is better than rpm, but when asked why, have never been able to get a coherent answer (usually get some zealous ranting and copious amounts of spittle instead).</p>

<p>I understand there may be some historical reasons, but for modern distributions using the two different packaging methods, can anybody give the technical (or other) merits of one vs. the other?</p>
","<p>Main difference for a package maintainer (I think that would be 'developer' in Debian lingo) is the way package meta-data and accompanying scripts come together.</p>

<p>In the RPM world, all your packages (the RPMs you maintain) are located in something like <code>~/rpmbuild</code>. Underneath, there is the <code>SPEC</code> directory for your spec-files, a <code>SOURCES</code> directory for source tarballs, <code>RPMS</code> and <code>SRPMS</code> directories to put newly created RPMs and SRPMs into, and some other things that are not relevant now.</p>

<p><em>Everything</em> that has to do with how the RPM will be created is in the spec-file: what patches will be applied, possible pre- and post-scripts, meta-data, changelog, everything. All source tarballs and all patches of <em>all your packages</em> are in SOURCES.</p>

<p>Now, personally, I like the fact that everything goes into the spec-file, and that the spec-file is a separate entity from the source tarball, but I'm not overly enthusiastic about having <em>all</em> sources in SOURCES. IMHO, SOURCES gets cluttered pretty quick and you tend to lose track of what is in there. However, opinions differ.</p>

<p>For RPMs it is important to use the <em>exact</em> same tarball as the one the upstream project releases, up to the timestamp. Generally, there are no exceptions to this rule. Debian packages also require the same tarball as upstream, though Debian policy requires some tarballs to be repackaged (thanks, Umang).</p>

<p>Debian packages take a different approach. (Forgive any mistakes here: I am a lot less experienced with deb's that I am with RPM's.) Debian packages' development files are contained in a directory per package.</p>

<p>What I (think to) like about this approach is the fact that everything is contained in a single directory.</p>

<p>In the Debian world, it is a bit more accepted to carry patches in a package that are not (yet) upstream. In the RPM world (at least among the Red Hat derivatives) this is frowned upon. See <a href=""http://fedoraproject.org/wiki/Staying_close_to_upstream_projects"">""FedoraProject: Staying close to upstream projects""</a>.</p>

<p>Also, Debian has a vast amount of scripts that are able to automate a huge portion of creating a package. For example, creating a - simple - package of a setuptool'ed Python program, is as simple as creating a couple of meta-data files and running <code>debuild</code>. That said, the spec-file for such package in RPM format would be pretty short and in the RPM world, too, there's a lot of stuff that is automated these days.</p>
","701"
"Can I resize the root partition without uninstalling and reinstalling Linux (or losing data)?","92204","","<p>I have started using a machine that has both Debian and Windows 7 installed. However, after installing some programs I started getting a message that there is not enough space. I knew that the system had more than 1TB of hard disk space in total and did some research. It seems that the root partition is only 5GB.</p>

<p>Is there any way to allocate more disk space to the specific partition without reinstalling Linux? </p>

<p>Below you can find the results of a couple of commands that I executed:</p>

<pre><code>fdisk -l

Disk /dev/sda: 640.1 GB, 640135028736 bytes
255 heads, 63 sectors/track, 77825 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x4a47e2fd

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1          13      102400    7  HPFS/NTFS
Partition 1 does not end on cylinder boundary.
/dev/sda2              13       53507   429687500    7  HPFS/NTFS
Partition 2 does not end on cylinder boundary.
/dev/sda3           53507       77826   195340289    5  Extended
Partition 3 does not end on cylinder boundary.
/dev/sda5           53507       53537      243712   83  Linux
/dev/sda6           53538       77826   195095552   8e  Linux LVM

Disk /dev/sdb: 640.1 GB, 640135028736 bytes
255 heads, 63 sectors/track, 77825 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x4a47e2fe

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               2       62261   500097657+   f  W95 Ext'd (LBA)
/dev/sdb5               2       62261   500097656+   7  HPFS/NTFS

Disk /dev/dm-0: 4999 MB, 4999610368 bytes
255 heads, 63 sectors/track, 607 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/dm-0 doesn't contain a valid partition table

Disk /dev/dm-1: 1996 MB, 1996488704 bytes
255 heads, 63 sectors/track, 242 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/dm-1 doesn't contain a valid partition table

Disk /dev/dm-2: 20.0 GB, 19998441472 bytes
255 heads, 63 sectors/track, 2431 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/dm-2 doesn't contain a valid partition table
</code></pre>

<hr>

<pre><code>df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/HU-root   4.6G  4.4G   32M 100% /
tmpfs                 2.0G     0  2.0G   0% /lib/init/rw
udev                  2.0G  220K  2.0G   1% /dev
tmpfs                 2.0G  356K  2.0G   1% /dev/shm
/dev/sda5             231M   22M  198M  10% /boot
/dev/mapper/HU-home    19G  751M   17G   5% /home
</code></pre>

<hr>

<p>Edit (extra info based on comment):</p>

<pre><code># mount
/dev/mapper/HU-root on / type ext3 (rw,errors=remount-ro)
tmpfs on /lib/init/rw type tmpfs (rw,nosuid,mode=0755)
proc on /proc type proc (rw,noexec,nosuid,nodev)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
udev on /dev type tmpfs (rw,mode=0755)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=620)
/dev/sda5 on /boot type ext3 (rw)
/dev/mapper/HU-home on /home type ext3 (rw)
fusectl on /sys/fs/fuse/connections type fusectl (rw)
binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,noexec,nosuid,nodev)
</code></pre>

<hr>

<pre><code># pvdisplay
  --- Physical volume ---
  PV Name               /dev/sda6
  VG Name               HU
  PV Size               186.06 GiB / not usable 3.00 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              47630
  Free PE               41194
  Allocated PE          6436
  PV UUID               wmEFAc-eSb6-r3qo-jIjy-vuKH-v9JK-eQfJFZ
</code></pre>

<hr>

<p>As a sidenote, I would also appreciate any suggestions for good books that explain these subjects in depth (file systems, how the various operating systems understand the file systems etc).</p>
","<hr>

<blockquote>
  <p><strong>Warning</strong>: doing anything to your filesystems without a known-restorable backup is ill-advised.</p>
</blockquote>

<hr>

<p><strong>Do not run any of the following steps if you're not sure your <code>/</code> is clean.</strong> If you're not sure, run the following (as root):</p>

<pre><code># touch /forcefsck
</code></pre>

<p>and reboot. This will do an <code>fsck</code> of all your partitions, to be on the safe side.</p>

<hr>

<p>That being said, since you're using LVM for your root device and an <code>ext3</code> filesystem, you can extend it online. Before you start make sure you have <code>resize2fs</code> installed. If not, it's usually in a package called <code>e2fsprogs</code>. (If you can't install it because you don't have enough room, try to do a bit of cleanup in <code>/var/log</code> for instance.)</p>

<p>First you extend the underlying volume with (as root):</p>

<pre><code># lvextend -L+2G /dev/mapper/HU-root
</code></pre>

<p>(Adjust the <code>2G</code> part as you wish - it's the amount of space you want to add.)</p>

<p>Then you need to resize the filesystem. This can be done online with <code>resize2fs</code>:</p>

<pre><code># resize2fs /dev/mapper/HU-root
</code></pre>

<p>This can take a few seconds/minutes. Don't interrupt. Do another reboot at the end, possibly with another <code>/forcefsck</code>, if you want to. It's not necessary, but I often do it anyway.</p>
","28126"
"How to determine if NFS mount is mounted as v3 or v4?","92193","","<p>Red Hat 5/6 when I do mount it says type nfs, I would like to know how to determine version if it isn't listed in mount options or fstab.  Please don't say remount it with the version option, I want to know how to determine the currently mounted NFS version.  I am guessing it will default based on NFS server/client settings, but how to I determine what it is currently?  I am pretty sure it's NFS v3 because nfs4_setfacl is not supported it seems.</p>
","<p>Here are 2 ways to do it:</p>

<h3>mount</h3>

<p>Using mount's <code>-v</code> switch:</p>

<pre><code>$ mount -v | grep /home/sam
mulder:/export/raid1/home/sam on /home/sam type nfs (rw,intr,tcp,nfsvers=3,rsize=16384,wsize=16384,addr=192.168.1.1)
</code></pre>

<h3>nfsstat</h3>

<p>Using <code>nfsstat -m</code>:</p>

<pre><code>$ nfsstat -m | grep -A 1 /home/sam
/home/sam from mulder:/export/raid1/home/sam
 Flags: rw,vers=3,rsize=16384,wsize=16384,hard,intr,proto=tcp,timeo=600,retrans=2,sec=sys,addr=mulder
</code></pre>
","115929"
"Get the chmod numerical value for a file","91680","","<p>In FreeBSD and also in Linux, how can I get the numerical <code>chmod</code> value of a file? For example, <code>644</code> instead of <code>-rw-r--r--</code>? I need an automatic way for a Bash script.</p>
","<p>You can get the value directly using a stat output format, e.g. BSD/OS X:</p>

<pre><code>stat -f ""%OLp"" &lt;file&gt;
</code></pre>

<p>or in Linux</p>

<pre><code>stat --format '%a' &lt;file&gt;
</code></pre>
","46921"
"What are common rights for /tmp ? I unintentionally set it all public recursively","91596","","<p>I have abused <code>sudo</code>.</p>

<p>I have created a really really short life temporary directory that I wanted to share between some users for a few hours... and I named this directory <code>/some/path/tmp</code></p>

<p>Unfortunately I have launched <code>sudo chown 777 -R /tmp</code> instead of <code>sudo chown 777 -R tmp</code></p>

<p>so my <code>/tmp</code> file is now completely public.</p>

<p>I use the common <code>/tmp</code> pretty often (every day, almost every hour) personally for short life files, scripts, lots of scripts.</p>

<p>Is it a security breach now that it is completely set to public? Should I change it back to more secure settings, or like common default settings for a Debian / Ubuntu distro - (I don't know which they were)? What are the common right settings for <code>/tmp</code>?</p>
","<p>The normal settings for <code>/tmp</code> are 1777, which <code>ls</code> shows as <code>drwxrwxrwt</code>. That is: wide open, except that only the owner of a file can remove it (that's what this extra <code>t</code> bit means for a directory).</p>

<p>The problem with a <code>/tmp</code> with mode 777 is that another user could remove a file that you've created and substitute the content of their choice.</p>

<p>If your <code>/tmp</code> is a tmpfs filesystem, a reboot will restore everything. Otherwise, run <code>chmod 1777 /tmp</code>.</p>

<p>Additionally, a lot of files in <code>/tmp</code> need to be private. However, at least one directory critically needs to be world-readable. The following command should mostly set things right:</p>

<pre><code>chmod -R go-rwx /tmp/* /tmp/.[!.]*
chmod 777 /tmp/.X11-unix /tmp/.X11-unix/*
chmod 1777 /tmp
</code></pre>

<p>I.e. make all files and directories private (remove all permissions for group and other), but make the X11 socket accessible to all. Access control on these sockets is enforced by the server, not by the file permissions. There may be other sockets that need to be publicly available. Run <code>find /tmp -type s -user 0</code> to discover root-owned sockets which you may need to make world-accessible. There may be sockets owned by other system users as well (e.g. to communicate with a system bus); explore with <code>find /tmp -type s ! -user $UID</code> (where <code>$UID</code> is your user ID).</p>
","71625"
"How to see full log from systemctl status service?","91557","","<p>I check service status with <code>systemctl status service-name</code>.</p>

<p>By default, I see few rows only, so I add <code>-n50</code> to see more.</p>

<p>Sometimes, I want to see full log, from start. It could have 1000s of rows.<br>
Now, I check it with <code>-n10000</code> but that doesn't look like neat solution.</p>

<p>Is there an option to check full systemd service log similar to <code>less</code> command?</p>
","<p>Just use the <code>journalctl</code> command, as in:</p>

<pre><code>journalctl -u service-name.service
</code></pre>

<p>Or, to see only log messages for the current boot:</p>

<pre><code>journalctl -u service-name.service -b
</code></pre>

<p>For things named <code>&lt;something&gt;.service</code>, you can actually just use <code>&lt;something&gt;</code>, as in:</p>

<pre><code>journalctl -u service-name
</code></pre>

<p>But for other sorts of units (sockets, targets, timers, etc), you need to be explicit.</p>
","225407"
"""Symbolic link not allowed or link target not accessible"" / Apache on CentOS 6","91450","","<p>I've got a brand new CentOS 6 installation, which has a symlink in the document root to my development files:</p>

<pre><code>[root@localhost html]# ls -l
total 4
-rwxrwxrwx. 1 root root  0 Sep 18 20:16 index.html
-rwxrwxrwx. 1 root root 17 Sep 18 20:16 index.php
lrwxrwxrwx. 1 root root 24 Sep 18 20:19 refresh-app -&gt; /home/billy/refresh-app/
</code></pre>

<p>My httpd.conf has this:</p>

<pre><code>&lt;Directory ""/""&gt;
    Options All
    AllowOverride None
    Order allow,deny
    Allow from all
&lt;/directory&gt;
</code></pre>

<p>The target of the symbolic link has permissions which should allow apache to read anything it wants:</p>

<pre><code> [root@localhost billy]# ls -l
total 40 (Some entries were omitted because the list was too long
drwxr-xr-x. 7 billy billy 4096 Sep 18 20:03 refresh-app
</code></pre>

<p>I've also tried disabling SELinux by changing <code>/etc/selinux/conf</code>:</p>

<pre><code>SELINUX=disabled
</code></pre>

<p>Yet no matter what I do, when someone tries to go to that link, <code>http://localhost/refresh-app/</code>, I get a 403 FORBIDDEN error page and this is written in the <code>/var/log/httpd/error_log</code>:</p>

<pre><code>Symbolic link not allowed or link target not accessible
</code></pre>

<p>Why can't Apache access the target of the symlink?</p>
","<p>Found the issue. Turns out, Apache wants access to not just the directory I'm serving, <code>/home/billy/refresh-app/</code>, but also every directory above that, namely <code>/home/billy/</code>, <code>/home</code>, and <code>/</code>. (I have no idea why... giving someone access to a subdirectory shouldn't require giving away permissions to everything above that subdirectory....)</p>

<p>I would guess it's looking for <code>.htaccess</code> or something, or perhaps *nix being strange about how it treats permissions for directory transversal.</p>
","21339"
"How to pass each line of a text file as an argument to a command?","91403","","<p>I'm looking to write a script that takes a <code>.txt</code> filename as an argument, reads the file line by line, and passes each line to a command. For example, it runs <code>command --option ""LINE 1""</code>, then <code>command --option ""LINE 2""</code>, etc. The output of the command is written to another file. How do I go about doing that? I don't know where to start.</p>
","<p>Use <code>while read</code> loop:</p>

<pre><code>: &gt; another_file  ## Truncate file.

while read -r LINE; do
    command --option ""$LINE"" &gt;&gt; another_file
done &lt; file
</code></pre>

<p>Another is to redirect output by block:</p>

<pre><code>while read -r LINE; do
    command --option ""$LINE""
done &lt; file &gt; another_file
</code></pre>

<p>Last is to open the file:</p>

<pre><code>exec 4&gt; another_file

while read -r LINE; do
    command --option ""$LINE"" &gt;&amp;4
    echo xyz  ## Another optional command that sends output to stdout.
done &lt; file
</code></pre>

<p>If one of the commands reads input, it would be a good idea to use another fd for input so the commands won't eat it (here assuming <code>ksh</code>, <code>zsh</code> or <code>bash</code> for <code>-u 3</code>, use <code>&lt;&amp;3</code> instead portably):</p>

<pre><code>while read -ru 3 LINE; do
    ...
done 3&lt; file
</code></pre>

<p>Finally to accept arguments, you can do:</p>

<pre><code>#!/bin/bash

FILE=$1
ANOTHER_FILE=$2

exec 4&gt; ""$ANOTHER_FILE""

while read -ru 3 LINE; do
    command --option ""$LINE"" &gt;&amp;4
done 3&lt; ""$FILE""
</code></pre>

<p>Which one could run as:</p>

<pre><code>bash script.sh file another_file
</code></pre>

<p>Extra idea. With <code>bash</code>, use <code>readarray</code>:</p>

<pre><code>readarray -t LINES &lt; ""$FILE""
for LINE in ""${LINES[@]}""; do
    ...
done
</code></pre>

<p>Note: It's not necessary to reset value of <code>IFS</code> (i.e. <code>IFS= read</code>) when only using a single parameter.</p>
","149727"
"How to find which Process is causing High CPU usage","91388","","<p>Is there any way to find out from terminal which process is causing high CPU Usage ?</p>

<p>It would also be useful to order processes in descending order of cpu Usage </p>
","<p><code>top</code> will display what is using your CPU. If you have it installed, <code>htop</code> allows you more fine-grained control, including filtering by&mdash;in your case&mdash;CPU</p>
","20484"
"How can I expand ext4 partition size on debian","91284","","<p>I have a drive with this configuration:</p>

<pre><code>fdisk -l

Disk /dev/sda: 500.1 GB, 500107862016 bytes
255 heads, 63 sectors/track, 60801 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x000f1b8b

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           1        2612    20971520   83  Linux
/dev/sda3           60736       60801      525312   82  Linux swap / Solaris
</code></pre>

<p>There is 478GB unallocated space, how would I go about adding this space to /dev/sda1 without losing the data that is currently on /dev/sda1? (the filesystem is ext4).</p>

<p>NOTE: it is a server with only SSH, no GUI for running stuff like gparted.</p>
","<p>Enlarge the partition: <code>fdisk -u /dev/sda</code>.</p>

<p><code>p</code> to print the partition table, take note of the number, start, end, type of sda1.</p>

<p>Delete it: <code>d</code>:</p>

<p>Recreate it with same number (1), start and type but with a bigger <em>end</em> (taking care not to overlap with other partitions). Try to align things on a megabyte boundary that is for <em>end</em>, make it a multiple of 2048 minus 1. Change the type if needed with <code>t</code> (for partitions holding an <code>extX</code> or <code>btrfs</code> filesystem, the default of <code>83</code> is fine).</p>

<p>Then <code>w</code> to write and <code>q</code> to quit.</p>

<p>The partition table will have been modified but the kernel will not be able to take that into account as some partitions are mounted.</p>

<p>However, if in-use partitions were only enlarged, you should be able to force the kernel to take the new layout with:</p>

<pre><code>partx /dev/sda
</code></pre>

<p>If that fails, you'll need to reboot. The system should boot just fine.</p>

<p>Then, resize the filesystem so it spreads to the extent of the enlarged partition:</p>

<pre><code>resize2fs /dev/sda1
</code></pre>

<p>Which for <code>ext4</code> will work just fine even on a live FS.</p>
","67107"
"Refresh env variables after editing bashrc file","91192","","<p>I frequently edited the <code>.bashrc</code> file to export new environment variables.</p>

<p>Rather than close the console and start a new one to refresh the env variables, is there a convenient way to refresh?</p>
","<p>Within the same window, you can simply type <code>bash</code> to start a new one. This is equivalent to closing the window and re-opening a new one.</p>

<p>Alternatively, you can type <code>source ~/.bashrc</code> to source the <code>.bashrc</code> file.</p>
","26696"
"List subdirectories only n level deep","91074","","<p>Festival stores voicepack data in the following example directory structure:</p>

<p><code>/usr/share/festival/voices/&lt;language&gt;/&lt;voicepack name&gt;</code></p>

<p>What is the simplest one-liner (preferably using <code>ls</code>) to print out just the <code>&lt;voicepack name&gt;</code>'s, in all the potentially numerous <code>&lt;language&gt;</code> subdirectories?</p>
","<p>I'm on Fedora, and these voicepacks are in a slightly different location:</p>

<pre><code>$ ls /usr/share/festival/lib/voices/*/ -1 | grep -vE ""/usr|^$""
kal_diphone
ked_diphone
nitech_us_awb_arctic_hts
nitech_us_bdl_arctic_hts
nitech_us_clb_arctic_hts
nitech_us_jmk_arctic_hts
nitech_us_rms_arctic_hts
nitech_us_slt_arctic_hts
</code></pre>

<p>You can just modify this like so:</p>

<pre><code>$ ls /usr/share/festival/voices/*/ -1 | grep -vE ""/usr|^$""
</code></pre>

<h3>Using find</h3>

<p>Using <code>ls</code> in this manor is typically frowned upon because the output of <code>ls</code> is difficult to parse. Better to use the <code>find</code> command, like so:</p>

<pre><code>$ find /usr/share/festival/lib/voices -maxdepth 2 -mindepth 2 \
    -type d -exec basename {} \;
nitech_us_awb_arctic_hts
nitech_us_bdl_arctic_hts
nitech_us_slt_arctic_hts
nitech_us_jmk_arctic_hts
nitech_us_clb_arctic_hts
nitech_us_rms_arctic_hts
ked_diphone
kal_diphone
</code></pre>

<h3>Details of find &amp; basename</h3>

<p>This command works by producing a list of full paths to files that are exactly 2 levels deep with respect to this directory:</p>

<pre><code>/usr/share/festival/lib/voices
</code></pre>

<p>This list looks like this:</p>

<pre><code>$ find /usr/share/festival/lib/voices -maxdepth 2 -mindepth 2 
/usr/share/festival/lib/voices/us/nitech_us_awb_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_bdl_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_slt_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_jmk_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_clb_arctic_hts
/usr/share/festival/lib/voices/us/nitech_us_rms_arctic_hts
/usr/share/festival/lib/voices/english/ked_diphone
/usr/share/festival/lib/voices/english/kal_diphon
</code></pre>

<p>But we want the last part of these directories, the leaf node. So we can make use of <code>basename</code> to parse it out:</p>

<pre><code>$ basename /usr/share/festival/lib/voices/us/nitech_us_awb_arctic_hts
nitech_us_awb_arctic_hts
</code></pre>

<p>Putting it all together, we can make the <code>find</code> command pass each 2 level deep directory to the <code>basename</code> command. The notation <code>basename {}</code> is what is doing these basename conversions. Find calls it via it's <code>-exec</code> switch.</p>
","93326"
"How can I communicate with a Unix domain socket via the shell on Debian Squeeze?","91008","","<p>I’m running a Debian Squeeze web server. I’ve installed memcached on it, and configured memcached to listen on a Unix domain socket (at <code>/tmp/memcached.sock</code>), as it only needs to receive messages from the website, which lives on the same server.</p>

<p>It seems to be working fine, but I’d also like to communicate with memcached via the shell, to check that it’s doing what I think it’s doing.</p>

<p>memcached accepts messages via a simple ASCII protocol (if I understand correctly). If it was listening on TCP/IP, I could send messages to it via e.g. <code>nc</code>:</p>

<pre><code>$ echo ""stats settings"" | nc localhost 11211
</code></pre>

<p>But I can’t figure out how to send that text to the domain socket instead.</p>

<p>On my laptop (which runs OS X Lion), both <code>nc</code> and <code>telnet</code> have options (<code>-U</code> and <code>-u</code> respectively) to use domain sockets. However, on my Debian Squeeze web server, these options aren’t present.</p>
","<p>With <code>netcat-openbsd</code>, there is a <code>-U</code> option. If you don't have it, you probably have <code>netcat-traditional</code> installed instead; I'd suggest switching.</p>

<p>Example command:
<code>nc -U /var/run/socket</code></p>
","26781"
"Opening Firefox from terminal","90967","","<p>When I type <code>firefox</code> in terminal, it starts Firefox but the terminal ""hangs"".</p>

<p>What is happening behind the scenes?</p>

<p>Can I open Firefox from terminal and keep on using the same terminal tab for other things without closing Firefox?</p>
","<p>The terminal locks when you are running an application from it as long as the application is running. With the ampersand (&amp;) you can start the application in the background and still use the terminal. Type:</p>

<pre><code>user@host:~# firefox &amp;
</code></pre>

<p>To start firefox in the background. Output of the application will still be in the terminal.</p>

<p>Or, if firefox is already running you can do this:</p>

<ol>
<li><kbd>Ctrl</kbd>+<kbd>z</kbd> to put firefox into the backgroound.</li>
<li><p>Type:</p>

<pre><code>jobs
</code></pre>

<p>You should see  your jobs like :</p>

<pre><code>[1]+  Stopped  firefox.
</code></pre></li>
<li><p>Type: </p>

<pre><code>bg %1 
</code></pre>

<p>(or number of your job)</p></li>
</ol>
","103486"
"How can I create a virtual ethernet interface on a machine without a physical adapter?","90904","","<p>I have a Dell XPS 13 ultrabook which has a wifi nic, but no physical ethernet nic (wlan0, but no eth0).  I need to create a virtual adapter for using Vagrant with NFS, but am finding that the typical <code>ifup eth0:1...</code> fails with <code>ignoring unknown interface eth0:1=eth0:1</code>.  I also tried creating a virtual interface against <code>wlan0</code>, but received the same result.</p>

<p>How can I create a virtual interface on this machine with no physical interface?</p>
","<h3>Setting up a dummy interface</h3>

<p>If you want to create network interfaces, but lack a physical NIC to back it, you can use the dummy link type. You can read more about them here: <a href=""https://en.wikipedia.org/wiki/Iproute2"">iproute2 Wikipedia page</a>.</p>

<h3>Creating eth10</h3>

<p>To make this interface you'd first need to make sure that you have the dummy kernel module loaded. You can do this like so:</p>

<pre><code>$ sudo lsmod | grep dummy
$ sudo modprobe dummy
$ sudo lsmod | grep dummy
dummy                  12960  0 
</code></pre>

<p>With the driver now loaded you can create what ever dummy network interfaces you like:</p>

<pre><code>$ sudo ip link set name eth10 dev dummy0
</code></pre>

<p>And confirm it:</p>

<pre><code>$ ip link show eth10
6: eth10: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default 
    link/ether c6:ad:af:42:80:45 brd ff:ff:ff:ff:ff:ff
</code></pre>

<h3>Changing the MAC</h3>

<p>You can then change the MAC address if you like:</p>

<pre><code>$ sudo ifconfig eth10 hw ether 00:22:22:ff:ff:ff
$ ip link show eth10
6: eth10: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default 
    link/ether 00:22:22:ff:ff:ff brd ff:ff:ff:ff:ff:ff
</code></pre>

<h3>Creating an alias</h3>

<p>You can then create aliases on top of eth10.</p>

<pre><code>$ sudo ip addr add 192.168.100.199/24 brd + dev eth10 label eth10:0
</code></pre>

<p>And confirm them like so:</p>

<pre><code>$ ifconfig -a eth10
eth10: flags=130&lt;BROADCAST,NOARP&gt;  mtu 1500
        ether 00:22:22:ff:ff:ff  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

$ ifconfig -a eth10:0
eth10:0: flags=130&lt;BROADCAST,NOARP&gt;  mtu 1500
        inet 192.168.100.199  netmask 255.255.255.0  broadcast 192.168.100.255
        ether 00:22:22:ff:ff:ff  txqueuelen 0  (Ethernet)
</code></pre>

<p>Or using <code>ip</code>:</p>

<pre><code>$ ip a | grep -w inet
    inet 127.0.0.1/8 scope host lo
    inet 192.168.1.20/24 brd 192.168.1.255 scope global wlp3s0
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
    inet 192.168.100.199/24 brd 192.168.100.255 scope global eth10:0
</code></pre>

<h3>Removing all this?</h3>

<p>If you want to unwind all this you can run these commands to do so:</p>

<pre><code>$ sudo ip addr del 192.168.100.199/24 brd + dev eth10 label eth10:0
$ sudo ip link delete eth10 type dummy
$ sudo rmmod dummy
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""http://www.unixwerk.eu/linux/redhat/ipalias.html"">MiniTip: Setting IP Aliases under Fedora</a></li>
<li><a href=""http://www.pocketnix.org/posts/Linux%20Networking:%20Dummy%20Interfaces%20and%20Virtual%20Bridges"">Linux Networking: Dummy Interfaces and Virtual Bridges</a></li>
<li><a href=""http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=ip-link"">ip-link man page</a></li>
<li><a href=""http://www.policyrouting.org/iproute2.doc.html"">iproute2 HOWTO</a></li>
<li><a href=""http://baturin.org/docs/iproute2/"">iproute2 cheatsheet</a></li>
</ul>
","152334"
"Gnome Shell Integration in Chrome not working","90766","","<p>Just upgraded to Ubuntu 14.04, which seems to also make a full reinstall of Chromium (as all my plugins were removed).</p>

<p>Now, trying to access <code>https://extensions.gnome.org/</code> to enable Gnome Shell extensions, but the message:</p>

<blockquote>
  <p>We cannot detect a running copy of GNOME on this system, so some parts of the interface may be disabled. See our <a href=""https://extensions.gnome.org/about/#no-detection"">troubleshooting</a> entry for more information.</p>
</blockquote>

<p>keeps appearing. There is nothing in my <code>chrome://plugins</code> page, but the site still seems to be whitelisted in my ""click to play"" settings. Has anyone found out how to force <code>Chrome</code> to get this plugin?</p>
","<p>Chrome and Chromium dropped support for the NPAPI plugins (Netscape Plugin Application Programming Interface) in favor of PPAPI (Pepper Plugin Application Programming Interface) so all plugins that use NPAPI (like GNOME Extension plugin) are just not supported.</p>

<p>The only alternative is using another browser that allows them (like Firefox) or asking the developers to move to PPAPI (unlikely).</p>

<p><strong>NOTE:</strong> This is the blog post from the Chromium blog mentioning this, titled: <a href=""http://blog.chromium.org/2013/09/saying-goodbye-to-our-old-friend-npapi.html"">Saying Goodbye to Our Old Friend NPAPI</a>.</p>
","129281"
"How can I check if swap is active from the command line?","90759","","<p>How can I check if swap is active, and which swap devices are set up, on the command line?</p>
","<p>in linux, you can use </p>

<ul>
<li><code>cat /proc/meminfo</code> to see total swap, and free swap  (all linux)</li>
<li><code>cat /proc/swaps</code> to see which swap devices are being used  (all linux)</li>
<li><code>swapon -s</code> to see swap devices and sizes (where swapon is installed)</li>
<li><code>vmstat</code> for current virtual memory statistics</li>
</ul>

<p>in Mac OS X, you can use</p>

<ul>
<li><code>vm_stat</code> to see information about virtual memory (swap)</li>
<li><code>ls -lh /private/var/vm/swapfile*</code> to see how many swap files are being used.</li>
</ul>

<p>in Solaris, you can use</p>

<ul>
<li><code>swap -l</code> to see swap devices/files, and their sizes</li>
<li><code>swap -s</code> to see total swap size, used &amp; free</li>
<li><code>vmstat</code> to see virtual memory statistics</li>
</ul>

<p>On some systems, ""virtual memory"" refers only to disk-backed memory devices, and on other systems, like Solaris, Virtual Memory can refer to any user process address space, including tmpfs filesystems (like /tmp) and shared memory space.</p>
","23074"
"What color codes can I use in my PS1 prompt?","90676","","<p>I used several colors in my PS1 prompt such as </p>

<pre><code>\033]01;31\] # pink
\033]00m\]   # white
\033]01;36\] # bold green
\033]02;36\] # green
\033]01;34\] # blue
\033]01;33\] # bold yellow
</code></pre>

<p>Where can I find a list of the color codes I can use?</p>

<p>I looked at <a href=""https://unix.stackexchange.com/questions/74024/colorize-bash-console-color"">Colorize Bash Console Color</a> but it didn't answer my question about a list of the actual codes.</p>

<p>It would be nice if there was a more readable form also.</p>

<p>See also <a href=""https://unix.stackexchange.com/a/127800/10043"">https://unix.stackexchange.com/a/127800/10043</a></p>
","<p>Those are <a href=""http://en.wikipedia.org/wiki/ANSI_escape_code#Colors"">ANSI escape sequences</a>; that link is to a chart of color codes but there are other interesting things on that wikipedia page as well.  Not all of them work on (e.g.) a normal linux console.</p>

<p>This is incorrect:</p>

<blockquote>
  <p><code>\033]00m\]   # white</code></p>
</blockquote>

<p><code>0</code> resets the terminal to its default (which is probably white).  The actual code for white foreground is 37.  Also, the escaped closing brace at the end (<code>\]</code>) is not part of the color sequence (see the last few paragraphs below for an explanation of their purpose in setting a prompt).</p>

<p>Note that some GUI terminals allow you to specify a customized color scheme. This will affect the output.</p>

<p>There's <a href=""http://misc.flogisoft.com/bash/tip_colors_and_formatting"">a list here</a> which adds 7 foreground and 7 background colors I had not seen before, but they seem to work:</p>

<pre><code># Foreground colors
90   Dark gray  
91   Light red  
92   Light green    
93   Light yellow   
94   Light blue 
95   Light magenta  
96   Light cyan  

# Background colors
100  Dark gray  
101  Light red  
102  Light green    
103  Light yellow   
104  Light blue 
105  Light magenta  
106  Light cyan 
</code></pre>

<p>In addition, if you have a 256 color GUI terminal (I think most of them are now), you can apply colors from this chart: </p>

<p><img src=""https://i.stack.imgur.com/UQVe5.png"" alt=""enter image description here""></p>

<p>The ANSI sequence to select these, using the number in the bottom left corner, starts <code>38;5;</code> for the foreground and <code>48;5;</code> for the background, then the color number, so e.g.:</p>

<pre><code>echo -e ""\\033[48;5;95;38;5;214mhello world\\033[0m""
</code></pre>

<p>Gives me a light orange on tan (meaning, the color chart is roughly approximated).</p>

<p>You can see the colors in this chart as they would appear on your terminal fairly easily:</p>

<pre><code>#!/bin/bash

color=16;

while [ $color -lt 245 ]; do
    echo -e ""$color: \\033[38;5;${color}mhello\\033[48;5;${color}mworld\\033[0m""
    ((color++));
done  
</code></pre>

<p>The output is self-explanatory.  </p>

<p>Some systems set the $TERM variable to <code>xterm-256color</code> if you are on a 256 color terminal via some shell code in <code>/etc/profile</code>.  On others, you should be able to configure your terminal to use this.  That will let TUI applications know there are 256 colors, and allow you to add something like this to your <code>~/.bashrc</code>:</p>

<pre><code>if [[ ""$TERM"" =~ 256color ]]; then
     PS1=""MyCrazyPrompt...""
fi
</code></pre>

<p>Beware that when you use color escape sequences in your prompt, you should enclose them in escaped (<code>\</code> prefixed) square brackets, like this:</p>

<pre><code>PS1=""\[\033[01;32m\]MyPrompt: \[\033[0m\]""
</code></pre>

<p>Notice the <code>[</code>'s interior to the color sequence are not escaped, but the enclosing ones are.  The purpose of the latter is to indicate to the shell that the enclosed sequence does not count toward the character length of the prompt.  If that count is wrong, weird things will happen when you scroll back through the history, e.g., if it is too long, the excess length of the last scrolled string will appear attached to your prompt and you won't be able to backspace into it (it's ignored the same way the prompt is).</p>

<p>Also note that if you want to include the output of a command run every time the prompt is used (as opposed to just once when the prompt is set), you should set it as a literal string with single quotes, e.g.:</p>

<pre><code>PS1='\[\033[01;32m\]$(date): \[\033[0m\]'
</code></pre>

<p>Although this is not a great example if you are happy with using bash's special <code>\d</code> or <code>\D{format}</code> prompt escapes -- which are not the topic of the question but can be found in <code>man bash</code> under <code>PROMPTING</code>.  There are various other useful escapes such as <code>\w</code> for current directory, <code>\u</code> for current user, etc.</p>
","124409"
"Copy only regular files from one directory to another","90674","","<p>I'd like to copy a content of directory 1 to directory 2. However, I'd like to only copy files (and not directories) from my directory 1. How can I do that ?</p>

<pre><code>cp dir1/* dir2/*
</code></pre>

<p>then I still have the directories issue.</p>

<p>Also, all my files don't have any extension, so <em>.</em> won't do the trick</p>
","<pre><code>cp dir1/* dir2
</code></pre>

<p><code>cp</code> will not copy directories unless explicitly told to do so (with <code>--recursive</code> for example, see <code>man cp</code>).</p>

<p><strong>Note 1:</strong> <code>cp</code> will most likely exit with a non-zero status, but the files will have been copied anyway. This may be an issue when chaining commands based on exit codes:<code>&amp;&amp;</code>, <code>||</code>, <code>if cp -r dir1/* dir2; then ...</code>, etc. (Thanks to <a href=""https://unix.stackexchange.com/users/73248"">contrebis</a> for <a href=""https://unix.stackexchange.com/questions/101916/copy-only-regular-files-from-one-directory-to-another/101923?noredirect=1#comment459686_101923"">their comment on that issue</a>)</p>

<p><strong>Note 2</strong>: <code>cp</code> expects the last parameter to be a single file name or directory. There really should be no wildcard <code>*</code> after the name of the target directory. <code>dir2\*</code> will be expanded by the shell just like <code>dir1\*</code>. Unexpected things <em>will</em> happen:</p>

<ul>
<li>If <code>dir2</code> is empty and depending on your shell and settings:

<ul>
<li>you may just get an error message, which is the best case scenario.</li>
<li><code>dir2/*</code> will be taken literally (looking for a file/directory named <code>*</code>), which will probably lead to an error, too, unless <code>*</code> actually exists.</li>
<li><code>dir2/*</code> it will just be removed from the command entirely, leaving <code>cp dir1/*</code>. Which, depending on the expansion of <code>dir1/*</code>, may even destroy data:

<ul>
<li>If <code>dir1/*</code> matches only one file or directory, you will get an error from <code>cp</code>.</li>
<li>If <code>dir1/*</code> matches exactly two files, one will be overwritten by the other (<em>Bad</em>).</li>
<li>If <code>dir/*</code> matches multiple files and the last match is a, you will get an error message.</li>
<li>If the last match of <code>dir/*</code> is a directory all other matches will be moved into it.</li>
</ul></li>
</ul></li>
<li>If <code>dir2</code> is not empty, it again depends:

<ul>
<li>If the last match of <code>dir2/*</code> is a directory, <code>dir1/*</code> and the other matches of <code>dir2/*</code> will be moved into. </li>
<li>If the last match of <code>dir2/*</code> is a file, you probably will get an error message, <em>unless</em> <code>dir1/*</code> matches only one file.</li>
</ul></li>
</ul>
","101923"
"Is there a way to stop having to write 'sudo' for every little thing in Linux?","90457","","<p>I'm a newbie when it comes to Linux.  I do most of my development in a Windows environment.  That said, I'm going to be doing a fair amount of PHP work shortly, and I'm interested in learning RoR, so I installed Linux Mint 12 in my VirtualBox.</p>

<p>The most frustrating aspect of the switch, so far, has been dealing with Linux permissions.  It seems like I can't do anything useful (like, say, copy the Symfony2 tarball from my Downloads directory to my document root and extract it) without posing as the root via sudo.</p>

<p>Is there an easy way to tell linux to give me unfettered access to certain directories without simply blowing open all of their permissions?</p>
","<p>Two options come to my mind:</p>

<ol>
<li><p>Own the directory you want by using <code>chown</code>:   </p>

<pre><code>sudo chown your_username directory 
</code></pre>

<p>(replace your_username with your username and directory with the directory you want.)</p></li>
<li><p>The other thing you can do is work as root as long as you <strong>KNOW WHAT YOU ARE DOING</strong>. To use root do: </p>

<pre><code>sudo -s
</code></pre>

<p>and then you can do anything without having to type <code>sudo</code> before every command.</p></li>
</ol>
","26077"
"Cron job to delete files older than 3 days","90281","","<p>I need to remove files older than 3 days with a cron job in 3 different directories. (these 3 directories are children of a parent directory <code>/a/b/c/1</code> &amp; <code>/a/b/c/2</code> &amp; <code>/a/b/c/3</code>)  Can this be done with one line in the crontab?</p>
","<p>This is easy enough (although note that this goes by a modification time more than 3 days ago since a creation time is only available on certain filesystems with special tools):</p>

<pre><code>find /a/b/c/1 /a/b/c/2 -type f -mtime +3 #-delete
</code></pre>

<p>Remove the <code>#</code> before the <code>-delete</code> once you are sure that it is finding the files you want to remove.</p>

<p>To have it run by cron, I would probably just create an executable script (add a shebang - <code>#!bin/sh</code> to the top line of the file and make executable with <code>chmod a+x</code>), then put it in an appropriate <code>cron</code> directory like <code>/etc/cron.daily</code> or <code>/etc/cron.weekly</code>. Provided of course that you do not need a more specific schedule and that these directories exist on your distro.</p>

<h3>Update</h3>

<p>As noted below, the <code>-delete</code> option for <code>find</code> isn't very portable. A POSIX compatible approach would be:</p>

<pre><code>find /a/b/c/1 /a/b/c/2 -type f -mtime +3 #-exec rm {} +
</code></pre>

<p>Again remove the <code>#</code> when you are sure you have the right files.</p>

<h3>Update2</h3>

<p>To quote from <a href=""https://unix.stackexchange.com/questions/136804/cron-job-to-delete-files-older-than-3-days/136807?noredirect=1#comment218309_136807"">Stéphane Chazelas comment</a> below:</p>

<blockquote>
  <p>Note that <code>-exec rm {} +</code> has race condition vulnerabilities which <code>-delete</code> (where available) doesn't have. So don't use it on directories that are writeable by others. Some finds also have a <code>-execdir</code> that mitigates against those vulnerabilities.</p>
</blockquote>
","136807"
"Harddisk serial number from terminal?","90045","","<p>I have multiple harddisks which get connected to my server and I'm not sure which one is what in the view of sdXY . If I can see the serial numbers of my harddisks from terminal, I can easily identify them.</p>

<p>Is there any way I can see them from terminal?</p>
","<p>Another solution which does not require root privileges:</p>

<p><code>udevadm info --query=all --name=/dev/sda | grep ID_SERIAL</code></p>

<p>This is actually the library that <code>lsblk</code>, mentioned by don_crissti, leverages, but my version of lsblk does not include the option for serial.</p>
","207386"
"How do I kill all screens?","89804","","<p>I accidentally created over 1000 screens. How do I kill them all with one command? (Or a few)</p>
","<p>You can use :</p>

<pre><code>pkill screen
</code></pre>

<p>Or </p>

<pre><code>killall screen
</code></pre>

<hr>

<p>In OSX the process is called SCREEN in all caps. So, use:</p>

<pre><code>pkill SCREEN
</code></pre>

<p>Or</p>

<pre><code>killall SCREEN
</code></pre>
","94528"
"Convert a .xlsx (MS Excel) file to .csv on command line with semicolon separated fields","89641","","<p>I realize that this is <em>not</em> an entirely unix/linux related question. But since this is something I'll do on linux, I hope someone has an answer.</p>

<p>I have an online excel file (<code>.xlsx</code>) which gets updated periodically (by someone else). I want to write a script and put it in as a cronjob in order to to process that excel sheet. But to do that, I need to convert that into a text file (so a <code>.csv</code>) with semicolon separated columns. It can't be comma separated unfortunately since some columns have commas in them. Is it at all possible to do this conversion from shell? I have Open office installed and I can do this by using its GUI, but want to know if it is possible to do this from command line. Thanks!</p>

<p>PS: I have a Mac machine as well, so if some solution can work there, thats good as well. :)</p>
","<p>OpenOffice comes with the <a href=""http://linux.die.net/man/1/unoconv"">unoconv</a> program to perform format conversions on the command line.</p>

<pre><code>unoconv -f csv filename.xlsx
</code></pre>

<p>For more complex requirements, you can parse XLSX files with <a href=""http://search.cpan.org/perldoc?Spreadsheet%3a%3aXLSX""><code>Spreadsheet::XLSX</code></a> in Perl or <a href=""http://packages.python.org/openpyxl/""><code>openpyxl</code></a> in Python. For example, here's a quickie script to print out a worksheet as a semicolon-separated CSV file (warning: untested, typed directly in the browser):</p>

<pre><code>perl -MSpreadsheet::XLSX -e '
    $\ = ""\n""; $, = "";"";
    my $workbook = Spreadsheet::XLSX-&gt;new()-&gt;parse($ARGV[0]);
    my $worksheet = ($workbook-&gt;worksheets())[0];
    my ($row_min, $row_max) = $worksheet-&gt;row_range();
    my ($col_min, $col_max) = $worksheet-&gt;col_range();
    for my $row ($row_min..$row_max) {
        print map {$worksheet-&gt;get_cell($row,$_)-&gt;value()} ($col_min..$col_max);
    }
' filename.xlsx &gt;filename.csv
</code></pre>
","23786"
"What is Fedora's equivalent of 'apt-get purge'?","89610","","<p>In Debian, there's at least two ways to delete a package:</p>

<ul>
<li><code>apt-get remove pkgname</code></li>
<li><code>apt-get purge pkgname</code></li>
</ul>

<p>The first preserves system-wide config files (i.e. those found in ""<strong>/etc</strong>""), while the second doesn't.</p>

<p>What is Fedora's equivalent of the second form, <code>purge</code>? Or maybe I should rather ask if <code>yum remove pkgname</code> actually preserves config files.</p>
","<p><code>yum remove</code> is not guaranteed to preserve configuration files.</p>

<p>As stated in the <a href=""http://www.phy.duke.edu/~rgb/General/yum_HOWTO/yum_HOWTO/yum_HOWTO-10.html"">yum HOWTO</a>:</p>

<blockquote>
  <p>In any event, the command syntax for package removal is:</p>

<pre><code># yum remove package1 [package2 package3...]
</code></pre>
  
  <p>As noted above, it removes package1 and all packages in the dependency tree that depend on package1, <b>possibly irreversibly as far as configuration data is concerned</b>.</p>
</blockquote>

<p><b>Update</b></p>

<p>As James points out, you can use the <code>rpm -e</code> command to erase a package but save backup copies of any configuration files that have changed. </p>

<p>For more information, see <a href=""http://www.rpm.org/max-rpm/ch-rpm-erase.html"">Using RPM to Erase Packages</a>. In particular:</p>

<blockquote>
  <p>It checks to see if any of the package's config files have been modified. If so, it saves copies of them.</p>
</blockquote>
","8193"
"Converting multiple image files from JPEG to PDF format","89582","","<p>I want to convert some files from jpeg to pdf. I am using following command. </p>

<pre><code>$ convert image1.jpg image1.pdf 
</code></pre>

<p>But I have 100 images. How should I convert all of them to corresponding pdfs?</p>

<p>I tried </p>

<pre><code>$ convert image*.jpg image*.pdf 
</code></pre>

<p>It doesn't work. </p>
","<p>In bash:</p>

<pre><code>for f in *.jpg; do
  convert ./""$f"" ./""${f%.jpg}.pdf""
done
</code></pre>
","29871"
"How can I see dmesg output as it changes?","89478","","<p>I'm writing a device driver that prints error message into <em>ring buffer</em> dmesg output.
I want to see the output of <code>dmesg</code> as it changes.</p>

<p>How can I do this?</p>
","<p>Relatively recent <code>dmesg</code> versions <a href=""http://karelzak.blogspot.com/2012/09/util-linux-222.html"">provide a follow option</a> (<code>-w</code>, <code>--follow</code>) which works analogously to <code>tail -f</code>.</p>

<p>Thus, just use following command:</p>

<pre><code>$ dmesg -wH
</code></pre>

<p>(<code>-H</code>, <code>--human</code> enables user-friendly features like colors, relative time)</p>

<p>Those options are available for example in Fedora 19.</p>
","95852"
"How to install/remove/upgrade .rpm packages on Red Hat?","89450","","<p>I am new to linux, and at the university we are using this distribution to mount servers on VM, but I need to install a package that is missing in my virtual machine that is running Red Hat, but how to do that, what command should I use to install/remove/upgrade .rpm packages.</p>
","<p><strong>EDIT:</strong> As recommended in a comment below, you can just use <code>yum</code> for everything and you will be fine.</p>

<p>Check <a href=""https://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/s1-rpm-using.html"">Red Hat's documentation</a> for the <code>rpm</code> command.</p>

<p>Basically:</p>

<ul>
<li><code>rpm -Uvh package_file.rpm</code> installs/upgrades a package <strong>from a .rpm file</strong></li>
<li><code>rpm -e package_name</code> removes a package</li>
</ul>

<p>To install packages from Red Hat's repositories, use <a href=""http://linux.die.net/man/8/yum""><code>yum</code></a>, whose commands are mostly self-explanatory (<code>install</code>, <code>update</code> etc...). <a href=""https://www.centos.org/docs/5/html/yum/"">This documentation</a> is for CentOS 5, but I think it should work for newer versions of Red Hat/CentOS.</p>
","43115"
"How to download a folder from google drive using terminal?","89375","","<p>I want to download a folder from my google drive using terminal? Is there any way to do that? I tried this:</p>

<pre><code>$ wget ""https://drive.google.com/folderview?id=0B-Zc9K0k9q-WWUlqMXAyTG40MjA&amp;usp=sharing""
</code></pre>

<p>But it is downloading this text file: <code>folderview?id=0B-Zc9K0k9q-WdEY5a1BCUDBaejQ&amp;usp=sharing</code>. Is there any way to download google drive folder from terminal?</p>
","<p>Use the project <a href=""https://github.com/google/skicka"">https://github.com/google/skicka</a>,
on the readme you can find the instructions to make it work.</p>
","222572"
"Linux ls to show only filename date and size","89316","","<p>How can I use <code>ls</code> in linux to get a listing of filenames date and size only. I don't need to see the other info such as owner or permission. Is this possible?</p>
","<p>Why not use <code>stat</code> instead of <code>ls</code>?</p>

<pre><code>stat -c ""%y %s %n"" *
</code></pre>
","22227"
"Passing named arguments to shell scripts","89293","","<p>Is there any easy way to pass (receive) named parameters to a shell script?</p>

<p>For example, </p>

<pre><code>my_script -p_out '/some/path' -arg_1 '5'
</code></pre>

<p>And inside <code>my_script.sh</code> receive them as:</p>

<pre><code># I believe this notation does not work, but is there anything close to it?
p_out=$ARGUMENTS['p_out']
arg1=$ARGUMENTS['arg_1']

printf ""The Argument p_out is %s"" ""$p_out""
printf ""The Argument arg_1 is %s"" ""$arg1""
</code></pre>

<p>Is this possible in Bash or Zsh?</p>
","<p>The probably closest syntax to that is:</p>

<pre><code>p_out='/some/path' arg_1='5' my_script
</code></pre>
","129394"
"Batch renaming files","89142","","<p>I have a directory full of images:</p>

<pre><code>image0001.png
image0002.png
image0003.png
...
</code></pre>

<p>And I would like a one-liner to rename them to (say).</p>

<pre><code>0001.png
0002.png
0003.png
...
</code></pre>

<p>How do I do this?</p>
","<p>If you are using <code>bash</code>:</p>

<pre><code>for f in *.png; do mv ""$f"" ""${f#image}""; done
</code></pre>
","1139"
"Add lines to the beginning and end of the huge file","88850","","<p>I have the scenario where lines to be added on begining and end of the huge files.</p>

<p>I have tried as shown below.</p>

<ul>
<li><p>for the first line: </p>

<pre><code>sed -i '1i\'""$FirstLine"" $Filename
</code></pre></li>
<li><p>for the last line: </p>

<pre><code>sed -i '$ a\'""$Lastline"" $Filename  
</code></pre></li>
</ul>

<p>But the issue with this command is that it is appending the first line of the file and traversing entire file. For the last line it's again traversing the entire file and appending a last line.  Since its very huge file (14GB) this is taking very long time.</p>

<p>How can I add a line to the beginning and another to the end of a file while only reading the file once?</p>
","<p><code>sed -i</code> uses tempfiles as an implementation detail, which is what you are experiencing; however, prepending data to the beginning of a data stream without overwriting the existing contents requires rewriting the file, there's no way to get around that, even when avoiding <code>sed -i</code>.</p>

<p>If rewriting the file is not an option, you might consider manipulating it when it is read, for example:</p>

<pre class=""lang-bash prettyprint-override""><code>{ echo some prepended text ; cat file ; } | command
</code></pre>

<p>Also, sed is for editing streams  -- a file is not a stream. Use a program that is meant for this purpose, like ed or ex. The <code>-i</code> option to sed is not only not portable, it will also break any symlinks to your file, since it essentially deletes it and recreates it, which is pointless.</p>

<p>You can do this in a single command with <code>ed</code> like so:</p>

<pre class=""lang-bash prettyprint-override""><code>ed -s file &lt;&lt; 'EOF'
0a
prepend these lines
to the beginning
.
$a
append these lines
to the end
.
w
EOF
</code></pre>

<p>Note that depending on your implementation of ed, it may use a paging file, requiring you to have at least that much space available.</p>
","87774"
"What is /usr/local/bin?","88835","","<p>Before today, I've used the terminal to a limited extent of moving in and out of directories and changing the dates of files using the <code>touch</code> command. I had realised the full extent of the terminal after installing a fun script on Mac and having to <code>chmod 755</code> the file to make it executable afterwards.</p>

<p>I'd like to know what <code>/usr/local/bin</code> is, though. <code>/usr/</code>, I assume, is the user of the computer. I'm not sure why <code>/local/</code> is there, though. It obviously stands for the local computer, but since it's on the computer (or a server), would it really be necessary? Wouldn't <code>/usr/bin</code> be fine?</p>

<p>And what is <code>/bin</code>? Why is this area usually used for installing scripts onto the terminal?</p>
","<p><code>/usr/local/bin</code> is for programs that a normal user may run.</p>

<ul>
<li>The <code>/usr/local</code> hierarchy is for use by the system administrator when installing software locally. </li>
<li>It needs to be safe from being overwritten when the system software is updated. </li>
<li>It may be used for programs and data that
are shareable amongst a group of hosts, but not found in <code>/usr</code>.</li>
<li>Locally installed software must be placed within <code>/usr/local</code> rather than /usr unless it is being installed to
replace or upgrade software in <code>/usr</code>.</li>
</ul>

<p>This source helps explain the <a href=""http://www.pathname.com/fhs/"">filesystem hierarchy standard</a> on a deeper level.</p>

<p>You might find <a href=""http://aplawrence.com/Opinion/religion.html"">this article on the use and abuse of <code>/usr/local/bin</code></a> interesting as well.</p>
","4187"
"Is it possible to find out the hosts in the known_hosts file?","88822","","<p>I would like to see what hosts are in my known_hosts file but it doesn't appear to be human readable.  Is it possible to read it?</p>

<p>More specifically there is a host that I can connect to via several names and I want to find out what the fingerprint I expect for it from my known hosts file.</p>

<p>Update: I'm using OpenSSH_5.3p1 Debian-3ubuntu7, OpenSSL 0.9.8k 25 Mar 2009</p>

<p>A line from my known_hosts file looks something like this,</p>

<pre><code>|1|guO7PbLLb5FWIpxNZHF03ESTTKg=|r002DA8L2JUYRVykUh7jcVUHeYE= ssh-rsa AAAAB3NzaC1yc2EAAFADAQABAAABAQDWp73ulfigmbbzif051okmDMh5yZt/DlZnsx3DEOYHu3Nu/+THJnUAfkfEc1XkOFiFgbUyK/08Ty0K6ExUaffb1ERfXXyyp63rpCTHOPonSrnK7adl7YoPDd4BcIUZd1Dk7HtuShMmuk4l83X623cr9exbfm+DRaeyFNMFSEkMzztBYIkhpA2DWlDkd90OfVAvyoOrJPxztmIZR82qu/5t2z58sJ6Jm2xdp2ckySgXulq6S4k+hnnGuz2p1klviYCWGJMZfyAB+V+MTjGGD/cj0SkL5v/sa/Fie1zcv1SLs466x3H0kMllz6gAk0/FMi7eULspwnIp65g45qUAL3Oj
</code></pre>
","<p>You've got <code>HashKnownHosts</code> set to ""<code>yes</code>"" in your <code>ssh_config</code> file, so the hostnames aren't available in plaintext.</p>

<p>If you know the hostname you're looking for ahead of time, you can search for it with:</p>

<pre><code>    ssh-keygen -H -F hostname
</code></pre>

<p>Here's the relevant section from the <a href=""http://manpages.ubuntu.com/manpages/oneiric/en/man1/ssh-keygen.1.html""><code>ssh-keygen(1)</code></a> man page:</p>

<pre><code> -F hostname
         Search for the specified hostname in a known_hosts file, listing
         any occurrences found.  This option is useful to find hashed host
         names or addresses and may also be used in conjunction with the
         -H option to print found keys in a hashed format.
</code></pre>
","31556"
"How to make user passwords shown as a clear text in Linux?","88744","","<p>We know that users' passwords are saved in <code>/etc/passwd</code>, but in an encrypted way, so even the root can't see them:</p>

<pre><code>jane:x:501:501::/home/jane:/bin/bash
fred:x:502:502::/home/fred:/bin/bash
</code></pre>

<p>As shown above, <code>:x:</code> represents the password.</p>

<p>Is there a way (possible configuration) to save the password in the <code>/etc/passwd</code> in clear text and such that the root can see them?</p>
","<p>The other two answers have told you—correctly!—that <strong>this is a Bad Idea™</strong>. But they've also told you its <em>hard</em> to do, requiring changing a bunch of programs.</p>

<p>That's not true. It's very easy. You only need to change one or two configuration files. I feel its important to point this out, because you should be aware of it when logging into systems you don't control. These won't actually put a plain-text password in <code>/etc/passwd</code> or <code>/etc/shadow</code>, it'll go into a different file. Note I haven't tested these, as I'd rather not have my password in plain text.</p>

<ol>
<li><p>Edit <code>/etc/pam.d/common-password</code> (to catch on password changed) or <code>/etc/pam.d/common-auth</code> (to catch on login) and add in <code>… pam_exec expose_authtok log=/root/passwords /bin/cat</code></p></li>
<li><p>Edit both of those, and switch from pam_unix to pam_userdb with <code>crypt=none</code>. Alternatively, you could put it only in common-password (leaving pam_unix as well) to just record passwords when they're changed.</p></li>
<li><p>You could remove the <code>shadow</code> (as well as any strong hash options) option from pam_unix to disable the shadow file, and go back to traditional crypt passwords. Not plain text, but John the Ripper will fix that for you.</p></li>
</ol>

<p>For further details, check <a href=""http://www.linux-pam.org/Linux-PAM-html/Linux-PAM_SAG.html"">the PAM System Admin Guide</a>.</p>

<p>You could also edit the source code of PAM, or write your own module. You'd only need to compile PAM (or your module), nothing else.</p>
","145500"
"Using --exclude with the du command","88582","","<p>This is probably something basic but I'm not able to make it work. I'm trying to use DU to get a total size of files minus certain directories. I need to exclude one specific directory called <code>uploads</code> but not every directory called <code>uploads</code>. For example, my file structure looks a bit like this:</p>

<pre><code>/store
  /uploads
    /junk_to_ignore
    /more_junk_to_ignore
  /user_one
    /uploads
  /user_two
</code></pre>

<p>I can run the following command: </p>

<pre><code>du -ch --exclude=uploads* 
</code></pre>

<p>and it gives me the file size minus all the ""uploads"" directories. However, in trying to exclude certain directories (and all its sub-directories) I fail. I've tried variations of:</p>

<pre><code>du -ch --exclude=./uploads*
du -ch --exclude='/full/path/to/uploads/*'
</code></pre>

<p>but can't seem to figure it out. How do I exclude a specific directory?</p>
","<p>You've almost found it :)</p>

<pre><code>du -ch --exclude=./relative/path/to/uploads
</code></pre>

<p>Note <strong>no</strong> asterisk at the end. The asterisk means all <em>subdirectories under ""upload""</em> should be omitted - but not the files directly in that directory.</p>
","23693"
"How can I disown a running process and associate it to a new screen shell?","88494","","<p>I have a running program on a SSH shell. I want to pause it and be able to unpause its execution when I come back.</p>

<p>One way I thought of doing that was to transfer its ownership to a screen shell, thus keeping it running in there.</p>

<p>Is there a different way to proceed?</p>
","<p>Using GNU <code>screen</code> is your best bet.</p>

<p>Start screen running when you first login - I run <code>screen -D -R</code>, run your command, and either disconnect or suspend it with <code>CTRL-Z</code> and then disconnect from screen by pressing <code>CTRL-A</code> then <code>D</code>.</p>

<p>When you login to the machine again, reconnect by running <code>screen -D -R</code>. You will be in the same shell as before. You can run <code>jobs</code> to see the suspended process if you did so, and run <code>%1</code> (or the respective job #) to foreground it again.</p>
","8696"
"Simple command line HTTP server","87986","","<p>I have a script which generates a daily report which I want to serve to the so called general public. The problem is I don't want to add to my headaches maintance of a HTTP server (e.g. Apache) with all the configurations and security implications.</p>

<p>Is there a dead simple solution for serving one small HTML page without the effort of configuring a full blown HTTP server?</p>
","<p>Yes, nweb.</p>

<p>Can be found here: <a href=""https://gist.githubusercontent.com/sumpygump/9908417/raw/5fa991fda103d0b7a0c38512394a83ccada9ad6c/nweb23.c"" rel=""noreferrer"">nweb.c</a></p>

<p>(previously at <a href=""http://www.ibm.com/developerworks/systems/library/es-nweb/index.html"" rel=""noreferrer"">ibm.com</a>)</p>

<p>To compile nweb.c:</p>

<pre><code>gcc -O -DLINUX nweb.c -o nweb
</code></pre>
","32184"
"Why can't I run GUI apps from 'root': ""No protocol specified""?","87978","","<p>I installed debian onto my machine last night. Now, I don't understand why I can't run GUI apps from a terminal when running as root.</p>

<p>For example:</p>

<pre><code>sudo -i
glxgears
</code></pre>

<p>Generates the following output:</p>

<pre><code>No protocol specified
Error: couldn't open display :0
</code></pre>

<p>But when I first open the terminal I can run <code>glxgears</code> from the user account. Its only after I do <code>sudo -i</code> that the problem crops up.  This happens for any GUI app that I try to run. I think its probably related to X11, but I'm not sure.</p>
","<p>Accessing the X server requires two things:</p>

<ul>
<li>The <code>$DISPLAY</code> variable pointing to the correct display (usually <code>:0</code>)</li>
<li>Proper authentication information</li>
</ul>

<p>The authentication information can be explicitly specified via <code>$XAUTHORITY</code>, and defaults to <code>~/.Xauthority</code> otherwise.</p>

<p>If <code>$DISPLAY</code> and <code>$XAUTHORITY</code> is set for your user, <code>sudo</code> will set them for the new shell, too, and everything should work fine.</p>

<p>If they are not set, they will probably default to the wrong values and you cannot start and X applications.</p>

<p>In Debian <code>$XAUTHORITY</code> is usually not set explicitly. Just add</p>

<pre><code>export XAUTHORITY=~/.Xauthority
</code></pre>

<p>to your <code>.bashrc</code> or explicitly say <code>XAUTHORITY=~/.Xauthority sudo ...</code> and everything should work.</p>

<p>You can also use <code>xauth list</code> to check whether proper authentication information are available.</p>
","118826"
"Resolving MAC Address from IP Address in Linux","87753","","<p>I need to write a bash script wherein I have to create a file which holds the details of IP Addresses of the hosts and their mapping with corresponding MAC Addresses.</p>

<p>Is there any possible way with which I can find out the MAC address of any (remote) host when IP address of the host is available?</p>
","<p>If you just want to find out the MAC address of a given IP address you can use the command <code>arp</code> to look it up, once you've pinged the system 1 time.</p>

<h3>Example</h3>

<pre><code>$ ping skinner -c 1
PING skinner.bubba.net (192.168.1.3) 56(84) bytes of data.
64 bytes from skinner.bubba.net (192.168.1.3): icmp_seq=1 ttl=64 time=3.09 ms

--- skinner.bubba.net ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 3.097/3.097/3.097/0.000 ms
</code></pre>

<p>Now look up in the ARP table:</p>

<pre><code>$ arp -a
skinner.bubba.net (192.168.1.3) at 00:19:d1:e8:4c:95 [ether] on wlp3s0
</code></pre>

<h3>fing</h3>

<p>If you want to sweep the entire LAN for MAC addresses you can use the command line tool <code>fing</code> to do so. It's typically not installed so you'll have to go download it and install it manually.</p>

<pre><code>$ sudo fing 10.9.8.0/24
</code></pre>

<p>&nbsp;&nbsp;&nbsp; <img src=""https://i.stack.imgur.com/0v6Yb.png"" alt=""fing example""></p>

<h3>References</h3>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/96700/equivalent-of-iwlist-to-see-who-is-around/96707#96707"">Equivalent of iwlist to see who is around?</a></li>
</ul>
","120155"
"How to view a binary file?","87496","","<p>From what I understand, a compiler makes a binary file that consists of 1's and 0's that a CPU can read. I have a binary file but how do I open it to see the 1's and 0's that are there? A text editor says it can't open it...</p>

<p>P.S. I have an assembly compiled binary that should be plain binary code of 1's and 0's?</p>
","<p>According to <a href=""https://stackoverflow.com/a/1765339/309308"">this answer</a> by <a href=""https://stackoverflow.com/users/191953/tyranid"">tyranid</a>:</p>

<blockquote>
<pre><code>hexdump -C yourfile.bin 
</code></pre>
  
  <p>unless you want to edit it of course. Most Linux distros have <code>hexdump</code> by default (but obviously not all).</p>
</blockquote>

<hr>

<h2>Update</h2>

<p>According to <a href=""https://stackoverflow.com/a/20305782/309308"">this answer</a> by <a href=""https://stackoverflow.com/users/1409834/emilio-bool"">Emilio Bool</a>:</p>

<blockquote>
  <p><code>xxd</code> does both binary and hexadecimal</p>
  
  <p>For bin :</p>

<pre><code>xxd -b file
</code></pre>
  
  <p>For hex :</p>

<pre><code>xxd file
</code></pre>
</blockquote>
","282220"
"how can I recursively delete empty directories in my home directory?","87429","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/8430/how-to-remove-all-empty-directories-in-a-subtree"">How to remove all empty directories in a subtree?</a>  </p>
</blockquote>



<p>I create directories very often, scattered over my home directory, and I find it very hard to locate and delete them.</p>

<p>I want any alias/function/script to find/locate and delete all empty directories in my home directory.</p>
","<p>The <code>find</code> command is the primary tool for recursive file system operations.
Use the <code>-type d</code> expression to tell <code>find</code> you're interested in finding directories only (and not plain files). The GNU version of <code>find</code> supports the <code>-empty</code> test, so</p>

<pre><code>$ find . -type d -empty -print
</code></pre>

<p>will print all empty directories below your current directory.</p>

<p>Use <code>find ~ -…</code> or <code>find ""$HOME"" -…</code> to base the search on your home directory (if it isn't your current directory).</p>

<p>After you've verified that this is selecting the correct directories, use <code>-delete</code> to delete all matches:</p>

<pre><code>$ find . -type d -empty -delete
</code></pre>


","46326"
"Kernel inotify watch limit reached","87356","","<p>I'm currently facing a problem on a linux box where as root I have commands returning error because inotify watch limit has been reached.</p>

<pre><code># tail -f /var/log/messages
[...]
tail: cannot watch '/var/log/messages': No space left on device
# inotifywatch -v /var/log/messages
Establishing watches...
Failed to watch /var/log/messages; upper limit on inotify watches reached!
Please increase the amount of inotify watches allowed per user via '/proc/sys/fs/inotify/max_user_watches'.` 
</code></pre>

<p>I googled a bit and every solution I found is to increase the limit with:</p>

<pre><code>sudo sysctl fs.inotify.max_user_watches=&lt;some random high number&gt;
</code></pre>

<p>But I was unable to find any information of the consequences of raising that value. I guess the default kernel value was set for a reason but it seems to be inadequate for particular usages. (e.g., when using Dropbox with a large number of folder, or software that monitors a lot of files)</p>

<p>So here are my questions: </p>

<ul>
<li>Is it safe to raise that value and what would be the consequences of a too high value? </li>
<li>Is there a way to find out what are the currently set watches and which process set them to be able to determine if the reached limit is not caused by a faulty software?</li>
</ul>
","<p><strong>Is it safe to raise that value and what would be the consequences of a too high value?</strong></p>

<p>Yes, it's safe to raise that value and below are the possible costs [<a href=""https://askubuntu.com/questions/154255/how-can-i-tell-if-i-am-out-of-inotify-watches"">source</a>]:</p>

<ul>
<li>Each <em>used</em> inotify watch takes up 540 bytes (32-bit system), or 1 kB (double - on 64-bit) [sources: <a href=""https://groups.google.com/forum/#!msg/lsyncd/KZfQE9VUZFw/twTMjxWa41wJ"" rel=""noreferrer"">1</a>, <a href=""https://stackoverflow.com/q/535768"">2</a>]</li>
<li>This comes out of <em>kernel memory</em>, which is unswappable.</li>
<li>Assuming you set the max at 524288 and all were used (improbable), you'd be using approximately 256MB/512MB of 32-bit/64-bit kernel memory.

<ul>
<li>Note that your application will also use additional memory to keep track of the inotify handles, file/directory paths, etc. -- how much depends on its design.</li>
</ul></li>
</ul>

<p><strong>To check the max number of inotify watches:</strong></p>

<pre><code>cat /proc/sys/fs/inotify/max_user_watches
</code></pre>

<p><strong>To set max number of inotify watches</strong></p>

<p>Temporarily:</p>

<ul>
<li>Run <code>sudo sysctl fs.inotify.max_user_watches=</code> with your preferred value at the end.</li>
</ul>

<p>Permanently (<a href=""https://github.com/guard/listen/wiki/Increasing-the-amount-of-inotify-watchers"" rel=""noreferrer"">more detailed info</a>):</p>

<ul>
<li>put <code>fs.inotify.max_user_watches=524288</code> into your sysctl settings. Depending on your system they might be in one of the following places:

<ul>
<li>Debian/RedHat: <code>/etc/sysctl.conf</code></li>
<li>Arch: put a new file into <code>/etc/sysctl.d/</code>, e.g. <code>/etc/sysctl.d/40-max-user-watches.conf</code></li>
</ul></li>
<li>you may wish to reload the sysctl settings to avoid a reboot: <code>sysctl -p</code> (Debian/RedHat) or <code>sysctl --system</code> (Arch)</li>
</ul>

<p><strong>Check to see if the max number of inotify watches have been reached:</strong></p>

<p>Use <code>tail</code> with the <code>-f</code> (follow) option on any old file, e.g. <code>tail -f /var/log/dmesg</code>:
  - If all is well, it will show the last 10 lines and pause; abort with Ctrl-C
  - If <strong>you are out of watches</strong>, it will fail with this <a href=""https://bugs.launchpad.net/ubuntu/+source/coreutils/+bug/700958"" rel=""noreferrer"">somewhat cryptic error</a>:<br>
   <pre>tail: cannot watch '/var/log/dmsg': No space left on device</pre></p>

<p><strong>To see what's using up inotify watches</strong></p>

<pre><code>find /proc/*/fd -lname anon_inode:inotify |
   cut -d/ -f3 |
   xargs -I '{}' -- ps --no-headers -o '%p %U %c' -p '{}' |
   uniq -c |
   sort -nr
</code></pre>

<p>The first column indicates the number of inotify fds (not the number of watches though) and the second shows the PID of that process [sources: <a href=""https://github.com/atom/atom/issues/2082#issuecomment-55533087"" rel=""noreferrer"">1</a>, <a href=""https://github.com/atom/atom/issues/2082#issuecomment-62388531"" rel=""noreferrer"">2</a>].</p>
","13757"
"How do I update grub in Arch Linux","87345","","<p>In Ubuntu we use below command to update grub : </p>

<pre><code># update-grub
</code></pre>

<p>But how do I update grub version <strong>2.00</strong> in Arch Linux ?
`</p>
","<p>The <code>update-grub</code> command is just a script which runs the <code>grub-mkconfig</code> tool to generate a grub.cfg file. See the Archlinux GRUB <a href=""https://wiki.archlinux.org/index.php/GRUB#Generating_main_configuration_file"">documentation</a>. It refers to the following: </p>

<pre><code># grub-mkconfig -o /boot/grub/grub.cfg
</code></pre>
","111924"
"Running a script during booting/startup; init.d vs cron @reboot","87292","","<p>I am currently trying to understand the difference between <code>init.d</code> and cron <code>@reboot</code> for running a script at startup/booting of the system. </p>

<p>The use of <code>@reboot</code> (this method was mentioned in <a href=""https://askubuntu.com/questions/290099/how-to-run-a-script-during-boot-as-root"">this forum</a> by <em>hs.chandra</em>) is some what simpler, by simply going into <code>crontab -e</code> and creating a <code>@reboot /some_directory/to_your/script/your_script.txt</code> and then <code>your_script.txt</code> shall be executed every time the system is rebooted. An in depth explanation of <code>@reboot</code> is <a href=""http://www.unixdaemon.net/linux/how-does-cron-reboot-work.html"" rel=""nofollow noreferrer"">here</a></p>

<p>Alternatively by embedding <code>/etc/init.d/your_script.txt</code> into the <em>second line</em> of your script ie:</p>

<pre><code>#!/bin/bash
# /etc/init.d/your_script.txt</code></pre>

<p>You can run <code>chmod +x /etc/init.d/your_script.txt</code> and that should also result for <code>your_script.txt</code> to run every time the system is booted.</p>

<p><strong>Q1:</strong> What are the key differences between the two? <br><strong>Q2:</strong> Which is more robust? <br><strong>Q3:</strong> Is there a better one out of the two? <br><strong>Q4:</strong> Is this the correct way of embedding a script to run during booting?</p>

<p>I will be incorporating a bash .sh file to run during startup. </p>
","<p><strong><code>init.d</code></strong>, also known as SysV script, is meant to start and stop services during system initialization and shutdown. (<code>/etc/init.d/</code> scripts are also run on systemd enabled systems for compatibility).</p>

<ul>
<li>The script is executed during the boot and shutdown (by default).</li>
<li>The script should be an init.d script, not just a script . It should support <code>start</code> and <code>stop</code> and more (see <a href=""https://www.debian.org/doc/debian-policy/ch-opersys.html#s-sysvinit"">Debian policy</a>)</li>
<li>The script can be executed <strong>during</strong> the system boot (you can define when).</li>
</ul>

<p><strong><code>crontab</code></strong> (and therefore <code>@reboot</code>).</p>

<ul>
<li>cron will execute any regular command or script, nothing special here.</li>
<li>any user can add a <code>@reboot</code> script (not just root)</li>
<li>on a Debian system with systemd: cron's  @reboot is executed during <code>multi-user.target</code>.</li>
<li>on a Debian system with SysV (not systemd), crontab(5) mention: <em>Please note that startup, as far as @reboot is concerned, is the time when the  cron(8)  daemon  startup.   In particular, it may be before some system daemons, or other facilities, were startup.  This is due to the boot order sequence of the machine.</em></li>
<li>it's easy to schedule the same script at boot and periodically.</li>
</ul>

<p><strong><code>/etc/rc.local</code></strong> is often considered to be ugly or deprecated (at least by <a href=""https://bugzilla.redhat.com/show_bug.cgi?id=734268"">redhat</a>), still it had some nice features:</p>

<ul>
<li>rc.local will execute any regular command or script, nothing special here.</li>
<li>on a Debian system with SysV (not systemd): <code>rc.local</code> was (almost) the last service to start.</li>
<li>but on a Debian system with systemd: <code>rc.local</code> is executed after <code>network.target</code> by default (not <code>network-online.target</code> !)</li>
</ul>

<p>Regarding systemd's <code>network.target</code> and <code>network-online.target</code>, read <a href=""http://www.freedesktop.org/wiki/Software/systemd/NetworkTarget/"">Running Services After the Network is up</a>.</p>
","188825"
"How to modify iptables rules via editing a file rather than interacting via command line?","87223","","<p>In the history, i would simply edit a file and then <br>
reboot the whole server.</p>

<p>i would clone the line that had port 22 open <br>
change it to 80 and then save the file.. and reboot <br>
the whole system so the iptables would start with port 80 open.</p>

<p>but in the recent times.. that file is no longer in existent <br>
in my centos 6.5 O.S.</p>

<p>most answers on google suggest i must interact with iptables <br>
in order to enable and disable ports.</p>

<p>is it possible to not interact with iptables but rather <br>
just see everything infront of you as one editable file ?</p>
","<p>In CentOS you have the file <code>/etc/sysconfig/iptables</code> if you don't have it there, you can create it simply by using <code>iptables-save</code> to dump the current rule set into a file.</p>

<pre><code>iptables-save &gt; /etc/sysconfig/iptables
</code></pre>

<p>To load the file you don't need to restart the machine, you can use <code>iptables-restore</code></p>

<pre><code>iptables-restore &lt; /etc/sysconfig/iptables
</code></pre>
","144603"
"How can I move files and view the progress (e.g. with a progress bar)?","87033","","<p>When moving large directories using <code>mv</code>, is there a way to view the progress (%)?
The <code>cp</code> command on gentoo had a <code>-g</code> switch that showed the progress.</p>
","<p>You can build a patched cp and mv which then both support the -g switch to show progress. There are instructions and patches at <a href=""https://github.com/atdt/advcpmv"" rel=""noreferrer"">this page</a>. <strong>However</strong>: The page instructs you to do</p>

<pre><code>$ sudo cp src/cp /usr/bin/cp
$ sudo cp src/mv /usr/bin/mv
</code></pre>

<p>which overwrites the original cp and mv. This has two disadvantages: Firstly, if an updated coreutils package arrives at your system, they are overwritten. Secondly, if the patched version has a problem, they might break scripts relying on standard cp and mv. I would rather do something like this:</p>

<pre><code>$ sudo cp src/cp /usr/local/bin/cpg
$ sudo cp src/mv /usr/local/bin/mvg
</code></pre>

<p>which copies the files to /usr/local/bin which is intended for user compiled programs and gives them a different name. So when you want a progress bar, you say <code>mvg -g bigfile /mnt/backup</code> and use mv normally.</p>

<p>Also you can do <code>alias mvg=""/usr/local/mvg -g""</code> then you only need to say <code>mvg bigfile /mnt/backup</code> and directly get the progress bar.</p>
","2579"
"What does $# mean in shell?","86941","","<p>What does <code>$#</code> mean in shell?</p>

<p>I have code such as</p>

<pre><code>if [ $# -eq 0 ]
then
</code></pre>

<p>I want to understand what <code>$#</code> means, but Google search is very bad for searching these kinds of things.</p>
","<p>You can always check the man page of your shell. <code>man bash</code> says:</p>

<pre><code>Special Parameters
   #      Expands to the number of positional parameters in decimal.
</code></pre>

<p>Therefore a shell script can check how many parameters are given with code like this:</p>

<pre><code>if [ ""$#"" -eq 0 ]; then
  echo ""you did not pass any parameter""
fi
</code></pre>
","122346"
"How to automatically mount an USB device on plugin-time on an already running system?","86858","","<p>I know how to use <code>/etc/fstab</code> to automatically mount devices on boot or when doing <code>sudo mount -a</code>, which works perfectly fine. For example, here is my current line for my device</p>

<pre><code>UUID=B864-497A /media/usbstick vfat defaults,users,noatime,nodiratime,umask=000 0 0
</code></pre>

<p><strong>How do I achieve automatic mounting when this USB device with known UUID is plugged in while the system is already running, so that I <em>don't</em> have to run <code>sudo mount -a</code> after it is plugged in?</strong></p>

<p>Additional info: I'm working on an up-to-date console-only Debian wheezy linux.</p>
","<p>I use the <code>usbmount</code> package to automount USB drives on my Ubuntu server install.  I have confirmed that <a href=""https://packages.debian.org/wheezy/usbmount"">the package exists for Wheezy</a> too. Recently also added for <a href=""https://packages.debian.org/search?keywords=usbmount"">Jessie</a>.  </p>

<pre><code>sudo apt-get install usbmount  
</code></pre>

<p><code>usbmount</code> will automount hfsplus, vfat, and ext (2, 3, and 4) file systems. You can configure it to mount more/different file systems in <code>/etc/usbmount/usbmount.conf</code>.  By default it mounts these file systems with the <code>sync,noexec,nodev,noatime,nodiratime</code> options, however this can also be changed in the aforementioned configuration file.  </p>

<p><code>usbmount</code> also supports custom mount options for different file system types and custom mountpoints.  </p>
","134906"
"Parenthesis in expr arithmetic: 3 * (2 + 1)","86843","","<p><code>expr</code> does not seem to like parenthesis (used in mathematics to explicit operator priority):</p>

<pre><code>expr 3 * (2 + 1)
bash: syntax error near unexpected token `('
</code></pre>

<p>How to express operator priority in bash?</p>
","<p>Another way to use <code>let</code> bash builtin:</p>

<pre><code>$ let a=""3 * (2 + 1)""
$ printf '%s\n' ""$a""
9
</code></pre>

<p><strong>Note</strong></p>

<p>As <a href=""https://unix.stackexchange.com/a/149916/38906"">@Stéphane Chazelas pointed out</a>, in <code>bash</code> you should use <code>((...))</code> to do arithmetic over <code>expr</code> or <code>let</code> for legibility.</p>

<p>For portability, use <code>$((...))</code> like <a href=""https://unix.stackexchange.com/a/149830/38906"">@Bernhard answer</a>.</p>
","149832"
"Get exit status of process that's piped to another","86812","","<p>I have two processes <code>foo</code> and <code>bar</code>, connected with a pipe:</p>

<pre><code>$ foo | bar
</code></pre>

<p><code>bar</code> always exits 0; I'm interested in the exit code of <code>foo</code>. Is there any way to get at it?</p>
","<p>If you are using <code>bash</code>, you can use the <code>PIPESTATUS</code> array variable to get the exit status of each element of the pipeline.</p>

<pre><code>$ false | true
$ echo ""${PIPESTATUS[0]} ${PIPESTATUS[1]}""
1 0
</code></pre>

<p>If you are using <code>zsh</code>, they array is called <code>pipestatus</code> (case matters!) and the array indices start at one:</p>

<pre><code>$ false | true
$ echo ""${pipestatus[1]} ${pipestatus[2]}""
1 0
</code></pre>

<p>To combine them within a function in a manner that doesn't lose the values:</p>

<pre><code>$ false | true
$ retval_bash=""${PIPESTATUS[0]}"" retval_zsh=""${pipestatus[1]}"" retval_final=$?
$ echo $retval_bash $retval_zsh $retval_final
1 0
</code></pre>

<p>Run the above in <code>bash</code> or <code>zsh</code> and you'll get the same results; only one of <code>retval_bash</code> and <code>retval_zsh</code> will be set.  The other will be blank.  This would allow a function to end with <code>return $retval_bash $retval_zsh</code> (note the lack of quotes!).</p>
","14276"
"How to upgrade Debian stable (Wheezy) to testing (Jessie)?","86812","","<p>I downloaded and installed Debian 7 Wheezy, the stable version, but I would like more recent apps and libs so I'd like to switch to testing version (aka Jessie).
How do I proceed?</p>
","<p>Simply switch your repos to testing and do a full upgrade:</p>

<pre><code># cp /etc/apt/sources.list{,.bak}
# sed -i -e 's/ \(stable\|wheezy\)/ testing/ig' /etc/apt/sources.list
# apt-get update
# apt-get --download-only dist-upgrade
# apt-get dist-upgrade
</code></pre>

<p>Make sure you stay plugged in for the duration of the last command, though. If it's botched or incomplete and you have to restart, you may need to resort to a re-install. Needless to say, never try this on a production machine.</p>

<p>The safer thing, of course, is to download a testing image and try it on a virtual machine first or something...</p>

<p><strong>Edit</strong></p>

<p><code>apt-get --download-only dist-upgrade</code> added based on Michael Kjörling's suggestion.</p>
","90391"
"Understanding the exclamation mark (!) in bash","86617","","<p>I used </p>

<pre><code>history | less
</code></pre>

<p>to get the lines of previous commands and from the numbers on the left hand side I found the line I wanted repeated (eg. 22) and did</p>

<pre><code>!22
</code></pre>

<p>at the command prompt and it worked -- executing the set of commands on the line I did at that time. I cannot figure out where the exclamation mark is used, what does it represent in terms of actions taken by bash, and where to use it. From the documentation I do not see an explanation that is 'tangible'.</p>
","<p><code>!</code> is a feature that originally appeared in the <a href=""http://en.wikipedia.org/wiki/C_shell"">C shell</a>, back in the days before you could count on terminals to have arrow keys.  It's especially useful if you add the current command number to the prompt (<code>PS1=""\!$ ""</code>) so you can quickly look at your screen to get numbers for past commands.</p>

<p>Now that you can use arrow keys and things like <kbd>Ctrl-R</kbd> to search the command history, I don't see much use for the feature.  </p>

<p>One variant of it you might still find useful is <code>!!</code>, which re-executes the previous command. On its own, I don't find <kbd>!</kbd><kbd>!</kbd><kbd>Enter</kbd> any faster than just <kbd>&uarr;</kbd> <kbd>Enter</kbd>, but it can be helpful when combined into a larger command.</p>

<p><strong>Example:</strong> A common <a href=""http://foldoc.org/pilot+error"">pilot error</a> on <a href=""http://www.sudo.ws/""><code>sudo</code></a> based systems is to forget the <code>sudo</code> prefix on a command that requires extra privileges. A novice retypes the whole command. The diligent student edits the command from the shell's command history. The enlightened one types <code>sudo !!</code>.</p>

<p>Bash lets you disable <code>!</code> processing in the shell with <code>set +o histexpand</code> or <code>set +H</code>. You can disable it in Zsh with <code>set -K</code>.</p>
","3748"
"How to uncompress zlib data in UNIX?","86458","","<p>I have created zlib-compressed data in Python, like this:</p>

<pre><code>import zlib
s = '...'
z = zlib.compress(s)
with open('/tmp/data', 'w') as f:
    f.write(z)
</code></pre>

<p>(or one-liner in shell: <code>echo -n '...' | python2 -c 'import sys,zlib; sys.stdout.write(zlib.compress(sys.stdin.read()))' &gt; /tmp/data</code>)</p>

<p>Now, I want to uncompress the data in shell. Neither <code>zcat</code> nor <code>uncompress</code> work:</p>

<pre><code>$ cat /tmp/data | gzip -d -
gzip: stdin: not in gzip format

$ zcat /tmp/data 
gzip: /tmp/data.gz: not in gzip format

$ cat /tmp/data | uncompress -
gzip: stdin: not in gzip format
</code></pre>

<p>It seems that I have created gzip-like file, but without any headers. Unfortunately I don't see any option to uncompress such raw data in gzip man page, and the zlib package does not contain any executable utility.</p>

<p>Is there a utility to uncompress raw zlib data?</p>
","<p>It is also possible to decompress it using standard shell script + gzip. The trick is to prepend the gzip magic number and compress method (see <a href=""http://www.onicos.com/staff/iz/formats/gzip.html"">http://www.onicos.com/staff/iz/formats/gzip.html</a>) to the actual data:</p>

<pre><code>printf ""\x1f\x8b\x08\x00\x00\x00\x00\x00"" |cat - zlib.raw |gzip -dc
</code></pre>
","49066"
"Method to check connectivity to other server","86432","","<p>I want to check connectivity between 2 servers (i.e. if ssh will succeed).</p>

<p>The main idea is to check the shortest way between server-a and server-b using a list of middle servers (for example if I'm on dev server and I want to connect to prod server - usually a direct ssh will fail).</p>

<p>Because this can take a while, I prefer not to use SSH - rather I prefer to check first if I can connect and if so then try to connect through SSH.</p>

<p>Some possible routes to get the idea:</p>

<pre><code>server-a -&gt; server-b
server-a -&gt; middle-server-1 -&gt; server-b
server-a -&gt; middle-server-6 -&gt; server-b
server-a -&gt; middle-server-3 -&gt; middle-server-2 -&gt; server-b
</code></pre>

<p>Hope you understand what I'm looking for?</p>
","<p>For checking server connectivity you have 4 tools at your disposal.</p>

<ol>
<li><p>ping</p>

<p>This will check to see if any of the servers you're attempting to connect through, but won't be able to see if middle-server-1 can reach server-b, for example.</p>

<p>You can gate how long ping will attempt to ping another server through the use of the count switch (<code>-c</code>). Limiting it to 1 should suffice.</p>

<pre><code>$ ping -c 1 skinner
PING skinner (192.168.1.3) 56(84) bytes of data.
64 bytes from skinner (192.168.1.3): icmp_req=1 ttl=64 time=5.94 ms

--- skinner ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 5.946/5.946/5.946/0.000 ms
</code></pre>

<p>You can check the status of this command through the use of this variable, <code>$?</code>. If it has the value 0 then it was successful, anything else and a problem occurred.</p>

<pre><code>$ echo $?
0
</code></pre></li>
<li><p>traceroute</p>

<p>Another command you can use to check connectivity is <code>traceroute</code>.</p>

<pre><code>$ traceroute skinner
traceroute to skinner (192.168.1.3), 30 hops max, 60 byte packets
1  skinner (192.168.1.3)  0.867 ms  0.859 ms  0.929 ms
</code></pre>

<p>Again this tool will not show connectivity through one server to another (same issue as ping), but it will show you the path through the network that your taking to get to another server.</p></li>
<li><p>ssh</p>

<p><code>ssh</code> can be used in <code>BatchMode</code> to test connectivity. With <code>BatchMode=yes</code> you'll attempt to connect to another server, bypassing the use of username/passwords and only public/private keys. This typically speeds things up quite a bit.</p>

<pre><code>$ ssh -o ""BatchMode=yes"" skinner
</code></pre>

<p>You can construct a rough one liner that will check for connectivity to a server:</p>

<pre><code>$ ssh -q -o ""BatchMode=yes"" skinner ""echo 2&gt;&amp;1"" &amp;&amp; echo $host SSH_OK || echo $host SSH_NOK

SSH_OK
</code></pre>

<p>If it works you'll get a <code>SSH_OK</code> message, if it fails you'll get a <code>SSH_NOK</code> message.</p>

<p>An alternative to this method is to also include the <code>ConnectTimeout</code> option. This will guard the <code>ssh</code> client from taking a long time. Something like this typically is acceptable, <code>ConnectTimeout=5</code>. For example:</p>

<pre><code>$ ssh -o BatchMode=yes -o ConnectTimeout=5 skinner echo ok 2&gt;&amp;1
ok
</code></pre>

<p>If it fails it will look something like this:</p>

<pre><code>$ ssh -o BatchMode=yes -o ConnectTimeout=5 mungr echo ok 2&gt;&amp;1
ssh: connect to host 192.168.1.2 port 22: No route to host
</code></pre>

<p>It will also set the return status:</p>

<pre><code>$ echo $?
255
</code></pre></li>
<li><p>telnet</p>

<p>You can use this test to see if an <code>ssh</code> server is accessible on another server using just a basic <code>telnet</code>:</p>

<pre><code>$ echo quit | telnet skinner 22 2&gt;/dev/null | grep Connected
Connected to skinner.
</code></pre></li>
</ol>
","88287"
"rename multiple files with mv","86348","","<p>I want to rename a couple of files in the same directory. </p>

<p>When you start to google this there is plenty of help available.
What I saw the most if you only want to change the extension is <code>mv *.txt *.tsv</code> but when doing this I get : </p>

<blockquote>
  <p>*.tsv is not a directory </p>
</blockquote>

<p>I find it somewhat strange that the first 10 google hits show mv should work like this.</p>

<p>I found another way of doing it but i was simply wondering if there is someone who can explain this properly. Thanks!</p>
","<p>When you issue the command:</p>

<pre><code>mv *.txt *.tsv
</code></pre>

<p>the shell, lets assume bash, <em>expands</em> the wildcards <strong><em>if</em></strong> there are any matching files (including directories). The list of files are passed to the program, here <code>mv</code>. If no matches are found the unexpanded version is passed.</p>

<p>Again: the <em>shell</em> expands the patterns, not the program.</p>

<hr>

<p>Loads of examples is perhaps best way, so here we go:</p>

<h3>Example 1:</h3>

<pre><code>$ ls
file1.txt file2.txt

$ mv *.txt *.tsv
</code></pre>

<p>Now what happens on the <code>mv</code> line is that <em>the shell</em> expands <code>*.txt</code> to the matching files. As there are no <code>*.tsv</code> files that is not changed.</p>

<p>The <code>mv</code> command is called <em>with two special arguments</em>:</p>

<ul>
<li><code>argc</code>: Number of arguments, including the program.</li>
<li><code>argv</code>: An array of arguments, including the program as first entry.</li>
</ul>

<p>In the above example that would be:</p>

<pre><code> argc = 4
 argv[0] = mv
 argv[1] = file1.txt
 argv[2] = file2.txt
 argv[3] = *.tsv
</code></pre>

<p>The <code>mv</code> program check to see if last argument, <code>*.tsv</code>, is a directory. As it is not, the program can not continue as it is not designed to concatenate files. (Typically move all the files into one.) Nor create directories on a whim.</p>

<p>As a result it aborts and reports the error:</p>

<blockquote>
<pre><code>mv: target ‘*.tsv’ is not a directory
</code></pre>
</blockquote>

<hr>

<h3>Example 2:</h3>

<p>Now if you instead say:</p>

<pre><code>$ mv *1.txt *.tsv
</code></pre>

<p>The <code>mv</code> command is executed with:</p>

<pre><code> argc = 3
 argv[0] = mv
 argv[1] = file1.txt
 argv[2] = *.tsv
</code></pre>

<p>Now, again, <code>mv</code> check to see if <code>*.tsv</code> exists. As it does not the file <code>file1.txt</code> is moved to <code>*.tsv</code>. That is: the file is renamed to <code>*.tsv</code> with the asterisk and all.</p>

<pre><code>$ mv *1.txt *.tsv
‘file1.txt’ -&gt; ‘*.tsv’

$ ls
file2.txt *.tsv
</code></pre>

<hr>

<h3>Example 3:</h3>

<p>If you instead said:</p>

<pre><code>$ mkdir *.tsv
$ mv *.txt *.tsv
</code></pre>

<p>The <code>mv</code> command is executed with:</p>

<pre><code> argc = 3
 argv[0] = mv
 argv[1] = file1.txt
 argv[1] = file2.txt
 argv[2] = *.tsv
</code></pre>

<p>As <code>*.tsv</code> now is a directory, the files ends up being moved there.</p>

<hr>

<p>Now: using commands like <code>some_command *.tsv</code> when the intention is to actually keep the wildcard one should always quote it. By quoting you prevent the wildcards from being expanded if there should be any matches. E.g. say <code>mkdir ""*.tsv""</code>.</p>

<h3>Example 4:</h3>

<p>The expansion can further be viewed if you do for example:</p>

<pre><code>$ ls
file1.txt file2.txt

$ mkdir *.txt
mkdir: cannot create directory ‘file1.txt’: File exists
mkdir: cannot create directory ‘file2.txt’: File exists
</code></pre>

<hr>

<h3>Example 5:</h3>

<p>Now: the <code>mv</code> command can and do work on multiple files. But if there is more then two the last has to be a target directory. (Optionally you can use the <code>-t TARGET_DIR</code> option, at least for GNU mv.)</p>

<p>So this is OK:</p>

<pre><code>$ ls -F
b1.tsv  b2.tsv  f1.txt  f2.txt  f3.txt  foo/

$ mv *.txt *.tsv foo
</code></pre>

<p>Here <code>mv</code> would be called with:</p>

<pre><code> argc = 7
 argv[0] = mv
 argv[1] = b1.tsv
 argv[2] = b2.tsv
 argv[3] = f1.txt
 argv[4] = f2.txt
 argv[5] = f3.txt
 argv[6] = foo
</code></pre>

<p>and all the files end up in the directory <code>foo</code>.</p>

<hr>

<p>As for your links. You have provided one (in a comment), where <code>mv</code> is not mentioned at all, but <code>rename</code>. If you have more links you could share. As well as for man pages where you claim this is expressed.</p>
","181156"
"What is the correct syntax to add CFLAGS and LDFLAGS to ""configure""?","86278","","<p>I am a Unix/OpenBSD beginner and wish to install OpenVPN on OpenBSD 5.5 using OpenVPN source tarball.</p>

<p>According to the instructions <a href=""https://github.com/OpenVPN/openvpn/blob/master/INSTALL"">here</a>, I have to install lzo and </p>

<blockquote>
  <p>add CFLAGS=""-I/usr/local/include"" LDFLAGS=""-L/usr/local/lib""
  directives to ""configure"", since gcc will not find them otherwise.</p>
</blockquote>

<p>I have googled extensively for guide on how to do the above on OpenBSD but there is none.</p>

<p>This is what I plan to do:</p>

<ol>
<li>Untar the source tarball to a freshly created directory</li>
<li>Issue the command ./configure CFLAGS=""-I/usr/local/include"" LDFLAGS=""-L/usr/local/lib""</li>
<li>Issue the command make</li>
<li>Issue the command make install</li>
</ol>

<p>Which of the following syntax is correct?</p>

<pre><code>./configure CFLAGS=""-I/usr/local/include"" LDFLAGS=""-L/usr/local/lib""
</code></pre>

<p>or</p>

<pre><code>./configure --CFLAGS=""-I/usr/local/include"" LDFLAGS=""-L/usr/local/lib""
</code></pre>

<p>or</p>

<pre><code>./configure --CFLAGS=""-I/usr/local/include"" --LDFLAGS=""-L/usr/local/lib""
</code></pre>
","<p>The correct way is:</p>

<pre><code>./configure CFLAGS=""-I/usr/local/include"" LDFLAGS=""-L/usr/local/lib""
</code></pre>

<p>but this may not work with all <code>configure</code> scripts. It's probably better to set environment variables such as <code>CPATH</code> and <code>LIBRARY_PATH</code> (see <code>gcc</code> man page).</p>

<p>An example:</p>

<pre><code>export CPATH=/usr/local/include
export LIBRARY_PATH=/usr/local/lib
export LD_LIBRARY_PATH=/usr/local/lib
</code></pre>

<p>in your <code>.profile</code>, for instance. The <code>LD_LIBRARY_PATH</code> can be needed in case of shared libraries if a run path is not used (this depends on the OS, the build tools and the options that are used, but it shouldn't hurt).</p>
","149361"
"Why is FreeBSD deprecating GCC in favor of Clang/LLVM?","86277","","<p>So I was surfing the net and stumbled upon <a href=""http://www.phoronix.com/scan.php?page=news_item&amp;px=MTE4NDQ"" rel=""noreferrer"">this article</a>. It basically states that <a href=""http://www.freebsd.org/"" rel=""noreferrer"">FreeBSD</a>, starting from Version 10 and above will deprecate <a href=""http://gcc.gnu.org/"" rel=""noreferrer"">GCC</a> in favor of <a href=""http://llvm.org/"" rel=""noreferrer"">Clang/LLVM</a>. </p>

<p>From what I have seen around the net so far, <a href=""http://llvm.org/"" rel=""noreferrer"">Clang/LLVM</a> is a fairly ambitious project, but in terms of reliability it can not match <a href=""http://gcc.gnu.org/"" rel=""noreferrer"">GCC</a>.</p>

<p>Are there any <strong>technical</strong> reasons FreeBSD are choosing LLVM as their compiler infrastructure, or does the whole matter boil down to the eternal GNU/GPL vs. BSD licenses?</p>

<p><a href=""https://unix.stackexchange.com/questions/27206/why-does-freebsd-use-the-gpl-licensed-gcc?rq=1"">This question</a> has (somehow) relevant information about the usage of <a href=""http://gcc.gnu.org/"" rel=""noreferrer"">GCC</a> in <a href=""http://www.freebsd.org/"" rel=""noreferrer"">FreeBSD</a></p>
","<p><em><strong>Summary:</em></strong> <em>The primary reason for switching from <a href=""http://gcc.gnu.org/"">GCC</a> to <a href=""http://clang.llvm.org/"">Clang</a> is the incompatibility of GCC's <a href=""http://www.gnu.org/copyleft/gpl.html"">GPL v3</a> license with the <a href=""http://www.freebsd.org/doc/faq/introduction.html#FreeBSD-goals"">goals of the FreeBSD project</a>. There are also political issues to do with corporate investment, as well as user base requirements. Finally, there are expected technical advantages to do with standards compliance and ease of debugging. Real world performance improvements in compilation and execution are code-specific and debatable; cases can be made for both compilers.</em></p>

<p><strong>FreeBSD and the GPL:</strong> <a href=""http://www.freebsd.org/"">FreeBSD</a> has an uneasy relationship with the GPL. BSD-license advocates believe that truly free software has <a href=""http://www.freebsd.org/doc/faq/introduction.html#bsd-license-restrictions"">no usage restrictions</a>. GPL advocates believe that <a href=""http://www.gnu.org/philosophy/pragmatic.html"">restrictions are necessary</a> in order to protect software freedom, and specifically that the ability to create non-free software from free software is an <a href=""http://www.gnu.org/philosophy/freedom-or-power.html"">unjust form of power</a> rather than a freedom. The FreeBSD project, where possible, tries to <a href=""http://www.freebsd.org/doc/faq/introduction.html#FreeBSD-goals"">avoid the use of the GPL</a>:</p>

<blockquote>
  <p>Due to the additional complexities that can evolve in the commercial
  use of GPL software, we do, however, endeavor to replace such software
  with submissions under the more relaxed FreeBSD license whenever
  possible.</p>
</blockquote>

<p><strong>FreeBSD and the GPL v3:</strong> The <a href=""http://www.gnu.org/copyleft/gpl.html"">GPL v3</a> explicitly forbids the so-called <a href=""http://en.wikipedia.org/wiki/Tivoization"">Tivoisation</a> of code, a loophole in the <a href=""http://www.gnu.org/licenses/gpl-2.0.html"">GPL v2</a> which enabled hardware restrictions to disallow otherwise legal software modifications by users. Closing this loophole was an <a href=""http://www.freebsdfoundation.org/press/2007Aug-newsletter.shtml"">unacceptable step</a> for many in the FreeBSD community:</p>

<blockquote>
  <p>Appliance vendors in particular have the most to lose if the large
  body of software currently licensed under GPLv2 today migrates to the
  new license. They will no longer have the freedom to use GPLv3
  software and restrict modification of the software installed on their
  hardware... In short, there is a large
  base of OpenSource consumers that are suddenly very interested in
  understanding alternatives to GPL licensed software.</p>
</blockquote>

<p>Because of GCC's move to the GPL v3, FreeBSD was forced to remain using GCC 4.2.1 (GPL v2), which was <a href=""http://gcc.gnu.org/gcc-4.2/"">released way back in 2007</a>, and is now significantly outdated. The fact that FreeBSD did not move to use more modern versions of GCC, even with the additional maintenance headaches of running an old compiler and backporting fixes, gives some idea of the strength of the requirement to avoid the GPL v3. The C compiler is a major component of the FreeBSD base, and ""<a href=""http://wiki.freebsd.org/GPLinBase"">one of the (tentative) goals for FreeBSD 10 is a GPL-free base system</a>"".</p>

<p><strong>Corporate investment:</strong> Like many major open source projects, FreeBSD receives <a href=""http://www.freebsdfoundation.org/donate/sponsors.shtml"">funding</a> and <a href=""http://www.kuro5hin.org/story/2008/11/17/16268/141"">development work</a> from corporations. Although the extent to which FreeBSD is funded or given development by Apple is not easily discoverable, there is considerable overlap because Apple's <a href=""http://en.wikipedia.org/wiki/Darwin_%28operating_system%29"">Darwin OS</a> makes use of substantial BSD-originated <a href=""http://en.wikipedia.org/wiki/Darwin_%28operating_system%29#Kernel"">kernel code</a>. Additionally, Clang itself was originally an in-house Apple project, before being <a href=""http://lists.cs.uiuc.edu/pipermail/cfe-dev/2007-July/000000.html"">open-sourced in 2007</a>. Since corporate resources are a key enabler of the FreeBSD project, meeting sponsor needs is probably <a href=""http://lists.freebsd.org/pipermail/freebsd-questions/2012-June/242495.html"">a significant real-world driver</a>.</p>

<p><strong>Userbase:</strong> FreeBSD is an attractive open source option for many companies, because the licensing is simple, unrestrictive and unlikely to lead to lawsuits. With the arrival of GPL v3 and the new <a href=""http://fsfe.org/campaigns/gplv3/brussels-rms-transcript#tivoisation"">anti-Tivoisation provisions</a>, it has been suggested that there is an <a href=""http://www.itworld.com/it-managementstrategy/233753/gpl-copyleft-use-declining-faster-ever"">accelerating, vendor-driven trend towards more permissive licenses</a>. Since FreeBSD's perceived advantage to commercial entities lies in its permissive license, there is increasing pressure from the corporate user base to move away from GCC, and the GPL in general.</p>

<p><strong>Issues with GCC:</strong> Apart from the license, using GCC has <a href=""http://lists.freebsd.org/pipermail/freebsd-questions/2012-June/242709.html"">some perceived issues</a>. GCC is not fully-standards compliant, and has <a href=""http://gcc.gnu.org/onlinedocs/gcc-4.0.4/gcc/C-Extensions.html"">many extensions not found in ISO standard C</a>. At over 3 million lines of code, it is also ""<a href=""http://www.cse.iitb.ac.in/grc/"">one of the most complex and free/open source software projects</a>"". This complexity makes distro-level code modification a challenging task.</p>

<p><strong>Technical advantages:</strong> Clang does have some <a href=""http://clang.llvm.org/comparison.html#gcc"">technical advantages compared to GCC</a>. Most notable are <a href=""http://clang.llvm.org/diagnostics.html"">much more informative error messages</a> and an <a href=""http://clang.llvm.org/doxygen/group__CINDEX.html"">explicitly designed API</a> for IDEs, refactoring and source code analysis tools. Although the Clang website <a href=""http://clang.llvm.org/features.html#performance"">presents plots</a> indicating much more efficient compilation and memory usage, real world results are <a href=""http://en.wikipedia.org/wiki/Clang#Performance_and_GCC_compatibility"">quite variable</a>, and broadly in line with GCC performance. In general, Clang-produced binaries <a href=""http://www.phoronix.com/scan.php?page=article&amp;item=gcc_llvm_clang&amp;num=6"">run more slowly</a> than the equivalent GCC binaries:</p>

<blockquote>
  <p>While using LLVM is faster at building code than GCC... in most
  instances the GCC 4.5 built binaries had performed better than
  LLVM-GCC or Clang... in the rest of the tests the performance was
  either close to that of GCC or well behind. In some tests, the
  performance of the Clang generated binaries was simply awful.</p>
</blockquote>

<p><strong>Conclusion:</strong> It's highly unlikely that compilation efficiency would be a significant motivator to take the substantial risk of moving a large project like FreeBSD to an entirely new compiler toolchain, particularly when binary performance is lacking. However, the situation was not really tenable. Given a choice between 1) running an out-of-date GCC, 2) Moving to a modern GCC and being forced to use a license incompatible with the goals of the project or 3) moving to a stable BSD-licensed compiler, the decision was probably inevitable. Bear in mind that this only applies to the base system, and support from the distribution; nothing prevents a user from installing and using a modern GCC on their FreeBSD box themselves.</p>
","49970"
"Creating a GIF animation from PNG files","86224","","<p>Is there a tool to create a <code>gif</code> animation from a set of <code>png</code> files?</p>

<p>I tried the <code>convert</code> command from the <a href=""http://www.imagemagick.org/"">ImageMagick</a> suite, but this doesn't always succeed. Also, I have several issues with this:</p>

<ol>
<li>I can't tell what the progress is.</li>
<li>No matter what I try, the <code>-delay</code> flag doesn't change the frame rate of the gif animation.</li>
<li><code>convert</code> determines the frame order based upon the alphabetical order of the files names. This means that <code>name500.png</code> will be placed right after <code>name50.png</code> and not after <code>name450.png</code> I can fix this by adding 0's but this is annoying.</li>
</ol>
","<blockquote>
  <p>Newer versions of ffmpeg have no -sameq (<a href=""https://ffmpeg.org/faq.html#Why-was-the-ffmpeg-_002dsameq-option-removed_003f-What-to-use-instead_003f"">see faq</a>) but do have GIF support.</p>

<pre><code>ffmpeg -i %03d.png output.gif
</code></pre>
  
  <p>Where %03d is the frame ID in 3 digits.</p>
</blockquote>

<p>You may also try to use <code>ffmpeg</code> to create a movie out of a sequence of images and then convert the movie to a GIF animation (again using <code>ffmpeg</code>).</p>

<pre><code># cf. http://pages.uoregon.edu/noeckel/MakeMovie.html

# first convert an image sequence to a movie
ffmpeg -sameq -i %03d.jpg output.mp4

# ... and then convert the movie to a GIF animation
ffmpeg -i output.mp4 -pix_fmt rgb24 -s qcif -loop_output 0 output.gif
</code></pre>
","24103"
"Difference between 2>&1 > output.log and 2>&1 | tee output.log","86197","","<p>I wanted to know the difference between the following two commands </p>

<pre><code>2&gt;&amp;1 &gt; output.log 
</code></pre>

<p>and </p>

<pre><code>2&gt;&amp;1 | tee output.log
</code></pre>

<p>I saw one of my colleague use second option to redirect. I know what 2>&amp;1 does, my only question is what is the purpose of using tee where a simple redirection "">"" operator can be used?</p>
","<p><code>2&gt;&amp;1 &gt;output.log</code> means first start sending all file handle 2 stuff (standard error) to file handle 1 (standard output) <em>then</em> send that to the file <code>output.log</code>. In other words, send standard error and standard output to the log file.</p>

<p><code>2&gt;&amp;1 | tee output.log</code> is the same with the <code>2&gt;&amp;1</code> bit, it combines standard output and standard error on to the standard output stream. It then pipes that through the <code>tee</code> program which will send its standard input to its standard output (like <code>cat</code>) and <em>also</em> to the file. So it combines the two streams (error and output), then outputs that to the terminal and the file.</p>

<p>The bottom line is that the first sends <code>stderr</code>/<code>stdout</code> to the file, while the second sends it to <em>both</em> the file and standard output (which is <em>probably</em> the terminal unless you're inside another construct which has redirected standard output).</p>

<p>I mention that last possibility because you can have stuff like:</p>

<pre><code>(echo hello | tee xyzzy.txt) &gt;plugh.txt
</code></pre>

<p>where nothing ends up on the terminal.</p>
","20472"
"gcc can't link to pthread?","86183","","<p>I have recently installed XUbuntu 11.10 64bit, but I am having problem compiling the most simple pthread example.</p>

<p>Here is the code <code>pthread_simple.c</code>:</p>

<pre class=""lang-c prettyprint-override""><code>#include &lt;stdio.h&gt;
#include &lt;pthread.h&gt; 
main()  {
  pthread_t f2_thread, f1_thread; 
  void *f2(), *f1();
  int i1,i2;
  i1 = 1;
  i2 = 2;
  pthread_create(&amp;f1_thread,NULL,f1,&amp;i1);
  pthread_create(&amp;f2_thread,NULL,f2,&amp;i2);
  pthread_join(f1_thread,NULL);
  pthread_join(f2_thread,NULL);
}
void *f1(int *x){
  int i;
  i = *x;
  sleep(1);
  printf(""f1: %d"",i);
  pthread_exit(0); 
}
void *f2(int *x){
  int i;
  i = *x;
  sleep(1);
  printf(""f2: %d"",i);
  pthread_exit(0); 
}
</code></pre>

<p>And here is the compile command</p>

<pre>
gcc -lpthread pthread_simple.c
</pre>

<p>The results:</p>

<pre>
lptang@tlp-linux:~/test/test-pthread$ gcc -lpthread pthread_simple.c 
/tmp/ccmV0LdM.o: In function `main':
pthread_simple.c:(.text+0x2c): undefined reference to `pthread_create'
pthread_simple.c:(.text+0x46): undefined reference to `pthread_create'
pthread_simple.c:(.text+0x57): undefined reference to `pthread_join'
pthread_simple.c:(.text+0x68): undefined reference to `pthread_join'
collect2: ld returned 1 exit status
</pre>

<p>Does anyone know what's causing the problem?</p>
","<p>In the latest versions of <code>gcc</code> compiler require that libraries follow the object or source files.</p>

<p>So to compile this it should be: </p>

<pre><code>gcc pthread_sample.c -lpthread
</code></pre>

<p>Normally though pthread code is compiled this way:</p>

<pre><code>gcc -pthread pthread_sample.c
</code></pre>
","33398"
"Fix ""firefox is already running"" issue in Linux","86139","","<p>I am trying to open Firefox in CentOS, but I'm getting the following message:</p>

<blockquote>
  <p>Firefox is already running but is not responding</p>
</blockquote>

<p>and Firefox doesn't open. I tried this in command line:  </p>

<pre><code>kill Firefox
</code></pre>

<p>but it didn't work. Also, I don't know in which directory I must execute the right commands.</p>

<p>How can I fix this?</p>
","<ol>
<li><p>First find the process id of firefox using the following command in any directory:</p>

<pre><code>pidof firefox
</code></pre></li>
<li><p>Kill firefox process using the following command in any directory:</p>

<pre><code>kill [firefox pid]
</code></pre></li>
</ol>

<p>Then start firefox again.</p>

<p>Or you can do the same thing in just one command.As don_crissti said:</p>

<pre><code>kill $(pidof firefox)
</code></pre>
","78690"
"Getting tmux to copy a buffer to the clipboard","86130","","<p>I am trying to figure out a decent way to copy what I have in a tmux buffer into my clipboard.  I have tried a couple of different things like </p>

<pre><code>bind-key p select-pane -t 2 \; split-window 'xsel -i -b' \; paste-buffer
</code></pre>

<p>which gets me fairly close, all I have to do is hit control-d after I do prefix-p.</p>

<p>I tried fixing that by doing </p>

<pre><code>bind-key p select-pane -t 2 \; split-window 'xsel -i -b &lt;&lt; HERE\; tmux paste-buffer\; echo HERE'
</code></pre>

<p>But that just doesn't work.  In fact if I pair this down to just </p>

<pre><code>bind-key p select-pane -t 2 \; split-window 'xsel -i -b &lt;&lt; HERE'
</code></pre>

<p>it doesn't do anything so I am assuming that split-window doesn't like &lt;&lt; in a shell command.</p>

<p>Any ideas?</p>

<p>Edit:
You can skip the <code>select-pane -t 2</code> if you want, it isn't really important.  I just use a specific layout and pane 2 is the one I prefer to split when I doing something else so that goes into my bindings involving splits by default.</p>
","<p>Here documents need newlines. For example, in a shell script, you can write</p>

<pre><code>cat &lt;&lt;EOF &gt;somefile; echo  done
file contents
EOF
</code></pre>

<p>I don't think tmux lets you put newlines there, and even if it did, this wouldn't be a good approach. What if the data itself contains <code>HERE</code> alone on a line (e.g. because you're copying your <code>.tmux.conf</code>)?</p>

<p>I suggest to write the buffer contents to a temporary file. Untested:</p>

<pre><code>bind-key p save-buffer ~/.tmux-buffer \; run-shell ""xsel -i -b &lt;~/.tmux-buffer &amp;&amp; rm ~/.tmux-buffer""
</code></pre>

<p>There's a race condition if you use this command in two separate tmux instances. I don't know how to solve this.</p>
","15716"
"How to do an IF statement from the result of a executed command","85975","","<p>I am trying to do an IF statement from the output of an executed commmand.  Here is how I am trying to do it, but it doesn't work.   Does anyone know the right way to do this?</p>

<pre><code>if [ ""`netstat -lnp | grep ':8080'`"" == *java* ]; then
  echo ""Found a Tomcat!""
fi
</code></pre>
","<p>Use the bash <code>[[</code> conditional construct and prefer the <code>$(</code>&lt;command&gt;<code>)</code> command substitution convention. Additionally, <code>[[</code> prevents word splitting of variable values therefore there is no need to quote the command substitution bit..</p>

<pre><code>if [[ $(netstat -lnp | grep ':8080') = *java* ]]; then
  echo ""Found a Tomcat!""
fi
</code></pre>
","52801"
"How to print only the duplicate values from a text file?","85956","","<p>Suppose there is a column of numeric values like following:</p>

<p>File1:</p>

<pre><code>1 
2
3
3
3
4
4
4
5
6</code></pre>

<p>I want the output: </p>

<pre><code>3  
4</code></pre>

<p>That is, only the repeated lines. Are there any command line tools to find this out in Linux? (NB: The values are numerically sorted).</p>
","<p>You can use <code>uniq(1)</code> for this:</p>

<pre><code>uniq -d file.txt
</code></pre>

<p>This will print out the duplicates only. The input file needs to be sorted such that all duplicates are consecutive (which they appear to be), so run it through sort first if it is not.</p>
","52540"
"CentOS 6 ""Default"" Installation Options","85912","","<p>Can someone tell me the difference between a Desktop Install, a Basic Server install, and a Minimal Install? During installation, it doesn't give a description and I can't find documentation on it either.</p>

<p>This is for a CentOS 6 installation.</p>
","<p>As you've already noticed, <a href=""http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html-single/Installation_Guide/index.html#s1-pkgselection-x86"">Red Hat's description</a> is vague about what each suite actually includes. Below is a list of the package groups the each suite will install. </p>

<p>You can get more information about what package group by running <code>yum groupinfo foo-bar</code>. The names listed below differ from what <code>yum grouplist</code> will list but the groupinfo cobase, console-internet, core, debugging, directory-client, hardware-monitoring, java-platform, large-systems, network-file-system-client, performance, perl-runtime, server-platformmmand still works on them. </p>

<p>I got this by mounting <a href=""http://mirror.centos.org/centos-6/6/os/x86_64/images/install.img"">http://mirror.centos.org/centos-6/6/os/x86_64/images/install.img</a> and looking at /usr/lib/anaconda/installclasses/rhel.py inside the image.</p>

<p><strong>Desktop</strong>: base, basic-desktop, core, debugging, desktop-debugging, desktop-platform, directory-client, fonts, general-desktop, graphical-admin-tools, input-methods, internet-applications, internet-browser, java-platform, legacy-x, network-file-system-client, office-suite, print-client, remote-desktop-clients, server-platform, x11</p>

<p><strong>Minimal Desktop</strong>: base, basic-desktop, core, debugging, desktop-debugging, desktop-platform, directory-client, fonts, input-methods, internet-browser, java-platform, legacy-x, network-file-system-client, print-client, remote-desktop-clients, server-platform, x11</p>

<p><strong>Minimal</strong>: core</p>

<p><strong>Basic Server</strong>: base, console-internet, core, debugging, directory-client, hardware-monitoring, java-platform, large-systems, network-file-system-client, performance, perl-runtime, server-platform</p>

<p><strong>Database Server</strong>: base, console-internet, core, debugging, directory-client, hardware-monitoring, java-platform, large-systems, network-file-system-client, performance, perl-runtime, server-platform, mysql-client, mysql, postgresql-client, postgresql, system-admin-tools</p>

<p><strong>Web Server</strong>: base, console-internet, core, debugging, directory-client, java-platform, mysql-client, network-file-system-client, performance, perl-runtime, php, postgresql-client, server-platform, turbogears, web-server, web-servlet</p>

<p><strong>Virtual Host</strong>: base, console-internet, core, debugging, directory-client, hardware-monitoring, java-platform, large-systems, network-file-system-client, performance, perl-runtime, server-platform, virtualization, virtualization-client, virtualization-platform</p>

<p><strong>Software Development Workstation</strong>: additional-devel, base, basic-desktop, core, debugging, desktop-debugging, desktop-platform, desktop-platform-devel, development, directory-client, eclipse, emacs, fonts, general-desktop, graphical-admin-tools, graphics, input-methods, internet-browser, java-platform, legacy-x, network-file-system-client, performance, perl-runtime, print-client, remote-desktop-clients, server-platform, server-platform-devel, technical-writing, tex, virtualization, virtualization-client, virtualization-platform, x11</p>
","20528"
"Why would someone choose FreeBSD over Linux?","85765","","<p>Why would someone choose <a href=""http://en.wikipedia.org/wiki/FreeBSD"">FreeBSD</a> over Linux? What are the advantages of FreeBSD compared to Linux? (My shared hosting provider uses FreeBSD.)</p>
","<p>If you want to know what's different so you can use the system more efficiently, here is a <a href=""http://www.over-yonder.net/~fullermd/rants/bsd4linux/01"">commonly referenced introduction to BSD to people coming from a Linux background</a>.</p>

<p>If you want more of the historical context for this decision, I'll just take a guess as to why they chose FreeBSD. Around the time of the first dot-com bubble, FreeBSD 4 was <em>extremely</em> popular with ISPs. This may or may not have been related to the addition of <a href=""http://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;sektion=2""><code>kqueue</code></a>. The Wikipedia page describes the feelings for FreeBSD 4 thusly: ""…widely regarded as one of the most stable and high performance operating systems of the whole Unix lineage."" FreeBSD in particular has added other features over time which would appeal to hosting providers, such as <a href=""http://www.freebsd.org/doc/handbook/jails.html""><code>jail</code></a> and ZFS support.</p>

<p>Personally, I really like the BSD systems because they just <em>feel</em> like they fit together better than most Linux distros I've used. Also, the documentation provided directly in the various handbooks, etc. is outstanding. If you're going to be using FreeBSD, I highly recommend the <a href=""http://www.freebsd.org/doc/handbook/"">FreeBSD Handbook</a>.</p>
","14491"
"Grabbing the first [x] characters for a string from a pipe","85669","","<p>If I have really long output from a command (single line) but I know I only want the first [x] (let's say 8) characters of the output, what's the easiest way to get that? There aren't any delimiters.</p>
","<p>One way is to use <code>cut</code>:</p>

<pre><code> command | cut -c1-8
</code></pre>

<p>This will give you the first 8 characters of each line of output.  Since <code>cut</code> is part of POSIX, it is likely to be on most Unices.</p>
","3455"
"Returning a value from a bash function","85346","","<p>I have a function which returns 1 if the number is a valid ten digit number:</p>

<pre><code>valNum()
{
    flag=1
    if [[ $1 != [1-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9] ]]; then
        echo ""Invalid Number""
        flag=0
    fi
    return $flag
}
</code></pre>

<p>It is getting called by:</p>

<pre><code>if [[ $(valNum $num) -eq 1 ]]; then
      #do something
fi
</code></pre>

<p>The function is working fine if the number is valid but is showing syntax error if input a invalid number. </p>
","<p>@choroba's answer is correct, however this example might be clearer:</p>

<pre><code>valNum $num
valNumResult=$? # '$?' is the return value of the previous command
if [[ $valNumResult -eq 1 ]]
then
  : # do something
fi
</code></pre>

<p>This example is a little longer (setting <code>$valNumResult</code> then querying that value), but more-explicitly describes what happens: that <code>valNum()</code> returns a value, and that value can be queried and tested.</p>

<p>P.S. Please do yourself a favor and return <code>0</code> for <code>true</code> and non-zero for <code>false</code>.  That way you can use the return value to indicate ""why we failed"" in the failure case.</p>
","90739"
"What makes grep consider a file to be binary?","85188","","<p>I have some database dumps from a Windows system on my box. They are text files. I'm using cygwin to grep through them. These appear to be plain text files; I open them with text editors such as notepad and wordpad and they look legible. However, when I run grep on them, it will say <code>binary file foo.txt matches</code>.</p>

<p>I have noticed that the files contain some ascii <code>NUL</code> characters, which I believe are artifacts from the database dump.</p>

<p>So what makes grep consider these files to be binary? The <code>NUL</code> character? Is there a flag on the filesystem? What do I need to change to get grep to show me the line matches?</p>
","<p>If there is a <code>NUL</code> character anywhere in the file, grep will consider it as a binary file.</p>

<p>There might a workaround like this <code>cat file | tr -d '\000' | yourgrep</code> to eliminate all null first, and then to search through file.</p>
","19911"
"Redirecting the content of a file to the command ""echo""","85065","","<p>I have a file named <code>my_file.txt</code> whose content is just the string <code>Hello</code>. How could I redirect its content to the command <code>echo</code>?</p>

<p>I know I have the commands <code>less</code>, <code>cat</code>, <code>more</code>... but I need to do it with <code>echo</code>.</p>

<p>I tried this: </p>

<pre><code>$ cat my_file.txt | echo
</code></pre>

<p>and also this:</p>

<pre><code>$ echo &lt; my_file.txt
</code></pre>

<p>But in both cases it appears only a blank in the stdout, not the content of my_file.txt.</p>

<p>How could I do that? </p>

<p>Thanks in advance :-)</p>
","<p>You can redirect all you want to  <code>echo</code> but it won't do anything with it. <code>echo</code> doesn't read its standard input. All it does is write to standard output its <em>arguments</em> separated by a space character and terminated by a newline character (and with some <code>echo</code> implementations with some escape sequences in them expanded).</p>

<p>If you want <code>echo</code> to display the content of a file, you have to pass that content as an argument to <code>echo</code>. Something like:</p>

<pre><code>echo ""$(cat my_file.txt)""
</code></pre>

<p>Note that <code>$(...)</code> strips the trailing newline character<b>s</b> from the output of that <code>cat</code> command, and <code>echo</code> adds <strong>one</strong> back.</p>

<p>Also note that except with <code>zsh</code>, you can't pass NUL characters in the arguments of a command, so that above will typically not work with binary files. <code>yash</code> will also remove bytes that don't form part of valid characters.</p>

<p>If the reason for wanting to do that is because you want <code>echo</code> to expand the <code>\n</code>, <code>\b</code>, <code>\0351</code>... escape sequences in the file (as UNIX conformant <code>echo</code> implementations do, <a href=""https://unix.stackexchange.com/q/65803"">but not all</a>), then <a href=""/q/65803"">you'd rather use <code>printf</code> instead</a>:</p>

<pre><code>printf '%b\n' ""$(cat my_file.txt)""
</code></pre>

<p>Contrary to <code>echo</code>, that one is portable and won't have problems if the content of the file starts with <code>-</code>.</p>

<hr>

<p>As an alternative to <code>$(cat file)</code>, <a href=""/a/368663"">with <code>ksh</code>, <code>zsh</code> and <code>bash</code>, one can also do: <code>$(&lt;file)</code></a>. That's a special operator whereby the <em>shell</em> as opposed to <code>cat</code> reads the content of the  file to make up the expansion. It still strips the trailing newlines and chokes on NUL bytes except in <code>zsh</code>. In <code>bash</code>, that still forks an extra process. Also note that one difference is that you won't get any error if trying to read a file of type directory that way. Also, while <code>$(&lt; file)</code> is special, <code>$(&lt; file; other command)</code> is not (in <code>zsh</code>, when not emulating other shell, that would still expand the content of the <code>file</code>, by running the <em>implicit</em> <code>$READNULLCMD</code> command (typically a pager)).</p>
","63663"
"rm -rf all files and all hidden files without . & .. error","85038","","<p><code>rm -rf /some/path/*</code> deletes all non-hidden files in that dir (and subdirs).</p>

<p><code>rm -rf /some/path/.*</code> deletes all hidden files in that dir (but not subdirs) and also gives the following error/warning:</p>

<pre><code>rm: cannot remove directory: `/some/dir/.'
rm: cannot remove directory: `/some/dir/..'
</code></pre>

<p>What is the proper way to remove all hidden and non-hidden files and folders recursively in a target directory without receiving the warning/error about <code>.</code> and <code>..</code>?</p>
","<p>You could always send error messages to <code>/dev/null</code></p>

<pre><code>rm -rf /some/path/.* 2&gt; /dev/null
</code></pre>

<p>You could also just</p>

<pre><code>rm -rf /some/path/
mkdir /some/path/
</code></pre>

<p>...then you won't have to bother with hidden files in the first place.</p>
","77196"
"What is the difference between Halt and Shutdown commands?","84737","","<p>What is the difference between the <code>halt</code> and <code>shutdown</code> commands?</p>
","<p>Generally, one uses the <a href=""http://linux.die.net/man/8/shutdown""><code>shutdown</code> command</a>. It allows a time delay and warning message before shutdown or reboot, which is important for system administration of multiuser shell servers; it can provide the users with advance notice of the downtime.</p>

<p>As such, the shutdown command has to be used like this to halt/switch off the computer immediately (on Linux and FreeBSD at least):</p>

<pre><code>shutdown -h now
</code></pre>

<p>Or to reboot it with a custom, 30 minute advance warning:</p>

<pre><code>shutdown -r +30 ""Planned software upgrades""
</code></pre>

<p>After the delay, <code>shutdown</code> tells <a href=""http://linux.die.net/man/8/init""><code>init</code></a> to change to runlevel 0 (halt) or 6 (reboot). (Note that omitting <code>-h</code> or <code>-r</code> will cause the system to go into single-user mode (runlevel 1), which kills most system processes but does not actually halt the system; it still allows the administrator to remain logged in as root.)</p>

<p>Once system processes have been killed and filesystems have been unmounted, the system halts/powers off or reboots automatically. This is done using the <a href=""http://linux.die.net/man/8/halt""><code>halt</code> or <code>reboot</code> command</a>, which syncs changes to disks and then performs the actual halt/power off or reboot.</p>

<p>On Linux, if <code>halt</code> or <code>reboot</code> is run when the system has not already started the shutdown process, it will invoke the <code>shutdown</code> command automatically rather than directly performing its intended action. However, <a href=""http://www.gsp.com/cgi-bin/man.cgi?section=8&amp;topic=halt"">on systems such as FreeBSD</a>, these commands first log the action in <a href=""http://www.gsp.com/cgi-bin/man.cgi?section=5&amp;topic=wtmp""><code>wtmp</code></a> and then will <em>immediately</em> perform the halt/reboot themselves, without first killing processes or unmounting filesystems.</p>
","8694"
"BASH: launch background process and check when it ends","84560","","<p>How can I launch a process in background and check when it ends within a bash script? 
My idea is a script like this:</p>

<pre><code>launch backgroundprocess &amp;
while [ Process is running ];do
   echo ""PROCESS IS RUNNING\r""
done;

echo ""PROCESS TERMINATED""
</code></pre>
","<p>The key is the ""wait"" command:</p>

<pre><code>#!/bin/bash

/my/process &amp;
/another/process &amp;
wait
echo ""All processes done!""
</code></pre>
","76719"
"Difference between pts and tty","84522","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/4126/what-is-the-exact-difference-between-a-terminal-a-shell-a-tty-and-a-cons"">What is the exact difference between a &#39;terminal&#39;, a &#39;shell&#39;, a &#39;tty&#39; and a &#39;console&#39;?</a>  </p>
</blockquote>



<p>I always see pts and tty when I use the <code>who</code> command but I never understand how they are different? Can somebody please explain me this?</p>
","<p>A <em>tty</em> is a native terminal device, the backend is either hardware or kernel emulated.</p>

<p>A <em>pty</em> (pseudo terminal device) is a terminal device which is emulated by an other program (example: <code>xterm</code>, <code>screen</code>, or <code>ssh</code> are such programs). A <em>pts</em> is the slave part of a <em>pty</em>.</p>

<p>(More info can be found in <code>man pty</code>.)</p>

<p><strong>Short summary</strong>:</p>

<p>A <em>pty</em> is created by a process through <code>posix_openpt()</code> (which usually opens the special device <code>/dev/ptmx</code>), and is constituted by a pair of bidirectional character devices:</p>

<ol>
<li><p>The master part, which is the file descriptor obtained by this process through this call, is used to emulate a terminal. After some initialization, the second part can be unlocked with <code>unlockpt()</code>, and the master is used to receive or send characters to this second part (slave).</p></li>
<li><p>The slave part, which is anchored in the filesystem as <code>/dev/pts/x</code> (the real name can be obtained by the master through <code>ptsname()</code> ) behaves like a native terminal device (<code>/dev/ttyx</code>). In most cases, a shell is started that uses it as a controlling terminal.</p></li>
</ol>
","21294"
"How to merge two files based on the matching of two columns?","84459","","<p>I have file1 likes:</p>

<pre><code>0   AFFX-SNP-000541  NA
0   AFFX-SNP-002255  NA
1   rs12103          0.6401
1   rs12103_1247494  0.696
1   rs12142199       0.7672
</code></pre>

<p>And a file2:</p>

<pre><code>0   AFFX-SNP-000541   1
0   AFFX-SNP-002255   1
1   rs12103           0.5596
1   rs12103_1247494   0.5581
1   rs12142199        0.4931
</code></pre>

<p>And would like a file3 such that:</p>

<pre><code>0   AFFX-SNP-000541     NA       1
0   AFFX-SNP-002255     NA       1
1   rs12103             0.6401   0.5596
1   rs12103_1247494     0.696    0.5581
1   rs12142199          0.7672   0.4931
</code></pre>

<p>Which means to put the 4th column of file2 to file1 by the name of the 2nd column. </p>
","<p>This should do it:</p>

<pre><code>join -j 2 -o 1.1,1.2,1.3,2.3 file1 file2
</code></pre>

<p><strong>Important</strong>: this assumes your files are sorted (as in your example) according to the SNP name. If they are not, sort them first:</p>

<pre><code>join -j 2 -o 1.1,1.2,1.3,2.3 &lt;(sort -k2 file1) &lt;(sort -k2 file2)
</code></pre>

<h3>Output:</h3>

<pre><code>0 AFFX-SNP-000541 NA 1
0 AFFX-SNP-002255 NA 1
1 rs12103 0.6401 0.5596
1 rs12103_1247494 0.696 0.5581
1 rs12142199 0.7672 0.4931
</code></pre>

<h3>Explanation (from <code>info join</code>):</h3>

<blockquote>
  <p>`join' writes to standard output a line for each pair of input lines
  that have identical join fields.  </p>
</blockquote>

<pre><code>`-1 FIELD'
     Join on field FIELD (a positive integer) of file 1.

`-2 FIELD'
     Join on field FIELD (a positive integer) of file 2.

`-j FIELD'
     Equivalent to `-1 FIELD -2 FIELD'.

`-o FIELD-LIST'

 Otherwise, construct each output line according to the format in
 FIELD-LIST.  Each element in FIELD-LIST is either the single
 character `0' or has the form M.N where the file number, M, is `1'
 or `2' and N is a positive field number.
</code></pre>

<p>So, the command above joins the files on the second field and prints the 1st,2nd and 3rd field of file one, followed by the 3rd field of file2.</p>
","113903"
"Is there a way to dynamically refresh the less command?","84357","","<p>I like the <code>watch</code> command, but it has <a href=""https://unix.stackexchange.com/questions/3842/how-can-i-scroll-within-the-output-of-my-watch-command"">its limitations</a>.</p>

<p>I'm curious to know whether I could mimic the functionality of <code>watch</code> with <code>less</code>. I'm mainly looking for the ability to scroll through my directory as it dynamically gets modified via a running script.</p>
","<p>In <code>less</code>, you can type <code>F</code> to keep reading at the end of a file (like <code>tail -f</code>); you can type <code>:e</code> and a file name to view a different file, but unfortunately, if you enter the name of the current file, <code>less</code> doesn't reload the file. However there's a trick to make it re-read the current file, suggested by <a href=""https://unix.stackexchange.com/users/67483/sabgenton"">sabgenton</a>: <strong>type <code>:e</code> and enter the name of a non-existent file</strong>; that causes less to display an error message and then reload the current file.</p>

<p>If you're looking for an alternative to <code>watch ls</code>, here are a few:</p>

<ul>
<li>Modern file managers (e.g. Nautilus, Thunar, Konqueror, Dolphin, Finder) refresh views in real time.</li>
<li>Emacs doesn't have real-time refresh, but with <code>auto-revert-mode</code>, it will reload the file or directory every 5 seconds (the delay is configurable).</li>
<li>Although w3m is primarily a web browser, it makes a passable directory and text file viewer. Press <code>R</code> to reload the (local) URL.</li>
</ul>
","4361"
"What's the best way to join files again after splitting them?","84329","","<p>If I have a large file and need to split it into 100 megabyte chunks I will do</p>

<pre><code>split -b 100m myImage.iso
</code></pre>

<p>That usually give me something like</p>

<pre><code>xaa
xab
xac
xad
</code></pre>

<p>And to get them back together I have been using</p>

<pre><code>cat x* &gt; myImage.iso
</code></pre>

<p>Seems like there should be a more efficient way than reading through each line of code in a group of files with <code>cat</code> and redirecting the output to a new file. Like a way of just opening two files, removing the <code>EOF</code> marker from the first one, and connecting them - without having to go through all the contents.</p>

<p>Windows/DOS has a copy command for binary files. The help mentions that this command was designed to able able to combine multiple files. It works with this syntax: (<code>/b</code> is for binary mode)</p>

<pre><code>copy /b file1 + file2 + file3 outputfile
</code></pre>

<p>Is there something similar or a better way to join large files on Linux than cat?</p>

<p><strong>Update</strong></p>

<p>It seems that <code>cat</code> is in fact the right way and best way to join files. Glad to know i was using the right command all along :) Thanks everyone for your feedback.</p>
","<p>That's just what <strong><code>cat</code></strong> was made for. Since it is one of the oldest GNU tools, I think it's very unlikely that any other tool does that faster/better. And it's not <em>piping</em> - it's only redirecting output.</p>
","24631"
"Sorting files according to size recursively","84203","","<p>how to scan a folder recursively and find the largest files in the folder, and sort them by size. I have tried using <code>ls -R -S</code> but it tends to list the directories as well I need to scan the folders and print the files in the folder and sub folders in sorted manner. </p>

<p>I tried using <code>find</code> but coudn't find a suitable solution.</p>
","<p>You can also do this with just <code>du</code>. Just to be on the safe side I'm using this version of <code>du</code>:</p>

<pre><code>$ du --version
du (GNU coreutils) 8.5
</code></pre>

<p>The approach:</p>

<pre><code>$ du -ah ..DIR.. | grep -v ""/$"" | sort -rh
</code></pre>

<h3>Breakdown of approach</h3>

<p>The command <code>du -ah DIR</code> will produce a list of all the files and directories in a given directory <code>DIR</code>. The <code>-h</code> will produce human readable sizes which I prefer. If you don't want them then drop that switch. I'm using the <code>head -6</code> just to limit the amount of output!</p>

<pre><code>$ du -ah ~/Downloads/ | head -6
4.4M    /home/saml/Downloads/kodak_W820_wireless_frame/W820_W1020_WirelessFrames_exUG_GLB_en.pdf
624K    /home/saml/Downloads/kodak_W820_wireless_frame/easyshare_w820.pdf
4.9M    /home/saml/Downloads/kodak_W820_wireless_frame/W820_W1020WirelessFrameExUG_GLB_en.pdf
9.8M    /home/saml/Downloads/kodak_W820_wireless_frame
8.0K    /home/saml/Downloads/bugs.xls
604K    /home/saml/Downloads/netgear_gs724t/GS7xxT_HIG_5Jan10.pdf
</code></pre>

<p>Easy enough to sort it smallest to biggest:</p>

<pre><code>$ du -ah ~/Downloads/ | sort -h | head -6
0   /home/saml/Downloads/apps_archive/monitoring/nagios/nagios-check_sip-1.3/usr/lib64/nagios/plugins/check_ldaps
0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/0/index/write.lock
0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/0/translog/translog-1365292480753
0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/1/index/write.lock
0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/1/translog/translog-1365292480946
0   /home/saml/Downloads/data/elasticsearch/nodes/0/indices/logstash-2013.04.06/2/index/write.lock
</code></pre>

<p>Reverse it, biggest to smallest:</p>

<pre><code>$ du -ah ~/Downloads/ | sort -rh | head -6
10G /home/saml/Downloads/
3.8G    /home/saml/Downloads/audible/audio_books
3.8G    /home/saml/Downloads/audible
2.3G    /home/saml/Downloads/apps_archive
1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip
1.5G    /home/saml/Downloads/digital_blasphemy
</code></pre>

<p>Don't show me the directory, just the files:</p>

<pre><code>$ du -ah ~/Downloads/ | grep -v ""/$"" | sort -rh | head -6 
3.8G    /home/saml/Downloads/audible/audio_books
3.8G    /home/saml/Downloads/audible
2.3G    /home/saml/Downloads/apps_archive
1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip
1.5G    /home/saml/Downloads/digital_blasphemy
835M    /home/saml/Downloads/apps_archive/cad_cam_cae/salome/Salome-V6_5_0-LGPL-x86_64.run
</code></pre>

<p>If you just want the list of smallest to biggest, but the top 6 offending files you can reverse the sort switch, drop (<code>-r</code>), and use <code>tail -6</code> instead of the <code>head -6</code>.</p>

<pre><code>$ du -ah ~/Downloads/ | grep -v ""/$"" | sort -h | tail -6
835M    /home/saml/Downloads/apps_archive/cad_cam_cae/salome/Salome-V6_5_0-LGPL-x86_64.run
1.5G    /home/saml/Downloads/digital_blasphemy
1.5G    /home/saml/Downloads/digital_blasphemy/db1440ppng.zip
2.3G    /home/saml/Downloads/apps_archive
3.8G    /home/saml/Downloads/audible
3.8G    /home/saml/Downloads/audible/audio_books
</code></pre>
","88069"
"Full DD copy from hdd to hdd","84116","","<p><em>ORIGINAL QUESTION:</em></p>

<p>If I have <a href=""http://www.pcworld.co.uk/gbuk/components-upgrades/internal-hard-drives/3-5-inch-hard-drives/wd-mainstream-3-5-internal-hard-drive-3tb-21651952-pdt.html"" rel=""noreferrer"">2 identical hard drives</a> with the following characteristics:</p>

<ul>
<li>SATA 6.0 Gb/s</li>
<li>5400 rpm</li>
<li>3TB</li>
</ul>

<p>How long should a full dd copy take to complete?</p>

<p>So far it's been running for 5 hours and still going...</p>

<p>I am using Linux Ubuntu 12.04 64bit and the command I am using is:</p>

<pre><code>dd if=/dev/sdb of=/dev/sdc
</code></pre>

<hr>

<p><strong>UPDATE: 1</strong></p>

<p>I can now see the progress, and it's been 6+ hours for copy 430GB.  The HDD is 3TB...</p>

<p>Is there no faster way of doing this?</p>

<hr>

<p><strong>UPDATE: 2</strong></p>

<p>This seems a lot better than before (Thanks to Groxxda for the suggestions):</p>

<pre><code>sudo dd if=/dev/sdb bs=128K | pv -s 3000G | sudo dd of=/dev/sdc bs=128K
</code></pre>

<p>ETA is about 9 hours for 3TB, whereas before it reached 430GB after 6 hours, so I am guessing it would have taken about 36 hours using the previous command.</p>
","<p><code>dd</code> was useful in the old days when people used tapes (when block sizes mattered) and when simpler tools such as <code>cat</code> might not be binary-safe.</p>

<p>Nowadays, <code>dd if=/dev/sdb of=/dev/sdc</code> is a just complicated, error-prone, <a href=""https://unix.stackexchange.com/questions/9432/is-there-a-way-to-determine-the-optimal-value-for-the-bs-parameter-to-dd/9492#9492"">slow</a> way of writing <code>cat /dev/sdb &gt;/dev/sdc</code>. While <a href=""https://unix.stackexchange.com/questions/12532/dd-vs-cat-is-dd-still-relevant-these-days/12538#12538""><code>dd</code> still useful for some relatively rare tasks</a>, it is a lot less useful than the number of tutorials mentioning it would let you believe. There is no magic in <code>dd</code>, the magic is all in <code>/dev/sdb</code>.</p>

<p>Your new command <code>sudo dd if=/dev/sdb bs=128K | pv -s 3000G | sudo dd of=/dev/sdc bs=128K</code> is again needlessly slow and complicated. The data is read 128kB at a time (which is better than the <code>dd</code> default of 512B, but not as good as even larger values). It then goes through two pipes before being written.</p>

<p>Use the simpler and faster <code>cat</code> command. (In some <a href=""https://unix.stackexchange.com/questions/9432/is-there-a-way-to-determine-the-optimal-value-for-the-bs-parameter-to-dd/9492#9492"">benchmarks</a> I made a couple of years ago under Linux, <code>cat</code> was faster than <code>cp</code> for a copy between different disks, and <code>cp</code> was faster than <code>dd</code> with any block size; <code>dd</code> with a large block size was slightly faster when copying onto the same disk.)</p>

<pre><code>cat /dev/sdb &gt;/dev/sdc
</code></pre>

<p>If you want to run this command in <code>sudo</code>, you need to make the redirection happen as root:</p>

<pre><code>sudo sh -c 'cat /dev/sdb &gt;/dev/sdc'
</code></pre>

<p>If you want a progress report, since you're using Linux, you can easily get one by noting the PID of the <code>cat</code> process (say 1234) and looking at the position of its input (or output) file descriptor.</p>

<pre><code># cat /proc/1234/fdinfo/0
pos:    64155648 
flags:  0100000
</code></pre>

<p>If you want a progress report and your unix variant doesn't provide an easy way to get at a file descriptor positions, you can install and use <a href=""http://www.ivarch.com/programs/pv.shtml"" rel=""noreferrer""><code>pv</code></a> instead of <code>cat</code>.</p>
","144227"
"Where to find the Crontab logs in CentOS","84021","","<p>First of all I'm using CentOS</p>

<pre><code> [root@a etc]# cat system-release
 CentOS release 6.5 (Final)

[root@a cron.daily]# ps -ef | grep cron
root       982     1  0 Jun14 ?        00:01:15 crond
root      5692  5441  0 00:49 pts/0    00:00:00 grep cron
[root@a cron.daily]#
</code></pre>

<p>And I'm running out of my resources, so I want to delete the old log files.In this case i would like to delete the old secure logs which are almost more than 100MB's in size, So i gave the below crontab entries for <code>root</code> user. </p>

<pre><code>[root@a etc]# crontab -l
0 1 * * * find /var/log -name ""secure-*"" -mtime +5 -exec rm {} \;
[root@a etc]#
</code></pre>

<p>After very few days later i came to know this crontab entry doesn't work and still i see old files.</p>

<pre><code>[root@a log]# find /var/log -name ""secure-*""
/var/log/secure-20141214
/var/log/secure-20141107
/var/log/secure-20141130
/var/log/secure-20141221
[root@a log]#
</code></pre>

<p>Later i tried to search for the crontab logs under /etc/crontab.daily directory and not found any relevant results. Where to find the crontab logs and how to know whether the crontab is running successfully or not ?</p>
","<p>Cron logs on CentOS 6 are located in <code>/var/log/cron</code> by default. This only logs the execution of commands, not the results or exit statuses. The output of the executed command goes to the user's mail by default (root's mail in this case). An alternate email can be specified by the MAILTO variable inside of the crontab. </p>

<p>You should look at adjusting <code>logrotate</code> rules, instead of your custom <code>cron</code>, which already handles deletion of /var/log/secure logs.</p>
","176232"
"How to dd a remote disk using SSH on local machine and save to a local disk","83799","","<p>How can I create a backup of a remote disk using SSH on my local machine and save it to a local disk?</p>

<hr>

<p>I've tried the following:</p>

<pre><code>ssh hostname@my.ip.address ""sudo dd if=/dev/sdX "" | \
  dd of=/home/username/Documents/filename.image`
</code></pre>

<p>However, I receive the following error:</p>

<blockquote>
  <p>no tty present and no askpass program specified</p>
</blockquote>
","<p>If your intent is to backup a remote computer's <strong>HDD A</strong> via SSH to a single file that's on your local computer's HDD, you could do one of the following.</p>

<h3>Examples</h3>

<p><em>run from remote computer</em></p>

<pre><code>$ dd if=/dev/sda | gzip -1 - | ssh user@local dd of=image.gz
</code></pre>

<p><em>run from local computer</em></p>

<pre><code>$ ssh user@remote ""dd if=/dev/sda | gzip -1 -"" | dd of=image.gz
</code></pre>

<h3>Live example</h3>

<pre><code>$ ssh skinner ""dd if=/dev/sda5 | gzip -1 -"" | dd of=image.gz
208782+0 records in
208782+0 records out
106896384 bytes (107 MB) copied, 22.7608 seconds, 4.7 MB/s
116749+1 records in
116749+1 records out
59775805 bytes (60 MB) copied, 23.9154 s, 2.5 MB/s

$ ll | grep image.gz
-rw-rw-r--.   1 saml saml  59775805 May 31 01:03 image.gz
</code></pre>

<h3>Methods for monitoring?</h3>

<ol>
<li>Login via <code>ssh</code> in another terminal and <code>ls -l</code> the file to see what it's size is.</li>
<li><p>You can use <code>pv</code> to monitor the progress of a large dd operation, for instance, for the remote example above, you can do:</p>

<pre><code>$ dd if=/dev/sda | gzip -1 - | pv | ssh user@local dd of=image.gz
</code></pre></li>
<li><p>Send a ""SIGUSR1"" signal to <code>dd</code> and it will print stats. Something like: </p>

<pre><code>$ pkill -USR1 dd
</code></pre></li>
</ol>

<h3>References</h3>

<p>The methods mentioned above for monitoring were originally left via comments by <a href=""https://unix.stackexchange.com/users/112595/ryan"">@Ryan</a> &amp; <a href=""https://unix.stackexchange.com/users/63905/bladt"">@bladt</a> and myself. I've moved them into the answer to make them more obvious.</p>
","132800"
"How to search for a word in entire content of a directory in linux","83780","","<p>need to search for something in entire content</p>

<p>I am trying:</p>

<pre><code>find . | xargs grep word
</code></pre>

<p>I get error:</p>

<blockquote>
  <p>xargs: unterminated quote</p>
</blockquote>

<p>How to achieve this?</p>
","<p><code>xargs</code> expects input in a format that no other command produces, so it's hard to use effectively. What's going wrong here is that you have a file whose name must be quoted on input to <code>xargs</code> (probably containing a <code>'</code>).</p>

<p>If your grep supports the <code>-r</code> or <code>-R</code> option for recursive search, use it.</p>

<pre><code>grep -r word .
</code></pre>

<p>Otherwise, use the <code>-exec</code> primary of <code>find</code>. This is the usual way of achieving the same effect as <code>xargs</code>, except without constraints on file names. Reasonably recent versions of <code>find</code> allow you to group several files in a single call to the auxiliary command. Passing <code>/dev/null</code> to <code>grep</code> ensures that it will show the file name in front of each match, even if it happens to be called on a single file.</p>

<pre><code>find . -type f -exec grep word /dev/null {} +
</code></pre>

<p>Older versions of <code>find</code> (on older systems or OpenBSD, or reduced utilities such as BusyBox) can only call the auxiliary command on one file at a time.</p>

<pre><code>find . -type f -exec grep word /dev/null {} \;
</code></pre>

<p>Some versions of <code>find</code> and <code>xargs</code> have extensions that let them communicate correctly, using null characters to separate file names so that no quoting is required. These days, only OpenBSD has this feature without having <code>-exec … {} +</code>.</p>

<pre><code>find . -type f -print0 | xargs -0 grep word /dev/null
</code></pre>
","21224"
"Can I safely remove /var/cache?","83692","","<p>I am running out of disk space and noted that I have a large <code>/var/cache</code> directory. Can I safely remove this? (using Arch Linux, BTW).</p>
","<p><strong>No</strong>.</p>

<p>For one, I believe that <code>/var/cache/bind/</code> is the default directory where bind9 expects its zone files to be stored (at least on Debian; I don't know offhand if other distros follow suit)</p>

<p>For another, according to <a href=""https://wiki.archlinux.org/index.php/Pacman"">this documentation</a>, pacman (the package manager used by Arch linux) stores its package cache under <code>/var/cache/pacman/pkg/</code> and it most likely expects nothing but itself to modify the contents.</p>

<p>I recommend you read through the documentation more closely and decide whether this is a good time to clear the package cache.</p>
","23158"
"How to search text throughout entire file system?","83576","","<p>Assuming that the grep tool should be used, I'd like to search for the text string ""800x600"" throughout the entire file system.</p>

<p>I tried:</p>

<pre><code>grep -r 800x600 /
</code></pre>

<p>but it doesn't work.</p>

<p>What I believe my command should do is grep recursively through all files/folders under root for the text ""800x600"" and list the search results.</p>

<p>What am I doing wrong?</p>
","<p>I normally use this style of command to run <code>grep</code> over a number of files:</p>

<pre><code>find / -xdev -type f -print0 | xargs -0 grep -H ""800x600""
</code></pre>

<p>What this actually does is make a list of every file on the system, and then for each file, execute <code>grep</code> with the given arguments and the name of each file.</p>

<p>The <code>-xdev</code> argument tells find that it must ignore other filesystems - this is good for avoiding special filesystems such as <code>/proc</code>. However it will also ignore normal filesystems too - so if, for example, your /home folder is on a different partition, it won't be searched - you would need to say <code>find / /home -xdev ...</code>.</p>

<p><code>-type f</code> means search for files only, so directories, devices and other special files are ignored (it will still recurse into directories and execute <code>grep</code> on the files within - it just won't execute <code>grep</code> on the directory itself, which wouldn't work anyway). And the <code>-H</code> option to <code>grep</code> tells it to always print the filename in its output.</p>

<p><code>find</code> accepts all sorts of options to filter the list of files. For example, <code>-name '*.txt'</code> processes only files ending in .txt. <code>-size -2M</code> means files that are smaller than 2 megabytes. <code>-mtime -5</code> means files modified in the last five days. Join these together with -a for <em>and</em> and -o for <em>or</em>, and use <code>'('</code> parentheses <code>')'</code> to group expressions (in quotes to prevent the shell from interpreting them). So for example:</p>

<pre><code>find / -xdev '(' -type f -a -name '*.txt' -a -size -2M -a -mtime -5 ')' -print0 | xargs -0 grep -H ""800x600""
</code></pre>

<p>Take a look at <code>man find</code> to see the full list of possible filters.</p>
","16140"
"cp: cannot stat `/vol/examples/tutorial/science.txt': No such file or directory","83554","","<p>I tried a command <code>cp /vol/examples/tutorial/science.txt .</code> and I get the error - </p>

<pre><code>cp: cannot stat `/vol/examples/tutorial/science.txt': No such file or directory
</code></pre>

<p>I don't understand why this is happening. I went to cygwin home folder and created the necessary folder structure and text file. I also checked the folder structure and names. </p>

<p>Also, I go into <code>~/vol/examples/tutorial</code> and then do <code>cat science.txt</code> and I can read the document. Then why does the CP command not work ?</p>
","<p>Do this and you will understand:</p>

<pre><code>cd ~/vol/examples/tutorial 
pwd
</code></pre>

<p>most likely the result of pwd is not just <code>/vol/examples/tutorial</code> but <code>/home/username/vol/examples/tutorial</code> </p>

<p>~ is replaced by the user home directory, usually and even under cygiwn /home.</p>
","84303"
"Setting PATH vs. exporting PATH in ~/.bash_profile","83509","","<p>What's the difference and which is better to use when customizing my bash profile? Documentation on the <code>export</code> command is scarce, as it's a builtin cmd.</p>

<p>Excerpt from version 1 of my ~/.bash_profile:</p>

<pre><code>#PATH
export PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:$HOME/bin

#add Homebrew’s sbin to PATH
export PATH=/usr/local/sbin:$PATH
</code></pre>

<p>Output from: <code>echo $PATH</code>
<code>/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Users/njboot/bin</code></p>

<p>Excerpt from version 2:</p>

<pre><code>#PATH
PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:$HOME/bin

#add Homebrew’s sbin to PATH
export PATH=/usr/local/sbin:$PATH
</code></pre>

<p>Output from <code>echo $PATH</code> is the same as in version 1. <code>env</code> is the same as well. </p>

<p>So:</p>

<ul>
<li>1) What's the benefit of using <code>export</code> vs. setting PATH explicitly?</li>
<li>2) Is there any functional difference between version 1 and version 2 when applied?</li>
<li>3) Which should I use and why?</li>
</ul>
","<p>To answer your questions specifically:</p>

<ol>
<li><p><code>export</code> <em>does</em> set the <code>$PATH</code> explicitly.</p></li>
<li><p>No. <code>export</code> sets environment for child processes, but <code>$PATH</code> is already set for the current environment. So, in the second example, when the command is read-in - and <em>before</em> <code>export</code> is executed - the current environment's value for <code>$PATH</code> is expanded into the <code>$PATH</code> word.</p></li>
<li><p>You should use whichever is necessary and/or comfortable for you. Neither makes any difference functionally, so this is primarily a question of style.</p></li>
</ol>

<p>POSIX defines the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#export"" rel=""nofollow noreferrer""><code>export</code> builtin</a> so:</p>

<blockquote>
  <p>The shell shall give the <code>export</code> attribute to the variables corresponding to the specified names, which shall cause them to be in the environment of subsequently executed commands. If the name of a variable is followed by <em>= word</em>, then the value of that variable shall be set to <em>word</em>.</p>
</blockquote>

<p>From another of my <a href=""https://unix.stackexchange.com/a/137707/52934"">answers</a>:</p>

<p><em>There is little difference between declaring a shell variable and an environment variable. Because export is a builtin it declares an environment variable for the process next invoked, but if you don't invoke one that process remains the shell, and so your variable is twice evaluated.</em></p>

<p>You can remove all exports without any effect at all on exported variables so long as you don't use <code>export</code> to twice evaluate. By twice evaluate I mean:</p>

<pre><code>var1=var2 
export ""${var1}=var3""
echo ""$var2""
var3
</code></pre>

<p>Instead just use:</p>

<pre><code>set -a 
</code></pre>

<p>...at the top of the script. All variables defined thereafter will be automatically <code>exported</code> - which would include variables you might not have previously <code>export</code>ed. Alternatively you could only <code>set -a</code> for a portion of the script and later <code>set +a</code> to unset it - it could also work as function.</p>

<p>But subshells automatically inherit variable values anyway, so:</p>

<pre><code>var1=value
( echo ""$(echo ""$var1"")"" )
value
</code></pre>

<p><code>export</code> makes no difference in that case.</p>

<p>But if your script calls another script, or any other executable that interprets values you've <code>export</code>ed and you cease to <code>export</code> them, then those values will no longer be available in their environment. In the following example I use the shell variable <code>$PS1</code> - which defines the contents of an interactive shell's prompt - to demonstrate how variations on <code>export</code>ed variables affect child processes.</p>

<pre><code>export PS1=""$(printf ""this is another executable\n &gt; "")""
echo exit | sh -i

###OUTPUT###

this is another executable
 &gt; exit
exit
</code></pre>

<p>But ...</p>

<pre><code>PS1=""$(printf ""this is another executable\n &gt; "")""
echo exit | sh -i

###OUTPUT###

sh-4.3$ exit
exit
</code></pre>

<p>But then again, if you explicitly declare environment variables while invoking a process...</p>

<pre><code>PS1=""$(printf ""this is another executable\n &gt; "")""
{
echo exit | PS1=$PS1 sh -i
echo exit | sh -i
}

###OUTPUT###

this is another executable
 &gt; exit
exit
sh-4.3$ exit
exit
</code></pre>

<p>Any of the <code>ENV</code> files first invoked by a shell such as <code>.bashrc</code> or <code>.profile</code> will set variable values for the life of that shell. So any variables that are set and <code>export</code>ed within those files will maintain that <code>export</code> characteristic and be <code>export</code>ed to all child processes invoked by that shell for the life of the shell or until they are <code>unset</code>. </p>

<p>It is notable, though, that <code>bash</code> extends the <code>export</code> builtin somewhat to include the <code>-n</code> option - which enables you to remove the <code>export</code> attribute from a variable without <code>unset</code>ting it, but this is not portable behavior.</p>
","138557"
"Alternatives for ""lsof"" command?","83437","","<p>In many cases ""lsof"" is not installed on the machines that with I have to work, but the ""function"" of lsof would be needed very much (ex. on AIX). :\</p>

<p>Are there any ""lsof"" like applications in the non-windows world? </p>

<p>UPDATE:
for ex.: I need to know that what processes use the ""/home/username"" directory?</p>
","<p>I know of <a href=""http://en.wikipedia.org/wiki/Fuser_%28Unix%29""><code>fuser</code></a>, see if is available on your system.</p>
","18616"
"Yum Check Available Package Updates","83425","","<p>Red Hat <a href=""https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-yum.html#sec-Checking_For_Updates"">docs</a> say:</p>

<blockquote>
  <p>To see which installed packages on your system have updates available,
  use the following command:</p>
</blockquote>

<p><code>yum check-update</code></p>

<p>What command must I run to view all <code>available</code> versions for a package installed on my system?</p>

<p>Example: <code>yum check-update</code> tells me <code>java6</code> update <code>#43</code> is available, but what if I want update <code>#40</code>?</p>
","<p>It won't focus specifically on one package because it's using a regex to do the matching but I often use this:</p>

<pre><code>$ yum list available java\*
java-1.4.2-gcj-compat.i386                                                   1.4.2.0-40jpp.115                                                      installed
java-1.6.0-openjdk.i386                                                      1:1.6.0.0-1.36.1.11.9.el5_9                                            installed
Available Packages
java-1.4.2-gcj-compat-devel.i386                                             1.4.2.0-40jpp.115                                                      base     
java-1.4.2-gcj-compat-javadoc.i386                                           1.4.2.0-40jpp.115                                                      base     
java-1.4.2-gcj-compat-src.i386                                               1.4.2.0-40jpp.115                                                      base     
java-1.6.0-openjdk.i386                                                      1:1.6.0.0-1.40.1.11.11.el5_9                                           updates  
java-1.6.0-openjdk-demo.i386                                                 1:1.6.0.0-1.40.1.11.11.el5_9
</code></pre>

<p>You can make it ""smarter"" by filtering the output using <code>grep</code>.</p>
","75982"
"How to check if there are no parameters provided to a command?","83345","","<p>How do you check if <code>$*</code> is empty? In other words, how to check if there were no arguments provided to a command?</p>
","<p>To check if there were no arguments provided to the command, check value of <code>$#</code> variable then,</p>

<pre><code>if [ $# -eq 0 ]; then
    echo ""No arguments provided""
    exit 1
fi
</code></pre>

<p>If you want to use <code>$*</code>(<strong>not preferable</strong>) then,</p>

<pre><code>if [ ""$*"" == """" ]; then
    echo ""No arguments provided""
    exit 1
fi
</code></pre>

<p>Some explanation:</p>

<p>The second approach is not preferable because in positional parameter expansion <code>*</code> expands to the positional parameters, starting from one. When the expansion occurs within double quotes, it expands to a single word with the value of each parameter separated by the first character of the IFS special variable. That means a string is constructed. So there is extra overhead. </p>

<p>On the other hand <code>#</code> expands to the number of positional parameters.</p>

<p>Example:</p>

<p><code>$ command param1 param2</code></p>

<p>Here,</p>

<p>Value of <code>$#</code> is 2 and value of <code>$*</code> is string ""param1 param2"" (without quotes), if IFS is unset. Because if IFS is unset, the parameters are separated by spaces</p>

<p>For more details <code>man bash</code> and read topic named <strong>Special Parameters</strong></p>
","25947"
"Finding text between two specific characters or strings","83332","","<p>Say I have lines like this:</p>

<pre><code>*[234]*
*[23]*
*[1453]*
</code></pre>

<p>where <code>*</code> represents any string (except a string of the form <code>[number]</code>). How can I parse these lines with a command line utility and extract the number between brackets?  </p>

<p>More generally, which of these tools <code>cut</code>, <code>sed</code>, <code>grep</code> or <code>awk</code> would be appropriate for such task?</p>
","<p>If you have GNU grep, you can use its <code>-o</code> option to search for a regex and output only the matching part. (Other grep implementations can only show the whole line.) If there are several matches on one line, they are printed on separate lines.</p>

<pre><code>grep -o '\[[0-9]*\]'
</code></pre>

<p>If you only want the digits and not the brackets, it's a little harder; you need to use a zero-width assertion: a regexp that matches the empty string, but only if it is preceded, or followed as the case may be, by a bracket. Zero-width assertions are only available in Perl syntax.</p>

<pre><code>grep -P -o '(?&lt;=\[)[0-9]*(?=\])'
</code></pre>

<p>With sed, you need to turn off printing with <code>-n</code>, and match the whole line and retain only the matching part. If there are several possible matches on one line, only the last match is printed. See <a href=""https://unix.stackexchange.com/questions/31476/extracting-a-regex-matched-with-sed-without-printing-the-surrounding-character/31479#31479"">Extracting a regex matched with &#39;sed&#39; without printing the surrounding characters</a> for more details on using sed here.</p>

<pre><code>sed -n 's/^.*\(\[[0-9]*\]\).*/\1/p'
</code></pre>

<p>or if you only want the digits and not the brackets:</p>

<pre><code>sed -n 's/^.*\[\([0-9]*\)\].*/\1/p'
</code></pre>

<p>Without <code>grep -o</code>, Perl is the tool of choice here if you want something that's both simple and comprehensible. On every line (<code>-n</code>), if the line contains a match for <code>\[[0-9]*\]</code>, then print that match (<code>$&amp;</code>) and a newline (<code>-l</code>).</p>

<pre><code>perl -l -ne '/\[[0-9]*\]/ and print $&amp;'
</code></pre>

<p>If you only want the digits, put parentheses in the regex to delimit a group, and print only that group.</p>

<pre><code>perl -l -ne '/\[([0-9]*)\]/ and print $1'
</code></pre>

<p>P.S. If you only want to require one or more digits between the brackets, change <code>[0-9]*</code> to <code>[0-9][0-9]*</code>, or to <code>[0-9]+</code> in Perl.</p>
","33718"
"Find files with certain extensions","83260","","<p>How can I use <code>find</code> to find all files that have a <code>.xls</code> or <code>.csv</code> extension? I have seen a <code>-regex</code> option but I don't know how to use it.</p>
","<p>Why not simply use this:</p>

<pre><code>find -name ""*.xls"" -o -name ""*.csv""
</code></pre>

<p>You don't need regex for this.</p>

<p>If you absolutely want to use regex simply use</p>

<pre><code>find -regex "".*\.\(xls\|csv\)""
</code></pre>
","28157"
"lvm devices under /dev/mapper missing","83228","","<p>I'm using Debian squeeze, and running LVM on top of software RAID 1.
I just accidentally just discovered that most of the links under <code>/dev/mapper</code> are missing,
though my system seems to be still functioning correctly.</p>

<p>I'm not sure what happened. 
The only thing I can imagine that caused it was my failed attempt to get a LXC fedora container to work.
I ended up deleting a directory <code>/cgroup/laughlin</code>, corresponding to the container,
but I can't imagine why that should have caused the problem. 
<code>/dev/mapper</code> looked (I made some changes, see below) approximately like</p>

<pre><code>orwell:/dev/mapper# ls -la
total 0
drwxr-xr-x  2 root root     540 Apr 12 05:08 .
drwxr-xr-x 22 root root    4500 Apr 12 05:08 ..
crw-------  1 root root  10, 59 Apr  8 10:32 control
lrwxrwxrwx  1 root root       7 Mar 29 08:28 debian-root -&gt; ../dm-0
lrwxrwxrwx  1 root root       8 Apr 12 03:32 debian-video -&gt; ../dm-23
</code></pre>

<p>debian-video corresponds to a LV I had just created.</p>

<p>However, I have quite a number of VGs on my system, corresponding to 4 VGs spread across 4 disks. <code>vgs</code> gives</p>

<pre><code>orwell:/dev/mapper# vgs
  VG         #PV #LV #SN Attr   VSize   VFree  
  backup       1   2   0 wz--n- 186.26g  96.26g
  debian       1   7   0 wz--n- 465.76g 151.41g
  olddebian    1  12   0 wz--n- 186.26g  21.26g
  testdebian   1   3   0 wz--n- 111.75g  34.22g
</code></pre>

<p>I tried running</p>

<pre><code> /dev/mapper# vgscan --mknodes
</code></pre>

<p>and some devices were created (see output below), but they aren't symbolic links to the dm devices as they should be,
so I'm not sure if this is useless or worse. Would they get in the way of recreation of the correct links?
Should I delete these devices again?</p>

<p>I believe that udev creates these links, so would a reboot fix this problem,
or would I get an unbootable system? What should I do to fix this?
Are there any diagnostics/sanity checks I should run to make sure there aren't
other problems I haven't noticed? Thanks in advance for any assistance.</p>

<pre><code>orwell:/dev/mapper# ls -la
total 0
drwxr-xr-x  2 root root     540 Apr 12 05:08 .
drwxr-xr-x 22 root root    4500 Apr 12 05:08 ..
brw-rw----  1 root disk 253,  1 Apr 12 05:08 backup-local_src
brw-rw----  1 root disk 253,  2 Apr 12 05:08 backup-video
crw-------  1 root root  10, 59 Apr  8 10:32 control
brw-rw----  1 root disk 253, 15 Apr 12 05:08 debian-boot
brw-rw----  1 root disk 253, 16 Apr 12 05:08 debian-home
brw-rw----  1 root disk 253, 22 Apr 12 05:08 debian-lxc_laughlin
brw-rw----  1 root disk 253, 21 Apr 12 05:08 debian-lxc_squeeze
lrwxrwxrwx  1 root root       7 Mar 29 08:28 debian-root -&gt; ../dm-0
brw-rw----  1 root disk 253, 17 Apr 12 05:08 debian-swap
lrwxrwxrwx  1 root root       8 Apr 12 03:32 debian-video -&gt; ../dm-23
brw-rw----  1 root disk 253, 10 Apr 12 05:08 olddebian-etch_template
brw-rw----  1 root disk 253, 13 Apr 12 05:08 olddebian-fedora
brw-rw----  1 root disk 253,  8 Apr 12 05:08 olddebian-feisty
brw-rw----  1 root disk 253,  9 Apr 12 05:08 olddebian-gutsy
brw-rw----  1 root disk 253,  4 Apr 12 05:08 olddebian-home
brw-rw----  1 root disk 253, 11 Apr 12 05:08 olddebian-lenny
brw-rw----  1 root disk 253,  7 Apr 12 05:08 olddebian-msi
brw-rw----  1 root disk 253,  5 Apr 12 05:08 olddebian-oldchresto
brw-rw----  1 root disk 253,  3 Apr 12 05:08 olddebian-root
brw-rw----  1 root disk 253, 14 Apr 12 05:08 olddebian-suse
brw-rw----  1 root disk 253,  6 Apr 12 05:08 olddebian-vgentoo
brw-rw----  1 root disk 253, 12 Apr 12 05:08 olddebian-wsgi
brw-rw----  1 root disk 253, 20 Apr 12 05:08 testdebian-boot
brw-rw----  1 root disk 253, 18 Apr 12 05:08 testdebian-home
brw-rw----  1 root disk 253, 19 Apr 12 05:08 testdebian-root
</code></pre>
","<p>These days <code>/dev</code> is on tmpfs and is created from scratch each boot by <code>udev</code>. You can safely reboot and these links will come back.</p>

<p>You should also find LVM symlinks to the <code>/dev/dm-X</code> nodes in the <code>/dev/&lt;vg&gt;</code> directories, one directory for each volume group. However, those nodes re-created by <code>vgscan --mknodes</code> will also work fine, assuming they have the right major/minor numbers - and it's a safe assumption they were created properly.</p>

<p>You can probably also get <code>udev</code> to re-create the symlinks using <code>udevadm trigger</code> with an appropriate match, testing with <code>--dry-run</code> until it is right. It hardly seems worth the effort though when a reboot will fix it too.</p>
","11162"
"Easy way to determine virtualization technology","83110","","<p>I have command line access to a Linux machine which may or may not be virtualized. I want to determine what kind of virtualization technology it runs on, if any (VMWare, VirtualBox, KVM, OpenVZ, Xen, ). This isn't a hostile environment: I'm not trying to work against a VM that is trying to disguise itself, I'm diagnosing a flaky server that I know little about.</p>

<p>More precisely, I'm helping someone diagnose the issue, I'm not sitting at the helm. So I have to convey instructions like “copy-paste this command” and not “poke around <code>/proc</code> somewhere”. Ideally, it would be something like <code>lshw</code>: an easily-installable (if not preinstalled) command that does the poking around and prints out relevant information.</p>

<p>What's the easiest way of determining what virtualization technology this system may be a guest of? I'd appreciate if proposals mentioned which technologies (including bare hardware) can be conclusively detected and which can be conclusively eliminated. I'm mostly interested in Linux, but if it also works for other unices that's nice.</p>
","<h2><code>dmidecode -s system-product-name</code></h2>

<p>I have tested on Vmware Workstation, VirtualBox, QEMU with KVM, standalone QEMU with Ubuntu as the guest OS. Others have added additional platforms that they're familiar with as well.</p>

<h3>Virtualization technologies</h3>

<ul>
<li><p>VMware Workstation</p>

<pre><code>root@router:~# dmidecode -s system-product-name
VMware Virtual Platform
</code></pre></li>
<li><p>VirtualBox</p>

<pre><code>root@router:~# dmidecode -s system-product-name
VirtualBox
</code></pre></li>
<li><p>Qemu with KVM</p>

<pre><code>root@router:~# dmidecode -s system-product-name
KVM
</code></pre></li>
<li><p>Qemu (emulated)</p>

<pre><code>root@router:~# dmidecode -s system-product-name
Bochs
</code></pre></li>
<li><p>Microsoft VirtualPC</p>

<pre><code>root@router:~# dmidecode | egrep -i 'manufacturer|product'
Manufacturer: Microsoft Corporation
Product Name: Virtual Machine
</code></pre></li>
<li><p>Virtuozzo</p>

<pre><code>root@router:~# dmidecode
/dev/mem: Permission denied
</code></pre></li>
<li><p>Xen</p>

<pre><code>root@router:~# dmidecode | grep -i domU
Product Name: HVM domU
</code></pre></li>
</ul>

<p>On bare metal, this returns an identification of the computer or motherboard model.</p>

<h2><code>/dev/disk/by-id</code></h2>

<p>If you don't have the rights to run <code>dmidecode</code> then you can use:</p>

<p>Virtualization Technology: QEMU</p>

<pre><code>ls -1 /dev/disk/by-id/
</code></pre>

<p><strong>Output</strong></p>

<pre><code>[root@host-7-129 ~]# ls -1 /dev/disk/by-id/
ata-QEMU_DVD-ROM_QM00003
ata-QEMU_HARDDISK_QM00001
ata-QEMU_HARDDISK_QM00001-part1
ata-QEMU_HARDDISK_QM00002
ata-QEMU_HARDDISK_QM00002-part1
scsi-SATA_QEMU_HARDDISK_QM00001
scsi-SATA_QEMU_HARDDISK_QM00001-part1
scsi-SATA_QEMU_HARDDISK_QM00002
scsi-SATA_QEMU_HARDDISK_QM00002-part1
</code></pre>

<h3>References</h3>

<ul>
<li><a href=""http://www.dmo.ca/blog/detecting-virtualization-on-linux/"" rel=""noreferrer"">How to detect virtualization at dmo.ca</a></li>
</ul>
","89718"
"Why did my crontab not trigger?","83022","","<p>I used <code>crontab -e</code> to add the following line to my crontab:</p>

<pre><code>* * * * * echo hi &gt;&gt; /home/myusername/test
</code></pre>

<p>Yet, I don't see that the test file is written to. Is this a permission problem, or is crontab not working correctly?</p>

<p>I see that the cron process is running. How can I debug this?</p>

<p><strong>Edit</strong> - Ask Ubuntu has <a href=""https://askubuntu.com/questions/23009/reasons-why-crontab-does-not-work"">a nice question about crontab</a>, unfortunately that still doesn't help me.</p>

<p><strong>Edit 2</strong> - Hmm, it seems my test file has 214 lines, which means for the last 214 minutes it has been written to every minute. I'm not sure what was the problem, but it's evidently gone.</p>
","<p>There are implementations of <code>cron</code> (not all of them, and I don't remember which offhand, but I've encountered one under Linux) that check for updated crontab files every minute on the minute, and do not consider new entries until the next minute. Therefore, a crontab can take up to two minutes to fire up for the first time. This may be what you observed.</p>
","9486"
"is my linux ARM 32 or 64 bit?","82906","","<p>under an intel I know I can look at the outcome of <code>uname -m</code> to know if my OS is 32 or 64 bit, but under ARM this gives:</p>

<pre><code>armv7l
</code></pre>

<p>I deduced from</p>

<pre><code>file /usr/bin/ls
</code></pre>

<p>that I'm on a 32-bit OS, but how can I know this in an easier way?</p>
","<p>There are several gradations, since you can run a 32-bit or mixed operating system on a 64-bit-capable CPU. See <a href=""https://unix.stackexchange.com/questions/134391/64-bit-kernel-but-all-32-bit-elf-executable-running-processes-how-is-this/134394#134394"">64-bit kernel, but all 32-bit ELF executable running processes, how is this?</a> for a detailed discussion (written for x86, but most of it applies to arm as well).</p>

<p>You can find the processor model in <code>/proc/cpuinfo</code>. For example:</p>

<pre><code>$ cat /proc/cpuinfo
Processor       : ARMv7 Processor rev 10 (v7l)
</code></pre>

<p>ARMv7 (and below) is 32-bit. <a href=""http://en.wikipedia.org/wiki/ARMv8#ARMv8-A"" rel=""noreferrer"">ARMv8</a> introduces the 64-bit instruction set.</p>

<p>If you want to see whether your system supports 64-bit binaries, check the kernel architecture:</p>

<pre><code>$ uname -m
armv7l
</code></pre>

<p>On a 64-bit processor, you'd see <code>armv8</code> (or above).</p>

<hr>

<p>Right now, if you were running a 64-bit ARM, you'd know.</p>
","136519"
"How to properly configure sudoers file, on debian wheezy?","82904","","<p>I have seen many blog posts that say, it is enough to do</p>

<pre><code>aptitude install sudo
su root
adduser USERNAME sudo
</code></pre>

<p>But that only protects <code>aptitude</code>, in other words:</p>

<ul>
<li><p><code>aptitude install sendmail</code> will ask for password, you need to be
<code>sudo</code> to run <code>aptitude</code></p></li>
<li><p><code>apt-get install sendmail</code> won't ask for password, no <code>sudo</code> privileges
needed</p></li>
<li><p>If you edit protected files, like files in <code>etc</code> it won't ask for
password, no <code>sudo</code> privileges needed</p></li>
<li><p>You can run and stop services like <code>apache</code>, it won't ask for
password, no <code>sudo</code> privileges needed</p></li>
</ul>

<p>How to fix this? This is my sudoers file:</p>

<pre><code> This file MUST be edited with the 'visudo' command as root.
#
# Please consider adding local content in /etc/sudoers.d/ instead of
# directly modifying this file.
#
# See the man page for details on how to write a sudoers file.
#
Defaults        env_reset
Defaults        mail_badpass
Defaults        secure_path=""/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:$

# Host alias specification

# User alias specification

# Cmnd alias specification
</code></pre>

<p>This is the output of <code>sudo -l</code>:</p>

<pre><code>Matching Defaults entries for root on this host:
    env_reset, mail_badpass,
    secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin

User root may run the following commands on this host:
    (ALL : ALL) ALL
    (ALL : ALL) ALL
</code></pre>
","<p>You haven't added any sudo rule, so you can't use sudo for anything.</p>

<p>The command <code>adduser USERNAME sudo</code> adds the specified user to the group called <code>sudo</code>. A group with that name must exist; create it with <code>addgroup sudo</code> if it doesn't. After adding the user to the group, the user must log out and back in for the group membership to take effect.</p>

<p><code>sudo</code> is not a special group name. It's a convention to allow users in the group called <code>sudo</code> to run commands as root via the <code>sudo</code> utility. This requires the following line in the <code>sudoers</code> file:</p>

<pre><code>%sudo ALL = (ALL) ALL
</code></pre>

<p>Run <code>visudo</code> to edit the sudoers file, never edit it directly.</p>

<p>I have no idea why you believe that “that only protects aptitude”. There is nothing special about aptitude. Once you've authorized a user to run commands as root, that user can run <code>sudo aptitude …</code> or <code>sudo apt-get …</code> or <code>sudo service …</code>, or <code>sudoedit</code> to edit files that require root permission to edit. Being in the sudoers file doesn't directly change the privileges of your user, what it does is that it allows you to run <code>sudo</code> to run commands as root. Commands run as root only when you run them through <code>sudo</code>. Some programs may do that automatically, especially GUI programs where the user interface runs without special privileges and only the backend runs as root, but commands executed as root are always executed by <code>sudo</code>.</p>
","86812"
"How to skip ""permission denied"" errors when running find in Linux?","82765","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/19430/how-do-i-remove-permission-denied-printout-statements-from-the-find-program"">How do I remove &ldquo;permission denied&rdquo; printout statements from the find program?</a>  </p>
</blockquote>



<p>When I run this command in Linux (SuSE):</p>

<pre><code>find / -name ant
</code></pre>

<p>I get many error messages of the form:</p>

<pre><code>find: `/etc/cups/ssl': Permission denied
</code></pre>

<p>Does <code>find</code> take an argument to skip showing these errors and only try files that I have permission to access?</p>
","<p>you can filter out messages to <code>stderr</code>. I prefer to redirect them to <code>stdout</code> like this.</p>

<pre><code> find / -name art  2&gt;&amp;1 | grep -v ""Permission denied""
</code></pre>



<p><em>Explanation</em>:</p>

<p>In short, all regular output goes to standard output (<code>stdout</code>). All error messages to standard error (<code>stderr</code>).</p>

<p><code>grep</code> usually finds/prints the specified string, the <code>-v</code> inverts this, so it finds/prints every string that <em>doesn't</em> contain ""Permission denied"". All of your output from the find command, including error messages usually sent to <code>stderr</code> (file descriptor 2) go now to <code>stdout</code>(file descriptor 1) and then get filtered by the <code>grep</code> command.</p>

<p>This assumes you are using the <code>bash/sh</code> shell.</p>

<p>Under <code>tcsh/csh</code> you would use  </p>

<pre><code> find / -name art |&amp; grep ....
</code></pre>
","42842"
"keyserver timed out when trying to add a GPG public key","82684","","<p>I am trying to add a public key for installing a program with CPG. But I am pretty new to this but every command I found gave me the same error:</p>

<pre><code>gpg --keyserver keyserver.ubuntu.com --recv-keys 94558F59
gpg: requesting key 94558F59 from hkp server keyserver.ubuntu.com
gpg: keyserver timed out
gpg: keyserver receive failed: keyserver error
</code></pre>

<p>How is this possible it seems that the I am behind some kind of blockade which makes it impossible to establish a connection to the key server. I looked into many OP questions and tried all commands I could find but nothing worked. Anyone had this problem before?</p>
","<p>This is usually caused by your firewall blocking the port <code>11371</code>. You could unblock the port in your firewall. In case you don't have access to the firewall you could either:</p>

<ol>
<li><p>Force it to use port <code>80</code> instead of <code>11371</code></p>

<pre><code>gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 94558F59
</code></pre></li>
<li><p>Alternatively</p>

<ul>
<li>Find and open the key from the key server.</li>
<li>Copy it's contents into a text file.</li>
<li>Go to System Tool > Preferences > Software Sources > Authentication > Add key, and select the text file created.</li>
</ul></li>
</ol>
","110594"
"How to use find command to search for multiple extensions","82568","","<p>I can get all <code>jpg</code> images by using:  </p>

<pre><code>find . -name ""*.jpg""  
</code></pre>

<p>But how can I add <code>png</code> files to the results as well?</p>
","<p>Use the <code>-o</code> flag between different parameters.</p>

<p><code>find ./ -type f \( -iname \*.jpg -o -iname \*.png \)</code> works like a charm.</p>
","15309"
"List all available ssl ca certificates","82378","","<p>My git client claims</p>

<pre><code>error: Peer's Certificate issuer is not recognized.
</code></pre>

<p>That means it can not find the corresponding ssl server key in the global system keyring. I want to check this by looking at the <strong>list of all system wide available ssl keys</strong> on a gentoo linux system. How can I get this list?</p>
","<p>It's not SSL keys you want, it's certificate authorities, and more precisely their certificates.</p>

<p>You could try:</p>

<pre><code>awk -v cmd='openssl x509 -noout -subject' '
    /BEGIN/{close(cmd)};{print | cmd}' &lt; /etc/ssl/certs/ca-certificates.crt
</code></pre>

<p>To get the ""subject"" of every CA certificate in <code>/etc/ssl/certs/ca-certificates.crt</code></p>

<p>Beware that sometimes, you get that error when SSL servers forget to provided the intermediate certificates.</p>

<p>Use <code>openssl s_client -showcerts -connect the-git-server:443</code> to get the list of certificates  being sent.</p>
","97252"
"What do the numbers in a man page mean?","82294","","<p>So, for example, when I type <code>man ls</code> I see <code>LS(1)</code>. But if I type <code>man apachectl</code> I see <code>APACHECTL(8)</code> and if I type <code>man cd</code> I end up with <code>cd(n)</code>.</p>

<p>I'm wondering what the significance of the numbers in the parentheses are, if they have any.</p>
","<p>The number corresponds to what section of the manual that page is from; 1 is user commands, while 8 is sysadmin stuff. The man page for man itself (<code>man man</code>) explains it and lists the standard ones:</p>

<pre><code>MANUAL SECTIONS
    The standard sections of the manual include:

    1      User Commands
    2      System Calls
    3      C Library Functions
    4      Devices and Special Files
    5      File Formats and Conventions
    6      Games et. al.
    7      Miscellanea
    8      System Administration tools and Daemons

    Distributions customize the manual section to their specifics,
    which often include additional sections.
</code></pre>

<p>There are certain terms that have different pages in different sections (e.g. <code>printf</code> as a command appears in section 1, as a <code>stdlib</code> function appears in section 3); in cases like that you can pass the section number to <code>man</code> before the page name to choose which one you want, or use <code>man -a</code> to show every matching page in a row:</p>

<pre><code>$ man 1 printf
$ man 3 printf
$ man -a printf
</code></pre>

<hr>

<p>You can tell what sections a term falls in with <code>man -k</code> (equivalent to the <code>apropos</code> command). It will do substring matches too (e.g. it will show <code>sprintf</code> if you run <code>man -k printf</code>), so you need to use <code>^term</code> to limit it:</p>

<pre><code>$ man -k '^printf'
printf               (1)  - format and print data
printf               (1p)  - write formatted output
printf               (3)  - formatted output conversion
printf               (3p)  - print formatted output
printf [builtins]    (1)  - bash built-in commands, see bash(1)
</code></pre>
","3587"
"How can I create an SFTP user in CentOS?","82028","","<p>I'd like to give temporary SFTP access to a support guy. How do I create an SFTP user? And how can I delete it once the job is done?</p>

<p>Also, how do I specify a home directory for them? Can I prevent them from accessing certain subdirectories within their home directory?</p>

<p>We use CentOS 6.3 and fzSftp</p>
","<h3>Non-chroot access</h3>

<p>If you don't have a FTP server setup, and you trust the user that will be logging in, not to go poking around your server too much, I'd be inclined to give them an account to SFTP into the system instead.</p>

<p>The CentOS wiki maintains a simple howto titled: <a href=""http://wiki.centos.org/HowTos/sftp"">Simple SFTP setup</a> that makes this pretty pain free.</p>

<p>I say it's pain free because you literally just have to make the account and make sure that the firewall allows SSH traffic, make sure SSH the service is running, and you're pretty much done.</p>

<p>If <code>sshd</code> isn't already running:</p>

<pre><code>$ /etc/init.d/sshd start
</code></pre>

<p>To add a user:</p>

<pre><code>$ sudo useradd userX
$ sudo passwd userX
... set the password ...
</code></pre>

<p>When you're done with the account:</p>

<pre><code>$ sudo userdel -r userX
</code></pre>

<h3>Chroot access</h3>

<p>If on the other hand you want to limit this user to a designated directory, the SFTP server included with SSH (openssh) provides a configuration that makes this easy to enable too. It's a bit more work but not too much. The steps are covered here in this tutorial titled: <a href=""http://www.thegeekstuff.com/2012/03/chroot-sftp-setup/"">How to Setup Chroot SFTP in Linux (Allow Only SFTP, not SSH)</a>.</p>

<p>Make these changes to your <code>/etc/ssh/sshd_config</code> file.</p>

<pre><code>Subsystem       sftp    internal-sftp

## You want to put only certain users (i.e users who belongs to sftpusers group) in the chroot jail environment. Add the following lines at the end of /etc/ssh/sshd_config

Match Group sftpusers
  ChrootDirectory /sftp/%u
  ForceCommand internal-sftp
</code></pre>

<p>Now you'll need to make the chrooted directory tree where this user will get locked into.</p>

<pre><code>$ sudo mkdir -p /sftp/userX/{incoming,outgoing}
$ sudo chown guestuser:sftpusers /sftp/guestuser/{incoming,outgoing}
</code></pre>

<p>Permissions should look like the following:</p>

<pre><code>$ ls -ld /sftp/guestuser/{incoming,outgoing}
drwxr-xr-x 2 guestuser sftpusers 4096 Dec 28 23:49 /sftp/guestuser/incoming
drwxr-xr-x 2 guestuser sftpusers 4096 Dec 28 23:49 /sftp/guestuser/outgoing
</code></pre>

<p>The top level directories like this:</p>

<pre><code>$ ls -ld /sftp /sftp/guestuser
drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp
drwxr-xr-x 3 root root 4096 Dec 28 23:49 /sftp/guestuser
</code></pre>

<p>Don't forget to restart the <code>sshd</code> server:</p>

<pre><code>$ sudo service sshd restart
</code></pre>

<p>Now create the userX account:</p>

<pre><code>$ sudo useradd -g sftpusers -d /incoming -s /sbin/nologin userX
$ sudo passwd userX
... set password ...
</code></pre>

<p>You can check that the account was created correctly:</p>

<pre><code>$ grep userX /etc/passwd
userX:x:500:500::/incoming:/sbin/nologin
</code></pre>

<p>When you're done with the account, delete it in the same way above:</p>

<pre><code>$ sudo userdel -r userX
</code></pre>

<p>...and don't forget to remove the configuration file changes we made above, then restart <code>sshd</code> to make them active once more.</p>
","110598"
"How to disable requiretty for a single command in sudoers?","81894","","<p>I want to disable requiretty so that I can sudo within scripts, but I'd rather only disable it for a single command rather than everything. Is that possible within the sudoers config?</p>
","<p>You can override the default setting for options such as <code>requiretty</code> for a specific user or for a specific command (or for a specific run-as-user or host), but not for a specific command when executed as a specific user.</p>

<p>For example, assuming that <code>requiretty</code> is set in the compile-default options, the following <code>sudoers</code> file allows both <code>artbristol</code> and <code>bob</code> to execute <code>/path/to/program</code> as root from a script. <code>artbristol</code> needs no password whereas <code>bob</code> must have to enter a password (presumably <code>tty_tickets</code> is off and <code>bob</code> entered his password on some terminal recently).</p>

<pre><code>artbristol ALL = (root) NOPASSWD: /path/to/program
bob ALL = (root) /path/to/program
Defaults!/path/to/program !requiretty
</code></pre>

<p>If you want to change the setting for a command with specific arguments, you need to use a command alias (this is a syntax limitation). For example, the following fragment allows <code>artbristol</code> to run <code>/path/to/program --option</code> in a script, but not <code>/path/to/program</code> with other arguments.</p>

<pre><code>Cmnd_Alias MYPROGRAM = /path/to/program --option  
artbristol ALL = (root) /path/to/program
artbristol ALL = (root) NOPASSWD: MYPROGRAM
Defaults!MYPROGRAM !requiretty
</code></pre>
","80159"
"Actual memory usage of a process","81583","","<p>The following are the memory usage of <code>mysql</code> and <code>apache</code> respectively on my server. As per the output of <code>pmap</code> say, <code>mysql</code> is using about 379M and <code>apache</code> is using 277M. </p>

<pre><code>[root@server ~]# pmap 10436 | grep total
 total           379564K

[root@server ~]# pmap 10515 | grep total
 total           277588K
</code></pre>

<p>Comparing this with the output of <code>top</code>, I see the values are almost matching.</p>

<pre><code>  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
10515 apache    20   0  271m  32m 3132 S  0.0  6.6   0:00.73 /usr/sbin/httpd
10436 mysql     20   0  370m  21m 6188 S  0.0  4.3   0:06.07 /usr/libexec/mysqld --basedir=....
</code></pre>

<p>Now these values definitely are not the current memory usage of those two processes, since if it were it would've exceeded the 512M <code>ram</code> on my system and I understand the fact that these are the size of the pages assigned to these two processes and not really the size of the memory actively used by them. Now, when we use <code>pmap -x</code>, I see an extra coloumn <code>Dirty</code> which shows far less memory usage for the process. As seen in the example show below, the <code>Dirty</code> coloumn shows 15M as opposed to 379M in the first coloumn. My question is: Is the value under coloumn <code>Dirty</code> is the 'real' amount of memory actively used by that process? If its not, then how can we find out the real memory usage of a process? Not <code>ps</code> and <code>top</code> for the same reasons above. Do we have anything under <code>/proc</code> that will give this info?</p>

<pre><code>[root@server ~]# pmap -x 10436 | grep total
total kB          379564   21528   15340
[root@server ~]#


[root@server ~]# free -m
             total       used       free     shared    buffers     cached
Mem:           489        447         41          0         52        214
-/+ buffers/cache:        180        308
Swap:         1023          0       1023
[root@server ~]#
</code></pre>
","<p>There is no command that gives the “actual memory usage of a process” because <strong>there is no such thing as the actual memory usage of a process</strong>.</p>

<p>Each memory page of a process could be (among other distinctions):</p>

<ul>
<li>Transient storage used by that process alone.</li>
<li>Shared with other processes using a variety of mechanisms.</li>
<li>Backed up by a disk file.</li>
<li>In physical memory or swap.</li>
</ul>

<p>I think the “dirty” figure adds up everything that is in RAM (not swap) and not backed by a file. This includes both shared and non-shared memory (though in most cases other than forking servers, shared memory consists of memory-mapped files only).</p>

<p>The information displayed by <code>pmap</code> comes from <code>/proc/<em>PID</em>/maps</code> and <code>/proc/<em>PID</em>/smaps</code>. That is the real memory usage of the process — it can't be summarized by a single number.</p>
","164721"
"How to check available package versions in rpm systems?","81487","","<p>If I want to check available versions of a package in Debian, I run <code>apt-cache policy pkgname</code> which in the case of <code>wajig</code> gives:</p>

<pre><code>wajig:
  Installed: 2.01
  Candidate: 2.01
  Version table:
 *** 2.01 0
        100 /var/lib/dpkg/status
     2.0.47 0
        500 file:/home/wena/.repo_bin/ squeeze/main i386 Packages
        500 ftp://ftp.is.co.za/debian/ squeeze/main i386 Packages
</code></pre>

<p>That means that there are three wajig packages, one that is installed (<code>/var/lib/dpkg/status</code>), and two others (which are the same version). One of these two is in a local repository and the other is available from a remote repository.</p>

<p>How do I achieve a similar result on rpm systems?</p>
","<p><strong>yum</strong> For RHEL/Fedora/Centos/Scientific Linux</p>

<p>Provides the command <code>list</code> to display information about installed and upgradeable (and older) packages.</p>

<pre><code>yum --showduplicates list &lt;package&gt;
</code></pre>

<p><strong>zypper</strong> For SuSE Linux</p>

<p>Can return a detailed list of available and installed packages or patches.</p>

<pre><code>zypper search -s &lt;package&gt;
</code></pre>

<p>Adding <code>--exact-match</code> can help, if there are multiple packages.</p>

<p>As a side-note, <a href=""https://wiki.archlinux.org/index.php/Pacman_Rosetta"" rel=""noreferrer"">here</a> is a comparison of package-management commands.</p>
","6266"
"How do I identify which Linux distro is running?","81276","","<p>We have some new hardware in our office which runs its own customized Linux OS. </p>

<p>How do I go about figuring which distro it's based on?</p>
","<p>A question very close to this one was posted on Unix.Stackexchange <a href=""https://unix.stackexchange.com/questions/24717/how-to-determine-bitness-of-hardware-and-os"">HERE</a>
<code>Giles</code> has a pretty complete | cool answer for the ways he describes.</p>

<pre><code># cat /proc/version

Linux version 2.6.32-71.el6.x86_64 (mockbuild@c6b6.centos.org) (gcc version 4.4.4 20100726 (Red Hat 4.4.4-13) (GCC) ) #1 SMP Fri May 20 03:51:51 BST 2011  
</code></pre>

<pre># uname -a

Linux system1.doofus.local 2.6.32-71.el6.x86_64 #1 SMP Fri May 20 03:51:51 BST 2011 x86_64 x86_64 x86_64 GNU/Linux</pre>

<pre><code># cat /etc/issue

CentOS Linux release 6.0 (Final)
Kernel \r on an \m
</code></pre>

<p><code>cat /proc/config.gz</code> <code>cat /usr/src/linux/config.gz</code> <code>cat /boot/config*</code></p>

<p>Though I did some checking and this was not very reliable except on SUSE.</p>

<pre># zcat /proc/config.gz | grep -i kernel
CONFIG_SUSE_KERNEL=y
# CONFIG_KERNEL_DESKTOP is not set
CONFIG_LOCK_KERNEL=y</pre>

<p>Release Files in <code>/etc</code> (<a href=""http://www.unix.com/shell-programming-scripting/27932-how-know-linux-distribution-i-am-using.html"" rel=""noreferrer"">from Unix.com</a>)</p>

<ul>
<li>Novell SuSE---> /etc/SuSE-release    </li>
<li>Red Hat--->/etc/redhat-release, /etc/redhat_version  </li>
<li>Fedora-->/etc/fedora-release    </li>
<li>Slackware--->/etc/slackware-release, /etc/slackware-version    </li>
<li>Old Debian--->/etc/debian_release, /etc/debian_version </li>
<li>New Debian--->/etc/os-release</li>
<li>Mandrake--->/etc/mandrake-release  </li>
<li>Yellow dog-->/etc/yellowdog-release     </li>
<li>Sun JDS--->/etc/sun-release  </li>
<li>Solaris/Sparc--->/etc/release      </li>
<li>Gentoo--->/etc/gentoo-release</li>
</ul>

<p>There is also a bash script at the Unix.com link someone wrote to automate checking.</p>

<p>Figuring out what package manager you have is a good clue.</p>

<p><code>rpm</code> <code>yum</code> <code>apt-get</code> <code>zypper</code> +many more</p>

<p>Though this is by no means foolproof as the vendor could use anything they want. It really just gives you a place to start.</p>

<pre># dmesg | less

Linux version 2.6.32.12-0.7-default (geeko@buildhost) (gcc version 4.3.4 [gcc-4_3-branch revision 152973] (SUSE Linux) ) #1 SMP 2010-05-20 11:14:20 +0200</pre>

<p>pretty much the same information as <code>cat /proc/version</code> &amp; <code>uname</code></p>
","35190"
"How to change locale environment variable?","81248","","<p>I have generated <em>en_US.utf8</em>, <em>et_EE.iso88591</em> and <em>ru_RU.utf8</em> localisation files. Now if I try to change any of the locale variables to a <em>ru_RU.utf8</em> or <em>en_US.utf8</em>, then this does not have any effect:</p>

<pre><code># locale -a
C
en_US.utf8
et_EE
et_EE.iso88591
POSIX
ru_RU.utf8
# LC_TIME=ru_RU.utf8
# locale | grep LC_TIME
LC_TIME=""et_EE.iso88591""
# LC_TIME=""ru_RU.utf8""
# locale | grep LC_TIME
LC_TIME=""et_EE.iso88591""
# 
</code></pre>

<p>However, if I change the <em>LANG=</em> variable, then all other variables but <em>LANGUAGE=</em> and <em>LC_ALL=</em> take the value of the <em>LANG=</em> variable. Is there a way to modify each locale variable separately? In addition, am I correct that locale variables aren't regular shell variables, but more like parameters to <em>locale</em> utility?</p>
","<p>You can set any locale category independently. <code>LANG</code> applies only to the categories that are not explicitly set.</p>

<p><code>LANG</code> and <code>LC_xxx</code> are ordinary environment variables. They are not settings for the <code>locale</code> utility: the <code>locale</code> program isn't involved in any locale processing, it's just a small utility to report current and available locale settings.</p>

<p>When you write <code>LC_TIME=ru_RU.utf8</code>, this doesn't set an environment variable, only a shell variable. Shell variables are internal to the shell, they are not seen by other programs. Environment variables, on the other hand, are inherited by the programs that the shell starts. You need to export the variable to the environment as well:</p>

<pre><code>$ LC_TIME=ru_RU.utf8
$ locale | grep LC_TIME
LC_TIME=""et_EE.iso88591""
$ export LC_TIME
$ locale | grep LC_TIME
LC_TIME=""ru_RU.utf8""
</code></pre>

<p>or directly</p>

<pre><code>$ export LC_TIME=ru_RU.utf8
$ locale | grep LC_TIME
LC_TIME=""ru_RU.utf8""
</code></pre>
","74634"
"How to attach terminal to detached process?","81237","","<p>I have detached a process from my terminal, like this:</p>

<pre><code>$ process &amp;
</code></pre>

<p>That terminal is now long closed, but process is still running and I want to send some commands to that process's stdin. Is that possible?</p>
","<p>Yes, it is. First, create a pipe:
<code>mkfifo /tmp/fifo</code>.
 Use gdb to attach to the process:
<code>gdb -p PID</code></p>

<p>Then close stdin: <code>call close (0)</code>; and open it again: <code>call open (""/tmp/fifo"", 0600)</code></p>

<p>Finally, write away (from a different terminal, as gdb will probably hang):</p>

<p><code>echo blah &gt; /tmp/fifo</code></p>
","31830"
"Markdown Viewer","81099","","<p>I found a file formatted with Markdown. Could you guys suggest what viewer that I could use to view this type of files? Hopefully one without gui (if it's possible)</p>

<p><strong>Update</strong>
I was actually looking for a viewer that could parse markdown file format that does not need any conversion. But something close to that should be ok.</p>
","<p>The following website provides a tool that will translate markdown into HTML:</p>

<p><a href=""http://daringfireball.net/projects/markdown/"">http://daringfireball.net/projects/markdown/</a></p>

<p>Once you convert the file to HTML, there are a number of command line tools to use to view the file.  Using a test file that contains markdown formatted-text, I found the following worked nicely.</p>

<pre><code>$ wget http://daringfireball.net/projects/downloads/Markdown_1.0.1.zip
$ unzip Markdown_1.0.1.zip
$ cd Markdown_1.0.1/
$ ./Markdown.pl ~/testfile.markdown | html2text
</code></pre>

<p><a href=""http://www.mbayer.de/html2text/"">html2text</a> is one of many tools you can use to view html formatted text from the command line.  Another option, if you want slightly nicer output would be to use <a href=""http://lynx.browser.org/"">lynx</a>:</p>

<pre><code>$ ./Markdown.pl ~/testfile.markdown | lynx -stdin
</code></pre>

<p>If you are an emacs user, someone has written a mode for markdown which is available here: <a href=""http://jblevins.org/projects/markdown-mode/"">http://jblevins.org/projects/markdown-mode/</a>.  This provides nice syntax highlighting as can be seen in the screenshot on that website.</p>

<p>All of these tools should be available for slackware. </p>
","4141"
"Change permissions for a symbolic link","81093","","<p>I have a symlink with these permissions:</p>

<pre><code>lrwxrwxrwx 1 myuser myuser       38 Aug 18 00:36 npm -&gt; ../lib/node_modules/npm/bin/npm-cli.js*
</code></pre>

<p>The symlink is located in a .tar.gz archive. Now when I unpack the tar.gz archive using maven the symlink is no longer valid. I'm therefore trying to reconstruct the symlink. First I create the symlink using <strong>ln</strong> but how do I set the same permissions as the original symlink?</p>
","<p>You can make a new symlink and move it to the location of the old link.</p>

<pre><code>ln -s &lt;new_location&gt; npm2
mv -f npm2 npm
</code></pre>

<p>That will preserve the link ownership. Alternatively, you can use <code>chown</code> to set the link's ownership manually.</p>

<pre><code>chown -h myuser:myuser npm
</code></pre>

<p>On most systems, symlink permissions don't matter. When using the symlink, the permissions of the components of symlink's target will be checked.</p>
","87202"
"The name > org.freedesktop.PolicyKit1 was not provided by any .service files","80252","","<p>When attempting to launch <code>system-config-users</code> from command line, I get the following warning, and the tool does not open.   I'm using CentOS 7 with Mate 1.8.1.</p>

<blockquote>
  <p>WARNING **: Error enumerating actions:
  GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name
  org.freedesktop.PolicyKit1 was not provided by any .service files</p>
  
  <p>Error checking for authorization org.freedesktop.policykit.exec:
  GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name
  org.freedesktop.PolicyKit1 was not provided by any .service files</p>
</blockquote>

<p>yum list polkit*</p>

<pre><code>Installed Packages
polkit.x86_64   0.112-5.el7 @anaconda
polkit-devel.x86_64 0.112-5.el7 @base    
polkit-docs.noarch 0.112-5.el7 @base    
polkit-gnome.x86_64 0.105-6.el7 @epel    
polkit-pkla-compat.x86_64 0.1-4.el7  @anaconda
</code></pre>

<p>What is missing from my system to cause this error? </p>
","<p>I just had the same return when installing deluged on arch, I typed:</p>

<pre><code> systemctl start deluged
</code></pre>

<p>I tried with sudo and it worked fine. Seems to be a group permissions issue. </p>

<p>All I did was enable permissions for my user account and then typed:</p>

<pre><code> sudo systemctl start deluged
</code></pre>

<p>worked like a charm... hope this helps!</p>
","166785"
"Reserved space for root on a filesystem - why?","80245","","<p>I understand that by default, newly created filesystems will be created with 5% of the space allocated for root. I also know you can change the defined space with:</p>

<pre><code>tune2fs -m 1 /dev/sdXY
</code></pre>

<p>What I'm curious about though, is what the actual purpose for this reserved space is. Does it serve any practical purpose which would merit more than 5% space in some circumstances?</p>

<p>The reason I've stumbled upon this question is that we recently built a 1TB filestore, and couldn't quite figure out why a df -h left us missing 5% of our capacity.</p>
","<p>Saving space for important root processes (and possible rescue actions) is one reason.</p>

<p>But there's another. Ext3 is pretty good at avoiding filesystem fragmentation, but once you get above about 95% full, that behavior falls off the cliff, and suddenly filesystem performance becomes a mess. So leaving 5% reserved gives you a buffer against this.</p>

<p>Ext4 should be better at this, as <a href=""https://www.redhat.com/archives/ext3-users/2009-January/msg00026.html"">explained by Linux filesystem developer/guru Theodore Ts'o</a>:</p>

<blockquote>
  <p>If you set the reserved block count to
  zero, it won't affect performance much
  except if you run for long periods of
  time (with lots of file creates and
  deletes) while the filesystem is
  almost full (i.e., say above 95%), at
  which point you'll be subject to
  fragmentation problems.  Ext4's
  multi-block allocator is much more
  fragmentation resistant, because it
  tries much harder to find contiguous
  blocks, so even if you don't enable
  the other ext4 features, you'll see
  better results simply mounting an ext3
  filesystem using ext4 before the
  filesystem gets completely full.</p>
  
  <p>If you are just using the filesystem
  for long-term archive, where files
  aren't changing very often (i.e., a
  huge mp3 or video store), it obviously
  won't matter.</p>
</blockquote>
","7965"
"What is the purpose of using shift in shell scripts?","80166","","<p>I have came across this script:</p>

<pre><code>#! /bin/bash                                                                                                                                                                                           

if (( $# &lt; 3 )); then
  echo ""$0 old_string new_string file [file...]""
  exit 0
else
  ostr=""$1""; shift
  nstr=""$1""; shift  
fi

echo ""Replacing \""$ostr\"" with \""$nstr\""""
for file in $@; do
  if [ -f $file ]; then
    echo ""Working with: $file""
    eval ""sed 's/""$ostr""/""$nstr""/g' $file"" &gt; $file.tmp 
    mv $file.tmp $file
  fi  
done
</code></pre>

<p>and I cannot wrap my head around the lines where they use <code>shift</code>. </p>

<p>What is the meaning of this? I presume the script should be used with at least arguments so...?</p>
","<p><code>shift</code> is a <code>bash</code> built-in which kind of removes arguments in beginning of the argument list. Given that the arguments provided to the script are 3 available in $1, $2, $3, then a call to <code>shift</code>  will make $2 the new $1.
a <code>shift 2</code> will shift by two making new $1 the old $3.
for more info see here</p>

<ul>
<li><a href=""http://ss64.com/bash/shift.html"">http://ss64.com/bash/shift.html</a></li>
<li><a href=""http://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_07.html"">http://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_07.html</a></li>
</ul>
","174568"
"Use shared libraries in /usr/local/lib","80113","","<p>I have build some libraries from sources, and the files after <code>make install</code> are in <code>/usr/local/lib</code></p>

<p>For example, in my case I have the file <code>libodb-2.2.so</code> which is in this directory.</p>

<p>However when I launch the executable that has linked with <code>libodb</code>, I got the error: error while loading shared libraries: <code>libodb-2.2.so: cannont open shared object file: No such file or directory.</code></p>

<p>Does it mean that I have build my executable not correctly ? or should I indicate the system that there may be some interesting libs in the folder <code>/usr/local/lib</code> also ?</p>

<p>I'm using Ubuntu 12.04, Linux kernel 3.2.0-38-generic.</p>
","<p>For the current session you can</p>

<pre>
export LD_LIBRARY_PATH=/lib:/usr/lib:/usr/local/lib
</pre>

<p>or to make the change permanent you can add <code>/usr/local/lib</code> to <code>/etc/ld.so.conf</code> (or something it includes) and run <code>ldconfig</code> as root.</p>

<p>If you're still having problems, running <code>ldd [executable name]</code> will show you the libraries it's trying to find, and which ones can't be found.</p>
","67783"
"Disable user shell for security reasons","80110","","<p>We have several user accounts that we create for automated tasks that require fine-grained permissions, such as file transfer across systems, monitoring, etc. </p>

<p>How do we lock down these user accounts so that these ""users"" have no shell and are not able to login? We want to prevent the possibility that someone can SSH in as one of these user accounts.</p>
","<p>You can use the <code>usermod</code> command to change a user's login shell.</p>

<pre><code>usermod -s /sbin/nologin myuser
</code></pre>

<p>or</p>

<pre><code>usermod -s /usr/sbin/nologin myuser
</code></pre>

<p>If your OS does not provide /sbin/nologin, you can set the shell to a NOOP command such as /bin/false:</p>

<pre><code>usermod -s /bin/false myuser
</code></pre>
","55113"
"How can I output the difference between 2 files?","80025","","<p>My file consists of the the following; roughly:</p>

<pre><code>username:username:username:username:username 
</code></pre>

<p>The above line continues to about 600 characters.</p>

<p>I use the <code>awk</code> command in order to use it as an argument in a API/HTTP request sent from the command line.  </p>

<p>I'm using my script to get a list of user accounts 'following' me, and every 24 hours or so, comparing the original list on my hard disk to the newly outputted username list (and <strong>echo'ing out who is no longer following me</strong>.
I will have to encapsulate my logic into a loop using bash.. testing each username. </p>

<p>My current script:</p>

<pre><code>user=$(awk -F: '{ print $1 }' FILE)  # Grab $User to use as an argument.  
following=$(exec CURRENT_FOLLOWERS) # Outputs the new file

echo ""X amount of users are following you on 78B066B87AF16A412556458AC85EFEF66155""  

          SAVE CURRENT FOLLOWERS TO NEW A FILE.  


if [[ DIFFERENCE IS DETECTED ]] ; then    

          echo -ne ""$User NO LONGER FOLLOWING YOU\r""

   else echo -ne ""This user is following you still.\r""
fi
</code></pre>

<p>My question is;<br>
<strong>How can I output the difference between 2 files?</strong></p>
","<p>The utility you're looking for is <code>diff</code>. Take a peek at the manual for details. </p>
","144637"
"Why does ""ls | wc -l"" show the correct number of files in current directory?","79998","","<p>Trying to count number of files in current directory, I found <code>ls -1 | wc -l</code>, which means: send the list of files (where every filename is printed in a new line) to the input of wc, where <code>-l</code> will count the number of lines on input. This makes sense.</p>

<p>I decided to try simply <code>ls | wc -l</code> and was very surprised about it also gives me a correct number of files. I wonder why this happens, because <code>ls</code> command with no options prints the filenames on a single line.</p>
","<p>From <code>info ls</code>:</p>

<blockquote>
  <p>'-1'<br>
  '--format=single-column'</p>
  
  <blockquote>
    <p>List one file per line.  <strong>This is the default for 'ls' when standard
         output is not a terminal.</strong></p>
  </blockquote>
</blockquote>

<p>When you pipe the output of <code>ls</code>, you get one filename per line.<br>
<code>ls</code> only outputs the files in columns when the output is destined for human eyes.</p>

<hr>

<p>Here's where <code>ls</code> decides what to do:</p>

<pre class=""lang-c prettyprint-override""><code>  switch (ls_mode)
    {
    case LS_MULTI_COL:
      /* This is for the 'dir' program.  */
      format = many_per_line;
      set_quoting_style (NULL, escape_quoting_style);
      break;

    case LS_LONG_FORMAT:
      /* This is for the 'vdir' program.  */
      format = long_format;
      set_quoting_style (NULL, escape_quoting_style);
      break;

    case LS_LS:
      /* This is for the 'ls' program.  */
      if (isatty (STDOUT_FILENO))
        {
          format = many_per_line;
          /* See description of qmark_funny_chars, above.  */
          qmark_funny_chars = true;
        }
      else
        {
          format = one_per_line;
          qmark_funny_chars = false;
        }
      break;

    default:
      abort ();
    }
</code></pre>

<p>source: <a href=""http://git.savannah.gnu.org/cgit/coreutils.git/tree/src/ls.c"">http://git.savannah.gnu.org/cgit/coreutils.git/tree/src/ls.c</a></p>
","157289"
"Is Mac OS X UNIX?","79664","","<p>I have this argument recently saying Mac OS X was not UNIX, but Unix-like. </p>

<p>I know there is a Single Unix Specification and those spec complaint could use the UNIX trade mark.</p>

<p>Is Mac OS X an UNIX operating system or is it an Unix-like?</p>
","<p>All but one release of Mac OS X (now macOS) has been <a href=""http://www.opengroup.org/openbrand/register/"" rel=""nofollow noreferrer"">certified as Unix</a> by <a href=""http://www.opengroup.org/"" rel=""nofollow noreferrer"">The Open Group</a>, starting with 10.5:</p>

<ul>
<li><a href=""https://www.opengroup.org/openbrand/register/brand3632.htm"" rel=""nofollow noreferrer"">10.13 (High Sierra)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3627.htm"" rel=""nofollow noreferrer"">10.12 (Sierra)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3612.htm"" rel=""nofollow noreferrer"">10.11 (El Capitan)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3607.htm"" rel=""nofollow noreferrer"">10.10 (Yosemite)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3602.htm"" rel=""nofollow noreferrer"">10.9 (Mavericks)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3591.htm"" rel=""nofollow noreferrer"">10.8 (Mountain Lion)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3581.htm"" rel=""nofollow noreferrer"">10.6 (Snow Leopard)</a></li>
<li><a href=""http://www.opengroup.org/openbrand/register/brand3555.htm"" rel=""nofollow noreferrer"">10.5 (Leopard)</a></li>
</ul>

<p>At any given time, <a href=""http://www.opengroup.org/openbrand/register/apple.htm"" rel=""nofollow noreferrer"">Apple's page on The Open Group site</a> only lists the current version of macOS and sometimes the previous version, but all of the links above were at one point found via that page.</p>

<p>OS X's status as a certified Unix is called out in Apple's <a href=""http://images.apple.com/macosx/docs/OSX_for_UNIX_Users_TB_July2011.pdf"" rel=""nofollow noreferrer"">Unix technology brief</a>, which also has other good technical bits in it that will help you compare it to other UNIX&reg; and Unix-like systems.</p>

<p>I monitor Apple's page on the Open Group's web site due to the popularity of this answer, and I never saw Lion show up there. Poking around with the clear URL scheme in the links above also turns up no hidden Lion certification page. I have no idea why Lion never was certified.</p>
","1490"
"Create a tar archive split into blocks of a maximum size","79620","","<p>I need to backup a fairly large directory, but I am limited by the size of individual files. I'd like to essentially create a <code>tar.(gz|bz2)</code> archive which is split into 200MB maximum archives. Clonezilla does something similar to this by splitting image backups named like so:</p>

<pre><code>sda1.backup.tar.gz.aa
sda1.backup.tar.gz.ab
sda1.backup.tar.gz.ac
</code></pre>

<p>Is there a way I can do this in one command? I understand how to use the <code>split</code> command, but I'd like to not have to create one giant archive, then split it into smaller archives, as this would double the disk space I'd need in order to initially create the archive.</p>
","<p>You can pipe tar to the split command:</p>

<pre><code>tar cvzf - dir/ | split --bytes=200MB - sda1.backup.tar.gz.
</code></pre>

<p>On some *nix systems (like OS X) you may get the following error:</p>

<pre><code>split: illegal option -- -
</code></pre>

<p>In that case try this (note the <code>-b 200m</code>):</p>

<pre><code>tar cvzf - dir/ | split -b 200m - sda1.backup.tar.gz.
</code></pre>

<p>If you happen to be trying to split file to fit on a FAT32 formatted drive use a byte limit of 4294967295. For example:</p>

<pre><code>tar cvzf - /Applications/Install\ macOS\ Sierra.app/ | \
split -b 4294967295 - /Volumes/UNTITLED/install_macos_sierra.tgz.
</code></pre>
","61776"
"How to pull a file from a server using scp?","79555","","<p>I am on machine A and want to pull a file from machine B.</p>

<pre><code>A$ scp &lt;myuserid&gt;@hostB:&lt;path of file in B&gt; .
</code></pre>

<p>it says that:</p>

<pre><code>scp: &lt;path of file in B&gt;: No such file or directory
</code></pre>

<p>But on machine B, this file exists in this path.</p>

<p>What is going wrong?</p>
","<p>You didn't specify any file: you have to add the file (with path) after the colon:</p>

<pre><code>A$ scp &lt;myuserid&gt;@hostB:/absolutepath/file .
</code></pre>

<p>or</p>

<pre><code>A$ scp &lt;myuserid&gt;@hostB:relativepath/file .
</code></pre>

<p>for a path relative to your home directory.</p>

<p>If you don't specify a different user (i.e., the user on A and B are the same) you don't need the <code>@</code></p>

<pre><code>A$ scp hostB:/path/file .
</code></pre>
","22503"
"How do I run 32-bit programs on a 64-bit Debian/Ubuntu?","79424","","<p>I have a 64-bit (amd64 a.k.a. x86_64) Debian or Ubuntu installation. I need to run 32-bit (i386/i686) programs occasionally, or to compile programs for a 32-bit system. How can I do this with a minimum of fuss?</p>

<p>Bonus: what if I want to run or test with an older or newer release of the distribution?</p>
","<h2>Native support</h2>

<p>Since Ubuntu 11.04 and Debian wheezy (7.0), Debian and Ubuntu have multiarch support: you can mix x86_32 (i386) and x86_64 (amd64) packages on the same system in a straightforward way. This is known as <a href=""https://help.ubuntu.com/community/MultiArch"" rel=""noreferrer"">multiarch support</a> See <a href=""https://unix.stackexchange.com/questions/12956/how-do-i-run-32-bit-programs-on-a-64-bit-ubuntu/47003#47003"">warl0ck's answer</a> for more details.</p>

<p>In older releases, Debian and Ubuntu ship with a number of 32-bit libraries on amd64. Install the <a href=""http://packages.ubuntu.com/ia32-libs"" rel=""noreferrer""><code>ia32-libs</code></a> <a href=""http://apt.ubuntu.com/p/ia32-libs"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install ia32-libs""></a> package to have a basic set of 32-bit libraries, and possibly other packages that depend on this one. Your 32-bit executables should simply run if you have all the required libraries. For development, install <a href=""http://packages.ubuntu.com/gcc-multilib"" rel=""noreferrer""><code>gcc-multilib</code></a> <a href=""http://apt.ubuntu.com/p/gcc-multilib"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install gcc-multilib""></a>, and again possibly other packages that depend on it such as <code>g++-multilib</code>. You may find <a href=""http://packages.ubuntu.com/binutils-multiarch"" rel=""noreferrer""><code>binutils-multiarch</code></a> <a href=""http://apt.ubuntu.com/p/binutils-multiarch"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install binutils-multiarch""></a> useful as well, and <a href=""http://packages.debian.org/ia32-libs-dev"" rel=""noreferrer""><code>ia32-libs-dev</code></a> on Debian. Pass the <code>-m32</code> option to gcc to compile for ix86.</p>

<p>Note that <code>uname -m</code> will still show <code>x64_64</code> if you're running a 64-bit kernel, regardless of what 32-bit user mode components you have installed. Schroot described below takes care of this.</p>

<h2>Schroot</h2>

<p>This section is a guide to installing a Debian-like distribution “inside” another Linux distribution. It is worded in terms of installing a 32-bit Ubuntu inside a 64-bit Ubuntu, but should apply with minor modifications to other situations, such as installing Debian unstable inside Debian stable or vice versa.</p>

<h3>Introduction</h3>

<p>The idea is to install an alternate distribution in a subtree and run from that. You can install a 32-bit system on a 64-bit system that way, or a different release of your distribution, or a testing environment with different sets of packages installed.</p>

<p>The <a href=""http://en.wikipedia.org/wiki/Chroot"" rel=""noreferrer""><code>chroot</code></a> command and system call starts a process with a view of the filesystem that's restricted to a subtree of the directory tree. Debian and Ubuntu ship <a href=""http://www.debian-administration.org/articles/566"" rel=""noreferrer"">schroot</a>, a utility that wraps around this feature to create a more usable sub-environment.</p>

<p>Install the <a href=""http://packages.ubuntu.com/schroot"" rel=""noreferrer""><code>schroot</code> package</a> <a href=""http://apt.ubuntu.com/p/schroot"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install schroot""></a> (<a href=""http://packages.debian.org/schroot"" rel=""noreferrer"">Debian</a>) and the <a href=""http://packages.ubuntu.com/debootstrap"" rel=""noreferrer""><code>debootstrap</code> package</a> <a href=""http://apt.ubuntu.com/p/debootstrap"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install debootstrap""></a> (<a href=""http://packages.debian.org/debootstrap"" rel=""noreferrer"">Debian</a>). Debootstrap is only needed for the installation of the alternate distribution and can be removed afterwards.</p>

<h3>Set up schroot</h3>

<p>This example describes how to set up a 32-bit Ubuntu 10.04LTS (lucid lynx) alternate environment. A similar setup should work with other releases of Debian and Ubuntu. Create a file <code>/etc/schroot/chroot.d/lucid32</code> with the following contents:</p>

<pre><code>[lucid32]
description=Ubuntu 10.04LTS 32-bit
directory=/32
type=directory
personality=linux32
users=yourusername
groups=users,admin
</code></pre>

<p>The line <code>directory=/32</code> tells schroot where we'll put the files of the 32-bit installation. The line <code>username=yourusername</code> says the user <code>yourusername</code> will be allowed to use the schroot. The line <code>groups=users,admin</code> says that users in either group will be allowed to use the schroot; you can also put a <code>users=…</code> directive.</p>

<h3>Install the new distribution</h3>

<p>Create the directory and start populating it with debootstrap. Debootstrap downloads and installs a core set of packages for the specified distribution and architecture.</p>

<pre><code>mkdir /32
debootstrap --arch i386 lucid /32 http://archive.ubuntu.com/ubuntu
</code></pre>

<p>You almost have a working system already; what follows is minor enhancements. Schroot automatically overwrites several files in <code>/32/etc</code> when you run it, in particular the DNS configuration in <code>/etc/resolv.conf</code> and the user database in <code>/etc/passwd</code> and other files (this can be overridden, see the documentation). There are a few more files you may want to copy manually once and for all:</p>

<pre><code>cp -p /etc/apt/apt.conf /32/etc/apt/      # for proxy settings
cp -p /etc/apt/sources.list /32/etc/apt/  # for universe, security, etc
cp -p /etc/environment /32/etc/           # for proxy and locale settings
cp -p /etc/sudoers /32/etc/               # for custom sudo settings
</code></pre>

<p>There won't be a file <code>/etc/mtab</code> or <code>/etc/fstab</code> in the chroot. I don't recommend using the <code>mount</code> command manually in the chroot, do it from outside. But do create a good-enough <code>/etc/mtab</code> to make commands such as <code>df</code> work reasonably.</p>

<pre><code>ln -s /proc/mounts /32/etc/mtab
</code></pre>

<p>With the <code>directory</code> type, schroot will perform <a href=""http://wiki.openvz.org/Bind_mounts"" rel=""noreferrer"">bind mounts</a> of a number of directories, i.e. those directories will be shared with the parent installation: <code>/proc</code>, <code>/dev</code>, <code>/home</code>, <code>/tmp</code>.</p>

<h3>Services in the chroot</h3>

<p>As described here, a schroot is not suitable for running daemons. Programs in the schroot will be killed when you exit the schroot. Use a “plain” schroot instead of a “directory” schroot if you want it to be more permanent, and set up permanent bind mounts in <code>/etc/fstab</code> on the parent installation.</p>

<p>On Debian and Ubuntu, services start automatically on installation. To avoid this (which could disrupt the services running outside the chroot, in particular because network ports are shared), establish a <a href=""http://people.debian.org/~hmh/invokerc.d-policyrc.d-specification.txt"" rel=""noreferrer"">policy</a> of not running services in the chroot. Put the following script as <a href=""http://manpages.ubuntu.com/manpages/lucid/man8/runit-policy-rc.d.8.html"" rel=""noreferrer""><code>/32/usr/sbin/policy-rc.d</code></a> and make it executable (<code>chmod a+rx /32/usr/sbin/policy-rc.d</code>).</p>

<pre><code>#!/bin/sh
## Don't start any service if running in a chroot.
## See /usr/share/doc/sysv-rc/README.policy-rc.d.gz
if [ ""$(stat -c %d:%i /)"" != ""$(stat -c %d:%i /proc/1/root/.)"" ]; then
  exit 101
fi
</code></pre>

<h3>Populate the new system</h3>

<p>Now we can start using the chroot. You'll want to install a few more packages at this point.</p>

<pre><code>schroot -c lucid32
sudo apt-get update
apt-get install lsb-core nano
...
</code></pre>

<p>You may need to generate a few locales, e.g.</p>

<pre><code>locale-gen en_US en_US.utf8
</code></pre>

<p>If the schroot is for an older release of Ubuntu such as 8.04 (hardy), note that the package ubuntu-standard pulls in an MTA. Select <code>nullmailer</code> instead of the default <code>postfix</code> (you may want your chroot to send mail but you definitely don't want it to receive any).</p>

<h3>Going further</h3>

<p>For more information, see the <a href=""http://manpages.ubuntu.com/manpages/lucid/man1/schroot.1.html"" rel=""noreferrer""><code>schroot</code> manual</a>, the <a href=""http://manpages.ubuntu.com/manpages/natty/en/man7/schroot-faq.7.html"" rel=""noreferrer"">schroot FAQ</a> and the
<a href=""http://manpages.ubuntu.com/manpages/lucid/man5/schroot.conf.5.html"" rel=""noreferrer""><code>schroot.conf</code> manual</a>. Schroot is part of the <a href=""http://alioth.debian.org/projects/buildd-tools"" rel=""noreferrer"">Debian autobuilder (buildd) project</a>. There may be additional useful tips on the <a href=""https://help.ubuntu.com/community/DebootstrapChroot"" rel=""noreferrer"">Ubuntu community page about debootstrap</a>.</p>

<h2>Virtual machine</h2>

<p>If you need complete isolation of the alternate environment, use a virtual machine such as <a href=""http://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine"" rel=""noreferrer"">KVM</a> (<a href=""http://packages.ubuntu.com/qemu-kvm"" rel=""noreferrer"">qemu-kvm</a> <a href=""http://apt.ubuntu.com/p/qemu-kvm"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M0p7o.png"" alt=""Install qemu-kvm""></a>) or <a href=""http://www.virtualbox.org/"" rel=""noreferrer"">VirtualBox</a>.</p>
","12957"
"How to introduce timeout for shell scripting?","79416","","<p>I want to run a shell script that got a loop in it and it can go for ever which I do not want to happen. So I need to introduce a timeout for the whole script. </p>

<p>How can I introduce a timeout for the whole shell script under SuSE?</p>
","<p>If GNU <code>timeout</code> is not available you can use <code>expect</code> (Mac OS X, BSD, ... do not usually have GNU tools and utilities by default).</p>

<pre><code>################################################################################
# Executes command with a timeout
# Params:
#   $1 timeout in seconds
#   $2 command
# Returns 1 if timed out 0 otherwise
timeout() {

    time=$1

    # start the command in a subshell to avoid problem with pipes
    # (spawn accepts one command)
    command=""/bin/sh -c \""$2\""""

    expect -c ""set echo \""-noecho\""; set timeout $time; spawn -noecho $command; expect timeout { exit 1 } eof { exit 0 }""    

    if [ $? = 1 ] ; then
        echo ""Timeout after ${time} seconds""
    fi

}
</code></pre>

<p><strong>Edit</strong>
Example:</p>

<pre><code>timeout 10 ""ls ${HOME}""
</code></pre>
","43346"
"ps switches to display PID, PPID, PGID, and SID collectively","79358","","<p>I tried ps with different kinds of switches e.g. -A, aux, ef, and so forth but I cannot seem to find the right combination of switches that will tell me the Process ID (PID), Parent Process ID (PPID), Process Group ID (PGID), and the Session ID (SID) of a process in the same output.</p>
","<p>Here you go:</p>

<pre><code>$ ps  xao pid,ppid,pgid,sid | head
  PID  PPID  PGID   SID
    1     0     1     1
    2     0     0     0
    3     2     0     0
    6     2     0     0
    7     2     0     0
   21     2     0     0
   22     2     0     0
   23     2     0     0
   24     2     0     0
</code></pre>

<p>If you want to see the process' name as well, use this:</p>

<pre><code>$ ps  xao pid,ppid,pgid,sid,comm | head
  PID  PPID  PGID   SID COMMAND
    1     0     1     1 init
    2     0     0     0 kthreadd
    3     2     0     0 ksoftirqd/0
    6     2     0     0 migration/0
    7     2     0     0 watchdog/0
   21     2     0     0 cpuset
   22     2     0     0 khelper
   23     2     0     0 kdevtmpfs
   24     2     0     0 netns
</code></pre>
","82727"
"Health check of web page using curl","79323","","<p>I'd like to do a health check of a service by calling a specific url on it. Feels like the simplest solution would be to use cron to do the check every minute or so. In case of errors, cron sends me an email. </p>

<p>I tried using cUrl for this but I can't get it to output messages only on errors. If I try to direct output to /dev/null, it prints out progress report.</p>

<pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  5559  100  5559    0     0   100k      0 --:--:-- --:--:-- --:--:--  106k
</code></pre>

<p>I tried looking through the curl options but I just can't find anything to suit the situation where you want it to be silent on success but make noise on errors.</p>

<p>Is there a way to make curl do what I want or is there some other tool I should be looking at?</p>
","<p>What about <code>-sSf</code>? From the man pages:</p>

<blockquote>
<pre><code>  -s/--silent
     Silent or quiet mode. Do not show progress meter or error messages.  
     Makes Curl mute.

  -S/--show-error
     When used with -s it makes curl show an error message if it fails.

  -f/--fail
     (HTTP)  Fail silently (no output at all) on server errors. This is mostly
     done to better enable scripts etc to better deal with failed attempts. In
     normal  cases  when a HTTP server fails to deliver a document, it returns
     an HTML document stating so (which often also describes  why  and  more).
     This flag will prevent curl from outputting that and return error 22.

     This method is not fail-safe and there are occasions where non-successful
     response codes will  slip  through,  especially  when  authentication  is
     involved (response codes 401 and 407).
</code></pre>
</blockquote>

<p>For example:</p>

<pre><code>curl -sSf http://example.org &gt; /dev/null
</code></pre>
","84818"
"Linux: allowing an user to listen to a port below 1024","79209","","<p>I need to allow an user (different from root) to run a server listening on port 80.</p>

<p>Is there any way to do this?</p>
","<p><code>setcap 'cap_net_bind_service=+ep' /path/to/program</code></p>

<p>this will work for specific processes. But to allow a particular user to bind to ports below 1024 you will have to add him to sudoers. </p>

<p>Have a look at this <a href=""https://stackoverflow.com/questions/413807/is-there-a-way-for-non-root-processes-to-bind-to-privileged-ports-1024-on-li"">discussion</a> for more.</p>
","10737"
"pdksh missing from RHEL 6 and CentOS 6?","79124","","<p>The package <code>pdksh</code> does not exist on RHEL 6 or CentOS 6 when I try to install it through <code>yum</code>.</p>

<p>So my questions:</p>

<ol>
<li>Why is it no longer available in the default repositories? It is available in version 5.x.</li>
<li>Is there a replacement shell?</li>
<li>What about programs that depend on <code>pdksh</code>? Can they use the replacement shell?</li>
</ol>

<p>My workaround has been to use the package from version 5, which I feel is not the proper solution and I should understand it better than the workaround.</p>

<p>My concern is mainly regarding Oracle Database. It asks for <code>pdksh</code>, so will it be a problem to use a replacment shell other than <code>pdksh</code>?</p>
","<p>I was under the impression that pdksh was not POSIX compliant, and with the release of open source AT&amp;T KSH, there was significant movement from pdksh to <a href=""http://www.kornshell.com/software/"" rel=""nofollow"">ksh</a>. Just my $0.02</p>

<p>AT&amp;T ksh is licensed by the Eclipse Public License 1.0 (EPL-1.0).</p>
","29556"
"sudo: unable to execute ./script.sh: no such file or directory","79043","","<p>I'm stumped. I have a script in my <code>/home</code> directory which is executable:</p>

<pre><code>[user@server ~]$ ll
total 4
-rwx------ 1 user user 2608 Jul 15 18:23 qa.sh
</code></pre>

<p>However, when I attempt to run it with <code>sudo</code> it says it can't find it:</p>

<pre><code>[user@server ~]$ sudo ./qa.sh 
[sudo] password for user: 
sudo: unable to execute ./qa.sh: No such file or directory
</code></pre>

<p>This is on a fresh build. No changes have been made which would cause problems. In fact, the point of the script is to ensure that it is actually built according to our policies. Perhaps maybe it isn't and <code>sudo</code> is actually being broken during the build?</p>

<p>I should also note that I can run <code>sudo</code> with other commands in other directories.</p>

<p>EDIT: The script ( I didn't write it so don't <code>/bin/bash</code> me over it, please ;) )</p>

<pre><code>#! /bin/bash

. /root/.bash_profile

customer=$1

if [ -z ""$customer"" ]; then

        echo ""Customer not provided. Exiting...""
        exit 1

fi

space ()
{
echo
echo '###########################################################################'
echo '###########################################################################'
echo '###########################################################################'
echo
}

g=/bin/egrep

$g ^Listen /etc/ssh/sshd_config
$g ^PermitR /etc/ssh/sshd_config
$g ^LogL /etc/ssh/sshd_config
$g ^PubkeyA /etc/ssh/sshd_config
$g ^HostbasedA /etc/ssh/sshd_config
$g ^IgnoreR /etc/ssh/sshd_config
$g ^PermitE /etc/ssh/sshd_config
$g ^ClientA /etc/ssh/sshd_config

space

$g 'snyder|rsch|bream|shud|mweb|dam|kng|cdu|dpr|aro|pvya' /etc/passwd ; echo ; echo ; $g 'snyder|rsch|bream|shud|mweb|dam|kng|cdu|dpr|aro|pvya' /etc/shadow

space

$g 'dsu|scan' /etc/passwd ; echo ; echo ; $g 'dsu|scan' /etc/shadow

space

$g ${customer}admin /etc/passwd

space

chage -l ${customer}admin

space

$g 'urs|cust|dsu' /etc/sudoers

space

$g dsu /etc/security/access.conf

space

$g account /etc/pam.d/login

space

/sbin/ifconfig -a | $g addr | $g -v inet6

space

echo ""10.153.156.0|10.153.174.160|10.120.80.0|10.152.80.0|10.153.193.0|172.18.1.0|10.153.173.0""
echo
$g '10.153.156.0|10.153.174.160|10.120.80.0|10.152.80.0|10.153.193.0|172.18.1.0|10.153.173.0' /etc/sysconfig/network-scripts/route-eth1

space

cat /etc/sysconfig/network-scripts/route-eth2

space

netstat -rn | tail -1

space

cat /etc/sysconfig/iptables

space

cat /etc/hosts

space

##file /usr/local/groundwork ; echo ; echo ; /sbin/service gdma status

##space

cat /etc/resolv.conf

space

HOSTNAME=`echo $HOSTNAME | awk -F. '{ print $1 }'`

nslookup ${HOSTNAME}

echo
echo

nslookup ${HOSTNAME}-mgt

echo
echo

nslookup ${HOSTNAME}-bkp

space

/sbin/service rhnsd status ; echo ; echo ; /sbin/chkconfig --list rhnsd ; echo ; echo ; yum update --security

space

/sbin/service osad status ; echo ; echo ; /sbin/chkconfig --list osad

space

/sbin/service sshd status ; echo ; echo ; /sbin/chkconfig --list sshd

space

/sbin/service snmpd status ; echo ; echo ; /sbin/chkconfig --list snmpd ; echo ; echo ; echo ; cat /etc/snmp/snmpd.conf

space

df -h

space

cat /proc/cpuinfo | $g ^processor

space

free -g

space

if [ -f /etc/rsyslog.conf ]; then

        tail -3 /etc/rsyslog.conf

else

        echo ""This system is not running rsyslog.""

fi

rm -f $0
</code></pre>
","<p>This usually happens when the shebang (<code>#!</code>) line in your script is broken.</p>

<p>The shebang is what tells the kernel the file needs to be executed using an interpreter. When run without <code>sudo</code>, the message is a little more meaningful. But with <code>sudo</code> you get the message you got.</p>

<p>For example:</p>

<pre><code>$ cat test.sh
#!/bin/foo
echo bar

$ ./test.sh
bash: ./test.sh: /bin/foo: bad interpreter: No such file or directory

$ bash test.sh
bar

$ sudo ./test.sh
sudo: unable to execute ./test.sh: No such file or directory

$ sudo bash ./test.sh
bar
</code></pre>

<p>The <code>bad interpreter</code> message clearly indicates that it's the shebang which is faulty.</p>
","144719"
"Sort data in descending order of first column, for equal values, use second column in ascending order","78963","","<p>Allow me to clarify:</p>

<p>Assume I have some keywords with frequency of their usage:</p>

<pre><code>12 Hi
7  Hash
7  C++  
9  Superuser
17 Stackoverflow
9  LaTeX  
42 Life
9  Ubuntu
</code></pre>

<p>What I want, is to sort this data based on frequency in descending order and if there are some equal values, it should use the second column in ascending order.</p>

<pre><code>sort -n -r foo.txt
</code></pre>

<p>Does the first part but then second column are also <code>reversed</code>:</p>

<pre><code>42 Life
17 Stackoverflow
12 Hi
9  Ubuntu
9  Superuser
9  LaTeX  
7  Hash
7  C++
</code></pre>

<p>How can I achieve the following results?</p>

<pre><code>42 Life
17 Stackoverflow
12 Hi
9  LaTeX  
9  Superuser
9  Ubuntu
7  C++ 
7  Hash
</code></pre>

<p>I think I have to use <code>-k</code> argument but I can't figure out how! </p>

<p>I want to know how this can be done using solely <code>sort</code> command of <code>bash</code>. However if it's not possible to achieve this only by <code>sort</code>, other commands should be Bourne shell compatible.</p>
","<p>Specify the sort keys separately with the criteria:</p>

<pre><code>sort -k1,1nr -k2,2 inputfile
</code></pre>

<p>This specifies that the first key is sorted numerically in reverse order while the second is sorted as per the <em>default</em> sort order.</p>

<p>Quoting from <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/sort.html"">POSIX sort</a>:</p>

<blockquote>
  <p><strong>-k</strong>  <em>keydef</em></p>
  
  <p>The <em>keydef</em> argument is a restricted sort key field definition. The format of this definition is:</p>
  
  <p><em>field_start</em><strong>[</strong><em>type</em><strong>][</strong><em>,field_end</em><strong>[</strong><em>type</em><strong>]]</strong></p>
  
  <p>where <em>field_start</em> and <em>field_end</em> define a key field restricted to a portion of the line (see the EXTENDED DESCRIPTION section), and type
  is a modifier from the list of characters 'b', 'd', 'f', 'i', 'n',
  'r'. The 'b' modifier shall behave like the <code>-b</code> option, but shall apply
  only to the <em>field_start</em> or <em>field_end</em> to which it is attached. The
  other modifiers shall behave like the corresponding options, but shall
  apply only to the key field to which they are attached; they shall
  have this effect if specified with <em>field_start</em>, <em>field_end</em>, or both. If
  any modifier is attached to a <em>field_start</em> or to a <em>field_end</em>, no option
  shall apply to either. Implementations shall support at least nine
  occurrences of the <code>-k</code> option, which shall be significant in command
  line order. If no <code>-k</code> option is specified, a default sort key of the
  entire line shall be used.</p>
  
  <p>When there are multiple key fields, later keys shall be compared only after all earlier keys compare equal. Except when the <code>-u</code> option
  is specified, lines that otherwise compare equal shall be ordered as
  if none of the options <code>-d</code>, <code>-f</code>, <code>-i</code>, <code>-n</code>, or <code>-k</code> were present (but with <code>-r</code>
  still in effect, if it was specified) and with all bytes in the lines
  significant to the comparison. The order in which lines that still
  compare equal are written is unspecified.</p>
</blockquote>

<p>This would produce:</p>

<pre><code>42 Life
17 Stackoverflow
12 Hi
9  LaTeX
9  Superuser
9  Ubuntu
7  C++
7  Hash
</code></pre>
","122391"
"How do I append text to the beginning and end of multiple text files in Bash?","78936","","<p>I have a directory full of text files. My goal is to append text to the beginning and end of all of them. The text that goes at the beginning and end is the same for each file.</p>

<p>Based on code I got from the web, this is the code for appending to the beginning of the file:</p>

<pre><code>echo -e 'var language = {\n$(cat $BASEDIR/Translations/Javascript/*.txt)' &gt; $BASEDIR/Translations/Javascript/*.txt
</code></pre>

<p>This is the code for appending to the end of the file. The goal is to add the text <code>};</code> at the end of each file:</p>

<pre><code>echo ""};"" &gt;&gt; $BASEDIR/Translations/Javascript/*.txt
</code></pre>

<p>The examples I drew from were for acting on individual files. I thought I'd try acting on multiple files using the wildcard, <code>*.txt</code>.</p>

<p>I might be making other mistakes as well. In any case, how do I append text to the beginning and end of multiple files?</p>
","<p>To prepend text to a file you can use (with the GNU implementation of <code>sed</code>):</p>

<pre><code>sed -i '1i some string' file
</code></pre>

<p>Appending text is as simple as</p>

<pre><code>echo 'Some other string' &gt;&gt; file
</code></pre>

<p>The last thing to do is to put that into a loop which iterates over all the
files you intend to edit:</p>

<pre><code>for file in *.txt; do
  sed -i '1i Some string' ""$file"" &amp;&amp;
  echo 'Some other string' &gt;&gt; ""$file""
done
</code></pre>
","65514"
"How to grep the output of cURL?","78895","","<p>I need to retrieve the expiry date of an SSL cert. The <code>curl</code> application does provide this information:</p>

<pre><code>$ curl -v https://google.com/
* Hostname was NOT found in DNS cache
*   Trying 212.179.180.121...
* Connected to google.com (212.179.180.121) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
* SSLv3, TLS handshake, Server hello (2):
* SSLv3, TLS handshake, CERT (11):
* SSLv3, TLS handshake, Server key exchange (12):
* SSLv3, TLS handshake, Server finished (14):
* SSLv3, TLS handshake, Client key exchange (16):
* SSLv3, TLS change cipher, Client hello (1):
* SSLv3, TLS handshake, Finished (20):
* SSLv3, TLS change cipher, Client hello (1):
* SSLv3, TLS handshake, Finished (20):
* SSL connection using ECDHE-ECDSA-AES128-GCM-SHA256
* Server certificate:
*        subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=*.google.com
*        start date: 2014-10-22 13:04:07 GMT
*        expire date: 2015-01-20 00:00:00 GMT
*        subjectAltName: google.com matched
*        issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
*        SSL certificate verify ok.
&gt; GET / HTTP/1.1
&gt; User-Agent: curl/7.35.0
&gt; Host: google.com
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 302 Found
&lt; Cache-Control: private
&lt; Content-Type: text/html; charset=UTF-8
&lt; Location: https://www.google.co.il/?gfe_rd=cr&amp;ei=HkxbVMzCM-WkiAbU6YCoCg
&lt; Content-Length: 262
&lt; Date: Thu, 06 Nov 2014 10:23:26 GMT
* Server GFE/2.0 is not blacklisted
&lt; Server: GFE/2.0
&lt; 
&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv=""content-type"" content=""text/html;charset=utf-8""&gt;
&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;
&lt;H1&gt;302 Moved&lt;/H1&gt;
The document has moved
&lt;A HREF=""https://www.google.co.il/?gfe_rd=cr&amp;amp;ei=HkxbVMzCM-WkiAbU6YCoCg""&gt;here&lt;/A&gt;.
&lt;/BODY&gt;&lt;/HTML&gt;
* Connection #0 to host google.com left intact
</code></pre>

<p>However, when piping the output via <code>grep</code> the result is not <strong>less</strong> information on the screen, but rather <strong>much more</strong>:</p>

<pre><code>$ curl -v https://google.com/ | grep expire
* Hostname was NOT found in DNS cache
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 212.179.180.84...
* Connected to google.com (212.179.180.84) port 443 (#0)
* successfully set certificate verify locations:
*   CAfile: none
  CApath: /etc/ssl/certs
* SSLv3, TLS handshake, Client hello (1):
} [data not shown]
* SSLv3, TLS handshake, Server hello (2):
{ [data not shown]
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* SSLv3, TLS handshake, CERT (11):
{ [data not shown]
* SSLv3, TLS handshake, Server key exchange (12):
{ [data not shown]
* SSLv3, TLS handshake, Server finished (14):
{ [data not shown]
* SSLv3, TLS handshake, Client key exchange (16):
} [data not shown]
* SSLv3, TLS change cipher, Client hello (1):
} [data not shown]
* SSLv3, TLS handshake, Finished (20):
} [data not shown]
* SSLv3, TLS change cipher, Client hello (1):
{ [data not shown]
* SSLv3, TLS handshake, Finished (20):
{ [data not shown]
* SSL connection using ECDHE-ECDSA-AES128-GCM-SHA256
* Server certificate:
*        subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=*.google.com
*        start date: 2014-10-22 13:04:07 GMT
*        expire date: 2015-01-20 00:00:00 GMT
*        subjectAltName: google.com matched
*        issuer: C=US; O=Google Inc; CN=Google Internet Authority G2
*        SSL certificate verify ok.
&gt; GET / HTTP/1.1
&gt; User-Agent: curl/7.35.0
&gt; Host: google.com
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 302 Found
&lt; Cache-Control: private
&lt; Content-Type: text/html; charset=UTF-8
&lt; Location: https://www.google.co.il/?gfe_rd=cr&amp;ei=IkxbVMy4K4OBbKuDgKgF
&lt; Content-Length: 260
&lt; Date: Thu, 06 Nov 2014 10:23:30 GMT
* Server GFE/2.0 is not blacklisted
&lt; Server: GFE/2.0
&lt; 
{ [data not shown]
100   260  100   260    0     0    714      0 --:--:-- --:--:-- --:--:--   714
* Connection #0 to host google.com left intact
</code></pre>

<p>I <em>suspect</em> that <code>curl</code> detects that it is not printing to a terminal and is thus gives different output, not all of which is recognized by <code>grep</code> as being <code>stdout</code> and is thus passed through to the terminal. However, the closest thing to this that I could find in <code>man curl</code> <strong>(don't ever google for that!)</strong> is this:</p>

<pre><code>PROGRESS METER
   curl  normally  displays  a  progress meter during operations, indicating the amount of transferred data, transfer speeds and estimated time
   left, etc.

   curl displays this data to the terminal by default, so if you invoke curl to do an operation and it is about to write data to the  terminal,
   it disables the progress meter as otherwise it would mess up the output mixing progress meter and response data.

   If you want a progress meter for HTTP POST or PUT requests, you need to redirect the response output to a file, using shell redirect (&gt;), -o
   [file] or similar.

   It is not the same case for FTP upload as that operation does not spit out any response data to the terminal.

   If you prefer a progress ""bar"" instead of the regular meter, -# is your friend.
</code></pre>

<p><strong>How can I get just the <code>expiry</code> line out of the <code>curl</code> output?</strong> Furthermore, what should I be reading to understand the situation better?</p>
","<p>curl writes the output to stderr, so redirect that and also suppress the progress:</p>

<pre><code>curl -v --silent https://google.com/ 2&gt;&amp;1 | grep expire
</code></pre>

<p>The reason why <code>curl</code> writes the information to stderr is so you can do:<br>
<code>curl &lt;url&gt; | someprgram</code> without that information clobbering the input of <code>someprogram</code></p>
","166360"
"How to Determine the Amount of RAM Slots In Use?","78689","","<p>I forgot how many RAM modules are installed on my laptop. I do not want to unscrew it but want to look it up on the console using bash. How do I gather this information?</p>
","<p>Since you don't mention, I'm assuming this is on Linux.</p>

<pre><code>dmidecode -t memory
</code></pre>



<pre><code>dmidecode -t 16
</code></pre>



<pre><code>lshw -class memory
</code></pre>
","33252"
"OpenVPN: Push a route to client with a different gateway","78581","","<p>I would like my OpenVPN server to push a route down to the client with a different default gateway. </p>

<p>Specifically, my OpenVPN server has an internal IP address of <code>10.0.0.1</code>, and I would like it to push a route of <code>10.10.10.1/24</code> using gateway <code>10.0.0.2</code>.</p>

<p>Is it possible to do this by specifying a push route in the server config file?</p>
","<pre><code>push ""route 10.10.10.0 255.255.255.0 10.0.0.2 1""
</code></pre>

<p>From the OpenVPN man page:</p>

<pre><code>--route network/IP [netmask] [gateway] [metric]
</code></pre>

<p>This tells the server config to ""push"" to the client, the <code>route</code> command which sets a networking route of the 10.10.10.0/24 subnet via the gateway 10.0.0.2 with a metric of 1. Metrics are used to give ""preference"" if multiple routes exist (such that the lowest cost wins).</p>
","117775"
"why would curl and wget result in a 403 forbidden?","78547","","<p>I try to download a file with <code>wget</code> and <code>curl</code> and it is rejected with a 403 error (forbidden).</p>

<p>I can view the file using the web browser on the same machine.</p>

<p>I try again with my browser's user agent, obtained by <a href=""http://www.whatsmyuseragent.com"">http://www.whatsmyuseragent.com</a>. I do this:</p>

<pre><code>wget -U 'Mozilla/5.0 (X11; Linux x86_64; rv:30.0) Gecko/20100101 Firefox/30.0' http://...
</code></pre>

<p>and</p>

<pre><code>curl -A 'Mozilla/5.0 (X11; Linux x86_64; rv:30.0) Gecko/20100101 Firefox/30.0' http://...
</code></pre>

<p>but it is still forbidden. What other reasons might there be for the 403, and what ways can I alter the <code>wget</code> and <code>curl</code> commands to overcome them?</p>

<p>(this is not about being able to get the file - I know I can just save it from my browser; it's about understanding why the command-line tools work differently)</p>

<p><strong>update</strong></p>

<p>Thanks to all the excellent answers given to this question. The specific problem I had encountered was that the server was checking the referrer. By adding this to the command-line I could get the file using <code>curl</code> and <code>wget</code>.</p>

<p>The server that checked the referrer bounced through a 302 to another location that performed no checks at all, so a <code>curl</code> or <code>wget</code> of that site worked cleanly.</p>

<p>If anyone is interested, this came about because I was reading <a href=""http://css-tricks.com/forums/topic/font-face-in-base64-is-cross-browser-compatible"">this</a> page to learn about embedded CSS and was trying to look at the site's css for an example. The actual URL I was getting trouble with was <a href=""http://cloud.typography.com/610186/691184/css/fonts.css"">this</a> and the <code>curl</code> I ended up with is</p>

<pre><code>curl -L -H 'Referer: http://css-tricks.com/forums/topic/font-face-in-base64-is-cross-browser-compatible/' http://cloud.typography.com/610186/691184/css/fonts.css
</code></pre>

<p>and the wget is</p>

<pre><code> wget --referer='http://css-tricks.com/forums/topic/font-face-in-base64-is-cross-browser-compatible/' http://cloud.typography.com/610186/691184/css/fonts.css
</code></pre>

<p>Very interesting.</p>
","<p>A HTTP request may contain more headers that are not set by curl or wget. For example:</p>

<ul>
<li>Cookie: this is the most likely reason why a request would be rejected, I have seen this happen on download sites. Given a cookie <code>key=val</code>, you can set it with the <code>-b key=val</code> (or <code>--cookie key=val</code>) option for <code>curl</code>.</li>
<li>Referer (sic): when clicking a link on a web page, most browsers tend to send the current page as referrer. It should not be relied on, but even eBay failed to reset a password when this header was absent. So yes, it may happen. The <code>curl</code> option for this is <code>-e URL</code> and <code>--referer URL</code>.</li>
<li>Authorization: this is becoming less popular now due to the uncontrollable UI of the username/password dialog, but it is still possible. It can be set in <code>curl</code> with the <code>-u user:password</code> (or <code>--user user:password</code>) option.</li>
<li>User-Agent: some requests will yield different responses depending on the User Agent. This can be used in a good way (providing the real download rather than a list of mirrors) or in a bad way (reject user agents which do not start with <code>Mozilla</code>, or contain <code>Wget</code> or <code>curl</code>).</li>
</ul>

<p>You can normally use the Developer tools of your browser (Firefox and Chrome support this) to read the headers sent by your browser. If the connection is not encrypted (that is, not using HTTPS), then you can also use a packet sniffer such as Wireshark for this purpose.</p>

<p>Besides these headers, websites may also trigger some actions behind the scenes that change state. For example, when opening a page, it is possible that a request is performed on the background to prepare the download link. Or a redirect happens on the page. These actions typically make use of Javascript, but there may also be a hidden frame to facilitate these actions.</p>

<p>If you are looking for a method to easily fetch files from a download site, have a look at plowdown, included with <a href=""https://code.google.com/p/plowshare/"">plowshare</a>.</p>
","139704"
"Generate File of a certain size?","78501","","<p>I'd like to generate a file with the name <code>example.file</code>. I could use</p>

<pre><code>touch example.file
</code></pre>

<p>but I want the file to be exactly 24MB in size. I already checked the manpage of touch, but there is no parameter like this. Is there an easy way to generate files of a certain size?</p>
","<p>You can use dd:</p>

<pre><code>dd if=/dev/zero of=output.dat  bs=24M  count=1
</code></pre>

<p>or</p>

<pre><code>dd if=/dev/zero of=output.dat  bs=1M  count=24
</code></pre>
","101334"
"check if interface eth0 is up (configured)","78472","","<p>I need to check with a script, whether <code>eth0</code> is configured. If so, the script will do nothing. Otherwise it will start <code>wlan0</code>. (I don't want both <code>eth0</code> and <code>wlan0</code> to be up at the same time).</p>

<p>What would be the easiest way to check, whether <code>eth0</code> is already up?</p>

<p>I am using Debian Wheezy</p>

<h1>CLARIFICATION:</h1>

<p>I would like to check not only that the cable in <code>eth0</code> is plugged in, but rather that the interface is configured (i.e. it has either static IP set, or it has received a DHCP IP). If cable is plugged in, but <code>eth0</code> is not configured correctly, I want to start <code>wlan0</code></p>
","<p>You can do it many ways. Here an example:</p>

<pre><code>$ cat /sys/class/net/eth0/operstate
up
</code></pre>
","121526"
"Preserve the permissions with rsync","78453","","<p>Let's say I have a file a.txt in LINUX with permission of 0664.
When I use rsync to copy the file to my Mac with <code>rsync -r -t -v LINUX MAC</code>, the file's permission becomes 0644. </p>

<p>How can I keep the permission for a file when using rsync? The -g option doesn't work. </p>
","<p>You want the -p flag:</p>

<pre><code>    -p, --perms                 preserve permissions
</code></pre>

<p>I tend to always use the -a flag, which is an aggregation of -p and several other useful ones:</p>

<pre><code>    -a, --archive               archive mode; equals -rlptgoD (no -H,-A,-X)
</code></pre>

<p>Both taken straight from <a href=""http://ss64.com/bash/rsync.html"">the rsync manpage</a>.</p>
","12199"
"./executable: cannot execute binary file","78397","","<p>I have a script that works well when I ssh to the server to execute it myself, but has problems when <a href=""http://hudson-ci.org"">Hudson</a>, a continuous integration server, runs it.</p>

<p>I am automating tests on an embedded linux system (the target). The target is connected to Server A (RHEL 5) via serial and operated over minicom. Server B (FC 12) builds the tests that actually run on the target, and can ssh to Server A. Server C (RH) hosts Hudson, with Server B as a slave. </p>

<p>I've written a runscript (http://linux.die.net/man/1/runscript) script to do everything needed on the actual target; it boots the image, mounts a directory from Server B and executes the tests. A bash script on Server B invokes minicom with the runscript script along with some companion actions. I have a bash script on Server B which uses </p>

<pre><code>ssh -t -t ServerA bashScript.sh
</code></pre>

<p>to get those tests run on the target. I am on Server C, I can get those tests run by ssh'ing to Server B and executing the script that ssh's to Server A which executes minicom with runscript. <em>Whew.</em>  To review:</p>

<p>Server A: Hudson uses its slave mechanism to ssh to Server B.</p>

<p>Server B: <code>kickOffTests.sh</code> has the line <code>ssh -t -t ServerA runTests.sh</code></p>

<p>Server A: <code>runTests.sh</code> calls a perl script which invokes <code>minicom -S my.script ttyE1</code></p>

<p>Target, after booting: Mounts a directory from Server B, where the tests are, and enters that directory. It invokes yet another bash script, which runs the tests, which are compiled C executables.</p>

<p>Now, when <em>I</em> execute any of these scripts myself, they do what they should. However, when Hudson tries to do the same thing, over in the minicom session it complains about a line in the ""yet another bash script"" that invokes the C executable, <code>./executable</code>, with <code>./executable: cannot execute binary file</code></p>

<p>I still have a lot to learn about linux, but I surmise this problem is a result of Hudson not connecting with a console. I don't know exactly what Hudson does to control its slave. I tried using the line <code>export TERM=console</code> in the configuration just before running kickOffTests.sh, but the problem remains.</p>

<p>Can anyone explain to me what is happening and how I can fix it? I cannot remove any of the servers from this equation. It may be possible to take minicom out of the equation but that would add an unknown amount of time to this project, so I'd much prefer a solution that uses what I already have.</p>
","<p>The message <code>cannot execute binary file</code> has nothing to do with terminals (I wonder what led you to think that — and I recommend avoiding making such assumptions in a question, as they tend to drown your actual problem in a mess of red herrings). In fact, it's bash's way of expressing <code>ENOEXEC</code> (more commonly expressed as <code>exec format error</code>.</p>

<p>First, make sure you didn't accidentally try to run this executable as a script. If you wrote <code>. ./executable</code>, this tells bash to execute <code>./executable</code> in the same environment as the calling script (as opposed to a separate process). That can't be done if the file is not a script.</p>

<p>Otherwise, this message means that <code>./executable</code> is not in a format that the kernel recognizes. I don't have any definite guess as to what is happening though. If you can run the script on that same machine by invoking it in a different way, it can't just be a corrupt file or a file for the wrong architecture (it might be that, but there's more to it). I wonder if there could be a difference in the way the target boots (perhaps a race condition).</p>

<p>Here's a list of additional data that may help:</p>

<ul>
<li>Output of <code>file …/executable</code> on server B.</li>
<li>Some information about the target, such as the output of <code>uname -a</code> if it's unix-like.</li>
<li>Check that the target sees the same file contents each time: run <code>cksum ./executable</code> or <code>md5sum ./executable</code> or whatever method you have on the target just before yet-another-bash-script invokes <code>./executable</code>. Check that the results are the same in the Hudson invocation, in your successful manual invocation and on server B.</li>
<li>Add <code>set -x</code> at the top of yet-another-bash-script (just below the <code>#!/bin/bash</code> line). This will produce a trace of everything the script does. Compare the traces and report any difference or oddity.</li>
<li>Describe how the target is booting when you run the scripts manually and when Hudson is involved. It could be that the target is booted differently and some loadable module that provides the support for the format of <code>./executable</code> doesn't get loaded (or is not loaded yet) in the Hudson invocations. You might want to use <code>set -x</code> in other scripts to help you there, and inspect the boot logs from the target.</li>
</ul>
","3581"
"Static IPv4 & IPv6 configuration on CentOS 6.2","78172","","<p>I try to configure static IPv4 &amp; IPv6 configuration on CentOS 6.2.</p>

<p>The configuration below works perfectly :</p>

<pre><code># ifconfig eth0 x.x.x.x/29
# route add defalt gw x.x.x.y

# ip addr add dev eth0 XXXX:C810:3001:D00::3/56
# ip -6 route add default XXXX:C810:3001:D00::1
</code></pre>

<p>However, I want to keep this configuration after a reboot.</p>

<p>So I made the following configuration:</p>

<p>Enabling IPv6</p>

<pre><code>[root@test network-scripts]# cat /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=test.net
NETWORKING_IPV6=yes
</code></pre>

<p>Interface Configuration</p>

<pre><code>[root@test network-scripts]# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
DEVICE=""eth0""
BOOTPROTO=""static""
ONBOOT=""yes""
HWADDR=""2C:C3:AC:A8:C3:3E""
IPADDR=x.x.x.x
GATEWAY=x.x.x.x
NETMASK=255.255.255.248
TYPE=Ethernet
IPV6INIT=yes
IPV6ADDR=XXXX:C810:3001:D00::3/56
IPV6_DEFAULTGW=XXXX:C810:3001:D00::1

DNS1=208.67.222.222
DNS2=208.67.220.220
# Only DNS{1,2} according to /usr/share/doc/initscripts-9.03.27/sysconfig.txt
# DNS3=2620:0:ccc::2
# DNS4=2620:0:ccD::2
</code></pre>

<p>Restarting the Network</p>

<pre><code>[root@test network-scripts]# service network restart
Arrêt de l'interface eth0 :  État du périphérique&amp;nbsp;: 3 (déconnecté)
                                                           [  OK  ]
Arrêt de l'interface loopback :                            [  OK  ]
Activation de l'interface loopback :                       [  OK  ]
Activation de l'interface eth0 :  État de connexion active&amp;nbsp;: activation
État de chemin actif&amp;nbsp;: /org/freedesktop/NetworkManager/ActiveConnection/3
état&amp;nbsp;: activé
Connexion activée
                                                           [  OK  ]
</code></pre>

<p>[root@test network-scripts]# cat /var/log/message</p>

<pre><code>Mar 13 14:32:13 test NetworkManager[8299]: &lt;info&gt; (eth0): device state change: 8 -&gt; 3 (reason 39)
Mar 13 14:32:13 test NetworkManager[8299]: &lt;info&gt; (eth0): deactivating device (reason: 39).
Mar 13 14:32:13 test avahi-daemon[8311]: Withdrawing address record for x.x.x.x on eth0.
Mar 13 14:32:13 test avahi-daemon[8311]: Leaving mDNS multicast group on interface eth0.IPv4 with address x.x.x.x.
Mar 13 14:32:13 test avahi-daemon[8311]: Interface eth0.IPv4 no longer relevant for mDNS.
Mar 13 14:32:14 test kernel: lo: Disabled Privacy Extensions
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) starting connection 'System eth0'
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; (eth0): device state change: 3 -&gt; 4 (reason 0)
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 1 of 5 (Device Prepare) scheduled...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 1 of 5 (Device Prepare) started...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 2 of 5 (Device Configure) scheduled...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 1 of 5 (Device Prepare) complete.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 2 of 5 (Device Configure) starting...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; (eth0): device state change: 4 -&gt; 5 (reason 0)
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 2 of 5 (Device Configure) successful.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 3 of 5 (IP Configure Start) scheduled.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 2 of 5 (Device Configure) complete.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 3 of 5 (IP Configure Start) started...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; (eth0): device state change: 5 -&gt; 7 (reason 0)
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP4 Configure Get) scheduled...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Beginning IP6 addrconf.
Mar 13 14:32:14 test avahi-daemon[8311]: Withdrawing address record for fe80::1ec1:deff:feb8:a2fd on eth0.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 3 of 5 (IP Configure Start) complete.
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP4 Configure Get) started...
Mar 13 14:32:14 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP4 Configure Get) complete.
Mar 13 14:32:15 test avahi-daemon[8311]: Registering new address record for fe80::1ec1:deff:feb8:a2fd on eth0.*.
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; (eth0): IP6 addrconf timed out or failed.
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP6 Configure Timeout) scheduled...
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP6 Configure Timeout) started...
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 5 of 5 (IP Configure Commit) scheduled...
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 4 of 5 (IP6 Configure Timeout) complete.
Mar 13 14:32:35 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 5 of 5 (IP Configure Commit) started...
Mar 13 14:32:35 test avahi-daemon[8311]: Joining mDNS multicast group on interface eth0.IPv4 with address x.x.x.x.
Mar 13 14:32:35 test avahi-daemon[8311]: New relevant interface eth0.IPv4 for mDNS.
Mar 13 14:32:35 test avahi-daemon[8311]: Registering new address record for x.x.x.x on eth0.IPv4.
Mar 13 14:32:36 test NetworkManager[8299]: &lt;info&gt; (eth0): device state change: 7 -&gt; 8 (reason 0)
Mar 13 14:32:36 test NetworkManager[8299]: &lt;info&gt; Policy set 'System eth0' (eth0) as default for IPv4 routing and DNS.
Mar 13 14:32:36 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) successful, device activated.
Mar 13 14:32:36 test NetworkManager[8299]: &lt;info&gt; Activation (eth0) Stage 5 of 5 (IP Configure Commit) complete.
</code></pre>

<p>IPv6 configuration is not working ...</p>

<pre><code>[root@test network-scripts]# ip addr show
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 1c:c1:de:b8:a3:fd brd ff:ff:ff:ff:ff:ff
    inet x.x.x.x/29 brd x.x.x.x scope global eth0
    inet6 fe80::1ec1:deff:feb8:a3fd/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre>

<p>IPv6 addresses of the resolvers are not even in the resolv.conf !</p>

<p><strong>Did I miss a configuration step ?</strong> </p>

<p>I thought that the IPv6 configuration would be a formality ..</p>

<pre><code>[root@test network-scripts]# lsb_release -a
LSB Version:    :core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:    CentOS
Description:    CentOS release 6.2 (Final)
Release:    6.2
Codename:    Final
</code></pre>
","<p>Network Manager is trying to override your static configuration settings. As root or sudo user, run:</p>

<pre><code>service NetworkManager stop
</code></pre>

<p>If you don't have service, try:</p>

<pre><code>/etc/init.d/NetworkManager stop
</code></pre>

<p>Also, you can set the static interfaces to not be managed by the NetworkManager, which is what I did in my CentOS configs merely by adding the line</p>

<pre><code>NM_CONTROLLED=no
</code></pre>

<p>to your static config files. Your static configuration files don't have that line, meaning the NetworkManager will try to control those interfaces instead of ignoring them. </p>

<p>See <a href=""http://www.linuxquestions.org/questions/linux-networking-3/disable-networkmanager-598907/"">here for reference</a> on disabling and/or uninstalling NM. </p>
","34151"
"How to avoid the need to issue ""y"" several times when removing protected file","78069","","<p>I'm looking for a solution to be used as 
a response to ""rm: remove write-protected regular file [x] ?""</p>

<p>I was thinking of issuing a character followed by carriage return for several amount of times, in bashrc. How do we do that?</p>
","<p><strong>Edit based on updated question:</strong></p>

<p>To avoid being asked about removing files, add the <code>-f</code> (""force"") option:</p>

<pre><code>rm -f /path/to/file
</code></pre>

<p>This has one side effect you should be aware of: If any of the given paths do not exist, it will <em>not</em> report this, and it will return successfully:</p>

<pre><code>$ rm -f /nonexistent/path
$ echo $?
0
</code></pre>

<hr>

<p>Original answer:</p>

<p>Here's one simple solution:</p>

<pre><code>yes ""$string"" | head -n $number | tr $'\n' $'\r'
</code></pre>

<p><code>yes</code> repeats any string you give it infinitely, separated by newlines. <code>head</code> stops it after <code>$number</code> times, and <code>tr</code> translates the newlines to carriage returns. You might not see any output because of the carriage returns, but passing it to this command (in <code>bash</code>) should illustrate it:</p>

<pre><code>printf %q ""$(yes ""$string"" | head -n $number | tr $'\n' $'\r')""
</code></pre>

<p>Users without <code>bash</code> can pipe the result to <code>od</code>, <code>hexdump</code> or <code>xxd</code> to see the actual characters returned.</p>
","72879"
"Only return the matched string in sed","77965","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/31476/extracting-a-regex-matched-with-sed-without-printing-the-surrounding-character"">Extracting a regex matched with &#39;sed&#39; without printing the surrounding characters</a>  </p>
</blockquote>



<p>How do I make this only print <code>test</code>:</p>

<pre><code>echo ""atestb"" | sed -n 's/\(test\)/\1/p'
</code></pre>
","<p>You need to match the whole line:</p>

<pre><code>echo ""atestb"" | sed -n 's/.*\(test\).*/\1/p'
</code></pre>

<p>or</p>

<pre><code>echo ""atestb"" | sed 's/.*\(test\).*/\1/'
</code></pre>
","43134"
"How to add a carriage return before every newline?","77714","","<p>I have a file that only uses <code>\n</code> for new lines, but I need it to have <code>\r\n</code> for each new line. How can I do this? </p>

<p>For example, I solved it in Vim using <code>:%s/\n/\r\n/g</code>, but I would like to use a script or command-line application. Any suggestions? </p>

<p>I tried looking this up with <code>sed</code> or <code>grep</code>, but I got immediately confused by the escape sequence workarounds (I am a bit green with these commands).</p>

<p>If interested, the application is related to my question/answer <a href=""https://stackoverflow.com/questions/25596538/debugging-mercurial-patch-import-from-git-repo-hg-import-is-failing-but-the"">here</a></p>
","<p>You can use <a href=""http://waterlan.home.xs4all.nl/dos2unix.html"">unix2dos</a> (which found on Debian):</p>

<pre><code>unix2dos file
</code></pre>

<p>Note that this implementation won't insert a <code>CR</code> before every <code>LF</code>, only before those <code>LF</code>s that are not already preceded by one (and only one) <code>CR</code> and will skip binary files (those that contain byte values in the 0x0 -> 0x1f range other than <code>LF</code>, <code>FF</code>, <code>TAB</code> or <code>CR</code>).</p>

<p>or use <code>sed</code>:</p>

<pre><code>CR=$(printf '\r')
sed ""s/\$/$CR/"" file
</code></pre>

<p>or use <code>awk</code>:</p>

<pre><code>awk '{printf ""%s\r\n"", $0}' file
</code></pre>

<p>or use <code>perl</code>:</p>

<pre><code>perl -pe 's|\n|\r\n|' file
</code></pre>
","153096"
"How can I tell whether a package is installed via yum in a bash script?","77629","","<p>I am trying to write a script that installs packages, but if it fails at any point later in the script rolls back whatever it installed. Of course if the user has already previously installed a package I don't want to uninstall it out from under them. How can my script tell whether a package has been previously installed via yum?</p>
","<p>I found the following on a semi-related <a href=""https://stackoverflow.com/questions/8447011/how-to-ensure-yum-install-was-successful-in-a-shell-script"">StackOverflow question</a>; the answer I needed didn't actually quite answer the question there (and was not selected as the correct answer) so I figured I'd post it here for others to find easier.</p>

<p><code>yum list installed PACKAGE_NAME</code></p>

<p>This command returns some human-readable output, but more importantly returns an exit status code; 0 indicates the package <em>is</em> installed, 1 indicates the package is <em>not</em> installed (does not check whether the package is valid, so <code>yum list installed herpderp-beepbopboop</code> will return a ""1"" just as <code>yum list installed traceroute</code> will if you don't have traceroute installed). You can subsequently check ""$?"" for this exit code. </p>

<p>Since the output is somewhat counter-intuitive, I used @Chris Downs' ""condensed"" version below in a wrapper function to make the output more ""logical"" (i.e. 1=installed 0=not installed):</p>

<pre><code>function isinstalled {
  if yum list installed ""$@"" &gt;/dev/null 2&gt;&amp;1; then
    true
  else
    false
  fi
}
</code></pre>

<p>usage would be</p>

<p><code>if isinstalled $package; then echo ""installed""; else echo ""not installed""; fi</code></p>

<h3>EDIT:</h3>

<p>Replaced <code>return</code> statements with calls to <code>true</code> and <code>false</code> which help make the function more readable/intuitive, while returning the values bash expects (i.e. 0 for true, 1 for false).</p>

<p>If you're just checking for one package in your script, you may just be better off testing <code>yum list installed</code> directly, but (IMHO) the function makes it easier to understand what's going on, and its syntax is much easier to remember than <code>yum</code> with all the redirects to supress its output.</p>
","122682"
"How to get my own IP address and save it to a variable in a shell script?","77588","","<p>How can I get my own IP address and save it to a variable in a shell script?</p>
","<p>It's not so easy if you want to take into account wlan and other alternative interfaces. If you know which interface you want the address for (e.g., eth0, the first Ethernet card), you can use this:</p>

<pre><code>ip=""$(ifconfig | grep -A 1 'eth0' | tail -1 | cut -d ':' -f 2 | cut -d ' ' -f 1)""
</code></pre>

<p>In other words, get me the network configuration information, look for <code>eth0</code>, get that line and the next one (<code>-A 1</code>), get <em>only</em> the last line, get the second part of that line when splitting with <code>:</code>, then get the first part of that when splitting with space.</p>
","8519"
"Change top's sorting back to CPU","77456","","<p>A former coworker did something to <code>top</code> that whenever it runs as root the data is sorted by MEM usage instead of the default CPU usage. According to multiple searches, the man page and even the options within the top console itself (O), just pressing <code>k</code> it should be sorted by CPU, but instead when I hit <code>k</code> it asks me for a pid to kill.</p>

<p>So how can I get back default sorting to CPU?  </p>
","<p>You can change the sort field in the interactive <code>top</code> window with the <kbd>&lt;</kbd> and <kbd>&gt;</kbd> keys. I'm not sure what operating system you're running but at least on my GNU top, <kbd>k</kbd> is <em>supposed</em> to kill, not reset.</p>

<p>Presumably, your friend changed the sort field and hit <kbd>Shift</kbd>+<kbd>W</kbd>  to save to <code>~/.toprc</code>. Just use the keys I mentioned to choose the sort field you want and then, when it's set up as you like it, hit <kbd>Shift</kbd>+<kbd>W</kbd> again and it should save that state and open that way next time.</p>
","158587"
"Press space to continue","77440","","<p>How do I stop a bash script until a user has pressed <kbd>Space</kbd>?</p>

<p>I would like to have the question in my script</p>

<blockquote>
  <p>Press space to continue or CTRL+C to exit</p>
</blockquote>

<p>and then the script should stop and wait until Space is pressed.</p>
","<p>You can use <code>read</code>:</p>

<pre><code>read -n1 -r -p ""Press space to continue..."" key

if [ ""$key"" = '' ]; then
    # Space pressed, do something
    # echo [$key] is empty when SPACE is pressed # uncomment to trace
else
    # Anything else pressed, do whatever else.
    # echo [$key] not empty
fi
</code></pre>
","134444"
"Find out current working directory of a running process?","77182","","<p>What command(s) can one use to find out the current working directory (CWD) of a running process? These would be commands you could use externally from the process.</p>
","<p>There are 3 methods that I'm aware of:</p>

<h3>pwdx</h3>

<pre><code>$ pwdx &lt;PID&gt;
</code></pre>

<h3>lsof</h3>

<pre><code>$ lsof -p &lt;PID&gt; | grep cwd
</code></pre>

<h3>/proc</h3>

<pre><code>$ readlink -e /proc/&lt;PID&gt;/cwd
</code></pre>

<h3>Examples</h3>

<p>Say we have this process.</p>

<pre><code>$ pgrep nautilus
12136
</code></pre>

<p>Then if we use <code>pwdx</code>:</p>

<pre><code>$ pwdx 12136
12136: /home/saml
</code></pre>

<p>Or you can use <code>lsof</code>:</p>

<pre><code>$ lsof -p 12136 | grep cwd
nautilus 12136 saml  cwd    DIR              253,2    32768  10354689 /home/saml
</code></pre>

<p>Or you can poke directly into the <code>/proc</code>:</p>

<pre><code>$ readlink -e /proc/12136/cwd/
/home/saml
</code></pre>
","94359"
"Moving tmux pane to window","77105","","<p>How do I move an existing pane into another window in <strong>tmux</strong> when I have multiple windows, and vice versa?</p>

<p>I'm coming from <strong>screen</strong>, where I can switch to the pane and then switch windows until I get to the one I want; <strong>tmux</strong> does not seem to allow this.</p>
","<p>The command to do this is <a href=""http://man.openbsd.org/tmux.1"" rel=""noreferrer""><code>join-pane</code></a> in tmux 1.4.</p>

<blockquote>
<pre><code>join-pane [-dhv] [-l size | -p percentage] [-s src-pane] [-t dst-pane]  
    (alias: joinp)
    Like split-window, but instead of splitting dst-pane and creating
    a new pane, split it and move src-pane into the space.  This can
    be used to reverse break-pane.
</code></pre>
</blockquote>

<p>To simplify this, I have these binds in my <code>.tmux.conf</code> for that:</p>

<pre><code># pane movement
bind-key j command-prompt -p ""join pane from:""  ""join-pane -s '%%'""
bind-key s command-prompt -p ""send pane to:""  ""join-pane -t '%%'""
</code></pre>

<p>The first grabs the pane from the target window and joins it to the current, the second does the reverse.</p>

<p>You can then reload your tmux session by running the following from within the session:</p>

<pre><code>$ tmux source-file ~/.tmux.conf
</code></pre>
","14301"
"Replacing string based on line number","77089","","<p>I have a situation where i want to replace a particular string in many files</p>

<p>Replace a string <strong>AAA</strong> with another string <strong>BBB</strong> but there are lot of strings starting with AAA or ending in AAA ,and i want to replace only one on line 34 and keep others intact.</p>

<p>Is it possible to specify by line number,on all files this string is exactly on 34th line.</p>
","<p>You can specify line number in sed or NR (number of record) in awk.</p>

<pre><code>awk 'NR==34 { sub(""AAA"", ""BBB"") }'
</code></pre>

<p>or use FNR (file number record) if you want to specify more than one file on the command line.</p>

<pre><code>awk 'FNR==34 { sub(""AAA"", ""BBB"") }'
</code></pre>

<p>or</p>

<pre><code>sed '34s/AAA/BBB/'
</code></pre>

<p>to do in-place replacement with sed</p>

<pre><code>sed -i '34s/AAA/BBB/' file_name
</code></pre>
","70879"
"How to copy some, but not all files?","76822","","<p>So, you can use the <code>*</code> as a wild card for all files when using <code>cp</code> within context of a directory.  Is there a way to copy all files except <code>x</code> file?</p>
","<p>In <code>bash</code> you can use <code>extglob</code>:</p>

<pre><code> $ shopt -s extglob  # to enable extglob
 $ cp !(b*) new_dir/
</code></pre>

<p>where <code>!(b*)</code> exclude all <code>b*</code> files.</p>

<p>You can later disable <code>extglob</code> with</p>

<pre><code> $ shopt -u extglob
</code></pre>
","41696"
"How to copy-merge two directories?","76753","","<p>I have two directories images and images2 with this structure in Linux:</p>

<pre><code>/images/ad  
/images/fe  
/images/foo  
</code></pre>

<p>... and other 4000 folders</p>

<p>and the other is like:  </p>

<pre><code>/images2/ad  
/images2/fe  
/images2/foo
</code></pre>

<p>... and other 4000 folders</p>

<p>Each of these folders contain images and the directories' names under images and images2 are exactly the same, however their content is different. Then I want to know how I can copy-merge the images of /images2/ad into images/ad, the images of /images2/foo into images/foo and so on with all the 4000 folders..</p>
","<p>This is a job for <a href=""http://en.wikipedia.org/wiki/Rsync"" rel=""noreferrer"">rsync</a>. There's no benefit to doing this manually with a shell loop unless you want to move the file rather than copy them.</p>

<pre><code>rsync -a /images/ /images2/
</code></pre>

<p>(Note trailing slash on <code>images</code>, otherwise it would copy to <code>/images2/images</code>.)</p>

<p>If images with the same name exist in both directories, the command above will overwrite <code>/images2/SOMEPATH/SOMEFILE</code> with <code>/images/SOMEPATH/SOMEFILE</code>. If you want to replace only older files, add the option <code>-u</code>. If you want to always keep the version in <code>/images2</code>, add the option <code>--ignore-existing</code>.</p>

<p>If you want to move the files from <code>/images</code>, with rsync, you can pass the option <code>--remove-source-files</code>. Then rsync copies all the files in turn, and removes each file when it's done. This is a lot slower than moving if the source and destination directories are on the same filesystem.</p>
","149986"
"How to download a file through an SSH server?","76722","","<p>I have a server in USA (Linux box B), and my home PC (Linux box A),
and I need download a file from website C,</p>

<p>The issue is, it is very slow to download a file direct from A,
so I need download the file when I log in B, and <code>sftp</code> get the file from A.</p>

<p>Is there any way that I can download file and use B as proxy directly through only one line command?</p>
","<p>(Strange situation, doesn't something like the <a href=""http://en.wikipedia.org/wiki/Triangle_inequality"" rel=""noreferrer"">triangle inequality</a> hold for internet routing?)</p>

<p>Anyway, try the following, on <strong>A</strong>, <code>ssh</code> into <strong>B</strong> with a <code>-D</code> argument,</p>

<pre><code>ssh -D 1080 address-of-B
</code></pre>

<p>which acts as a SOCKS5 proxy on <code>127.0.0.1:1080</code>, which can be used by anything supporting SOCKS5 proxied connections.  <a href=""https://superuser.com/a/262972/102592"">Apparently, <code>wget</code> can do this</a>, by using the environment variable</p>

<pre><code>export SOCKS_SERVER=127.0.0.1:1080
wget http://server-C/whatever
</code></pre>

<p>Note that sometimes <code>curl</code> is more handy (i.e. I'm not sure if <code>wget</code> can do hostname lookups via SOCKS5; but this is not one of your concerns I suppose); also Firefox is able to work completely through such a SOCKS5 proxy.</p>

<p><strong>Edit</strong> I've just now noticed that you're looking for a <strong>one-line</strong> solution.  Well, how about </p>

<pre><code>ssh address-of-B 'wget -O - http://server-C/whatever' &gt;&gt; whatever
</code></pre>

<p>i.e. redirection the <code>wget</code>-fetched output to <code>stdout</code>, and redirecting the local output (from <code>ssh</code> running <code>wget</code> remotely) to a file.</p>

<p>This seems to work, the <code>wget</code> output is just a little confusing (""<em>saved to -</em>""), you can get rid of it by adding <code>-q</code> to the <code>wget</code> call.</p>
","38761"
"How to pass the output of one command as the command-line argument to another?","76709","","<p>So I have a script that, when I give it two addresses, will search two HTML links:</p>

<pre><code>echo ""http://maps.google.be/maps?saddr\=$1\&amp;daddr\=$2"" | sed 's/ /%/g'
</code></pre>

<p>I want to send this to <code>wget</code> and then save the output in a file called <code>temp.html</code>. I tried this, but it doesn't work. Can someone explain why and/or give me a solution please?</p>

<pre><code>#!/bin/bash
url = echo ""http://maps.google.be/maps?saddr\=$1\&amp;daddr\=$2"" |  sed 's/ /%/g'
wget $url
</code></pre>
","<p>You can use backticks (`) to evaluate a command and substitute in the command's output, like:</p>

<pre><code>echo ""Number of files in this directory: `ls | wc -l`""
</code></pre>

<p>In your case:</p>

<pre><code>wget `echo http://maps.google.be/maps?saddr\=$1\&amp;daddr\=$2 | sed 's/ /%/g'`
</code></pre>
","4797"
"SSH inside SSH fails with ""stdin: is not a tty""","76694","","<p>I'm trying to connect to machine one with ssh and then connect to another machine two with ssh, but I get this error.</p>

<pre><code>ssh user@computerone.com 'ssh otheruser@computertwo.com'

stdin: is not a tty
</code></pre>

<p>Why?</p>
","<p>By default, when you run a command on the remote machine using ssh, a TTY is not allocated for the remote session.  This lets you transfer binary data, etc. without having to deal with TTY quirks.  This is the environment provided for the command executed on <code>computerone</code>.</p>

<p>However, when you run ssh without a remote command, it DOES allocate a TTY, because you are likely to be running a shell session.  This is expected by the <code>ssh otheruser@computertwo.com</code> command, but because of the previous explanation, there is no TTY available to that command.</p>

<p>If you want a shell on <code>computertwo</code>, use this instead, which will force TTY allocation during remote execution:</p>

<pre><code>ssh -t user@computerone.com 'ssh otheruser@computertwo.com'
</code></pre>

<p>This is typically appropriate when you are eventually running a shell or other interactive process at the end of the ssh chain.  If you were going to transfer data, it is neither appropriate nor required to add <code>-t</code>, but then every ssh command would contain a data-producing or -consuming command, like:</p>

<pre><code>ssh user@computerone.com 'ssh otheruser@computertwo.com ""cat /boot/vmlinuz""'
</code></pre>
","48530"
"How to find free disk space and analyze disk usage?","76667","","<p>In CentOS and Ubuntu, how do I find out how much free disk space I have left and other disk stats like disk usage?</p>
","<p>I covered this pretty extensively in a blog post titled: <a href=""http://www.lamolabs.org/blog/5845/command-line-tools-for-analyzing-disk-usage-on-fedoracentosrhel/#more-5845"" rel=""noreferrer"">Command Line Tools for Analyzing Disk Usage on Fedora/CentOS/RHEL</a>. </p>

<h3><a href=""http://dev.yorhel.nl/ncdu"" rel=""noreferrer"">ncdu</a></h3>

<p>It’s ncurses based, feature rich and has a nice clean interface and it works from within a shell.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/esvsq.png"" alt=""ss of ncdu""></p>

<h3><a href=""http://gt5.sourceforge.net/"" rel=""noreferrer"">gt5</a></h3>

<ul>
<li>display diskspace used by files &amp; directories within a directory</li>
<li>display what’s happened since the last ran (see screenshots below)</li>
<li>optionally provides links to the files, so you can also browse them</li>
<li>displays entries with their size &amp; the percentage of their parent</li>
<li>ommits small files/directories</li>
<li>easy browsing using the cursor-keys</li>
<li>produces html files for browsing ‘offline’ afterwards</li>
</ul>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/Fj0I0.png"" alt=""ss of gt5""></p>

<h3><a href=""http://www.marzocca.net/linux/baobab/"" rel=""noreferrer"">Disk Usage Analyzer (aka. Baobab)</a></h3>

<ul>
<li>Single folder scan</li>
<li>Remote scan</li>
<li>Monitoring of Home</li>
<li>Display Data in Treemaps or as Ringschart</li>
</ul>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/Zylyb.png"" alt=""ss of baobab""></p>

<h3>others...</h3>

<ul>
<li><a href=""http://xdiskusage.sourceforge.net/"" rel=""noreferrer"">xdiskusage</a></li>
<li><a href=""http://www.methylblue.com/filelight/"" rel=""noreferrer"">filelight</a></li>
<li><a href=""http://lxr.kde.org/source/kde/kde-baseapps/konq-plugins/fsview/README"" rel=""noreferrer"">fsview</a></li>
</ul>

<p>In particular fsview is a very nice GUI. I like how it organizes the disk usage visually. It’s actually a KDE application (a plugin to Konqueror) but runs just fine under GNOME. It’s typically part of a package called kdeaddons, and shows up in the Applications menu as “File System Viewer” under Accessories.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/0qmzn.png"" alt=""ss of fsview""></p>
","73834"
"How to install latest NodeJS on Debian Jessie?","76593","","<p>I just installed NodeJS &amp; NPM on Debian Jessie using the recommended approach:</p>

<pre><code>apt-get install curl
curl -sL https://deb.nodesource.com/setup | bash -
apt-get install -y nodejs
</code></pre>

<p>However it’s a pretty old version (node v0.10.38 &amp; npm 1.4.28).</p>

<p>Any suggestions on the easiest way to install newer versions, e.g., currently node is v0.12.4 and npm is 2.7.4? Is installing from source my only approach?</p>
","<p>There is a setup script available for Node.js (see <a href=""https://github.com/nodesource/distributions#installation-instructions"" rel=""noreferrer"">installation insctructions</a>):</p>

<pre><code># Adapt version number to the version you want
curl -sL https://deb.nodesource.com/setup_0.12 | sudo bash -
sudo apt-get install -y nodejs
</code></pre>

<p>A little comment: In my humble opinion, it's a <em>very</em> bad idea to <code>curl | sudo bash</code>. You are running a script you did not check with root privileges. It's always better to download the script, read through it, check for malicious commands, and after <strong>that</strong>, run it. But that's just my two cents.</p>

<p>The installation can be achieved manually in a few steps following the <a href=""https://github.com/nodesource/distributions#debmanual"" rel=""noreferrer"">manual installation procedure</a>:</p>

<ul>
<li>Remove old PPA (if applicable)</li>
<li>Add node repo ssh key</li>
<li>Add node repo to <code>sources.list</code></li>
<li>update package list and install using favorite apt tool</li>
</ul>
","207594"
"How to check if NTPD updates machine's time successfully using shell?","76569","","<p>I'm trying to use NTPD to update my Linux machine's time to a specified NTP server.<br>
Here is the scenario:  </p>

<p>Each time the Linux machine starts up, I want to update the time from NTP server and <strong>if it's not successful, I want to try again every 5 minutes until successfully</strong> (max is 2 hours).</p>

<p>I searched around and find that I should(?) use NTPD and use some command like:  </p>

<p><code>#ntpdate  ntp.server.com</code> (before starting NTPD)<br>
<code>#ntpd some_options_to_start</code></p>

<p>The questions are:</p>

<ol>
<li>How can I know if the time was successfully updated by these commands?</li>
<li>Can I set the interval to update time from ntpd? (or I have to use something like <code>sleep</code> and loop with <code>do</code>..<code>while</code>/<code>for</code> in shell?)</li>
</ol>

<p>Note that I want to execute the above commands in a shell script and will put the shell in  a web server. Then clients (with a web browser browser) will execute the script on the website.  So I need to check if the update is successful or not to send result to the client (over the web).</p>
","<p>Using a script to monitor <code>ntpd</code> is not commonly done.  Usually a monitoring tool like <code>nagios</code> or <code>munin</code> is used to monitor the daemon.  The tool can send you an alert when things go wrong.  I have <code>munin</code> emailing me if the offset exceeds 15 milliseconds. </p>

<p>Normally, you should use an odd number of servers so that the daemon can perform an election among the servers if one goes off.  Three is usually adequate, and more than five is excessive.  Clients on your internal network should be able to get by with one internal server if you monitor it.  Use legitimate servers or your ISPs NTP or DNS servers as clock sources.  There are public pools as well as public servers. </p>

<p><code>ntpd</code> is self tuning and you should not need to adjust it once it is configured and started.  With recent <code>ntpd</code> implementations you can drop use of <code>ntpdate</code> entirely as they can do the initial setting of the date.</p>

<p>The following script will parse the offsets in the output of ntpd and report an excessive offset.  You could run it from cron to email you if there are problems.  The script defaults to alerting on an offset of 0.1 seconds.</p>

<pre><code>#!/bin/bash
limit=100   # Set your limit in milliseconds here
offsets=$(ntpq -nc peers | tail -n +3 | cut -c 62-66 | tr -d '-')
for offset in ${offsets}; do
    if [ ${offset:-0} -ge ${limit:-100} ]; then
        echo ""An NTPD offset is excessive - Please investigate""
        exit 1  
    fi  
done
# EOF
</code></pre>
","18682"
"How do I check if a variable exists in an 'if' statement?","76458","","<p>I need to check a variable's existence in an <code>if</code> statement. Something to the effect of:</p>

<pre><code>if [ -v $somevar ]
then
    echo ""Variable somevar exists!""
else
    echo ""Variable somevar does not exist!""
</code></pre>

<p>And the closest question to that was <a href=""https://unix.stackexchange.com/questions/138255/checking-if-line-in-file-exist/"" title=""this"">this</a>, which doesn't actually answer my question.</p>
","<p>In modern bash (version 4.2 and above):</p>

<pre><code>[[ -v name_of_var ]]
</code></pre>

<p>From <code>help test</code>:</p>

<blockquote>
  <p>-v VAR, True if the shell variable VAR is set</p>
</blockquote>
","212192"
"How to properly set up 2 network interfaces in CentOS (running in VirtualBox)?","76293","","<p>I have a VirtualBox machine running CentOS 6.5. I've created 2 network adapters for it in VirtualBox's networking settings for the machine. The first is NAT which I want the guest to use to connect to the Internet and the second I set to Host-only which I will use to SSH and access web server from the host computer. <a href=""https://forums.virtualbox.org/viewtopic.php?f=35&amp;t=42385"">This tutorial over at the VirtualBox forums</a> is what I want to achieve but I can't seem to find the <code>/etc/network/interfaces</code> file on CentOS 6.5 so I presume configuration is done differently.</p>

<p>Having setup both NAT and Host-only adapters, I started the VM and did an <code>ip addr show</code> and I have 2 interfaces, <code>eth0</code> and <code>eth2</code> (I don't know why <code>eth1</code> was somehow skipped). So I configure both <code>ifcfg-ethX</code> files in <code>/etc/sysconfig/network-scripts</code>, setting <code>eth0</code> to be DHCP (for NAT) and <code>eth2</code> to be static IP (for Host-only). For <code>eth0</code>, <code>ONBOOT=yes</code> while for <code>eth2</code>, <code>ONBOOT=no</code>. With this setup I can <code>ping</code> the Internet i.e. <code>ping -c 3 www.google.com</code> and it gets a response so I know I can connect to the Internet (while <code>eth2</code> is down).</p>

<p>Next, I <code>ifup eth2</code>. From the host machine, I SSH using the static IP I've set and I can connect without issue. But when I do <code>ping -c www.google.com</code> this time around, it fails and I know I've lost my connection to the Internet even if <code>eth0</code> is still up. Doing a <code>route -n</code> shows me a tabular output with <code>eth2</code> taking up the first and last rows while <code>eth0</code> is in between (sorry I can't post the actual output because I deleted the VM out of frustration...).</p>

<p>I've created a new CentOS VM and would like to proceed with the same set-up. My networking-fu is close to non-existent so I'm a n00b at it. I would like some tips on how to do it properly (i.e. how to set the NAT adapter to use <code>eth0</code> and set the Host-only adapter to use <code>eth1</code> and not <code>eth2</code>, how to make <code>eth0</code> still usable even if <code>eth1</code>/<code>eth2</code> is up and running).</p>

<p><strong>Update</strong></p>

<p>So I've got the new VM up and running and it seems to work now if I have the first adapter set to host-only and the second adapter set to NAT. It's set up like so:</p>

<p><em>VirtualBox network settings</em></p>

<ul>
<li>Network adapter 1: Host-only (<code>vboxnet0</code> IP address is set to <code>10.3.0.1</code>, network mask <code>255.255.255.0</code> and DHCP is off)</li>
<li>Network adapter 2: NAT (DHCP)</li>
</ul>

<p><em>CentOS VM <code>/etc/sysconfig/network-scripts/ifcfg-ethX</code> settings</em></p>

<pre><code># ifcfg-eth0
DEVICE=eth0
HWADDR=08:00:27:EC:6C:B9
TYPE=Ethernet
UUID=1339bc30-...
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
DHCPCLASS=
IPADDR=10.3.0.100
NETMASK=255.255.255.0
GATEWAY=10.3.0.1

# ifcfg-eth2
DEVICE=eth2
HWADDR=08:00:27:EB:73:BA
TYPE=Ethernet
UUID=1339bc30-...
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=dhcp
</code></pre>

<p><em>Result of <code>ip route</code></em></p>

<pre><code>10.0.3.0/24  dev  eth2  proto  kernel  scope  link  src  10.0.3.15
10.3.0.0/24  dev  eth0  proto  kernel  scope  link  src  10.3.0.100
169.254.0.0/16  dev  eth0  scope  link  metric  1002
169.254.0.0/16  dev  eth2  scope  link  metric  1003
default via 10.0.3.2 dev eth2
</code></pre>

<p><em>Result of <code>ip addr</code></em></p>

<pre><code>1:  lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN
     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00:00
     inet 127.0.0.1/8 scope host lo
     inet6 ::1/128 scope host
        valid_lft forever preferred_lft forever
2:  eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
     link/ether 08:00:27:ec:6c:b9 brd ff:ff:ff:ff:ff:ff
     inet 10.3.0.100/24 brd 10.3.0.255 scope global eth0
     inet6 fe80::a00:27ff:feec:6cb9/64 scope link
        valid_lft forever preferred_lft forever
3:  eth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
     link/ether 08:00:27:eb:73:ba brd ff:ff:ff:ff:ff:ff
     inet 10.0.0.15/24 brd 10.0.3.255 scope global eth2
     inet6 fe80::a00:27ff:feeb:73ba/64 scope link
        valid_lft forever preferred_lft forever
</code></pre>
","<p>I guess the problem is (or: was) that the gateway definition collides with DHCP. Somehow the interface order seems to influence that.</p>

<p>The basic problem is: How shall routing be done with two network cards? As I understand you the host-only card shall be used for connections to the host only. Thus you should not define any gateway for this interface at all. But maybe this works only if the first interface uses DHCP.</p>

<p>In addition you may consider avoiding DHCP completely. The NAT interface works with a static configuration, too.</p>
","115504"
"Bash: Float to Integer","76109","","<p>Is this right way of doing float to integer in bash, Or is there any other method ?</p>

<pre><code>flotToint() {
    printf ""%.0f\n"" ""$@""
}
</code></pre>
","<h2>bash</h2>

<p>In <code>bash</code>, that's probably as good as it gets. That uses a shell builtin. If you need the result in a variable, you could use command substitution, or the <code>bash</code> specific:</p>

<pre><code>printf -v int %.0f ""$float""
</code></pre>

<p>You could do:</p>

<pre><code>float=1.23
int=${float%.*}
</code></pre>

<p>But that would remove the fractional part instead of giving you the nearest integer and that wouldn't work for values of <code>$float</code> like <code>1.2e9</code> or <code>.12</code> for instance.</p>

<p>Also note the possible limitations due to the internal representation of floats:</p>

<pre><code>$ printf '%.0f\n' 1e50
100000000000000007629769841091887003294964970946560
</code></pre>

<p>You do get an integer, but chances are that you won't be able to use that integer anywhere.</p>

<p>Also, as noted by @BinaryZebra, in several <code>printf</code> implementations (bash, ksh93, yash, not GNU, zsh, dash), it is affected by the locale (the decimal separator which can be <code>.</code> or <code>,</code>).</p>

<p>So, if your floats are always expressed with the period as the decimal separator and you want it to be treated as such by <code>printf</code> regardless of the locale of the user invoking your script, you'd need to fix the locale to C:</p>

<pre><code>LC_ALL=C printf '%.0f' ""$float""
</code></pre>

<p>With <code>yash</code>, you can also do:</p>

<pre><code>printf '%.0f' ""$(($float))""
</code></pre>

<p>for the content of <code>$float</code> to be converted to the format in the current locale.</p>

<h2>POSIX</h2>

<pre><code>printf ""%.0f\n"" 1.1
</code></pre>

<p>is not POSIX as <code>%f</code> is not required to be supported by POSIX.</p>

<p>POSIXly, you can do:</p>

<pre><code>f2i() {
  awk 'BEGIN{for (i=1; i&lt;ARGC;i++)
   printf ""%.0f\n"", ARGV[i]}' ""$@""
}
</code></pre>

<p>That one is not affected by the locale (the comma cannot be a decimal separator in <code>awk</code> since it's already a special character in the syntax there (<code>print 1,2</code>, same as <code>print 1, 2</code> to pass two arguments to <code>print</code>)</p>

<h2>zsh</h2>

<p>In <code>zsh</code> (which supports floating point arithmetic (decimal separator is always the period)), you have the <code>rint()</code> math function to give you the nearest integer as a float (like in <code>C</code>) and <code>int()</code> to give you an integer from a float (like in <code>awk</code>). So you can do:</p>

<pre><code>$ zmodload zsh/mathfunc
$ i=$((int(rint(1.234e2))))
$ echo $i
123
</code></pre>

<p>Or:</p>

<pre><code>$ integer i=$((rint(5.678e2)))
$ echo $i
568
</code></pre>

<p>However note that while <code>double</code>s can represent very large numbers, integers are much more limited.</p>

<pre><code>$ printf '%.0f\n' 1e123
999999999999999977709969731404129670057984297594921577392083322662491290889839886077866558841507631684757522070951350501376
$ echo $((int(1e123)))
-9223372036854775808
</code></pre>

<h2>ksh93</h2>

<p>ksh93 was the first Bourne-like shell to support floating point arithmetic. ksh93 optimises command substitution by not using a pipe or forking when the commands are only builtin commands. So</p>

<pre><code>i=$(printf '%.0f' ""$f"")
</code></pre>

<p>doesn't fork.</p>

<p>You can also do:</p>

<pre><code>i=$((rint(f)))
</code></pre>

<p>But beware of:</p>

<pre><code>$ echo ""$((rint(1e18)))""
1000000000000000000
$ echo ""$((rint(1e19)))""
1e+19
</code></pre>

<p>You could also do:</p>

<pre><code>integer i=$((rint(f)))
</code></pre>

<p>But like for <code>zsh</code>:</p>

<pre><code>$ integer i=1e18
$ echo ""$i""
1000000000000000000
$ integer i=1e19
$ echo ""$i""
-9223372036854775808
</code></pre>

<p>Beware that <code>ksh93</code> floating point arithmetic honour the decimal separator setting in the locale (even though <code>,</code> is otherwise a math operator (<code>$((1,2))</code> would be 6/5 in a French/German... locale, and the same as <code>$((1, 2))</code>, that is 2 in an English locale).</p>

<h2>yash</h2>

<p>yash also supports floating point arithmetic but doesn't have math functions like <code>ksh93</code>/<code>zsh</code>'s <code>rint()</code>. You can convert a number to integer though by using the <em>binary or</em> operator for instance (also works in <code>zsh</code> but not in <code>ksh93</code>). Note however that it truncates the decimal part, it doesn't give you the nearest integer:</p>

<pre><code>$ echo ""$((0.237e2 | 0))""
23
$ echo ""$((1e19))""
-9223372036854775808
</code></pre>

<p><code>yash</code> honours the locale's decimal separator on output, but not for the floating point literal constants in its arithmetic expressions, which can cause surprises:</p>

<pre><code>$ LC_ALL=fr_FR.UTF-8 ./yash -c 'a=$((1e-2)); echo $(($a + 1))'
./yash: arithmetic: `,' is not a valid number or operator
</code></pre>

<p>It's good in a way in that you can use floating point constants in your scripts that use the period and not have to worry that it will stop working in other locales, but still be able to deal with the numbers as expressed by the user as long as you remember to do:</p>

<pre><code>var=$((10.3)) # and not var=10.3
... ""$((a + 0.1))"" # and not ""$(($a + 0.1))"".

printf '%.0f\n' ""$((10.3))"" # and not printf '%.0f\n' 10.3
</code></pre>
","89748"
"Crontab job start +1 min after @reboot","76101","","<p>Need a script executed at each reboot in +1 minute. I placed a record <code>@reboot</code> but it is too early for my script. How to execute it sometime after reboot?</p>
","<p>Is the script only ever intended to run one minute after boot up, or can it be used at other times, too? In the former case, you can add <code>sleep 60</code> to the beginning of your script, or in the latter case, add it to the crontab file:</p>

<pre><code>@reboot sleep 60 &amp;&amp; my_script.sh
</code></pre>

<p>As has been pointed out by sr_, though, perhaps you are tackling this in the wrong way, and a proper init.d or rc.d script would be a more robust solution.</p>
","57856"
"How to cause kernel panic with a single command?","76006","","<p>Is it possible to cause a kernel panic with a single command line?</p>

<p>What would be the most straightforward such command for a sudoing user and what would it be for a regular user, if any?</p>

<p>Scenarios that suggest downloading something as a part of the command do not count.</p>
","<p>FreeBSD:</p>

<pre><code>sysctl debug.kdb.panic=1
</code></pre>

<p>Linux (more info <a href=""https://web.archive.org/web/20160816230132/https://www.kernel.org/doc/Documentation/sysrq.txt"" rel=""noreferrer"">here</a>):</p>

<pre><code>echo c &gt; /proc/sysrq-trigger
</code></pre>
","66205"
"Reading grep patterns from a file","75947","","<p>I have a couple of big text files and in the file <code>UNIQS.txt</code> I have a list of strings to <code>grep</code> from another file. The code I use is</p>

<pre><code>grep -f UNIQS.txt EEP_VSL.uniqs.sam &gt; UNIQ_templates.sam
</code></pre>

<p>which does nothing - the file generated is empty. But when I do</p>

<pre><code>grep -F -f UNIQS.txt EEP_VSL.uniqs.sam &gt; UNIQ_templates.sam
</code></pre>

<p>it works correctly. This confuses me because I didn't think <code>grep</code> would interpret the entries in <code>UNIQS.txt</code> as <code>regexp</code> patterns without quotes and slashes and so on being in the file (which there aren't). Is it the case in general that if you are getting the patterns from a file then it will automatically think that they are <code>regexp</code> patterns?</p>

<p><strong>Edit:</strong> In the <code>UNIQS.txt</code> file, there are newline separated strings of the form</p>

<pre><code>HWI-ST365:215:D0GH0ACXX:2:1101:10034:186783
</code></pre>

<p>(called template names) and the file <code>EEP_VSL...</code> tab separated columns, with about 14 columns and the first column is the template name, so basically I want to extract the line corresponding to each template in the file.</p>
","<p>The <code>-f</code> option specifies a file where grep reads patterns. That's just like passing patterns on the command line (with the <code>-e</code> option if there's more than one), except that when you're calling from a shell you may need to quote the pattern to protect special characters in it from being expanded by the shell.</p>

<p>The argument <code>-E</code> or <code>-F</code> or <code>-P</code>, if any, tells grep which syntax the patterns are written in. With no argument, grep expects <a href=""http://en.wikipedia.org/wiki/Regular_expression#POSIX_basic_and_extended"">basic regular expressions</a>; with <code>-E</code>, grep expects <a href=""http://en.wikipedia.org/wiki/Regular_expression#POSIX_extended"">extended regular expressions</a>; with <code>-P</code> (if supported), grep expects <a href=""http://en.wikipedia.org/wiki/Regular_expression#Standard_Perl"">Perl regular expressions</a>; and with <code>-F</code>, grep expects literal strings. Whether the patterns come from the command line or from a file doesn't matter.</p>

<p>Note that the strings are substrings: if you pass <code>a+b</code> as a pattern then a line containing <code>a+b+c</code> is matched. If you want to search for lines containing exactly one of the supplied strings and no more, then pass the <code>-x</code> option.</p>
","83315"
"Allow non-superusers to mount any filesystem","75881","","<p>Is it possible to allow some particular users (e.g. members of a group) to mount <strong>any</strong> filesystem without superuser privileges on Linux?</p>

<p>(Another question might have been ""in what ways a user can harm a system by mounting filesystems?"")</p>
","<p>There are a couple approaches, some of them mostly secure, others not at all.</p>

<h2>The insecure way</h2>

<p>Let any use run <code>mount</code>, e.g., through sudo. You might as well give them root; it's the same thing. The user could mount a filesystem with a suid root copy of <code>bash</code>—running that instantly gives root (likely without any logging, beyond the fact that <code>mount</code> was run).</p>

<p>Alternatively, a user could mount his own filesystem on top of <code>/etc</code>, containing his/her own copy of <code>/etc/shadow</code> or <code>/etc/sudoers</code>, then obtain root with either <code>su</code> or <code>sudo</code>. Or possibly bind-mount (<code>mount --bind</code>) over one of those two files. Or a new file into <code>/etc/sudoers.d</code>.</p>

<p>Similar attacks could be pulled off over <code>/etc/pam.d</code> and many other places.</p>

<p>Remember that filesystems need not even be on a device, <code>-o loop</code> will mount a file which is owned (and thus modifiable) by the user.</p>

<h2>The mostly secure way: udisks or similar</h2>

<p>The various desktop environments have actually already built solutions to this, to allow users to mount removable media. They work by mounting in a subdirectory of <code>/media</code> only and by turning off set-user/group-id support via kernel options. Options here include <code>udisks</code>, <code>udisks2</code>, <code>pmount</code>, <code>usbmount</code>, </p>

<p>If you must, you could write your own script to do something similar, and invoke it through sudo—but you have to be really careful writing this script to not leave root exploits. If you don't want your users to have to remember sudo, you can do something like this in a script:</p>

<pre><code>#!/bin/bash
if [ $UID -ne 0 ]; then       # or `id -u`
    exec sudo -- ""$0"" ""$@""
fi

# rest of script goes here 
</code></pre>

<h2>The will-be-secure someday way: user namespaces</h2>

<p>As of Linux 3.10, the namespaces support is basically complete, the major missing thing apparently being support in XFS (a filesystem). Namespaces are a very lightweight form of virtualization (containers, to be more specific). In particular, with user namespaces, <em>any</em> user on the system can create their own environment in which they are root, and thus can mount filesystems.</p>

<p>Many distro kernels, however, have disabled the feature (ex: <a href=""http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=712870"">Debian</a>) due to the XFS conflict and worries over security; indeed there was an root exploit discovered earlier this year (<a href=""http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-1858"">CVE-2013-1858</a>). You may need to recompile the kernel with <code>CONFIG_USER_NS=y</code>.</p>

<p>The best documentation I know of about user namespaces is an LWN article <em><a href=""http://lwn.net/Articles/532593/"">
Namespaces in operation, part 5: User namespaces</a>.</em></p>
","96643"
"How to signal the end of stdin input","75789","","<ol>
<li>In Bash, I learned that the ending
signal can be changed by here
document. But by default how can I
signal the end of stdin input?</li>
<li>I happened to find that with cat and
chardet, their stdin inputs can be
signaled as finished by Ctrl+d. But
I seems to remember that Ctrl+d and
Ctrl+c are similar to ending
execution of a running command. So
am I wrong?</li>
</ol>
","<p><code>Ctrl+D</code>, when typed at the start of a line on a terminal, signifies the end of the input. This is not a <a href=""http://en.wikipedia.org/wiki/Signal_%28computing%29"">signal</a> in the unix sense: when an application is reading from the terminal and the user presses <code>Ctrl+D</code>, the application is notified that the end of the file has been reached (just like if it was reading from a file and had passed the last byte).</p>

<p><code>Ctrl+C</code> does send a signal, <a href=""http://en.wikipedia.org/wiki/SIGINT_%28POSIX%29"">SIGINT</a>. By default SIGINT (the interrupt signal) kills the foreground application, but the application can catch the signal and react in some different way (for example, the shell itself catches the signal and aborts the line you've begun typing, but it doesn't exit, it shows a new prompt and waits for a new command line).</p>

<p><sub>
You can change the characters associated with end-of-file and SIGINT with the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/stty.html#tag_04_135_05_05""><code>stty</code></a> command, e.g. <code>stty eof a</code> would make <code>a</code> the end-of-file character, and <code>stty intr ^-</code> would disable the SIGINT character. This is rarely useful.
</sub></p>
","16338"
"How can I tell what version of Linux I'm using?","75579","","<p>Often times I will ssh into a new client's box to make changes to their website configuration without knowing much about the server configuration. I have seen a few ways to get information about the system you're using, but are there some standard commands to tell me what version of Unix/Linux I'm on and basic system information (like if it is a 64-bit system or not), and that sort of thing?</p>

<p>Basically, if you just logged into a box and didn't know anything about it, what things would you check out and what commands would you use to do it?</p>
","<p>If I need to know what it is say Linux/Unix , 32/64 bit</p>

<pre><code>uname -a 
</code></pre>

<p>This would give me almost all information that I need, </p>

<p>If I further need to know what release it is say (Centos 5.4, or 5.5 or 5.6)
on a Linux box I would further check the file <code>/etc/issue</code> to see its release info ( or for Debian / Ubuntu <code>/etc/lsb-release</code> )</p>

<p>Alternative way is to use the <code>lsb_release</code> utility:</p>

<pre><code>lsb_release -a
</code></pre>

<p>Or do a <code>rpm -qa | grep centos-release</code> or <code>redhat-release</code> for RHEL derived systems</p>
","23840"
"How to recover data from a bad SD card?","75534","","<p>I have a 4GB SD card with some family pictures on it that I need to recover. When I insert the card into my card reader, it shows up as an unknown 32MB device (as <code>/dev/sde</code>) and cannot be mounted. When inserting back into the camera (a Nikon D60), it says the cards needs to be formatted (as does inserting it into a Windows machine). I want to recover all of the pictures on the card (there were others before the family pictures) because I don't know how many I took or their exact sizes (but I believe they were all JPEGs). The card should be formatted as a FAT32 filesystem. </p>

<p>What Linux or Unix utilities are available to recover the files? Can I do it myself or do I need to seek professional help? </p>

<p>Edit: It appears that my card reader has damaged the card in some way, making it unreadable and and unformattable. When I checked another card that was the exact same (save for no files), it ""ruined"" the second one. I would like to use the second card again, so is there a tool to format a damaged card that doesn't know (or cannot report properly) how large it is? </p>
","<p>First, from your experience with the second card, it seems that your reader is damaged and now damages the cards you insert into it. Stop using that reader immediately, and try to recover the card with another reader. If your data is at all valuable, try to get a brand-name reader with better quality than a bottom-price one.</p>

<p>If the card is merely <em>partly</em> unreadable and not completely unreadable, first try to copy what you can from the card to an image file. Don't use <code>dd</code> for this as it'll stop reading on the first error. Use tools such as <a href=""http://www.forensicswiki.org/wiki/Dd_rescue""><code>dd_rescue</code></a> or <a href=""http://www.forensicswiki.org/wiki/Ddrescue""><code>ddrescue</code></a>. Both tools try to grab as much data as possible from the disk. Example usage (<code>/dev/sdc</code> being the device corresponding to the card; if you don't know which one it is, run <code>cat /proc/partitions</code> and pick the one that seems to have the right size):</p>

<pre><code>ddrescue -dr3 /dev/sdc card.image logfile
</code></pre>

<p>Since it looks like the filesystem structure is damaged (your OSes offer to format the drive because they don't see a valid filesystem on it), you'll have to try to recover the files individually. Fortunately, image files start with a recognizable header, and there are many existing <a href=""http://www.forensicswiki.org/wiki/Tools%3aData_Recovery#Carving"">carving tools</a> that recognize images: <a href=""http://foremost.sourceforge.net/"">Foremost</a>, <a href=""http://www.itu.dk/people/jobr/magicrescue/"">MagicRescue</a>, <a href=""http://www.cgsecurity.org/wiki/PhotoRec"">PhotoRec</a> (from the makers of <a href=""http://www.cgsecurity.org/wiki/TestDisk"">TestDisk</a>), <a href=""http://www.rfc1149.net/devel/recoverjpeg.html"">RecoverJPEG</a>, …</p>

<p>Most of these tools are available on typical unix distributions. But if you prefer, you can run a <a href=""http://www.cgsecurity.org/wiki/TestDisk_Livecd"">special-purpose distribution</a> or other live CD including recovery tools such as <a href=""http://www.sysresccd.org/Main_Page"">SysRescueCD</a>, <a href=""http://www.knopper.net/knoppix/index-en.html"">Knoppix</a>, <a href=""http://www.caine-live.net/"">CAINE</a>…</p>
","14577"
"How to add new lines when using echo","75518","","<p>Why the following command does not inserts new lines in the generated file and what's the solution?</p>

<pre><code>$echo ""Line 1\r\nLine2"" &gt;&gt; readme.txt

$ cat readme.txt 
Line 1\r\nLine2
</code></pre>
","<h3><code>echo</code></h3>

<hr>

<p>An <code>echo</code> implementation which strictly conforms to the Single Unix Specification will add newlines if you do:</p>

<pre class=""lang-sh prettyprint-override""><code>echo 'line1\nline2'
</code></pre>

<p>But that is not a reliable behavior. In fact, there really <em>isn't</em> any standard behavior which you can expect of <code>echo</code>.</p>

<blockquote>
  <p><strong><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/echo.html"" rel=""noreferrer"">OPERANDS</a></strong></p>
  
  <blockquote>
    <p><em><code>string</code></em></p>
    
    <blockquote>
      <p>A string to be written to standard output. If the first operand is <code>-n</code>, or if any of the operands contain a <em>&lt;<code>\</code>backslash></em> character, the <strong><em>results are implementation-defined</em></strong>.</p>
      
      <p>On <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/help/codes.html#XSI"" rel=""noreferrer"">XSI-conformant</a> systems, if the first operand is <code>-n</code>, it shall be treated as a string, <strong><em>not an option</em></strong>. The following character sequences shall be recognized on XSI-conformant systems within any of the arguments:</p>
      
      <blockquote>
        <p><code>\a</code> - Write an <em>&lt;alert></em>.</p>
        
        <p><code>\b</code> - Write a <em>&lt;backspace></em>.</p>
        
        <p><code>\c</code> - Suppress the <em>&lt;newline></em> that otherwise follows the final argument in the output. All characters following the <code>\c</code> in the arguments shall be ignored.</p>
        
        <p><code>\f</code> - Write a <em>&lt;form-feed></em>.</p>
        
        <p><code>\n</code> - Write a <em>&lt;newline></em>.</p>
        
        <p><code>\r</code> - Write a <em>&lt;carriage-return></em>.</p>
        
        <p><code>\t</code> - Write a <em>&lt;tab></em>.</p>
        
        <p><code>\v</code> - Write a <em>&lt;vertical-tab></em>.</p>
        
        <p><code>\\</code> - Write a <em>&lt;backslash></em> character.</p>
        
        <p><code>\0num</code> - Write an 8-bit value that is the zero, one, two, or three-digit octal number <em><code>num</code></em>.</p>
      </blockquote>
    </blockquote>
  </blockquote>
</blockquote>

<p>And so there really isn't any general way to know how to write a newline with <code>echo</code>, except that you can generally rely on just doing <code>echo</code> to do so.</p>

<p>A <code>bash</code> shell typically does <em>not</em> conform to the specification, and handles the <code>-n</code> and other options, but even that is uncertain. You can do:</p>

<pre class=""lang-sh prettyprint-override""><code>shopt -s xpg_echo
echo hey\\nthere
</code></pre>

<hr>

<pre><code>hey
there
</code></pre>

<p>And <em>not even that</em> is necessary if <code>bash</code> has been built with the build-time option...</p>

<blockquote>
  <p><a href=""http://www.gnu.org/software/bash/manual/html_node/Optional-Features.html"" rel=""noreferrer""><em><code>--enable-xpg-echo-default</code></em></a></p>
  
  <blockquote>
    <p>Make the <code>echo</code> builtin expand backslash-escaped characters by default, without requiring the <code>-e</code> option. This sets the default value of the <em><code>xpg_echo</code></em> shell option to <em><code>on</code></em>, which makes the Bash <code>echo</code> behave more like the version specified in the Single Unix Specification, version 3. See <a href=""http://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html#Bash-Builtins"" rel=""noreferrer"">Bash Builtins</a>, for a description of the escape sequences that <code>echo</code> recognizes.</p>
  </blockquote>
</blockquote>

<hr>

<h3><code>printf</code></h3>

<hr>

<p>On the other hand, <code>printf</code>'s behavior is pretty tame in comparison.</p>

<blockquote>
  <p><a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/printf.html"" rel=""noreferrer""><strong>RATIONALE</strong></a></p>
  
  <blockquote>
    <p>The <code>printf</code> utility was added to provide functionality that has historically been provided by <code>echo</code>. However, due to <strong>irreconcilable differences</strong> in the various versions of <code>echo</code> extant, the version has few special features, leaving those to this new <code>printf</code> utility, which is based on one in the Ninth Edition system.</p>
    
    <p>The EXTENDED DESCRIPTION section almost exactly matches the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/functions/printf.html"" rel=""noreferrer""><em><code>printf()</code></em></a> function in the ISO C standard, although it is described in terms of the file format notation in <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap05.html#tag_05"" rel=""noreferrer"">XBD File Format Notation</a>.</p>
  </blockquote>
</blockquote>

<p>It handles format strings which describe its arguments - which can be any number of things, but for strings are pretty much either <code>%b</code>yte strings or literal <code>%s</code>trings. Other than the <code>%f</code>ormats in the first argument, it behaves most like a <code>%b</code>yte string argument, except that it doesn't handle the <code>\c</code> escape.</p>

<pre class=""lang-sh prettyprint-override""><code>printf ^%b%s$ '\n' '\n' '\t' '\t'
</code></pre>

<hr>

<pre><code>^
\n$^    \t$
</code></pre>

<p>See <a href=""https://unix.stackexchange.com/a/65819/52934"">Why is <code>printf</code> better than <code>echo</code>?</a> for more.</p>

<hr>

<h3><code>echo() printf</code></h3>

<hr>

<p>You might write your own standards conformant <code>echo</code> like...</p>

<pre class=""lang-sh prettyprint-override""><code>echo(){
    printf '%b ' ""$@\n\c""
}
</code></pre>

<p>...which should pretty much always do the right thing automatically.</p>

<p>Actually, no... That prints a literal <code>\n</code> at the tail of the arguments if the last argument ends in an odd number of <em>&lt;backslashes></em>.</p>

<p>But this doesn't:</p>

<pre class=""lang-sh prettyprint-override""><code>echo()
    case    ${IFS- } in
    (\ *)   printf  %b\\n ""$*"";;
    (*)     IFS=\ $IFS
            printf  %b\\n ""$*""
            IFS=${IFS#?}
    esac
</code></pre>
","219274"
"Can't Install build-essential on CentOS","75368","","<p>When I run the command:</p>

<pre><code>sudo apt-get install build-essential
</code></pre>

<p>I get the following error message:</p>

<pre class=""lang-none prettyprint-override""><code>Reading Package Lists... Done
Building Dependency Tree... Done
E: Couldn't find package build-essential
</code></pre>
","<p>I believe this still should work.</p>

<pre><code>sudo yum groupinstall 'Development Tools'
</code></pre>
","32439"
"Running Python on ChromeOS","75297","","<p>Is it possible to run the Python interpreter on a ChromeOS machine? I've found various editors you can use, but I would like the ability to run python applications as well.</p>

<p>I would like to purchase the Samsung Chromebook, and being a computer science student, I'd love to be able to do my CS homework on it instead of carrying around my 15 inch Macbook or Toshiba. </p>
","<h3>Python Shell</h3>

<p>You can install this plugin, <a href=""https://chrome.google.com/webstore/detail/python-shell/gdiimmpmdoofmahingpgabiikimjgcia?hl=en"">Python Shell</a> into Chrome. Here's some info from that extensions info page in the store:</p>

<blockquote>
  <p>Python shell for your browser.<br/>
  A Python shell for Chrome.<br/></p>
  
  <p>Features:</p>
  
  <ul>
  <li>Python 2.7</li>
  <li>Ruby 1.8</li>
  <li>JavaScript</li>
  </ul>
  
  <p>These are the only languages that have been currently compiled to
  JavaScript by the jsrepl project as this time.</p>
</blockquote>

<h3>Developer Mode</h3>

<p>Alternatively you can go put your device in <a href=""http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices/samsung-series-5-chromebook"">Developer Mode</a> and gain access to a <a href=""http://www.chromium.org/chromium-os/poking-around-your-chrome-os-device"">shell</a> from where you can install/launch Python.</p>

<h3>Skulpt Interpreter</h3>

<p>Lastly you can check out the <a href=""https://chrome.google.com/webstore/detail/skulpt-interpreter/bocjplmmdjglmffmpofmmndklbdpcmeb?hl=en&amp;gl=US"">Skulpt Interpreter</a>. Main site's <a href=""http://www.skulpt.org/"">here</a>.</p>

<blockquote>
  <p>Skulpt is an entirely in-browser implementation of Python.</p>
</blockquote>

<h3>Crouton</h3>

<p>You can install a full fledged Linux on the Chromebook hardware using the project <a href=""https://github.com/dnschneid/crouton"">Crouton</a>.</p>

<blockquote>
  <p>crouton is a set of scripts that bundle up into an easy-to-use,
  Chromium OS-centric chroot generator. Currently Ubuntu and Debian are
  supported (using debootstrap behind the scenes), but ""Chromium OS
  Debian, Ubuntu, and Probably Other Distros Eventually Chroot
  Environment"" doesn't acronymize as well (crodupodece is admittedly
  pretty fun to say, though).</p>
</blockquote>

<p>There's a easy to follow tutorial on Life Hacker which walks you through the installation and setup, titled: <a href=""http://lifehacker.com/how-to-install-linux-on-a-chromebook-and-unlock-its-ful-509039343"">How to Install Linux on a Chromebook and Unlock Its Full Potential</a>.</p>

<h3>Which way to go?</h3>

<p>If you're serious about using the Chromebook hardware as a development box I would go with Crouton. The other options only give you pieces of Python. If you're serious about doing any real development this is really the only option.</p>
","84167"
"How create a temporary file in shell script?","75291","","<p>While running a script, I want to create a temporary file in <code>/tmp</code> directory.</p>

<p>After execution of that script, that will be cleaned by that script.</p>

<p>How to do that in shell script?</p>
","<pre><code>tmpfile=$(mktemp /tmp/abc-script.XXXXXX)
: ...
rm ""$tmpfile""
</code></pre>

<p>You can make sure that a file is deleted when the scripts exits (including kills and crashes) by opening a file descriptor to the file and deleting it. The file keeps available (for the script; not really for other processes but <code>/proc/$PID/fd/$FD</code> is a work-around) as long as the file descriptor is open. When it gets closed (which the kernel does automatically when the process exits) the filesystem deletes the file.</p>

<pre><code>tmpfile=$(mktemp /tmp/abc-script.XXXXXX)
exec 3&gt;""$tmpfile""
rm ""$tmpfile""
: ...
echo foo &gt;&amp;3
</code></pre>
","181938"
"apt-get install fails with error ""Unable to locate package""","75242","","<p>I'm getting the error message that the system is not able to locate the package kamailio. </p>

<p>I navigated the repo on that site and I can see that there is a kamailio package, but it's located under <a href=""http://repo.pouf.org/raspbian/pool/main/k/"" rel=""nofollow"">http://repo.pouf.org/raspbian/pool/main/k/</a>
folder.</p>

<p>I've also tried to change the sources.list file to read:</p>

<pre><code>deb http://repo.pouf.org/raspbian/dists/ wheezy main
</code></pre>

<p>But that didn't fix the issue. </p>
","<p>Looks like you just haven't updated your package lists, this is missing from the link that you gave -</p>

<pre><code>sudo apt-get update
</code></pre>

<p>This should download the list files from the repos in <code>/etc/apt/sources.list</code> so that <code>apt-get install</code> knows what packages to look for.</p>

<p>Note also that you should do this regularly as the repository will change over time. In particularly do it before installing software if it hasn't been done for a while!</p>
","120266"
"Unable to ping gateway & other Linux boxes on same network","75218","","<p>I'm somewhat new to Linux, but I'm trying to network a few Linux machines. I can't seem to be able ping the gateway or the other Linux machines. </p>

<p>My scenario: I have a few Linux boxes set up on a <code>10.45.89.x</code> subnet with the gateway being <code>10.45.89.1</code>. On all of these, I'm unable to ping anything except <code>127.0.0.1</code>. I'm wondering if I need to modify any of the routing tables or hosts file. Below is a few commands and their output. Perhaps I need to check something else?</p>

<h3>netstat -r output:</h3>

<pre><code>10.45.89.0   *   255.255.255.0    U
10.112.0.0    *    255.255.0.0    U
169.254.0.0    *    255.255.0.0    U
default    10.45.89.1    0.0.0.0    UG
</code></pre>

<h3>/etc/hosts</h3>

<pre><code>127.0.0.1 localhost.localdomain  localhost
127.0.0.1 localhost.localdomain  localhost
9.37.253.154 rpt.rhn.linux.ul.com
</code></pre>

<h3>/etc/resolv.conf</h3>

<pre><code>domain demolotus.com
namserver 127.0.0.1
</code></pre>

<h3>/etc/sysconfig/network</h3>

<pre><code>NETWORKING=yes
NETWORKING_IPV6=no
HOSTNAME=st852.demolotus.com
</code></pre>

<h3>ip route show dev eth0</h3>

<pre><code>10.45.89.0/24  proto  kernel  scope  link  src  10.45.89.138
default via 10.45.89.1
</code></pre>

<h3>ifconfig</h3>

<pre><code>eth0    Link encap:Ethernet  HWaddr 00:0C:29:01:28:CD
inet addr:10.45.89.138  Bcast:10.45.89.255 Mask:255.255.255.0
UP BROADCAST MULTICAST MTU:1500 Metric:1

eth0:0 Link encap:Ethernet HWaddr 00:0C:29:01:28:CD
inet addr:10.45.89.139  Bcast:10.45.89.255 Mask:255.255.255.0
UP BROADCAST MULTICAST MTU:1500 Metric:1
</code></pre>

<h3>ip addr show dev eth0</h3>

<pre><code>2: etho0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000
link/ether 00:0c:29:01:28:cd brd ff:ff:ff:ff:ff:ff
inet 10.45.89.138/24 brd 10.45.89.255 scope global eth0
inet 10.45.89.139/24 brd 10.45.89.255 scope global secondary eth0:0
</code></pre>

<p>Any suggestions are greatly appreciated. Thanks!</p>
","<p>on each host, they should have at least their hostname and a corresponding entry in /etc/hosts with their IP in the relevant network, no? ^^
You only show 127.0.0.1 in there, and the ""localhost"" hostname... So how can it know it is also part of the 192.whatever network and should therefore be able to ping all the other hosts on there?
In other words: just edit the hosts file and add a line with:</p>

<pre><code>10.45.89.xxx  st852.demolotus.com
</code></pre>

<p>xxx being the last IP digit of  st852.demolotus.com in the 10.45.89.X/24 subnetwork</p>
","60211"
"Mounting a squashfs filesystem in read-write","75094","","<p>I have a Clonezilla installation on a USB stick and I'd like to make some modifications to the operating system. Specifically, I'd like to insert a runnable script into <code>/usr/sbin</code> to make it easy to run my own backup command to make backups less painful. </p>

<p>The main filesystem lives under <code>/live/filesystem.squashfs</code> on the USB FAT-32 partition. </p>

<p>How can I mount this read/write on my Linux machine in order to be able to add/remove/change files? I'm running an Ubuntu 12.04 derivative.</p>
","<p>As root, copy <code>filesystem.squashfs</code> to some empty dir, e.g.:</p>

<pre><code>cp /mnt/clonezilla/live/filesystem.squashfs /path/to/workdir
cd /path/to/workdir
</code></pre>

<p>Unpack the file then move it somewhere else (so you still have it as a backup):</p>

<pre><code>unsquashfs filesystem.squashfs
mv filesystem.squashfs /path/to/backup/
</code></pre>

<p>Go in <code>squashfs-root</code>, add/modify as per your taste then recreate <code>filesystem.squashfs</code>:</p>

<pre><code>cd /path/to/workdir
mksquashfs squashfs-root filesystem.squashfs -b 1024k -comp xz -Xbcj x86 -e boot
</code></pre>

<p>copy the newly created <code>filesystem.squashfs</code> over the existing one on your USB drive, e.g.:</p>

<pre><code>cp filesystem.squashfs /mnt/clonezilla/live/
</code></pre>

<p>then reboot and use your LIVE USB.</p>

<p>Note: the above commands are part of <code>squashfs-tools</code>.</p>
","80312"
"get first X characters from the cat command?","74995","","<p>I have a text file I'm outputting to a variable in my shell script.  I only need the first 50 characters however.</p>

<p>I've tried using <code>cat ${filename} cut -c1-50</code> but I'm getting far more than the first 50 characters?  That may be due to <code>cut</code> looking for lines (not 100% sure), while this text file could be one long string-- it really depends.  </p>

<p>Is there a utility out there I can pipe into to get the first X characters from a <code>cat</code> command?</p>
","<pre><code>head -c 50 file
</code></pre>

<p>This returns the first 50 bytes.</p>
","167816"
"Solving ""mv: Argument list too long""?","74799","","<p>I have a folder with more than a million files that needs sorting, but I can't really do anything because <code>mv</code> outputs this message all the time</p>

<pre><code>-bash: /bin/mv: Argument list too long
</code></pre>

<p>I'm using this command to move extension-less files:</p>

<pre><code>mv -- !(*.jpg|*.png|*.bmp) targetdir/
</code></pre>
","<p><a href=""http://linux.die.net/man/1/xargs""><code>xargs</code></a> is the tool for the job. That, or <a href=""http://linux.die.net/man/1/find""><code>find</code></a> with <code>-exec … {} +</code>. These tools run a command several times, with as many arguments as can be passed in one go.</p>

<p>Both methods are easier to carry out when the variable argument list is at the end, which isn't the case here: the final argument to <code>mv</code> is the destination. With GNU utilities (i.e. on non-embedded Linux or Cygwin), the <code>-t</code> option to <code>mv</code> is useful, to pass the destination first.</p>

<p>If the file names have no whitespace nor any of <code>\""'</code>, then you can simply provide the file names as input to <code>xargs</code> (the <code>echo</code> command is a bash builtin, so it isn't subject to the command line length limit):</p>

<pre><code>echo !(*.jpg|*.png|*.bmp) | xargs mv -t targetdir
</code></pre>

<p>You can use the <code>-0</code> option to <code>xargs</code> to use null-delimited input instead of the default quoted format.</p>

<pre><code>printf '%s\0' !(*.jpg|*.png|*.bmp) | xargs -0 mv -t targetdir
</code></pre>

<p>Alternatively, you can generate the list of file names with <code>find</code>. To avoid recursing into subdirectories, use <code>-type d -prune</code>. Since no action is specified for the listed image files, only the other files are moved.</p>

<pre><code>find . -name . -o -type d -prune -o \
       -name '*.jpg' -o -name '*.png' -o -name '*.bmp' -o \
       -exec mv -t targetdir/ {} +
</code></pre>

<p>(This includes dot files, unlike the shell wildcard methods.)</p>

<p>If you don't have GNU utilities, you can use an intermediate shell to get the arguments in the right order. This method works on all POSIX systems.</p>

<pre><code>find . -name . -o -type d -prune -o \
       -name '*.jpg' -o -name '*.png' -o -name '*.bmp' -o \
       -exec sh -c 'mv ""$@"" ""$0""' targetdir/ {} +
</code></pre>

<hr>

<p>In zsh, you can load the <a href=""http://zsh.sourceforge.net/Doc/Release/Zsh-Modules.html#index-mv""><code>mv</code> builtin</a>:</p>

<pre><code>setopt extended_glob
zmodload zsh/files
mv -- ^*.(jpg|png|bmp) targetdir/
</code></pre>

<p>or if you prefer to let <code>mv</code> and other names keep referring to the external commands:</p>

<pre><code>setopt extended_glob
zmodload -Fm zsh/files b:zf_\*
zf_mv -- ^*.(jpg|png|bmp) targetdir/
</code></pre>

<p>or with ksh-style globs:</p>

<pre><code>setopt ksh_glob
zmodload -Fm zsh/files b:zf_\*
zf_mv -- !(*.jpg|*.png|*.bmp) targetdir/
</code></pre>

<p>Alternatively, using GNU <code>mv</code> and <a href=""http://zsh.sourceforge.net/Doc/Release/User-Contributions.html#index-zargs""><code>zargs</code></a>:</p>

<pre><code>autoload -U zargs
setopt extended_glob
zargs -- ./^*.(jpg|png|bmp) -- mv -t targetdir/
</code></pre>
","128563"
"How to easily build your own Linux Distro?","74668","","<p>I wanted to have a go at creating my very own Linux Distribution. Could you suggest some nice and easy-to-follow tutorials (preferably text based and not videos).
I have heard something about Arch Linux but I don't know how to go from there. What do I need? </p>
","<p>Part of the answer depends on what you mean by your own distro. if you mean a version of Linux custom built to your own purposes for you to use on your own machines, or even in your own office, there are a couple of pretty cool tools that allow you to customize existing distributions that are known working. </p>

<p><a href=""http://www.centos.org/docs/5/html/Installation_Guide-en-US/ch-kickstart2.html"">http://www.centos.org/docs/5/html/Installation_Guide-en-US/ch-kickstart2.html</a> covers kickstart installations of CentOS (also applies to Scientific, Fedora and RedHat.) There's also <a href=""http://susestudio.com/"">http://susestudio.com/</a> which allows you to make a customized installation disk of SuSe Linux, meaning you can get the packages you want installed right off the bat. The advantage to this method, more so with the kickstart, is that you can choose individual packages and leave out whatever fluff you don't want to bother with, but also get the advantages of knowing that updated packages will be available to you and work without a significant amount of testing and overhead on your part.</p>

<p>If you're just looking to make it look the way you want to look, custom splash screens, logos, etc, there are a ton of guides available for making these kinds of changes.</p>

<p>Now, if you really just want to get nuts and bolts and really do up your own thing, then the suggestion by @vfbsilva to look at LFS is irreplaceable. You really do learn how things get put together and what the requirements are to make Linux ... well, Linux. However, doing this a couple of times was just enough for me personally to realize I didn't want to have to deal with rebuilding every package that had a security update released on a weekly basis. :)</p>
","87075"
"How do I exit a script in a conditional statement?","74602","","<p>I'm writing a bash script where I want to exit if the user is not root.  The conditional works fine, but the script does not exit.</p>

<pre><code>[[ `id -u` == 0 ]] || (echo ""Must be root to run script""; exit)
</code></pre>

<p>I've tried using <code>&amp;&amp;</code> instead of <code>;</code> but neither work.</p>
","<p>You could do that this way:</p>

<pre><code>[[ $(id -u) -eq 0 ]] || { echo &gt;&amp;2 ""Must be root to run script""; exit 1; }
</code></pre>

<p>(""ordinary"" conditional expression with an arithmetic binary operator in the first statement), or:</p>

<pre><code>(( $(id -u) == 0 )) || { echo &gt;&amp;2 ""Must be root to run script""; exit 1; }
</code></pre>

<p>(arithmetic evaluation for the first test).</p>

<p>Notice the change <code>()</code> -> <code>{}</code> - the curly brackets do <strong>not</strong> spawn a subshell. (Search <code>man bash</code> for ""subshell"".)</p>
","23963"
"Number of processors in /proc/cpuinfo","74577","","<p>While I was learning about cpu load, I came to know that it depends on the number of cores. If I have 2 cores then load 2 will give 100% cpu utilization.</p>

<p>So I tried to find out cores.( I already know that system has 2 cores, 4 threads so 2 virtual cores <a href=""http://ark.intel.com/products/75460/Intel-Core-i7-4500U-Processor-4M-Cache-up-to-3_00-GHz"" rel=""nofollow noreferrer"">Check here about processor</a>).So I ran <code>cat /proc/cpuinfo</code>
Which gave me </p>

<pre><code>processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 69
model name  : Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz
stepping    : 1
microcode   : 0x17
cpu MHz     : 774.000
cache size  : 4096 KB
physical id : 0
siblings    : 4
core id     : 0
cpu cores   : 2
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 3591.40
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 69
model name  : Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz
stepping    : 1
microcode   : 0x17
cpu MHz     : 1600.000
cache size  : 4096 KB
physical id : 0
siblings    : 4
core id     : 0
cpu cores   : 2
apicid      : 1
initial apicid  : 1
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 3591.40
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:

processor   : 2
vendor_id   : GenuineIntel
cpu family  : 6
model       : 69
model name  : Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz
stepping    : 1
microcode   : 0x17
cpu MHz     : 800.000
cache size  : 4096 KB
physical id : 0
siblings    : 4
core id     : 1
cpu cores   : 2
apicid      : 2
initial apicid  : 2
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 3591.40
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:

processor   : 3
vendor_id   : GenuineIntel
cpu family  : 6
model       : 69
model name  : Intel(R) Core(TM) i7-4500U CPU @ 1.80GHz
stepping    : 1
microcode   : 0x17
cpu MHz     : 774.000
cache size  : 4096 KB
physical id : 0
siblings    : 4
core id     : 1
cpu cores   : 2
apicid      : 3
initial apicid  : 3
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 fma cx16 xtpr pdcm pcid sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 3591.40
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:
</code></pre>

<p>Now I am totally confused. It shows 4 processors, with 2 cpu cores.
Can anyone explain this output? </p>

<p>Once my cpu load was 3.70, Is this maximum load? Still at that time cpu was at &lt;50%.</p>

<p>What about turbo boost? Are all cores are turbo boosted or only physical?</p>

<p>Any method in Ubuntu to get current cpu frequency to see if the processor is on turbo boost or not?</p>

<p>Load was to 3.70 about 100%. But CPU usage wasn't 100% because of IO response time. This does not means that IO device will be at maximum speed, but io device will be 100% busy, which sometimes affects applications using IO ex: music may break.</p>
","<p>The words “CPU”, “processor” and “core” are used in somewhat confusing ways. They refer to the processor architecture. A core is the smallest independent unit that implements a general-purpose processor; a processor is an assemblage of cores (on some ARM systems, a processor is an assemblage of clusters which themselves are assemblages of cores). A chip can contain one or more processors (x86 chips contain a single processor, in this sense of the word <em>processor</em>).</p>

<p>Hyperthreading means that some parts of a core are duplicated. A core with hyperthreading is sometimes presented as an assemblage of two “virtual cores” — meaning not that each core is virtual, but that the plural is virtual because these are not actually separate cores and they will sometimes have to wait while the other core is making use of a shared part.</p>

<p>As far as software is concerned, there is only one concept that's useful almost everywhere: the notion of parallel threads of execution. So in most software manuals, the terms <em>CPU</em> and <em>processor</em> are used to mean any one piece of hardware that executes program code. In hardware terms, this means one core, or one virtual core with hyperthreading.</p>

<p>Thus <code>top</code> shows you 4 CPUs, because you can have 4 threads executing at the same time. <code>/proc/cpuinfo</code> has 4 entries, one for each CPU (in that sense). The <code>processor</code> numbers (which are the number of the <code>cpu<em>NUMBER</em></code> entries in <code>/sys/devices/system/cpu</code>) correspond to these 4 threads.</p>

<p><code>/proc/cpuinfo</code> is one of the few places where you get information about what hardware implements these threads of execution:</p>

<blockquote>
<pre><code>physical id : 0
siblings    : 4
core id     : 0
cpu cores   : 2
</code></pre>
</blockquote>

<p>means that <code>cpu0</code> is one of 4 threads inside physical component (processor) number 0, and that's in core 0 among 2 in this processor.</p>
","146240"
"Delete last line from the file","74571","","<p>I use <code>sed</code>  to quickly delete lines with specific position as </p>

<pre><code>sed '1d'
sed '5d'
</code></pre>

<p>But, what if I want to delete the last line of the file and I don't know the count of lines (I know I can get that using <code>wc</code> and several other tricks). </p>

<p>Currently, using a workaround with <code>head</code> and <code>tail</code>combined with <code>wc</code> to do so. Any quick twists here?</p>
","<p>in sed <code>$</code> is the last line so to delete the last line:</p>

<pre><code>sed '$d' &lt;file&gt;
</code></pre>
","52783"
"Best way to free disk space from deleted files that are held open","74511","","<p>Hi I have many files that have been deleted but for some reason the disk space associated with the deleted files is unable to be utilized until I explicitly kill the process for the file taking the disk space</p>

<pre><code>$ lsof /tmp/
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF      NODE NAME
cron     1623 root    5u   REG   0,21        0 395919638 /tmp/tmpfPagTZ4 (deleted)
</code></pre>

<p>The disk space taken up by the deleted file above causes problems such as when trying to use the tab key to autocomplete a file path I get the error <code>bash: cannot create temp file for here-document: No space left on device</code></p>

<p>But after I run <code>kill -9 1623</code> the space for that PID is freed and I no longer get the error.</p>

<p>My questions are:</p>

<ul>
<li>why is this space not immediately freed when the file is first deleted?</li>
<li>what is the best way to get back the file space associated with the deleted files?</li>
</ul>

<p>and please let me know any incorrect terminology I have used or any other relevant and pertinent info regarding this situation.</p>
","<p>On unices, filenames are just pointers (inodes) that point to the memory where the file resides (which can be a hard drive or even a RAM-backed filesystem). Each file records the number of links to it: the links can be either the filename (plural, if there are multiple hard links to the same file), and also every time a file is opened, the process actually holds the ""link"" to the same space. </p>

<p>The space is physically freed only if there are no links left (therefore, it's impossible to get to it). That's the only sensible choice: while the file is being used, it's not important if someone else can no longer access it: you are using it and until you close it, you still have control over it - you won't even notice the filename is gone or moved or whatever. That's even used for tempfiles: some implementations create a file and immediately unlink it, so it's not visible in the filesystem, but the process that created it is using it normally. Flash plugin is especially fond of this method: all the downloaded video files are held open, but the filesystem doesn't show them.</p>

<p>So, the answer is, while the processes have the files still opened, you <em>shouldn't</em> expect to get the space back. It's not freed, it's being actively used. This is also one of the reasons that applications should really close the files when they finish using them. In normal usage, you shouldn't think of that space as free, and this also shouldn't be very common at all - with the exception of temporary files that are unlinked on purpose, there shouldn't really be any files that you would consider being unused, but still open. Try to review if there is a process that does this a lot and consider how you use it, or just find more space.</p>
","182082"
"How can I copy/paste data to and from the Windows clipboard to an OpenSuse clipboard using VNC?","74471","","<p>I am connecting to a remote OpenSuse computer using VNC from Windows. On other Linuces that I've used over VNC, I can copy text between the remote machine and the local Windows machine using the clipboard. In this case, I can seem to connect the local clipboard to the remote.</p>

<p>The VNC server is whatever is set by default on OpenSuse. I don't know which it is.</p>

<p>How can I fix copy/paste over VNC?</p>
","<p>I resolved this by installing the <code>autocutsel</code> RPM from the software management section of Yast, and then running:</p>

<pre><code>$ autocutsel -s PRIMARY -fork
</code></pre>

<p>This enabled copy/paste between my VNC and my Windows clipboard.</p>

<p>Thanks to <a href=""http://thomas.patrickdepinguin.com/knowledgebase/tight-vnc"" rel=""noreferrer"">this source</a>.</p>
","35275"
"How to find a space in a text. using grep?","74415","","<p>How to grep one space in text ?</p>

<pre><code>cat a.txt| grep ' '
</code></pre>

<p>or </p>

<pre><code>cat a.txt| grep '\s '
</code></pre>
","<p>I think I found it:</p>

<pre><code>cat a.xml | grep  ""\+[[:space:]]\+""
</code></pre>
","82167"
"How do I remove all sub-directories from within a directory?","74370","","<p>This question is kind of a phase II to the first question I posted at <a href=""https://unix.stackexchange.com/questions/68489/command-to-zip-multiple-directories-into-individual-zip-files"">here</a></p>

<p>I have a directory that contains a bunch of sub-directories, .zip files, and other random files not contained within a sub-directory.</p>

<p>I'd like a command line script to remove all sub-directories from within the parent directory, but keep all zip files and loose files that don't belong to any sub-directories. All of the sub-directories have content, so I believe I'd need to force their deletion with the -f command.</p>

<p>So basically, a command that looks inside the parent directory (or the current directory), deletes all folders from within it, but keeps all other content and files that are not a folder or contained within a folder.</p>

<p>I understand that deleting items from the command line requires special care, but I have already taken all necessary precautions to back up remotely.</p>
","<p>In BASH you can use the trailing slash (I <em>think</em> it should work in any POSIX shell):</p>

<pre><code>rm -R -- */
</code></pre>

<p>Note the <code>--</code> which separates options from arguments and allows one to remove entries starting with a hyphen - otherwise after expansion by the shell the entry name would be interpreted as an option by <code>rm</code> (the same holds for many other command line utilities).</p>

<p>Add the <code>-f</code> option if you don't want to be prompted for confirmation when deleting non-writeable files.</p>

<p>Note that by default, hidden directories (those whose name starts with <code>.</code>) will be left alone.</p>

<p>An important caveat: the expansion of <code>*/</code> will also include symlinks that eventually resolve to files of type <em>directory</em>. And depending on the <code>rm</code> implementation, <code>rm -R -- thelink/</code> will either just delete the symlink, or (in most of them) delete the <strong>content</strong> of the linked directory recursively but not that directory itself nor the symlink.</p>

<p>If using <code>zsh</code>, a better approach would be to use a <em>glob qualifier</em> to select files of type directory only:</p>

<pre><code>rm -R -- *(/) # or *(D/) to include hidden ones
</code></pre>

<p>or:</p>

<pre><code>rm -R -- *(-/)
</code></pre>

<p>to include symlinks to directories (but because, this time, the expansion doesn't have trailing <code>/</code>s, it's the symlink only that is removed with all <code>rm</code> implementations).</p>

<p>With <code>bash</code>, AT&amp;T <code>ksh</code>, <code>yash</code> or <code>zsh</code> you can do:</p>

<pre><code>set -- */
rm -R -- ""${@%/}""
</code></pre>

<p>to strip the trailing <code>/</code>.</p>
","68847"
"wget with wildcards in http downloads","74322","","<p>I need to download a file using wget, however I don't know exactly what the file name will be. </p>

<pre><code>https://foo/bar.1234.tar.gz
</code></pre>

<p>According to the <a href=""http://www.gnu.org/software/wget/manual/wget.html#index-accept-wildcards-143"">man page</a>, wget lets you turn off and on globbing when dealing with a ftp site, however I have a http url. </p>

<p>How can I use a wildcard while using a wget? I'm using gnu wget. </p>

<p>Things I've tried. </p>

<pre><code>/usr/local/bin/wget -r ""https://foo/bar.*.tar.gz"" -P /tmp
</code></pre>

<p><strong>Update</strong>  </p>

<p>Using the -A causes all files ending in .tar.gz on the server to be downloaded. </p>

<pre><code>/usr/local/bin/wget -r ""https://foo/"" -P /tmp -A ""bar.*.tar.gz""
</code></pre>

<p><strong>Update</strong></p>

<p>From the answers, this is the syntax which eventually worked. </p>

<pre><code>/usr/local/bin/wget -r -l1 -np ""https://foo"" -P /tmp -A ""bar*.tar.gz""
</code></pre>
","<p>I think these switches will do what you want with <code>wget</code>:</p>

<pre><code>   -A acclist --accept acclist
   -R rejlist --reject rejlist
       Specify comma-separated lists of file name suffixes or patterns to 
       accept or reject. Note that if any of the wildcard characters, *, ?,
       [ or ], appear in an element of acclist or rejlist, it will be 
       treated as a pattern, rather than a suffix.

   --accept-regex urlregex
   --reject-regex urlregex
       Specify a regular expression to accept or reject the complete URL.
</code></pre>

<h3>Example</h3>

<pre><code>$ wget -r --no-parent -A 'bar.*.tar.gz' http://url/dir/
</code></pre>
","117998"
"What is the Fedora equivalent of the Debian build-essential package?","74285","","<p>What is the Fedora equivalent of the Debian build-essential package? </p>
","<p>The closest equivalent would probably be to install the below packages:</p>

<pre><code>su -    
yum install make automake gcc gcc-c++ kernel-devel
</code></pre>

<p>However, if you don't care about exact equivalence and are ok with pulling in a lot of packages you can install all the development tools and libraries with the below command.</p>

<pre><code>su -
yum groupinstall ""Development Tools"" ""Development Libraries""
</code></pre>
","1344"
"rsync all files of remote machine over SSH without root user?","74233","","<p>I have this command to backup a remote machine. The problem is that I need root rights to read and copy all files. I have no root user enabled for security reasons and use <code>sudo</code> the Ubuntu way. Would I need some cool piping or something to do this?</p>

<pre><code>rsync -chavzP --stats user@192.168.1.2:/ .
</code></pre>
","<p>I would recommend that you just use the root account in the first place. If you set it up like this:</p>

<ul>
<li>Configure your <code>sshd_config</code> on the target machine to <code>PermitRootLogin without-password</code>.</li>
<li>Use <code>ssh-keygen</code> on the machine that pulls the backup to create an SSH private key (only if you don't already have an SSH key). Do not set a passphrase. Google a tutorial if you need details for this, there should be plenty.</li>
<li>Append the contents of <code>/root/.ssh/id_rsa.pub</code> of the backup machine to the <code>/root/.ssh/authorized_keys</code> of your target machine.</li>
<li>Now your backup machine has root access to your target machine, without having to use password authentication.</li>
</ul>

<p>then the resulting setup should be pretty safe.</p>

<hr>

<p><code>sudo</code>, especially combined with <code>NOPASSWD</code> as recommended in the comments, has no security benefits over just using the root account. For example this suggestion:</p>

<blockquote>
  <p>add the following to your <code>/etc/sudoers</code> file: <code>rsyncuser ALL= NOPASSWD:/usr/bin/rsync</code></p>
</blockquote>

<p>essentially gives <code>rsyncuser</code> root permissions anyway. You ask:</p>

<blockquote>
  <p>@MartinvonWittich Easy to gain a full root shell because <code>rsync</code> executed with <code>sudo</code>? Walk [m]e [through] that please.</p>
</blockquote>

<p>Well, simple. With the recommended configuration, <code>rsyncuser</code> may now run <code>rsync</code> as root without even being asked for a password. <code>rsync</code> is a very powerful tool to manipulate files, so now <code>rsyncuser</code> has a very powerful tool to manipulate files with root permissions. Finding a way to exploit this took me just a few minutes (tested on Ubuntu 13.04, requires <code>dash</code>, <code>bash</code> didn't work):</p>

<pre><code>martin@martin ~ % sudo rsync --perms --chmod u+s /bin/dash /bin/rootdash
martin@martin ~ % rootdash
# whoami
root
# touch /etc/evil
# tail -n1 /etc/shadow
dnsmasq:*:15942:0:99999:7:::
</code></pre>

<p>As you can see, I have created myself a root shell; <code>whoami</code> identifies my account as root, I can create files in <code>/etc</code>, and I can read from <code>/etc/shadow</code>. My exploit was to set the <a href=""http://en.wikipedia.org/wiki/Setuid"">setuid</a> bit on the <code>dash</code> binary; it causes Linux to always run that binary with the permissions of the owner, in this case root.</p>

<blockquote>
  <p>Having a real root is not [recommended] for good reasons. – redanimalwar 15 hours ago</p>
</blockquote>

<p>No, clumsily working around the root account in situations where it is absolutely appropriate to use it is not for good reasons. This is just another form of <a href=""http://en.wikipedia.org/wiki/Cargo_cult_programming"">cargo cult programming</a> - you don't really understand the concept behind sudo vs root, you just blindly apply the belief ""root is bad, sudo is good"" because you've read that somewhere.</p>

<p>On the one hand, there are situations where <code>sudo</code> is definitely the right tool for the job. For example, when you're interactively working on a graphical Linux desktop, let's say Ubuntu, then having to use <code>sudo</code> is fine in those rare cases where you sometimes need root access. Ubuntu intentionally has a disabled root account and forces you to use <code>sudo</code> by default to prevent users from just always using the root account to log in. When the user just wants to use e.g. the web browser, then logging in as root would be a dangerous thing, and therefore not having a root account by default prevents stupid people from doing this.</p>

<p>On the other hand, there are situations like yours, where an automated script requires root permissions to something, for example to make a backup. Now using <code>sudo</code> to work around the root account is not only pointless, it's also dangerous: at first glance <code>rsyncuser</code> looks like an ordinary unprivileged account. But as I've already explained, it would be very easy for an attacker to gain full root access if he had already gained <code>rsyncuser</code> access. So essentially, you now have an additional root account that doesn't look like a root account at all, which is not a good thing.</p>
","92397"
"Shell: How to read the bytes of a binary file and print as hexadecimal?","74190","","<p>In shell, how can I read the bytes of a binary file I have, and print the output as hexadecimal numbers?</p>
","<p>Use <code>hexdump(1)</code></p>

<pre><code>$ hexdump -x /usr/bin/hexdump 
0000000    feca    beba    0000    0300    0001    0700    0080    0300
0000010    0000    0010    0000    5080    0000    0c00    0000    0700
0000020    0000    0300    0000    00a0    0000    b06f    0000    0c00
0000030    0000    1200    0000    0a00    0100    0010    0000    107c
0000040    0000    0c00    0000    0000    0000    0000    0000    0000
0000050    0000    0000    0000    0000    0000    0000    0000    0000
</code></pre>

<p>...</p>
","10829"
"Purpose and typical usage of /etc/rc.local","74179","","<p>The header looks like this:</p>

<pre><code>#!/bin/sh -e
#
# rc.local - executed at the end of each multiuser runlevel
#
# Make sure that the script will ""exit 0"" on success or any other
# value on error.
</code></pre>

<p>What is the reason for this file (it does not contain much), and what commands do you usually put in it? What is a ""multiuser runlevel""? (I guess <code>rc</code> is ""run commands""?)</p>
","<p>A <a href=""http://en.wikipedia.org/wiki/Runlevel"" rel=""noreferrer"">runlevel</a> is a state of the system, indicating whether it is in the process of booting or rebooting or shutting down, or in single-user mode, or running normally. The traditional <a href=""http://en.wikipedia.org/wiki/Init"" rel=""noreferrer"">init</a> program handles these actions by switching to the corresponding runlevel. Under Linux, the runlevels are by convention: S while booting, 0 while shutting down, 6 while rebooting, 1 in single-user mode and 2 through 5 in normal operation. Runlevels 2 through 5 are known as multiuser runlevels since they allow multiple users to log in, unlike runlevel 1 which is intended for only the system administrator.</p>

<p>When the runlevel changes, init runs <a href=""https://unix.stackexchange.com/questions/30728/whats-the-difference-between-etc-rc-d-rc-d-and-etc-rc-d"">rc scripts</a> (on systems with a traditional init — there are alternatives, such as <a href=""http://en.wikipedia.org/wiki/Upstart"" rel=""noreferrer"">Upstart</a> and <a href=""http://en.wikipedia.org/wiki/Systemd"" rel=""noreferrer"">Systemd</a>). These rc scripts typically start and stop system services, and are provided by the distribution.</p>

<p>The script <code>/etc/rc.local</code> is for use by the system administrator. It is traditionally executed after all the normal system services are started, at the end of the process of switching to a multiuser runlevel. You might use it to start a custom service, for example a server that's installed in <code>/usr/local</code>. Most installations don't need <code>/etc/rc.local</code>, it's provided for the minority of cases where it's needed.</p>
","49635"
"Is a restart of cron or crond necessary after each new schedule addition or modification?","74169","","<p>When I schedule a job, some seem to be applied immediately, while others after a reboot. So is it recommended to restart <code>cron</code> (<code>crond</code>) after adding a new cron job? How to do that properly (esp. in a Debian system), and should that be done with <code>sudo</code> (like <code>sudo service cron restart</code>) even for that of normal users'?</p>

<p>I tried:</p>

<pre><code>/etc/init.d/cron restart
</code></pre>

<p>which doesn't seem to work (neither does <code>/etc/init.d/cron stop</code> or <code>service cron stop</code>) and completes with return code 1.</p>

<p>Here's a part of the message output:</p>

<blockquote>
  <p>Since the script you are attempting to invoke has been converted to an
  Upstart job, you may also use the stop(8) utility, e.g. stop cron
  stop: Rejected send message, 1 matched rules; type=""method_call"", sender="":1.91"" (uid=1000 pid=3647 comm=""stop cron "") interface=""com.ubuntu.Upstart0_6.Job"" member=""Stop"" error name=""(unset)""  requested_reply=""0"" destination=""com.ubuntu.Upstart"" (uid=0 pid=1 comm=""/sbin/init"")</p>
</blockquote>

<p><em>(what does that mean?)</em></p>
","<p>No you don't have to restart <code>cron</code>, it will notice the changes to your crontab files (either <code>/etc/crontab</code> or  a users crontab file).</p>

<p>At the top of your /etc/crontab you probably have (if you have the Vixie implementation of <code>cron</code> that IIRC is the one on Debian):</p>

<pre><code># /etc/crontab: system-wide crontab
# Unlike any other crontab you don't have to run the `crontab'
# command to install the new version when you edit this file
# and files in /etc/cron.d. These files also have username fields,
# that none of the other crontabs do.
</code></pre>

<p>The reason you might not see specific changes implemented is if you add things to e.g. <code>/etc/cron.daily</code> and the daily run has already occurred. </p>

<p>The message that you get is because you use an old way of restarting cron on your system. The recommended way (but not necessary if you just edit cron files) is:</p>

<pre><code> restart cron
</code></pre>

<p><em>You of course have to reboot in order to see the effects of a @reboot cron job</em></p>
","111622"
"`cp` permission denied when copy a file owned by `root`","74097","","<p>I have a folder <code>udp_folder2</code></p>

<pre><code>d------r-T 41 root           root     4096 Apr 26 21:17 udp_folder2
</code></pre>

<p>when I'm with user other than <code>root</code>, I can't <code>cp -r</code> it into a new folder
it says: Permission denied</p>

<p>why? and how can I copy it with a user other than <code>root</code></p>
","<p>Well, </p>

<p>That would be because the way your current permissions are set, no one can move that file. ( Other than root, because root doesn't follow the same rules. )</p>

<p>You would need to either change the owner of the file (chown), OR add the other user to the group 'root' and chmod it so the group can execute on the directory, OR allow everyone else to execute the file. </p>

<p>So, a quick fix would be:</p>

<pre><code>chmod -R o+rwx udp_folder2
</code></pre>

<p>That will give everyone the ability to read, write and execute on that directory. </p>

<p>Also... if you're attempting to copy 'udp_folder2' into the same directory that it is located now, you'll need the 'w' permission on that directory as well. For example:</p>

<p>/foo/udp_folder2 - you'll need 'w' on /foo to copy that directory in /foo</p>

<p>I'd suggest learning linux file permissions:
<a href=""http://www.linux.com/learn/tutorials/309527-understanding-linux-file-permissions"">Linux File Permission Tutorial</a></p>
","73841"
"Day of week {0-7} in crontab has 8 options, but we have only 7 days in a week","73995","","<p><strong>Day-of-week: Allowed range 0 – 7. Sunday is either 0 or 7.</strong></p>

<p>I found this after Googling, my question is why should both values (0,7) correspond to Sunday?</p>
","<p>This is a matter of portability. In early Unices, some versions of cron accepted 0 as Sunday, and some accepted 7 as Sunday -- this format is an attempt to be portable with both. From <code>man 5 crontab</code> in vixie-cron (emphasis my own):</p>

<blockquote>
  <p>When specifying day of week, both day 0 and day 7  will   be  considered
  Sunday.   <em>BSD and AT&amp;T seem to disagree about this.</em></p>
</blockquote>
","106009"
"How to change the order of the network cards (eth1 <-> eth0) on linux","73971","","<p>Is there any way to swap network interfaces (<em>eth1</em> &lt;-> <em>eth0</em>) after system installation.</p>

<p>My brand new Debian 6.0 install assigned PCI network card as ""<em>eth0</em>"" and motherboards integrated network device as ""<em>eth1</em>"" by default. The problem is I want to use the integrated device as default (<em>eth0</em>) network interface.</p>

<p>I already edited :</p>

<blockquote>
  <p><strong>/etc/udev/rules.d/70-persistent-net.rules</strong></p>
</blockquote>

<p>to swap the names and everything seems to be ok and network is working but programs are still trying to use the PCI network card (which is now ""<em>eth1</em>"") as the default interface. For example <em>iftop</em> now tries to use ""<em>eth1</em>"" as default device as it used ""<em>eth0</em>"" before the swap.</p>

<p>Is this purely a software problem as the applications are trying to use the first found device as a default device despite their interface naming or is there any way to fix this by configuring OS?</p>

<hr>

<p><em>edit:</em> I wrote a small app to print out iflist and the PCI device (<em>eth1</em>) came up before ""<em>eth0</em>"". Any ideas how to swap the device order.</p>

<hr>

<p><em>edit:</em> I found a <a href=""http://www.mail-archive.com/debian-isp@lists.debian.org/msg13466.html""><strong>thread</strong></a> about the same problem and I tried everything they suggested and none of the solutions are working except for swapping the names ""virtually"".</p>
","<p>I am answering to my own question now because I finally found a workaround for this problem.</p>

<p>I found out that it is possible to reorder the devices by unloading the drivers and then loading them in correct order.</p>

<h2>First method (bruteforce):</h2>

<p>So the first method I came up with was simple to bruteforce the driver reload with init.d script.</p>

<p>Following init script is tailored for Debian 6.0, but the same principle should work on almost any distribution using proper init.d scripts.</p>

<pre><code>#!/bin/sh -e

### BEGIN INIT INFO
# Provides:          reorder-nics
# Required-Start:
# Required-Stop:
# Default-Start:     S
# Default-Stop:
# Short-Description: Reloads the nics in correct order
### END INIT INFO

#
# This script should reload the nic drivers in corrected order.
# Basically it just unloads and then loads the drivers in different order.
#

echo ""Reloading NICs!""

# unload the drivers
modprobe -r driver_0        # eth0 nic interface
modprobe -r driver_1        # eth1 nic interface

# load the drivers in corrected order
modprobe driver_1
modprobe driver_0

#EOF
</code></pre>

<p>Then the script must be added to proper runlevel directory. This can be done easily on Debian with ""<em>update-rc.d</em>"" command. For example: <code>update-rc.d reorder-nics start S</code></p>

<hr>

<h2>Second method (Better I think):</h2>

<p>I also found a bit more elegant way (at least for Debian &amp; Ubuntu systems).</p>

<p>First Make sure that kernel doesn't automatically load the NIC drivers. This can be done by creating a blacklist file in <strong><code>/etc/modprobe.d/</code></strong>. I created a file named ""<code>disable-nics.conf</code>"". Note that files in <code>/etc/modprobe.d/</code> must have <code>.conf</code> suffix. Also naming modules in <code>/etc/modprobe.d/blacklist.conf</code> do not affect autoloading of modules by the kernel, so you have to make your own file.</p>

<pre><code># Disable automatic loading of kernel driver modules
# Disable NIC drivers

blacklist driver_0     # eth0 by default
blacklist driver_1     # eth1 by default
</code></pre>

<p>Then run '<em>depmod -ae</em>' as root</p>

<p>Recreate your initrd with '<em>update-initramfs -u</em>'</p>

<p>And finally add the driver names in corrected order into <strong>/etc/modules</strong> file.</p>

<pre><code># /etc/modules: kernel modules to load at boot time.
#
# This file contains the names of kernel modules that should be loaded
# at boot time, one per line. Lines beginning with ""#"" are ignored.
# Parameters can be specified after the module name.

# drivers in wanted order
driver_1    # this one should be loaded as eth0
driver_0    # this one should be loaded as eth1
</code></pre>

<p>Changes should come in effect after the next boot.</p>

<p>Reboot is not necessary though; it's easy to switch the devices with following command (as root, of course):</p>

<pre><code>modprobe -r driver_0; modprobe -r driver_1; modprobe driver_1; modprobe driver_0
</code></pre>

<hr>

<p>Some useful links I found while searching the solution:</p>

<ul>
<li><a href=""http://www.macfreek.nl/mindmaster/Logical_Interface_Names"" rel=""nofollow noreferrer"">http://www.macfreek.nl/mindmaster/Logical_Interface_Names</a></li>
<li><a href=""http://wiki.debian.org/KernelModuleBlacklisting"" rel=""nofollow noreferrer"">http://wiki.debian.org/KernelModuleBlacklisting</a></li>
<li><a href=""http://www.science.uva.nl/research/air/wiki/LogicalInterfaceNames"" rel=""nofollow noreferrer"">http://www.science.uva.nl/research/air/wiki/LogicalInterfaceNames</a></li>
</ul>
","10299"
"Remove multiple lines after a pattern match but only after pattern match","73925","","<p>I am trying to remove all instances of a pattern match from a file if it matches a pattern. If there is a match, the (complete) line with the matching pattern and the next line get removed. </p>

<p>The next line always appears after the line with the pattern match, but in addition it appears in other areas of the file.</p>

<p>I am using grep and it is deleting all occurrences of the next line in the file, as expected.</p>

<p>Is there a way I can remove that next line if and only if it is after the line with the pattern match?</p>
","<p>You can use <code>sed</code> with the <a href=""https://www.gnu.org/software/sed/manual/sed.html#index-N-_0028append-Next-line_0029-command-155""><code>N</code></a> and <a href=""https://www.gnu.org/software/sed/manual/sed.html#index-d-_0028delete_0029-command-93""><code>d</code></a> commands and a <a href=""https://www.gnu.org/software/sed/manual/sed.html#index-Grouping-commands-101""><code>{}</code> block</a>:</p>

<pre><code>sed -e '/pattern here/ { N; d; }'
</code></pre>

<p>For every line that matches <code>pattern here</code>, the code in the <code>{}</code> gets executed. <code>N</code> takes the next line into the pattern space as well, and then <code>d</code> deletes the whole thing before moving on to the next line. This works in any POSIX-compatible <code>sed</code>.</p>
","152316"
"How do I know which version of Debian I'm running?","73684","","<p>In a tutorial, I'm prompted <em>""If you are running Squeeze, follow these instructions...""</em> and <em>""If you are running Wheezy, follow these other instructions...""</em></p>

<p>When I run <code>uname</code>, I get the following information:</p>

<pre><code>Linux dragon-debian 3.2.0-4-686-pae #1 SMP Debian 3.2.63-2+deb7u2 i686 GNU/Linux
</code></pre>

<p>Is that information enough to know if I'm using <em>Squeeze</em> or <em>Wheezy</em>, or do I get that from somewhere else?</p>
","<p>Commands to try:</p>

<p>• <code>cat /etc/*-release</code></p>

<p>• <code>cat /proc/version</code></p>

<p>• <code>lsb_release -a</code><br>
<em>- this shows ""certain LSB (Linux Standard Base) and distribution-specific information""</em>.</p>

<p>For a shell script to get the details on different platforms, there's <a href=""https://unix.stackexchange.com/questions/6345/how-can-i-get-distribution-name-and-version-number-in-a-simple-shell-script"">this</a> related question.</p>
","177206"
"What are the pros/cons of Upstart and systemd?","73565","","<p>It appears <a href=""http://freedesktop.org/wiki/Software/systemd"">systemd</a> is the hot new <a href=""http://en.wikipedia.org/wiki/Init"">init</a> system on the block, same as <a href=""http://upstart.ubuntu.com/"">Upstart</a> was a few years ago. What are the pros/cons for each? Also,  how does each compare to other init systems?</p>
","<h1>2016 Update</h1>

<p>Most answers here are five years old so it's time for some updates.</p>

<p>Ubuntu used to use upstart by default but they abandoned it last year in favor of systemd - see:</p>

<ul>
<li><a href=""http://www.theregister.co.uk/2015/03/07/ubuntu_to_switch_to_systemd/"" rel=""noreferrer"">Grab your pitchforks: Ubuntu to switch to systemd on Monday</a> (The Register)</li>
</ul>

<p>Because of that there is a nice article <a href=""https://wiki.ubuntu.com/SystemdForUpstartUsers"" rel=""noreferrer"">Systemd for Upstart Users</a> on Ubuntu wiki - very detailed comparison between upstart and systemd and a transition guide from upstart to systemd.</p>

<p>(Note that <a href=""https://wiki.ubuntu.com/SystemdForUpstartUsers#Permanent_switch_back_to_upstart"" rel=""noreferrer"">according to the Ubuntu wiki</a> you can still run upstart on current versions of Ubuntu by default by installing the <code>upstart-sysv</code> and running <code>sudo update-initramfs -u</code> but considering the scope of the systemd project I don't know how it works in practice, or whether or not systemd is possible to uninstall.)</p>

<p>Most of the info in the Commands and Scripts sections below is adapted from some of the examples used in that article (that is conveniently licensed just like Stack Exchange user contributions under the <a href=""https://creativecommons.org/licenses/by-sa/3.0/"" rel=""noreferrer"">Creative Commons Attribution-ShareAlike 3.0 License</a>).</p>

<p>Here is a quick comparison of common commands and simple scripts, see sections below for detailed explanation. This answer is comparing the old behavior of Upstart-based systems with the new behavior of systemd-based systems, as asked in the question, but note that the commands tagged as ""Upstart"" are not necessarily Upstart-specific - they are often commands that are common to every non-systemd Linux and Unix system. </p>

<h2>Commands</h2>

<h3>Running su:</h3>

<ul>
<li><strong>upstart:</strong> <code>su</code></li>
<li><strong>systemd:</strong> <code>machinectl shell</code></li>
</ul>

<p>(see ""su command replacement"" section below)</p>

<h3>Running screen:</h3>

<ul>
<li><strong>upstart:</strong> <code>screen</code></li>
<li><strong>systemd:</strong> <code>systemd-run --user --scope screen</code></li>
</ul>

<p>(see ""Unexpected killing of background processes"" section below)</p>

<h3>Running tmux:</h3>

<ul>
<li><strong>upstart:</strong> <code>tmux</code></li>
<li><strong>systemd:</strong> <code>systemd-run --user --scope tmux</code></li>
</ul>

<p>(see ""Unexpected killing of background processes"" section below)</p>

<h3>Starting job foo:</h3>

<ul>
<li><strong>upstart:</strong> <code>start foo</code></li>
<li><strong>systemd:</strong> <code>systemctl start foo</code></li>
</ul>

<h3>Stopping job foo:</h3>

<ul>
<li><strong>upstart:</strong> <code>stop foo</code></li>
<li><strong>systemd:</strong> <code>systemctl stop foo</code></li>
</ul>

<h3>Restarting job foo:</h3>

<ul>
<li><strong>upstart:</strong> <code>restart foo</code></li>
<li><strong>systemd:</strong> <code>systemctl restart foo</code></li>
</ul>

<h3>Listing jobs:</h3>

<ul>
<li><strong>upstart:</strong> <code>initctl list</code></li>
<li><strong>systemd:</strong> <code>systemctl status</code></li>
</ul>

<h3>Checking configuration of job foo:</h3>

<ul>
<li><strong>upstart:</strong> <code>init-checkconf /etc/init/foo.conf</code></li>
<li><strong>systemd:</strong> <code>systemd-analyze verify /lib/systemd/system/foo.service</code></li>
</ul>

<h3>Listing job's environement variables:</h3>

<ul>
<li><strong>upstart:</strong> <code>initctl list-env</code></li>
<li><strong>systemd:</strong> <code>systemctl show-environment</code></li>
</ul>

<h3>Setting job's environment variable:</h3>

<ul>
<li><strong>upstart:</strong> <code>initctl set-env foo=bar</code></li>
<li><strong>systemd:</strong> <code>systemctl set-environment foo=bar</code></li>
</ul>

<h3>Removing job's environment variable:</h3>

<ul>
<li><strong>upstart:</strong> <code>initctl unset-env foo</code></li>
<li><strong>systemd:</strong> <code>systemctl unset-environment foo</code></li>
</ul>

<h2>Logs</h2>

<p>In upstart, the logs are normal text files in the /var/log/upstart directory, so you can process them as usual:</p>

<pre><code>cat /var/log/upstart/foo.log
tail -f /var/log/upstart/foo.log
</code></pre>

<p>In systemd logs are stored in an internal binary format (not as text files) so you need to use <code>journalctl</code> command to access them:</p>

<pre><code>sudo journalctl -u foo
sudo journalctl -u foo -f
</code></pre>

<h2>Scripts</h2>

<p>Example <strong>upstart script</strong> written in <code>/etc/init/foo.conf</code>:</p>

<pre><code>description ""Job that runs the foo daemon""
start on runlevel [2345]
stop on runlevel [016]
env statedir=/var/cache/foo
pre-start exec mkdir -p $statedir
exec /usr/bin/foo-daemon --arg1 ""hello world"" --statedir $statedir
</code></pre>

<p>Example <strong>systemd script</strong> written in <code>/lib/systemd/system/foo.service</code>:</p>

<pre><code>[Unit]
Description=Job that runs the foo daemon
Documentation=man:foo(1)
[Service]
Type=forking
Environment=statedir=/var/cache/foo
ExecStartPre=/usr/bin/mkdir -p ${statedir}
ExecStart=/usr/bin/foo-daemon --arg1 ""hello world"" --statedir ${statedir}
[Install]
WantedBy=multi-user.target
</code></pre>

<h2>su command replacement</h2>

<p>A <code>su</code> command replacement was merged into systemd in pull request #1022:</p>

<ul>
<li><a href=""https://github.com/systemd/systemd/pull/1022"" rel=""noreferrer"">Add new ""machinectl shell"" command for su(1)-like behaviour</a></li>
</ul>

<p>because, according to Lennart Poettering, <a href=""https://github.com/systemd/systemd/issues/825#issuecomment-127917622"" rel=""noreferrer"">""su is really a broken concept""</a>.</p>

<p>He explains that <a href=""https://github.com/systemd/systemd/issues/825#issuecomment-127957710"" rel=""noreferrer"">""you can use su and sudo as before, but <strong>don't expect that it will work in full</strong>""</a>.</p>

<p>The official way to achieve a <code>su</code>-like behavior is now:</p>

<pre><code>machinectl shell
</code></pre>

<p>It has been further
<a href=""https://github.com/systemd/systemd/issues/825#issuecomment-127917622"" rel=""noreferrer"">explained by Lennart Poettering</a>
in the discussion to issue #825:</p>

<blockquote>
  <p>""Well, there have been long discussions about this, but the problem is
  that what su is supposed to do is very unclear. [...]
  Long story short:  su is really a broken concept. It will given you
  kind of a shell, and it’s fine to use it for that, but it’s not a full
  login, and shouldn’t be mistaken for one."" - Lennart Poettering</p>
</blockquote>

<p>See also:</p>

<ul>
<li><a href=""https://tlhp.cf/lennart-poettering-su/"" rel=""noreferrer"">Lennart Poettering merged “su” command replacement into systemd: Test Drive on Fedora Rawhide</a></li>
<li><a href=""https://linux.slashdot.org/story/15/08/29/1526217/systemd-absorbs-su-command-functionality"" rel=""noreferrer"">Systemd Absorbs ""su"" Command Functionality</a></li>
<li><a href=""https://news.ycombinator.com/item?id=10141715"" rel=""noreferrer"">Systemd Absorbs “su”</a> (Hacker News)</li>
</ul>

<h2>Unexpected killing of background processes</h2>

<p>Commands like:</p>

<ul>
<li><a href=""https://www.gnu.org/software/screen/"" rel=""noreferrer"">screen</a></li>
<li><a href=""https://tmux.github.io/"" rel=""noreferrer"">tmux</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Nohup"" rel=""noreferrer"">nohup</a></li>
</ul>

<p><strong>no longer work as expected</strong>. For example, <code>nohup</code> is a POSIX command to make sure that the process keeps running after you log out from your session. It <a href=""https://news.ycombinator.com/item?id=11782364"" rel=""noreferrer"">no longer works</a> on systemd. Also programs like <code>screen</code> and <code>tmux</code> need to be invoked in a special way or otherwise <a href=""https://linux.slashdot.org/story/16/05/29/212204/systemd-starts-killing-your-background-processes-by-default"" rel=""noreferrer"">the processes that you run with them will get killed</a> (while not getting those processes killed is usually the main reason of running screen or tmux in the first place).</p>

<p>This is not a mistake, it is a deliberate decision, so it is not likely to get fixed in the future. This is what Lennart Poettering <a href=""https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/message/XW7V5A3RAWYCACU2ZMPA27ARRLIZUI37/"" rel=""noreferrer"">has said</a> about this issue:</p>

<blockquote>
  <p>In my view it was actually quite strange of UNIX that it by default let arbitrary user code stay around unrestricted after logout. It has been discussed for ages now among many OS people, that this should possible but certainly not be the default, but nobody dared so far to flip the switch to turn it from a default to an option. Not cleaning up user sessions after logout is not only ugly and somewhat hackish but also a security problem.  <strong>systemd 230 now finally flipped the switch and finally by default cleans everything up correctly when the user logs out.</strong></p>
</blockquote>

<p>For more info see:</p>

<ul>
<li><a href=""https://linux.slashdot.org/story/16/05/29/212204/systemd-starts-killing-your-background-processes-by-default"" rel=""noreferrer"">Systemd Starts Killing Your Background Processes By Default</a></li>
<li><a href=""https://news.ycombinator.com/item?id=11782364"" rel=""noreferrer"">Systemd v230 kills background processes after user logs out, breaks screen, tmux</a></li>
<li>Debian Bug #825394: <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=825394"" rel=""noreferrer"">systemd kill background processes after user logs out</a></li>
</ul>

<h2>High-level startup concept</h2>

<p>In a way systemd works backwards - in upstart jobs start as soon as they can and in systemd jobs start when they have to. At the end of the day the same jobs can be started by both systems and in pretty much the same order, but you think about it looking from an opposite direction so to speak.</p>

<p>Here is how <a href=""https://wiki.ubuntu.com/SystemdForUpstartUsers"" rel=""noreferrer"">Systemd for Upstart Users</a> explains it:</p>

<blockquote>
  <p><strong>Upstart</strong>'s model for starting processes (jobs) is ""greedy event-based"", i. e. all available jobs whose startup events happen are
  started as early as possible. During boot, upstart synthesizes some
  initial events like startup or rcS as the ""tree root"", the early
  services start on those, and later services start when the former are
  running. A new job merely needs to install its configuration file into
  /etc/init/ to become active.</p>
  
  <p><strong>systemd</strong>'s model for starting processes (units) is ""lazy dependency-based"", i. e. a unit will only start if and when some other
  starting unit depends on it. During boot, systemd starts a ""root unit""
  (default.target, can be overridden in grub), which then transitively
  expands and starts its dependencies. A new unit needs to add itself as
  a dependency of a unit of the boot sequence (commonly
  multi-user.target) in order to become active.</p>
</blockquote>

<h2>Usage in distributions</h2>

<p>Now some recent data according to Wikipedia:</p>

<h3>Distributions using upstart by default:</h3>

<ul>
<li><a href=""http://www.ubuntu.com/"" rel=""noreferrer"">Ubuntu</a> (from 9.10 to 14.10)</li>
<li><a href=""http://www.google.com/chromebook/"" rel=""noreferrer"">Chrome OS</a></li>
<li><a href=""https://www.chromium.org/chromium-os"" rel=""noreferrer"">Chromium OS</a></li>
</ul>

<h3>Distributions using systemd by default:</h3>

<ul>
<li><a href=""https://www.archlinux.org/"" rel=""noreferrer"">Arch Linux</a> - since October 2012</li>
<li><a href=""https://www.centos.org/"" rel=""noreferrer"">CentOS</a> - since April 2014 (7.14.04)</li>
<li><a href=""https://coreos.com/"" rel=""noreferrer"">CoreOS</a> - sice October 2013 (v94.0.0)</li>
<li><a href=""https://www.debian.org/"" rel=""noreferrer"">Debian</a> - since April 2015 (v8)</li>
<li><a href=""https://getfedora.org/"" rel=""noreferrer"">Fedora</a> - since May 2011 (v15)</li>
<li><a href=""https://www.mageia.org/en/"" rel=""noreferrer"">Mageia</a> - since May 2012 (v2.0)</li>
<li><a href=""https://www.opensuse.org/"" rel=""noreferrer"">openSUSE</a> - since September 2012 (v12.2)</li>
<li><a href=""https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux"" rel=""noreferrer"">Red Hat Enterprise Linux</a> - since June 2014 (v7.0)</li>
<li><a href=""https://www.suse.com/products/server"" rel=""noreferrer"">SUSE Linux Enterprise Server</a> - since October 2014 (v12)</li>
<li><a href=""http://www.ubuntu.com/"" rel=""noreferrer"">Ubuntu</a> - since April 2015 (v15.04)</li>
</ul>

<p>(See <a href=""https://en.wikipedia.org/wiki/Systemd#Adoption_and_reception"" rel=""noreferrer"">Wikipedia</a> for up to date info)</p>

<h3>Distributions using neither Upstart nor systemd:</h3>

<ul>
<li><a href=""https://devuan.org/"" rel=""noreferrer"">Devuan</a> (Debian fork created that resulted from the systemd controversies in the Debian community that led to a resignation of <a href=""https://en.wikipedia.org/wiki/Ian_Jackson"" rel=""noreferrer"">Ian Jackson</a>) -  specifically promotes <a href=""https://devuan.org/os/init-freedom/"" rel=""noreferrer"">Init Freedom</a> with the following init systems considered for inclusion: <a href=""http://core.suckless.org/sinit"" rel=""noreferrer"">sinit</a>, <a href=""https://wiki.gentoo.org/wiki/Project:OpenRC"" rel=""noreferrer"">OpenRC</a>, <a href=""http://smarden.org/runit/"" rel=""noreferrer"">runit</a>, <a href=""http://skarnet.org/software/s6/"" rel=""noreferrer"">s6</a> and <a href=""https://www.gnu.org/software/shepherd/"" rel=""noreferrer"">shepherd</a>.</li>
<li><a href=""http://www.voidlinux.eu/"" rel=""noreferrer"">Void Linux</a> - uses <a href=""http://smarden.org/runit/"" rel=""noreferrer"">runit</a> as the init system and service supervisor</li>
<li><a href=""https://www.gentoo.org/"" rel=""noreferrer"">Gentoo</a> - uses <a href=""https://wiki.gentoo.org/wiki/OpenRC"" rel=""noreferrer"">OpenRC</a></li>
<li><a href=""http://www.apple.com/osx/"" rel=""noreferrer"">OS X</a> - uses <a href=""https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man8/launchd.8.html"" rel=""noreferrer"">launchd</a></li>
<li><a href=""https://www.freebsd.org/"" rel=""noreferrer"">FreeBSD</a> uses a <a href=""https://www.freebsd.org/doc/en/articles/linux-users/startup.html"" rel=""noreferrer"">a traditional BSD-style init</a> (not SysV init)</li>
<li><a href=""https://www.netbsd.org/"" rel=""noreferrer"">NetBSD</a> uses <a href=""https://www.netbsd.org/docs/guide/en/chap-rc.html"" rel=""noreferrer"">rc.d</a></li>
<li><a href=""https://www.dragonflybsd.org/"" rel=""noreferrer"">DragonFly</a> uses traditional <a href=""https://www.dragonflybsd.org/docs/handbook/Booting/#index5h2"" rel=""noreferrer"">init</a></li>
<li><a href=""http://www.openbsd.org/"" rel=""noreferrer"">OpenBSD</a> uses the <a href=""http://man.openbsd.org/rc.8"" rel=""noreferrer"">rc</a> system startup script described <a href=""http://www.openbsd.org/faq/faq10.html#rc"" rel=""noreferrer"">here</a></li>
<li><a href=""https://alpinelinux.org/"" rel=""noreferrer"">Alpine Linux</a> (relatively new and little known distribution, with strong emphasis on security is getting more popular - e.g. <a href=""https://www.docker.com/"" rel=""noreferrer"">Docker</a> is <a href=""https://www.brianchristner.io/docker-is-moving-to-alpine-linux/"" rel=""noreferrer"">moving its official images from Ubuntu to Alpine</a>) uses the <a href=""https://wiki.gentoo.org/wiki/OpenRC"" rel=""noreferrer"">OpenRC</a> init system </li>
</ul>

<h2>Controversy</h2>

<p>In the past <a href=""http://www.pcworld.com/article/2854717/meet-devuan-the-debian-fork-born-from-a-bitter-systemd-revolt.html"" rel=""noreferrer"">A fork of Debian has been proposed to avoid systemd</a>. The <a href=""https://devuan.org/"" rel=""noreferrer"">Devuan GNU+Linux</a> was created - a fork of Debian without systemd (thanks to <a href=""https://unix.stackexchange.com/users/7919/fpmurphy1"">fpmurphy1</a> for pointing it out in the comments).</p>

<p>For more info about this controversy, see:</p>

<ul>
<li><p><a href=""https://wiki.debian.org/Debate/initsystem/systemd"" rel=""noreferrer"">The official Debian position on systemd</a></p></li>
<li><p><a href=""https://talk.devuan.org/t/the-systemd-controversy/14"" rel=""noreferrer"">The systemd controversy</a></p></li>
<li><p>Debian <a href=""https://devuan.org/os/debian-fork/"" rel=""noreferrer"">Exodus declaration in 2014</a>:</p></li>
</ul>

<blockquote>
  <p>As many of you might know already, the Init GR Debian vote promoted by
  Ian  Jackson wasn't useful to protect Debian's legacy and its users
  from the  systemd avalanche.</p>
  
  <p>This situation prospects a lock in systemd dependencies which is
  de-facto  threatening freedom of development and has serious
  consequences for Debian,  its upstream and its downstream.</p>
  
  <p>The CTTE managed to swap a dependency and gain us time over a subtle
  install  of systemd over sysvinit, but even this process was
  exhausting and full of  drama. Ultimately, a week ago, Ian Jackson
  resigned. [...]</p>
</blockquote>

<ul>
<li><a href=""https://lists.debian.org/debian-ctte/2014/11/msg00091.html"" rel=""noreferrer"">Ian Jackson's resignation</a>:</li>
</ul>

<blockquote>
  <p>I am resigning from the Technical Committee with immediate effect.</p>
  
  <p>While it is important that the views of the 30-40% of the project who
  agree with me should continue to be represented on the TC, I myself am
  clearly too controversial a figure at this point to do so.  I should
  step aside to try to reduce the extent to which conversations about
  the project's governance are personalised. [...]</p>
</blockquote>

<ul>
<li><a href=""https://devuan.org/os/init-freedom/"" rel=""noreferrer"">The Init Freedom</a>:</li>
</ul>

<blockquote>
  <p>Devuan was born out of a controversy over the decision to use as the
  default init system for Debian. The <a href=""https://wiki.debian.org/Debate/initsystem/systemd"" rel=""noreferrer"">official Debian position on
  systemd</a> is full of claims that <a href=""http://suckless.org/sucks/systemd"" rel=""noreferrer"">others have debunked</a>. Interested
  readers can continue discussing this hot topic in <a href=""https://talk.devuan.org/t/the-systemd-controversy/14"" rel=""noreferrer"">The systemd
  controversy</a>. However we encourage you to keep your head cool and your
  voice civil. At Devuan we’re more interested in programming them wrong
  than looking back. [...]</p>
</blockquote>

<p>Some websites and articles dedicated to the systemd controversy has been created:</p>

<ul>
<li><a href=""http://without-systemd.org/"" rel=""noreferrer"">Without-Systemd.org</a></li>
<li><a href=""http://systemd-free.org/"" rel=""noreferrer"">Systemd-Free.org</a> </li>
<li><a href=""https://devuan.org/os/init-freedom/"" rel=""noreferrer"">The Init Freedom</a></li>
<li><a href=""http://suckless.org/sucks/systemd"" rel=""noreferrer"">Systemd on Suckless</a></li>
</ul>

<p>There is <strong>a lot</strong> of interesting discussion on Hacker News:</p>

<ul>
<li><a href=""https://news.ycombinator.com/item?id=7728692"" rel=""noreferrer"">https://news.ycombinator.com/item?id=7728692</a></li>
<li><a href=""https://news.ycombinator.com/item?id=13387845"" rel=""noreferrer"">https://news.ycombinator.com/item?id=13387845</a></li>
<li><a href=""https://news.ycombinator.com/item?id=11797075"" rel=""noreferrer"">https://news.ycombinator.com/item?id=11797075</a></li>
<li><a href=""https://news.ycombinator.com/item?id=12600413"" rel=""noreferrer"">https://news.ycombinator.com/item?id=12600413</a></li>
<li><a href=""https://news.ycombinator.com/item?id=11845051"" rel=""noreferrer"">https://news.ycombinator.com/item?id=11845051</a></li>
<li><a href=""https://news.ycombinator.com/item?id=11782364"" rel=""noreferrer"">https://news.ycombinator.com/item?id=11782364</a></li>
<li><a href=""https://news.ycombinator.com/item?id=12877378"" rel=""noreferrer"">https://news.ycombinator.com/item?id=12877378</a></li>
<li><a href=""https://news.ycombinator.com/item?id=10483780"" rel=""noreferrer"">https://news.ycombinator.com/item?id=10483780</a></li>
<li><a href=""https://news.ycombinator.com/item?id=13469935"" rel=""noreferrer"">https://news.ycombinator.com/item?id=13469935</a></li>
</ul>

<p>Similar tendencies in other distros can be observed as well:</p>

<ul>
<li><a href=""https://www.mail-archive.com/nix-dev@lists.science.uu.nl/msg34206.html"" rel=""noreferrer"">The Church of Suckless NixOS is looking for followers</a></li>
</ul>

<h2>Philosophy</h2>

<p><strong>upstart</strong> follows the Unix philosophy of DOTADIW - ""Do One Thing and Do It Well."" It is a replacement for the traditional init daemon. It doesn't do anything other than starting and stopping services. Other tasks are delegated to other specialized subsystems.</p>

<p><strong>systemd</strong> does much more than that. In addition to starting and stopping services it also manages passwords, logins, terminals, power management, factory resets, log processing, file system mount points, networking and much more - see the <a href=""https://cgit.freedesktop.org/systemd/systemd/tree/NEWS"" rel=""noreferrer"">NEWS</a> file for some of the features.</p>

<h2>Plans of expansion</h2>

<p>According to <a href=""http://0pointer.de/public/gnomeasia2014.pdf"" rel=""noreferrer"">A Perspective for systemd
What Has Been Achieved, and What Lies Ahead</a> presentation 
by Lennart Poettering in 2014 at GNOME.asia, here are the main objectives of systemd, areas that were already covered and those that were still in progress:</p>

<h3>systemd objectives:</h3>

<blockquote>
  <p><strong>Our objectives</strong></p>
  
  <ul>
  <li>Turning Linux from a bag of bits into a competitive General Purpose Operating System.</li>
  <li>Building the Internet’s Next Generation OS Unifying pointless differences between distributions</li>
  <li><p>Bringing innovation back to the core OS</p></li>
  <li><p>Desktop, Server, Container, Embedded, Mobile, Cloud, Cluster, . . . These areas are closer together than you might think</p></li>
  <li>Reducing administrator complexity, reliability without supervision</li>
  <li>Everything introspectable</li>
  <li>Auto discovery, plug and play is key</li>
  <li>We fix things where they are broken, never tape over them</li>
  </ul>
</blockquote>

<h3>Areas already covered:</h3>

<blockquote>
  <p><strong>What we already cover:</strong></p>
  
  <p>init system, journal logging, login management, device management,
  temporary and volatile file management, binary format registration,
  backlight save/restore, rfkill save/restore, bootchart, readahead,
  encrypted storage setup, EFI/GPT partition discovery, virtual
  machine/container registration, minimal container management, hostname
  management, locale management, time management, random seed
  management, sysctl variable management, console managment, . . .</p>
</blockquote>

<h3>Work in progress:</h3>

<blockquote>
  <p><strong>What we are working on:</strong></p>
  
  <ul>
  <li>network management</li>
  <li>systemd-networkd</li>
  <li>Local DNS cache, mDNS responder, LLMNR responder, DNSSEC verification</li>
  <li>IPC in the kernel</li>
  <li>kdbus, sd-bus</li>
  <li>Time synchronisation with NTP</li>
  <li>systemd-timesyncd</li>
  <li>More integration with containers</li>
  <li>Sandboxing of Services</li>
  <li>Sandboxing of Apps</li>
  <li>OS Image format</li>
  <li>Container image format</li>
  <li>App image format</li>
  <li>GPT with auto-discovery</li>
  <li>Stateless systems, instantiatable systems, factory reset</li>
  <li>/usr is the OS</li>
  <li>/etc is (optional) configuration</li>
  <li>/var is (optional) state</li>
  <li>Atomic node initialisation and updates</li>
  <li>Integration with the cloud</li>
  <li>Service management across nodes</li>
  <li>Verifiable OS images</li>
  <li>All the way to the firmware</li>
  <li>Boot Loading</li>
  </ul>
</blockquote>

<h2>Scope of this answer</h2>

<p>As <a href=""https://unix.stackexchange.com/users/7919/fpmurphy1"">fpmurphy1</a> noted in the comments, ""It should be pointed out that systemd has expanded its scope of work over the years far beyond simply that of system startup.""</p>

<p>I tried to include most of the relevant info here. Here I am comparing the common features of Upstart and systemd when used as init systems as asked in the question and I only mention features of systemd that go beyond the scope of an init system because those cannot be compared to Startup, but their presence is important to understand the difference between those two projects. The relevant documentation should be checked for more info.</p>

<h1>More info</h1>

<p>More info can be found at:</p>

<ul>
<li><a href=""http://upstart.ubuntu.com/"" rel=""noreferrer"">upstart</a> website</li>
<li><a href=""https://freedesktop.org/wiki/Software/systemd/"" rel=""noreferrer"">systemd</a> website</li>
<li><a href=""https://en.wikipedia.org/wiki/Upstart"" rel=""noreferrer"">Upstart</a> on Wikipedia</li>
<li><a href=""https://en.wikipedia.org/wiki/Systemd"" rel=""noreferrer"">Systemd</a> on Wikipedia</li>
<li><a href=""https://en.wikipedia.org/wiki/Systemd#Design"" rel=""noreferrer"">The architecture of systemd</a> on Wikipedia</li>
<li><a href=""http://www.zdnet.com/article/linus-torvalds-and-others-on-linuxs-systemd/"" rel=""noreferrer"">Linus Torvalds and others on Linux's systemd</a> (ZDNet)</li>
<li><a href=""http://blog.erratasec.com/2015/08/about-systemd-controversy.html"" rel=""noreferrer"">About the systemd controversy</a> by Robert Graham</li>
<li><a href=""https://devuan.org/os/init-freedom/"" rel=""noreferrer"">Init Freedom Campaign</a></li>
<li><a href=""https://askubuntu.com/questions/613366/rationale-for-switching-from-upstart-to-systemd/787332#787332"">Rationale for switching from upstart to systemd?</a></li>
</ul>

<h1>Extras</h1>

<p>The <a href=""http://linoxide.com/"" rel=""noreferrer"">LinOxide</a> Team has created a <a href=""http://linoxide.com/linux-command/systemd-vs-sysvinit-cheatsheet/"" rel=""noreferrer"">Systemd vs SysV Init Linux Cheatsheet</a>.</p>
","287282"
"Where does postfix store email?","73500","","<p>I can't find any documentation on how postfix stores email. Where is it stored, and in what format? I'm using Ubuntu server 11</p>
","<p>Probably <code>/var/mail/[username]</code> or the more traditional <code>/var/spool/mail/[username]</code></p>

<p>The normal format, called ""mbox"", uses a line that starts with ""From "" to indicate the start of each message - this is one reason why many email clients will change ""From "" in the body of the message to "">From "".  You can also configure it to use ""maildir"", in which <code>/var/mail/[username]</code> is a directory in which every email message is a file in that directory.</p>
","23278"
"In linux, how to delete all files EXCEPT the pattern *.txt?","73445","","<p>I know how to delete all <code>txt</code> file under current directory by <code>rm *.txt</code>.
Does anyone know how to delete all files in current directory EXCEPT <code>txt</code> file?</p>
","<p>You can use <code>find</code></p>

<pre><code>find . -type f ! -name '*.txt' -delete
</code></pre>

<p>Or bash's extended globbing features</p>

<pre><code>shopt -s extglob
rm *.!(txt)
</code></pre>

<p>Or in zsh</p>

<pre><code>rm *~*.txt(.)
#  ||     ^^^ Only plain files
#  ||^^^^^ files ending in "".txt""
#  | \Except
#   \Everything
</code></pre>
","78378"
"What difference between openssh key and putty key?","73232","","<p>I've found that <code>ssh-keygen</code> (""ssh"" package) produces different keys from <code>puttygen</code> (""putty"" package). </p>

<p>If I create public and private keys with <code>ssh-keygen</code> some ssh-servers will not accept my keys. If I create keys with <code>puttygen</code> only one server does accept it. </p>

<p>Why don't linux repositories propose some common solution (package) for it?
I've found another package <a href=""https://www.google.ru/?q=ssh-3.2.9.1"" rel=""noreferrer"">ssh-3.2.9.1</a> which creates keys that work with putty. But why there is no handy solution in SSH?</p>
","<p>OpenSSH is the de facto standard implementation of the SSH protocol. If PuTTY and OpenSSH differ, PuTTY is the one that's incompatible.</p>

<p>If you generate a key with OpenSSH using <code>ssh-keygen</code> with the default options, it will work with virtually every server out there. A server that doesn't accept such a key would be antique, using a different implementation of SSH, or configured in a weird restrictive way. Keys of a non-default type may not be supported on some servers, in particular ECDSA keys make session establishment very slightly faster but are only supported by recent versions of OpenSSH.</p>

<p>PuTTY uses a different key file format. It comes with tools to <a href=""http://the.earth.li/~sgtatham/putty/0.62/htmldoc/Chapter8.html#puttygen-conversions"">convert</a> between its own <code>.ppk</code> format and the format of OpenSSH.</p>

<p>This ssh-3.2.9.1 you found is a <a href=""http://www.ssh.com/index.php/products.html"">commercial product</a> which has its own different private key format. There's no reason to use it instead of OpenSSH, it can only be less compatible, it requires paying, and there's about zero tutorial on how to use it out there.</p>
","74628"
"Clone ownership and permissions from another file?","73140","","<p>Is there a command or flag to clone the user/group ownership and permissions on a file from another file? To make the perms and ownership exactly ""like"" that of another file?</p>
","<p>On GNU/Linux <code>chown</code> and <code>chmod</code> have a <code>--reference</code> option</p>

<pre><code>chown --reference=otherfile thisfile
chmod --reference=otherfile thisfile
</code></pre>
","20646"
"How to XZ a directory with TAR using maximum compression?","73081","","<p>So I need to compress a directory with max compression. </p>

<p>How can I do it with <code>xz</code>? I mean I will need <code>tar</code> too because I can't compress a directory with only <code>xz</code>. Is there a oneliner to produce e.g. <code>foo.tar.xz</code>?</p>
","<p>Assuming <code>xz</code> honors the standard set of commandline flags - including compression level flags, you could try:</p>

<pre><code>tar -cf - foo/ | xz -9 -c - &gt; foo.tar.xz 
</code></pre>
","28978"
"cp: cannot stat : No such file or directory","72993","","<p>I am getting an error when I try to copy a file from my local computer to a remote server with <code>ssh</code>. I get the same error whether I use <code>cp</code> or <code>scp</code>.</p>

<p>Here is my input at the resulting error:</p>

<pre><code>[root@xxx.xx.xxx.xx /]# cp /home/username/some.xml root@xxx.xx.xxx.xx:/path/to/directory/  
cp: cannot stat ‘/home/username/some.xml’: No such file or directory
</code></pre>

<p>I have checked, and there is definitely a file at the path <code>/home/username/some.xml</code> on my local machine.</p>

<p>Both the local computer and the remote server are running <code>CentOS 7</code>. How can I resolve this error and copy successfully?  </p>
","<p>If you are <strong>logged into the local machine</strong>, you would use <code>scp</code> like this:</p>

<pre><code>scp /home/username/some.xml root@remote.machine.ip.address:/path/to/directory/
</code></pre>

<p>If you are <strong>logged into the remote machine</strong> (as in the OP), use <code>scp</code> like this:</p>

<pre><code>scp username@local.machine.ip.address:/home/username/some.xml /path/to/directory
</code></pre>

<p>Substitute the IP addresses as directed in the commands.</p>
","172410"
"Parse XML to get node value in bash script?","72836","","<p>I would like to know how I can get the value of a node with the following paths: </p>

<pre><code>config/global/resources/default_setup/connection/host
config/global/resources/default_setup/connection/username
config/global/resources/default_setup/connection/password
config/global/resources/default_setup/connection/dbname
</code></pre>

<p>from the following XML:</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;config&gt;
    &lt;global&gt;
        &lt;install&gt;
            &lt;date&gt;&lt;![CDATA[Tue, 11 Dec 2012 12:31:25 +0000]]&gt;&lt;/date&gt;
        &lt;/install&gt;
        &lt;crypt&gt;
            &lt;key&gt;&lt;![CDATA[70e75d7969b900b696785f2f81ecb430]]&gt;&lt;/key&gt;
        &lt;/crypt&gt;
        &lt;disable_local_modules&gt;false&lt;/disable_local_modules&gt;
        &lt;resources&gt;
            &lt;db&gt;
                &lt;table_prefix&gt;&lt;![CDATA[]]&gt;&lt;/table_prefix&gt;
            &lt;/db&gt;
            &lt;default_setup&gt;
                &lt;connection&gt;
                    &lt;host&gt;&lt;![CDATA[localhost]]&gt;&lt;/host&gt;
                    &lt;username&gt;&lt;![CDATA[root]]&gt;&lt;/username&gt;
                    &lt;password&gt;&lt;![CDATA[pass123]]&gt;&lt;/password&gt;
                    &lt;dbname&gt;&lt;![CDATA[testdb]]&gt;&lt;/dbname&gt;
                    &lt;initStatements&gt;&lt;![CDATA[SET NAMES utf8]]&gt;&lt;/initStatements&gt;
                    &lt;model&gt;&lt;![CDATA[mysql4]]&gt;&lt;/model&gt;
                    &lt;type&gt;&lt;![CDATA[pdo_mysql]]&gt;&lt;/type&gt;
                    &lt;pdoType&gt;&lt;![CDATA[]]&gt;&lt;/pdoType&gt;
                    &lt;active&gt;1&lt;/active&gt;
                &lt;/connection&gt;
            &lt;/default_setup&gt;
        &lt;/resources&gt;
        &lt;session_save&gt;&lt;![CDATA[files]]&gt;&lt;/session_save&gt;
    &lt;/global&gt;
    &lt;admin&gt;
        &lt;routers&gt;
            &lt;adminhtml&gt;
                &lt;args&gt;
                    &lt;frontName&gt;&lt;![CDATA[admin]]&gt;&lt;/frontName&gt;
                &lt;/args&gt;
            &lt;/adminhtml&gt;
        &lt;/routers&gt;
    &lt;/admin&gt;
&lt;/config&gt;
</code></pre>

<p>Also I want to assign that value to the variable for further use. Let me know your idea.</p>
","<p>Using <code>bash</code> and <code>xmllint</code> (as given by the tags):</p>

<pre><code>xmllint --version  #  xmllint: using libxml version 20703

# Note: Newer versions of libxml / xmllint have a --xpath option which 
# makes it possible to use xpath expressions directly as arguments. 
# --xpath also enables precise output in contrast to the --shell &amp; sed approaches below.
#xmllint --help 2&gt;&amp;1 | grep -i 'xpath'
</code></pre>

<hr>

<pre><code>{
# the given XML is in file.xml
host=""$(echo ""cat /config/global/resources/default_setup/connection/host/text()"" | xmllint --nocdata --shell file.xml | sed '1d;$d')""
username=""$(echo ""cat /config/global/resources/default_setup/connection/username/text()"" | xmllint --nocdata --shell file.xml | sed '1d;$d')""
password=""$(echo ""cat /config/global/resources/default_setup/connection/password/text()"" | xmllint --nocdata --shell file.xml | sed '1d;$d')""
dbname=""$(echo ""cat /config/global/resources/default_setup/connection/dbname/text()"" | xmllint --nocdata --shell file.xml | sed '1d;$d')""
printf '%s\n' ""host: $host"" ""username: $username"" ""password: $password"" ""dbname: $dbname""
}

# output
# host: localhost
# username: root
# password: pass123
# dbname: testdb
</code></pre>

<hr>

<p>In case there is just an XML string and the use of a temporary file is to be avoided, file descriptors are the way to go with <code>xmllint</code> (which is given <code>/dev/fd/3</code> as a file argument here):</p>

<pre><code>set +H
{
xmlstr='&lt;?xml version=""1.0""?&gt;
&lt;config&gt;
    &lt;global&gt;
        &lt;install&gt;
            &lt;date&gt;&lt;![CDATA[Tue, 11 Dec 2012 12:31:25 +0000]]&gt;&lt;/date&gt;
        &lt;/install&gt;
        &lt;crypt&gt;
            &lt;key&gt;&lt;![CDATA[70e75d7969b900b696785f2f81ecb430]]&gt;&lt;/key&gt;
        &lt;/crypt&gt;
        &lt;disable_local_modules&gt;false&lt;/disable_local_modules&gt;
        &lt;resources&gt;
            &lt;db&gt;
                &lt;table_prefix&gt;&lt;![CDATA[]]&gt;&lt;/table_prefix&gt;
            &lt;/db&gt;
            &lt;default_setup&gt;
                &lt;connection&gt;
                    &lt;host&gt;&lt;![CDATA[localhost]]&gt;&lt;/host&gt;
                    &lt;username&gt;&lt;![CDATA[root]]&gt;&lt;/username&gt;
                    &lt;password&gt;&lt;![CDATA[pass123]]&gt;&lt;/password&gt;
                    &lt;dbname&gt;&lt;![CDATA[testdb]]&gt;&lt;/dbname&gt;
                    &lt;initStatements&gt;&lt;![CDATA[SET NAMES utf8]]&gt;&lt;/initStatements&gt;
                    &lt;model&gt;&lt;![CDATA[mysql4]]&gt;&lt;/model&gt;
                    &lt;type&gt;&lt;![CDATA[pdo_mysql]]&gt;&lt;/type&gt;
                    &lt;pdoType&gt;&lt;![CDATA[]]&gt;&lt;/pdoType&gt;
                    &lt;active&gt;1&lt;/active&gt;
                &lt;/connection&gt;
            &lt;/default_setup&gt;
        &lt;/resources&gt;
        &lt;session_save&gt;&lt;![CDATA[files]]&gt;&lt;/session_save&gt;
    &lt;/global&gt;
    &lt;admin&gt;
        &lt;routers&gt;
            &lt;adminhtml&gt;
                &lt;args&gt;
                    &lt;frontName&gt;&lt;![CDATA[admin]]&gt;&lt;/frontName&gt;
                &lt;/args&gt;
            &lt;/adminhtml&gt;
        &lt;/routers&gt;
    &lt;/admin&gt;
&lt;/config&gt;
'

# exec issue
#exec 3&lt;&amp;- 3&lt;&lt;&lt;""$xmlstr""
#exec 3&lt;&amp;- 3&lt; &lt;(printf '%s' ""$xmlstr"")
exec 3&lt;&amp;- 3&lt;&lt;EOF
$(printf '%s' ""$xmlstr"")
EOF

{ read -r host; read -r username; read -r password; read -r dbname; } &lt; &lt;(
       echo ""cat /config/global/resources/default_setup/connection/*[self::host or self::username or self::password or self::dbname]/text()"" | 
          xmllint --nocdata --shell /dev/fd/3 | 
          sed -e '1d;$d' -e '/^ *--* *$/d'
       )

printf '%s\n' ""host: $host"" ""username: $username"" ""password: $password"" ""dbname: $dbname""

exec 3&lt;&amp;-
}
set -H


# output
# host: localhost
# username: root
# password: pass123
# dbname: testdb
</code></pre>
","83501"
"How to convert TXT to PDF?","72732","","<p>I want to convert <code>.txt</code> files to <code>.pdf</code>. I'm using this: </p>

<pre><code>ls | while read ONELINE; do convert -density 400 ""$ONELINE"" ""$(echo ""$ONELINE"" | sed 's/.txt/.pdf/g')""; done
</code></pre>

<p>But this produces one ""error"" -- if there's a very long line in the text file, it doesn't get wrapped.</p>

<h3>Input text</h3>

<p><img src=""https://i.stack.imgur.com/J03Hd.png"" alt=""Screenshot of the input file""></p>

<h3>Output PDF</h3>

<p><img src=""https://i.stack.imgur.com/CUbzw.png"" alt=""Screenshot of the output PDF""></p>

<p>--</p>

<p>Also, it would also be great if the output PDF could contain text, instead of images of text.</p>

<p>I have many-many-many TXT files. So don't want to do it by hand. I need an automatic solution, like the one I mentioned above.</p>
","<p>One method is to use CUPS and the PDF psuedo-printer to ""print"" the text to a PDF file. </p>

<p>Another is to use <a href=""http://www.gnu.org/software/enscript/enscript.html"">enscript</a> to encode to postscript and then convert from postscript to PDF using the ps2pdf file from ghostscript package.</p>
","17407"
"Sorting the output of ""find""?","72673","","<p>I need to be able to alphabetically sort the output of <code>find</code> before piping it to a command. Entering <code>| sort |</code> between didn't work, so what could I do?</p>

<pre><code>find folder1 folder2 -name ""*.txt"" -print0 | xargs -0 myCommand
</code></pre>
","<p>Use <code>find</code> as usual and delimit your lines with NUL. GNU <code>sort</code> can handle these with the -z switch:</p>

<pre><code>find . -print0 | sort -z | xargs -r0 yourcommand
</code></pre>
","34328"
"Is there a faster alternative to cp for copying large files (~20 GB)?","72638","","<p>I am a graduate student, and the group in which I work maintains a Linux cluster.  Each node of the cluster has its own local disk, but these local disks are relatively small and are not equipped with automatic backup.  So the group owns a fileserver with many TBs of storage space.  I am a relative Linux novice, so I am not sure what are the specs of the fileserver in terms of speed, networking ability, etc.  I do know from experience that the local disks are significantly faster than the fileserver in terms of I/O.  About a dozen or so people use the fileserver.  </p>

<p>Using <code>cp</code> to copy a ~20 GB file from the fileserver to one of the local disks takes about 11.5 minutes in real time on average (according to <code>time</code>).  I know that this <code>cp</code> operation is not very efficient because (1) <code>time</code> tells me that the system time for such a copy is only ~45 seconds; and because (2) when I examine <code>top</code> during the copy, <strong>%CPU</strong> is quite low (by inspection, roughly <strong>0-10%</strong> on average).  </p>

<p>Using <code>cp</code> to copy the same ~20 GB file from one folder on the local disk to another folder on the same local disk takes less time -- about 9 minutes in real time (~51 seconds in system time, according to <code>time</code>).  So apparently the fileserver is somewhat slower than the local disk, as expected, but perhaps not significantly slower.  I am surprised that copying from local to same local is not faster than 9 minutes.</p>

<p>I need to copy ~200 large files -- each ~20 GB -- from the fileserver to one of the local disks.  So, my question is: <strong>Is there a faster alternative to <code>cp</code> for copying large files in Linux?</strong>  (Or are there any flags within <code>cp</code> that I could use which would speed up copying?)  Even if I could somehow shave a minute off this copying time, that would help immensely.</p>

<p>I am sure that buying new, faster hardware disks, but I don't have access to such resources.  I am also not a system administrator -- I am only a (novice) user -- so I don't have access to more detailed information on the load that is on the disks.  I do know that while about a dozen people use the fileserver daily, I am the only person using this particular node/local disk.</p>
","<p>%CPU <em>should</em> be low during a copy. The CPU tells the disk controller ""grab data from sectors X–Y into memory buffer at Z"". Then it goes and does something else (or sleep, if there is nothing else). The hardware triggers an interrupt when the data is in memory. Then the CPU has to copy it a few times, and tells the network card ""transmit packets at memory locations A, B, and C"". Then it goes back to doing something else.</p>

<p>You're pushing ~240mbps. On a gigabit LAN, you ought to be able to do at least 800mbps, but:</p>

<ol>
<li>That's shared among everyone using the file server (and possibly a connection between switches, etc.)</li>
<li>That's limited by the speed the file server can handle the write, keeping in mind its disk I/O bandwidth is shared by everyone using it.</li>
<li>You didn't specify how you're accessing the file server (NFS, CIFS (Samba), AFS, etc.). You may need to tune your network mount, but on anything half-recent the defaults are usually pretty sane.</li>
</ol>

<p>For tracking down the bottleneck, <code>iostat -kx 10</code> is going to be a useful command. It'll show you the utilization on your local hard disks. If you can run that on the file server, it'll tell you how busy the file server is.</p>

<p>The general solution is going to be to speed up that bottleneck, which of course you don't have the budget for. But, there are a couple of special cases where you can find a faster approach:</p>

<ul>
<li>If the files are compressible, and you have a fast CPU, doing a <em>minimal</em> compress on-the-fly might be quicker. Something like <code>lzop</code> or maybe <code>gzip --fastest</code>.</li>
<li>If you are only changing a few bits here and there, and then sending the file back, only sending deltas will be much faster. Unfortunately, <code>rsync</code> won't really help here, as it will need to read the file on both sides to find the delta. Instead, you need something that keeps track of the delta as you change the file... Most approaches here are app-specific. But its possible that you could rig something up with, e.g., device-mapper (see the brand new <a href=""http://lwn.net/Articles/593668/"">dm-era target</a>) or btrfs.</li>
<li>If you're copying the same data to <em>multiple</em> machines, you can use something like udpcast to send it to all the machines at once.</li>
</ul>

<p>And, since you note you're not the sysadmin, I'm guessing that means you have a sysadmin. Or at least someone responsible for the file server &amp; network. You should probably ask him/her/them, they should be much more familiar with the specifics of your setup. Your sysadmin(s) should at least be able to tell you what transfer rate you can reasonably expect.</p>
","79753"
"/usr/bin vs /usr/local/bin on Linux","72582","","<p>Why are there so many places to put a binary in Linux? There are at least these five:</p>

<ol>
<li><code>/bin/</code></li>
<li><code>/sbin/</code></li>
<li><code>/usr/bin/</code></li>
<li><code>/usr/local/bin/</code></li>
<li><code>/usr/local/sbin/</code></li>
</ol>

<p>And on my office box, I do not have write permissions to some of these.</p>

<p>What type of binary goes into which of these <code>bin</code>s?</p>
","<ol>
<li><p><code>/bin</code> (and <code>/sbin</code>) were intended for programs that needed to be on a small <code>/</code> partition before the larger <code>/usr</code>, etc. partitions were mounted.  These days, it mostly serves as a standard location for key programs like <code>/bin/sh</code>, although the original intent may still be relevant for e.g. installations on small embedded devices.</p></li>
<li><p><code>/sbin</code>, as distinct from <code>/bin</code>, is for system management programs (not normally used by ordinary users) needed before <code>/usr</code> is mounted.</p></li>
<li><p><code>/usr/bin</code> is for distribution-managed normal user programs.</p></li>
<li><p>There is a <code>/usr/sbin</code> with the same relationship to <code>/usr/bin</code> as <code>/sbin</code> has to <code>/bin</code>.</p></li>
<li><p><code>/usr/local/bin</code> is for normal user programs <em>not</em> managed by the distribution package manager, e.g. locally compiled packages. You should not install them into <code>/usr/bin</code> because future distribution upgrades may modify or delete them without warning.</p></li>
<li><p><code>/usr/local/sbin</code>, as you can probably guess at this point, is to <code>/usr/local/bin</code> as <code>/usr/sbin</code> to <code>/usr/bin</code>.</p></li>
</ol>

<p>In addition, there is also <code>/opt</code> which is for monolithic non-distribution packages, although before they were properly integrated various distributions put Gnome and KDE there.  Generally you should reserve it for large, poorly behaved third party packages such as Oracle.</p>
","8658"
"mv: cannot stat No such file or directory in shell script","72581","","<p>I wrote a script to move some files form one folder to another folder
but I got the following error, I checked 2 folders and notice for 1 folder there are such files and another there is no such files, but why all of them shows ""mv cannot stat No such files or directory""</p>

<pre><code>    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/194.199.68.165_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/194.42.17.124_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/195.113.161.13_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/203.159.127.3_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/212.199.61.205_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/212.51.218.235_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/213.73.40.105_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/41.225.7.4_tcp.folder/data/*': No such file or directory
    mv: cannot stat `/home/esolve/project/capture/tcp_50x50/dest_folder/83.230.127.122_tcp.folder/data/*': No such file or directory
    [esolve@kitty tcp_50x50]$ ls /home/wgong/project/capture/tcp_50x50/dest_folder/194.199.68.165_tcp.folder/
    [esolve@kitty tcp_50x50]$ ls /home/wgong/project/capture/tcp_50x50/dest_folder/203.159.127.3_tcp.folder/data/
    129.88.70.226   132.187.230.1    138.96.116.22   155.185.54.250   192.38.109.144  193.136.227.163  193.175.135.61  195.113.161.13  83.230.127.122
    130.104.72.200  132.227.62.122   147.83.29.232   156.17.10.52     192.42.43.22    193.137.173.218  193.205.215.74  212.199.61.205
    131.130.69.164  132.252.152.194  148.81.140.193  157.181.175.249  192.43.193.71   193.144.21.131   193.226.19.30   212.51.218.235
    131.188.44.102  134.151.255.180  152.66.245.162  160.78.253.31    193.1.170.136   193.145.46.243   194.199.68.165  213.73.40.105
    131.254.208.10  138.48.3.203     152.81.47.4     192.114.4.3      193.136.166.56  193.166.160.98   194.42.17.124   41.225.7.4
</code></pre>

<p>the script is :</p>

<pre><code>    list=`ls dest_folder`
    cd dest_folder
    cwd=`pwd`
    for folder in $list;do
            mv ${cwd}/${folder}'/data/*' ${cwd}/${folder}
    done
</code></pre>

<p>I ran it in <code>/home/esolve/project/capture/tcp_50x50/</code></p>
","<pre><code>mv ${cwd}/${folder}'/data/*' ${cwd}/${folder}
</code></pre>

<p>The quotes (<code>'</code>) there prevent the shell from doing globbing. The <code>*</code> is being passed literally to the <code>mv</code> command, which fails since it doesn't find files called <code>*</code> in the directories indicated.</p>

<p>Change this to:</p>

<pre><code>mv ""${cwd}/${folder}/data""/* ""${cwd}/${folder}""
</code></pre>

<p>(Double quotes to prevent problems if you ever have a directory name with spaces in it. <code>*</code> outside the quotes.)</p>

<p>You'll still get the errors for the empty directories though. (Same sort of reason: if the file doesn't find a match for the pattern, it passes the pattern itself as an argument to the command.)</p>
","77012"
"ulimit: difference between hard and soft limits","72553","","<p>What is the difference between hard and soft limits in ulimit?</p>

<p>For number of open files, I have a soft limit of 1024 and a hard limit of 10240.
It is possible to run programs opening more than 1024 files. What is the soft limit for?</p>
","<p>A hard limit can only be raised by root (any process can lower it). So it is useful for security: a non-root process cannot overstep a hard limit. But it's inconvenient in that a non-root process can't have a lower limit than its children.</p>

<p>A soft limit can be changed by the process at any time. So it's convenient as long as processes cooperate, but no good for security.</p>

<p>A typical use case for soft limits is to disable core dumps (<code>ulimit -Sc 0</code>) while keeping the option of enabling them for a specific process you're debugging (<code>(ulimit -Sc unlimited; myprocess)</code>).</p>

<p>The <code>ulimit</code> shell command is a wrapper around the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/functions/setrlimit.html""><code>setrlimit</code></a> system call, so that's where you'll find the definitive documentation.</p>

<p>Note that some systems may not implement all limits. Specifically, some systems don't support per-process limits on file descriptors (Linux does); if yours doesn't, the shell command may be a no-op.</p>
","29579"
"Eject USB drives / eject command","72516","","<p>I know that the <strong><code>eject</code></strong> command can be used to eject almost any hardware component attached, but can it be used to eject USB drives?</p>

<p>Is it possible to eject USB drives and external HDD's with the <strong><code>eject</code></strong> command?</p>
","<p>No. Nor do they need to be; <code>eject</code> is used for opening optical drives, where one cannot pull the media from directly.</p>

<p>Unmounting is sufficient for USB/eSATA/etc. storage devices.</p>
","35509"
"How does tcp-keepalive work in ssh?","72512","","<p>I am trying to code a shell-script that uses a ssh-connection for doing ""heartbeats"". I want to terminate the client- and server-side of that connection after a certain timeout (after the connection drops).</p>

<p>What I found so far:</p>

<ul>
<li><em>TCPKeepAlive</em> yes/no for <strong>ssh</strong> and <strong>sshd</strong></li>
<li><em>ClientAliveCountMax</em> for <strong>sshd</strong></li>
<li><em>ClientAliveInterval</em> for <strong>sshd</strong></li>
<li><em>ServerAliveCountMax</em> for <strong>ssh</strong></li>
<li><em>ServerAliveInterval</em> for <strong>ssh</strong></li>
</ul>

<p>To change ""ClientAliveCountMax"" I would have to modify the sshd_config on each target machine (this option is disabled by default).</p>

<p>So my question is - can I use ""TCPKeepAlive"" for my purposes, too (without changing anything else on the source/target machines)?</p>

<p>Target operating system is SLES11 SP2 - but I do not think that is relevant here.</p>
","<p>You probably want to use the ServerAlive settings for this. They do not require any configuration on the server, and can be set on the command line if you wish.</p>

<pre><code>ssh -o ServerAliveInterval=5 -o ServerAliveCountMax=1 $HOST
</code></pre>

<p>This will send a ssh keepalive message every 5 seconds, and if it comes time to send another keepalive, but a response to the last one wasn't received, then the connection is terminated.</p>

<p>The critical difference between <code>ServerAliveInterval</code> and <code>TCPKeepAlive</code> is the layer they operate at.</p>

<ul>
<li><code>TCPKeepAlive</code> operates on the TCP layer. It sends an empty TCP ACK packet. Firewalls can be configured to ignore these packets, so if you go through a firewall that drops idle connections, these may not keep the connection alive.</li>
<li><code>ServerAliveInterval</code> operates on the ssh layer. It will actually send data through ssh, so the TCP packet has encrypted data in and a firewall can't tell if its a keepalive, or a legitimate packet, so these work better.</li>
</ul>
","34201"
"Can sed replace new line characters?","72494","","<p>Is there an issue with sed and new line character?<br>
I have a file test.txt with the following contents  </p>

<pre><code>aaaaa  
bbbbb  
ccccc  
ddddd  
</code></pre>

<p>The following does not work:<br>
<code>sed -r -i 's/\n/,/g' test.txt</code>  </p>

<p>I know that I can use <code>tr</code> for this but my question is why it seems not possible with sed.   </p>

<p>If this is a side effect of processing the file line by line I would be interested in why this happens. I think <code>grep</code> removes new lines. Does sed do the same? </p>
","<p>With GNU <code>sed</code> and provided <code>POSIXLY_CORRECT</code> is not in the environment (for single-line input):</p>

<pre><code>sed -i ':a;N;$!ba;s/\n/,/g' test.txt
</code></pre>

<p>From <a href=""https://stackoverflow.com/questions/1251999/sed-how-can-i-replace-a-newline-n"">https://stackoverflow.com/questions/1251999/sed-how-can-i-replace-a-newline-n</a> :</p>

<ol>
<li>create a label via <code>:a</code></li>
<li>append the current and next line to the pattern space via <code>N</code></li>
<li>if we are before the last line, branch to the created label <code>$!ba</code> (<code>$!</code> means not to do it on the last line (as there should be one final newline)).</li>
<li>finally the substitution replaces every newline with a comma on the pattern space (which is the whole file).</li>
</ol>
","114948"
"File extensions for unix shell scripts","72460","","<p>On wikipedia, the article for <a href=""http://en.wikipedia.org/wiki/.sh"">.sh</a> says: </p>

<blockquote>
  <p>For the .sh file extension type, see <a href=""http://en.wikipedia.org/wiki/Bourne_shell"">Bourne shell</a>.</p>
</blockquote>

<p>How about other unix shells?</p>

<p>I know that the <a href=""http://en.wikipedia.org/wiki/Shebang_%28Unix%29"">shebang</a>  is used <em>inside the file</em> to indicate an interpreter for execution, but I wonder: </p>

<ul>
<li>What are  good practices for file extensions for unix shell scripts? </li>
<li>Is it common for shell scripts to end with <code>.sh</code> <strong>regardless</strong> of which shell they run on?</li>
<li>Are there any other commonly used file extensions for unix shell scripts?</li>
</ul>
","<p>I would only call <code>.sh</code> something that is <em>meant</em> to be portable (and hopefully <em>is</em> portable).</p>

<p>Otherwise I think it's just better to hide the language. The careful reader will find it in the shebang line anyway. (In practice, <code>.bash</code> or <code>.zsh</code>, etc… suffixes are rarely used.)</p>
","31768"
"Which process has PID 0?","72435","","<p>I'm looking for the process started in Linux which has  process ID 0. I know <code>init</code> has PID 1 , which is the first process in Linux, is there any process with PID  0? </p>
","<p>From the wikipedia page titled: <a href=""http://en.wikipedia.org/wiki/Process_identifier"">Process identifier</a>:</p>

<blockquote>
  <p>There are two tasks with specially distinguished process IDs: <em>swapper</em>
  or <em>sched</em> has process ID 0 and is responsible for <a href=""http://en.wikipedia.org/wiki/Paging"">paging</a>, and is
  actually part of the kernel rather than a normal <a href=""http://en.wikipedia.org/wiki/User-mode"">user-mode</a> process.
  Process ID 1 is usually the <a href=""http://en.wikipedia.org/wiki/Init"">init</a> process primarily responsible for
  starting and shutting down the system. Originally, process ID 1 was
  not specifically reserved for init by any technical measures: it
  simply had this ID as a natural consequence of being the first process
  invoked by the kernel. More recent Unix systems typically have
  additional kernel components visible as 'processes', in which case PID
  1 is actively reserved for the init process to maintain consistency
  with older systems.</p>
</blockquote>

<p>You can see the evidence of this if you look at the parent PIDs (PPID) of <code>init</code> and <code>kthreadd</code>:</p>

<pre><code>$ ps -eaf
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 Jun24 ?        00:00:02 /sbin/init
root         2     0  0 Jun24 ?        00:00:00 [kthreadd]
</code></pre>

<p><code>kthreadd</code> is the kernel thread daemon. All kthreads are forked from this thread. You can see evidence of this if you look at other processes using <code>ps</code> and seeing who their PPID is:</p>

<pre><code>$ ps -eaf
root         3     2  0 Jun24 ?        00:00:57 [ksoftirqd/0]
root         4     2  0 Jun24 ?        00:01:19 [migration/0]
root         5     2  0 Jun24 ?        00:00:00 [watchdog/0]
root        15     2  0 Jun24 ?        00:01:28 [events/0]
root        19     2  0 Jun24 ?        00:00:00 [cpuset]
root        20     2  0 Jun24 ?        00:00:00 [khelper]
</code></pre>

<p>Notice they're all <code>2</code>.</p>
","83325"
"Adding a Column of values in a tab delimited file","72420","","<p>How can I add a Column of values in a file which has a certain number of rows.
I have a input file like this:</p>

<p>Input file:</p>

<pre><code>SPATA17 1   217947738
LYPLAL1 1   219383905
FAM47E  4   77192838
SHROOM3 4   77660162
SHROOM3 4   77660731
SHROOM3 4   77662248
</code></pre>

<p>Output file:</p>

<pre><code>SPATA17 1   217947738 file1
LYPLAL1 1   219383905 file1
FAM47E  4   77192838  file1
SHROOM3 4   77660162  file1
SHROOM3 4   77660731  file1
SHROOM3 4   77662248  file1
</code></pre>

<p>In this case, I want to add a Column of values, upto the number of rows in the file.The value remains consistent, such as ""file1"".</p>

<p>The reason is I have 100 of those files.I don't want to open each file and paste a column.
Also is there any way to automate this, by going in a directory and adding a column of values.
The value comes from the filename, which has to be added in each row of the file in the last/first column.</p>
","<p>You can use a one-liner loop like this:</p>

<pre><code>for f in file1 file2 file3; do sed -i ""s/$/\t$f/"" $f; done
</code></pre>

<p>For each file in the list, this will use <code>sed</code> to append to the end of each line a tab and the filename.</p>

<p>Explanation:</p>

<ul>
<li>Using the <code>-i</code> flag with <code>sed</code> to perform a replacement in-place, overwriting the file</li>
<li>Perform a substitution with <code>s/PATTERN/REPLACEMENT/</code>. In this example PATTERN is <code>$</code>, the end of the line, and REPLACEMENT is <code>\t</code> (= a TAB), and <code>$f</code> is the filename, from the loop variable. The <code>s///</code> command is within double-quotes so that the shell can expand variables.</li>
</ul>
","117579"
"On-the-fly monitoring HTTP requests on a network interface?","72377","","<p>For debugging purposes I want to monitor the http requests on a network interface.</p>

<p>Using a naive <code>tcpdump</code> command line I get too much low-level information and the information I need is not very  clearly represented.</p>

<p>Dumping the traffic via <code>tcpdump</code> to a file and then using <code>wireshark</code> has the disadvantage that it is not on-the-fly.</p>

<p>I imagine a tool usage like this:</p>

<pre><code>$ monitorhttp -ieth0 --only-get --just-urls
2011-01-23 20:00:01 GET http://foo.example.org/blah.js
2011-01-23 20:03:01 GET http://foo.example.org/bar.html
...
</code></pre>

<p>I am using Linux.</p>
","<p>Try <code>tcpflow</code>:</p>

<pre><code>tcpflow -p -c -i eth0 port 80 | grep -oE '(GET|POST|HEAD) .* HTTP/1.[01]|Host: .*'
</code></pre>

<p>Output is like this:</p>

<pre><code>GET /search?q=stack+exchange&amp;btnI=I%27m+Feeling+Lucky HTTP/1.1
Host: www.google.com
</code></pre>

<p>You can obviously add additional HTTP methods to the grep statement, and use <code>sed</code> to combine the two lines into a full URL.</p>
","6300"
"Which Fedora package does a specific file belong to?","72351","","<p>In the Debian family of OSes, <code>dpkg --search /bin/ls</code> gives:</p>

<pre><code>coreutils: /bin/ls
</code></pre>

<p>That is, the file <code>/bin/ls</code> belongs to the Debian package named <strong>coreutils</strong>. (see <a href=""https://unix.stackexchange.com/questions/6311/how-to-find-out-which-uninstalled-package-a-file-belongs-to"">this post</a> if you are interested in a package containing a file that isn't installed)</p>

<p>What is the Fedora equivalent?</p>
","<p>You can use <code>rpm -qf /bin/ls</code> to figure out what package your installed version belongs to:</p>

<pre><code>[09:46:58] ~ $ rpm -qf /bin/ls
coreutils-8.5-7.fc14.i686
[09:47:01] ~ $ 
</code></pre>

<p><strong>Update:</strong> Per your comment, the following should work if you want only the name of the package (I just got a chance to test):</p>

<pre><code>[01:52:49] ~ $ rpm -qf /bin/ls --queryformat '%{NAME}\n'
coreutils
[01:52:52] ~ $ 
</code></pre>

<p>You can also use <code>dnf provides /bin/ls</code> to get a list of all available repository packages that will provide the file:</p>

<pre><code># dnf provides /bin/ls
Last metadata expiration check: 0:17:06 ago on Tue Jun 27 18:04:08 2017.
coreutils-8.25-17.fc25.x86_64 : A set of basic GNU tools commonly used in shell scripts
Repo        : @System

coreutils-8.25-17.fc25.x86_64 : A set of basic GNU tools commonly used in shell scripts
Repo        : updates

coreutils-8.25-14.fc25.x86_64 : A set of basic GNU tools commonly used in shell scripts
Repo        : fedora
</code></pre>
","4706"
"How can I change the default ""ens33"" network device to old ""eth0"" on Fedora 19?","72312","","<p>I've just installed a Fedora 19 on VMware workstation 9.
The default network device is ""ens33"" instead of ""eth0"" on RHEL.</p>

<p>The reason I have to use ""eth0"" is that the license component of one of our products has be to be linked with ""eth0"".</p>

<p>There are some posts discussing about similar issues, most of which are for older OS.
I haven't found one that exactly match my situation.</p>

<p>Can anyone advise please. Thank you.</p>
","<p>The easiest way to restore the old way Kernel/modules/udev rename your ethernet interfaces is supplying these kernel parameters to <strong>Fedora 19</strong>:</p>

<ol>
<li><strong>net.ifnames=0</strong></li>
<li><strong>biosdevname=0</strong></li>
</ol>

<p>To do so follow this steps:</p>

<ol>
<li>Edit <strong>/etc/default/grub</strong></li>
<li>At the end of <strong>GRUB_CMDLINE_LINUX</strong> line append ""<strong>net.ifnames=0
biosdevname=0</strong>""</li>
<li>Save the file</li>
<li>Type ""<strong>grub2-mkconfig -o /boot/grub2/grub.cfg</strong>""</li>
<li>Type ""<strong>reboot</strong>""</li>
</ol>

<p>If you didn't supply these parameters during the installation, you will probably need to adjust and/or rename interface files at <strong>/etc/sysconfig/network-scripts/ifcfg-*</strong>.</p>

<p>Up to <strong>Fedora 18</strong>, just <strong>biosdevname=0</strong> was enough.</p>

<p>As an example, in a certain machine, in a exhaustive research, I got:<br></p>

<p>-No parameters: NIC identified as ""<strong>enp5s2</strong>"".<br>
-Parameter biosdevname=0: NIC identified as ""<strong>enp5s2</strong>"".<br>
-Parameter net.ifnames=0: NIC identified as ""<strong>em1</strong>"".<br>
-Parameter net.ifnames=0 AND biosdevname=0: NIC identified as ""<strong>eth0</strong>"".<br></p>
","88619"
"CentOS 7 - Rename network interface without rebooting","72269","","<p>I'm renaming network interfaces by modifying the files in <code>/etc/sysconfig/network-scripts</code>.</p>

<ul>
<li>eth0 -> nic0</li>
<li>eth1 -> nic1</li>
</ul>

<p>The content of the network scripts looks like this, after modification:</p>

<pre><code># cat /etc/sysconfig/network-scripts/ifcfg-nic0
DEVICE=nic0
BOOTPROTO=static
ONBOOT=yes
HWADDR=xx:xx:xx:xx:xx:xx
USERCTL=no
IPV6INIT=no
MASTER=bond0
SLAVE=yes
</code></pre>

<p>A reboot activates the new config. But how do I activate this configuration <strong>without</strong> rebooting?</p>

<p>A <code>systemctl restart network</code> doesn't do the trick.</p>

<p>I can shut down one interface by its old name (<code>ifdown eth0</code>) but <code>ifup</code> results in below message no matter if the old or new name was provided:</p>

<blockquote>
  <p>ERROR    : [/etc/sysconfig/network-scripts/ifup-eth] Device nic0 does not seem to be present, delaying initialization.</p>
</blockquote>

<p><code>/etc/init.d/network status</code> shows this output:</p>

<pre><code>Configured devices:
lo bond0 nic0 nic1
Currently active devices:
lo eth0 eth1 bond0
</code></pre>

<p>Both, <code>ifconfig</code> and <code>ip a</code> show the old interface names.</p>
","<p>You can rename the device using the ip command:</p>

<pre><code>/sbin/ip link set eth1 down
/sbin/ip link set eth1 name eth123
/sbin/ip link set eth123 up
</code></pre>

<p><strong>Edit</strong>:</p>

<blockquote>
  <p>I am leaving the below for the sake of completeness and posterity (and for informational purposes,) but I have confirmed <a href=""https://unix.stackexchange.com/questions/205010/centos-7-rename-network-interface-without-rebooting/219277#comment622544_219277"">swill's comment</a> and <a href=""https://unix.stackexchange.com/a/290848/104565"">Marco Macuzzo's answer</a> that simply changing the name and device of the interface /etc/sysconfig/network-scripts/ifcfg-eth0 (and renaming the file) will cause the device to be named correctly <strong>as long as the hwaddr= field is included in the configuration file.</strong> I recommend using this method instead after the referenced update.</p>
</blockquote>

<p>You may also want to make sure that you configure a udev rule, so that this will work on the next reboot too. The path for udev moved in CentOS 7 to /usr/lib/udev/rules.d/60-net.rules but you are still able to manage it the same way. If you <a href=""https://www.certdepot.net/rhel7-restore-old-network-interface-name/"" rel=""noreferrer"">added ""net.ifnames=0 biosdevname=0"" to your kernel boot string</a> to return to the old naming scheme for your nics, you can remove </p>

<pre><code>ACTION==""add"", SUBSYSTEM==""net"", DRIVERS==""?*"", ATTR{type}==""1"", PROGRAM=""/lib/udev/rename_device"", RESULT==""?*"", NAME=""$result""
</code></pre>

<p>And replace it with</p>

<pre><code>ACTION==""add"", SUBSYSTEM==""net"", DRIVERS==""?*"", ATTR{address}==""00:50:56:8e:3f:a7"", NAME=""eth123""
</code></pre>

<p>You need one entry per nic. Be sure to use the correct MAC address and update the NAME field. If you did not use ""net.ifnames=0 biosdevname=0"", be careful as there could be unintended consequences.</p>
","219277"
"How to use GoToMeeting in Linux","72190","","<p>My employer wants me and the team of developers to communicate with them using <a href=""http://www.gotomeeting.co.uk/fec/""><code>gotomeeting.com</code> service</a>. Is it possible to use GoToMeeting in Debian? I know that officially GoToMeeting supports only Mac and Win.</p>

<p>The reason: I am very happy with software development under Linux and don't want to migrate to Windows just because of one or two programs.</p>
","<p>There is a HTML5 version, which runs fine under Chrome, no need to use phone for voice.</p>

<p>Open it at:</p>

<p><a href=""https://app.gotomeeting.com"" rel=""noreferrer"">https://app.gotomeeting.com</a></p>

<p>Or if you want to directly open the meeting using the ID:</p>

<pre><code>https://app.gotomeeting.com/index.html?meetingid=&lt;id&gt;
</code></pre>

<p>Then you can create a Web App in Chrome, clicking in the sandwich icon, then <code>More Tools</code> > <code>Add to the Desktop</code>.</p>

<p>You can use your webcam and <strong>see shared</strong> desktops.</p>

<p>You'll need a Chrome <a href=""https://chrome.google.com/webstore/detail/gotomeeting-pro-screensha/gcgikpombjkodabhbdalkcdhmllafipp"" rel=""noreferrer"">extension</a> to <strong>share your</strong> desktop.</p>

<p>GoToMeeting said the HTML5 is their future platform and missing features will be added.</p>
","190984"
"Where is .bashrc file found in Linux?","72027","","<p>I am not finding my .bash_login and .bash_profile</p>

<pre><code>root@linux:~# locate .bash*
/etc/bash.bashrc
/etc/skel/.bashrc
/etc/skel/.bashrc.original
/home/noroot/.bashrc
/home/noroot/.bashrc.original
/root/.bash_history
/root/.bashrc
/usr/share/base-files/dot.bashrc
/usr/share/doc/adduser/examples/adduser.local.conf.examples/bash.bashrc
/usr/share/doc/adduser/examples/adduser.local.conf.examples/skel/dot.bashrc
/usr/share/kali-defaults/.bashrc
root@linux:~# 
</code></pre>

<p>Is there always only one .bashrc and .bash_profile file for every user?</p>

<p>And, is .bashrc and .bash_profile always found in the /home/""user name"" directory?</p>
","<p>The only ones that bash looks at by default are in the user's home directory, yes. There is also typically a single source for them in Linux -- /etc/skel. The user's home directory does not need to be under /home, though.</p>

<p>I see you've edited your question to ask where your .bash_login and .bash_profile files are. Based on the <code>#</code> prompt, I'm going to assume you're running this as root. In that case, your files are</p>

<pre><code>/root/.bash_history
/root/.bashrc
</code></pre>

<p>See my original answer above regarding a user's home directory -- it's not always /home; in this case, root's home directory is <code>/root</code>.</p>
","211719"
"How to determine which process is creating a file?","72022","","<p>Given file path, how can I determine which process creates it (and/or reads/writes to it)?</p>
","<p>The <strong><a href=""http://en.wikipedia.org/wiki/Lsof"" rel=""noreferrer"">lsof</a></strong> command (already mentioned in several answers) will tell you what process has a file open at the time you run it. <code>lsof</code> is available for just about every unix variant.</p>

<pre><code>lsof /path/to/file
</code></pre>

<p><code>lsof</code> won't tell you about file that were opened two microseconds ago and closed one microsecond ago. If you need to watch a particular file and react when it is accessed, you need different tools.</p>

<p>If you can plan a little in advance, you can put the file on a <strong><a href=""http://loggedfs.sourceforge.net/"" rel=""noreferrer"">LoggedFS</a></strong> filesystem. LoggedFS is a <a href=""http://fuse.sourceforge.net/"" rel=""noreferrer"">FUSE</a> stacked filesystem that logs all accesses to files in a hierarchy. The logging parameters are highly configurable. FUSE is available on <a href=""http://sourceforge.net/apps/mediawiki/fuse/index.php?title=OperatingSystems"" rel=""noreferrer"">all major unices</a>. You'll want to log accesses to the directory where the file is created. Start with the provided sample configuration file and tweak it according to <a href=""https://unix.stackexchange.com/questions/13794/loggedfs-configuration-file-syntax/13797#13797"">this guide</a>.</p>

<pre><code>loggedfs -l /path/to/log_file -c /path/to/config.xml /path/to/directory
tail -f /path/to/log_file
</code></pre>

<p>Many unices offer other monitoring facilities. Under Linux, you can use the relatively new  <a href=""http://people.redhat.com/sgrubb/audit/"" rel=""noreferrer"">audit subsystem</a>. There isn't much literature about it (but more than about loggedfs); you can start with <a href=""http://www.cyberciti.biz/tips/linux-audit-files-to-see-who-made-changes-to-a-file.html"" rel=""noreferrer"">this tutorial</a> or <a href=""https://stackoverflow.com/questions/5926947/getting-nfsstat-like-statistics-for-ext3-and-other-filesystems/5927229#5927229"">a</a> <a href=""https://unix.stackexchange.com/questions/13017/log-every-invocation-of-every-suid-program/13024#13024"">few</a> <a href=""https://unix.stackexchange.com/questions/12390/tracking-huge-buffer-usage-under-linux/12398#12398"">examples</a> or just with the <a href=""http://manpages.ubuntu.com/manpages/lucid/man8/auditctl.8.html"" rel=""noreferrer""><code>auditctl</code> man page</a>. Here, it should be enough to make sure the daemon is started, then run <code>auditctl</code>:</p>

<pre><code>auditctl -w /path/to/file
</code></pre>

<p>(I think older systems need <code>auditctl -a exit,always -w /path/to/file</code>) and watch the logs in <code>/var/log/audit/audit.log</code>.</p>
","13791"
"How to fix boot failure due to incorrect fstab?","72011","","<p>It seems that I have added incorrect record to <code>/etc/fstab</code>:</p>

<pre><code>//servername/share    /mnt/share    cifs     defaults,username=myuser     0 0
</code></pre>

<p>When I did <code>mount -a</code>, it asked user password to mount network share. It seems that it cannot proceed without password on boot, so it is just hung. </p>

<p>How can I fix fstab to prevent boot failure?</p>
","<p>It seems that I've found a solution:</p>

<ul>
<li>at the grub prompt, hit <kbd>a</kbd> to append options</li>
<li>add <code>init=/bin/bash</code> to the end of the kernel command line and press enter</li>
</ul>

<p>The system will boot to a prompt like 'bash-3.2#' enter the following commands at the prompt</p>

<ul>
<li><code>mount -o remount,rw /</code></li>
<li><code>vim /etc/fstab</code></li>
</ul>

<p>edit the fstab file commenting the errors by adding a # at the begining of each problematic line, save the file </p>

<ul>
<li>reboot by pressing CTRL+ALT+DEL </li>
</ul>
","44033"
"Which MP3 tagging tool for Linux?","71974","","<p>Which application would you recommend for Linux to tag MP3s? Under Windows I used to use Tag&amp;Rename and liked it a lot; it works well under Wine, but I want something that runs natively.</p>
","<p>There are various:</p>

<ul>
<li><a href=""http://easytag.sourceforge.net/"">easytag</a> has a lot of options</li>
<li><a href=""http://kid3.sourceforge.net/"">kid3</a> if you're on a Qt/KDE environment</li>
<li><code>id3v2</code> or <code>eyeD3</code> for the command line</li>
<li>Generally music players can also edit common tags, f.e. <code>banshee</code>, <code>rhythmbox</code> or <code>amarok</code></li>
</ul>

<p>and a lot others, try searching your distributions repository and test some of them.</p>
","4962"
"How to check password with Linux?","71892","","<p>I want to check, from the linux command line, if a given cleartext password is the same of a crypted password on a /etc/shadow</p>

<p>(I need this to authenticate web users. I'm running an embedded linux.)</p>

<p>I have access to the /etc/shadow file itself.</p>
","<p>You can easily extract the encrypted password with awk. You then need to extract the prefix <code>$algorithm$salt$</code> (assuming that this system isn't using the traditional DES, which is strongly deprecated because it can be brute-forced these days).</p>

<pre class=""lang-bsh prettyprint-override""><code>correct=$(&lt;/etc/shadow awk -v user=bob -F : 'user == $1 {print $2}')
prefix=${correct%""${correct#\$*\$*\$}""}
</code></pre>

<p>For password checking, the underlying C function is <a href=""http://pubs.opengroup.org/onlinepubs/009695399/functions/crypt.html"" rel=""nofollow noreferrer""><code>crypt</code></a>, but there's no standard shell command to access it.</p>

<p>On the command line, you can use a Perl one-liner to invoke <code>crypt</code> on the password.</p>

<pre class=""lang-bsh prettyprint-override""><code>supplied=$(echo ""$password"" |
           perl -e '$_ = &lt;STDIN&gt;; chomp; print crypt($_, $ARGV[0])' ""$prefix"")
if [ ""$supplied"" = ""$correct"" ]; then …
</code></pre>

<p>Since this can't be done in pure shell tools, if you have Perl available, you might as well do it all in Perl. (Or Python, Ruby, … whatever you have available that can call the <code>crypt</code> function.) Warning, untested code.</p>

<pre class=""lang-perl prettyprint-override""><code>#!/usr/bin/env perl
use warnings;
use strict;
my @pwent = getpwnam($ARGV[0]);
if (!@pwent) {die ""Invalid username: $ARGV[0]\n"";}
my $supplied = &lt;STDIN&gt;;
chomp($supplied);
if (crypt($supplied, $pwent[1]) eq $pwent[1]) {
    exit(0);
} else {
    print STDERR ""Invalid password for $ARGV[0]\n"";
    exit(1);
}
</code></pre>

<p>On an embedded system without Perl, I'd use a small, dedicated C program. Warning, typed directly into the browser, I haven't even tried to compile. This is meant to illustrate the necessary steps, not as a robust implementation!</p>

<pre class=""lang-c prettyprint-override""><code>/* Usage: echo password | check_password username */
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;pwd.h&gt;
#include &lt;shadow.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;unistd.h&gt;
int main(int argc, char *argv[]) {
    char password[100];
    struct spwd shadow_entry;
    char *p, *correct, *supplied, *salt;
    if (argc &lt; 2) return 2;
    /* Read the password from stdin */
    p = fgets(password, sizeof(password), stdin);
    if (p == NULL) return 2;
    *p = 0;
    /* Read the correct hash from the shadow entry */
    shadow_entry = getspnam(username);
    if (shadow_entry == NULL) return 1;
    correct = shadow_entry-&gt;sp_pwdp;
    /* Extract the salt. Remember to free the memory. */
    salt = strdup(correct);
    if (salt == NULL) return 2;
    p = strchr(salt + 1, '$');
    if (p == NULL) return 2;
    p = strchr(p + 1, '$');
    if (p == NULL) return 2;
    p[1] = 0;
    /*Encrypt the supplied password with the salt and compare the results*/
    supplied = crypt(password, salt);
    if (supplied == NULL) return 2;
    return !!strcmp(supplied, correct);
}
</code></pre>

<p>A different approach is to use an existing program such as <code>su</code> or <code>login</code>. In fact, if you can, it would be ideal to arrange for the web application to perform whatever it needs via <code>su -c somecommand username</code>. The difficulty here is to feed the password to <code>su</code>; this requires a terminal. The usual tool to emulate a terminal is <a href=""http://www.nist.gov/el/msid/expect.cfm"" rel=""nofollow noreferrer"">expect</a>, but it's a big dependency for an embedded system. Also, while <code>su</code> is in BusyBox, it's often omitted because many of its uses require the BusyBox binary to be setuid root. Still, if you can do it, this is the most robust approach from a security point of view.</p>
","21728"
"How to find out if a system uses SysV, Upstart or Systemd initsystem","71887","","<p>Is there a simple way to find out which initsystem is being used e.g by a recent <code>Debian wheezy</code> or <code>Fedora</code> system?  I'm aware that <code>Fedora 21</code> uses <code>systemd</code> initsystem but that is because I read that and because all relevant scripts/symlinks are stored in <code>/etc/systemd/</code>. However, I'm not sure about e.g <code>Debian squeeze</code> or <code>CentOS 6 or 7</code> and so on. </p>

<p>Which techniques exist to verify such initsystem? </p>
","<p>You can poke around the system to find indicators. One way is to check for the existence of three directories:</p>

<ul>
<li><p><code>/usr/lib/systemd</code> tells you you're on a systemd based system.</p></li>
<li><p><code>/usr/share/upstart</code> is a pretty good indicator that you're on an Upstart-based system.</p></li>
<li><p><code>/etc/init.d</code> tells you the box has SysV init in its history</p></li>
</ul>

<p>The thing is, these are heuristics that must be considered together, possibly with other data, not certain indicators by themselves. The Ubuntu 14.10 box I'm looking at right now has all three directories. Why? Because Ubuntu just switched to systemd from Upstart in that version, but keeps Upstart and SysV init for backwards compatibility.</p>

<p>In the end, I think the best answer is ""experience."" You will see that you have logged into a CentOS 7 box and know that it's systemd. How do you learn this? Playing around, RTFMing, etc. The same way you gain all experience.</p>

<p>I realize this is not a very satisfactory answer, but that's what happens when there is fragmentation in the market, creating nonstandard designs. It's like asking how you know whether <code>ls</code> accepts <code>-C</code>, or <code>--color</code>, or doesn't do color output at all. Again, the answer is ""experience.""</p>
","196183"
"Setup static IP in redhat 6","71721","","<p>I installed Redhat 6 x86_64. I am using the Network connection screen to set a static IP address like below (I want two PC's in my house to see each other: one Redhat PC and one Mac)</p>

<pre>
192.168.0.5  
255.255.255.0  
192.168.0.1  
</pre>

<p>When I run <code>ifconfig</code> it displays only <code>lo</code> and <code>virbr0</code> information. I don't know what these items are (I don't really know much about network settings). </p>

<p>When I try <code>ifconfig -a</code> it displays <code>eth0</code>, <code>lo</code>, <code>sit0</code> and <code>virbr0</code>. The information for  <code>eth0</code> is as follows:</p>

<pre>
Link encap : Ethernet HWaddr 90:2B:34:74:05:30
BROADCAST MULTICAST MTU:1500 Metric:1
RX packets:192 errors:0 dropped:0 overruns:0 frame:0
TX packets:6 errors:0 dropped:0 overruns:0 frame:0
collisions:0 txqueuelen:1000
RX bytes 53811 (52.5 KiB) TX bytes:468 (468.0 b)
Interrupt:29 Base address:0xc000
</pre>

<p>Could someone help me to point out if anything wrong with my setting or how to resolve this problem?</p>
","<p>You can provide static IP by editing the file <code>/etc/sysconfig/network-scripts/ifcfg-eth0</code> as <code>root</code> user in Redhat.</p>

<p>It should look like this:</p>

<pre><code>DEVICE=eth0
BOOTPROTO=STATIC
IPADDR=192.168.0.5
NETMASK=255.255.255.0
GATEWAY=192.168.0.1
ONBOOT=yes
</code></pre>

<p>After saving this file. You need to restart the network daemon using following command.</p>

<pre><code>$ sudo /etc/init.d/network stop
$ sudo /etc/init.d/network start
</code></pre>

<p>This should provide IP address to <code>eth0</code> interface also. And <code>ifconfig</code> command should list <code>eth0</code> also.</p>
","52475"
"VirtualBox: two network interfaces (NAT and host-only ones) in a Debian guest on Ubuntu","71480","","<p>I created a Debian VM on VirtualBox with two interfaces: a NAT one (for accessing internet) and a host-only one. However, I do not know how to make both interfaces work at the same time. If I the define the host-only as the adapter 1, I can access my VM from the host but not the internet; if I define the NAT one as adapter 1, I can access the internet but cannot reach my guest Debian.</p>

<p>So, how could I make both interfaces work together?</p>

<p><strong>Note</strong>: I am still trying to map some port from my host to the SSH port from my guest SO, so there is no need to suggest me to do it :)</p>

<p><strong>EDIT</strong>: This is the output of  <code>ifconfig</code> when the first adapter is the <strong>host-only</strong> one:</p>

<pre><code>eth0      Link encap:Ethernet  HWaddr 08:00:27:f6:b2:45  
          inet addr:192.168.56.101  Bcast:192.168.56.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef6:b245/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:495 errors:0 dropped:0 overruns:0 frame:0
          TX packets:206 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:48187 (47.0 KiB)  TX bytes:38222 (37.3 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:8 errors:0 dropped:0 overruns:0 frame:0
          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:560 (560.0 B)  TX bytes:560 (560.0 B)
</code></pre>

<p>This is the output of <code>netstat -nr</code> when the first adapter is the <strong>host-only</strong> one:</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
192.168.56.0    0.0.0.0         255.255.255.0   U         0 0          0 eth0
</code></pre>

<p>This is the output of <code>ifconfig</code> when  the first adapter is the <strong>NAT</strong> one:</p>

<pre><code>eth0      Link encap:Ethernet  HWaddr 08:00:27:f6:b2:45  
          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef6:b245/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:53 errors:0 dropped:0 overruns:0 frame:0
          TX packets:59 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:6076 (5.9 KiB)  TX bytes:5526 (5.3 KiB)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:16 errors:0 dropped:0 overruns:0 frame:0
          TX packets:16 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:1664 (1.6 KiB)  TX bytes:1664 (1.6 KiB)
</code></pre>

<p>This is the output of <code>netstat -nr</code> when the first adapter is the <strong>NAT</strong> one:</p>

<pre><code>Kernel IP routing table
Destination     Gateway         Genmask         Flags   MSS Window  irtt Iface
10.0.2.0        0.0.0.0         255.255.255.0   U         0 0          0 eth0
0.0.0.0         10.0.2.2        0.0.0.0         UG        0 0          0 eth0
</code></pre>
","<p>The solution was pretty simple: I just had to add the following lines in <em>Debian virtual machine</em>'s <code>/etc/network/interfaces</code> file:</p>

<pre><code>allow-hotplug eth1
iface eth1 inet dhcp
</code></pre>

<p>The second line instructs the interface to obtain an IP via DHCP. However, doing it would only work after I've called <code>ifup eth1</code>. So I added the first line, which would load the interface at boot time.</p>

<p><strong>EDIT</strong>: full <code>/etc/network/interfaces</code>:</p>

<pre><code># This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
allow-hotplug eth0
iface eth0 inet dhcp

allow-hotplug eth1
iface eth1 inet dhcp
</code></pre>
","37227"
"User without a password - how can one login into that account from a non-root account","71469","","<p>I have just started to use Scientific Linux (7.0) (although I assume this question might be distribution neutral..). The kernel version is   3.10.0-123.20.1.el7.x86_64.</p>

<p>Coming back to my question.</p>

<p>I switched to <code>root</code> account and from there created an new user account <code>test-account</code> using the command <code>adduser test-account</code>. It didn't prompt me for a password neither did I use the option to provide password. So I guess it's a ""without password"" account. I can login into this account from root account - which I suppose I'd be able to without providing password even if the test account had a password. However when I try to login into this(test-account) from a third account - it prompts me for password. And just pressing <code>Enter</code> doesn't work.</p>

<p>Is it possible to login into this account from a non-root account. Is there a way (without switching to root or using <code>sudo</code>) ?</p>
","<p>By default on enterprise GNU/Linux and it's derivatives, the <code>adduser</code> command creates a user which is disabled until you explicitly specify a password for that user.</p>

<p>Here is an example on CentOS 6.5, which should be the same as Scientific Linux.</p>

<pre><code> $ sudo adduser test
 $ sudo grep test /etc/shadow
 test:!!:123456:0:99999:7:::
</code></pre>

<p>the reason for this is because in the <code>/etc/shadow</code> file, the password field is <code>!!</code> as you can see in the example. </p>

<p>once you run <code>passwd</code> for this account it will change the user's password and allow the user to be able to login.</p>

<p>so what you should be able to do is the following to have a user without a password, simply create an account then delete the password.</p>

<pre><code> $ sudo adduser test
 $ sudo passwd -d test
 Removing password for user test.
 passwd: Success
 $ su test
 $ whoami
 test
</code></pre>

<p>now any user should be able to use <code>su</code> and login as the user <code>test</code> in my example. you will not have to use <code>sudo</code> to login as the account.</p>

<p>Although this is possible and you can have an account without a password, it is not advised. If you simply set the password for the user, you should be allowed to login.</p>

<pre><code>$ sudo passwd test
[sudo] password for &lt;YOURACCOUNT&gt;:
Changing password for user test.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.
</code></pre>
","192956"
"Understanding IFS","71465","","<p>The following few threads on this site and StackOverflow were helpful for understanding how <code>IFS</code> works:</p>

<ul>
<li><a href=""https://unix.stackexchange.com/questions/16192/what-is-ifs-in-context-of-for-looping"">What is IFS in context of for looping?</a></li>
<li><a href=""https://unix.stackexchange.com/questions/7011/how-to-loop-over-the-lines-of-a-file"">How to loop over the lines of a file</a></li>
<li><a href=""https://stackoverflow.com/questions/4385772/bash-read-line-by-line-from-file-with-ifs"">Bash, read line by line from file, with IFS</a></li>
</ul>

<p>But I still have some short questions. I decided to ask them in the same post since I think it may help better future readers:</p>

<p><strong>Q1.</strong> <code>IFS</code> is typically discussed in the context of ""field splitting"". Is <strong>field splitting</strong> the same as <strong>word splitting</strong> ?</p>

<p><strong>Q2:</strong> The POSIX specification <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_05%60"" rel=""noreferrer"">says</a>: </p>

<blockquote>
  <p>If the value of IFS is null, no field splitting shall be performed.</p>
</blockquote>

<p>Is setting <code>IFS=</code> the same as setting <code>IFS</code> to null? Is this what is meant by setting it to an <code>empty string</code> too?</p>

<p><strong>Q3:</strong> In the POSIX specification, I <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_05_03"" rel=""noreferrer"">read</a> the following:</p>

<blockquote>
  <p>If IFS is not set, the shell shall behave as if the value of IFS is
  <code>&lt;space&gt;, &lt;tab&gt; and &lt;newline&gt;</code></p>
</blockquote>

<p>Say I want to restore the default value of <code>IFS</code>. How do I do that? (more specifically, how do I refer to <code>&lt;tab&gt;</code> and <code>&lt;newline&gt;</code>?)</p>

<p><strong>Q4:</strong> Finally, how would this code:</p>

<pre><code>while IFS= read -r line
do    
    echo $line
done &lt; /path_to_text_file
</code></pre>

<p>behave if we we change the first line to </p>

<pre><code>while read -r line # Use the default IFS value
</code></pre>

<p>or to:</p>

<pre><code>while IFS=' ' read -r line
</code></pre>
","<ol>
<li>Yes, they are the same.</li>
<li>Yes.</li>
<li>In bash, and similar shells, you could do something like <code>IFS=$' \t\n'</code>. Otherwise, you could insert the literal control codes by using <code>[space] CTRL+V [tab] CTRL+V [enter]</code>. If you are planning to do this, however, it's better to use another variable to temporarily store the old <code>IFS</code> value, and then restore it afterwards (or temporarily override it for one command by using the <code>var=foo command</code> syntax).</li>
<li>
<ul>
<li>The first code snippet will put the entire line read, verbatim, into <code>$line</code>, as there are no field separators to perform word splitting for. Bear in mind however that since many shells use cstrings to store strings, the first instance of a NUL may still cause the appearance of it being prematurely terminated.</li>
<li>The second code snippet may not put an exact copy of the input into <code>$line</code>. For example, if there are multiple consecutive field separators, they will be made into a single instance of the first element. This is often recognised as loss of surrounding whitespace.</li>
<li>The third code snippet will do the same as the second, except it will only split on a space (not the usual space, tab, or newline).</li>
</ul></li>
</ol>
","26787"
"Can scp create a directory if it doesn't exist?","71423","","<p>I want to use <code>scp</code> to upload files but sometimes the target directory may not exist.</p>

<p>Is it possible to create the folder automatically? If so, how? If not, what alternative way can I try?</p>
","<p>This is one of the many things that <a href=""https://rsync.samba.org/""><code>rsync</code></a> can do.</p>

<p>If you're using a version of <code>rsync</code> released in the past several years,¹ its basic command syntax is similar to <code>scp</code>:²</p>

<pre><code>$ rsync -r local-dir remote-machine:path
</code></pre>

<p>That will copy <code>local-source</code> and its contents to <code>$HOME/path/local-dir</code> on the remote machine, creating whatever directories are required.³</p>

<p><code>rsync</code> does have some restrictions here that can affect whether this will work in your particular situation. It won't create multiple levels of missing remote directories, for example; it will only create up to one missing level on the remote. You can easily get around this by preceding the <code>rsync</code> command with something like this:</p>

<pre><code>$ ssh remote-host 'mkdir -p foo/bar/qux'
</code></pre>

<p>That will create the <code>$HOME/foo/bar/qux</code> tree if it doesn't exist. It won't complain or do anything else bad if it <em>does</em> already exist.</p>

<p><code>rsync</code> sometimes has other surprising behaviors. Basically, you're asking it to figure out what you meant to copy, and its guesses may not match your assumptions. Try it and see. If it doesn't behave as you expect and you can't see why, post more details about your local and remote directory setups, and give the command you tried.</p>

<hr>

<p><strong>Footnotes</strong>:</p>

<ol>
<li><p>Old versions of <code>rsync</code> required the <code>-e ssh</code> flag to make it behave like <code>scp</code> because it defaulted to the obsolete <a href=""http://en.wikipedia.org/wiki/Remote_Shell"">RSH protocol</a>.</p></li>
<li><p><code>scp</code> and <code>rsync</code> share some flags, but there is only a bit of overlap.</p></li>
<li><p>When using <code>SSH</code> as the transfer protocol, <code>rsync</code> uses the same defaults. So, just like <code>scp</code>, it will assume there is a user with the same name as your local user on the remote machine by default.</p></li>
</ol>
","193374"
"Comparing two files in Vim","71319","","<p>Is it possible to view two files side-by-side in Vim? If so, how can I set up my editor to do this, and is there a way to <code>diff</code> between the two files within Vim?</p>

<p>I am aware of the <code>:next</code> and <code>:prev</code> commands, but this is not what I'm after. It would really be nice to view the two files in tandem.</p>
","<p>Open the side by side view:</p>

<pre><code>Ctrl+w v
</code></pre>

<p>Change between them:</p>

<pre><code>Ctrl+w h or l
</code></pre>

<p>Checkout the <em>vimdiff</em> command, part of the vim package, if you want a diff-like view..</p>
","1387"
"How to check if a user can access a given file?","71095","","<p>*nix user permissions are really simple, but things can get messy when you have to take in account all the parent directory access before reaching a given file. How can I check if the user has enough privileges? If not, then which directory is denying access?</p>

<p>For example, suppose a user <code>joe</code>, and the file <code>/long/path/to/file.txt</code>. Even if <code>file.txt</code> was chmoded to 777, joe still has to be able to access <code>/long/</code>, and then <code>/long/path/</code> and then <code>/long/path/to/</code> before. What I need is a way to automatically check this. If <code>joe</code> does not have access, I would also like to know where he has been denied. Maybe he can access <code>/long/</code>, but not <code>/long/path/</code>.</p>
","<p>You can use </p>

<pre><code>namei -m /path/to/really/long/directory/with/file/in
</code></pre>

<p>which will output all of the permissions in the path in a vertical list.</p>

<p>or </p>

<pre><code>namei -l /path/to/really/long/directory/with/file/in
</code></pre>

<p>to list all owners and the permissions</p>
","157673"
"Disable screen blanking on text console","71012","","<p>I'm running linux clusters, mostly on SLES10. The servers are mostly blades, accessed via remote console. There is a real console in the server room, but switched off.</p>

<p>I would like to disable the screen blanking as it serves no purpose and is a
nuisance. You have to press key to see if you are connected which is a pain. We are running in runlevel 3, so the console is in text mode, no X11 involved.</p>
","<p>I've implemented and tested the following configuration, which works fine on sles10, my workhorse at the moment.</p>

<p>In</p>

<pre><code>/etc/init.d/boot.local
</code></pre>

<p>add</p>

<pre><code>setterm -blank
</code></pre>

<p>it looks like that is all it takes. Thanks for Uku Loskit and Gilles for the push in the right direction.</p>
","8058"
"What do the scripts in /etc/profile.d do?","70928","","<p>I am reading about basic shell scripting from <a href=""http://rads.stackoverflow.com/amzn/click/047025128X"">Linux Command Line and Shell Scripting Bible</a>. </p>

<p>It says that the <code>/etc/profile</code> file sets the environment variables at startup of the Bash shell. The <code>/etc/profile.d</code> directory contains other scripts that contain application-specific startup files, which are also executed at startup time by the shell.</p>

<ul>
<li><p>Why are these files not a part of <code>/etc/profile</code> if they are also critical to Bash startup ?</p></li>
<li><p>If these files are application-specific startup files not critical to Bash startup, then why are they part of the startup process ? Why are they not run only when the specific applications, for which they contain settings, are executed ?</p></li>
</ul>
","<blockquote>
  <p><strong>Why are these files not a part of /etc/profile if they are also critical to Bash startup ?</strong></p>
</blockquote>

<p>If you mean, ""Why are they not just combined into one giant script?"", the answer is:</p>

<ol>
<li>Because that would be a maintenance nightmare for the people who are responsible for the scripts. </li>
<li>Because having the scripts loaded as independent modules makes the whole system more dynamically adjustable -- individual scripts can be added and removed without affecting the others.  Etc.</li>
<li>Because they are loaded via /etc/profile which makes them a part of the bash ""profile"" in the same way anyway.</li>
</ol>

<blockquote>
  <p><strong>If these files are application-specific startup files not critical to Bash startup, then why are they part of the startup process ? Why
  are they not run only when the specific applications, for which they
  contain settings, are executed ?</strong></p>
</blockquote>

<p>That seems to me like a broader design philosophy question that I'll split into two.  The first question is about the value and appropriateness of using the shell environment.  Does it have positive value?  Yes, it is useful.  Is it the best solution to all configuration issues?  No, but it is very efficient for managing simple parameters, and also widely recognized and understood. Contrast that to say, deciding to configure such things heterogeneously, perhaps $PATH could be managed by a separate independent tool, preferred tools such as $EDITOR could be in an sqlite file somewhere, $LC lang stuff could be in a text file with a custom format somewhere else, etc -- doesn't just using env variables and <code>/etc/profile.d</code> suddenly seem simpler?  You probably already know what an env variable is, how they work and how to use them, vs. learning 5 completely different mechanisms for 5 different ubiquitous aspects of what is <em>appropriately named</em> ""the environment"".</p>

<p>The second question is, ""Is startup the appropriate time for this?"", which begs the objection that it is not very efficient (all that data which may or may not get used, etc). But:  </p>

<ul>
<li>Realistically, it is not all that much data, partially because no one in their right mind would use it for more than a few simple parameters (since there are other means of configuring an application).</li>
<li>If it is used wisely, with regard to things that are commonly invoked, then setting, eg, default $CFLAGS from a file somewhere every time you invoke <code>gcc</code> would be less efficient.  Keep in mind that the amount of memory involved is, again, infinitesimal.  </li>
<li>It can involve systemic things which more than one application may be involved with, and the shell is a <em>common ground</em>.</li>
</ul>

<p>More could be added to that list, but hopefully this gives you some idea about the pros and cons of the issue -- the major 'pro' and the major 'con' being that it is a global namespace.</p>
","64262"
"Why is my find not recursive?","70692","","<p>I am running the following command, but it is not performed recursively:</p>

<pre><code>find . -name *.java
</code></pre>

<p>I know there are java files further down in the current directory but it is performing the <code>find</code> on the current directory only. I am using OS X, 10.9.</p>
","<p>The problem is, you didn't quote your <code>-name</code> parameter. Do this instead:</p>

<pre><code>find . -name '*.java'
</code></pre>

<p><strong>Explanation</strong></p>

<p>Without the quotes, the shell interprets <code>*.java</code> as a glob pattern and expands it to any file names matching the glob before passing it to <code>find</code>. This way, if you had, say, <code>foo.java</code> in the current directory, <code>find</code>'s actual command line would be:</p>

<pre><code>find . -name foo.java
</code></pre>

<p>which would obviously list the file in the current directory only (unless you happen to have some similarly-named files further down the tree).</p>

<p>Quoting prevents glob expansion and passes the command line to <code>find</code> as-is.</p>

<p>Incidentally, if the glob had failed to match (no <code>*.java</code> files in the current directory), you would get one of two behaviors depending on how your shell is set up to handle globs that don't match (this is governed by the <code>nullglob</code> option in Bash, for example):</p>

<ol>
<li>If a glob that doesn't match is not expanded by the shell, <code>find</code> will (accidentally, mind you) exhibit correct behavior.</li>
<li>If a glob that doesn't match is expanded into an empty string by the shell, <code>find</code> will complain that it is missing an argument to <code>-name</code>.</li>
</ol>
","123441"
"Searching for a string on multiple zip files","70642","","<p>I have a folder that contains about 200 zip files. Each zip file contains only one text file in it. 
I would like to search for a specific string in all the text files in all the zip files. </p>

<p>I tried this (which searches for any text file in the zip file that contains the string ""ORA-"")  but it didn't work. </p>

<pre><code>zipgrep ORA-1680 *.zip
</code></pre>

<p>What is the correct of doing it without uncompressing the zip files (on a SunOS 5.10 )?</p>
","<p>It is in general not possible to search for content within a compressed file without uncompressing it one way or another.
Since zipgrep is only a shellscript, wrapping unzip and egrep itself, you might just as well do it manually:</p>

<pre><code>for file in *.zip; do unzip -c ""$file"" | grep ""ORA-1680""; done
</code></pre>

<p>If you need just the list of matching zip files, you can use something like:</p>

<pre><code>for file in *.zip; do
    if ( unzip -c ""$file"" | grep -q ""ORA-1680""); then
        echo ""$file""
    fi
done
</code></pre>

<p>This way you are only decompressing to stdout (ie. to memory) instead of decompressing the files to disk. You can of course try to just <code>grep -a</code> the zip files but depending on the content of the file and your pattern, you might get false positives and/or false negatives.</p>
","18553"
"No such file or directory /etc/init.d/functions","70585","","<p>I created a startup script to start/restart/stop a group of applications. I used the lib <code>/etc/init.d/functions</code> in my script. It is working well on my system, but it not working for my client; he is getting the error:</p>

<blockquote>
  <p>No such file or directory /etc/init.d/functions</p>
</blockquote>

<p>Right now I don't know which linux distro my client uses. Is the <code>init.d/functions</code> file different for different Linux distros? If so, how can I find it?</p>
","<p>It's specific to whatever distribution you're running.  Debian and Ubuntu have <code>/lib/lsb/init-functions</code>; SuSE has <code>/etc/rc.status</code>; <strong>none of them are compatible with the others.</strong>  In fact, some distributions don't use <code>/etc/init.d</code> at all, or use it in an incompatible way (Slackware and Arch occur to me off the top of my head; there are others).</p>
","9315"
"How do I create a service for a shell script so I can start and stop it like a daemon?","70374","","<p>I'm using CentOS 7 what my aim is to create a cron for every five seconds but as I researched we can use cron only for a minute so what I am doing now is I have created a shell file.<br>
<strong>hit.sh</strong></p>

<pre><code>while sleep 5; do curl http://localhost/test.php; done
</code></pre>

<p>but I have hit it manually through right clicking it.</p>

<p>What I want is to create a service for that file so that i can start and stop it automatically.</p>

<p>I found the <a href=""https://unix.stackexchange.com/questions/20357/how-can-i-make-a-script-in-etc-init-d-start-at-boot"">script to create a service</a> </p>

<pre><code>#!/bin/bash
# chkconfig: 2345 20 80
# description: Description comes here....

# Source function library.
. /etc/init.d/functions

start() {
    # code to start app comes here 
    # example: daemon program_name &amp;
}

stop() {
    # code to stop app comes here 
    # example: killproc program_name
}

case ""$1"" in 
    start)
       start
       ;;
    stop)
       stop
       ;;
    restart)
       stop
       start
       ;;
    status)
       # code to check status of app comes here 
       # example: status program_name
       ;;
    *)
       echo ""Usage: $0 {start|stop|status|restart}""
esac

exit 0 
</code></pre>

<p>But I don't know what to write in start or stop methods I tried placing the same content of hit.sh in <code>start(){}</code> but it gave error for <code>}</code> in stop method.</p>

<p>I tried searching it but could not find any help. I am new to this, if anybody can help it would be a great help.</p>
","<p>If you'd like to reuse your code sample, it could look something like:</p>

<pre><code>#!/bin/bash

case ""$1"" in 
start)
   /path/to/hit.sh &amp;
   echo $!&gt;/var/run/hit.pid
   ;;
stop)
   kill `cat /var/run/hit.pid`
   rm /var/run/hit.pid
   ;;
restart)
   $0 stop
   $0 start
   ;;
status)
   if [ -e /var/run/hit.pid ]; then
      echo hit.sh is running, pid=`cat /var/run/hit.pid`
   else
      echo hit.sh is NOT running
      exit 1
   fi
   ;;
*)
   echo ""Usage: $0 {start|stop|status|restart}""
esac

exit 0 
</code></pre>

<p>Naturally, the script you want to be executed as a service should go to e.g. <code>/usr/local/bin/hit.sh</code>, and the above code should go to <code>/etc/init.d/hitservice</code>.</p>

<p>For each runlevel which needs this service running, you will need to create a respective symlink. For example, a symlink named <code>/etc/init.d/rc5.d/S99hitservice</code> will start the service for runlevel 5. Of course, you can still start and stop it manually via <code>service hitservice start</code>/<code>service hitservice stop</code></p>
","236307"
"How to close ports in Linux?","70329","","<p>I have some question in closing port, I think I got some strange things.</p>

<p>When I use execute</p>

<pre><code>nmap --top-ports 10 192.168.1.1
</code></pre>

<p>it shows that 23/TCP port is open.</p>

<p>But when I execute</p>

<pre><code>nmap --top-ports 10 localhost
</code></pre>

<p>it show that 23/tcp port is closed.</p>

<p>Which of them is true? I want to close this port on my whole system, how can I do it?</p>
","<p>Nmap is a great port scanner, but sometimes you want something more authoritative. You can ask the kernel what processes have which ports open by using the <code>netstat</code> utility:</p>

<pre>
me@myhost:~$ sudo netstat -tlnp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address   Foreign Address   State    PID/Program name
tcp        0      0 127.0.0.1:53    0.0.0.0:*         LISTEN   1004/dnsmasq    
tcp        0      0 0.0.0.0:22      0.0.0.0:*         LISTEN   380/sshd        
tcp        0      0 127.0.0.1:631   0.0.0.0:*         LISTEN   822/cupsd       
tcp6       0      0 :::22           :::*              LISTEN   380/sshd        
tcp6       0      0 ::1:631         :::*              LISTEN   822/cupsd       
</pre>

<p>The options I have given are:</p>

<ul>
<li><code>-t</code> TCP only</li>
<li><code>-l</code> Listening ports only</li>
<li><code>-n</code> Don't look up service and host names, just display numbers</li>
<li><code>-p</code> Show process information (requires root privilege)</li>
</ul>

<p>In this case, we can see that <code>sshd</code> is listening on any interface (<code>0.0.0.0</code>) port 22, and <code>cupsd</code> is listening on loopback (<code>127.0.0.1</code>) port 631. Your output may show that <code>telnetd</code> has a local address of <code>192.168.1.1:23</code>, meaning it will not answer to connections on the loopback adapter (e.g. you can't <code>telnet 127.0.0.1</code>).</p>

<p>There are other tools that will show similar information (e.g. <code>lsof</code> or <code>/proc</code>), but netstat is the most widely available. It even works on Windows (<code>netstat -anb</code>). BSD netstat is a little different: you'll have to use <a href=""http://www.freebsd.org/cgi/man.cgi?query=sockstat&amp;sektion=1"">sockstat(1)</a> to get the process information instead.</p>

<p>Once you have the process ID and program name, you can go about finding the process and killing it if you wish to close the port. For finer-grained control, you can use a firewall (iptables on Linux) to limit access to only certain addresses. You may need to disable a service startup. If the PID is ""-"" on Linux, it's probably a kernel process (this is common with NFS for instance), so good luck finding out what it is.</p>

<p>Note: I said ""authoritative"" because you're not being hindered by network conditions and firewalls. If you trust your computer, that's great. However, if you suspect that you've been hacked, you may not be able to trust the tools on your computer. Replacing standard utilities (and sometimes even system calls) with ones that hide certain processes or ports (a.k.a. rootkits) is a standard practice among attackers. Your best bet at this point is to make a forensic copy of your disk and restore from backup; then use the copy to determine the way they got in and close it off.</p>
","140035"
"Zip everything in current directory","70136","","<p>I'd like to compress and package everything, including files and folders in current directory, into a single ZIP file on Ubuntu.</p>

<p>What would be the most convenient command for this (and name of the tool needed to be installed if any)?</p>

<p>Edit: What if I need to exclude one folder or several files?</p>
","<p>Install <code>zip</code> and use</p>

<pre><code>zip -r foo.zip .
</code></pre>

<p>You can use the flags <code>-0</code> (none) to <code>-9</code> (best) to change compressionrate</p>

<p>Excluding files can be done via the <code>-x</code> flag. From the man-page:</p>

<pre><code>-x files
--exclude files
          Explicitly exclude the specified files, as in:

                 zip -r foo foo -x \*.o

          which  will  include the contents of foo in foo.zip while excluding all the files that end in .o.  The backslash avoids the shell filename substitution, so that the name matching
          is performed by zip at all directory levels.

          Also possible:

                 zip -r foo foo -x@exclude.lst

          which will include the contents of foo in foo.zip while excluding all the files that match the patterns in the file exclude.lst.

          The long option forms of the above are

                 zip -r foo foo --exclude \*.o

          and

                 zip -r foo foo --exclude @exclude.lst

          Multiple patterns can be specified, as in:

                 zip -r foo foo -x \*.o \*.c

          If there is no space between -x and the pattern, just one value is assumed (no list):

                 zip -r foo foo -x\*.o

          See -i for more on include and exclude.
</code></pre>
","27364"
"What does the 's' attribute in file permissions mean?","70124","","<p>Please compare the following two lines:</p>

<pre><code>-rws---r-x 1 root root 21872 2009-10-13 21:06 prg1 

-rwx---r-x 1 root root 21872 2009-10-13 21:06 prg2 
</code></pre>

<p>Does the setuid bit on <code>prg1</code>, along with the read and execute bits for 'other' mean that any user can run it with root privileges?  The  <code>prg2</code> also has read and execute for 'other', but does not have the setuid bit set, so does that mean it can still be run by any user but without root privileges?</p>
","<p>Precisely the opposite, you don't need to use sudo or switch to root, the executable does it for you.</p>
","118854"
"tar exits on ""Cannot stat: No such file of directory"", why?","70023","","<p>I'm trying to create tar.gz file using the following command:</p>

<pre><code>sudo tar -vcfz dvr_rdk_v1.tar.gz dvr_rdk/
</code></pre>

<p>It... SE. ZWZ, Zthen start to create files (many files in folder), but then I get the following error:</p>

<pre><code>tar: dvr_rdk_v1.tar.gz: Cannot stat: No such file or directory
tar: Exiting with failure status due to previous errors
</code></pre>

<p>I don't see any description of this error, what does it mean?</p>
","<p>Remove <code>-</code> from <code>vcfz</code> options. <code>tar</code> does not need hyphen for options.</p>

<p>Also check your write permission to the directory from which you are executing the command.</p>
","149496"
"Do the parent directory's permissions matter when accessing a subdirectory?","70006","","<p>If I have a root folder with some restrictive permission, let's say 600, and if the child folders/files have 777 permission will everybody be able to read/write/execute the child file even though the root folder has 600?</p>
","<p>The precise rule is: you can traverse a directory if and only if you have execute permission on it.</p>

<p>So for example to access <code>dir/subdir/file</code>, you need execute permission on <code>dir</code> and <code>dir/subdir</code>, plus the permissions on <code>file</code> for the type of access you want. Getting into corner cases, I'm not sure whether it's universal that you need execute permission on the current directory to access a file through a relative path (you do on Linux).</p>

<p>The way you access a file matters. For example, if you have execute permissions on <code>/foo/bar</code> but not on <code>/foo</code>, but your current directory is <code>/foo/bar</code>, you can access files in <code>/foo/bar</code> through a relative path but not through an absolute path. You can't change to <code>/foo/bar</code> in this scenario; a more privileged process has presumably done <code>cd /foo/bar</code> before going unprivileged. If a file has multiple hard links, the path you use to access it determines your access constraints.</p>

<p>Symbolic links change nothing. The kernel uses the access rights of the calling process to traverse them. For example, if <code>sym</code> is a symbolic link to the directory <code>dir</code>, you need execute permission on <code>dir</code> to access <code>sym/foo</code>. The permissions on the symlink itself may or may not matter depending on the OS and filesystem (some respect them, some ignore them).</p>

<p>Removing execute permission from the root directory effectively restricts a user to a part of the directory tree (which a more privileged process must change into). This requires access control lists to be any use. For example, if <code>/</code> and <code>/home/joe</code> are off-limits to <code>joe</code> (<code>setfacl -m user:joe:0 / /home</code>) and <code>/home/joe</code> is <code>joe</code>'s home directory, then <code>joe</code> won't be able to access the rest of the system (including running shell scripts with <code>/bin/sh</code> or dynamically linked binaries that need to access <code>/lib</code>, so you'd need to go deeper for practical use, e.g. <code>setfacl -m user:joe:0</code> /*; setfacl -d user:joe /bin /lib').</p>

<p>Read permission on a directory gives the right to enumerate the entries. Giving execute permission without giving read permission is occasionally useful: the names of entries serve as passwords to access them. I can't think of any use in giving read or write permission to a directory without execute permission.</p>
","13891"
"Awesome symbols and characters in a bash prompt","70002","","<p>I just ran across a screenshot of someone's terminal:</p>

<p><img src=""https://i.stack.imgur.com/BX5xV.png"" alt=""enter image description here""></p>

<p>Is there a list of all of the characters which can be used in a Bash prompt, or can someone get me the character for the star and the right arrow? </p>
","<p>You can use any printable character, bash doesn't mind. You'll probably want to configure your terminal to support <a href=""http://en.wikipedia.org/wiki/Unicode"">Unicode</a> (in the form of <a href=""http://en.wikipedia.org/wiki/UTF-8"">UTF-8</a>).</p>

<p>There are a lot of characters in Unicode, so here are a few tips to help you search through the Unicode charts:</p>

<ul>
<li>You can try to draw the character on <a href=""http://shapecatcher.com/"">Shapecatcher</a>. It tries to recognize a Unicode character in what you draw.</li>
<li>You can try to figure out which block the character is in. For example, that weird-looking symbol and that star would be in a block of miscellaneous symbols; characters like <code>Ǫ</code> and <code>ı</code> are latin letters with modifiers; <code>∉</code> is a mathematical symbol, and so on.</li>
<li>You can try to think of a word in the description of the character and look for it in a list of unicode symbol names and descriptions. <a href=""http://live.gnome.org/Gucharmap"">Gucharmap</a> or <a href=""http://utils.kde.org/projects/kcharselect/"">Kcharselect</a> can help.</li>
</ul>

<p>P.S. On Shapecatcher, I got <a href=""http://shapecatcher.com/unicode_info/8756.html"">U+2234 THEREFORE</a> for <code>∴</code>, <a href=""http://shapecatcher.com/unicode_info/8594.html"">U+2192 RIGHTWARDS ARROW</a> for <code>→</code>, <a href=""http://shapecatcher.com/unicode_info/9791.html"">U+263F MERCURY</a> for <code>☿</code> and <a href=""http://shapecatcher.com/unicode_info/9733.html"">U+2605 BLACK STAR</a> for <code>★</code>.</p>

<p>In a bash script, up to bash 4.1, you can write a byte by its code point, but not a character. If you want to avoid non-ASCII characters to make your <code>.bashrc</code> resilient to file encoding changes, you'll need to enter the bytes corresponding to these characters in the UTF-8 encoding. You can see the hexidecimal values by running <code>echo ∴ → ☿ ★ | hexdump -C</code> in a UTF-8 terminal, e.g. <code>∴</code> is encoded by <code>\xe2\x88\xb4</code> in UTF-8.</p>

<pre><code>if [[ $LC_CTYPE =~ \.[Uu][Tt][Ff]-?8 ]]; then
  PS1=$'\\[\e[31m\\]\xe2\x88\xb4\\[\e[0m\\]\n\xe2\x86\x92 \xe2\x98\xbf \\~ \\[\e[31m\\]\xe2\x98\x85 $? \\[\e[0m\\]'
end
</code></pre>

<p>Since bash 4.2, you can use <code>\u</code> followed by 4 hexadecimal digits in a <code>$'…'</code> string.</p>

<pre><code>  PS1=$'\\[\e[31m\\]\u2234\\[\e[0m\\]\n\u2192 \u263f \\~ \\[\e[31m\\]\u2605 $? \\[\e[0m\\]'
</code></pre>
","25907"
"List files sorted numerically","69997","","<p>I have a bunch of files from <code>log1</code> to <code>log164</code>.</p>

<p>I'm trying to LIST the directory (sorted) in a UNIX terminal but the sort functions are only providing the format like this:</p>

<pre><code>home:logs Home$ ls -1 | sort
log1.gz
log10.gz
log100.gz
log101.gz
log102.gz
log103.gz
log104.gz
log105.gz
log106.gz
...etc
</code></pre>

<p>What I want is</p>

<pre><code>home:logs Home$ ls -1 | sort
log1.gz
log2.gz
log3.gz
log4.gz
log5.gz
log6.gz
log7.gz
...{more here}
log99.gz
log100.gz
log101.gz
log102.gz
...etc
</code></pre>

<p>Any suggestions in what I could use to do this? </p>
","<p><code>bash</code>'s braces, <code>{}</code>, will enumerate them in order:</p>

<pre><code>for file in log{1..164}.gz; do
    process ""$file""
done
</code></pre>
","33910"
"What's the quickest way to add text to a file from the command line?","69807","","<p>Occasionally I have a thought that I want to write into a file while I am at the terminal.  I would want these notes all in the same file, just listed one after the other.  I would also like a date / time tag on each one.</p>

<p>Is it possible to do this without having to open the file each time?  Can I just enter it into the terminal and have it appended to the file each time with a command or script?   </p>

<p>I am using GNU BASH.  </p>
","<p>Write yourself a shell script called ""n"".  Put this in it:</p>

<pre><code>#!/bin/sh
notefile=/home/me/notefile
date &gt;&gt; $notefile
emacs $notefile -f end-of-buffer
</code></pre>

<p>I recommend this instead of <code>cat &gt;&gt; notefile</code> because:</p>

<ol>
<li>One day you'll be in such a hurry that you'll fumblefinger the >> and type > instead and blow away your file.</li>
<li>Emacs starts in <em>five one-hundredths of a second</em> on my Mac Mini.  It takes a tenth of a second to start on a ten year old Celeron-based system I have sitting around.  If you can't wait that long to start typing, then you're already a machine and don't need to take notes. :)</li>
</ol>

<p>If you insist on avoiding a text editor, use a shell function:</p>

<pre><code>n () { date &gt;&gt; /home/me/notefile; cat &gt;&gt; /home/me/notefile; }
</code></pre>

<p>which should work in all shells claiming Bourne shell compatibility.</p>
","30133"
"Streaming to an Apple TV (3) from Linux","69765","","<p>Alright, here is the setup:</p>

<ul>
<li>One apple tv 3 in the living room, connected to the network (wired).</li>
<li>One pc running Linux (Arch Linux x64, AMD), containing all music, movies and series (wired).</li>
</ul>

<p>Now I know that the atv can stream music and stuff shared from a windows/mac using iTunes and home sharing. The box containing the files is however running Linux. </p>

<p>I've tried to setup forked-daapd, which is recognized as a server by all iTunes instances in house, but not by the atv. I've read that this is due to forked-daapd supporting DAAP, but not home sharing.</p>

<p>I've also tried several other DAAP servers for Linux:</p>

<ul>
<li>mt-daapd (the predecessor of forked-daapd), same story as with forked-daapd.</li>
<li>tangerine. Was recognized by iTunes, but didn't offer any media. Not sure whether the atv saw it, but I don't think so.</li>
<li>spydaap. This didn't work with the latest version of iTunes, which seems to be a <a href=""https://bugs.launchpad.net/spydaap/+bug/879924"">known issue</a></li>
</ul>

<p>I've also attempted to install the horror that is called iTunes on my pc running Linux, but that failed. It installed, gave a bunch of errors, starts, and does absolutely nothing. It's just a blank screen.</p>

<p>Now I know that normally such a setup would be possible by jailbreaking the atv, and installing XBMC on it. However, the atv3 is not yet jailbreakable, so unfortunately this is not an option for me.</p>

<p>Now my question is, what options do I have. Are there ways to make the atv recognize forked-daapd that I missed, or are there alternatives to forked-daapd that work with the atv? Is there an way to make iTunes run on my system? Is there any other way to stream the media on my Linux PC to the atv?</p>
","<p>One clunky solution would be to use virtualbox to emulate a lightweight xp install which would host the itunes software.  Create a samba share of your media files on the linux box and map that share to a network drive within the virtual pc, then add that mapped drive to the itunes library on the virtual machine.  Its not an elegant solution but at least it would work, and the host machine would still be running linux.</p>
","36836"
"How to start a cron job without reboot?","69716","","<p>I use a cron job to call <code>offlineimap</code> every 2 minutes:</p>

<pre><code>*/2 * * * * /usr/bin/offlineimap &gt; ~/Maildir/offlineimap.log 2&gt;&amp;1
</code></pre>

<p>I needed to kill the cron job to fix a problem. How can I then restart the cron job (without rebooting)? I found this 'solution' online:</p>

<pre><code>mylogin@myhost:~$ sudo /etc/init.d/cron restart
Rather than invoking init scripts through /etc/init.d, use the service(8)
utility, e.g. service cron restart

Since the script you are attempting to invoke has been converted to an
Upstart job, you may also use the stop(8) and then start(8) utilities,
e.g. stop cron ; start cron. The restart(8) utility is also available.
cron stop/waiting
cron start/running, process 26958
</code></pre>

<p>However, using <code>ps -ef | grep ...</code>, I don't see the job... What's wrong?</p>
","<h3>Cron approach</h3>

<p>If you have sudo privileges you could stop/start the cron service. I believe that's what that solution you found online was explaining. </p>

<p>Depending on which Linux distro you're using you could either do these commands:</p>

<pre><code># redhat distros
$ sudo /etc/init.d/crond stop
... do your work ...
$ sudo /etc/init.d/crond start
</code></pre>

<p>Or do these commands:</p>

<pre><code># Debian/Ubuntu distros
$ sudo service cron stop
... do your work ...
$ sudo service cron start
</code></pre>

<h3>Lock file type approach</h3>

<p>You could also put a ""dontrunofflineimap"" file in say the /tmp directory when you want the offlineimap task to hold off and not run for a bit.</p>

<p>The process would work like this. You touch a file in /tmp like so:</p>

<pre><code>touch /tmp/dontrunofflineimap
</code></pre>

<p>The cron job would be modified like so:</p>

<pre><code>*/2 * * * * [ -f /tmp/dontrunofflineimap ] || /usr/bin/offlineimap &gt; ~/Maildir/offlineimap.log 2&gt;&amp;1
</code></pre>

<p>While that file exists, it will essentially block the <code>offlineimap</code> app from running. When you want it to resume, simply delete the <code>/tmp/dontrunofflineimap</code> file.</p>
","74720"
"Break a large file into smaller pieces","69687","","<p>How do I break a large, <strong>+4GB</strong> file into smaller files of about <strong>500MB</strong> each. </p>

<p>And how do I <strong>re-assemble</strong> them again to get the original file? </p>
","<p>You can use <a href=""http://www.gnu.org/software/coreutils/manual/html_node/split-invocation.html""><em>split</em></a> and <em>cat</em>.</p>

<p>For example something like</p>

<pre><code>$ split --bytes 500M --numeric-suffixes --suffix-length=3 foo foo.
</code></pre>

<p>(where the input filename is <code>foo</code> and the last argument is the output prefix)</p>

<p>The same command with short options:</p>

<pre><code>$ split -b 100k -d -a 3 foo foo.
</code></pre>

<p>The split commands generate pieces named: <code>foo.000</code>, <code>foo.001</code> ...</p>

<p>For re-assembling the generated pieces again you can use e.g.:</p>

<pre><code>$ cat foo.* &gt; foo_2
</code></pre>

<p>(assuming that the shell sorts the results of shell globbing - and the number of parts does not exceed the system dependent limit of arguments)</p>

<p>You can compare the result via:</p>

<pre><code>$ cmp foo foo_2
$ echo $?
</code></pre>

<p>(which should output 0)</p>

<p>Alternatively, you can use a combination of find/sort/xargs to re-assemble the pieces:</p>

<pre><code>$ find -maxdepth 1 -type f -name 'foo.*'  | sort | xargs cat &gt; foo_3
</code></pre>
","1589"
"Can't resume screen, says I am already attached?","69668","","<p>I am working on a remote Debian Jessie server. I have started a screen session, started running a script, then been disconnected by a network timeout. </p>

<p>Now I have logged in again and want to resume the session.</p>

<p>This is what I see when I list screens:</p>

<pre><code>$ screen -ls
There are screens on:
    30608.pts-8.myserver    (11/03/2015 08:47:58 AM)    (Attached)
    21168.pts-0.myserver    (11/03/2015 05:29:24 AM)    (Attached)
    7006.pts-4.myserver (10/23/2015 09:05:45 AM)    (Detached)
    18228.pts-4.myserver    (10/21/2015 07:50:49 AM)    (Detached)
    17849.pts-0.myserver    (10/21/2015 07:43:53 AM)    (Detached)
5 Sockets in /var/run/screen/S-me.
</code></pre>

<p>I seem to be attached to two screens at once.</p>

<p>Now I want to resume the session I was running before, to see the results of my script:</p>

<pre><code>$ screen -r 30608.pts-8.myserver
There is a screen on:
    30608.pts-8.OpenPrescribing (11/03/2015 08:47:58 AM)    (Attached)
There is no screen to be resumed matching 30608.pts-8.myserver.
</code></pre>

<p>Why I can't I re-attach?</p>

<p>I have the same problem with the other screen:</p>

<pre><code>$ screen -r 21168.pts-0.myserver
There is a screen on:
    21168.pts-0.OpenPrescribing (11/03/2015 05:29:24 AM)    (Attached)
There is no screen to be resumed matching 21168.pts-0.myserver.
</code></pre>
","<p>The session is still attached on another terminal. The server hasn't detected the network outage on that connection: it only detects the outage when it tries to send a packet and gets an error back or no response after a timeout, but this hasn't happened yet. You're in a common situation where the client detected the outage because it tried to send some input and failed, but the server is just sitting there waiting for input. Eventually the server will send a keepalive packet and detect that the connection is dead.</p>

<p>In the meantime, use the <code>-d</code> option to detach the screen session from the terminal where it's in.</p>

<pre><code>screen -r -d 30608
</code></pre>

<p><strong><code>screen -rd</code></strong> is pretty much the standard way to attach to an existing screen session.</p>
","240606"
"How to set custom resolution using xrandr when the resolution is not available in 'Display Settings'","69667","","<p>I'm a new Linux user trying to change the screen resolution as there is no option under display. I have successfully managed to add new resolutions by fluke by following online guide. I don't have a GPU, I don't know if this is the issue? Below is my <code>xrandr -q</code> output.</p>

<pre><code>root@kali:~# xrandr -q
xrandr: Failed to get size of gamma for output default
Screen 0: minimum 1280 x 1024, current 1280 x 1024, maximum 1280 x 1024
default connected 1280x1024+0+0 0mm x 0mm
   1280x1024       0.0* 
  1920x1200_60.00 (0x145)  193.2MHz
        h: width  1920 start 2056 end 2256 total 2592 skew    0 clock   74.6KHz
        v: height 1200 start 1203 end 1209 total 1245           clock   59.9Hz
  1440x900_59.90 (0x156)  106.3MHz
        h: width  1440 start 1520 end 1672 total 1904 skew    0 clock   55.8KHz
        v: height  900 start  901 end  904 total  932           clock   59.9Hz
</code></pre>
","<p>Here are the steps you need to add a new custom resolution and apply it. Following steps are for adding a 1920x1080 resolution, but you can use it for any other resolution you want. But make sure your monitor and onboard graphics support that resolution.</p>

<pre><code># First we need to get the modeline string for xrandr
# Luckily, the tool ""gtf"" will help you calculate it.
# All you have to do is to pass the resolution &amp; the-
# refresh-rate as the command parameters:
gtf 1920 1080 60

# In this case, the horizontal resolution is 1920px the
# vertical resolution is 1080px &amp; refresh-rate is 60Hz.
# IMPORTANT: BE SURE THE MONITOR SUPPORTS THE RESOLUTION

# Typically, it outputs a line starting with ""Modeline""
# e.g. ""1920x1080_60.00""  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync
# Copy this entire string (except for the starting ""Modeline"")

# Now, use ""xrandr"" to make the system recognize a new
# display mode. Pass the copied string as the parameter
# to the --newmode option:
xrandr --newmode ""1920x1080_60.00""  172.80  1920 2040 2248 2576  1080 1081 1084 1118  -HSync +Vsync

# Well, the string within the quotes is the nick/alias
# of the display mode - you can as well pass something
# as ""MyAwesomeHDResolution"". But, careful! :-|

# Then all you have to do is to add the new mode to the
# display you want to apply, like this:
xrandr --addmode VGA1 ""1920x1080_60.00""

# VGA1 is the display name, it might differ for you.
# Run ""xrandr"" without any parameters to be sure.
# The last parameter is the mode-alias/name which
# you've set in the previous command (--newmode)

# It should add the new mode to the display &amp; apply it.
# Usually unlikely, but if it doesn't apply automatically
# then force it with this command:
xrandr --output VGA1 --mode ""1920x1080_60.00""
</code></pre>

<p>Original source: <a href=""https://gist.github.com/debloper/2793261"">https://gist.github.com/debloper/2793261</a></p>

<p>I also wrote a script that does all these steps automatically. You can try it out if the above steps seem too complicated for you: <a href=""https://gist.github.com/chirag64/7853413"">https://gist.github.com/chirag64/7853413</a></p>
","227894"
"How to install Python 3.6?","69623","","<p>I'd like to install the latest Python, which is 3.6 at the time of this post. However, the repository is saying that Python 3.4.2 is the newest version.</p>

<p>I've tried:</p>

<pre><code>sudo apt-get update

sudo apt-get install python3

python3 is already the newest version.

python -V

Python 3.4.2
</code></pre>

<p>To upgrade to Python 3.6 on my Windows workstation, I simply downloaded an exe, clicked ""next"" a few times, and it's done. What's the proper and officially accepted procedure to install Python 3.6 on Debian Jessie?</p>
","<p>Debian does not have Python 3.6 in its repositories, but testing has it. </p>

<pre><code>$ sudo nano /etc/apt/sources.list
# add
deb http://ftp.de.debian.org/debian testing main
$ echo 'APT::Default-Release ""stable"";' | sudo tee -a /etc/apt/apt.conf.d/00local
$ sudo apt-get update
$ sudo apt-get -t testing install python3.6
$ python3.6 -V
</code></pre>

<p>You asked for:</p>

<blockquote>
  <p>the proper and officially accepted procedure</p>
</blockquote>

<p>but I must point it out that this is not official solution because it uses testing repositories.</p>
","340482"
"Copy over existing files without confirmation?","69531","","<p>I need to copy and over-write a large amount of files, I've used the following command:</p>

<pre><code># cp -Rf * ../
</code></pre>

<p>But then whenever a file with the same name exists on the destination folder I get this question:</p>

<pre><code>cp: overwrite `../ibdata1'? 
</code></pre>

<p>The Problem is that I have about 200 files which are going to be over-written and I don't think that pressing <code>Y then Enter</code> 200 times is the right way to do it.</p>

<p>So, what is the right way to that?</p>
","<p>You can do <code>yes | cp -rf myxx</code>, Or if you do it as root - your .bashrc or .profile has an alias of cp to cp -i, most modern systems do that to root profiles.</p>

<p>You can temporarily bypass an alias and use the non-aliased version of a command by prefixing it with \, e.g. \cp whatever</p>
","146442"
"How to create tar archive in a different directory?","69473","","<p>I want to create a tar archive in a different directory rather than the current directory.</p>

<p>I tried this command:</p>

<pre><code>tar czf file.tar.gz file1 -C /var/www/
</code></pre>

<p>but it creates the archive in the current directory. Why?</p>
","<p>The easy way, if you don't particularly <em>need</em> to use <code>-C</code> to tell <code>tar</code> to change to some other directory, is to simply specify the full path to the archive on the command line. Then you can be in whatever directory you prefer to create the directory structure that you want inside the archive.</p>

<p>The following will create the archive <code>/var/www/file.tar.gz</code> and put <code>file1</code> from the current directory (whatever that happens to be) in it, with no in-archive path information.</p>

<pre><code>tar czf /var/www/file.tar.gz file1
</code></pre>

<p>The path (to either the archive, the constituent files, or both) can of course also be relative. If <code>file1</code> is in <code>/tmp</code>, you are in <code>/var/spool</code> and want to create the archive in <code>/var/www</code>, you could use something like:</p>

<pre><code>tar czf ../www/file1.tar.gz /tmp/file1
</code></pre>

<p>There's a million variations on the theme, but this should get you started. Add the <code>v</code> flag if you want to see what <code>tar</code> actually does.</p>
","64897"
"How can I reliably get the operating system's name?","69450","","<p>Say I am logged into a remote system, how can I know what it's running? On most modern Linuxes (Linuces?), you have the <code>lsb_release</code> command:</p>

<pre><code>$ lsb_release -ic    
Distributor ID: LinuxMint
Codename:       debian
</code></pre>

<p>Which as far as I can tell just gives the same info as <code>/etc/lsb-release</code>. What if that file is not present? I seem to recall that the <code>lsb_release</code> command is relatively new so what if I have to get the OS of an older system?</p>

<p>In any case, <code>lsb</code> stands for <code>Linux Standard Base</code> so I am assuming it won't work on non-Linux Unices. As far as I know, there is no way of getting this information from <code>uname</code> so how can I get this on systems that do not use <code>lsb_release</code>?</p>
","<p><code>lsb_release -a</code> is likely going to be your best option for finding this information out, and being able to do so in a consistent way. </p>

<h3>History of LSB</h3>

<p>The <code>lsb</code> in that command stands for the project <a href=""http://www.linuxfoundation.org/collaborate/workgroups/lsb"">Linux Standards Base</a> which is an umbrella project sponsored by the <a href=""http://www.linuxfoundation.org/"">Linux Foundation</a> to provide generic methods for doing basic kinds of things on various Linux distros. </p>

<p>The project is voluntary and vendors can participate within the project as just a user and also as facilitators of the various specifications around different modules that help to drive standardization within the different Linux distributions.</p>

<p><em>excerpt from the <a href=""http://www.linuxfoundation.org/collaborate/workgroups/lsb/about"">charter</a></em></p>

<blockquote>
  <p>The LSB workgroup has, as its core goal, to address these two
  concerns. We publish a standard that describes the minimum set of APIs
  a distribution must support, in consultation with the major
  distribution vendors. We also provide tests and tools which measure
  support for the standard, and enable application developers to target
  the common set. Finally, through our testing work, we seek to prevent
  unnecessary divergence between the distributions.</p>
</blockquote>

<h3>Useful links related to LSB</h3>

<ul>
<li><a href=""http://www.linuxfoundation.org/collaborate/workgroups/lsb/lsb-charter"">LSB Charter</a></li>
<li><a href=""http://www.linuxfoundation.org/collaborate/workgroups/lsb"">LSB Workgroup</a></li>
<li><a href=""http://linuxfoundation.org/en/LSB_Roadmap"">LSB Roadmap</a></li>
<li><a href=""http://lists.linuxfoundation.org/mailman/listinfo/lsb-discuss"">LSB Mailing List (current activity is here!)</a></li>
<li><a href=""https://www.linuxbase.org/lsb-cert/productdir.php?by_lsb"">List of certified LSB products</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Linux_Standard_Base"">LSB Wikipedia page</a></li>
</ul>

<h3>Criticisms</h3>

<p>There are a number of problems with LSB that make it problematic for distros such as Debian. The forced usage of RPM being one. See the <a href=""http://en.wikipedia.org/wiki/Linux_Standard_Base#Reception"">Wikipedia article for more on the matter</a>.</p>

<h3>Novell</h3>

<p>If you search you'll possibly come across a fairly dated looking page titled: <a href=""http://www.novell.com/coolsolutions/feature/11251.html"">Detecting Underlying Linux Distro</a> from Novell. This is one of the few places I""ve seen an actual list that shows several of the major distros and how you can detect what underlying one you're using.</p>

<p><em>excerpt</em></p>

<pre><code>Novell SUSE         /etc/SUSE-release
Red Hat             /etc/redhat-release, /etc/redhat_version
Fedora              /etc/fedora-release
Slackware           /etc/slackware-release, /etc/slackware-version
Debian              /etc/debian_release, /etc/debian_version,
Mandrake            /etc/mandrake-release
Yellow dog          /etc/yellowdog-release
Sun JDS             /etc/sun-release
Solaris/Sparc       /etc/release
Gentoo              /etc/gentoo-release
UnitedLinux         /etc/UnitedLinux-release
ubuntu              /etc/lsb-release
</code></pre>

<p>This same page also includes a handy script which attempts to codify for the above using just vanilla <code>uname</code> commands, and the presence of one of the above files.</p>

<p><strong>NOTE:</strong> This list is dated but you could easily drop the dated distros such as Mandrake from the list and replace them with alternatives. This type of a script might be one approach if you're attempting to support a large swath of Solaris &amp; Linux variants.</p>

<h3>Linux Mafia</h3>

<p>More searching will turn up the following page maintained on Linuxmafia.com, titled:  <a href=""http://linuxmafia.com/faq/Admin/release-files.html"">/etc/release equivalents for sundry Linux (and other Unix) distributions</a>. This is probably the most exhaustive list to date that I've seen. You could codify this list with a case/switch statement and include it as part of your software distribution.</p>

<p>In fact there is a script at the bottom of that page that does exactly that. So you could simply download and use the script as 3rd party to your software distribution.</p>

<p><em>script</em></p>

<pre><code>#!/bin/sh
# Detects which OS and if it is Linux then it will detect which Linux
# Distribution.

OS=`uname -s`
REV=`uname -r`
MACH=`uname -m`

GetVersionFromFile()
{
    VERSION=`cat $1 | tr ""\n"" ' ' | sed s/.*VERSION.*=\ // `
}

if [ ""${OS}"" = ""SunOS"" ] ; then
    OS=Solaris
    ARCH=`uname -p` 
    OSSTR=""${OS} ${REV}(${ARCH} `uname -v`)""
elif [ ""${OS}"" = ""AIX"" ] ; then
    OSSTR=""${OS} `oslevel` (`oslevel -r`)""
elif [ ""${OS}"" = ""Linux"" ] ; then
    KERNEL=`uname -r`
    if [ -f /etc/redhat-release ] ; then
        DIST='RedHat'
        PSUEDONAME=`cat /etc/redhat-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/redhat-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/SuSE-release ] ; then
        DIST=`cat /etc/SuSE-release | tr ""\n"" ' '| sed s/VERSION.*//`
        REV=`cat /etc/SuSE-release | tr ""\n"" ' ' | sed s/.*=\ //`
    elif [ -f /etc/mandrake-release ] ; then
        DIST='Mandrake'
        PSUEDONAME=`cat /etc/mandrake-release | sed s/.*\(// | sed s/\)//`
        REV=`cat /etc/mandrake-release | sed s/.*release\ // | sed s/\ .*//`
    elif [ -f /etc/debian_version ] ; then
        DIST=""Debian `cat /etc/debian_version`""
        REV=""""

    fi
    if [ -f /etc/UnitedLinux-release ] ; then
        DIST=""${DIST}[`cat /etc/UnitedLinux-release | tr ""\n"" ' ' | sed s/VERSION.*//`]""
    fi

    OSSTR=""${OS} ${DIST} ${REV}(${PSUEDONAME} ${KERNEL} ${MACH})""

fi

echo ${OSSTR}
</code></pre>

<p><strong>NOTE:</strong> This script should look familiar, it's an up to date version of the Novell one!</p>

<h3>Legroom script</h3>

<p>Another method I've seen employed is to roll your own script, similar to the above Novell method but making use of LSB instead. This article titled: <a href=""http://legroom.net/2010/05/05/generic-method-determine-linux-or-unix-distribution-name"">Generic Method to Determine Linux (or UNIX) Distribution Name</a>, shows one such method.</p>

<pre><code># Determine OS platform
UNAME=$(uname | tr ""[:upper:]"" ""[:lower:]"")
# If Linux, try to determine specific distribution
if [ ""$UNAME"" == ""linux"" ]; then
    # If available, use LSB to identify distribution
    if [ -f /etc/lsb-release -o -d /etc/lsb-release.d ]; then
        export DISTRO=$(lsb_release -i | cut -d: -f2 | sed s/'^\t'//)
    # Otherwise, use release info file
    else
        export DISTRO=$(ls -d /etc/[A-Za-z]*[_-][rv]e[lr]* | grep -v ""lsb"" | cut -d'/' -f3 | cut -d'-' -f1 | cut -d'_' -f1)
    fi
fi
# For everything else (or if above failed), just use generic identifier
[ ""$DISTRO"" == """" ] &amp;&amp; export DISTRO=$UNAME
unset UNAME
</code></pre>

<p>This chunk of code could be included into a system's <code>/etc/bashrc</code> or some such file which would then set the environment variable <code>$DISTRO</code>.</p>

<h3>gcc</h3>

<p>Believe it or not another method is to make use of <code>gcc</code>. If you query the command <code>gcc --version</code> you'll get the distro that gcc was built for, which is invaribly the same as the system it's running on.</p>

<p><em>Fedora 14</em></p>

<pre><code>$ gcc --version
gcc (GCC) 4.5.1 20100924 (Red Hat 4.5.1-4)
Copyright (C) 2010 Free Software Foundation, Inc.
</code></pre>

<p><em>CentOS 5.x</em></p>

<pre><code>$ gcc --version
gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-54)
Copyright (C) 2006 Free Software Foundation, Inc.
</code></pre>

<p><em>CentOS 6.x</em></p>

<pre><code>$ gcc --version
gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-3)
Copyright (C) 2010 Free Software Foundation, Inc.
</code></pre>

<p><em>Ubuntu 12.04</em></p>

<pre><code>$ gcc --version
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
</code></pre>

<h3>TL;DR;</h3>

<p>So which one should I use? I would tend to go with <code>lsb_release -a</code> for any Linux distributions that I would frequent (RedHat, Debian, Ubuntu, etc.). For situations where you're supporting systems that don't provide <code>lsb_release</code> I'd roll my own as part of the distribution of software that I'm providing, similar to one of the above scripts.</p>

<h3>UPDATE #1: Follow-up with SuSE</h3>

<p>In speaking with @Nils in the comments below it was determined that for whatever reason, SLES11 appeared to drop LSB from being installed by default. It was only an optional installation, which seemed counter for a package that provides this type of key feature.</p>

<p>So I took the opportunity to contact someone from the OpenSuSE project to get a sense of why.</p>

<p><em>excerpt of email</em></p>

<pre><code>Hi Rob,

I hope you don't mind me contacting you directly but I found your info here: 
https://en.opensuse.org/User:Rjschwei. I participate on one of the StackExchange 
sites, Unix &amp; Linux and a question recently came up regarding the best option 
for determining the underlying OS.

http://unix.stackexchange.com/questions/92199/how-can-i-reliably-get-the-operating-systems-name/92218?noredirect=1#comment140840_92218

In my answer I suggested using lsb_release, but one of the other users mentioned 
that this command wasn't installed as part of SLES11 which kind of surprised me. 
Anyway we were looking for some way to confirm whether this was intentionally 
dropped from SLES or it was accidental.

Would you know how we could go about confirming this one way or another?

Thanks for reading this, appreciate any help and/or guidance on this.

-Sam Mingolelli
http://unix.stackexchange.com/users/7453/slm
</code></pre>

<p><em>Here's Rob's response</em></p>

<pre><code>Hi,

On 10/01/2013 09:31 AM, Sam Mingo wrote:
- show quoted text -

lsb_release was not dropped in SLES 11. SLES 11 is LSB certified. However, it 
is not installed by default, which is consistent with pretty much every other
distribution. The lsb_release command is part of the lsb-release package.

At present almost every distribution has an entry in /etc such as 
/etc/SuSE-release for SLES and openSUSE. Since this is difficult for ISVs and 
others there is a standardization effort going on driven by the convergence to 
systemd. The standard location for distribution information in the future will 
be /etc/os-release, although Ubuntu will probably do something different.

HTH,    
Robert

--  Robert Schweikert                           MAY THE SOURCE BE WITH YOU    
SUSE-IBM Software Integration Center                   LINUX    
Tech Lead    
Public Cloud Architect 
</code></pre>
","92218"
"How to get memory used(RAM used) using Linux command?","69408","","<p>I am trying to retrieve memory used(RAM) in percentage using Linux commands. My cpanel shows Memory Used which I need to display on a particular webpage. </p>

<p>From forums, I found out that correct memory can be found from the following:</p>

<pre><code>free -m
</code></pre>

<p>Result:</p>

<pre><code>-/+ buffers/cache:        492       1555
</code></pre>

<p>-/+ buffers/cache: contains the correct memory usage. I don't know how to parse this info or if there is any different command to get the memory used in percentage.</p>
","<p>Here is sample output from free:</p>

<pre><code>% free
             total       used       free     shared    buffers     cached
Mem:      24683904   20746840    3937064     254920    1072508   13894892
-/+ buffers/cache:    5779440   18904464
Swap:      4194236        136    4194100
</code></pre>

<p>The first line of numbers (<code>Mem:</code>) lists</p>

<ul>
<li><code>total</code> memory</li>
<li><code>used</code> memory</li>
<li><code>free</code> memory</li>
<li>usage of <code>shared</code></li>
<li>usage of <code>buffers</code></li>
<li>usage filesystem caches (<code>cached</code>)</li>
</ul>

<p>In this line <code>used</code> <em>includes the buffers and cache</em> and this impacts free.
This is not your ""true"" free memory because the system will dump cache if needed to satisfy allocation requests.</p>

<p>The next line (<code>-/+ buffers/cache:</code>) gives us the actual used and free memory as if there were no buffers or cache.  </p>

<p>The final line (<code>Swap</code>) gives the usage of swap memory. There is no buffer or cache for swap as it would not make sense to put these things on a physical disk.</p>

<p>To output used memory (minus buffers and cache) you can use a command like:</p>

<pre><code>% free | awk 'FNR == 3 {print $3/($3+$4)*100}'
23.8521
</code></pre>

<p>This grabs the third line and divides used/total * 100. </p>

<p>And for free memory:</p>

<pre><code>% free | awk 'FNR == 3 {print $4/($3+$4)*100}' 
76.0657
</code></pre>
","152301"
"Changing timezone on Debian keeps Local Time in UTC","69285","","<p>I'm trying to get programs to log in local time for my own sanity.</p>

<p>I have updated my timezone with:</p>

<pre><code> dpkg-reconfigure tzdata
</code></pre>

<p>But the result of that command is:</p>

<pre><code>Current default time zone: 'Australia/Adelaide'
Local time is now:      Mon May 20 03:09:52 UTC 2013.
Universal Time is now:  Mon May 20 03:09:52 UTC 2013.
</code></pre>

<p>Notice the <strong>UTC</strong> in <code>Local time</code></p>

<p>Any reason why this may be?</p>

<p>I have done a lot of Googling but my problem seems different to all of them :(</p>

<p><strong>Here are some more details:</strong></p>

<pre><code># cat /etc/timezone
Australia/Adelaide

# date
Mon May 20 03:41:06 UTC 2013

# export TZ='Australia/Adelaide'; date
Mon May 20 13:16:11 CST 2013
</code></pre>

<p>Setting <code>export TZ='Australia/Adelaide';</code> in my <code>/etc/profile</code> makes <code>date</code> work by default in a bash session but does not change the system log date (after restarting the service)</p>

<p><strong>Edit:</strong></p>

<pre><code># ls -l /etc/localtime
lrwxrwxrwx 1 root root 20 May 10 14:48 /etc/localtime -&gt; /usr/share/zoneinfo/

# ls /etc/localtime/
Adelaide    Chile    GMT        Japan      PST8PDT    Universal
Africa      Cuba     GMT+0      Kwajalein  Pacific    W-SU
America     EET      GMT-0      Libya      Poland     WET
Antarctica  EST      GMT0       MET        Portugal   Zulu
Arctic      EST5EDT  Greenwich  MST        ROC        iso3166.tab
Asia        Egypt    HST        MST7MDT    ROK        localtime
Atlantic    Eire     Hongkong   Mexico     Singapore  localtime.dpkg-new
Australia   Etc      Iceland    Mideast    SystemV    posix
Brazil      Europe   Indian     NZ         Turkey     posixrules
CET         Factory  Iran       NZ-CHAT    UCT        right
CST6CDT     GB       Israel     Navajo     US         zone.tab
Canada      GB-Eire  Jamaica    PRC        UTC
</code></pre>

<h2>Answer:</h2>

<p>Worked it out thanks to jamzed. for some reason I had /etc/localtime as a symlink... the IT Guy here set up the server using Turnkey 12 so maybe that was the problem.</p>

<pre><code># mv /etc/localtime /etc/localtime.old
# cp /usr/share/zoneinfo/Australia/Adelaide /etc/localtime
# date
Thu May 23 09:36:17 CST 2013
</code></pre>
","<p>Try this way:</p>

<pre><code>$ sudo cp /usr/share/zoneinfo/Australia/Adelaide /etc/localtime
</code></pre>
","76711"
"How do I pass a list of files to grep","69274","","<p>I am using <code>find</code> and getting a list of files I want to <code>grep</code> through. How do I pipe that list to <code>grep</code>?</p>
","<p>Well, the generic case that works with any command that writes to stdout is to use <code>xargs</code>, which will let you attach any number of command-line arguments to the end of a command:</p>

<pre><code>$ find … | xargs grep 'search'
</code></pre>

<p>Or to embed the command in your <code>grep</code> line with backticks or <code>$()</code>, which will run the command and substitute its output:</p>

<pre><code>$ grep 'search' $(find …)
</code></pre>

<p>Note that these commands don't work if the file names contain whitespace, or certain other “weird characters” (<code>\'""</code> for xargs, <code>\[*?</code> for <code>$(find …)</code>).</p>

<hr>

<p>However, in the specific case of <code>find</code> the ability to execute a program on the given arguments is built-in:</p>

<pre><code>$ find … -exec grep 'search' {} \;
</code></pre>

<p>Everything between <code>-exec</code> and <code>;</code> is the command to execute; <code>{}</code> is replaced with the filename found by <code>find</code>. That will execute a separate <code>grep</code> for each file; since <code>grep</code> can take many filenames and search them all, you can change the <code>;</code> to <code>+</code> to tell find to pass all the matching filenames to <code>grep</code> at once:</p>

<pre><code>$ find … -exec grep 'search' {} \+
</code></pre>
","20264"
"Understanding /etc/aliases and what it does","69221","","<p>These are the contents of '/etc/aliases' file on my Debian (Wheezy) server, as it is:</p>

<pre><code># /etc/aliases
mailer-daemon: postmaster
postmaster: root
nobody: root
hostmaster: root
usenet: root
news: root
webmaster: root
www: root
ftp: root
abuse: root
noc: root
security: root
root: t
</code></pre>

<p><strong>1.</strong> I noticed that, by default, my server sends email from what looks like <code>root@hostname.domain.com</code>. So, which one of the rules above governs this? <code>postmaster: root;</code>?</p>

<p><strong>2.</strong> So, the rules in '/etc/aliases' are used to assign users to specific departments? That is, for example, all emails to be sent/received for 'abuse' will be delivered from/to root@hostname.domain.com (which'd be the default email for root, unless there's an alias). Correct?</p>

<p><strong>3.</strong> Can someone please explain what each of these really meant for -- mailer-daemon, postmaster, nobody, hostmaster, usenet, news, webmaster, www, ftp, abuse, noc, security, root?</p>

<p>I mean, a description like <em>""<code>mailer-daemon</code> for sending email delivery errors, but not really meant for receiving emails. <code>security</code> for where people should contact your about security issues""</em>, or something like that.</p>

<p>As obvious as it should be by now, this is a newbie question. So, please try to be as clear as possible.</p>
","<p>The <code>/etc/aliases</code> file is part of <code>sendmail</code>. It specifies which account mail sent to an alias should really be delivered to.  For example, mail to the <code>ftp</code> account would be sent to root's mailbox in the configuration you show.</p>

<p>Multiple recipients can be specified as comma-separated lists, too.</p>

<p>Redirecting mail to users isn't all that can be done.  Mail can be piped to programs, too, or simply directed into a file of your choice.  The following would ""bit-bucket"" all mail from the user ""somebody"":</p>

<p>somebody : /dev/null</p>

<p>Modifications to the <code>/etc/aliases</code> file are not complete until the <code>newaliases</code> command is run to build <code>/etc/aliases.db</code>.  It is in this later form that <code>sendmail</code> actually uses.</p>
","65015"
"Where can I find a list of 'make' error codes?","69168","","<p>I am trying to compile a program written in Fortran using <code>make</code> (I have a <strong>Makefile</strong> and, while in the directory containing the <strong>Makefile</strong>, I type the command <code>$ make target</code>, where ""target"" is a system-specific target specification is present in my <strong>Makefile</strong>.  As I experiment with various revisions of my target specification, I often get a variety of error messages when attempting to call <code>make</code>.  To give a few examples:</p>

<pre><code>make[1]: Entering directory
/bin/sh: line 0: test: too many arguments
./dpp   angfrc.f &gt; angfrc.tmp.f
/bin/sh: ./dpp: Permission denied
make[1]: *** [angfrc.o] Error 126
make[1]: Leaving directory
make: *** [cmu60] Error 2
</code></pre>

<p>and</p>

<pre><code>make[1]: Entering directory
/bin/sh: line 0: test: too many arguments
./dpp -DSTRESS -DMPI -P -D'pointer=integer'-I/opt/mpich_intel/include  angfrc.f &gt; angfrc.tmp.f
/bin/sh: ./dpp: Permission denied
make[1]: *** [angfrc.o] Error 126
make[1]: Leaving directory 
make: *** [mpich-c2] Error 2
</code></pre>

<p>and</p>

<pre><code>make[1]: Entering directory 
/bin/sh: line 0: test: too many arguments
./dpp -DSTRESS -DMPI -P -D'pointer=integer' -I/opt/mpich_intel/include  angfrc.f &gt; angfrc.tmp.f
/bin/sh: ./dpp: Permission denied
make[1]: *** [angfrc.o] Error 126
make[1]: Leaving directory 
make: *** [mpi-intel] Error 2
</code></pre>

<p>Do you know how I can find a list of what the error codes, such as ""Error 126"" and ""Error 2,"" mean?  I found <a href=""http://www.unix.com/unix-advanced-expert-users/19853-gnus-make-error-codes-list.html"">this thread</a> on another website, but I am not sure what the reply means.  Does it mean that there is no system-independent meaning of the <code>make</code> error codes?  Can you please help me?  Thank you.</p>
","<p>The error codes aren't from make: make is reporting the return status of the command that failed. You need to look at the documentation of each command to know what each status value means. Most commands don't bother with distinctions other than 0 = success, anything else = failure.</p>

<p>In each of your examples, <code>./dpp</code> cannot be executed. When this happens, the shell that tried to invoke it exits with status code 126 (this is <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_08_02"">standard behavior</a>). The instance of make that was running that shell detects a failed command (the shell) and exits, showing you <code>Error 126</code>. That instance of make is itself a command executed by a parent instance of make, and the <code>make</code> utility returns 2 on error, so the parent make reports <code>Error 2</code>.</p>

<p>The failure of your build is likely to stem from <code>test: too many arguments</code>. This could be a syntax error in the makefile, or it could be due to relying on bash-specific features when you have a <code>/bin/sh</code> that isn't bash. Try running <code>make SHELL=/bin/bash target</code> or <code>make SHELL=/bin/ksh target</code>; if that doesn't work, you need to fix your makefile.</p>
","18808"
"How to know where a program is stuck in linux?","69151","","<p>I am running the following command on my ubuntu server</p>

<pre><code>root@slot13:~# lxc-stop --name pavan --logfile=test1.txt --logpriority=trace
</code></pre>

<p>It seems to hang indefinitely. Whenever this happened on AIX, I simply used to get the PID of the offending process and say</p>

<pre><code>$ procstack &lt;pid_of_stuck_process&gt;
</code></pre>

<p>and it used to show the whole callstack of the process. Is there any equivalent of <code>procstack</code> in linux/ubuntu?</p>
","<p>My first step would be to run strace on the process, best</p>

<pre><code> strace -s 99 -ffp 12345
</code></pre>

<p>if your process ID is 12345. This will show you all syscalls the program is doing. <a href=""http://try-linux.blogspot.de/2013/12/how-to-strace-process.html"">How to strace a process</a> tells you more.</p>

<p>If you insist on getting a stacktrace, google tells me the equivalent is pstack. But as I do not have it installed I use gdb:</p>

<pre><code> tweedleburg:~ # sleep 3600 &amp;
 [2] 2621
 tweedleburg:~ # gdb
 (gdb) attach 2621
 (gdb) bt
 #0  0x00007feda374e6b0 in __nanosleep_nocancel () from /lib64/libc.so.6
 #1  0x0000000000403ee7 in ?? ()
 #2  0x0000000000403d70 in ?? ()
 #3  0x000000000040185d in ?? ()
 #4  0x00007feda36b8b05 in __libc_start_main () from /lib64/libc.so.6
 #5  0x0000000000401969 in ?? ()
 (gdb)
</code></pre>
","166544"
"How to solve the issue that a Terminal screen is messed up? (usually after a resizing)","69142","","<p>Sometimes, a terminal screen is messed up, and when we use <code>man ls</code> to read the manpages, or press the UP arrow to go to previous commands in history, the screen will show characters not as the right place.  (for example, treat the end of screen as some where in the middle of the screen).</p>

<p>The command <code>reset</code> is tried and it wouldn't work.  One way that works is to log out or close the window, and resize the window first, and then do <code>ssh</code> (or close that tab, and resize the window, and then open a new tab to get a new shell).</p>

<p>But this way, we will lose anything that we previously did, such as starting a virtual machine console, etc.  So if we don't close the shell, is there a way to fix this problem?</p>

<p>(this happened before right inside Fedora, and also for a Macbook <code>ssh</code> into a RHEL 5.4 box).</p>

<p><strong>Update:</strong> I remember now how it happened in Fedora: I opened up a Terminal, and did a FreeVM to use a console of a Virtual Machine (a shell).  I think it was size 80 x 25 and then after a while, I resized the Terminal to 130 x 50 approximately, and then the ""inner shell"" (of the VM) started to behave weird).</p>
","<p>If you are using bash, check if ""checkwinsize"" option is activated in your session using</p>

<pre><code>shopt | grep checkwinsize
</code></pre>

<p>If you don't get </p>

<pre><code>checkwinsize    on
</code></pre>

<p>then activate it with</p>

<pre><code>shopt -s checkwinsize
</code></pre>

<p>Bash documentation says for ""checkwinsize"" attribute : </p>

<blockquote>
  <p>""If set, Bash checks the window size after each command and, if
  necessary, updates the values of LINES and COLUMNS.""</p>
</blockquote>

<p>If you like the setting, you could activate <code>checkwinsize</code> in your <code>~/.bashrc</code>.</p>

<ul>
<li>To activate: <code>shopt -s checkwinsize</code></li>
<li>To deactivate: <code>shopt -u checkwinsize</code></li>
</ul>
","61608"
"make fatal error: openssl/sha.h: No such file or directory","69071","","<p>I'm trying to compile a program that, according to the documentation, requires the ""OpenSSL library"". I have OpenSSL installed, and it's still giving me the error <code>openssl/sha.h: No such file or directory</code>. Is there some other library that has to be installed?</p>
","<p>probably you are missing the openssl header files. depending on your distribution this package might have a different name, mostly it's something like <code>openssl-dev</code> or <code>openssl-devel</code>. after you installed the openssl header files, the compiler should be able to find openssl/sha.h.</p>

<p>In Ubuntu/Debian the package is called <code>libssl-dev</code>.</p>
","87463"
"What are high memory and low memory on Linux?","68911","","<p>I'm interested in the difference between Highmem and Lowmem:</p>

<ol>
<li>Why is there such a differentiation?</li>
<li>What do we gain by doing so?</li>
<li>What features does each have?  </li>
</ol>
","<p>On a 32-bit architecture, the address space range for addressing RAM is: </p>

<pre><code>0x00000000 - 0xffffffff
</code></pre>

<p>or <code>4'294'967'295</code> (4 GB).</p>

<p>The linux kernel splits that up 3/1 (could also be 2/2, or 1/3 <sup>1</sup>) into user space (high memory) and kernel space (low memory) respectively.</p>

<p>The user space range: </p>

<pre><code>0x00000000 - 0xbfffffff
</code></pre>

<p>Every newly spawned user process gets an address (range) inside this area. User processes are generally untrusted and therefore are forbidden to access the kernel space. Further, they are considered non-urgent, as a general rule, the kernel tries to defer the allocation of memory to those processes.</p>

<p>The kernel space range: </p>

<pre><code>0xc0000000 - 0xffffffff
</code></pre>

<p>A kernel processes gets its address (range) here. The kernel can directly access this 1 GB of addresses (well, not the full 1 GB, there are 128 MB reserved for high memory access).</p>

<p>Processes spawned in kernel space are trusted, urgent and assumed error-free, the memory request gets processed instantaneously.</p>

<p>Every kernel process can also access the user space range if it wishes to. And to achieve this, the kernel maps an address from the user space (the high memory) to its kernel space (the low memory), the 128 MB mentioned above are especially reserved for this.</p>

<hr>

<p><sup>1</sup> Whether the split is 3/1, 2/2, or 1/3 is controlled by the <code>CONFIG_VMSPLIT_...</code> option; you can probably check under <code>/boot/config*</code> to see which option was selected for your kernel.</p>
","5151"
"How can I find out which users are in a group within Linux?","68882","","<p>I've recently been creating new users and assigning them to certain groups. I was wondering if there is a command that shows all the users assigned to a certain group?
I have tried using the 'groups' command however whenever I use this it says 'groups: not found'</p>
","<p>You can use grep:</p>

<pre><code>grep '^group_name_here:' /etc/group
</code></pre>

<p>This only lists supplementary group memberships, not the user who have this group as their primary group. And it only finds local groups, not groups from a network service such as LDAP.</p>
","241216"
"What is the point of sshd “UseDNS” option?","68857","","<p>I know what it does, but I don't know <em>why</em>. What attack(s) does it prevent?</p>

<p>Is it relevant for all kind of authentication methods? (hostbased, password, publickey, keyboard-interactive ...)</p>
","<p>The <code>UseDNS</code> option is mostly useless. If the client machines are out there on the Internet, there is a high chance that they don't have any reverse DNS, their reverse DNS doesn't resolve forward, or their DNS doesn't provide any information other than “belongs to this ISP” which the IP address already tells you.</p>

<p>In typical configurations, DNS is only used for logging. It can be used for authentication, but only if <code>IgnoreRhosts no</code> is specified in <code>sshd_config</code>. This is for compatibility with old installations that used rsh, where you can say “the user called <code>bob</code> on the machine called <code>darkstar</code> may log in as <code>alice</code> without showing any credentials” (by writing <code>darkstar bob</code> in <code>~alice/.rhosts</code>). It is only secure if you trust all the machines that may possibly be connecting to the ssh server. In other words, this is very very rarely usable in a secure way.</p>

<p>Given that the DNS lookup doesn't provide any useful information except in very peculiar circumstances, it should be turned off. As far as I can tell, the only reason it's on by default is that it's technically more secure (if you're concerned about authentication, not availability), even though that only applies to a tiny set of circumstances.</p>

<p>Another argument for turning off this feature is that every superfluous feature is an unnecessary <a href=""https://googleonlinesecurity.blogspot.ca/2016/02/cve-2015-7547-glibc-getaddrinfo-stack.html"">security risk</a>.</p>
","56947"
"How to copy all files from a directory to a remote directory using scp?","68670","","<p>My goal is copy only <strong>all files</strong> from <strong>~/local_dir</strong> to <strong>user@host.com /var/www/html/target_dir</strong> using <strong>scp</strong> and do not create local_dir category in local_dir. </p>

<pre><code>/var/www/html/target_dir/files..
</code></pre>

<p>but not </p>

<p><code>/var/www/html/target_dir/local_dir/files..</code> when use -r parameter</p>
","<p>scp has the -r argument. So, try using:</p>

<pre><code>$ scp -r ~/local_dir user@host.com:/var/www/html/target_dir
</code></pre>

<p>The -r argument works just like the -r arg in cp, it will transfer your entire folder and all the files and subdirectories inside.</p>
","232947"
"As root, how can I list the crontabs for all users?","68470","","<p>I have a script being run automatically that I can't find in the crontab for the expected users, so I'd like to search all users' crontabs for it.</p>

<p>Essentially I want to run a <code>crontab -l</code> for all users.</p>
","<p>Well depends on the script but easily you can find your crontab as root with</p>

<pre><code>crontab -l -u &lt;user&gt;
</code></pre>

<p>Or you can find crontab from spool where is located file for all users</p>

<pre><code>cat /var/spool/cron/crontabs/&lt;user&gt;
</code></pre>

<p>To show all users' crontabs with the username printed at the beginning of each line:</p>

<pre><code>cd /var/spool/cron/crontabs/ &amp;&amp; grep . *
</code></pre>
","119602"
"Install python pip in Debian Wheezy","68295","","<p>How to install <code>pip</code> in Debian Wheezy?</p>

<p>I've found many advises <code>apt-get install python-pip</code> but the result is </p>

<blockquote>
  <p>""Unable to locate package python-pip""</p>
</blockquote>

<p>Is <code>pip</code> available in Debian Wheezy? I'm using <code>7.8</code></p>
","<p>Although <code>apt-get update</code> might seem to help you, I recommend <strong>strongly</strong> against using pip installed from the Wheeze repository with <code>apt-get install python-pip</code>:</p>

<ul>
<li>that <code>pip</code> is at version 1.1 while the current version is > 9.0 </li>
<li>version 1.1 of <code>pip</code> has known <strong>security problems</strong> when used to download packages</li>
<li>version 1.1 doesn't restrict downloads/installs to stable versions of packages</li>
<li>lacks a lot of new functionality (like support for the wheel format) and misses bug fixes (see the <a href=""https://pip.pypa.io/en/latest/news.html"">changelog</a>)</li>
<li><code>python-pip</code> installed via <code>apt-get</code> pulls in some perl modules for whatever reason</li>
</ul>

<p>Unless you are running python2.4 or so that is still supported by pip 1.1 (and which you should not use anyway) you should follow the <a href=""https://pip.pypa.io/en/latest/installing.html#install-pip"">installation instructions on the pip documentation page</a> to <strong>securely</strong> download <code>pip</code> (don't use the insecure <code>pip install --upgrade pip</code> with the 1.1 version, and certainly don't install any packages with <code>sudo pip ...</code> with that version)</p>

<p>If you already have made the mistake of installing pip version 1.1, immediately do:</p>

<pre><code>sudo apt-get remove python-pip
</code></pre>

<p>After that:</p>

<pre><code>wget https://bootstrap.pypa.io/get-pip.py
python get-pip.py
</code></pre>

<p>(for any of the python versions you have installed). </p>

<p>Python2 versions starting with 2.7.9 and Python3 version starting with 3.4 have pip included by default.</p>
","182467"
"Rm can't delete file","68234","","<pre><code>chmod 777  -R  /mnt  
rm -rf /mnt/*  

rm: cannot remove 'omitted': Read-only file system  
rm: cannot remove 'omitted': Read-only file system  
</code></pre>

<p>Please show me how I can do this?</p>
","<p>As the error message says: the filesystem on which <code>omitted</code> is located is read-only. You can't do anything to modify that filesystem, including removing files.</p>

<p>You can check the mount point of the filesystem by running <code>df omitted</code>. It is probably <code>/mnt</code> given the command you're running.</p>

<p>You can remount the filesystem as read-write by running</p>

<pre><code>mount -o remount,rw /mnt
</code></pre>

<p>However it would be a good idea to find out why the filesystem was mounted as read-only in the first place. This may be an indication that you should not be deleting those files.</p>

<p>Run <code>mount | grep /mnt</code> to see what options were specified when mounting that filesystem. For an ext2/ext3/ext4 filesystem, if the options did not include <code>ro</code> (read-only) but included <code>errors=remount-ro</code>, it looks like the filesystem was damaged and was automatically remounted as read-only to limit the damage; you will find more information in the kernel logs.</p>

<p>Note that your command attempts to remove the mount point itself, but this is harmless you won't have permission to do it anyway.</p>

<p>By the way, I strongly urge you not to use <code>chmod 777</code>. It is extremely rare to actually need these permissions, and they can cause a lot of harm (especially when you typo the argument, but even when not). If you try to remove a file and get a “permission denied” error, all you need to do is give yourself permission to write to the containing directory: generally, that's <code>chmod -R u+w /path/to/toplevel/directory</code>.</p>
","44920"
"What units of time does ""top"" use?","68228","","<p>If I issue the ""top"" command and receive results such as:</p>

<pre><code>PID   USER  PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND   
00001 bob   25   0 77380 1212 1200 R 95.8  0.0  89122:13 fee         
00002 bob   25   0 77380 1196 1184 R 95.4  0.0  88954:14 fi         
00003 sam   18   0  427m  16m 6308 R 30.0  0.1  54:46.43 fo         
00004 sam   18   0  427m  16m 6308 R 26.5  0.1  52:55.33 fum         
</code></pre>

<p><strong>Question:</strong> What are the units in the ""TIME+"" column?</p>

<p><strong>What I have tried:</strong> (please suggest a better strategy for searching documentation ...)</p>

<ul>
<li><code>man top | grep -C 4 time</code> or </li>
<li><code>man top | grep &lt;X&gt;</code> when I substitute <code>minute</code>, <code>hour</code>, <code>day</code>, or <code>HH</code> for <code>X</code> ... </li>
</ul>
","<p>minutes:seconds.hundredths</p>

<p>Searching for “TIME+” or for “seconds” gives the answer, kind of (I wouldn't call the man page clear).</p>

<p>This format is inherited from BSD, you also get it with <code>ps u</code> or <code>ps l</code> under Linux.</p>
","53271"
"exit tmux window without quitting the Terminal program","68215","","<p>OK I'm new to this. I installed <code>tmux</code> to run a several days experiment. After typing <code>tmux new -s name</code> I got a new window with green banner at the bottom. I compile and run java program. Now I do not know how to exit the window (while leave it running). The bash (or whatever) cursor is not responding because the java program is still running.  My solution so far is to quit the Terminal program completely and reopen it again. Any ideas on how to quit the tmux window without exiting the whole Terminal program? </p>
","<p>Detach from currently attached session</p>

<p>Session</p>

<p><kbd>Ctrl</kbd>+ <kbd>b</kbd> <kbd>d</kbd> or <kbd>Ctrl</kbd>+ <kbd>b</kbd> <code>:detach</code></p>

<p>Screen</p>

<p><kbd>Ctrl</kbd>+ <kbd>a</kbd> <kbd>Ctrl</kbd>+ <kbd>d</kbd> or <kbd>Ctrl</kbd>+ <kbd>a</kbd> <code>:detach</code></p>
","174443"
"Yum: How can I view variables like $releasever, $basearch & $YUM0?","68165","","<p>I am setting up a yum repository, and need to debug some of the URLs in the yum.conf file. I need to know why is Scientific Linux trying to grab this URL, when I was expecting it to grab another URL:</p>

<pre><code># yum install package 
http://192.168.1.100/pub/scientific/6.1/x86_64/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - ""The requested URL returned error: 404""
Trying other mirror.
Error: Cannot retrieve repository metadata (repomd.xml) for repository: sl. Please verify its path and try again
</code></pre>

<p>The <a href=""http://linux.die.net/man/5/yum.conf"">yum.conf(5)</a> manpage gives some information about these variables:</p>

<blockquote>
  <p>Variables</p>
  
  <p>There are a number of variables you can use to ease maintenance of
  yum's configuration files. They are available in the values of several
  options including name, baseurl and commands.</p>
  
  <p>$releasever This will be replaced with the value of the version of the
  package listed in distroverpkg. This defaults to the version of
  'redhat-release' package.</p>
  
  <p>$arch This will be replaced with your
  architecture as listed by os.uname()[4] in Python.</p>
  
  <p>$basearch This will be replaced with your base architecture in yum.
  For example, if your $arch is i686 your $basearch will be i386.</p>
  
  <p>$YUM0-$YUM9 These will be replaced with the value of the shell
  environment variable of the same name. If the shell environment
  variable does not exist then the configuration file variable will not
  be replaced.</p>
</blockquote>

<p>Is there a way to view these variables by using the <code>yum</code> commandline utility? I would prefer to not hunt down the version of the 'redhat-release' package, or manually get the value of os.uname()[4] in Python.</p>
","<p>If you install <code>yum-utils</code>, that will give you <code>yum-debug-dump</code> which will write those variables and more debugging info to a file. There is no option to write to stdout, it will always write to some file which really isn't that helpful.</p>

<p>This is obviously not a great solution so here's a python one-liner you can copy and paste which will print those variables to stdout.</p>

<p><code>python -c 'import yum, pprint; yb = yum.YumBase(); pprint.pprint(yb.conf.yumvar, width=1)'</code></p>

<p>This works on CentOS 5 and 6, but not 4. yum is written in python, so the yum python module is already on your server, no need to install anything exra.</p>

<p>Here's what it looks like on CentOS 5:</p>

<pre><code>[root@somebox]# python -c 'import yum, pprint; yb = yum.YumBase(); pprint.pprint(yb.conf.yumvar, width=1)'
{'arch': 'ia32e',
 'basearch': 'x86_64',
 'releasever': '5',
 'yum0': '200',
 'yum5': 'foo'}
[root@somebox]# 
</code></pre>
","20226"
"How to use ifconfig to show active interface only","68095","","<p>By default <code>ifconfig</code> will show me all available interfaces , but what if I just want to display <code>active</code> ones? Like, <code>en0</code> only in below. </p>

<pre><code>en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
    ether 14:10:9f:e0:eb:c9 
    inet6 fe80::1610:9fff:fee0:ebc9%en0 prefixlen 64 scopeid 0x4 
    inet X.X.X.X netmask 0xffffff00 broadcast 101.6.69.255
    nd6 options=1&lt;PERFORMNUD&gt;
    media: autoselect
    **status: active**
en3: flags=8963&lt;UP,BROADCAST,SMART,RUNNING,PROMISC,SIMPLEX,MULTICAST&gt; mtu 1500
    options=60&lt;TSO4,TSO6&gt;
    ether 32:00:14:e7:4f:80 
    media: autoselect &lt;full-duplex&gt;
    **status: inactive**
</code></pre>

<p>Notice <code>ifconfig en0</code> will not satisfy, <code>en0</code> is not always the active one ;)</p>

<p>I'm running Mac OS X.</p>
","<p>To get a complete description of all the active services, try:</p>

<pre><code>ifconfig | pcregrep -M -o '^[^\t:]+:([^\n]|\n\t)*status: active'
</code></pre>

<p>This simple regex should filter out only active interfaces and all their information.  I sugest you put an alias for this in your ~/.profile or ~/.bash_profile file (maybe ifconfiga?)</p>

<p>To just get the interface name (useful for scripts), use:</p>

<pre><code>ifconfig | pcregrep -M -o '^[^\t:]+:([^\n]|\n\t)*status: active' | egrep -o -m 1 '^[^\t:]+'
</code></pre>

<p>You have to install <a href=""http://pcre.org"">pcregrep</a> for this to work.  It's on <a href=""http://www.macports.org"">macports</a> in the pcre package.  Alternatively, this <em>should</em> work with GNU grep using <code>grep -Pzo</code> instead of <code>pcregrep -M -o</code> but with the rest the same, but I haven't tested this.  </p>
","108048"
"How to get the Job ID?","67967","","<p>As we know, the shell enables the user to run background processes using <code>&amp;</code> at the command line's end. Each background process is identified by a job ID and, of course, by it's PID.</p>

<p>When I'm executing a new job, the output is something like <code>[1] 1234</code> (the second number is the process ID). Trying to invoke commands like <code>stop 1</code> and <code>stop %1</code> causes a failure message: <code>stop: Unknown job: 1</code></p>

<p>Understanding that the <code>stop</code> command causes a job to be suspended, I was wondering <strong>how to get the job ID</strong> and do it right.
If the only way to kill a job is by it's process ID, <strong>what is the purpose of the job ID</strong>?</p>
","<p>After a process is sent to the background with <code>&amp;</code>, its PID can be retrieved from the variable <code>$!</code>. The job IDs can be displayed using the <code>jobs</code> command, the <code>-l</code> switch displays the PID as well.</p>

<pre><code> $ sleep 42 &amp;
 [1] 5260
 $ echo $!
 5260
 $ jobs -l
 [1]  - 5260 running    sleep 42
</code></pre>

<p>Some <code>kill</code> implementations allow killing by job ID instead of PID. But a more sensible use of the job ID is to selectively foreground a particular process. If you start five processes in the background and want to foreground the third one, you can run the <code>jobs</code> command to see what processes you launched and then <code>fg %3</code> to foreground the one with the job ID three.</p>
","110914"
"Bash ping script file for checking host availability","67915","","<p>I am trying to write a bash script in a file that would, when run start pinging a host until it becomes available, when the host becomes reachable it runs a command and stops executing, i tried writing one but the script continues pinging until the count ends,</p>

<p>Plus i need to put that process in the background but if i run the script with the $ sign it still runs in foreground,</p>

<pre><code>#!/bin/bash
ping -c30 -i3 192.168.137.163
if [ $? -eq 0 ]
then /root/scripts/test1.sh
exit 0
else echo “fail”
fi
</code></pre>

<p>Any help would be appreciated tnx in advance!</p>
","<p>I would use this, a simple one-liner:</p>

<pre><code>while ! ping -c1 HOSTNAME &amp;&gt;/dev/null; do echo ""Ping Fail - `date`""; done ; echo ""Host Found - `date`"" ; /root/scripts/test1.sh
</code></pre>

<p>Replace <code>HOSTNAME</code> with the host you are trying to ping.</p>

<hr>

<p>I missed the part about putting it in the background, put that line in a shellscript like so:</p>

<pre><code>#!/bin/sh

while ! ping -c1 $1 &amp;&gt;/dev/null
        do echo ""Ping Fail - `date`""
done
echo ""Host Found - `date`""
/root/scripts/test1.sh
</code></pre>

<p>And to background it you would run it like so:</p>

<pre><code>nohup ./networktest.sh HOSTNAME &gt; /tmp/networktest.out 2&gt;&amp;1 &amp;
</code></pre>

<p>Again replace <code>HOSTNAME</code> with the host you are trying to ping. In this approach you are passing the hostname as an argument to the shellscript.</p>

<p>Just as a general warning, if your host stays down, you will have this script continuously pinging in the background until you either kill it or the host is found. So I would keep that in mind when you run this. Because you could end up eating system resources if you forget about this.</p>
","184273"
"Is Linux a Unix?","67816","","<p>So, there are lots of different versions of Unix out there: HP-UX, AIX, BSD, etc. Linux is considered a Unix clone rather than an implementation of Unix. Are all the ""real"" Unices actual descendants of the original? If not, what separates Linux from Unix?</p>
","<p>That depends on what you mean by “Unix”, and by “Linux”.</p>

<ul>
<li><p><a href=""http://en.wikipedia.org/wiki/Unix"" rel=""noreferrer"">UNIX</a> is a <a href=""http://www.unix.org/trademark.html"" rel=""noreferrer"">registered trade mark</a> of <a href=""http://www.opengroup.org/"" rel=""noreferrer"">The Open Group</a>. The trade mark has had an eventful history, and it's not completely clear that it's not genericized due to the widespread usage of “Unix” refering to Unix-like systems (see below). Currently the Open Group grants use of the trade mark to any system that passes a <a href=""http://en.wikipedia.org/wiki/Single_UNIX_Specification"" rel=""noreferrer"">Single UNIX</a> <a href=""http://www.unix.org/what_is_unix/the_brand.html"" rel=""noreferrer"">certification</a>. See also <a href=""https://unix.stackexchange.com/questions/2342/"">Why is there a * When There is Mention of Unix Throughout the Internet?</a>.</p></li>
<li><p><a href=""http://en.wikipedia.org/wiki/Unix"" rel=""noreferrer"">Unix</a> is an operating system that was born in 1969 at <a href=""http://en.wikipedia.org/wiki/Bell_Labs"" rel=""noreferrer"">Bell Labs</a>. Various companies sold, and still sell, code derived from this original system, for example <a href=""http://en.wikipedia.org/wiki/AIX_operating_system"" rel=""noreferrer"">AIX</a>, <a href=""http://en.wikipedia.org/wiki/HP-UX"" rel=""noreferrer"">HP-UX</a>, <a href=""http://en.wikipedia.org/wiki/Solaris_Operating_System"" rel=""noreferrer"">Solaris</a>. See also <a href=""https://unix.stackexchange.com/q/3196/885"">Evolution of Operating systems from Unix</a>.</p></li>
<li><p>There are many systems that are Unix-like, in that they offer similar interfaces to programmers, users and administrators. The oldest production system is the <a href=""http://en.wikipedia.org/wiki/Berkeley_Software_Distribution"" rel=""noreferrer"">Berkeley Software Distribution</a>, which gradually evolved from Unix-based (i.e. containing code derived from the original implementation) to Unix-like (i.e. having a similar interface). There are many BSD-based or BSD-derived operating systems: <a href=""http://en.wikipedia.org/wiki/FreeBSD"" rel=""noreferrer"">FreeBSD</a>, <a href=""http://en.wikipedia.org/wiki/NetBSD"" rel=""noreferrer"">NetBSD</a>, <a href=""http://en.wikipedia.org/wiki/OpenBSD"" rel=""noreferrer"">OpenBSD</a>, <a href=""http://en.wikipedia.org/wiki/Mac_OS_X"" rel=""noreferrer"">Mac OS X</a>, etc. Other examples include <a href=""http://en.wikipedia.org/wiki/Tru64_UNIX"" rel=""noreferrer"">OSF/1</a> (now discontinued, it was a commercial Unix-like non-Unix-based system), <a href=""http://en.wikipedia.org/wiki/Minix"" rel=""noreferrer"">Minix</a> (originally a toy Unix-like operating system used as a teaching tool, now a production embedded Unix-like system), and most famously <a href=""http://en.wikipedia.org/wiki/Linux"" rel=""noreferrer"">Linux</a>.</p></li>
</ul>

<hr>

<ul>
<li><p>Strictly speaking, <a href=""http://en.wikipedia.org/wiki/Linux"" rel=""noreferrer"">Linux</a> is an operating system kernel that is designed like Unix's kernel.</p></li>
<li><p><a href=""http://en.wikipedia.org/wiki/Linux"" rel=""noreferrer"">Linux</a> is most commonly used as a name of Unix-like operating systems that use Linux as their kernel. As many of the tools outside the kernel are part of the <a href=""http://en.wikipedia.org/wiki/GNU"" rel=""noreferrer"">GNU project</a>, such systems are often known as <a href=""http://en.wikipedia.org/wiki/GNU/Linux_naming_controversy"" rel=""noreferrer"">GNU/Linux</a>. All major <a href=""http://en.wikipedia.org/wiki/Linux_distribution"" rel=""noreferrer"">Linux distributions</a> consist of GNU/Linux and other software.</p></li>
<li><p>There are Linux-based Unix-like systems that don't use many GNU tools, especially in the embedded world, but I don't think any of them does away with GNU development tools, in particular <a href=""http://en.wikipedia.org/wiki/GNU_Compiler_Collection"" rel=""noreferrer"">GCC</a>.</p></li>
<li><p>There are operating systems that have Linux as their kernel but are not Unix-like. The most well-known is <a href=""http://en.wikipedia.org/wiki/Android_%28operating_system%29"" rel=""noreferrer"">Android</a>, which doesn't have a Unix-like user experience (though you can install a Unix-like command line) or administrator experience or (mostly) programmer experience (“native” Android programs use an API that is completely different from Unix).</p></li>
</ul>
","4097"
"Unable to delete file, even when running as root","67781","","<p>I am in the process of migrating a machine from RHEL 4 to 5. Rather than actually do an upgrade we have created a new VM (both machines are in a cloud) and I am in the process of copying across data between the two.</p>

<p>I have come across the following file, which I need to remove from the new machine but am unable to, even when running as root:</p>

<pre><code>-rw-------  1 2003 2003  219 jan 11 14:22 .bash_history
</code></pre>

<p>This file is inside /home/USER/, where USER is the account of the guy who built the machine. He doesn't have an account on the old machine, so I am trying to remove his home folder so that the new machine tallies with the old one, but I get the following error:</p>

<pre><code>rm: ne peut enlever `.bash_history': Opération non permise
</code></pre>

<p>(translated from the French: cannot remove XXX, operation not permitted)</p>

<p>I have tried using the following command but this has made no difference:</p>

<pre><code>chattr -i .bash_history
</code></pre>

<p>Is the only choice to create a user with the ID 2003, or is there another way around it?</p>

<hr>

<h1>Edit</h1>

<p>I have tried using <code>rm -f</code>, and I get the same error. I get the same kind of error using <code>chmod 777</code> first.</p>

<p>I have been able to <code>chown</code> the folder that contains the file I am trying to delete, so it is:</p>

<pre><code>drwx------ 2 root root 1024 jan 24 15:58 USER
</code></pre>

<hr>

<h1>Edit2</h1>

<p>Running the <code>lsattr</code> command as suggested by Angus gave the following output:</p>

<pre><code>-----a------- USER/.bash_history
------------- USER/..
------------- USER/.
</code></pre>

<p>The file is flagged as append-only - on changing this flag using <code>chattr -a .bash_history</code> I was able to delete the file.</p>
","<p>Check the permissions of the <b>directory</b>. To delete a file inside it, it should be writable by you</p>

<pre><code>chmod ugo+w .
</code></pre>

<p>and not immutable or append-only:</p>

<pre><code>chattr -i -a .
</code></pre>

<p>Check with <code>ls -la</code> and <code>lsattr -a</code>.</p>
","29904"
"Save all the terminal output to a file","67559","","<p>Is there some way of saving all the terminal output to a file with a command?</p>

<ul>
<li>I'm not talking about redirection <code>command &gt; file.txt</code></li>
<li>Not the history <code>history &gt; file.txt</code>, I need the full terminal text</li>
<li>Not with hotkeys !</li>
</ul>

<p>Something like <code>terminal_text &gt; file.txt</code></p>
","<p>You can use <code>script</code>. It will basically save everything printed on the terminal in that <code>script</code> session.</p>

<p>From <a href=""http://man7.org/linux/man-pages/man1/script.1.html"" rel=""noreferrer""><code>man script</code></a>:</p>

<pre><code>script makes a typescript of everything printed on your terminal. 
It is useful for students who need a hardcopy record of an 
interactive session as proof of an assignment, as the typescript file 
can be printed out later with lpr(1).
</code></pre>

<p>You can start a <code>script</code> session by just typing <code>script</code> in the terminal, all the subsequent commands and their outputs will all be saved in a file named <code>typescript</code> in the current directory. You can save the result to a different file too by just starting <code>script</code> like:</p>

<pre><code>script output.txt
</code></pre>

<p>To logout of the <code>screen</code> session (stop saving the contents), just type <code>exit</code>.</p>

<p>Here is an example:</p>

<pre class=""lang-bsh prettyprint-override""><code>$ script output.txt
Script started, file is output.txt

$ ls
output.txt  testfile.txt  foo.txt

$ exit
exit
Script done, file is output.txt
</code></pre>

<p>Now if I read the file:</p>

<pre class=""lang-bsh prettyprint-override""><code>$ cat output.txt

Script started on Mon 20 Apr 2015 08:00:14 AM BDT
$ ls
output.txt  testfile.txt  foo.txt
$ exit
exit

Script done on Mon 20 Apr 2015 08:00:21 AM BDT
</code></pre>

<p><code>script</code> also has many options e.g. running quietly <code>-q</code> (<code>--quiet</code>) without showing/saving program messages, it can also run a specific command <code>-c</code> (<code>--command</code>) rather than a session, it also has many other options. Check <a href=""http://man7.org/linux/man-pages/man1/script.1.html"" rel=""noreferrer""><code>man script</code></a> to get more ideas.</p>
","200642"
"Debug out-of-memory with /var/log/messages","67545","","<p>The following report is thrown in my messages log:</p>

<pre><code>kernel: Out of memory: Kill process 9163 (mysqld) score 511 or sacrifice child
kernel: Killed process 9163, UID 27, (mysqld) total-vm:2457368kB, anon-rss:816780kB, file-rss:4kB
</code></pre>

<p>Doesn't matter if this problem is for <code>httpd</code>, <code>mysqld</code> or <code>postfix</code> but I am curious how can I continue debugging the problem.</p>

<p>How can I get more info about why the PID 9163 is killed and I am not sure if linux keeps history for the terminated PIDs somewhere.</p>

<p>If this occur in your message log file how you will troubleshoot this issue step by step?</p>

<pre><code># free -m

             total       used       free     shared    buffers     cached
Mem:          1655        934        721          0         10         52
-/+ buffers/cache:        871        784
Swap:          109          6        103`
</code></pre>
","<p>The kernel will have logged a bunch of stuff before this happened, but most of it will probably not be in <code>/var/log/messages</code>, depending on how your <code>(r)syslogd</code> is configured.  Try:</p>

<pre><code>grep oom /var/log/*
grep total_vm /var/log/*
</code></pre>

<p>The former should show up a bunch of times and the latter in only one or two places.  That is the file you want to look at.</p>

<p>Find the original ""Out of memory"" line in one of the files that also contains <code>total_vm</code>.  Thirty second to a minute (could be more, could be less) before that line you'll find something like:</p>

<pre><code>kernel: foobar invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0
</code></pre>

<p>You should also find a table somewhere between that line and the ""Out of memory"" line with headers like this:</p>

<pre><code>[ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name
</code></pre>

<p>This may not tell you much more than you already know, but the fields are:</p>

<ul>
<li><strong>pid</strong> The process ID.</li>
<li><strong>uid</strong> User ID.</li>
<li><strong>tgid</strong> Thread group ID.</li>
<li><strong>total_vm</strong> Virtual memory use (in 4 kB pages)</li>
<li><strong>rss</strong> Resident memory use (in 4 kB pages)</li>
<li><strong>nr_ptes</strong> Page table entries</li>
<li><strong>swapents</strong> Swap entries</li>
<li><strong>oom_score_adj</strong> Usually 0; a lower number indicates the process will be less likely to die when the OOM killer is invoked.</li>
</ul>

<p>You can mostly ignore <code>nr_ptes</code> and <code>swapents</code> although I believe these are factors in determining who gets killed.  This is not necessarily the process using the most memory, but it very likely is.  For more about the selection process, <a href=""https://unix.stackexchange.com/questions/92525/can-linux-run-out-of-ram/92544#92544"">see here</a>.  Basically, the process that ends up with the highest oom score is killed -- that's the ""score"" reported on the ""Out of memory"" line; unfortunately the other scores aren't reported but that table provides some clues in terms of factors.</p>

<p>Again, this probably won't do much more than illuminate the obvious: the system ran out of memory and <code>mysqld</code> was choosen to die <em>because killing it would release the most resources</em>.  This does not necessary mean <code>mysqld</code> is doing anything wrong.  You can look at the table to see if anything else went way out of line at the time, but there may not be any clear culprit: the system can run out of memory simply because you misjudged or misconfigured the running processes.</p>
","128667"
"How to enable diffie-hellman-group1-sha1 key exchange on Debian 8.0?","67480","","<p>I am unable to ssh to a server that asks for a <code>diffie-hellman-group1-sha1</code> key exchange method:</p>

<pre><code>ssh 123.123.123.123
Unable to negotiate with 123.123.123.123 port 22: no matching key exchange method found. Their offer: diffie-hellman-group1-sha1
</code></pre>

<p><strong>How to enable the <code>diffie-hellman-group1-sha1</code> key exchange method on Debian 8.0?</strong></p>

<p>I have tried (as proposed <a href=""https://www.howtoforge.com/community/threads/enable-diffie-hellman-group1-sha1-on-jessie.70764/"" rel=""noreferrer"">here</a>) to </p>

<ol>
<li><p>add the following lines to my <code>/etc/ssh/ssh_config</code></p>

<pre><code>KexAlgorithms diffie-hellman-group1-sha1,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group14-sha1
Ciphers 3des-cbc,blowfish-cbc,aes128-cbc,aes128-ctr,aes256-ctr
</code></pre></li>
<li><p>regenerate keys with </p>

<pre><code>ssh-keygen -A
</code></pre></li>
<li><p>restart ssh with </p>

<pre><code>service ssh restart
</code></pre>

<p>but still get the error.</p></li>
</ol>
","<p>The OpenSSH website has a page dedicated to <a href=""http://www.openssh.com/legacy.html"" rel=""noreferrer"">legacy issues</a> such as this one. It suggests the following approach, <em>on the client</em>:</p>

<pre><code>ssh -oKexAlgorithms=+diffie-hellman-group1-sha1 123.123.123.123
</code></pre>

<p>or more permanently, adding</p>

<pre><code>Host 123.123.123.123
    KexAlgorithms +diffie-hellman-group1-sha1
</code></pre>

<p>to <code>~/.ssh/config</code>.</p>

<p>This will enable the old algorithms <em>on the client</em>, allowing it to connect to the server.</p>
","340853"
"Bash script to detect when my server is down or offline","67475","","<p>Background : I need to receive an alert when my server is down. When the server is down, maybe the Sysload collector will not be able to send any alert. To receive an alert when the server is down, I have an external source (server) to detect it. </p>

<p>Question : Is there any way (i prefer bash script) to detect when my server is down or offline and sends an alert message (Email + SMS)?</p>
","<p>If you have a separate server to run your check script on, something like this would do a simple Ping test to see if the server is alive:</p>

<pre><code>#!/bin/bash
SERVERIP=192.168.2.3
NOTIFYEMAIL=test@example.com

ping -c 3 $SERVERIP &gt; /dev/null 2&gt;&amp;1
if [ $? -ne 0 ]
then
   # Use your favorite mailer here:
   mailx -s ""Server $SERVERIP is down"" -t ""$NOTIFYEMAIL"" &lt; /dev/null 
fi
</code></pre>

<p>You can cron the script to run periodically.</p>

<p>If you don't have mailx, you'll have to replace that line with whatever command line email program you have and probably change the options. If your carrier provides an SMS email address, you can send the email to that address. For example, with AT&amp;T, if you send an email to <em>phonenumber</em>@txt.att.net, it will send the email to your phone.</p>

<p>Here's a list of email to SMS gateways:</p>

<p><a href=""http://en.wikipedia.org/wiki/List_of_SMS_gateways"">http://en.wikipedia.org/wiki/List_of_SMS_gateways</a></p>

<p>If your server is a publicly accessible webserver, there are some free services to monitor your website and alert you if it's down, search the web for <em>free website monitoring</em> to find some.</p>
","56341"
"Limit max connections per IP address and new connections per second with iptables","67469","","<p>We have an Ubuntu 12.04 server with httpd on port 80 and we want to limit:</p>

<ul>
<li>the maximum connections per IP address to httpd to 10</li>
<li>the maximum new connections per second to httpd to 150</li>
</ul>

<p>How can we do this with iptables?</p>
","<pre><code>iptables -A INPUT -p tcp --syn --dport 80 -m connlimit --connlimit-above 15 --connlimit-mask 32 -j REJECT --reject-with tcp-reset  
</code></pre>

<p>This will reject connections above 15 from one source IP.  </p>

<pre><code>iptables -A INPUT -m state --state RELATED,ESTABLISHED -m limit --limit 150/second --limit-burst 160 -j ACCEPT  
</code></pre>

<p>In this 160 new connections (packets really) are allowed before the limit of 150 NEW connections (packets) per second is applied.</p>
","140796"
"How can I concatenate a shell variable to other other parameters in my command lines ?","67448","","<p>How can I concatenate a shell variable to other other parameters in my command lines?</p>

<p>For example,</p>

<pre><code>#!/bin/sh
WEBSITE=""danydiop"" 
/usr/bin/mysqldump --opt -u root --ppassword $WEBSITE &gt; $WEBSITE.sql
</code></pre>

<p>I need to concatenate <code>.sql</code> to <code>$WEBSITE</code></p>
","<p>Use <code>${ }</code> to enclosure a variable.</p>

<p>Without curly brackets:</p>

<pre><code>VAR=""foo""
echo $VAR
echo $VARbar
</code></pre>

<p>would give</p>

<pre><code>foo
</code></pre>

<p>and nothing, because the variable <code>$VARbar</code> doesn't exist.</p>

<p>With curly brackets:</p>

<pre><code>VAR=""foo""
echo ${VAR}
echo ${VAR}bar
</code></pre>

<p>would give</p>

<pre><code>foo
foobar
</code></pre>

<p>Enclosing the first <code>$VAR</code> is not necessary, but a good practice.</p>

<p>For your example:</p>

<pre><code>#!/bin/sh
WEBSITE=""danydiop"" 
/usr/bin/mysqldump --opt -u root --ppassword ${WEBSITE} &gt; ${WEBSITE}.sql
</code></pre>

<p>This works for <code>bash</code>, <code>zsh</code>, <code>ksh</code>, maybe others too.</p>
","5391"
"Where do executables look for shared objects at runtime?","67264","","<p>I understand how to define include shared objects at linking/compile time. However, I still wonder how do executables look for the shared object (<code>*.so</code> libraries) at execution time.</p>

<p>For instance, my app <code>a.out</code> calls functions defined in the <code>lib.so</code> library. After compiling, I move <code>lib.so</code> to a new directory in my <code>$HOME</code>.</p>

<p>How can I tell <code>a.out</code> to go look for it there?</p>
","<p>The <a href=""http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html"">shared library HOWTO</a> explains most of the mechanisms involved, and the <a href=""http://kernel.org/doc/man-pages/online/pages/man8/ld.so.8.html"">dynamic loader manual</a> goes into more detail. Each unix variant has its own way, but most use the same executable format (<a href=""http://en.wikipedia.org/wiki/Executable_and_Linkable_Format"">ELF</a>) and have similar <a href=""http://en.wikipedia.org/wiki/Dynamic_linker"">dynamic linkers</a> (derived from Solaris). Below I'll summarize the common behavior with a focus on Linux; check your system's manuals for the complete story.</p>

<p>In a nutshell, when it's looking for a dynamic library (<code>.so</code> file) the linker tries:</p>

<ul>
<li>directories listed in the <code>LD_LIBRARY_PATH</code> environment variable (<code>DYLD_LIBRARY_PATH</code> on OSX);</li>
<li>directories listed in the executable's <a href=""http://en.wikipedia.org/wiki/Rpath"">rpath</a>;</li>
<li>directories on the system search path, which (on Linux at least) consists of the entries in <code>/etc/ld.so.conf</code> plus <code>/lib</code> and <code>/usr/lib</code>.</li>
</ul>

<p>The rpath is stored in the executable (it's the <code>DT_RPATH</code> or <code>DT_RUNPATH</code> dynamic attribute). It can contain absolute paths or paths starting with <code>$ORIGIN</code> to indicate a path relative to the location of the executable (e.g. if the executable is in <code>/opt/myapp/bin</code> and its rpath is <code>$ORIGIN/../lib:$ORIGIN/../plugins</code> then the dynamic linker will look in <code>/opt/myapp/lib</code> and <code>/opt/myapp/plugins</code>). The rpath is normally determined when the executable is compiled, with the <code>-rpath</code> option to <code>ld</code>, but you can change it afterwards with <a href=""ftp://ftp.hungry.com/pub/hungry/chrpath/""><code>chrpath</code></a>.</p>

<p>In the scenario you describe, if you're the developer or packager of the application and intend for it to be installed in a <code>…/bin</code>, <code>…/lib</code> structure, then link with <code>-rpath='$ORIGIN/../lib'</code>. If you're installing a pre-built binary on your system, either put the library in a directory on the search path (<code>/usr/local/lib</code> if you're the system administrator, otherwise a directory that you add to <code>$LD_LIBRARY_PATH</code>), or try <code>chrpath</code>.</p>
","22999"
"How do I unset a variable at the command line?","67239","","<p>I have tried the following command to set Proxy on <code>yaourt</code>:</p>

<pre><code>export ALL_PROXY=http://proxy.example.com:8080
</code></pre>

<p>The question is how to unset the proxy on <code>yaourt</code>?
In general, how can I unset the value of a variable in the current shell?</p>
","<p>To remove an environment variable, run</p>

<pre><code>unset ALL_PROXY
</code></pre>

<p>Note that an environment variable only takes effect in a program and the program it launches. If you set an environment variable in one shell window, it doesn't affect other shell windows.</p>

<p>If you've added <code>export ALL_PROXY=…</code> to an initialization file, remove it from there.</p>

<p>You can run <code>export</code> with no arguments to see what environment variables are set in the current shell.</p>

<p>Remember that to make a shell variable available to the programs started by that shell, you need to export it, either by running <code>export VAR</code> after the assignment <code>VAR=VALUE</code> or by combining the two (<code>export VAR=VALUE</code>).</p>
","103578"
"Bash: How to read one line at a time from output of a command?","67182","","<p>I am trying to read the output of a command in bash using a <code>while loop</code>.</p>

<pre><code>while read -r line
do
    echo ""$line""
done &lt;&lt;&lt; $(find . -type f)
</code></pre>

<p>The output I got </p>

<pre><code>ranveer@ranveer:~/tmp$ bash test.sh
./test.py ./test1.py ./out1 ./test.sh ./out ./out2 ./hello
ranveer@ranveer:~/tmp$ 
</code></pre>

<p>After this I tried </p>

<pre><code>$(find . -type f) | 
while read -r line
do
    echo ""$line""
done 
</code></pre>

<p>but it generated an error <code>test.sh: line 5: ./test.py: Permission denied</code>.</p>

<p>So, how do I read it line by line because I think currently it is slurping the entire line at once.</p>

<p>Required output:</p>

<pre><code>./test.py
./test1.py
./out1
./test.sh
./out
./out2
./hello
</code></pre>
","<p>There's a mistake, you need <code>&lt; &lt;(command)</code> not <code>&lt;&lt;&lt;$(command)</code></p>

<p><code>&lt; &lt;( )</code> is a <a href=""http://mywiki.wooledge.org/ProcessSubstitution"">Process Substitution</a>, <code>$()</code> is a <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_03"">command substitution</a> and <code>&lt;&lt;&lt;</code> is a <em>here-string</em>. </p>
","52027"
"How do I list every file in a directory except those with specified extensions?","67159","","<p>Suppose that I have a folder containing <strong>.txt</strong>, <strong>.pdf</strong>, and other files.  I would like to list the ""other"" files (i.e., files not having the extensions <strong>.txt</strong> or <strong>.pdf</strong>). Do you have any advice on how to do this?</p>

<p>I know how to list files not having a given extension.  For example, if I want to list all files except the <strong>.txt</strong> files, then either </p>

<pre><code>find -not -iname ""*.txt""
</code></pre>

<p>or </p>

<pre><code>ls | grep -v '\.txt$' | column
</code></pre>

<p>seem to work.  But, how can I list everything except <strong>.txt</strong> files <em>or</em> <strong>.pdf</strong> files?  It seems that I need to use some sort of logical ""or"" in <code>find</code> or <code>grep</code>.</p>
","<p>Assuming one has an appropriate version of <code>ls</code>, this is possibly the simplest way:</p>

<pre><code>ls -I ""*.txt"" -I ""*.pdf""
</code></pre>

<p>If you want to iterate across all the subdirectories:</p>

<pre><code>ls -I ""*.txt"" -I ""*.pdf"" -R
</code></pre>
","47228"
"What's the difference between /etc/rc.local and /etc/init.d/rc.local?","67046","","<p>I want to add a permanent <code>iptables</code> rule to my new <code>VPS</code>, and after brief google search i was surprised that there are two places this rule can be added, that seems like identical: <code>/etc/rc.local</code> and <code>/etc/init.d/rc.local</code>. Maybe someone knows why where is two places for simple startup code to place? Is it linux flavor specific (but ubuntu has both!)? Or one of them is deprecated?</p>
","<p><code>/etc/init.d</code> is maintained on ubuntu for backward compatibility with sysvinit stuff.  If you actually look at <code>/etc/initd/rc.local</code> you'll see (also from a 12.04 LTS Server):</p>

<pre><code>#! /bin/sh
### BEGIN INIT INFO
# Provides:          rc.local
# Required-Start:    $remote_fs $syslog $all
# Required-Stop:
# Default-Start:     2 3 4 5
# Default-Stop:
# Short-Description: Run /etc/rc.local if it exist
### END INIT INFO
</code></pre>

<p>And <strong>""Run /etc/rc.local""</strong> is exactly what it does.  The entirety of <code>/etc/rc.local</code> is:</p>

<pre><code>#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will ""exit 0"" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.

exit 0
</code></pre>

<p>I would guess the purpose in doing this is to provide a dead simple place to put shell commands you want run at boot, without having to deal with the stop|start service stuff, which is in <code>/etc/init.d/rc.local</code>.</p>

<p>So it is in fact a service, and can be run as such.  I added a <code>echo</code> line to <code>/etc/rc.local</code> and:</p>

<pre><code>»service rc.local start
hello world
</code></pre>

<p>However, I do not believe it is referenced by anything in upstart's <code>/etc/init</code> (not init.d!) directory:</p>

<pre><code>»initctl start rc.local
initctl: Unknown job: rc.local
</code></pre>

<p>There are a few ""rc"" services in upstart:</p>

<pre><code>»initctl list | grep rc
rc stop/waiting
rcS stop/waiting
rc-sysinit stop/waiting
</code></pre>

<p>But none of those seem to have anything to do with rc.local.</p>
","59945"
"What are the settings to correct vsftpd ""500 OOPS: cannot change directory"" error?","66980","","<p>My question is what settings do I need to change and/or commands to run to allow me to log into my vsftpd system?</p>

<p>I am getting this error, when I login using ftp instead of sftp:</p>

<pre><code>Name (localhost:dbadmin): dbadmin
331 Please specify the password.
Password:
500 OOPS: cannot change directory:/home/dbadmin
Login failed.
ftp&gt; 
</code></pre>

<p>This works when logging in using <code>sftp@</code>, but my server is behind a firewall, and I need to be able to login using ftp as well as sftp.</p>

<p>I have been looking at quite a few posts about the ""OOPS"" error but so far have had no luck logging in.</p>

<p>Here is some information about my system and settings:</p>

<p>I am running CentOS 6.4.</p>

<p>iptables and ip6tables are stopped and disabled.</p>

<p>My home directory is protected 700, and I have tried 750, just to see if that made a difference. It did not.</p>

<p>Here are the active lines in <code>/etc/vsftpd/vsftpd.conf</code></p>

<pre><code>anonymous_enable=NO
local_enable=YES
write_enable=YES
local_umask=022
dirmessage_enable=YES
xferlog_enable=YES
connect_from_port_20=YES
xferlog_std_format=YES
listen=YES
pam_service_name=vsftpd
userlist_enable=YES
tcp_wrappers=YES
</code></pre>

<p>My login name is not in user_list.</p>
","<p>Run this one command, no need to restart any service &amp; server:</p>

<pre><code># setenforce 0
</code></pre>

<p>To check SELinux status : </p>

<pre><code># getenforce
</code></pre>

<p>or</p>

<p>edit the file <code>/etc/sysconfig/selinux</code> to include</p>

<pre><code>SELINUX=disabled
</code></pre>

<p>Doing so will require a reboot.</p>
","185415"
"Shell script throws a not found error when run from a sh file. But if entered manually the commands work","66769","","<p>I'm trying to use the following script to generate a sitemap for my website. When I run it as <code>sh thsitemap.sh</code> I get an error like this and creates an empty sitemap.xml file:</p>

<pre><code>thsitemap.sh: 22: thsitemap.sh: [[: not found
thsitemap.sh: 42: thsitemap.sh: [[: not found
thsitemap.sh: 50: thsitemap.sh: Syntax error: ""("" unexpected
</code></pre>

<p>But as the same user <code>root</code> when I manually copy and paste these lines on the terminal, it works without any error and the sitemap.xml file have all the urls. What's the problem? How can I fix this?</p>

<pre><code>#!/bin/bash
##############################################
# modified version of original http://media-glass.es/ghost-sitemaps/
# for ghost.centminmod.com
# http://ghost.centminmod.com/ghost-sitemap-generator/
##############################################
url=""techhamlet.com""
webroot='/home/leafh8kfns/techhamlet.com'
path=""${webroot}/sitemap.xml""
user='leafh8kfns'       # web server user
group='leafh8kfns'      # web server group

debug='n' # disable debug mode with debug='n'
##############################################
date=`date +'%FT%k:%M:%S+00:00'`
freq=""daily""
prio=""0.5""
reject='.rss, .gif, .png, .jpg, .css, .js, .txt, .ico, .eot, .woff, .ttf, .svg, .txt'
##############################################
# create sitemap.xml file if it doesn't exist and give it same permissions
# as nginx server user/group
if [[ ! -f ""$path"" ]]; then
    touch $path
    chown ${user}:${group} $path
fi

# check for robots.txt defined Sitemap directive
# if doesn't exist add one
# https://support.google.com/webmasters/answer/183669
if [ -f ""${webroot}/robots.txt"" ]; then
SITEMAPCHECK=$(grep 'Sitemap:' ${webroot}/robots.txt)
    if [ -z ""$SITEMAPCHECK"" ]; then
    echo ""Sitemap: http://${url}/sitemap.xml"" &gt;&gt; ${webroot}/robots.txt
    fi
fi
##############################################
echo """" &gt; $path

# grab list of site urls
list=`wget -r --delete-after $url --reject=${reject} 2&gt;&amp;1 |grep ""\-\-""  |grep http | grep -v 'normalize\.css' | awk '{ print $3 }'`

if [[ ""$debug"" = [yY] ]]; then
    echo ""------------------------------------------------------""
    echo ""Following list of urls will be submitted to Google""
    echo $list
    echo ""------------------------------------------------------""
fi

# put list into an array
array=($list)

echo ""------------------------------------------------------""
echo ${#array[@]} ""pages detected for $url"" 
echo ""------------------------------------------------------""

# formatted properly according to
# https://support.google.com/webmasters/answer/35738
echo ""&lt;?xml version=\""1.0\"" encoding=\""UTF-8\""?&gt;
&lt;urlset xsi:schemaLocation=\""http://www.sitemaps.org/schemas/sitemap/0.9 
http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\"" xmlns:xsi=\""http://www.w3.org/2001/XMLSchema-instance\""
xmlns=\""http://www.sitemaps.org/schemas/sitemap/0.9\""&gt;"" &gt; $path

echo ' 
   ' &gt;&gt; $path;
   for ((i=0;i&lt;${#array[*]};i++)); do
echo ""&lt;url&gt;
    &lt;loc&gt;${array[$i]:0}&lt;/loc&gt;
    &lt;lastmod&gt;$date&lt;/lastmod&gt;
    &lt;changefreq&gt;$freq&lt;/changefreq&gt;
    &lt;priority&gt;$prio&lt;/priority&gt;
&lt;/url&gt;"" &gt;&gt; $path
   done
echo """" &gt;&gt; $path
echo ""&lt;/urlset&gt;"" &gt;&gt; $path

# notify Google
# URL encode urls as per https://support.google.com/webmasters/answer/183669
if [[ ""$debug"" = [nN] ]]; then
    wget  -q --delete-after http://www.google.com/webmasters/tools/ping?sitemap=http%3A%2F%2F${url}%2Fsitemap.xml

    rm -rf ${url}
else
    echo ""wget  -q --delete-after http://www.google.com/webmasters/tools/ping?sitemap=http%3A%2F%2F${url}%2Fsitemap.xml""

    echo ""rm -rf ${url}""
fi
echo ""------------------------------------------------------""

exit 0
</code></pre>
","<p>Run the script either as:</p>

<pre><code>bash script.sh
</code></pre>

<p>or just:</p>

<pre><code>./script.sh
</code></pre>

<p>When <code>bash</code> is run using the name <code>sh</code>, it disables most of its extensions, such as the <code>[[</code> testing operator.</p>

<p>Since you have the <code>#!/bin/bash</code> shebang line, you don't need to specify the shell interpreter explicitly on the command line. Running the script as a command will use that line to find the shell.</p>
","155881"
"Udev : renaming my network interface","66684","","<p>I just installed <em>RHEL 6.3</em> on a Dell 1950 server.
This server as two GBit ports, Gb0 and Gb1.</p>

<p>For some obscure reason, <code>udev</code> chose to name <em>Gb0</em> <code>eth1</code> and <em>Gb1</em> <code>eth0</code>.
This is definitly not a good find for me and just gives confusion.</p>

<p>So I modified the configuration in <code>/etc/udev/rules.d/70-persistent-net.rules</code>:</p>

<pre><code># PCI device 0x14e4:0x164c (bnx2)
SUBSYSTEM==""net"", ACTION==""add"", DRIVERS==""?*"", \
  ATTR{address}==""00:20:19:52:d3:c0"",           \
  ATTR{type}==""1"", KERNEL==""eth*"", NAME=""eth1""

# PCI device 0x14e4:0x164c (bnx2)
SUBSYSTEM==""net"", ACTION==""add"", DRIVERS==""?*"", \
  ATTR{address}==""00:20:19:52:d3:be"",           \
  ATTR{type}==""1"", KERNEL==""eth*"", NAME=""eth0""
</code></pre>

<p>I just changed the ""NAME"" field on the file in order to reflect what I want.
I rebooted the server and it didn't worked. </p>

<p>In the <code>dmesg</code> log I can read the following :</p>

<pre><code>udev: renamed network interface eth1 to rename5
udev: renamed network interface eth0 to eth1
udev: renamed network interface rename5 to eth0
</code></pre>

<p>Any idea on what is wrong here?
Why is <code>udev</code> switching like this? I have another similar server, where I do not have this issue.</p>
","<p>In my case, the issue is coming from the fact that the mac address for each interface was set in three files :</p>

<pre><code>/etc/udev/rules.d/70-persistent-net.rules
/etc/sysconfig/network-scripts/ifcfg-eth0
/etc/sysconfig/network-scripts/ifcfg-eth1
</code></pre>

<p>We need consistency between ifcfg file and net.rules for the mac address.</p>
","177564"
"How do i extend a partition with a LVM and the contained physical volume and logical volume?","66616","","<p>On my 240 GB SSD i had at first two partitions, one containing the Logical Volume with Linux Mint and the other had contained a NTFS partition to share with Windows. 
Now i removed the NTFS partition and want to extend my logical volume group to use the released disk space. </p>

<p>How do i <strong>extend the volume group</strong>, my <strong>logical volume containing /home</strong> and the <strong>filesystem (ext4)</strong> on /home? Is this possible to do online?</p>

<p>PS: Yes, i know that i have to backup my data :)</p>

<pre><code>/dev/sdb/  (240GB)
    linuxvg  (160GB) should use 100% of the disk space
        swap
        root
        home (ext4, 128GB) should be extended to use the remaining space
</code></pre>

<p>output of <code>sudo vgdisplay</code>:</p>

<pre><code>  --- Volume group ---
  VG Name               linuxvg
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  4
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                3
  Open LV               3
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               160,00 GiB
  PE Size               4,00 MiB
  Total PE              40959
  Alloc PE / Size       40959 / 160,00 GiB
  Free  PE / Size       0 / 0   
  VG UUID               ...

  --- Logical volume ---
  LV Path                /dev/linuxvg/swap
  LV Name                swap
  VG Name                linuxvg
  LV UUID                ...
  LV Write Access        read/write
  LV Creation host, time mint, 2013-08-06 22:48:32 +0200
  LV Status              available
  # open                 2
  LV Size                8,00 GiB
  Current LE             2048
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           252:0

  --- Logical volume ---
  LV Path                /dev/linuxvg/root
  LV Name                root
  VG Name                linuxvg
  LV UUID                ...
  LV Write Access        read/write
  LV Creation host, time mint, 2013-08-06 22:48:43 +0200
  LV Status              available
  # open                 1
  LV Size                24,00 GiB
  Current LE             6144
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           252:1

  --- Logical volume ---
  LV Path                /dev/linuxvg/home
  LV Name                home
  VG Name                linuxvg
  LV UUID                ...
  LV Write Access        read/write
  LV Creation host, time mint, 2013-08-06 22:48:57 +0200
  LV Status              available
  # open                 1
  LV Size                128,00 GiB
  Current LE             32767
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           252:2

  --- Physical volumes ---
  PV Name               /dev/sdb1     
  PV UUID               ...
  PV Status             allocatable
  Total PE / Free PE    40959 / 0
</code></pre>

<p>output of <code>sudo fdisk -l</code>:</p>

<pre><code>Disk /dev/sdb: 240.1 GB, 240057409536 bytes
255 heads, 63 sectors/track, 29185 cylinders, total 468862128 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               1   468862127   234431063+  ee  GPT

Disk /dev/mapper/linuxvg-swap: 8589 MB, 8589934592 bytes
255 heads, 63 sectors/track, 1044 cylinders, total 16777216 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/mapper/linuxvg-root: 25.8 GB, 25769803776 bytes
255 heads, 63 sectors/track, 3133 cylinders, total 50331648 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000


Disk /dev/mapper/linuxvg-home: 137.4 GB, 137434759168 bytes
255 heads, 63 sectors/track, 16708 cylinders, total 268427264 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000
</code></pre>
","<p>The question was solved, after reading <a href=""http://blog.allanglesit.com/2012/08/linux-lvm-resize-partition-to-grow-lvm-volume-group/"" rel=""noreferrer"">this</a> blog post.
I will write the solution in short form:</p>

<ul>
<li>boot from a live cd with</li>
<li>use <code>gdisk</code> (if you use GPT) otherwise you could go with good old <code>fdisk</code></li>
<li>note your partition settings, in my case <code>gdisk -l /dev/sdb</code></li>
<li>delete your partition with</li>
<li>create a new partition with the exact same alignment as the previous one (in my example starting at block 2048)</li>
<li>write your new partition table</li>
<li>run $ partprobe -s to refresh the partition table without a reboot</li>
<li>resize your physical volume with <code>pvresize /dev/sdb1</code> or whereever your pv is (use <code>pvs</code> to determine if you dont know)</li>
<li>now resize yout logical volume with <code>lvextend -l +100%FREE /dev/file/of/your/lv</code>, in my case <code>sudo lvextend -l +100%FREE /dev/linuxvg/home</code></li>
<li>resize the filesystem <code>sudo resize2fs /dev/linuxvg/home</code></li>
<li>first check the consistency <code>sudo e2fsck -f /dev/linuxvg/home</code></li>
<li>enjoy :)</li>
</ul>
","98370"
"Can overwritten files be recovered?","66418","","<p>I am not talking about <a href=""https://unix.stackexchange.com/q/2677/24509"">recovering</a> <a href=""http://www.cyberciti.biz/tips/linux-ext3-ext4-deleted-files-recovery-howto.html"" rel=""noreferrer"">deleted files</a>, but <em>overwritten</em> files. Namely by the following methods:</p>

<pre><code># move
mv new_file old_file

# copy
cp new_file old_file

# edit
vi existing_file
&gt; D
&gt; i new_content
&gt; :x
</code></pre>

<p>Is it possible to retrieve anything if any of the above three actions is performed assuming no special programs are installed on the linux machine?</p>
","<p>The answer is ""Probably yes, but it depends on the filesystem type, and timing.""</p>

<p>None of those three examples will overwrite the physical data blocks of old_file or existing_file, except by chance.</p>

<ul>
<li><p><code>mv new_file old_file</code>. This will unlink old_file. If there are additional hard links to old_file, the blocks will remain unchanged in those remaining links. Otherwise, the blocks will generally (it depends on the filesystem type) be placed on a free list. Then, if the <code>mv</code> requires copying (a opposed to just moving directory entries), new blocks will be allocated as <code>mv</code> writes.</p>

<p>These newly-allocated blocks <strong>may or may not be the same ones that were just freed</strong>. On filesystems like <a href=""http://en.wikipedia.org/wiki/Unix_File_System"" rel=""noreferrer"">UFS</a>, blocks are allocated, if possible, from the same cylinder group as the directory the file was created in. So there's a chance that unlinking a file from a directory and creating a file in that same directory will re-use (and overwrite) some of the same blocks that were just freed. This is why the standard advice to people who accidentally remove a file is to not write any new data to files in their directory tree (and preferably not to the entire filesystem) until someone can attempt file recovery.</p></li>
<li><p><code>cp new_file old_file</code> will do the following (you can use <code>strace</code> to see the system calls):</p>

<pre>open(""old_file"", O_WRONLY|O_TRUNC) = 4</pre>

<p>The O_TRUNC flag will cause all the data blocks to be freed, just like <code>mv</code> did above. And as above, they will generally be added to a free list, and may or may not get reused by the subsequent writes done by the <code>cp</code> command.</p></li>
<li><p><code>vi existing_file</code>. If <code>vi</code> is actually <code>vim</code>, the <code>:x</code> command does the following:</p>

<pre>unlink(""existing_file~"") = -1 ENOENT (No such file or directory)</pre>

<pre>rename(""existing_file"", ""existing_file~"") = 0</pre>

<pre>open(""existing_file"", O_WRONLY|O_CREAT|O_TRUNC, 0664) = 3</pre>

<p>So it doesn't even remove the old data; the data is preserved in a backup file.</p>

<p>On FreeBSD, <code>vi</code> does <code>open(""existing_file"",O_WRONLY|O_CREAT|O_TRUNC, 0664)</code>, which will have the same semantics as <code>cp</code>, above.</p></li>
</ul>

<hr>

<p>You can recover some or all of the data without special programs; all you need is <code>grep</code> and <code>dd</code>, and access to the raw device.</p>

<p>For small text files, the single <code>grep</code> command in the <a href=""https://unix.stackexchange.com/a/2680/49439"">answer from @Steven D</a> in the question you linked to is the easiest way:</p>

<pre><code>grep -i -a -B100 -A100 'text in the deleted file' /dev/sda1
</code></pre>

<p>But for larger files that may be in multiple non-contiguous blocks, I do this:</p>

<pre><code>grep -a -b ""text in the deleted file"" /dev/sda1
13813610612:this is some text in the deleted file
</code></pre>

<p>which will give you the offset in bytes of the matching line. Follow this with a series of <code>dd</code> commands, starting with</p>

<pre><code>dd if=/dev/sda1 count=1 skip=$(expr 13813610612 / 512)
</code></pre>

<p>You'd also want to read some blocks before and after that block. On UFS, file blocks are usually 8KB and are usually allocated fairly contiguously, a single file's blocks being interleaved alternately with 8KB blocks from other files or free space. The tail of a file on UFS is up to 7 1KB fragments, which may or may not be contiguous.</p>

<p>Of course, on file systems that compress or encrypt data, recovery might not be this straightforward.</p>

<hr>

<p>There are actually very few utilities in Unix that will overwrite an existing file's data blocks. One that comes to mind is <code>dd conv=notrunc</code>. Another is <code>shred</code>.</p>
","150423"
"How do I check if a file is a symbolic link to a directory?","66264","","<p>I can check, if a file exists and is a symbolic link with -L</p>

<pre><code>for file in *; do
    if [[ -L ""$file"" ]]; then echo ""$file is a symlink""; else echo ""$file is not a symlink""; fi
done
</code></pre>

<p>and if it is a directory with -d:</p>

<pre><code>for file in *; do
    if [[ -d ""$file"" ]]; then echo ""$file is a directory""; else echo ""$file is a regular file""; fi
done
</code></pre>

<p>But how can I test for only links to directories?</p>

<hr>

<p>I simulated all cases in a test folder:</p>

<pre><code>/tmp/test# ls
a  b  c/  d@  e@  f@

/tmp/test# file *
a: ASCII text
b: ASCII text
c: directory
d: symbolic link to `c'
e: symbolic link to `a'
f: broken symbolic link to `nofile'
</code></pre>
","<p>Just combine the two tests with <code>&amp;&amp;</code>:</p>

<pre><code>if [[ -L ""$file"" &amp;&amp; -d ""$file"" ]]
then
    echo ""$file is a symlink to a directory""
fi
</code></pre>
","96910"
"Using sed to get rid of characters < > ,","66221","","<p>I have a file with lines as follows:</p>

<pre><code>...
... &lt;230948203[234]&gt;, ...
... &lt;234[24]&gt;, ...
..
</code></pre>

<p>I would like to use sed to remove the characters <code>&lt;</code> <code>,</code> and <code>&gt;</code> from every line </p>

<p>I tried using <code>sed 's/&lt;&gt;,//g'</code> but it didn't work (it didn't change anything). Do I need to escape these special characters. Is it possible to delete multiple characters using a single <code>sed</code> command?</p>
","<p>With <code>sed</code>:</p>

<pre><code>sed 's|[&lt;&gt;,]||g'
</code></pre>

<p>With <code>tr</code>:</p>

<pre><code>tr -d '&lt;&gt;,'
</code></pre>
","33113"
"How do you copy the public key to a ssh-server?","66208","","<p>Here is what I have tried, and I got an error:</p>

<pre><code>$ cat /home/tim/.ssh/id_rsa.pub | ssh tim@just.some.other.server 'cat &gt;&gt; .ssh/authorized_keys'
Password: 
cat: &gt;&gt;: No such file or directory
cat: .ssh/authorized_keys: No such file or directory
</code></pre>
","<p>OpenSSH comes with a command to do this, <code>ssh-copy-id</code>. You just give it the remote address and it adds your public key to the <code>authorized_keys</code> file on the remote machine:</p>

<pre><code>$ ssh-copy-id tim@just.some.other.server
</code></pre>
","29388"
"How to redirect output of wget as input to unzip?","66199","","<p>I have to download a file from this <a href=""http://www.vim.org/scripts/download_script.phpsrc_id=11834"">link</a>. The file download is a zip file which I will have to unzip in the current folder.</p>

<p>Normally, I would download it first, then run the unzip command.</p>

<pre><code>$ wget http://www.vim.org/scripts/download_script.php?src_id=11834 -O temp.zip
$ unzip temp.zip
</code></pre>

<p>But in this way, I need to execute two commands, wait for the completion of first one to execute the next one, also, I must know the name of the file <code>temp.zip</code> to give it to <code>unzip</code>.</p>

<p>Is it possible to redirect output of wget to unzip? Something like</p>

<pre><code>$ unzip &lt; `wget http://www.vim.org/scripts/download_script.php?src_id=11834`
</code></pre>

<p>But it didn't work. </p>

<blockquote>
  <p>bash: <code>wget
  <a href=""http://www.vim.org/scripts/download"">http://www.vim.org/scripts/download</a>_script.php?src_id=11834
  -O temp.zip</code>: ambiguous redirect</p>
</blockquote>

<p>Also, wget got executed twice, and downloaded the file twice.</p>

<p>Thanks</p>
","<p>You have to download your files to a temp file, because (quoting the unzip man page):</p>

<blockquote>
  <p>Archives  read  from  standard input
  are not yet supported, except with
  funzip  (and  then only  the  first 
  member  of  the  archive  can  be
  extracted).</p>
</blockquote>

<p>Just bring the commands together:</p>

<p><code>wget http://www.vim.org/scripts/download_script.php?src_id=11834 -O temp.zip; unzip temp.zip; rm temp.zip</code></p>

<p>But in order to make it more flexible you should probably put it into a script so you save some typing and in order to make sure you don't accidentally overwrite something you could use the <code>mktemp</code> command to create a safe filename for your temp file:</p>

<pre><code>#!/bin/bash
TMPFILE=`mktemp`
PWD=`pwd`
wget ""$1"" -O $TMPFILE
unzip -d $PWD $TMPFILE
rm $TMPFILE
</code></pre>
","2691"
"How to partition 22TB disk?","66183","","<p>I have a 22TB disk on <code>/dev/sdb</code>. How do I create a 22TB partition? Don't really care about the file system - <code>ext4</code> or <code>zfs</code> is fine.</p>

<p>Running CentOS 6.2 - Partition will be used as a data dump. Only a single stream of data so being picky/choosy over what file system isn't really a concern right now. The disk is formed from 12x2TB nearline SAS drives and a Dell Perc controller. </p>

<p>I just want a 22TB partition.</p>
","<p>The simplest solution is to use <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/GUID_Partition_Table"" rel=""nofollow noreferrer"">GPT partitioning</a>, a 64-bit version of Linux, and <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Xfs"" rel=""nofollow noreferrer"">XFS</a>:</p>

<ul>
<li><p>GPT is necessary because the <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Master_Boot_Record"" rel=""nofollow noreferrer"">MS-DOS-style MBR partition table</a> created by <code>fdisk</code> is limited to 2&nbsp;TiB disks. So, you need to use <a href=""http://www.gnu.org/software/parted/"" rel=""nofollow noreferrer""><code>parted</code></a> or another GPT-aware partitioning program instead of <code>fdisk</code>. (<a href=""http://www.rodsbooks.com/gdisk/"" rel=""nofollow noreferrer""><code>gdisk</code></a>, <a href=""http://gparted.sourceforge.net/"" rel=""nofollow noreferrer""><code>gparted</code></a>, etc.)</p></li>
<li><p>A 64-bit kernel is necessary because 32-bit kernels limit you to filesystems smaller than you're asking for. You either hit a size limit based on <a href=""https://en.wikipedia.org/wiki/32-bit#Range_for_storing_integers"" rel=""nofollow noreferrer"">32-bit integers</a> or end up not being able to address enough RAM to support the filesystem properly.</p></li>
<li><p>XFS is not the only solution, but in my opinion it is the easiest one for RHEL systems.</p>

<p>You cannot use ext4 for this in RHEL 6. Although the filesystem was <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Ext4#Features"" rel=""nofollow noreferrer"">designed to support 1&nbsp;EiB filesystems</a>, there is an artificial 16&nbsp;TiB volume size limit in the version of <code>e2fsprogs</code> included in RHEL 6 and its derivatives. Both <a href=""https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/ch-ext4.html"" rel=""nofollow noreferrer"">Red Hat</a> and <a href=""http://wiki.centos.org/About/Product#line-48"" rel=""nofollow noreferrer"">CentOS</a> call this out in their docs. (The ext4 16&nbsp;TiB limit was <a href=""http://rhelblog.redhat.com/2014/01/07/best-file-system/"" rel=""nofollow noreferrer"">raised considerably in RHEL 7</a> to 50&nbsp;TiB.)</p>

<p><a href=""https://unix.stackexchange.com/questions/186/zfs-under-linux-does-it-work"">ZFS may not be practical in your situation</a>. Because of its several legal and technical restrictions, I can't outright recommend it unless you need something only ZFS gives you.</p>

<p>Having ruled out your two chosen filesystems, I suggest XFS. It is the default filesystem in RHEL 7, it was available as a supported filesystem in all RHEL 6 versions, and was backported to the later RHEL 5 releases after RHEL 6 came out.</p></li>
</ul>

<p>Here's the process:</p>

<ol>
<li><p>Check whether you have <code>mkfs.xfs</code> installed by running it without arguments. If it's not present, install the userland XFS tools:</p>

<pre><code># yum install xfsprogs
</code></pre>

<p>If that failed, it's probably because you're on an older OS that doesn't have this in its default package repository. You really should upgrade, but if that is impossible, you can get this from <a href=""http://wiki.centos.org/AdditionalResources/Repositories/CentOSPlus"" rel=""nofollow noreferrer"">CentOSPlus</a> or <a href=""http://fedoraproject.org/wiki/EPEL"" rel=""nofollow noreferrer"">EPEL</a>. You may also need to install the <code>kmod_xfs</code> package.</p></li>
<li><p>Create the partition:</p>

<p>Since you say your 22 TiB volume is on <code>/dev/sdb</code>, the commands for <code>parted</code> are:</p>

<pre><code># parted /dev/sdb mklabel gpt
# parted -a optimal -- /dev/sdb mkpart primary xfs 1 -1
</code></pre>

<p>That causes it to take over the entire volume with a single partition. Actually, it ignores the first 1&nbsp;MiB of the volume, to achieve the <a href=""http://www.lifehacker.com.au/2011/09/make-sure-your-partitions-are-correctly-aligned-for-optimal-ssd-performance/"" rel=""nofollow noreferrer"">4&nbsp;KiB alignment</a> required to get the full performance from <a href=""http://en.wikipedia.org/wiki/Advanced_format"" rel=""nofollow noreferrer"">Advanced Format HDDs</a> and <a href=""https://en.wikipedia.org/wiki/Solid-state_drive"" rel=""nofollow noreferrer"">SSDs</a>.</p>

<p>You could skip this step and format the entire volume with XFS. That is, you would use <code>/dev/sdb</code> in the example below instead of <code>/dev/sdb1</code>. This avoids the problem of sector alignment. In the case of a RAID array, there are no downsides worth speaking about, but I'd caution doing this on a single removable disk, since some OSes (Windows and macOS, for instance) will offer to format a partitionless hard drive for you every time you plug it in. Putting the filesystem on a partition solves this.</p></li>
<li><p>Format the partition:</p>

<pre><code># mkfs.xfs -L somelabel /dev/sdb1
</code></pre></li>
<li><p>Add the <code>/etc/fstab</code> entry:</p>

<pre><code>LABEL=somelabel    /some/mount/point    xfs     defaults   0 0
</code></pre></li>
<li><p>Mount up!</p>

<pre><code> # mount /some/mount/point
</code></pre></li>
</ol>

<p>If you want to go down the <a href=""https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)"" rel=""nofollow noreferrer"">LVM</a> path, the above steps are basically just a more detailed version of the second set of commands in <a href=""https://unix.stackexchange.com/a/29109/138"">user <code>bsd</code>'s answer below</a>. You have to do his first set of commands before the ones above.</p>

<p>LVM offers certain advantages at a complexity cost. For instance, you can later ""grow"" an LVM volume group by adding more physical volumes to it, thereby making space to grow the logical volume (""partition"" kinda, sorta), which in turn lets you grow the filesystem living on the logical volume. (See what I mean about complexity? :))</p>
","29264"
"How to run custom scripts upon USB device plug-in?","66169","","<p>What is the state-of-the-art method for automatically executing custom scripts upon USB device plug-in under current Linux distributions like Debian/CentOS/Fedora?</p>

<p>For example if you want to automatically mount/copy some files/umount a USB mass storage device based on its UUID (or device ID etc.).</p>
","<p>Put a line like this in a file in <code>/etc/udev/rules.d</code>:</p>

<pre><code>KERNEL==""sd*"", ATTRS{vendor}==""Yoyodyne"", ATTRS{model}==""XYZ42"", ATTRS{serial}==""123465789"", RUN+=""/pathto/script""
</code></pre>

<p>Add a clause like <code>NAME=""subdir/mydisk%n""</code> if you want to use a custom entry path under <code>/dev</code>.</p>

<p>Run <code>udevadm info -a -n sdb</code> to see what attributes you can match against (<code>attribute==""value""</code>; replace <code>sdb</code> by the device name automatically assigned to the disk, corresponding to the new entry created in <code>/dev</code> when you plug it in). Note that you can use <code>ATTRS</code> clauses from any <em>one</em> stanza: you can pick any stanza, but the <code>ATTRS</code> clauses must all come from the same stanza, you can't mix and match. You can mix <code>ATTRS</code> clauses with other types of clauses listed in a different stanza.</p>
","28577"
"What is the difference between modify and change in stat command context?","66116","","<p>The <code>stat</code> command's manual page says:</p>

<blockquote>
<pre><code>   %x     Time of last access
   %y     Time of last modification
   %z     Time of last change
</code></pre>
</blockquote>

<p>I cannot understand the difference between <em>modify</em> and <em>change</em>. I understand the words are synonyms (English is not my native language), but their output is different.</p>

<p>I tried the following command</p>

<pre><code>stat --printf=""Change %z\nAccess %x\nModify %y\n"" p.txt
</code></pre>

<p>Now when I open p.txt, access time is changed, I go into insert mode, edit the file, modify and change time remains same. </p>

<pre>Change 2010-10-06 12:48:39.286252389 +0500
Access 2010-10-06 12:49:14.<b>962243456</b> +0500
Modify 2010-10-06 12:48:39.234498878 +0500
</pre>

<p>When I write the changes to file <code>:w</code>, modify and change, both change but give different values.</p>

<pre>Change 2010-10-06 12:51:21.<b>949082169</b> +0500
Access 2010-10-06 12:51:21.908246082 +0500
Modify 2010-10-06 12:51:21.<b>908246082</b> +0500
</pre>

<p>So what are the meanings of ""modify"" and ""change"" in this context? That is, time of modification and change give time of which events?</p>

<p>Thanks</p>
","<p>This has already been answered in <a href=""https://unix.stackexchange.com/questions/2464/timestamp-modification-time-and-created-time-of-a-file"">this question</a>, which I quote (original text by <a href=""https://unix.stackexchange.com/users/1339/echox"">echox</a>):</p>

<blockquote>
  <p>There are 3 kind of ""timestamps"":</p>
  
  <ul>
  <li>Access - the last time the file was read</li>
  <li>Modify - the last time the file was modified (content has been modified)</li>
  <li>Change - the last time meta data of the file was changed (e.g. permissions)</li>
  </ul>
</blockquote>

<p><a href=""https://stackoverflow.com/questions/3385203/regarding-access-time-unix"">This post on StackOverflow</a> explains the difference among the three different times from a programming interface point of view.</p>
","2803"
"Find the owner of a directory or file, but only return that and nothing else","66106","","<p>I am looking for a command that will return the owner of a directory and only that--such as a regex parsing the <code>ls -lat</code> command or something similar? I want to use the result in another script.</p>
","<p><a href=""http://www.gnu.org/software/coreutils/manual/html_node/stat-invocation.html"" rel=""noreferrer""><code>stat</code></a>  from <a href=""http://www.gnu.org/software/coreutils/"" rel=""noreferrer"">GNU coreutils</a> can do this:</p>

<pre><code>stat -c '%U' /path/of/file/or/directory
</code></pre>

<p>Unfortunately, there are a number of versions of <code>stat</code>, and there's not a lot of consistency in their syntax.  For example, on FreeBSD, it would be</p>

<pre><code>stat -f '%Su' /path/of/file/or/directory
</code></pre>

<p>If portability is a concern, you're probably better off using <a href=""https://unix.stackexchange.com/questions/7730/find-the-owner-of-a-directory-or-file-but-only-return-that-and-nothing-else/7733#7733"">Gilles's suggestion</a> of combining <code>ls</code> and <code>awk</code>.  It has to start two processes instead of one, but it has the advantage of using only POSIX-standard functionality:</p>

<pre><code>ls -ld /path/of/file/or/directory | awk '{print $3}'
</code></pre>
","7732"
"Local installation of .rpms using YUM","66026","","<p>I'm asking this question cautiously because I don't want to get this wrong.</p>

<p>I have a program_name.rpm file saved locally on my server (CentOS 6.5).</p>

<p>I have installed it previously just by navigating to it and using yum install program_name.rpm which worked fine but it didn't give me any option to specify where it is installed.</p>

<p>Is it possible to install this rpm to /opt/some_directory instead of it's default install location?</p>
","<p>Use rpm</p>

<pre><code>rpm -ivh package.rpm
</code></pre>

<p>If you want to install it on different place use:</p>

<pre><code>rpm -ivh -r /new/path package.rpm
</code></pre>

<p>but be aware under new root will be recreated the directory structure from package </p>
","183064"
"How do I change the default text editor in the Debian (squeeze) distro","65960","","<p>""Joe's own editor"" does not come naturally to me. How do I change to using nano or vim?</p>

<p>I've tried</p>

<pre><code>export EDITOR=nano
</code></pre>

<p>but it doesn't seem to be respected. I'd like <code>visudo</code> to respect this as well.</p>
","<p>To change the default editor at the system level:</p>

<pre><code>sudo update-alternatives --config editor
</code></pre>

<p>and then follow the onscreen prompts.</p>
","42727"
"What does 2>&1 in this command mean?","65939","","<p>I understand that this command attempts to write to nowhere or a null device but what does <code>2&gt;&amp;1</code> mean here?</p>

<pre><code>wget -q -O - http://yourwebsite.com/wp-cron.php?doing_wp_cron &gt; /dev/null 2&gt;&amp;1
</code></pre>
","<p><code>2</code> refers to the second file descriptor of the process, i.e. <code>stderr</code>.</p>

<p><code>&gt;</code> means redirection.</p>

<p><code>&amp;1</code> means the target of the redirection should be the same location as the first file descriptor, i.e. <code>stdout</code>.</p>

<p>So this command first redirects <code>stdout</code> to <code>/dev/null</code> and then redirects <code>stderr</code> there as well.  This effectively silences all output (regular or error) from the <code>wget</code> command.</p>

<p>::edit:: <a href=""http://www.tldp.org/LDP/abs/html/io-redirection.html"">Here</a> is an excellent quick reference for you.</p>
","99264"
"Yum update error: Could not retrieve mirrorlist","65935","","<p>I'm trying to do yum update. This is the error I'm getting.</p>

<pre><code>sudo yum update
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=6&amp;arch=x86_64&amp;repo=os error was
12: Timeout on http://mirrorlist.centos.org/?release=6&amp;arch=x86_64&amp;repo=os: (28,'connect() timed out!')
Error: Cannot retrieve metalink for repository: epel. Please verify its path and try again
</code></pre>

<p>I did <code>wget http://mirrorlist.centos.org/?release=6&amp;arch=x86_64&amp;repo=os</code>. It never goes through. So I know I've to change this somehow. 
Can anyone tell suggest me how to fix this. I found many posts related to this problem over here; but couldn't find a proper solution for this.</p>

<p>My <code>/etc/resolve.conf</code> is:</p>

<pre><code>nameserver 130.102.115.238
nameserver 130.102.128.53
nameserver 130.102.2.53
</code></pre>

<p>I tried adding <code>nameserver 8.8.8.8</code>. It didn't work.</p>

<p>My <code>/etc/yum.repos.d/Centos-Base.repo</code> is:</p>

<pre><code>[base]
name=CentOS-$releasever - Base
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=os
#baseurl=http://mirror.centos.org/centos/$releasever/os/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6

#released updates
[updates]
name=CentOS-$releasever - Updates
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=updates
#baseurl=http://mirror.centos.org/centos/$releasever/updates/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6

#additional packages that may be useful
[extras]
name=CentOS-$releasever - Extras
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=extras
#baseurl=http://mirror.centos.org/centos/$releasever/extras/$basearch/
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6

#additional packages that extend functionality of existing packages
[centosplus]
name=CentOS-$releasever - Plus
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=centosplus
#baseurl=http://mirror.centos.org/centos/$releasever/centosplus/$basearch/
gpgcheck=1
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6

#contrib - packages by Centos Users
[contrib]
name=CentOS-$releasever - Contrib
mirrorlist=http://mirrorlist.centos.org/?release=$releasever&amp;arch=$basearch&amp;repo=contrib
#baseurl=http://mirror.centos.org/centos/$releasever/contrib/$basearch/
gpgcheck=1
enabled=0
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
</code></pre>

<p>I tried commenting the mirrorlists and uncommenting the baseurls. Didn't work.
I really need some help with this. Stuck on this for sometime now.</p>

<p>PS I am trying to set a Hadoop node using Cloudera. That's when I encountered this problem. So I tried doing a yum update individually to figure out what's wrong.</p>
","<p>It looks like you don't have the proxy information configured in your repo file. According to <a href=""http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html"" rel=""nofollow"">http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html</a>, you have to specify your proxy, proxy_username, and proxy_password in <code>yum.conf</code>. This doc is for CentOS 5, but it should hold for CentOS 6 as well.</p>
","91994"
"Bash script error: integer expression expected","65900","","<p>I'm having a rather weird issue, I'm running a script (Bash) on multiple servers and it stopped working on one of the servers (works perfectly fine on all other servers).</p>

<p>Here is the problem part of the script: (I did not write it myself, all credits go to ""Rich"") (<a href=""http://www.notrainers.org/monitoring-memory-usage-on-linux-with-nagios-and-nrpe/"">http://www.notrainers.org/monitoring-memory-usage-on-linux-with-nagios-and-nrpe/</a>)</p>

<pre><code>    if [ ""$result"" -lt ""$warn_level"" ]; then     #Line 56
    echo ""Memory OK. $result% used.""
    exit 0;
elif [ ""$result"" -ge ""$warn_level"" ] &amp;&amp; [ ""$result"" -le ""$critical_level"" ]; then  #Line 59
    echo ""Memory WARNING. $result% used.""
    exit 1;
elif [ ""$result"" -gt ""$critical_level"" ]; then   #Line 62
    echo ""Memory CRITICAL. $result% used.""
    exit 2;
fi
</code></pre>

<p>Complete error message:</p>

<pre><code>./check_memory.sh: Line 56: [: 7.: integer expression expected

./check_memory.sh: Line 59: [: 7.: integer expression expected

./check_memory.sh: Line 62: [: 7.: integer expression expected
</code></pre>

<p>If you need more info, let me know and I will try to supply it as fast as possible.</p>

<p>Appreciate all inputs :)</p>
","<p>From the link you provided, I see the below line. </p>

<pre><code>result=$(echo ""$used / $total * 100"" |bc -l|cut -c -2)
</code></pre>

<p>As per @Graeme's comment, change the above line to below. </p>

<pre><code>result=$(echo ""$used / $total * 100"" |bc -l)
</code></pre>

<p>Now, after adding the above line, we have to change the output of the <code>result</code> to integer as below.  </p>

<pre><code>result1=${result/.*}
</code></pre>

<p>I guess in one of the machines where the error occurs, this output is not an integer. Just convert the output of result to integer so that you can handle such cases. Add the below line after you calculate the <code>result</code>.</p>

<pre><code>result1=${result/.*}
</code></pre>

<p>And instead of <code>result</code> change the variable names as <code>result1</code> inside the <code>if</code> loops, and the error won't occur. </p>

<p>I suspect, the <code>cut -c -2</code> attributes to the error mostly since it is cutting the first 2 characters only. What if result has just one character? Suppose if result is <code>1.23456</code>, the above cut will result in <code>1.</code> as the value for <code>result</code> which obviously is the cause of the <code>integer expected</code> error. </p>

<p>The reason it is working fine in the remaining servers is because it has not encountered a case where the <code>result</code> variable has just a single digit. It is highly likely to fail in the remaining servers too if the result is a single digit variable (something like I mentioned in the above example). </p>
","122995"
"bash shell - ssh remote script capture output and exit code?","65896","","<p>I wish to use shell to invoke a script on a remote server.
I would like to capture the output of that script (its logging messages) and the exit code it returns. </p>

<p>If I do this:</p>

<pre><code>ssh user@server /usr/local/scripts/test_ping.sh
echo ""$?""
</code></pre>

<p>I get the exit code but can't capture the remote logging messages .</p>

<p>If I do this:</p>

<pre><code>local RESULTS=$(ssh user@server /usr/local/scripts/test_ping.sh)
echo ""$?"" 
LOG ""${RESULTS}"";
</code></pre>

<p>I get to log my output using my LOG function but can't seem to get a correct exit code, I assume the code I get is the code from the varianble assignment. </p>

<p>I would like to continue to use my LOG function to capture all output as it formats and sends things to a file, syslog, and the screen for me. </p>

<p>How can I capture results in a var AND get the correct exit code from the remote script?</p>
","<p>The reason you are not getting the correct error code is because <code>local</code> is actually the last thing executed. You need to declare the variable as local prior to running the command.</p>

<pre><code>local RESULTS
RESULTS=$(ssh user@server /usr/local/scripts/test_ping.sh)
echo $?
</code></pre>

<p>You can see the issue here:</p>

<pre><code>$ bar() { foo=$(ls asdkjasd 2&gt;&amp;1); echo $?; }; bar
2
$ bar() { local foo=$(ls asdkjasd 2&gt;&amp;1); echo $?; }; bar
0
$ bar() { local foo; foo=$(ls asdkjasd 2&gt;&amp;1); echo $?; }; bar
2
</code></pre>
","66583"
"How can I install the `ll` command on Mac OS X?","65882","","<p>I'm using Mac OS X. When I SSH into servers I find the <code>ll</code> command useful, but it's not available on my local machine. How can I install it?</p>
","<p>MacOS:</p>

<pre><code>alias ll='ls -lG'
</code></pre>

<p>Linux:</p>

<pre><code>alias ll='ls -l --color=auto'
</code></pre>

<p>Stick that in <code>~/.bashrc</code>.</p>
","28426"
"Not able to ssh to another computer, but can ping it?","65644","","<p>Unable to ssh to another computer but can ping it?  Not sure what I am missing?<br>
Using a Netgear router</p>

<pre><code>bash-3.2$ ifconfig
lo0: flags=8049&lt;UP,LOOPBACK,RUNNING,MULTICAST&gt; mtu 16384
        inet6 ::1 prefixlen 128 
        inet6 fe80::1%lo0 prefixlen 64 scopeid 0x1 
        inet 127.0.0.1 netmask 0xff000000 
gif0: flags=8010&lt;POINTOPOINT,MULTICAST&gt; mtu 1280
stf0: flags=0&lt;&gt; mtu 1280
en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        ether xx:xx:xx:xx:xx:xx 
        media: autoselect (none)
        status: inactive
en1: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        ether xx:xx:xx:xx:xx:xx 
        inet6 xxxx::xxxx:xxxx:xxxx:xxxxxx prefixlen 64 scopeid 0x5 
        inet 10.0.0.3 netmask 0xffffff00 broadcast 10.0.0.255
        media: autoselect
        status: active
fw0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 4078
        lladdr xx:xx:xx:xx:xx:xx:xx:xx 
        media: autoselect &lt;full-duplex&gt;
        status: inactive
bash-3.2$ ssh jeremy@10.0.0.4
ssh: connect to host 10.0.0.4 port 22: Connection refused
bash-3.2$ ssh -p 5900 jeremy@10.0.0.4
ssh: connect to host 10.0.0.4 port 5900: Connection refused
bash-3.2$ ping 10.0.0.3
PING 10.0.0.3 (10.0.0.3): 56 data bytes
64 bytes from 10.0.0.3: icmp_seq=0 ttl=64 time=0.046 ms
64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=0.079 ms
64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=0.078 ms
64 bytes from 10.0.0.3: icmp_seq=3 ttl=64 time=0.077 ms
64 bytes from 10.0.0.3: icmp_seq=4 ttl=64 time=0.079 ms
64 bytes from 10.0.0.3: icmp_seq=5 ttl=64 time=0.081 ms
64 bytes from 10.0.0.3: icmp_seq=6 ttl=64 time=0.078 ms
^C
--- 10.0.0.3 ping statistics ---
7 packets transmitted, 7 packets received, 0.0% packet loss
round-trip min/avg/max/stddev = 0.046/0.074/0.081/0.011 ms
bash-3.2$ ping 10.0.0.4
PING 10.0.0.4 (10.0.0.4): 56 data bytes
64 bytes from 10.0.0.4: icmp_seq=0 ttl=64 time=2.667 ms
64 bytes from 10.0.0.4: icmp_seq=1 ttl=64 time=2.675 ms
64 bytes from 10.0.0.4: icmp_seq=2 ttl=64 time=2.969 ms
64 bytes from 10.0.0.4: icmp_seq=3 ttl=64 time=2.663 ms
64 bytes from 10.0.0.4: icmp_seq=4 ttl=64 time=2.723 ms
^C
--- 10.0.0.4 ping statistics ---
5 packets transmitted, 5 packets received, 0.0% packet loss
round-trip min/avg/max/stddev = 2.663/2.739/2.969/0.117 ms
bash-3.2$ 
</code></pre>
","<p>The server is either not running sshd (and hence not listening on port 22) or has a firewall blocking port 22 (the default ssh port), or in incredibly rare cases running ssh on some other port (which is almost certainly not the case).</p>

<p>First check to make sure sshd is installed (using debian examples)</p>

<pre><code>sudo apt-get install openssh-server
</code></pre>

<p>And if so, is it running:</p>

<pre><code>ps -ef | grep sshd
</code></pre>

<p>then check to see if it is listening to port 22</p>

<pre><code>sudo netstat -nlp | grep :22
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      946/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      946/sshd
</code></pre>

<p>then check your firewall rules (this varies significantly, so I'll show a debian/ubuntu/etc example):</p>

<pre><code>sudo ufw status

sudo ufw show listening
tcp:
  22 * (sshd)
  24224 * (ruby)
tcp6:
  22 * (sshd)
  8080 * (java)
udp:
  123 10.X.Y.Z (ntpd)
  123 * (ntpd)
  18649 * (dhclient)
  24224 * (ruby)
  34131 * (ruby)
  60001 10.87.43.24 (mosh-server)
  68 * (dhclient)
udp6:
  123 fe80::1031:AAAA:BBBB:CCCC (ntpd)
  123 * (ntpd)
  48573 * (dhclient)
</code></pre>

<p>If <code>ufw</code> shows it as closed then run (again a debian/ubuntu example)</p>

<pre><code>sudo ufw allow 22
</code></pre>
","105803"
"Why ext4 File System is better than NTFS?","65639","","<p>For a presentation, I need to show <strong>ext4</strong> File System is better than <strong>NTFS</strong>. I searched and got nice article on both ext4 and NTFS</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Ext4"">http://en.wikipedia.org/wiki/Ext4</a></li>
<li><a href=""http://en.wikipedia.org/wiki/NTFS"">http://en.wikipedia.org/wiki/NTFS</a></li>
</ul>

<p>But I need a comparison guideline with better example.
Would you guys help me?</p>
","<p>""Better"" is subjective and not very meaningful. Nevertheless, you can get a good comparison of filesystems (including NTFS and ext4) <a href=""http://en.wikipedia.org/wiki/Comparison_of_file_systems"" rel=""noreferrer"">on Wikipedia</a>. There's also <a href=""http://www.pcworld.com/article/230527/ubuntu_linux_day_16_ext4_vs_ntfs.html"" rel=""noreferrer"">an article</a> on PC World that covers it more briefly.</p>

<p>Ultimately you should remember that performance metrics in this case are not really a good measure of filesystem performance, there are too many variables involved, especially in that the performance of a filesystem is very related to the performance of the driver being used to access it.</p>
","55497"
"Extract date from a variable in a different format","65639","","<p>Let me explain you the problem </p>

<pre><code>$ date +%c -d ""$d""
Tue 31 Dec 2013 01:13:06 PM CET
$ date +'Today is %F' -d ""$d""
Today is 2013-12-31
</code></pre>

<p>This solution corresponds to current date.</p>

<p>But I have one variable which stores date other than current date</p>

<pre><code>$Prev_date=""Wed Dec 25 06:35:02 EST 2013"" 
</code></pre>

<p>I am looking for solution to read this date as 2013-12-25 and store it in a variable.</p>

<p>I have tried this:</p>

<pre><code>a=`date --date=$Prev_date '+%y/%m/d'`
echo $a
</code></pre>

<p>It's giving this error:</p>

<pre><code>date: illegal option -- date=Wed
usage: date [-u] mmddHHMM[[cc]yy][.SS]
date [-u] [+format]
date -a [-]sss[.fff]
</code></pre>
","<p>The <code>-d</code> option is GNU specific.</p>

<p>Here, you don't need to do date calculation, just rewrite the string which already contains all the information:</p>

<pre><code>a=$(printf '%s\n' ""$Prev_date"" | awk '{
  printf ""%04d-%02d-%02d\n"", $6, \
  (index(""JanFebMarAprMayJunJulAugSepOctNovDec"",$2)+2)/3,$3}')
</code></pre>
","107297"
"How to delete a file on remote machine via SSH by using a Shell Script?","65602","","<p>I am writing a Shell Script where I have to delete a file on a remote machine via a Shell Script. </p>

<p>Flow manually:</p>

<pre><code>ssh username@domain.com
</code></pre>

<p>.. then at domain:</p>

<pre><code>cd ./some/where
rm some_file.war
</code></pre>

<p>How to accomplish that task?</p>
","<p>You can pass the <code>ssh</code> client a command to execute in place of starting a shell by appending it to the ssh command.</p>

<pre><code>ssh username@domain.com 'rm /some/where/some_file.war'
</code></pre>

<p>You don't have to <code>cd</code> to a location to remove something as long as you specify the full path, so thats another step you can skip.</p>

<p>The next question is authentication. If you just run that, you will get prompted for a password. If you don't want to enter this interactively you should set up publickey authentication.</p>
","17468"
"Linux, how to change HDD state from ReadOnly after temporarly crash?","65575","","<p>At this time no ansver for this problem.</p>

<p>Usually after some problems with readings or writings to block device, kernel decides to switch flag for WHOLE DEVICE as read-only. After this any writings to any partition / filesystem located on this device cause switch it as readonly together with device state, because any writings are impossible.</p>

<p>Example from dmesg, this is simulation for guest linux on windows8 using VirtualBox when defrag takes guests device image:</p>

<pre><code>[11903.002030] ata3.00: exception Emask 0x0 SAct 0x1 SErr 0x0 action 0x6 frozen
[11903.003179] ata3.00: failed command: READ FPDMA QUEUED
[11903.003364] ata3.00: cmd 60/08:00:a8:77:57/00:00:00:00:00/40 tag 0 ncq 4096 in
[11903.003385]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
[11903.004074] ata3.00: status: { DRDY }
[11903.004248] ata3: hard resetting link
[11903.325703] ata3: SATA link up 3.0 Gbps (SStatus 123 SControl 300)
[11903.327097] ata3.00: configured for UDMA/133
[11903.328025] ata3.00: device reported invalid CHS sector 0
[11903.329664] ata3: EH complete
[11941.000472] ata3.00: exception Emask 0x0 SAct 0x1 SErr 0x0 action 0x6 frozen
[11941.000769] ata3.00: failed command: READ FPDMA QUEUED
[11941.000952] ata3.00: cmd 60/08:00:c8:77:57/00:00:00:00:00/40 tag 0 ncq 4096 in
[11941.000961]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
[11941.001353] ata3.00: status: { DRDY }
[11941.001504] ata3: hard resetting link
[11941.320297] ata3: SATA link up 3.0 Gbps (SStatus 123 SControl 300)
[11941.321252] ata3.00: configured for UDMA/133
[11941.321379] ata3.00: device reported invalid CHS sector 0
[11941.321553] ata3: EH complete
[11980.001746] ata3.00: exception Emask 0x0 SAct 0x11fff SErr 0x0 action 0x6 frozen
[11980.002070] ata3.00: failed command: WRITE FPDMA QUEUED
[11980.002255] ata3.00: cmd 61/18:00:28:23:59/00:00:00:00:00/40 tag 0 ncq 12288 out
[11980.002265]          res 40/00:01:00:00:00/00:00:00:00:00/00 Emask 0x4 (timeout)
-------------------
There are many other errors, like ""lost write page"", ""Journal has aborted"", ""Buffer I/O error"", ""hard resetting link"" and many others.
</code></pre>

<p>After this, remount cause:</p>

<pre><code>mount / -o remount,rw
mount: cannot remount block device /dev/sda1 read-write, is write-protected
</code></pre>

<p>because WHOLE device sda keeping rootfs sda1 is READONLY.</p>

<p>In my experience this occurs in situations:</p>

<ol>
<li>HDD is really damaged. Returned writing problems are depended on HDD condition</li>
<li>Host machine is overloaded, then linux guest virtual HDD writings are timeouted</li>
<li>FC cable or SAN device (array disks over Fibre Channel) is overloaded</li>
<li>Momentary lost connection over FC or FCoE. Maybe lost/timeouted FC packet</li>
</ol>

<p>At this situations device is really read-write, but linux kernel marks this device internally as read-only and is used as read-only. This is kernel functionality maked for damage prevention, but it is useable only at 1. point.</p>

<p><strong>Question is. How to manually tell to kernel, hdd block device operates normally?</strong></p>

<p>Witiout this, kernel serve device as read-only, like 'CD-ROM', and no other command has chance to works properly, including mount/remount -o read-write , fsck and others.</p>

<blockquote>
  <p>Unusable ansvers, really qualified as spam from people who wants to help, but doesn't understand about problem nature:</p>
  
  <ol>
  <li>Try remount as read-write   (impossible, device is R-O)</li>
  <li>fsck this  (what for? device is R-O, no repair is possible)</li>
  <li>'I don't know' (first with sense, but unusable)</li>
  <li>'Replace your device' *(usually the problem is something else)</li>
  </ol>
</blockquote>

<p>Has anybody any formula for question above? Switch flag for writeable block device that reverts it from read-only to read-write state ?
At this time it seems that no-one know how.</p>

<p>It is some workarounds, but usually semiusable or unusable:</p>

<ol>
<li>Remove module supports access to specified hdd or storage array. Unfortunately usually damaged device keeps rootfs, or driver keeps both damaged device and device that keeps rootfs</li>
<li>Remove FC access to device and join this again (fctools), not allways possible, not allways works.</li>
<li>Restart WHOLE machine. Usually only this is allways possible and we allways forced to.</li>
</ol>

<p>At points 1. and 2. we tell to kernel that we completly disconnect device and connect to it again. Kernel recognized this as joining new properly operatings device. We can simulate this using USB device and momentary remove power. Point 3. is last chance and usually works. But why we should restart all?
Unfortunately at all points we lost all journals updates and dirty buffers.</p>

<p><em>Notice, at the same situations I have no problems with Windows (desktop and server).</em></p>
","<p>try with <code>blockdev --setrw</code> or <code>hdparm -r 0</code></p>
","74091"
"Clear unused space with zeros (ext3,ext4)","65566","","<p>How to clear unused space with zeros ? (ext3,ext4)</p>

<p>I'm looking for something smarter than</p>

<pre><code>cat /dev/zero &gt; /mnt/X/big_zero ; sync; rm /mnt/X/big_zero
</code></pre>

<p>Like <a href=""http://www.fsarchiver.org"" rel=""noreferrer"">FSArchiver</a> is looking for ""used space"" and ignores unused, but opposite site.</p>

<p>Purpose: I'd like to compress partition images, so filling unused space with zeros is highly recommended.</p>

<p>Btw. For btrfs : <a href=""https://unix.stackexchange.com/q/44725"">Clear unused space with zeros (btrfs)</a></p>
","<p>Such an utility is <code>zerofree</code>.</p>

<p>From its description:</p>

<blockquote>
  <p>Zerofree finds the unallocated, non-zeroed blocks in an ext2 or ext3 file-system and fills them with zeroes. This is useful if the device on which this file-system resides is a disk image. In this case, depending on the type of disk image, a secondary utility may be able to reduce the size of the disk image after zerofree has been run. Zerofree requires the file-system to be unmounted or mounted read-only.</p>
  
  <p>The usual way to achieve the same result (zeroing the unused blocks) is to run ""dd"" do create a file full of zeroes that takes up the entire free space on the drive, and then delete this file. This has many disadvantages, which zerofree alleviates:</p>
  
  <ul>
  <li>it is slow</li>
  <li>it makes the disk image (temporarily) grow to its maximal extent</li>
  <li>it (temporarily) uses all free space on the disk, so other concurrent write actions may fail.</li>
  </ul>
  
  <p>Zerofree has been written to be run from GNU/Linux systems installed
  as guest OSes inside a virtual machine. If this is not your case, you
  almost certainly don't need this package.</p>
</blockquote>

<h3>UPDATE #1</h3>

<p>The description of the .deb package contains the following paragraph now which would imply this will work fine with ext4 too.</p>

<blockquote>
  <p>Description: zero free blocks from ext2, ext3 and ext4 file-systems
   Zerofree finds the unallocated blocks with non-zero value content in
   an ext2, ext3 or ext4 file-system and fills them with zeroes...</p>
</blockquote>
","44236"
"how to rename multiple files by replacing string in file name? this string contains a ""#""","65437","","<p><a href=""https://serverfault.com/questions/70939/how-to-replace-a-text-string-in-multiple-files-in-linux"">https://serverfault.com/questions/70939/how-to-replace-a-text-string-in-multiple-files-in-linux</a></p>

<p><a href=""https://serverfault.com/questions/228733/how-to-rename-multiple-files-by-replacing-word-in-file-name"">https://serverfault.com/questions/228733/how-to-rename-multiple-files-by-replacing-word-in-file-name</a></p>

<p><a href=""https://serverfault.com/questions/212153/replace-string-in-files-with-certain-file-extension"">https://serverfault.com/questions/212153/replace-string-in-files-with-certain-file-extension</a></p>

<p><a href=""https://serverfault.com/questions/33158/searching-a-number-of-files-for-a-string-in-linux"">https://serverfault.com/questions/33158/searching-a-number-of-files-for-a-string-in-linux</a></p>

<p>These mentioned articles have all answered my question. However none of them work for me. I suspect it is because the string I am trying to replace has a # in it. Is there a special way to address this?</p>

<p>I have image file that had an é replaced by #U00a9 during a site migration. These look like this:</p>

<pre><code>Lucky-#U00a9NBC-80x60.jpg
Lucky-#U00a9NBC-125x125.jpg
Lucky-#U00a9NBC-150x150.jpg
Lucky-#U00a9NBC-250x250.jpg
Lucky-#U00a9NBC-282x232.jpg
Lucky-#U00a9NBC-300x150.jpg
Lucky-#U00a9NBC-300x200.jpg
Lucky-#U00a9NBC-300x250.jpg
Lucky-#U00a9NBC-360x240.jpg
Lucky-#U00a9NBC-400x250.jpg
Lucky-#U00a9NBC-430x270.jpg
Lucky-#U00a9NBC-480x240.jpg
Lucky-#U00a9NBC-600x240.jpg
Lucky-#U00a9NBC-600x250.jpg
Lucky-#U00a9NBC.jpg
</code></pre>

<p>and I want to change it to something like this:</p>

<pre><code>Lucky-safeNBC-80x60.jpg
Lucky-safeNBC-125x125.jpg
Lucky-safeNBC-150x150.jpg
Lucky-safeNBC-250x250.jpg
Lucky-safeNBC-282x232.jpg
Lucky-safeNBC-300x150.jpg
Lucky-safeNBC-300x200.jpg
Lucky-safeNBC-300x250.jpg
Lucky-safeNBC-360x240.jpg
Lucky-safeNBC-400x250.jpg
Lucky-safeNBC-430x270.jpg
Lucky-safeNBC-480x240.jpg
Lucky-safeNBC-600x240.jpg
Lucky-safeNBC-600x250.jpg
Lucky-safeNBC.jpg
</code></pre>

<p>UPDATE: </p>

<p>These examples all start with ""LU00a9ucky but here are many images with different names. I am simply  targeting the ""#U00a9"" portion of the string to replace with ""safe"".</p>
","<p>This is not hard, simply make sure to escape the octothorpe (#) in the name by prepending a reverse-slash (\).</p>

<pre><code>find . -type f -name 'Lucky-*' | while read FILE ; do
    newfile=""$(echo ${FILE} |sed -e 's/\\#U00a9/safe/')"" ;
    mv ""${FILE}"" ""${newfile}"" ;
done 
</code></pre>
","175137"
"Pass shell variable as a /pattern/ to awk","65289","","<p>Having the following in one of my shell functions:</p>

<pre><code>function _process () {
  awk -v l=""$line"" '
  BEGIN {p=0}
  /'""$1""'/ {p=1}
  END{ if(p) print l &gt;&gt; ""outfile.txt"" }
  '
}
</code></pre>

<p>, so when called as <code>_process $arg</code>, <code>$arg</code> gets passed as <code>$1</code>, and used as a search pattern. It works this way, because shell expands <code>$1</code> in place of awk pattern! Also <code>l</code> can be used inside awk program, being declared with <code>-v l=""$line""</code>. All fine.</p>

<p>Is it possible in same manner give pattern to search as a variable?</p>

<p>Following will not work,</p>

<pre><code>awk -v l=""$line"" -v search=""$pattern"" '
  BEGIN {p=0}
  /search/ {p=1}
  END{ if(p) print l &gt;&gt; ""outfile.txt"" }
  '
</code></pre>

<p>,as awk will not interpret <code>/search/</code> as a variable, but instead literally.</p>
","<p>Use awk's <code>~</code> operator, and you don't need to provide a literal regex on the right-hand side:</p>

<pre><code>function _process () {
    awk -v l=""$line"" -v pattern=""$1"" '
        $0 ~ pattern {p=1} 
        END {if(p) print l &gt;&gt; ""outfile.txt""}
    '  
}
</code></pre>

<p>Although this would be more efficient (don't have to read the whole file)</p>

<pre><code>function _process () {
    grep -q ""$1"" &amp;&amp; echo ""$line""
}
</code></pre>

<p>Depending on the pattern, may want <code>grep -Eq ""$1""</code></p>
","120795"
"Test if there are files matching a pattern in order to execute a script","65207","","<p>I am trying to write an <code>if</code> statement to test whether there are any files matching a certain pattern. If there is a text file in a directory it should run a given script.</p>

<p>My code currently:</p>

<pre><code>if [ -f /*.txt ]; then ./script fi
</code></pre>

<p>Please give some ideas; I only want to run the script if there is a <code>.txt</code> in the directory. </p>
","<pre><code>[ -f /*.txt ]
</code></pre>

<p>would return true only if there's <strong>one</strong> (and only one) non-hidden file in <code>/</code> whose name ends in <code>.txt</code> and if that file is a regular file or a symlink to a regular file.</p>

<p>That's because wildcards are expanded by the shell prior to being passed to the command (here <code>[</code>).</p>

<p>So if there's a <code>/a.txt</code> and <code>/b.txt</code>, <code>[</code> will be passed 5 arguments: <code>[</code>, <code>-f</code>, <code>/a.txt</code>, <code>/b.txt</code> and <code>]</code>. <code>[</code> would then complain that <code>-f</code> is given too many arguments.</p>

<p>If you want to check that the <code>*.txt</code> pattern expands to at least one non-hidden file (regular or not):</p>

<pre><code>shopt -s nullglob
set -- *.txt
if [ ""$#"" -gt 0 ]; then
  ./script ""$@"" # call script with that list of files.
fi
# Or with bash arrays so you can keep the arguments:
files=( *.txt )
# apply C-style boolean on member count
(( ${#files[@]} )) &amp;&amp; ./script ""${files[@]}""
</code></pre>

<p><code>shopt -s nullglob</code> is <code>bash</code> specific, but shells like <code>ksh93</code>, <code>zsh</code>, <code>yash</code>, <code>tcsh</code> have equivalent statements.</p>

<p>Note that it finds those files by reading the content of the directory, it doesn't try and access those files at all which makes it more efficient than solutions that call commands like <code>ls</code> or <code>stat</code> on that list of files computed by the shell.</p>

<p>The standard <code>sh</code> equivalent would be:</p>

<pre><code>set -- [*].txt *.txt
case ""$1$2"" in
  ('[*].txt*.txt') ;;
  (*) shift; script ""$@""
esac
</code></pre>

<p>The problem is that with Bourne or POSIX shells, if a pattern doesn't match, it expands to itself. So if <code>*.txt</code> expands to <code>*.txt</code>, you don't know whether it's because there's no <code>.txt</code> file in the directory or because there's one file called <code>*.txt</code>. Using <code>[*].txt *.txt</code> allows to discriminate between the two.</p>
","79304"
"Installing OpenSSL shared libraries on CentOS 6.5","65169","","<pre><code>OS: CentOS-6.5-x86_64-minimal
</code></pre>

<p>I downloaded the latest version of <a href=""https://www.openssl.org/source/openssl-1.0.1e.tar.gz"" rel=""noreferrer"">OpenSSL</a> </p>

<p>Extracted it with <code>tar -xvzf openssl-1.0.1e.tar.gz</code></p>

<pre><code>cd openssl-1.0.1e
./config --prefix=/usr/local
make
</code></pre>

<p>it gives me the following error:</p>

<pre><code>making all in crypto...
make[1]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto'
making all in crypto/objects...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/objects'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/objects'
making all in crypto/md4...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/md4'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/md4'
making all in crypto/md5...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/md5'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/md5'
making all in crypto/sha...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/sha'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/sha'
making all in crypto/mdc2...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/mdc2'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/mdc2'
making all in crypto/hmac...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/hmac'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/hmac'
making all in crypto/ripemd...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ripemd'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ripemd'
making all in crypto/whrlpool...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/whrlpool'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/whrlpool'
making all in crypto/des...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/des'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/des'
making all in crypto/aes...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/aes'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/aes'
making all in crypto/rc2...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/rc2'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/rc2'
making all in crypto/rc4...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/rc4'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/rc4'
making all in crypto/idea...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/idea'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/idea'
making all in crypto/bf...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/bf'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/bf'
making all in crypto/cast...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/cast'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/cast'
making all in crypto/camellia...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/camellia'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/camellia'
making all in crypto/seed...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/seed'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/seed'
making all in crypto/modes...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/modes'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/modes'
making all in crypto/bn...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/bn'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/bn'
making all in crypto/ec...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ec'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ec'
making all in crypto/rsa...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/rsa'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/rsa'
making all in crypto/dsa...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/dsa'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/dsa'
making all in crypto/ecdsa...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ecdsa'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ecdsa'
making all in crypto/dh...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/dh'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/dh'
making all in crypto/ecdh...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ecdh'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ecdh'
making all in crypto/dso...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/dso'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/dso'
making all in crypto/engine...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/engine'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/engine'
making all in crypto/buffer...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/buffer'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/buffer'
making all in crypto/bio...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/bio'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/bio'
making all in crypto/stack...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/stack'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/stack'
making all in crypto/lhash...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/lhash'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/lhash'
making all in crypto/rand...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/rand'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/rand'
making all in crypto/err...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/err'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/err'
making all in crypto/evp...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/evp'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/evp'
making all in crypto/asn1...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/asn1'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/asn1'
making all in crypto/pem...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/pem'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/pem'
making all in crypto/x509...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/x509'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/x509'
making all in crypto/x509v3...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/x509v3'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/x509v3'
making all in crypto/conf...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/conf'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/conf'
making all in crypto/txt_db...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/txt_db'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/txt_db'
making all in crypto/pkcs7...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/pkcs7'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/pkcs7'
making all in crypto/pkcs12...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/pkcs12'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/pkcs12'
making all in crypto/comp...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/comp'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/comp'
making all in crypto/ocsp...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ocsp'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ocsp'
making all in crypto/ui...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ui'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ui'
making all in crypto/krb5...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/krb5'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/krb5'
making all in crypto/cms...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/cms'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/cms'
making all in crypto/pqueue...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/pqueue'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/pqueue'
making all in crypto/ts...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/ts'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/ts'
making all in crypto/srp...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/srp'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/srp'
making all in crypto/cmac...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/crypto/cmac'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto/cmac'
if [ -n """" ]; then \
                (cd ..; make libcrypto.so.1.0.0); \
        fi
make[1]: Leaving directory `/usr/local/src/openssl-1.0.1e/crypto'
making all in ssl...
make[1]: Entering directory `/usr/local/src/openssl-1.0.1e/ssl'
if [ -n """" ]; then \
                (cd ..; make libssl.so.1.0.0); \
        fi
make[1]: Leaving directory `/usr/local/src/openssl-1.0.1e/ssl'
making all in engines...
make[1]: Entering directory `/usr/local/src/openssl-1.0.1e/engines'
echo

making all in engines/ccgost...
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/engines/ccgost'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/engines/ccgost'
make[1]: Leaving directory `/usr/local/src/openssl-1.0.1e/engines'
making all in apps...
make[1]: Entering directory `/usr/local/src/openssl-1.0.1e/apps'
rm -f openssl
shlib_target=; if [ -n """" ]; then \
                shlib_target=""linux-shared""; \
        elif [ -n """" ]; then \
          FIPSLD_CC=""gcc""; CC=/usr/local/ssl/fips-2.0/bin/fipsld; export CC FIPSLD_CC; \
        fi; \
        LIBRARIES=""-L.. -lssl  -L.. -lcrypto"" ; \
        make -f ../Makefile.shared -e \
                APPNAME=openssl OBJECTS=""openssl.o verify.o asn1pars.o req.o dgst.o dh.o dhparam.o enc.o passwd.o gendh.o errstr.o ca.o pkcs7.o crl2p7.o crl.o rsa.o rsautl.o dsa.o dsaparam.o ec.o ecparam.o x509.o genrsa.o gendsa.o genpkey.o s_server.o s_client.o speed.o s_time.o apps.o s_cb.o s_socket.o app_rand.o version.o sess_id.o ciphers.o nseq.o pkcs12.o pkcs8.o pkey.o pkeyparam.o pkeyutl.o spkac.o smime.o cms.o rand.o engine.o ocsp.o prime.o ts.o srp.o"" \
                LIBDEPS="" $LIBRARIES -ldl"" \
                link_app.${shlib_target}
make[2]: Entering directory `/usr/local/src/openssl-1.0.1e/apps'
( :; LIBDEPS=""${LIBDEPS:--L.. -lssl  -L.. -lcrypto -ldl}""; LDCMD=""${LDCMD:-gcc}""; LDFLAGS=""${LDFLAGS:--DOPENSSL_THREADS -D_REENTRANT -DDSO_DLFCN -DHAVE_DLFCN_H -Wa,--noexecstack -m64 -DL_ENDIAN -DTERMIO -O3 -Wall -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DMD5_ASM -DAES_ASM -DVPAES_ASM -DBSAES_ASM -DWHIRLPOOL_ASM -DGHASH_ASM}""; LIBPATH=`for x in $LIBDEPS; do echo $x; done | sed -e 's/^ *-L//;t' -e d | uniq`; LIBPATH=`echo $LIBPATH | sed -e 's/ /:/g'`; LD_LIBRARY_PATH=$LIBPATH:$LD_LIBRARY_PATH ${LDCMD} ${LDFLAGS} -o ${APPNAME:=openssl} openssl.o verify.o asn1pars.o req.o dgst.o dh.o dhparam.o enc.o passwd.o gendh.o errstr.o ca.o pkcs7.o crl2p7.o crl.o rsa.o rsautl.o dsa.o dsaparam.o ec.o ecparam.o x509.o genrsa.o gendsa.o genpkey.o s_server.o s_client.o speed.o s_time.o apps.o s_cb.o s_socket.o app_rand.o version.o sess_id.o ciphers.o nseq.o pkcs12.o pkcs8.o pkey.o pkeyparam.o pkeyutl.o spkac.o smime.o cms.o rand.o engine.o ocsp.o prime.o ts.o srp.o ${LIBDEPS} )
../libcrypto.a(x86_64cpuid.o): In function `OPENSSL_cleanse':
(.text+0x1a0): multiple definition of `OPENSSL_cleanse'
../libcrypto.a(mem_clr.o):mem_clr.c:(.text+0x0): first defined here
../libcrypto.a(cmll-x86_64.o): In function `Camellia_cbc_encrypt':
(.text+0x1f00): multiple definition of `Camellia_cbc_encrypt'
../libcrypto.a(cmll_cbc.o):cmll_cbc.c:(.text+0x0): first defined here
../libcrypto.a(aes-x86_64.o): In function `AES_encrypt':
(.text+0x460): multiple definition of `AES_encrypt'
../libcrypto.a(aes_core.o):aes_core.c:(.text+0x5cf): first defined here
../libcrypto.a(aes-x86_64.o): In function `AES_decrypt':
(.text+0x9f0): multiple definition of `AES_decrypt'
../libcrypto.a(aes_core.o):aes_core.c:(.text+0xa4b): first defined here
../libcrypto.a(aes-x86_64.o): In function `private_AES_set_encrypt_key':
(.text+0xab0): multiple definition of `private_AES_set_encrypt_key'
../libcrypto.a(aes_core.o):aes_core.c:(.text+0x0): first defined here
../libcrypto.a(aes-x86_64.o): In function `private_AES_set_decrypt_key':
(.text+0xd80): multiple definition of `private_AES_set_decrypt_key'
../libcrypto.a(aes_core.o):aes_core.c:(.text+0x3e5): first defined here
../libcrypto.a(aes-x86_64.o): In function `AES_cbc_encrypt':
(.text+0xfa0): multiple definition of `AES_cbc_encrypt'
../libcrypto.a(aes_cbc.o):aes_cbc.c:(.text+0x0): first defined here
collect2: ld returned 1 exit status
make[2]: *** [link_app.] Error 1
make[2]: Leaving directory `/usr/local/src/openssl-1.0.1e/apps'
make[1]: *** [openssl] Error 2
make[1]: Leaving directory `/usr/local/src/openssl-1.0.1e/apps'
make: *** [build_apps] Error 1
</code></pre>

<p>I tried <code>yum -y install openssl</code>.</p>

<p>I want to install OpenSSL to be able to use the HTTPS protocol in CURL, and different applications.</p>

<p><code>openssl</code> (which is the binary) is installed, but OpenSSL (which is required for the HTTPS protocol is not installed).</p>

<p>Any solutions to this problem?</p>
","<p>I wanted to compile tomcat with OpenSSL support and OpenSSL source code alone wasn't enough.  Try installing the OpenSSL development libraries:</p>

<pre><code>yum install -y openssl-devel
</code></pre>
","122583"
"how to count total number of words in a file?","65150","","<p>I am looking for a command to count number of all words in a file. For instance if a file is like this,</p>

<pre><code>today is a 
good day
</code></pre>

<p>then it should print <code>5</code>, since there are <code>5</code> words there.</p>
","<p>The command <code>wc</code> aka. word count can do it:</p>

<pre><code>$ wc -w &lt;file&gt;
</code></pre>

<h3>example</h3>

<pre><code>$ cat sample.txt
today is a 
good day


$ wc -w sample.txt
5 sample.txt


# just the number (thanks to Stephane Chazelas' comment)
$ wc -w &lt; sample.txt
5
</code></pre>
","80018"
"How to replace the content of a specific column with awk?","65105","","<p>Given: there are 40 columns in a record. I want to replace the 35th column so that the 35th column will be replaced with the content of the 35th column and a ""$"" symbol. What came to mind is something like:</p>

<pre><code>awk '{print $1"" ""$2"" ""...$35""$ ""$36...$40}'
</code></pre>

<p>It works but because it is infeasible when the number of column is as large as 10k. I need a better way to do this.</p>
","<p>You can do like this:</p>

<pre><code>awk '$35=$35""$""'
</code></pre>
","136325"
"How can I grep in PDF files?","65036","","<p>Is there a way to search pdf files using the power of grep, without converting to text first in Ubuntu?</p>
","<p>Install the package <code>pdfgrep</code>, then use the command:</p>

<pre><code>find /path -iname '*.pdf' -exec pdfgrep pattern {} +
</code></pre>
","27517"
"yum installs kernel-devel different from my kernel version","64941","","<p>I am attempting to install the VMWare player in Fedora 19. I am running into the problem that multiple users have had where VMware player cannot find the kernel headers. I have installed the <code>kernel-headers</code> and <code>kernel-devel</code> packages through <code>yum</code> and the file that appears in <code>/usr/src/kernels</code> is:</p>

<pre><code>3.12.8-200.fc19.x86_64
</code></pre>

<p>However, when I do <code>uname -r</code> my Fedora kernel version is:</p>

<pre><code>3.9.5-301.fc19.x86_64
</code></pre>

<p>which is a different version. This seems to mean that when I point VMware player at the path of the kernels I get this error:</p>

<pre><code>C header files matching your running kernel were not found.  
Refer to your distribution's documentation for installation instructions.
</code></pre>

<p>How can I install the correct Kernel and where should I be pointing VMware if its not <code>/usr/src/kernels/&lt;my-kernel&gt;</code> ?</p>
","<p>You can install the correct kernel header files like so:</p>

<pre><code>$ sudo yum install ""kernel-devel-uname-r == $(uname -r)""
</code></pre>

<h3>Example</h3>

<p>This command will always install the right version.</p>

<pre><code>$ sudo yum install ""kernel-devel-uname-r == $(uname -r)""
Loaded plugins: auto-update-debuginfo, changelog, langpacks, refresh-packagekit
No package kernel-devel-uname-r == 3.12.6-200.fc19.x86_64 available.
Error: Nothing to do
</code></pre>

<p>Or you can search for them like this:</p>

<pre><code>$ yum search ""kernel-headers-uname-r == $(uname -r)"" --disableexcludes=all
Loaded plugins: auto-update-debuginfo, changelog, langpacks, refresh-packagekit
Warning: No matches found for: kernel-headers-uname-r == 3.12.6-200.fc19.x86_64
No matches found
</code></pre>

<p>However I've notice this issue as well where specific versions of headers are not present in the repositories. You might have to reach into Koji to find a particular version of a build.</p>

<ul>
<li><a href=""http://koji.fedoraproject.org/koji/buildinfo?buildID=486642"">Information for build kernel-3.12.6-200.fc19</a></li>
</ul>

<p>That page includes all the assets for that particular version of the Kernel.</p>
","110685"
"Can't start mysql service","64674","","<p>Trying to start my mysql service.</p>

<pre><code>/etc/init.d/mysql start
</code></pre>

<p>returns:</p>

<pre><code>Job failed. See system logs and 'systemctl status' for details.
</code></pre>

<p>Further:</p>

<pre><code>systemctl status mysql.service
</code></pre>

<p>returns:</p>

<pre><code>mysql.service - LSB: Start the MySQL database server
      Loaded: loaded (/etc/init.d/mysql)
      Active: failed since Mon, 04 Aug 2014 16:20:43 -0400; 38s ago
     Process: 14148 ExecStop=/etc/init.d/mysql stop (code=exited, status=0/SUCCESS)
     Process: 16457 ExecStart=/etc/init.d/mysql start (code=exited, status=1/FAILURE)
      CGroup: name=systemd:/system/mysql.service
</code></pre>

<p>Any ideas how I can find out what's happening?</p>

<p>That latest entry in <code>/var/log/mysql/mysqld.log</code>:</p>

<pre><code>140805 08:52:42 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql
140805  8:52:42 [ERROR] mysqld: Can't lock aria control file '/var/lib/mysql/aria_log_control' for exclusive use, error: 11. Will retry for 30 seconds
140805  8:53:13 [ERROR] mysqld: Got error 'Could not get an exclusive lock; file is probably in use by another process' when trying to use aria control file '/var/lib/mysql/aria_log_contr$
140805  8:53:13 [ERROR] Plugin 'Aria' init function returned error.
140805  8:53:13 [ERROR] Plugin 'Aria' registration as a STORAGE ENGINE failed.
140805  8:53:13 [ERROR] Failed to initialize plugins.
140805  8:53:13 [ERROR] Aborting

140805  8:53:13 [Note] /usr/sbin/mysqld: Shutdown complete

140805 08:53:13 mysqld_safe mysqld from pid file /var/run/mysql/mysqld.pid ended
</code></pre>
","<p>The <code>/var/lib/mysql/aria_log_control</code> file is open by another process and consequently, <code>mysqld</code> fails to start.</p>

<p>Check who/what is currently has the file open with:</p>

<pre><code>lsof `/var/lib/mysql/aria_log_control`
</code></pre>

<p>It should list the process(es) that has it open.</p>

<pre><code>COMMAND  PID  USER   FD   TYPE DEVICE SIZE/OFF   NODE NAME
mysqld  1506 mysql   10uW  REG  253,1       52 263948 /var/lib/mysql/aria_log_control
</code></pre>

<p>If the process definitely shouldn't be running, then shut it down with:</p>

<pre><code>sudo kill -SIGTERM &lt;PID&gt;
</code></pre>

<p>If that fails:</p>

<pre><code>sudo kill -SIGKILL &lt;PID&gt;
</code></pre>

<p>Or reboot.</p>
","148508"
"How to check the life left in SSD or the medium's wear level?","64652","","<p>We all know that SSDs have a limited predetermined life span. How do I check in Linux what the current health status of an SSD is?</p>

<p>Most Google search results would ask you to look up S.M.A.R.T. information for a percentage field called Media_Wearout_Indicator, or other jargons indicators like Longterm Data Endurance -- which don't exist -- Yes I did check two SSDs, both lack these fields. I could go on to find a third SSD, but I feel the fields are not standardized.</p>

<p>To demonstrate the problem here are the two examples.</p>

<hr>

<p>With the first SSD, it is not clear which field indicates wearout level. However there is only one Unknown_Attribute whose RAW VALUE is between 1 and 100, thus I can only assume that is what we are looking for:</p>

<pre><code>    $ sudo smartctl -A /dev/sda                                             
    smartctl 6.2 2013-04-20 r3812 [x86_64-linux-3.11.0-14-generic] (local build)
    Copyright (C) 2002-13, Bruce Allen, Christian Franke, www.smartmontools.org

    === START OF READ SMART DATA SECTION ===                                 
    SMART Attributes Data Structure revision number: 1                       
    Vendor Specific SMART Attributes with Thresholds:                        
    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE
      5 Reallocated_Sector_Ct   0x0002   100   100   000    Old_age   Always       -       0
      9 Power_On_Hours          0x0002   100   100   000    Old_age   Always       -       6568
     12 Power_Cycle_Count       0x0002   100   100   000    Old_age   Always       -       1555
    171 Unknown_Attribute       0x0002   100   100   000    Old_age   Always       -       0
    172 Unknown_Attribute       0x0002   100   100   000    Old_age   Always       -       0
    173 Unknown_Attribute       0x0002   100   100   000    Old_age   Always       -       57
    174 Unknown_Attribute       0x0002   100   100   000    Old_age   Always       -       296
    187 Reported_Uncorrect      0x0002   100   100   000    Old_age   Always       -       0
    230 Unknown_SSD_Attribute   0x0002   100   100   000    Old_age   Always       -       190
    232 Available_Reservd_Space 0x0003   100   100   005    Pre-fail  Always       -       0
    234 Unknown_Attribute       0x0002   100   100   000    Old_age   Always       -       350
    241 Total_LBAs_Written      0x0002   100   100   000    Old_age   Always       -       742687258
    242 Total_LBAs_Read         0x0002   100   100   000    Old_age   Always       -       1240775277
</code></pre>

<p>So this SSD has used 57% of its rewrite life-span, is it correct?</p>

<hr>

<p>With the other disk, the SSD_Life_Left ATTRIBUTE stands out, but its Raw value of 0, indicating 0% life left, is unlikely for an apparently-healthy SSD unless it happen to be in peril (we will see in a few days), and if it reads ""0% life has been used"", also impossible for a worn hard disk (worn = used for more than a year).</p>

<pre><code>    &gt; sudo /usr/sbin/smartctl -A /dev/sda
    smartctl 6.2 2013-07-26 r3841 [x86_64-linux-3.11.6-4-desktop] (SUSE RPM)
    Copyright (C) 2002-13, Bruce Allen, Christian Franke, www.smartmontools.org

    === START OF READ SMART DATA SECTION ===
    SMART Attributes Data Structure revision number: 10
    Vendor Specific SMART Attributes with Thresholds:
    ID# ATTRIBUTE_NAME          FLAG     VALUE WORST THRESH TYPE      UPDATED  WHEN_FAILED RAW_VALUE
      1 Raw_Read_Error_Rate     0x000f   104   100   050    Pre-fail  Always       -       0/8415644
      5 Retired_Block_Count     0x0033   100   100   003    Pre-fail  Always       -       0
      9 Power_On_Hours_and_Msec 0x0032   100   100   000    Old_age   Always       -       4757h+02m+17.130s
     12 Power_Cycle_Count       0x0032   099   099   000    Old_age   Always       -       1371
    171 Program_Fail_Count      0x0032   000   000   000    Old_age   Always       -       0
    172 Erase_Fail_Count        0x0032   000   000   000    Old_age   Always       -       0
    174 Unexpect_Power_Loss_Ct  0x0030   000   000   000    Old_age   Offline      -       52
    177 Wear_Range_Delta        0x0000   000   000   000    Old_age   Offline      -       2
    181 Program_Fail_Count      0x0032   000   000   000    Old_age   Always       -       0
    182 Erase_Fail_Count        0x0032   000   000   000    Old_age   Always       -       0
    187 Reported_Uncorrect      0x0032   100   100   000    Old_age   Always       -       0
    194 Temperature_Celsius     0x0022   030   030   000    Old_age   Always       -       30 (Min/Max 30/30)
    195 ECC_Uncorr_Error_Count  0x001c   104   100   000    Old_age   Offline      -       0/8415644
    196 Reallocated_Event_Count 0x0033   100   100   000    Pre-fail  Always       -       0
    231 SSD_Life_Left           0x0013   100   100   010    Pre-fail  Always       -       0
    233 SandForce_Internal      0x0000   000   000   000    Old_age   Offline      -       3712
    234 SandForce_Internal      0x0032   000   000   000    Old_age   Always       -       1152
    241 Lifetime_Writes_GiB     0x0032   000   000   000    Old_age   Always       -       1152
    242 Lifetime_Reads_GiB      0x0032   000   000   000    Old_age   Always       -       3072
</code></pre>
","<p>In your first example, what I think you are referring to is the ""Media Wearout Indicator"" on Intel drives, which is attribute 233. Yes, it has a range of 0-100, with 100 being a brand new, unused drive, and 0 being completely worn out. According to your ouptut, this field doesn't seem to exist.</p>

<p>In your second example, please read the <a href=""http://sourceforge.net/apps/trac/smartmontools/wiki/FAQ#TheSSD_Life_LeftAttributeofmynewSandForcebasedSSDreportszero"">official docs about SSD_Life_Left.</a> Per that page:</p>

<blockquote>
  <p>The RAW value of this attribute is always 0 and has no meaning. Check the normalized VALUE instead. It starts at 100 and indicates the approximate percentage of SDD life left. It typically decreases when Flash blocks are marked as bad, see the RAW value of Retired_Block_Count</p>
</blockquote>

<p>It's really important that you fully understand what smartctl(8) is saying, and not making assumptions. Unfortunately, the S.M.A.R.T. tools aren't always up to date with the latest SSDs and their attributes. As such, there isn't always a clean way to tell how many times the chips have been written to. Best you can do, is look at the ""Power_On_Hours"", which in your case is ""6568"", determine your average disk utilization, and average it out.</p>

<p>You should be able to lookup your drive specs, and determine the process used to make the chips. 32nm process chips will have a longer write endurance than 24nm process chips. However, it seems that ""on average"", you could probably expect about 3,000 to 4,000 writes, with a minimum of 1,000 and a max of 6,000. So, if you have a 64GB SSD, then you should expect somewhere in the neighborhood of a total of 192TB to 256TB written to the SSD, assuming wear leveling.</p>

<p>As an example, if you're sustaining a utilization of say 11 KBps to your drive, then you could expect to see about 40 MB written per hour. At 6568 powered on hours, you've written roughly 260 GB to disk. Knowing that you could probably sustain about 200 TB of total writes, before failure, you have about 600 years before failure due to wearing out the chips. Your disk will likely fail due to worn out capacitors or voltage regulation.</p>
","106679"
"How to create a Samba share that is writable from Windows without 777 permissions?","64522","","<p>I have a path on a Linux machine (Debian 8) which I want to share with Samba 4 to Windows computers (Win7 and 8 in a domain). In my <code>smb.conf</code> I did the following:</p>

<pre><code>[myshare]
path = /path/to/share
writeable = yes
browseable = yes
guest ok = yes
public = yes
</code></pre>

<p>I have perfect read access from Windows. But in order to have write access, I need to do <code>chmod -R 777 /path/to/share</code> in order to be able to write to it from Windows.</p>

<p>What I want is write access from Windows after I provide the Linux credentials of the Linux owner of <code>/path/to/share</code>.</p>

<p>I already tried:</p>

<pre><code>[myshare]
path = /path/to/share
writeable = yes
browseable = yes
</code></pre>

<p>Then Windows asks for credentials, but no matter what I enter, it's always denied.</p>

<p>What is the correct way to gain write access to Samba shares from a Windows domain computer without granting 777?</p>
","<p>I recommend to create a dedicated user for that share and specify it in <code>force user</code>.</p>

<p>Create a user (<code>shareuser</code> for example) and set the owner of everything in the share folder to that user:</p>

<pre><code>adduser --system shareuser
chown -R shareuser /path/to/share
</code></pre>

<p>Then add <code>force user</code> and permission mask settings in <code>smb.conf</code>:</p>

<pre><code>[myshare]
path = /path/to/share
writeable = yes
browseable = yes
public = yes
create mask = 0644
directory mask = 0755
force user = shareuser
</code></pre>

<p>Note that <code>guest ok</code> is a synonym for <code>public</code>.</p>
","206310"
"ssh server: reasons for sudden ""Connection closed by remote host""","64487","","<p>One of my ec2 servers has stopped receiving ssh connections. The OS is Ubuntu server 8.04, and the ssh server is the standard <code>openssh-server</code>.</p>

<p>After months of uptime, I tried to connet to it today, and got the following message:</p>

<pre><code>ssh_exchange_identification: Connection closed by remote host
</code></pre>

<p>Any idea what could have went wrong?</p>

<p><strong>Update:</strong> After a reboot, the server started receiving new connections. Disks are below 50% usage.</p>
","<p>Now that you once again have access, check the log to determine what, if any, clues there are as to why you were blocked.</p>

<pre><code>tail -n300 /var/log/auth.log | grep ssh <sup>1</sup></code></pre>

<p>The other thing to remember is that, if it happens again, you can run <code>ssh</code> in verbose mode with the <code>-vvv</code> option, which will return more detailed diagnostic information. From <code>man ssh</code>:</p>

<blockquote><b>-v &nbsp; &nbsp;Verbose mode</b>.  Causes ssh to print debugging messages about its progress.  This is helpful in debugging connection, authentication, and configuration problems.  Multiple -v options increase the verbosity.  The maximum is 3.</blockquote>

<p><br /><br />
[1] You may need to increase/decrease the amount you tail by (<code>-n</code>) to identify the relevant entries.</p>
","19418"
"How do I display log messages from previous boots under CentOS 7?","64463","","<p>Executing <code>journalctl</code> under a CentOS 7 system just prints messages generated after the last boot.</p>

<p>The command</p>

<pre><code># journalctl --boot=-1
</code></pre>

<p>prints</p>

<pre><code>Failed to look up boot -1: Cannot assign requested address
</code></pre>

<p>and exits with status 1.</p>

<p>Comparing it to a current Fedora system I notice that the CentOS 7 does not have <code>/var/log/journal</code> (and <code>journalctl</code> does not provide <code>--list-boots</code>).</p>

<p>Thus my question how to display log messages which were written before the last boot date.</p>

<p>Or, perhaps this functionality has to be enabled on CentOS 7?</p>

<p>(The <code>journalctl</code> man page lists 'systemd 208' as version number.)</p>
","<h2>tl;dr</h2>

<p>On CentOS 7, you have to enable the persistent storage of log messages:</p>

<pre><code># mkdir /var/log/journal
# systemd-tmpfiles --create --prefix /var/log/journal
# systemctl restart systemd-journald
</code></pre>

<p>Otherwise, the journal log messages are not retained between boots.</p>

<h2>Details</h2>

<p>Whether <code>journald</code> retains log messages from previous boots is configured via <code>/etc/systemd/journald.conf</code>. The default setting under CentOS 7 is:</p>

<pre><code>[Journal]
Storage=auto
</code></pre>

<p>Where the <a href=""http://www.freedesktop.org/software/systemd/man/journald.conf.html"">journald.conf man page</a> explains <code>auto</code> as:</p>

<blockquote>
  <p>One of ""volatile"", ""persistent"", ""auto"" and ""none"". If ""volatile"", journal log data will be stored only in memory, i.e. below the /run/log/journal hierarchy (which is created if needed). If ""persistent"", data will be stored preferably on disk, i.e. below the /var/log/journal hierarchy (which is created if needed), with a fallback to /run/log/journal (which is created if needed), during early boot and if the disk is not writable. ""<strong>auto</strong>"" is similar to ""persistent"" but the directory <strong>/var/log/journal</strong> is not created if needed, <strong>so that its existence controls where log data goes</strong>.</p>
</blockquote>

<p>(emphasize mine)</p>

<p>The <a href=""http://www.freedesktop.org/software/systemd/man/systemd-journald.service.html"">systemd-journald.service man page</a> thus states that:</p>

<blockquote>
  <p>By default, the journal stores log data in /run/log/journal/. Since /run/ is volatile, log data is lost at reboot. To make the data persistent, it is sufficient to create /var/log/journal/ where systemd-journald will then store the data.</p>
</blockquote>

<p>Apparently, the default was <a href=""http://0pointer.de/blog/projects/journalctl.html"">changed in Fedora 19</a> (to persitent storage) and since CentOS 7 is derived from Fedora 18 - it is still non-persisent there, by default. Persistency is implemented by default outside of journald via <code>/var/log/messages</code> and the rotated versions <code>/var/log/messages-YYYYMMDD</code> which are written by rsyslogd (which runs by default and gets its input from journald).</p>

<p>Thus, to enable persistent logging with journald under RHEL/CentOS 7 one has to</p>

<pre><code># mkdir /var/log/journal
</code></pre>

<p>and then fix permissions and restart journald, e.g. via</p>

<pre><code># systemd-tmpfiles --create --prefix /var/log/journal
# systemctl restart systemd-journald
</code></pre>
","159390"
"How do I kill all a user's processes using their UID","64443","","<p>I want to kill all running processes of a particular user from either a shell script or native code on a Linux system.</p>

<p>Do I have to read the /proc directory and look for these?</p>

<p>Any ideas? Is there a dynamic mapping of the pids under UIDs in Linux? Isn't this in the proc?</p>

<p>If not, then where is this list maintained? Should I read from it? Also where is the static list of all UIDs in the system so I can validate this this user exists and then proceed to kill all processes running under it?</p>
","<p>Use <code>pkill -U UID</code> or <code>pkill -u UID</code> or username instead of UID. Sometimes <code>skill -u USERNAME</code> may work, another tool is <a href=""http://linux.die.net/man/1/killall"" rel=""noreferrer""><code>killall -u USERNAME</code></a>.</p>

<p><a href=""http://linux.die.net/man/1/skill"" rel=""noreferrer"">Skill</a> was a linux-specific and is now outdated, and <a href=""http://linux.die.net/man/1/pkill"" rel=""noreferrer"">pkill</a> is more portable (Linux, Solaris, BSD).</p>

<p>pkill allow both numberic and symbolic UIDs, effective and real <a href=""http://man7.org/linux/man-pages/man1/pkill.1.html"" rel=""noreferrer"">http://man7.org/linux/man-pages/man1/pkill.1.html</a></p>

<blockquote>
  <p>pkill  -  ... signal processes based on name and other attributes</p>

<pre><code>    -u, --euid euid,...
         Only match processes whose effective user ID is listed.
         Either the numerical or symbolical value may be used.
    -U, --uid uid,...
         Only match processes whose real user ID is listed.  Either the
         numerical or symbolical value may be used.
</code></pre>
</blockquote>

<p>Man page of skill says is it allowed only to use username, not user id: <a href=""http://man7.org/linux/man-pages/man1/skill.1.html"" rel=""noreferrer"">http://man7.org/linux/man-pages/man1/skill.1.html</a></p>

<blockquote>
  <p>skill, snice ...  These tools are obsolete and unportable.  The command syntax is poorly defined.  Consider using the killall, pkill</p>

<pre><code>  -u, --user user
         The next expression is a username.
</code></pre>
</blockquote>

<p>killall is not marked as outdated in Linux, but it also will not work with numberic UID; only username: <a href=""http://man7.org/linux/man-pages/man1/killall.1.html"" rel=""noreferrer"">http://man7.org/linux/man-pages/man1/killall.1.html</a></p>

<blockquote>
  <p>killall - kill processes by name</p>

<pre><code>   -u, --user
         Kill only processes the specified user owns.  Command names
         are optional.
</code></pre>
</blockquote>

<p>I think, any utility used to find process in Linux/Solaris style /proc (procfs) will use full list of processes (doing some readdir of <code>/proc</code>). I think, they will iterate over <code>/proc</code> digital subfolders and check every found process for match.</p>

<p>To get list of users, use <a href=""http://www.kernel.org/doc/man-pages/online/pages/man3/getpwent.3.html"" rel=""noreferrer""><code>getpwent</code></a> (it will get one user per call).</p>

<p><a href=""https://gitlab.com/procps-ng/procps/blame/master/skill.c"" rel=""noreferrer"">skill</a> (procps &amp; procps-ng) and <a href=""https://sourceforge.net/p/psmisc/code/ci/master/tree/src/killall.c"" rel=""noreferrer"">killall</a> (psmisc) tools both uses <a href=""http://man7.org/linux/man-pages/man3/getpwnam.3.html"" rel=""noreferrer""><code>getpwnam</code></a> library call to parse argument of <code>-u</code> option, and only username will be parsed. <code>pkill</code> (procps &amp; procps-ng) <a href=""https://gitlab.com/procps-ng/procps/blob/625d0809daa5b666d9f5834bebcdc458799221f3/pgrep.c#L265"" rel=""noreferrer"">uses both atol and getpwnam</a> to parse <code>-u</code>/<code>-U</code> argument and allow both numeric and textual user specifier.</p>
","18044"
"Permit root to login via ssh only with key-based authentication","64338","","<p>I have some doubts about certain ssh server configurations  on <code>/etc/ssh/sshd_config</code>. I want the next behavior:</p>

<ol>
<li>Public key authentication is the only way to authenticate as root  (no password authentication or other)</li>
<li>Normal users can use both (password and public key authentication)</li>
</ol>

<p>If I set <code>PasswordAuthentication no</code> my first point is satisfied but not the second. There is a way to set <code>PasswordAuthentication no</code> only for root?</p>
","<p>You can do this using the <code>PermitRootLogin</code> directive. From the <code>sshd_config</code> manpage:</p>

<blockquote>
  <p>Specifies whether root can log in using ssh(1).  The argument must be
  “yes”, “without-password”, “forced-commands-only”, or “no”.  The
  default is “yes”.</p>
  
  <p>If this option is set to “without-password”, password authentication
  is disabled for root.</p>
</blockquote>

<p>The following will accomplish what you want:</p>

<pre><code>PasswordAuthentication yes
PermitRootLogin without-password
</code></pre>
","99308"
"What is the difference between curl and wget?","64316","","<p>I am keen to know the difference between <code>curl</code> and <code>wget</code>. Both are used to get files and documents but what the key difference between them.</p>

<p>Why are there two different programs?</p>
","<p>The main differences are:</p>

<ul>
<li><code>wget</code>'s major strong side compared to <code>curl</code> is its ability to download recursively.</li>
<li><code>wget</code> is command line only. There's no lib or anything, but <code>curl</code> features and is powered by libcurl.</li>
<li><code>curl</code> supports <code>FTP</code>, <code>FTPS</code>, <code>HTTP</code>, <code>HTTPS</code>, <code>SCP</code>, <code>SFTP</code>, <code>TFTP</code>, <code>TELNET</code>, <code>DICT</code>, <code>LDAP</code>, <code>LDAPS</code>, <code>FILE</code>, <code>POP3</code>, <code>IMAP</code>, <code>SMTP</code>, <code>RTMP</code> and <code>RTSP</code>. <code>wget</code> supports <code>HTTP</code>, <code>HTTPS</code> and <code>FTP</code>.</li>
<li><code>curl</code> builds and runs on more platforms than <code>wget</code>.</li>
<li><code>wget</code> is part of the GNU project and all copyrights are assigned to FSF. The <code>curl</code> project is entirely stand-alone and independent with no organization parenting at all</li>
<li><code>curl</code> offers upload and sending capabilities. <code>wget</code> only offers plain HTTP POST support. </li>
</ul>

<p>You can see more details at the following link:</p>

<p><a href=""http://daniel.haxx.se/docs/curl-vs-wget.html"">curl vs Wget </a> </p>
","47435"
"How to start a windows partition from the Grub command line?","64257","","<p>I have Windows 7 installed on my system. After I installed Windows 7, I installed Fedora on a separate partition so that I could dual boot.</p>

<p>I removed Fedora by deleting the partition it was installed on. Now I am unable to start my system. On boot, my system stops at the Grub command line.</p>

<p>I want to boot to my Windows 7 installation which I haven't removed from my system.</p>

<p>This is displayed on startup</p>

<pre><code>GNU GRUB version 0.97-71.fc15 (634k lower /306122k upper momory)&lt;br&gt; [
minimal BASH-like editing is supported.for the first word, TAB lists
possible commands completions.anywhere else TAB lists the possible
completion of device/filename.]
grub&gt;
</code></pre>

<p>How can I boot my Windows partition from this grub command?</p>
","<p>The ultimate goal is to restore the Master Boot Record (MBR) to the hard drive, removing Grub, so you can boot to your Windows partition in the future without stopping at the Grub command line.</p>

<p>The easiest way to achieve this is to boot from your Windows 7 installation media. Use the <code>Repair computer</code> link and choose <code>Command Line</code>. At the command line, enter <code>bootsect /nt60 SYS /mbr</code>. You can reboot your system and it will now boot into Windows.</p>
","37292"
"How to connect to a guest VM from the host system?","64175","","<p>I have a VM webserver setup and I have installed and started Apache.  The VM has a bridged network interface and can be pinged from the host using 192.168.0.2.</p>

<p>However, if I type that same IP address into the browser on the host machine, I was expecting to see the default apache page generated on the VM, but instead, I get <code>can't connect to 192.168.0.2</code> in the host machines browser.</p>

<p>I've clearly missed something out.  Anyone know what I have missed or done wrong?</p>

<p>Output from VM <code>netstat -tnlp</code></p>

<pre><code>tcp     0     0 0.0.0.0:22        0.0.0.0:*     LISTEN     950/sshd
tcp     0     0 127.0.0.1:25      0.0.0.0:*     LISTEN    1026/master
tcp     0     0 :::22                  :::*     LISTEN     904/sshd
tcp     0     0 ::1:25                 :::*     LISTEN     980/master
</code></pre>

<p>Rough drawing of what I'm thinking the network activity/connectivity would look like.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src=""https://i.stack.imgur.com/mOcL1.png"" alt=""enter image description here""></p>
","<h2>Issue #1 - VM networking types</h2>

<p>There are 3 modes of networking: </p>

<ol>
<li>NAT</li>
<li>Host Only</li>
<li>Bridged</li>
</ol>

<h3>Details on setting them up</h3>

<ul>
<li>This AU  Q&amp;A titled: ""<a href=""https://askubuntu.com/questions/293816/in-virtualbox-how-do-i-set-up-host-only-virtual-machines-that-can-access-the-in"">In VirtualBox, how do I set up host-only virtual machines that can access the Internet?</a>"", shows how to do #2.</li>
<li>This article titled: ""<a href=""http://www.thegeekstuff.com/2012/03/virtualbox-guest-additions/"" rel=""nofollow noreferrer"">How to Setup VirtualBox Guest Additions and Network</a>"", shows how to do #3.</li>
</ul>

<h3>When to use each?</h3>

<ul>
<li><strong>#1</strong>: For development of Facebook/web apps that are on other servers</li>
<li><strong>#2</strong>: If you want to build your own app, and test it from the VirtualBox host (not just the guest VM)</li>
<li><strong>#3</strong>: If you want to build an app and test it from other systems on LAN</li>
</ul>

<h2>Issue #2 - firewall blocking?</h2>

<p>Depending on which distro you're using, the firewall might be blocking your web browser from accessing your Apache instance. This would make sense given you're able to ping the system, but not access it via port 80, which is the port that Apache is listening on.</p>

<h3>temporarily disabling it</h3>

<p>On CentOS you use this command to disable it.</p>

<pre><code>$ /etc/init.d/iptables stop
</code></pre>

<h3>check that Apache's listening</h3>

<p>You can also confirm that it's listening on this port.</p>

<pre><code>$ netstat -antp | grep :80 | head -1 | column -t
tcp  0  0  :::80  :::*  LISTEN  3790/httpd
</code></pre>

<h3>confirm firewall's off</h3>

<p>The firewall can be confirmed that it's wide open.</p>

<pre><code>$ iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination      
</code></pre>

<p>If this solves your issue then you can permanently add a rule that allows traffic in via TCP port 80.</p>

<h3>adding a rule for TCP port 80</h3>

<pre><code>$ /etc/init.d/iptables restart
$ iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
$ /etc/init.d/iptables save
</code></pre>

<p><strong>NOTE:</strong> This will make the rule persist between reboots.</p>

<h3>firewall is accepting TCP port 80</h3>

<p>A system that has the port 80 open would look something like this:</p>

<pre><code>$ iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere            state RELATED,ESTABLISHED 
ACCEPT     icmp --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     tcp  --  anywhere             anywhere            state NEW tcp dpt:ssh 
ACCEPT     tcp  --  anywhere             anywhere            state NEW tcp dpt:http 
ACCEPT     tcp  --  anywhere             anywhere            state NEW tcp dpt:https 
ACCEPT     tcp  --  anywhere             anywhere            state NEW tcp dpt:8834 
REJECT     all  --  anywhere             anywhere            reject-with icmp-host-prohibited 

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             anywhere            state RELATED,ESTABLISHED 
ACCEPT     icmp --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
ACCEPT     all  --  anywhere             anywhere            
REJECT     all  --  anywhere             anywhere            reject-with icmp-host-prohibited 

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
</code></pre>

<h2>Issue #3 - Apache listening?</h2>

<p>In the above issue we saw that Apache was listening, but sometimes it's mis-configured so that it's only listening on 1 IP address, or that it's listening on a different network interface. The command <code>netstat</code> can be used to double check this as well as reviewing the Apache configuration files.</p>

<pre><code>$ netstat -anpt | grep :80 | column -t
tcp  0  0  0.0.0.0:80  0.0.0.0:*  LISTEN  1750/httpd
</code></pre>

<p>This shows that Apache is listening on all interfaces (IP 0.0.0.0).</p>

<p>I won't repeat what @<a href=""https://unix.stackexchange.com/a/108879/7453"">Lekensteyn</a>'s answer which covers this particular issue in more details here.</p>

<h3>References</h3>

<ul>
<li><a href=""https://www.virtualbox.org/manual/ch06.html"" rel=""nofollow noreferrer"">Chapter 6. Virtual networking</a></li>
</ul>
","108886"
"Testing if a variable is empty in a shell script","64156","","<p>I have seen the following technique used many times on many different shells, to test if a variable is empty:</p>

<pre><code>if [ ""x$1"" = ""x"" ]; then 
    # Variable is empty
fi
</code></pre>

<p>Are there any advantages on using this over the more canonical <code>if [ -z ""$1"" ]</code>? Could it be a portability issue?</p>
","<p>Some historical shells implemented a very simple parser that could get confused by things like <code>[ -n = """" ]</code> where the first operand to <code>=</code> looks like an operator, and would parse this as <code>[ -n = ]</code> or cause a syntax error. In <code>[ ""x$1"" = x"""" ]</code>, the <code>x</code> prefix ensures that <code>x""$1""</code> cannot possibly look like an operator, and so the only way the shell can parse this test is by treating <code>=</code> as a binary operator.</p>

<p>All modern shells, and even most older shells still in operation, follow the <a href=""http://pubs.opengroup.org/onlinepubs/009695399/utilities/test.html#tag_04_140_05"">POSIX rules</a> which mandate that all test expressions of up to 4 words be parsed correctly. So <strong><code>[ -z ""$1"" ]</code> is a proper way of testing if <code>$1</code> is empty</strong>, and <code>[ ""$x"" = ""$y"" ]</code> is a proper way to test the equality of two variables.</p>

<p>Even some current shells can get confused with longer expressions, and a few expressions are actually ambiguous, so avoid using the <code>-a</code> and <code>-o</code> operators to construct longer boolean tests, and instead use separate calls to <code>[</code> and the shell's own <code>&amp;&amp;</code> and <code>||</code> boolean operators.</p>
","32230"
"How do you fix apt-get update ""Hash Sum mismatch""","64154","","<p>I have an Ubuntu 12.04 virtual box vm that I instantiate using Vagrant. </p>

<pre><code>git clone https://github.com/spuder/puppet-gitlab
vagrant up
</code></pre>

<p>As soon as the vagrant box runs <code>apt-get update</code>, I get the following error. </p>

<pre><code>...
W: Failed to fetch gzip:/var/lib/apt/lists/partial/apt.puppetlabs.com_dists_precise_main_binary-amd64_Packages  Hash Sum mismatch
W: Failed to fetch gzip:/var/lib/apt/lists/partial/apt.puppetlabs.com_dists_precise_main_binary-i386_Packages  Hash Sum mismatch
W: Failed to fetch gzip:/var/lib/apt/lists/partial/apt.puppetlabs.com_dists_precise_dependencies_binary-i386_Packages  Hash Sum mismatch
W: Failed to fetch http://br.archive.ubuntu.com/ubuntu/dists/precise-updates/restricted/binary-i386/Packages  404  Not Found
</code></pre>

<p>Things I've tried to work around this error. </p>

<ul>
<li>Used 3 different ubuntu 12.04 boxes from '<a href=""http://www.vagrantbox.es"" rel=""noreferrer"">http://www.vagrantbox.es</a>'</li>
<li><p>solution suggested <a href=""http://ubuntuforums.org/showthread.php?t=2136473"" rel=""noreferrer"">here</a>:   </p>

<p>sudo rm -rf /var/lib/apt/lists/*<br>
sudo apt-get update<br>
sudo apt-get clean</p></li>
<li><p>Removed and readded the puppet labs packages</p></li>
</ul>

<p>I've also tried similar suggestions that I've found in the first few pages of google. I've even tried multiple computers, and multiple internet connections. </p>

<p>The fact that this has affected multiple ubuntu vm's on multiple internet connections makes me think there is something wrong with the ubuntu repo. </p>

<p>How else can I try to fix this issue? </p>

<p><strong>Update</strong> </p>

<p>I tried cleaning out '/var/lib/apt/lists/partial' and running apt-get clean then replaced the sources in /etc/sources/list by using the amazon mirrors <a href=""https://askubuntu.com/questions/37753/how-can-i-get-apt-to-use-a-mirror-close-to-me-or-choose-a-faster-mirror"">suggested here:</a> </p>

<p>I still get a similar error</p>

<pre><code>Fetched 18.9 MB in 10s (1,865 kB/s)                                                                                                                                                                            
W: Failed to fetch bzip2:/var/lib/apt/lists/partial/us-west-1.ec2.archive.ubuntu.com_ubuntu_dists_precise_main_binary-amd64_Packages  Hash Sum mismatch
W: Failed to fetch bzip2:/var/lib/apt/lists/partial/us-west-1.ec2.archive.ubuntu.com_ubuntu_dists_precise_universe_binary-amd64_Packages  Hash Sum mismatch
W: Failed to fetch bzip2:/var/lib/apt/lists/partial/us-west-1.ec2.archive.ubuntu.com_ubuntu_dists_precise_multiverse_binary-amd64_Packages  Hash Sum mismatch
W: Failed to fetch bzip2:/var/lib/apt/lists/partial/us-west-1.ec2.archive.ubuntu.com_ubuntu_dists_precise_main_binary-i386_Packages  Hash Sum mismatch
E: Some index files failed to download. They have been ignored, or old ones used instead.
</code></pre>

<p><strong>Update2</strong>  </p>

<p>I have 2 internet connections at home, both of them gave the same error. As soon as I took my laptop to my work internet connection, the problem went away. </p>

<p>I'm still curious to understand why my internet connection would make any difference. </p>

<p><strong>Update3</strong>  </p>

<p>See my answer below for an explanation. My internet filter was corrupting the download. </p>

<p>I'll rephrase the question since there are no answers yet. </p>

<p><strong>Is there a way to override Hash Sum mismatches in apt-get?</strong></p>
","<p>Figured it out. </p>

<p>My two computers are identical in every way except on my personal computer I installed an internet filter. </p>

<p>The k9 internet filter installed a kernel extension that apparently messes with the traffic. </p>

<p>As soon as I uninstalled the filter, the problem went away. </p>

<p>I'm still researching if there is any way to make <code>apt-get update</code> ignore Hash sum mismatches as a workaround. </p>
","116979"
"Run ./script.sh vs bash script.sh - permission denied","64083","","<p>When I try to run <code>./script.sh</code> I got <code>Permission denied</code> but when I run <code>bash script.sh</code> everything is fine. </p>

<p>What did I do wrong?</p>
","<h3>Incorrect POSIX permissions</h3>

<p>It means you don't have the execute permission bit set for <code>script.sh</code>. When running <code>bash script.sh</code>, you only need read permission for <code>script.sh</code>.  See <a href=""https://unix.stackexchange.com/questions/136547/what-is-the-difference-between-running-bash-script-sh-and-script-sh?rq=1"">What is the difference between running “bash script.sh” and “./script.sh”?</a> for more info.</p>

<p>You can verify this by running <code>ls -l script.sh</code>.</p>

<p>You may not even need to start a new Bash process. In many cases, you can simply run <code>source script.sh</code> or <code>. script.sh</code> to run the script commands in your current interactive shell. You would probably want to start a new Bash process if the script changes current directory or otherwise modifies the environment of the current process.</p>

<h3>Access Control Lists</h3>

<p>If the POSIX permission bits are set correctly, the Access Control List (ACL) may have been configured to prevent you or your group from executing the file.  E.g. the POSIX permissions would indicate that the test shell script is
executable.</p>

<pre><code>$ ls -l t.sh
-rwxrwxrwx+ 1 root root 22 May 14 15:30 t.sh
</code></pre>

<p>However, attempting to execute the file results in:</p>

<pre><code>$ ./t.sh
bash: ./t.sh: Permission denied
</code></pre>

<p>The <code>getfacl</code> command shows the reason why:</p>

<pre><code>$ getfacl t.sh
# file: t.sh
# owner: root
# group: root
user::rwx
group::r--
group:domain\040users:rw-
mask::rwx
other::rwx
</code></pre>

<p>In this case, my primary group is <code>domain users</code> which has had execute permissions revoked by restricting the ACL with <code>sudo setfacl -m 'g:domain\040users:rw-' t.sh</code>. This restriction can be lifted by either of the following commands:</p>

<pre><code>sudo setfacl -m 'g:domain\040users:rwx' t.sh
sudo setfacl -b t.sh
</code></pre>

<p>See:</p>

<ul>
<li><a href=""https://wiki.archlinux.org/index.php/Access_Control_Lists"" rel=""noreferrer"">Access Control Lists, Arch Linux Wiki</a></li>
<li><a href=""http://www.vanemery.com/Linux/ACL/linux-acl.html"" rel=""noreferrer"">Using ACLs with Fedora Core 2</a></li>
</ul>

<h3>Filesystem mounted with noexec option</h3>

<p>Finally, the reason in this specific case for not being able to run the script is that the filesystem the script resides on was mounted with the <code>noexec</code> option. This option overrides POSIX permissions to prevent any file on that filesystem from being executed.</p>

<p>This can be checked by running <code>mount</code> to list all mounted filesystems; the mount options are listed in parentheses in the entry corresponding to the filesystem, e.g.</p>

<pre><code>/dev/sda3 on /tmp type ext3 (rw,noexec)
</code></pre>

<p>You can either move the script to another mounted filesystem or remount the filesystem allowing execution:</p>

<pre><code>sudo mount -o remount,exec /dev/sda3 /tmp
</code></pre>

<p>Note: I’ve used <code>/tmp</code> as an example here since there are <a href=""https://serverfault.com/questions/72356/how-useful-is-mounting-tmp-noexec"">good security reasons</a> for keeping <code>/tmp</code> mounted with the <code>noexec,nodev,nosuid</code> set of options.</p>
","203372"
"Create and format exFAT partition from Linux","64044","","<p>Is it possible to create and format an exFAT partition from Linux? </p>
","<p>Yes, there is a project implementing exfat and the related utilities at <a href=""http://code.google.com/p/exfat/"">http://code.google.com/p/exfat/</a>.</p>

<p>To format a partition, use <code>mkexfatfs</code> / <code>mkfs.exfat</code> like with most filesystems, e.g.:</p>

<pre><code>mkfs.exfat /dev/sdX1
</code></pre>

<p>As for creating the partition in the first place, this is the same as for any other filesystem. Create a partition in your favourite partition manager. If you have a MBR partition type, set its type to NTFS (that is code <code>7h</code>).</p>

<p>Note, that some distributions only package the fuse module, so you may have to build it yourself.</p>
","61257"
"How to permanently change hostname in Fedora 21","64005","","<p>I've heard that changing the hostname in new versions of fedora is done with the <code>hostnamectl</code> command. In addition, I recently (and successfully) changed my hostname on Arch Linux with this method. However, when running:</p>

<pre class=""lang-bsh prettyprint-override""><code>[root@localhost ~]# hostnamectl set-hostname --static paragon.localdomain
[root@localhost ~]# hostnamectl set-hostname --transient paragon.localdomain
[root@localhost ~]# hostnamectl set-hostname --pretty paragon.localdomain
</code></pre>

<p>The changes are not preserved after a reboot (contrary to many people's claims that it does). What is wrong?</p>

<ul>
<li>I <em>really</em> don't want to edit <code>/etc/hostname</code> manually.</li>
</ul>

<p>I should also note that this is a <em>completely</em> stock fedora. I haven't even gotten around to installing my core apps yet.</p>
","<p>The command to set the hostname is definitely, <code>hostnamectl</code>.</p>

<pre><code>root ~ # hostnamectl set-hostname --static ""YOUR-HOSTNAME-HERE""
</code></pre>

<p>Here's an additional source that describes this functionality a bit more, titled: <a href=""https://ask.fedoraproject.org/en/question/37413/correctly-setting-the-hostname-fedora-20-on-amazon-ec2/"">Correctly setting the hostname - Fedora 20 on Amazon EC2</a>.</p>

<p>Additionally the man page for <code>hostnamectl</code>:</p>

<pre><code>HOSTNAMECTL(1)                    hostnamectl                   HOSTNAMECTL(1)

NAME
       hostnamectl - Control the system hostname

SYNOPSIS
       hostnamectl [OPTIONS...] {COMMAND}

DESCRIPTION
       hostnamectl may be used to query and change the system hostname and
       related settings.

       This tool distinguishes three different hostnames: the high-level
       ""pretty"" hostname which might include all kinds of special characters
       (e.g. ""Lennart's Laptop""), the static hostname which is used to
       initialize the kernel hostname at boot (e.g. ""lennarts-laptop""), and
       the transient hostname which is a default received from network
       configuration. If a static hostname is set, and is valid (something
       other than localhost), then the transient hostname is not used.

       Note that the pretty hostname has little restrictions on the characters
       used, while the static and transient hostnames are limited to the
       usually accepted characters of Internet domain names.

       The static hostname is stored in /etc/hostname, see hostname(5) for
       more information. The pretty hostname, chassis type, and icon name are
       stored in /etc/machine-info, see machine-info(5).

       Use systemd-firstboot(1) to initialize the system host name for mounted
       (but not booted) system images.
</code></pre>

<p>There is a bug in Fedora 21 where SELinux prevents hostnamectl access, found here, titled: <a href=""https://bugzilla.redhat.com/show_bug.cgi?id=1133368"">Bug 1133368 - SELinux is preventing systemd-hostnam from 'unlink' accesses on the file hostname</a>. </p>

<p>This bug seems to be related. There's an issue with the SELinux contexts not being applied properly to the file <code>/etc/hostname</code> upon installation. This manifests in the tool <code>hostnamectl</code> not being able to manipulate the file <code>/etc/hostname</code>. That same thread offered this workaround:</p>

<pre><code>$sudo restorecon -v /etc/hostname
</code></pre>

<p><strong>NOTE:</strong> That patches were applied to Anaconda (the installation tool) so that this issue should go away in the future for new users.</p>
","162881"
"How to integrate mv command after find command?","63987","","<p>I am searching for files which name which contain <code>AAA</code> within their path using following command:</p>

<pre><code>find path_A -name ""*AAA*""
</code></pre>

<p>Given the output showed by the above command, I want to move those files into another path, say <code>path_B</code>. Instead of moving those file one by one, can I  optimize the command by moving those files right after the find command?</p>
","<p>With GNU <a href=""http://linux.die.net/man/1/mv"">mv</a>:</p>

<pre><code>find path_A -name '*AAA*' -exec mv -t path_B {} +
</code></pre>

<p>That will use find's <code>-exec</code> option which replaces the <code>{}</code> with each find result in turn and runs the command you give it. As explained in <code>man find</code>:</p>

<pre><code>   -exec command ;
          Execute  command;  true  if 0 status is returned.  All following
          arguments to find are taken to be arguments to the command until
          an  argument  consisting of `;' is encountered.  
</code></pre>

<p>In this case, we are using the <code>+</code> version of <code>-exec</code> so that we run as few <code>mv</code> operations as possible:</p>

<pre><code>   -exec command {} +
          This  variant  of the -exec action runs the specified command on
          the selected files, but the command line is built  by  appending
          each  selected file name at the end; the total number of invoca‐
          tions of the command will  be  much  less  than  the  number  of
          matched  files.   The command line is built in much the same way
          that xargs builds its command lines.  Only one instance of  `{}'
          is  allowed  within the command.  The command is executed in the
          starting directory.
</code></pre>
","154819"
"How do I find how long ago a Linux system was installed?","63935","","<p>How can I find the time since a Linux system was first installed, provided that nobody has tried to hide it?</p>
","<pre><code>tune2fs -l /dev/sda1 **OR** /dev/sdb1*  | grep 'Filesystem created:'
</code></pre>

<p>This will tell you when the file system was created.</p>

<p>* = In the first column of <code>df /</code> you can find the exact partition to use.</p>
","9993"
"User's Login date and login time","63889","","<p>I want to fetch user's login time and login date, is there any command in Unix providing User's login date and User's login time ? this problem i want to perform in Shell-script where username is accepting from the end-user and after checking the availability of user, i would like to fetch that user's login time and login date in different variable and then display using 'echo' command.</p>
","<p>For past logins:</p>

<pre><code>last ""$USER_NAME""
</code></pre>

<p>Also, the command <code>who</code> lists current logins.</p>

<p>If you're looking for the date of the user's last login, some systems provide it directly, for example <code>lastlog -u ""$USER_NAME""</code> on Linux or <code>lastlogin ""$USER_NAME""</code> on FreeBSD. It's also available in the output of <code>finger</code>, but not in an easy-to-parse form. In any case, it's available in the output of <code>last</code> (on many unix variants, <code>last -n 1 ""$USER_NAME""</code> shows the last login; otherwise you can do <code>last ""$USER_NAME"" | head -n 1</code>). Note that last login may not correspond to the last logout (e.g. a user remained connected from one origin for a long time and made a quick network login recently).</p>
","2008"
"How to move 100 files from a folder containing thousands?","63854","","<p>I have a directory with thousands of files. How can I move 100 of the files (any files will do) to another location.</p>
","<pre><code>for file in $(ls -p | grep -v / | tail -100)
do
mv $file /other/location
done
</code></pre>

<p>That assumes file names don't contain blanks, newline (assuming the default value of <code>$IFS</code>), wildcard characters (<code>?</code>, <code>*</code>, <code>[</code>) or start with <code>-</code>.</p>
","12977"
"rsync to copy files which has changed","63839","","<p>I need to copy some files from a particular directory to a different location on a daily basis. I want to copy the changes only, so planning to use <code>rsync</code>. These files follows the following naming convention <code>mysql-bin.*</code> </p>

<p>My command looks as follows</p>

<pre><code># rsync --update -raz --progress /var/lib/mysql/mysql-bin.*  /dbdata/binarylog/
</code></pre>

<p>My confusion is since I am planning to copy only few files from a directory rather than full directory contents , I have used <code>*</code>  to copy only required files. 
Just want to know whether my command is correct to achieve the same. </p>
","<p>It looks OK.  Although why are you using the <code>-z</code> option to compress the transfer? This option is normally used when you are copying to a remote rsync server over a slow network. In this instance it will compress and instantly decompress the files which will only increase your CPU uage with no benefit.</p>

<p>The <code>-a</code> (archive) option implies the <code>-r</code> (recursive) option so there is no need to explicitly specify that on the command line.</p>

<p>You can use the <code>-n</code> option (or <code>--dry-run</code>) to check your command.  It will show what it would do without actually copying any files. To actually see what happens you should also use the <code>-v</code> option (or <code>--verbose</code>).</p>

<p>Therefore:</p>

<pre><code>rsync -uanv /var/lib/mysql/mysql-bin.*  /dbdata/binarylog/
</code></pre>

<p>and once you're happy that the files are listed correctly on the dry-run, remove the <code>nv</code>:</p>

<pre><code>rsync -ua --progress /var/lib/mysql/mysql-bin.*  /dbdata/binarylog/
</code></pre>
","146286"
"Server does not accept public key for ssh login without password","63834","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/36540/why-am-i-still-getting-a-password-prompt-with-ssh-with-public-key-authentication"">Why am I still getting a password prompt with ssh with public key authentication?</a>  </p>
</blockquote>



<p>I have ssh access to two sever. One old one and one new one. For the old one I use the tutorial <a href=""http://linuxproblem.org/art_9.html"" rel=""nofollow noreferrer"">SSH login without password</a> to login without typing the password every time. </p>

<p>For the new machine I followed the tutorial again, but this time it is not working. I looked at the debug output from ssh (<code>-v</code> option) and it seems to me that the new server does not <code>accept</code> my public key. But I checked and bot <code>authorized_keys</code> are the same, I even used <code>md5sum</code>.</p>

<p>What could be the problem and how could I fix this? </p>

<p><strong>Debug output for old server where it does work (snippet):</strong></p>

<pre><code>debug1: Authentications that can continue: publickey,password
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /home/NICK/.ssh/id_rsa
debug1: Server accepts key: pkalg ssh-rsa blen 277
</code></pre>

<p><strong>Debug output for new server where it does not work (snippet):</strong></p>

<pre><code>debug1: Authentications that can continue: publickey,password
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /home/NICK/.ssh/id_rsa
debug1: Authentications that can continue: publickey,password
debug1: Trying private key: /home/NICK/.ssh/id_dsa
</code></pre>

<p>[<strong>UPDATE] Ownership of authorized_keys on remote</strong></p>

<pre><code>NICK@server-new:~/.ssh$ ls -l
total 4
-rwx------ 1 NICK NICK 404 2012-08-08 16:11 authorized_keys
</code></pre>

<hr>

<p><strong>Complete debug output for the not working server:</strong></p>

<pre><code>OpenSSH_5.9p1 Debian-5ubuntu1, OpenSSL 1.0.1 14 Mar 2012
debug1: Reading configuration data /home/NICK/.ssh/config
debug1: /home/NICK/.ssh/config line 1: Applying options for foo2
debug1: Reading configuration data /etc/ssh/ssh_config
debug1: /etc/ssh/ssh_config line 19: Applying options for *
debug1: Connecting to foo-serv2.cs.bar.it [XXX.XXX.XXX.XXX] port 22.
debug1: Connection established.
debug1: identity file /home/NICK/.ssh/id_rsa type 1
debug1: Checking blacklist file /usr/share/ssh/blacklist.RSA-2048
debug1: Checking blacklist file /etc/ssh/blacklist.RSA-2048
debug1: identity file /home/NICK/.ssh/id_rsa-cert type -1
debug1: identity file /home/NICK/.ssh/id_dsa type -1
debug1: identity file /home/NICK/.ssh/id_dsa-cert type -1
debug1: identity file /home/NICK/.ssh/id_ecdsa type -1
debug1: identity file /home/NICK/.ssh/id_ecdsa-cert type -1
debug1: Remote protocol version 2.0, remote software version OpenSSH_5.5p1 Debian-4ubuntu6
debug1: match: OpenSSH_5.5p1 Debian-4ubuntu6 pat OpenSSH*
debug1: Enabling compatibility mode for protocol 2.0
debug1: Local version string SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1
debug1: SSH2_MSG_KEXINIT sent
debug1: SSH2_MSG_KEXINIT received
debug1: kex: server-&gt;client aes128-ctr hmac-md5 none
debug1: kex: client-&gt;server aes128-ctr hmac-md5 none
debug1: SSH2_MSG_KEX_DH_GEX_REQUEST(1024&lt;1024&lt;8192) sent
debug1: expecting SSH2_MSG_KEX_DH_GEX_GROUP
debug1: SSH2_MSG_KEX_DH_GEX_INIT sent
debug1: expecting SSH2_MSG_KEX_DH_GEX_REPLY
debug1: Server host key: RSA XXX
debug1: Host 'foo-serv2.cs.bar.it' is known and matches the RSA host key.
debug1: Found key in /home/NICK/.ssh/known_hosts:34
debug1: ssh_rsa_verify: signature correct
debug1: SSH2_MSG_NEWKEYS sent
debug1: expecting SSH2_MSG_NEWKEYS
debug1: SSH2_MSG_NEWKEYS received
debug1: Roaming not allowed by server
debug1: SSH2_MSG_SERVICE_REQUEST sent
debug1: SSH2_MSG_SERVICE_ACCEPT received
debug1: Authentications that can continue: publickey,password
debug1: Next authentication method: publickey
debug1: Offering RSA public key: /home/NICK/.ssh/id_rsa
debug1: Authentications that can continue: publickey,password
debug1: Trying private key: /home/NICK/.ssh/id_dsa
debug1: Trying private key: /home/NICK/.ssh/id_ecdsa
debug1: Next authentication method: password
</code></pre>
","<p>Did you make sure that the ownership and mode of your ~/.ssh directory on the remote side is correct? It should be owned by you, and have 0700 permissions, i.e. <code>chmod 700 ~/.ssh</code>. Also <code>chmod go-w ~</code> as this is checked also - because anyone with write permission on your home directory can change the permissions of the .ssh directory.</p>
","45065"
"Kill only one Java process","63798","","<p>I usually run few Java applications, one for server running locally and other for some IDE like NetBeans. And from time to time, after lots of redeployments, my server get stuck on OutOfMemoryException so I need to kill Java process in order to reboot. </p>

<p>So I do <code>pkill -9 java</code> but this also kills my running IDE which I don't want to. So how do I kill only application linked to running server and not the other ones?I assume that they all are running under same process but there has to be some way how to distuingish them.</p>
","<p>For killing a process that is associated with multiple processes, you need to kill that by using process id associated with that process. </p>

<p>To get the process id of that java process run</p>

<pre><code>ps -A |grep java
</code></pre>

<p>output of this command will give the list of java processes running on your system. Note down Process ID (PID) of that process whom you want to kill and run</p>

<pre><code>kill -9 PID
</code></pre>
","64624"
"Argument list too long for ls","63795","","<p>I get the following error when trying to <code>ls *.txt | wc -l</code> a directory that contains many files:</p>

<pre><code>-bash: /bin/ls: Argument list too long
</code></pre>

<p>Does the threshold of this ""Argument list"" dependent on distro or computer's spec?  Usually, I'd pipe the result of such big result to some other commands (<code>wc -l</code> for example), so I'm not concerned with limits of the terminal.</p>
","<p>Your error message <em>argument list too long</em> comes from the <strong>*</strong> of <code>ls *.txt</code>.</p>

<p>This limits is a safety for both binary programs and your Kernel. You'll see on <a href=""http://www.in-ulm.de/~mascheck/various/argmax/"">this page</a> more information about it, and how it's used and computed.</p>

<p>There is no such limit on pipe size. So you can simply issue this command:</p>

<pre><code>find -type f -name '*.txt'  | wc -l
</code></pre>

<p>NB: On modern Linux, weird characters in filenames (like newlines) will be escaped with tools like <code>ls</code> or <code>find</code>, but still displayed from <strong>*</strong>. If you are on an old Unix, you'll need this command</p>

<pre><code>find -type f -name '*.txt' -exec echo \;  | wc -l
</code></pre>

<p>NB2: I was wondering how one can create a file with a newline in its name. It's not that hard, once you know the trick:</p>

<pre><code>touch ""hello
world""
</code></pre>
","38960"
"How to create a TCP listener?","63792","","<p><strong>Introduction:</strong> I have created a bash function that is able to check whether a port is available and increments it by 1 if false until a certain maximum port number. E.g., if port 500 is unavailable then the availability of 501 will be checked until 550.</p>

<p><strong>Aim:</strong> In order to test this bash function I need to create a range of ports that are in LISTEN state.</p>

<p><strong>Attempts:</strong> On Windows it is possible to create a LISTEN port using <a href=""http://opensysblog.directorioc.net/2014/01/powershell-listen-on-tcp-port.html"">these PowerShell commands</a>:</p>

<pre><code>PS C:\Users\u&gt; netstat -nat | grep 1234
PS C:\Users\u&gt; $listener = [System.Net.Sockets.TcpListener]1234
PS C:\Users\u&gt; $listener.Start();
PS C:\Users\u&gt; netstat -nat | grep 1234
TCP    0.0.0.0:1234           0.0.0.0:0              LISTENING       InHost
PS C:\Users\u&gt; $listener.Stop();
PS C:\Users\u&gt; netstat -nat | grep 1234
PS C:\Users\u&gt;
</code></pre>

<p>Based on this I was trying to think about a command that could do the same on CentOS, but I do not know why and I started to Google without finding a solution that solves this issue.</p>

<p><strong>Expected answer</strong>: I will accept and upvote the answer that contains a command that is able to create a LISTEN port and once the command has been run the port should stay in LISTEN state, i.e.:</p>

<pre><code>[user@host ~]$ ss -nat | grep 500
LISTEN     0      128                       *:500                       *:*
</code></pre>
","<p>You could use <code>nc -l</code> as a method to do what you are looking for. Some implementations of <code>nc</code> have a <code>-L</code> option which allows the connections to persist. </p>

<p>If you only need them for a little while you could open this command in a <code>for</code> loop and have a bunch of ports opened that way.</p>

<p>If you need these opened longer you can use one of the super servers to create a daemon.</p>
","214473"
"Enable root login from GUI","63763","","<p>I just installed Mint 16 and I see that root user is not available at login screen. I log in from normal user and went to ""Login Window"" option and there I set ""Allow root login"". Then I restarted the PC and still I don't see root user in login window.</p>

<p>I also did the below but it also didn't work.</p>

<pre><code>sudo passwd root
sudo sh -c 'echo ""greeter-show-manual-login=true"" &gt;&gt; /etc/lightdm/lightdm.conf'
</code></pre>
","<p>Linux Mint 16 uses the Mint-X theme by default which only displays the password box for chosen non-root users.  In order to enable the <code>User</code> entry field (from which you will be able to specify root) do this.  From <code>Menu</code> ==> <code>Administration</code> ==> <code>Login Window</code> ==> <code>Theme</code> choose <code>Clouds</code> and logout.</p>
","106710"
"How to know when and which user logged into the system under Mac OS X? Last is not enough!","63761","","<p>In Mac OS X, if I don't touch it for a while, it will lock the screen and one must use password to unlock it, but this kind of log in is not recorded by last command. I want to know if anybody tried to break into my MacBook when I am not in front of it. Is there any way I can log such attempts?</p>
","<p>If you suspect that someone has correctly guessed your password and got in, you can check this via the <code>Console</code>. To access <code>Console</code> press ⌘+space and type 'console' in the Spotlight box that appears. Click return.</p>

<p>Click on 'Diagnostic and Usage Messages' on the left panel. At the time of the correct login attempt you see something like this:</p>

<p><img src=""https://i.stack.imgur.com/BxraM.png"" alt=""Console""></p>

<p>Note: 'screen locked, user typed correct password'.</p>

<p>Now if someone tried, yet failed, you'd see something like this under <code>system.log</code> (also accessible via <code>Console</code>):</p>

<p><img src=""https://i.stack.imgur.com/kqd5I.png"" alt=""fail""></p>

<p>I hope that's of some assistance to you.</p>
","14128"
"What is difference between User space and Kernel space?","63700","","<p>Is Kernel space used when Kernel is executing on the behalf of the user program i.e. System Call? Or is it the address space for all the Kernel threads (for example scheduler)?</p>

<p>If it is the first one, than does it mean that normal user program cannot have more than 3GB of memory (if the division is 3GB + 1GB)? Also, in that case how can kernel use High Memory, because to what virtual memory address will the pages from high memory be mapped to, as 1GB of kernel space will be logically mapped?</p>
","<blockquote>
  <p>Is Kernel space used when Kernel is executing on the behalf of the user program i.e. System Call? Or is it the address space for all the Kernel threads (for example scheduler)?</p>
</blockquote>

<p>Yes and yes.</p>

<p>Before we go any further, we should state this about memory.</p>

<p><strong>Memory get's divided into two distinct areas:</strong></p>

<ul>
<li><strong>The user space</strong>, which is a set of locations where normal user processes run (i.e everything other than the kernel). The role of the kernel is to manage applications running in this space from messing with each other, and the machine.</li>
<li><strong>The kernel space</strong>, which is the location where the code of the kernel is stored, and executes under.</li>
</ul>

<p>Processes running under the user space have access only to a limited part of memory, whereas the kernel has access to all of the memory. Processes running in user space also <em>don't</em> have access to the kernel space. User space processes can <em>only access a small part of the kernel</em> via an interface exposed by the kernel - <strong>the system calls</strong>. If a process performs a system call, a software interrupt is sent to the kernel, which then dispatches the appropriate interrupt handler and continues its work after the handler has finished.</p>

<p>Kernel space code has the property to run in ""kernel mode"", which (in your typical desktop -x86- computer) is what you <strong>call code that executes under ring 0</strong>. <em>Typically in x86 architecture, there are 4 rings of protection</em>. Ring 0 (kernel mode), Ring 1 (may be used by virtual machine hypervisors or drivers), Ring 2 (may be used by drivers, I am not so sure about that though). Ring 3 is what typical applications run under. It is the least privileged ring, and applications running on it have access to a subset of the processor's instructions. Ring 0 (kernel space) is the most privileged ring, and has access to all of the machine's instructions. For example to this, a ""plain"" application (like a browser) can not use x86 assembly instructions <code>lgdt</code> to load the global descriptor table or <code>hlt</code> to halt a processor.</p>

<blockquote>
  <p>If it is the first one, than does it mean that normal user program cannot have more than 3GB of memory (if the division is 3GB + 1GB)? Also, in that case how can kernel use High Memory, because to what virtual memory address will the pages from high memory be mapped to, as 1GB of kernel space will be logically mapped?</p>
</blockquote>

<p>For an answer to this, please refer to the excellent answer by <a href=""https://unix.stackexchange.com/users/3069/wag"">wag</a> <a href=""https://unix.stackexchange.com/a/5151/20260"">here</a></p>
","87629"
"ssh Connection refused: how to troubleshoot?","63644","","<p>I'm trying:</p>

<pre><code>$ ssh eric@myserver
</code></pre>

<p>where <code>myserver</code> is a machine in the intranet. I can ping <code>myserver</code> or respond to HTTP on port 8080, etc, but when I try <code>ssh</code>, I get</p>

<pre><code>ssh: connect to host myserver port 22: Connection refused
</code></pre>

<p>I'm somewhat new to linux, and I don't know how to troubleshoot this. I'm using Ubuntu 10 if that matters.</p>

<p><strong>Edit:</strong> trying <code>ps -ax</code>, as suggested gives:</p>

<pre><code>eric@Isaiah:~$ ps -ax | grep ssh
Warning: bad ps syntax, perhaps a bogus '-'? See http://procps.sf.net/faq.html
 1641 ?        Ss     0:04 /usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session gnome-session
18376 pts/3    S+     0:00 grep --color=auto ssh
</code></pre>
","<p>You don't have an SSH daemon running. If you look at the output from the <code>ps ax</code> command, you see that the only two processes with 'ssh' in the description are <code>ssh-agent</code> (which does something entirely different from <code>sshd</code>) and the <code>grep ssh</code> process that you're using to filter the output of <code>ps</code>.</p>

<p>Depending on what distribution installed, you may need to install or run the <code>ssh</code> server, usually called <code>openssh-server</code> or <code>sshd</code> depending on your package manager.</p>
","21304"
"How to use regex with AWK for string replacement?","63624","","<p>Suppose there is some text from a file:</p>

<pre><code>(bookmarks
(""Chapter 1 Introduction 1"" ""#1""
(""1.1 Problem Statement and Basic Definitions 23"" ""#2"")
(""Exercises 31"" ""#30"")
(""Notes and References 42"" ""#34""))
)
</code></pre>

<p>I want to add 11 to each number followed by a <code>""</code> in each line if there is one, ie </p>

<pre><code>(bookmarks
(""Chapter 1 Introduction 12"" ""#12""
(""1.1 Problem Statement and Basic Definitions 34"" ""#13"")
(""Exercises 42"" ""#41"")
(""Notes and References 53"" ""#45""))
)
</code></pre>

<p>Here is my solution by using GNU AWK and regex:</p>

<pre><code>awk -F'#' 'NF&gt;1{gsub(/""(\d+)\""""/, ""\1+11\"""")}'
</code></pre>

<p>i.e., I want to replace <code>(\d+)\""</code> with  <code>\1+10\""</code>, where <code>\1</code> is the group representing <code>(\d+)</code>. But it doesn't work. How can I make it work?</p>

<p>If gawk is not the best solution, what else can be used?</p>
","<p>Try this (gawk is needed).</p>

<pre><code>awk '{a=gensub(/.*#([0-9]+)(\"").*/,""\\1"",""g"",$0);if(a~/[0-9]+/) {gsub(/[0-9]+\""/,a+11""\"""",$0);}print $0}' YourFile
</code></pre>

<p><strong>Test</strong> with your example:</p>

<pre><code>kent$  echo '(bookmarks
(""Chapter 1 Introduction 1"" ""#1""
(""1.1 Problem Statement and Basic Definitions 2"" ""#2"")
(""Exercises 30"" ""#30"")
(""Notes and References 34"" ""#34""))
)
'|awk '{a=gensub(/.*#([0-9]+)(\"").*/,""\\1"",""g"",$0);if(a~/[0-9]+/) {gsub(/[0-9]+\""/,a+11""\"""",$0);}print $0}'   
(bookmarks
(""Chapter 1 Introduction 12"" ""#12""
(""1.1 Problem Statement and Basic Definitions 13"" ""#13"")
(""Exercises 41"" ""#41"")
(""Notes and References 45"" ""#45""))
)
</code></pre>

<p>Note that this command won't work if the two numbers (e.g. 1"" and ""#1"") are different. or there are more numbers in same line with this pattern (e.g. 23"" ...32""...""#123"") in one line.</p>

<p><br>
<strong>UPDATE</strong></p>

<p>Since @Tim (OP) said the number followed by <code>""</code> in same line could be different, I did some changes on my previous solution, and made it work for your new example.</p>

<p>BTW, from the example I feel that it could be a table of content structure, so I don't see how the two numbers could be different. First would be the printed page number, and 2nd with # would be the page index. Am I right?</p>

<p>Anyway, you know your requirement best. Now the new solution, still with gawk (I break the command into lines to make it easier to read):</p>

<pre><code>awk 'BEGIN{FS=OFS=""\"" \""#""}{if(NF&lt;2){print;next;}
        a=gensub(/.* ([0-9]+)$/,""\\1"",""g"",$1);
        b=gensub(/([0-9]+)\""/,""\\1"",""g"",$2); 
        gsub(/[0-9]+$/,a+11,$1);
        gsub(/^[0-9]+/,b+11,$2);
        print $1,$2
}' yourFile
</code></pre>

<p><strong>test</strong> with your <strong><em>new</em></strong> example:</p>

<pre><code>kent$  echo '(bookmarks
(""Chapter 1 Introduction 1"" ""#1""
(""1.1 Problem Statement and Basic Definitions 23"" ""#2"")
(""Exercises 31"" ""#30"")
(""Notes and References 42"" ""#34""))
)
'|awk 'BEGIN{FS=OFS=""\"" \""#""}{if(NF&lt;2){print;next;}
        a=gensub(/.* ([0-9]+)$/,""\\1"",""g"",$1);
        b=gensub(/([0-9]+)\""/,""\\1"",""g"",$2); 
        gsub(/[0-9]+$/,a+11,$1);
        gsub(/^[0-9]+/,b+11,$2);
        print $1,$2
}'                        
(bookmarks
(""Chapter 1 Introduction 12"" ""#12""
(""1.1 Problem Statement and Basic Definitions 34"" ""#13"")
(""Exercises 42"" ""#41"")
(""Notes and References 53"" ""#45""))
)
</code></pre>

<p><br>
<strong>EDIT2</strong> based on @Tim 's comment</p>

<blockquote>
  <p>(1) Does FS=OFS=""\"" \""#"" mean the separator of field in both input and
  output is double quote, space, double quote and #? Why specify double
  quote twice?</p>
</blockquote>

<p>You are right for the separator in both input and output part. It defined separator as:</p>

<pre><code>"" ""#
</code></pre>

<p>There are two double quotes, because it is easier to catch the two numbers you want (based on your example input). </p>

<blockquote>
  <p>(2) In /.* ([0-9]+)$/, does $ mean the end of the string?</p>
</blockquote>

<p>Exactly!</p>

<blockquote>
  <p>(3) In the third argument of gensub(), what is the difference between
  ""g"" and ""G""? there is no difference between G and g. Check this out:</p>
</blockquote>

<pre><code>gensub(regexp, replacement, how [, target]) #
    Search the target string target for matches of the regular expression regexp. 
    If ""how"" is a string beginning with ‘g’ or ‘G’ (short for “global”), then 
        replace all matches of regexp with replacement.
</code></pre>

<p>This is from <a href=""http://www.gnu.org/s/gawk/manual/html_node/String-Functions.html"" rel=""nofollow"">http://www.gnu.org/s/gawk/manual/html_node/String-Functions.html</a>. you can read to get detailed usage of gensub.</p>
","25123"
"Quoting within $(command substitution) in Bash","63537","","<p>
In my Bash environment I use variables containing spaces, and I use these variables within command substitution. Unfortunately I cannot find the answer on SE.</p>

<p>What is the correct way to quote my variables? And how should I do it if these are nested?</p>

<pre class=""lang-sh prettyprint-override""><code>DIRNAME=$(dirname ""$FILE"")
</code></pre>

<p>or do I quote outside the substitution?</p>

<pre class=""lang-sh prettyprint-override""><code>DIRNAME=""$(dirname $FILE)""
</code></pre>

<p>or both?</p>

<pre class=""lang-sh prettyprint-override""><code>DIRNAME=""$(dirname ""$FILE"")""
</code></pre>

<p>or do I use back-ticks?</p>

<pre class=""lang-sh prettyprint-override""><code>DIRNAME=`dirname ""$FILE""`
</code></pre>

<p>What is the right way to do this? And how can I easily check if the quotes are set right?</p>
","<p>
In order from worst to best:</p>

<ul>
<li><code>DIRNAME=""$(dirname $FILE)""</code> <a href=""http://mywiki.wooledge.org/Quotes"">will not do what you want</a> if <code>$FILE</code> contains whitespace or globbing characters <code>\[?*</code>.</li>
<li><code>DIRNAME=`dirname ""$FILE""`</code> is technically correct, but <a href=""http://mywiki.wooledge.org/BashFAQ/082"">backticks are not recommended for command expansion</a> because of the extra complexity when nesting them.</li>
<li><code>DIRNAME=$(dirname ""$FILE"")</code> is correct, but <a href=""http://mywiki.wooledge.org/BashPitfalls#for_i_in_.24.28ls_.2A.mp3.29"">only because this is an assignment</a>. If you use the command substitution in any other context, such as <code>export DIRNAME=$(dirname ""$FILE"")</code> or <code>du $(dirname ""$FILE"")</code>, the lack of quotes will cause trouble if the result of the expansion contain whitespace or globbing characters.</li>
<li><code>DIRNAME=""$(dirname ""$FILE"")""</code> is the recommended way. You can replace <code>DIRNAME=</code> with a command and a space without changing anything else, and <code>dirname</code> receives the correct string.</li>
</ul>

<p>To improve even further:</p>

<ul>
<li><code>DIRNAME=""$(dirname -- ""$FILE"")""</code> works if <code>$FILE</code> starts with a dash.</li>
<li><code>DIRNAME=""$(dirname -- ""$FILE""; printf x)"" &amp;&amp; DIRNAME=""${DIRNAME%?x}""</code> works even if <code>$FILE</code> ends with a newline, since <code>$()</code> chops off newlines at the end of output and <code>dirname</code> outputs a newline after the result. Sheesh <code>dirname</code>, why you gotta be different?</li>
</ul>

<p>You can nest command expansions as much as you like. With <code>$()</code> you always create a new quoting context, so you can do things like this:</p>

<pre class=""lang-sh prettyprint-override""><code>foo ""$(bar ""$(baz ""$(ban ""bla"")"")"")""
</code></pre>

<p>You do <em>not</em> want to try that with backticks.</p>
","118438"
"How to use SFTP on a system that requires sudo for root access & ssh key based authentication?","63398","","<p>I want to be able to use SFTP to edit files that require root permissions.</p>

<p>I'm using SSH Key based authentication - rsa key on smart card.</p>

<p>If the system requires sudo to perform root level commands, How do I get around this?</p>

<p>Can I create a way of bypassing sudo for SFTP only?</p>

<p>Is there a way to keep sudo &amp; key authentication.</p>

<p>I'm using windows to connect to Ubuntu.  I need this to work with Mac connecting to Ubuntu as well.</p>

<p>I understand how to do SSH Tunneling to admin the system services.   Currently, I use root user login directly, but password login is disabled. I didn't understand how to use sudo and SFTP at same time.  It seems to be a best practice to require login as a non-root user and then require use of sudo since the logs will record who was given escalated privileges for each command.</p>

<p>Should I concern myself with this when using Key based authentication or is this a trivial difference in security/logging?  It seems like Key based authentication records user's serial number in the logs, and you can have multiple keys for the root user to identify each user.  This seems to be the same effect as using sudo to me.  Am I wrong?</p>
","<p>SFTP is a command access to file operations, with the restrictions from the account you use. You must use ssh for make more administrative operations, making impossible use sudo and SFTP at same time. If you need access to the entire disk without restriction using SFTP, do it using the root account. Anyway you can make a login with root on sftp and ssh at same time, of course, using two different sessions.</p>

<p>The security keys improve the security and make more easy the logging, not requiring keyboard input. Only helps to make login, you can had several passwords for every account user and had the same effect.</p>

<p>EDIT: I forgot: you can create another account with the same effect than root if you assign the user id to 0, but not had any sense, being dangerous in the same way. Could give some obfuscation if somebody try to login like root, but apart of that, not had much sense.</p>
","111034"
"can not telnet to a server connection refuse","63395","","<p>I have two server with these specifications:</p>

<ol>
<li><p>IP address: 192.168.1.94</p></li>
<li><p>IP address: 192.168.1.221</p></li>
</ol>

<p>I want to <code>telnet</code> to the first one from the second one (to port 5029) but I get this error:</p>

<pre><code>root@debian:~# telnet 192.168.1.94 5029
Trying 192.168.1.94...
telnet: Unable to connect to remote host: Connection refused
</code></pre>

<p>I can telnet from 192.168.1.94 to itself. this means that a program is listening to 5029 port:</p>

<pre><code>[root@myelastix ~]# telnet localhost 5029
Trying 127.0.0.1...
Connected to myelastix.mohaymen.co (127.0.0.1).
Escape character is '^]'.
</code></pre>

<p>I thought may be firewall is blocking connection in 192.168.1.94 server but these are firewall rules:</p>

<pre><code>[root@myelastix ~]# iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
</code></pre>

<p>besides I stopped iptables also but nothing happens again:</p>

<pre><code>[root@myelastix ~]# service iptables stop
Flushing firewall rules:                                   [  OK  ]
Setting chains to policy ACCEPT: filter                    [  OK  ]
Unloading iptables modules:                                [  OK  ]
</code></pre>

<p>what is the problem?</p>
","<p>Can you do this?</p>

<pre><code>[root@myelastix ~]# telnet 192.168.1.94 5029
</code></pre>

<p>If not, make sure <code>telnetd</code> is configured to listen on an external interface.</p>

<pre><code>netstat -tulpn | grep :5029
</code></pre>
","167075"
"List packages on an apt based system by installation date","63270","","<p><strong>How can I list installed packages by installation date?</strong></p>

<p>I need to do this on debian/ubuntu. Answers for other distributions would be nice as well.</p>

<p>I installed a lot of stuff to compile a certain piece of code, and I want to get a list of the packages that I had to install.</p>
","<p>RPM-based distributions like Red Hat are easy:</p>

<pre><code>rpm -qa --last
</code></pre>

<p>On Debian and other dpkg-based distributions, your specific problem is easy too:</p>

<pre><code>grep install /var/log/dpkg.log
</code></pre>

<p>Unless the log file has been rotated, in which case you should try:</p>

<pre><code>grep install /var/log/dpkg.log /var/log/dpkg.log.1
</code></pre>

<p>In general, <code>dpkg</code> and <code>apt</code> don't seem to track the installation date, going by the lack of any such field in the <code>dpkg-query</code> man page.</p>

<p>And eventually old <code>/var/log/dpkg.log.*</code> files will be deleted by log rotation, so that way isn't guaranteed to give you the entire history of your system.</p>

<p>One suggestion that appears a few times (e.g. <a href=""http://www.debianadmin.com/debianubuntu-package-management-using-dpkg.html/comment-page-1#comment-2391"">this thread</a>) is to look at the <code>/var/lib/dpkg/info</code> directory.
The files there suggest you might try something like:</p>

<pre><code>ls -t /var/lib/dpkg/info/*.list | sed -e 's/\.list$//' | head -n 50
</code></pre>

<hr>

<p>To answer your question about selections, here's a first pass.</p>

<p><em>build list of packages by dates</em></p>

<pre><code>$ find /var/lib/dpkg/info -name ""*.list"" -exec stat -c $'%n\t%y' {} \; | \
    sed -e 's,/var/lib/dpkg/info/,,' -e 's,\.list\t,\t,' | \
    sort &gt; ~/dpkglist.dates
</code></pre>

<p><em>build list of installed packages</em></p>

<pre><code>$ dpkg --get-selections | sed -ne '/\tinstall$/{s/[[:space:]].*//;p}' | \
    sort &gt; ~/dpkglist.selections
</code></pre>

<p><em>join the 2 lists</em></p>

<pre><code>$ join -1 1 -2 1 -t $'\t' ~/dpkglist.selections ~/dpkglist.dates \
    &gt; ~/dpkglist.selectiondates
</code></pre>

<p>For some reason it's not printing very many differences for me, so there might be a bug or an invalid assumption about what <code>--get-selections</code> means.</p>

<p>You can obviously limit the packages either by using <code>find . -mtime -&lt;days&gt;</code> or <code>head -n &lt;lines&gt;</code>, and change the output format as you like, e.g.</p>

<pre><code>$ find /var/lib/dpkg/info -name ""*.list"" -mtime -4 | \
    sed -e 's,/var/lib/dpkg/info/,,' -e 's,\.list$,,' | \
    sort &gt; ~/dpkglist.recent

$ join -1 1 -2 1 -t $'\t' ~/dpkglist.selections ~/dpkglist.recent \
    &gt; ~/dpkglist.recentselections
</code></pre>

<p>to list only the selections that were installed (changed?) in the past 4 days.</p>

<p>You could probably also remove the <code>sort</code> commands after verifying the sort order used by <code>dpkg --get-selections</code> and make the <code>find</code> command more efficient.</p>
","12579"
"Get common name (CN) from SSL certificate?","63193","","<p>I have a SSL CRT file in PEM format. Is there a way that I can extract the common name (CN) from the certificate from the command line?</p>
","<p>If you have <code>openssl</code> installed you can run:</p>

<pre><code>openssl x509 -noout -subject -in server.pem
</code></pre>
","103464"
"How to speed my too-slow ssh login?","63159","","<p>Running <code>ssh user@hostname</code> takes ~30s. Here's the scenario:</p>

<ul>
<li>this is a VM on the local LAN</li>
<li>Windows and Mac machines get instant login</li>
<li>am using Debian and I could reproduce with an Ubuntu machine</li>
<li>someone using Ubuntu says that logging into my machine (local LAN) is also instant</li>
<li>using hostname IP address takes about half as much time (~15s)</li>
</ul>

<p>[<strong>update</strong>]</p>

<p>Using <code>ssh -vvv user@hostname</code>, here's where it waits the most:</p>

<pre><code>debug3: authmethod_lookup gssapi-with-mic
debug3: remaining preferred: publickey,keyboard-interactive,password
debug3: authmethod_is_enabled gssapi-with-mic
debug1: Next authentication method: gssapi-with-mic
</code></pre>

<p>And then it waits a bit here:</p>

<pre><code>debug1: Unspecified GSS failure.  Minor code may provide more information
Credentials cache file '/tmp/krb5cc_1000' not found

debug1: Unspecified GSS failure.  Minor code may provide more information
Credentials cache file '/tmp/krb5cc_1000' not found
</code></pre>
","<p>Edit your ""<strong>/etc/ssh/ssh_config</strong>"" and comment out these lines:</p>

<pre><code>GSSAPIAuthentication yes
GSSAPIDelegateCredentials no
</code></pre>
","6227"
"Can less retain colored output?","63128","","<p>Can I get less not to monochrome its output?</p>

<p>E.g., the output from <code>git diff</code> is colored, but <code>git diff | less</code> is not.</p>
","<p>Use:</p>

<pre><code>git diff --color=always | less -r
</code></pre>

<p><code>--color=always</code> is there to tell <code>git</code> to output color codes even if the output is a pipe (not a tty). And <code>-r</code> is there to tell <code>less</code> to interpret those color codes and other escape sequences. Use <code>-R</code> for ANSI color codes only.</p>
","19320"
"Different ways to execute a shell script","63104","","<p>There a several ways to execute a script, the ones I know are:</p>

<pre><code>/path/to/script # using the path (absolute or relative)
. script        # using the . (dot)
source script   # using the `source` command
</code></pre>

<p>Are the more of this? What are the differences between them? Are there situations that I must use one and not another?</p>
","<p>Another way is by calling the interpreter and passing the path to the script to it:</p>

<pre><code>/bin/sh /path/to/script
</code></pre>

<p>The dot and source are equivalent. (EDIT: no, they're not: as KeithB points out in a comment on another answer, ""."" only works in bash related shells, where ""source"" works in both bash and csh related shells.) It executes the script in-place (as if you copied and pasted the script right there). This means that any functions and non-local variables in the script remain. It also means if the script does a cd into a directory, you'll still be there when its done.</p>

<p>The other ways of running a script will run it in its own subshell. Variables in the script are not still alive when it's done. If the script changed directories, then it doesn't affect the calling environment.</p>

<p>/path/to/script and /bin/sh script are slightly different. Typically, a script has a ""shebang"" at the beginning that looks like this:</p>

<pre><code>#! /bin/bash
</code></pre>

<p>This is the path to the script interpreter. If it specifies a different interpreter than you do when you execute it, then it may behave differently (or may not work at all).</p>

<p>For example, Perl scripts and Ruby scripts begin with (respectively):</p>

<pre><code>#! /bin/perl
</code></pre>

<p>and</p>

<pre><code>#! /bin/ruby
</code></pre>

<p>If you execute one of those scripts by running <code>/bin/sh script</code>, then they will not work at all.</p>

<p>Ubuntu actually doesn't use the bash shell, but a very similar one called dash. Scripts that require bash may work slightly wrong when called by doing <code>/bin/sh script</code> because you've just called a bash script using the dash interpreter.</p>

<p>Another small difference between calling the script directly and passing the script path to the interpreter is that the script must be marked executable to run it directly, but not to run it by passing the path to the interpreter.</p>

<p>Another minor variation: you can prefix any of these ways to execute a script with eval, so, you can have</p>

<pre><code>eval sh script
eval script
eval . script
</code></pre>

<p>and so on. It doesn't actually change anything, but I thought I'd include it for thoroughness.</p>
","2978"
"How to clean up unnecessary files","63089","","<p>Please suggest me any particular unnecessary file that I can clean to back everything to normal condition(temporarily). (i.e. any log or archieve or anything ). My var/log has only 40MB and Home directory has 3GB of space(so I believe that's not a problem). Other than that what I can clean up to make space. </p>

<pre><code>[user@host]$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg_inamivm-lv_root
                       18G   17G     0 100% /
tmpfs                 1.9G     0  1.9G   0% /dev/shm
/dev/sda1             485M   71M  389M  16% /boot
</code></pre>

<p>I am in a debian machine. </p>

<p><strong>UPDATE1:</strong></p>

<p>output of  <code>cd /; du -sxh *</code></p>

<pre><code>6.1M    bin
61M     boot
156K    dev
22M        etc
3.3G    home
306M    lib
18M     lib64
16K     lost+found
4.0K    media
4.0K    mnt
408K    opt
du: cannot access `proc/18605/task/18605/fd/4': No such file or directory
du: cannot access `proc/18605/task/18605/fdinfo/4': No such file or directory
du: cannot access `proc/18605/fd/4': No such file or directory
du: cannot access `proc/18605/fdinfo/4': No such file or directory
0       proc
208K    root
9.7M    sbin
0       selinux
4.0K    srv
0       sys
8.0K    tmp
536M    usr
187M    var
</code></pre>

<p><strong>Update2</strong></p>

<p>Output of <code>ls -la /</code></p>

<pre><code>dr-xr-xr-x.  22 root root  4096 Aug  7 08:42 .
dr-xr-xr-x.  22 root root  4096 Aug  7 08:42 ..
-rw-r--r--.   1 root root     0 Aug  7 08:42 .autofsck
dr-xr-xr-x.   2 root root  4096 Mar 28 16:53 bin
dr-xr-xr-x.   5 root root  1024 Mar 28 16:54 boot
drwxr-xr-x.  16 root root  3580 Sep  9 03:13 dev
drwxr-xr-x.  69 root root  4096 Aug 23 09:19 etc
drwxr-xr-x.   9 root root  4096 Jun 29 16:10 home
dr-xr-xr-x.   8 root root  4096 Mar  7  2012 lib
dr-xr-xr-x.   9 root root 12288 Mar 28 16:53 lib64
drwx------.   2 root root 16384 Mar  7  2012 lost+found
drwxr-xr-x.   2 root root  4096 Sep 23  2011 media
drwxr-xr-x.   2 root root  4096 Sep 23  2011 mnt
drwxr-xr-x.   3 root root  4096 Mar  7  2012 opt
dr-xr-xr-x. 355 root root     0 Aug  7 08:42 proc
dr-xr-x---.   5 root root  4096 Aug 17 18:27 root
dr-xr-xr-x.   2 root root  4096 May  2 09:13 sbin
drwxr-xr-x.   7 root root     0 Aug  7 08:42 selinux
drwxr-xr-x.   2 root root  4096 Sep 23  2011 srv
drwxr-xr-x.  13 root root     0 Aug  7 08:42 sys
drwxrwxrwt.   3 root root  4096 Sep 13 03:37 tmp
drwxr-xr-x.  13 root root  4096 Mar 28 17:53 usr
drwxr-xr-x.  18 root root  4096 Mar  7  2012 var
</code></pre>
","<p>The best way of finding out disk consuming, is using graphical software like <code>baobab</code>:</p>

<p>Launch it with <code>sudo baobab /</code></p>

<p><img src=""https://i.stack.imgur.com/MHV4l.png"" alt=""enter image description here""></p>
","48066"
"why we should use create and copytruncate together?","62960","","<p><a href=""http://www.thegeekstuff.com/2010/07/logrotate-examples/"" rel=""noreferrer"">The Geek Stuff</a> has an example about <code>logrotate</code>. <code>copytruncate</code> will copy and truncate the original log file.</p>

<p>Then why we should use <code>create</code>? The original log file has not been removed.</p>

<pre><code>/tmp/output.log {
  size 1k
  copytruncate
  create
  compress
  compresscmd /bin/bzip2
  compressext .bz2
  rotate 4
  maxage 100
}
</code></pre>

<p>Another question is what is the use of <code>maxage 100</code>? The <code>rotate 4</code> directive will delete old log files after 4 rotation.</p>
","<p>That is probably a mistake, it is found only in one example on that tutorial. All other examples have <code>copytruncate</code> without the <code>create</code> option. Also <code>logrotate</code> man page stats that <strong>It will be actually ignored</strong>:</p>

<blockquote>
  <p><code>copytruncate</code></p>
  
  <p>Truncate  the  original log file to zero size in place after creating a copy, instead of moving the old log file and optionally creating a new one.  It can be used when some program cannot be told  to  close
                its  logfile  and  thus might continue writing (appending) to the previous log file forever.  Note that
                there is a very small time slice between copying the file and truncating it, so some logging data might
                be lost.  When this option is used, the create option will have no effect, as the old log file stays in
                place.</p>
</blockquote>

<hr>

<p>Regarding <code>maxage</code>, I think it can be useful for example for logfiles which can be empty for few rotation periods (days/weeks/months) — if you use <code>notifempty</code>, empty logfile will not be rotated, so you can have too old rotated files still in place. </p>
","39509"
"Best distro for programming","62955","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/24157/linux-distribution-geared-towards-developers"">Linux distribution geared towards developers</a>  </p>
</blockquote>



<p>I don't know if this question qualifies to be a question to be asked here. But can anyone tell me what's the best (linux) distro out there for programmers? I program in multiple languages, including java and lisp. I would be happy to know of a distro that's good enough for programmers yet remaining small enough for a quick download+install. Any suggestions and constructive criticisms are welcome. This is my first question here, please help me out.</p>

<pre><code>IDEs I use: Eclipse/Emacs/KDevelop
Languages : C, C++, Java, Python, Perl, Lisp 
</code></pre>
","<p>Well, the ""best"" thing about linux is that it's up to you how you going to use it,
and for whatever purposes, and it's all free :p
I <em>would</em> suggest you to start with debian, it's hairy enough to meet the begginner
linux programmers needs on software side and yet it's minimalistic and flexible enough for ""users""
Would <em>not</em> suggest to use ubuntu since it's not free enough :P</p>
","53124"
"Where did the ""wheel"" group get its name?","62884","","<p>The <code>wheel</code> group on *nix computers typically refers to the group with some sort of root-like access. I've heard that on some *nixes it's the group of users with the right to run <code>su</code>, but on Linux that seems to be anyone (although you need the root password, naturally). On Linux distributions I've used it seems to be the group that by default has the right to use <code>sudo</code>; there's an entry in <code>sudoers</code> for them:</p>

<pre><code>%wheel ALL=(ALL) ALL
</code></pre>

<hr>

<p>But that's all tangential; my actual question is: <strong>Why is this group called <code>wheel</code></strong>? I've heard miscellaneous explanations for it before, but don't know if any of them are correct. Does anyone know the actual history of the term?</p>
","<p>The <a href=""http://www.catb.org/jargon/html/"" rel=""noreferrer"">Jargon File</a> has an answer which seems to agree with <a href=""https://unix.stackexchange.com/questions/1262/where-did-the-wheel-group-get-its-name/1264#1264"">JanC</a>.</p>

<blockquote>
  <p>wheel: n. 
   [from slang ‘big wheel’ for a powerful person] A person who has an
   active wheel bit...The traditional name of security group zero in BSD (to which the major system-internal users like root belong) is ‘wheel’...</p>
</blockquote>

<p>A wheel bit is also helpfully defined:</p>

<blockquote>
  <p>A privilege bit that allows the possessor to perform some restricted operation on a timesharing system, such as read or write any file on the system regardless of protections, change or look at any address in the running monitor, crash or reload the system, and kill or create jobs and user accounts. The term was invented on the TENEX operating system, and carried over to TOPS-20, XEROX-IFS, and others. The state of being in a privileged logon is sometimes called wheel mode. This term entered the Unix culture from TWENEX in the mid-1980s and has been gaining popularity there (esp. at university sites). </p>
</blockquote>
","1271"
"Format of /etc/hosts on Linux (different from Windows?)","62841","","<p>Pasted below this question is a sample of a <code>/etc/hosts</code> file from a Linux (CentOS) and a Windows machine. The Linux file has two tabbed entries after the IP address (that is localhost.localdomain localhost) and Windows has only one. If I want to edit the hosts file in Windows to have the machine name (etest) instead of localhost, I simply replace the word localhost with the machine name I want. The machine need not be part of a domain.</p>

<p>In a Linux machine, the two entries <code>localhost.localdomain</code> and <code>localhost</code> seems to indicate that I will need the machine to be part of a domain. Is this true?</p>

<p>Can I simply edit both entries to <code>etest</code> so that it will read:</p>

<pre><code>127.0.0.1       etest etest
</code></pre>

<p>or is it required that I substitute one entry with a domain name?</p>

<p>Additionally, please let me know what the second line of the <code>/etc/hosts</code> file on the Linux machine is for.</p>

<pre><code>::1     localhost6.localdomain6 localhost6
</code></pre>

<p><code>hosts</code> file on a Linux machine:</p>

<pre><code># Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1       localhost.localdomain localhost
::1     localhost6.localdomain6 localhost6
</code></pre>

<p><code>hosts</code> file on a windows machine:</p>

<pre><code># Copyright (c) 1993-1999 Microsoft Corp.
#
# This is a sample HOSTS file used by Microsoft TCP/IP for Windows.
#
# This file contains the mappings of IP addresses to host names. Each
# entry should be kept on an individual line. The IP address should
# be placed in the first column followed by the corresponding host name.
# The IP address and the host name should be separated by at least one
# space.
#
# Additionally, comments (such as these) may be inserted on individual
# lines or following the machine name denoted by a '#' symbol.
#
# For example:
#
#      102.54.94.97     rhino.acme.com          # source server
#       38.25.63.10     x.acme.com              # x client host

127.0.0.1       localhost
</code></pre>
","<p>You always want the 127.0.0.1 address to resolve first to localhost. If there is a domain you can use that too, but then make sure localhost is listed second. If you want to add aliases for your machine that will lookup to the loopback address you can keep adding them as space separated values on that line. Specifying a domain here is optional, but don't remove ""localhost"" from the options.</p>
","13047"
"Mount Google Drive in Linux?","62781","","<p>Now that <a href=""https://drive.google.com/"" rel=""nofollow noreferrer"">Google Drive</a> is available, how do we mount it to a Linux filesystem? Similar solutions exist for <a href=""http://code.google.com/p/s3fs/wiki/FuseOverAmazon"" rel=""nofollow noreferrer"">Amazon S3</a> and <a href=""https://unix.stackexchange.com/questions/23646/mount-a-rackspace-drive-in-linux"">Rackspace Cloud Files</a>.</p>
","<p>Grive or inSync is a file sync tool which syncs up a local file system and remote Google Drive. You cannot ""mount"" Google Drive using these tools.</p>

<p>For mounting, use <a href=""http://gdfuse.forge.ocamlcore.org/"">google-drive-ocamlfuse</a>, FUSE-based filesystem for Google Drive.  </p>

<p>Installation instructions, and more details about configuration, and authorization are at the <a href=""https://github.com/astrada/google-drive-ocamlfuse/wiki/Installation"">Installation of FUSE filesystem over Google Drive</a> wiki page (on GitHub).</p>

<p>The project's GitHub homepage also has the readme file that is for the <code>google-drive-ocamlfuse</code> source code.</p>

<p>Here are distro-specific <a href=""http://xmodulo.com/2013/10/mount-google-drive-linux.html"">instructions to mount Google Drive</a> with google-drive-ocamlfuse.</p>
","96994"
"How to put a newline special character into a file using the echo command and redirection operator?","62779","","<p>I would like to create a file by using the echo command and the redirection operator, the file should be made of a few lines. </p>

<p>I tried to include a newline by ""\n"" inside the string:</p>

<pre><code>echo ""first line\nsecond line\nthirdline\n"" &gt; foo
</code></pre>

<p>but this way no file with three lines is created but a file with only one line and the verbatim content of the string. </p>

<p>How can I create using only this command a file with several lines ? </p>
","<p>You asked for using some syntax with the <code>echo</code> command:</p>

<pre><code>echo $'first line\nsecond line\nthirdline' &gt; foo
</code></pre>

<p>(But consider also the other answer you got.)</p>

<p>The <code>$'...'</code> construct expands embedded ANSI escape sequences.</p>
","191696"
"Output traffic on different interfaces based on destination port","62723","","<p>My question is basically the same as <a href=""https://unix.stackexchange.com/questions/12085/only-allow-certain-outbound-traffic-on-certain-interfaces"">Only allow certain outbound traffic on certain interfaces</a>.</p>

<p>I have two interfaces <code>eth1</code> (10.0.0.2) and <code>wlan0</code> (192.168.0.2).
My default route is for <code>eth1</code>.
Let's say I want all https-traffic to go through <code>wlan0</code>.
Now if I use the solution suggested in the other question, https traffic will go through <code>wlan0</code>, but will still have the source-address of <code>eth1</code> (10.0.0.2). Since this address is not routeable for the <code>wlan0</code> gateway, answers won't ever come back. The easy way would be to just set the bind-addr properly in the application, but in this case it is not applicable.</p>

<p>I figure I need to rewrite the src-addr:</p>

<pre><code># first mark it so that iproute can route it through wlan0
iptables -A OUTPUT -t mangle -o eth1 -p tcp --dport 443 -j MARK --set-mark 1
# now rewrite the src-addr
iptables -A POSTROUTING -t nat -o wlan0 -p tcp --dport 443 -j SNAT --to 192.168.0.2
</code></pre>

<p>Now tcpdump sees the outgoing packets just fine and ingoing packets arrive for 192.168.0.2, however they probably never end up in the application, because all I ever get to see, is that the application is resending the SYN-packet, although the SYN-ACK was already received.</p>

<p>So I thought, maybe I need to rewrite the incoming address too:</p>

<pre><code>iptables -A PREROUTING -t nat -i wlan0 -p tcp --sport 443 -j DNAT --to 10.0.0.2
</code></pre>

<p>but that didn't work either. So I’m kind of stuck here. Any suggestions?</p>
","<p>You're close.</p>

<p>The actual reason that the application isn't seeing the return traffic is because of the kernel's built in IP spoofing protection. I.e., the return traffic doesn't match the routing table and is therefore dropped. You can fix this by turning off spoofing protection like this:</p>

<pre><code>sudo sysctl net.ipv4.conf.wlan0.rp_filter=0
</code></pre>

<p>But I wouldn't recommend it. The more proper way is to create an alternate routing instance.</p>

<ol>
<li>The mark is necessary. Keep it.</li>
<li>Source NAT is also necessary.</li>
<li>The final DNAT is unnecessary, so you can remove it.</li>
</ol>

<p>Make sure you have the <code>iproute</code> package installed. If you have the <code>ip</code> command then you're set (which it looks like you do, but if not get that first).</p>

<p>Edit <code>/etc/iproute2/rt_tables</code> and add a new table by appending the following line:</p>

<pre><code>200 wlan-route
</code></pre>

<p>You then need to configure your new routing table named <code>wlan-route</code> with a default gateway and create rules to conditionally send traffic to that table. I'll assume your default gateway is 192.168.0.1. Naturally this needs to match your actual network, and not just my assumptions.</p>

<pre><code>ip route add default via 192.168.0.1 dev wlan0 table wlan-route
ip rule add fwmark 0x1 table wlan-route
</code></pre>

<p>Your final annotated script would look like this:</p>

<pre><code># Populate secondary routing table
ip route add default via 192.168.0.1 dev wlan0 table wlan-route
# Anything with this fwmark will use the secondary routing table
ip rule add fwmark 0x1 table wlan-route
# Mark these packets so that iproute can route it through wlan-route
iptables -A OUTPUT -t mangle -o eth1 -p tcp --dport 443 -j MARK --set-mark 1
# now rewrite the src-addr
iptables -A POSTROUTING -t nat -o wlan0 -p tcp --dport 443 -j SNAT --to 192.168.0.2
</code></pre>
","21118"
"Easily unpack DEB, edit postinst, and repack DEB","62702","","<p>I'm attempting to install Intel's OpenCL SDK but the DEB files are buggy conversions from RPM (see <a href=""http://%20https://software.intel.com/en-us/forums/topic/515680"" rel=""noreferrer"">here</a> for the curious).  I need to edit the <code>postinst</code> script in the DEB they provide.</p>

<p>How can I take an existing DEB, extract the contents (including the control information), then later repackage the contents to make a new DEB?  I will only edit files, no files will be added or removed.</p>
","<p>The primary command to manipulate deb packages is <a href=""http://manpages.debian.org/cgi-bin/man.cgi?query=dpkg-deb&amp;apropos=0&amp;sektion=1&amp;manpath=Debian+7.0+wheezy&amp;format=html&amp;locale=en""><code>dpkg-deb</code></a>.</p>

<p>To unpack the package, create an empty directory and switch to it, then run <code>dpkg-deb</code> to extract its control information and the package files. Use <code>dpkg-deb -b</code> to rebuild the package.</p>

<pre><code>mkdir tmp
dpkg-deb -R original.deb tmp
# edit DEBIAN/postinst
dpkg-deb -b tmp fixed.deb
</code></pre>

<p>Beware that unless your script is running as root, the files' permissions and ownership will be corrupted at the extraction stage. One way to avoid this is to run your script under <a href=""http://manpages.debian.org/cgi-bin/man.cgi?query=fakeroot&amp;apropos=0&amp;sektion=1&amp;manpath=Debian+7.0+wheezy&amp;format=html&amp;locale=en""><code>fakeroot</code></a>. Note that you need to run the whole sequence under <code>fakeroot</code>, not each <code>dpkg-deb</code> individually, since it's the <code>fakeroot</code> process that keeps the memory of the permissions of the files that can't be created as they are.</p>

<pre><code>fakeroot sh -c '
  mkdir tmp
  dpkg-deb -R original.deb tmp
  # edit DEBIAN/postinst
  dpkg-deb -b tmp fixed.deb
'
</code></pre>

<p>Rather than mess with permissions, you can keep the data archive intact and modify only the control archive. <code>dpkg-deb</code> doesn't provide a way to do that. Fortunately, deb packges are in a standard format: they're <a href=""http://manpages.debian.org/cgi-bin/man.cgi?query=ar&amp;apropos=0&amp;sektion=1&amp;manpath=Debian+7.0+wheezy&amp;format=html&amp;locale=en""><code>ar</code></a> archives. So you can use <code>ar</code> to extract the control archive, modify its files, and use <code>ar</code> again to replace the control archive by a new version.</p>

<pre><code>mkdir tmp
cd tmp
ar p ../original.deb control.tar.gz | tar -xz
# edit postinst
cp ../original.deb ../fixed.deb
tar czf control.tar.gz *[!z]
ar r ../fixed.deb control.tar.gz
</code></pre>

<p>You should <strong>add a <a href=""https://www.debian.org/doc/debian-policy/ch-source.html#s-dpkgchangelog"">changelog entry</a> and change the <a href=""https://www.debian.org/doc/debian-policy/ch-controlfields.html#s-f-Version"">version number</a></strong> if you modify anything in the package. The infrastructure to manipulate Debian packages assumes that if two packages have the same name and version, they're the same package. Add a suffix to the <em>debian_revision</em> part at the end of the version number; for sorting reasons the suffix should start with <code>~</code>, e.g. <code>1.2.3-4.1</code> becomes <code>1.2.3-4.1~johnjumper1</code>.</p>

<p>Instead of using shell tools, you can use Emacs. The <a href=""https://packages.debian.org/unstable/utils/dpkg-dev-el""><code>dpkg-dev-el</code> package</a> (which is its own upstream as this is a native Debian package) contains modes to edit <code>.deb</code> files and to edit Debian changelogs. Emacs can be used interactively or scripted.</p>
","138190"
"convert a hex string to binary and send with netcat","62652","","<p>I have a binary file that I can send with <code>netcat</code>:</p>

<pre><code>$ nc -l localhost 8181 &lt; my.dat
</code></pre>

<p>The file contains this:</p>

<pre><code>$ xxd my.dat
0000000: 0006 3030 3030 4e43                      ..0000NC
</code></pre>

<p>What I really want to do is send the hex string directly.  I've tried this:</p>

<pre><code>$ echo '0006303030304e43' | nc -l localhost 8181
</code></pre>

<p>However, the above command just sends the ascii string directly to <code>nc</code>.</p>
","<p>I used the <strong>-r</strong> and <strong>-p</strong> switch to <strong>xxd</strong>:</p>

<pre><code>$ echo '0006303030304e43' | xxd -r -p | nc -l localhost 8181
</code></pre>

<p>Thanks to inspiration from @Gilles answer, here's a perl version:</p>

<pre><code>$ echo '0006303030304e43' | perl -e 'print pack ""H*"", &lt;STDIN&gt;' | nc -l localhost 8181
</code></pre>
","82563"
"Set persistent routing table on Debian","62631","","<p>I have modified my routing table by deleting one rule, and adding two new rules for my second interface eth1:</p>

<pre><code>route del -net 10.1.2.0 netmask 255.255.255.0
route add -host 10.1.2.51 eth1
route add -host 10.1.2.52 eth1
</code></pre>

<p>I want the rules to survive reboot. I could write a simple rc script to remove 1 rule and insert 2 new, but I was wondering whether there is a way to configure ifconfig, not to add a route rule for the interface (eth1) when it is started, or even better, to specify which rules should be added.</p>

<p>What would be a clean way to do it? Does <code>ifconfig</code> allow me to specify custom rules for routing?</p>
","<p>You can add the calls to the <code>post-up</code> hook when the interface comes up. The
interface configuration sits in <code>/etc/network/interfaces</code>. Here an example:</p>

<pre><code>auto eth1
iface eth1 inet dhcp
  post-up route del -net 10.1.2.0 netmask 255.255.255.0
  post-up route add -host 10.1.2.51 eth1
  post-up route add -host 10.1.2.52 eth1
  pre-down route add -net 10.1.2.0 netmask 255.255.255.0
  pre-down route add -host 10.1.2.51 eth1
  pre-down route add -host 10.1.2.52 eth1
</code></pre>

<p>From the <a href=""http://manpages.debian.net/cgi-bin/man.cgi?query=interfaces&amp;apropos=0&amp;sektion=0&amp;manpath=Debian+7.0+wheezy&amp;format=html&amp;locale=en"">man page - interfaces</a>:</p>

<blockquote>
  <p><strong>post-up</strong> <code>command</code><br>
                Run command after bringing the interface up.   If  this  command
                fails then ifup aborts, refraining from marking the interface as
                configured (even though it has really been  configured),  prints
                an  error  message,  and exits with status 0.</p>
  
  <p><strong>pre-down</strong> <code>command</code><br>
                Run command before taking the interface down.  If  this  command
                fails  then  ifdown  aborts, marks the interface as deconfigured
                (even though it has not really  been  deconfigured),  and  exits
                with status 0.</p>
</blockquote>

<p>Furthermore, on Linux the “net-tools” <code>route</code> is deprecated and should be replaced
by the tools from the <a href=""http://en.wikipedia.org/wiki/Iproute2"">iproute2</a> package.</p>

<p>Further reading:</p>

<ul>
<li><a href=""http://www.cyberciti.biz/faq/howto-linux-configuring-default-route-with-ipcommand/"">Linux Set Up Routing with ip Command</a></li>
<li><a href=""http://www.policyrouting.org/iproute2.doc.html#ss9.5.1"">IPROUTE2 Utility Suite Howto</a></li>
</ul>
","84553"
"How do I add an entry to my crontab?","62605","","<p>I am using crontab for the first time. Want to write a few very simple test cron tasks, and run them.</p>

<pre><code>$crontab * * * * * echo ""Hi""
</code></pre>

<p>doesn't produce anything. </p>

<pre><code>crontab */1 * * * * echo ""hi""
</code></pre>

<p>says <code>*/1: No such file or directory</code>.</p>

<p>Also, how do I list the currently running cron tasks (not just the ones I own, but ones started by other users such as root as well).</p>

<p>And how do I delete a particular cron task?</p>
","<p>You can't use <code>crontab</code> like that. Use <code>man crontab</code> to read about the correct way of calling this utility.</p>

<p>You'll want to use <code>crontab -e</code> to edit the current user's cron entries (you can add/modify/remove lines). Use <code>crontab -l</code> to see the current list of configured tasks.</p>

<p>As for seeing other user's crontabs, that's not possible without being root on default installations. See <a href=""https://stackoverflow.com/questions/134906/how-do-i-list-all-cron-jobs-for-all-users"">How do I list all cron jobs for all users</a> for some ways to list everything (as root).</p>

<p>Note: be very careful when you use shell globbing characters on the command line (<code>*</code> and <code>?</code> especially). <code>*</code> will be expanded to the list of files in the current directory, which can have unexpected effects. If you want to pass <code>*</code> as an argument to something, quote it (<code>'*'</code>).</p>
","21300"
"Asking rsync to delete files on the receiving side that don't exist on the sending side, with exceptions on the receiving side","62538","","<p>I have read these threads:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/1813907/rsync-delete-files-from-list-dest-does-not-delete-unwanted-files"">rsync --delete --files-from=list / dest/ does not delete unwanted files</a> </li>
<li><a href=""https://unix.stackexchange.com/questions/5451/delete-extraneous-files-from-dest-dir-via-rsync"">Delete extraneous files from dest dir via rsync?</a></li>
</ul>

<p>But, as far as I can tell (maybe I am missing something), they don't cover the following question:</p>

<p>How do you ask <code>rsync</code> to copy files and delete those on the receiving side that do not exist on the sending side, with exceptions? (e.g. don't remove a mercurial repository <code>.hg</code> on the  receiving side, even if there is no repository on the sending side).</p>

<h2>One possibility?</h2>

<p>Borrowing from @Richard Holloway's answer below. Say I have the following line:</p>

<pre><code>rsync -av --exclude=dont_delete_me --delete /sending/path /receiving/path
</code></pre>

<p>As far as I understand, this line would make <code>rsync</code> delete everything on the receiving path that does not exist on the sending path, except those things matched by <code>dont_delete_me</code>. My question now is: Would rsync keep files on the receiving side that are matched by <code>dont_delete_me</code> <strong>even if nothing on the sending side matches <code>dont_delete_me</code>?</strong></p>
","<p>If you use <code>--delete</code> and <code>--exclude</code> together what is in the excluded location won't get deleted even if the source files are removed.</p>

<p>But that raises the issue that the folder won't be <code>rsync</code>'d at all. So you will need another <code>rsync</code> job to <code>sync</code> that folder.</p>

<p>Eg.</p>

<pre><code>rsync -nav /home/richardjh/keepall/ /home/backup/richardjh/keepall/
rsync -nav --exclude=keepall --delete /home/richardjh /home/backup/richardjh
</code></pre>

<p>You could run these the other way around, but then it would delete all removed files and then replace them, which is not as efficient.</p>

<p>You can't do it as a one liner.</p>
","18566"
"Why is `while IFS= read` used so often, instead of `IFS=; while read..`?","62504","","<p>It seems that normal practice would put the setting of IFS outside the while loop in order to not repeat setting it for each iteration... Is this just a habitual ""monkey see, monkey do"" style, as it has been for this monkey until I read <em>man read</em>, or am I missing some subtle (or blatantly obvious) trap here? </p>
","<p>The trap is that </p>

<pre><code>IFS=; while read..
</code></pre>

<p>sets the <code>IFS</code> for the whole shell environment outside the loop, whereas</p>

<pre><code>while IFS= read
</code></pre>

<p>redefines it only for the <strong><code>read</code> invocation</strong> (except in the Bourne shell).
You can check that doing a loop like</p>

<pre><code>while IFS= read xxx; ... done
</code></pre>

<p>then after such loop, <code>echo ""blabalbla $IFS ooooooo""</code> prints</p>

<pre><code>blabalbla
 ooooooo
</code></pre>

<p>whereas after </p>

<pre><code>IFS=; read xxx; ... done
</code></pre>

<p>the <code>IFS</code> <strong>stays</strong> redefined: now <code>echo ""blabalbla $IFS ooooooo""</code> prints</p>

<pre><code>blabalbla  ooooooo
</code></pre>

<p>So if you use the second form, you have to remember to reset : <code>IFS=$' \t\n'</code>.</p>

<hr>

<p><sub>
The second part of this question <a href=""https://unix.stackexchange.com/questions/18922/in-while-ifs-read-why-does-ifs-have-no-effect"">has been merged here</a>, so I've removed the related answer from here.
</sub></p>
","18889"
"What are pid and lock files for?","62492","","<p>I often see that programs specify pid and lock files. And I'm not quite sure what they do.</p>

<p>For example, when compiling nginx:</p>

<pre><code>--pid-path=/var/run/nginx.pid \
--lock-path=/var/lock/nginx.lock \
</code></pre>

<p>Can somebody shed some light on this one?</p>
","<p>pid files are written by some programs to record their process ID while they are starting.  This has multiple purposes:</p>

<ul>
<li>It's a signal to other processes and users of the system that that particular program is running, or at least started successfully.</li>
<li>It allows one to write a script really easy to check if it's running and issue a plain <code>kill</code> command if one wants to end it.</li>
<li>It's a cheap way for a program to see if a previous running instance of it did not exit successfully.</li>
</ul>

<p>Mere presence of a pid file doesn't guarantee that that particular process id is running, of course, so this method isn't 100% foolproof but ""good enough"" in a lot of instances.  Checking if a particular PID exists in the process table isn't totally portable across UNIX-like operating systems unless you want to depend on the <code>ps</code> utility, which may not be desirable to call in all instances (and I believe some UNIX-like operating systems implement <code>ps</code> differently anyway).</p>

<p>Lock files are used by programs to ensure two (well-behaved) separate instances of a program, which may be running concurrently on one system, don't access something else at the same time.  The idea is before the program accesses its resource, it checks for presence of a lock file, and if the lock file exists, either error out or wait for it to go away.  When it doesn't exist, the program wanting to ""acquire"" the resource creates the file, and then other instances that might come across later will wait for this process to be done with it.  Of course, this assumes the program ""acquiring"" the lock does in fact release it and doesn't forget to delete the lock file.</p>

<p>This works because the filesystem under all UNIX-like operating systems enforces <em>serialization</em>, which means only one change to the filesystem actually happens at any given time.  Sort of like locks with databases and such.</p>
","12818"
"Change sshd logging file location on CentOS?","62467","","<p>How do I change the <code>sshd</code> logging file location on CentOS? <code>sshd</code> logs to <code>/var/log/messages</code> instead of <code>/var/log/secure</code>. How can I change the setting so <code>sshd</code> will stop sending logs to <code>/var/log/messages</code>?</p>
","<p>Please post your <code>sshd_config</code> something else would seem to be up. A stock CentOS system always logs to <code>/var/log/secure</code>.</p>

<h3>Example</h3>

<pre><code>$ sudo tail -f /var/log/secure
Feb 18 23:23:34 greeneggs sshd[3545]: pam_succeed_if(sshd:auth): requirement ""uid &gt;= 1000"" not met by user ""root""
Feb 18 23:23:36 greeneggs sshd[3545]: Failed password for root from ::1 port 46401 ssh2
Feb 18 23:23:42 greeneggs unix_chkpwd[3555]: password check failed for user (root)
Feb 18 23:23:42 greeneggs sshd[3545]: pam_succeed_if(sshd:auth): requirement ""uid &gt;= 1000"" not met by user ""root""
Feb 18 23:23:43 greeneggs sshd[3545]: Failed password for root from ::1 port 46401 ssh2
Feb 18 23:23:48 greeneggs sshd[3545]: Accepted password for root from ::1 port 46401 ssh2
Feb 18 23:23:48 greeneggs sshd[3545]: pam_unix(sshd:session): session opened for user root by (uid=0)
Feb 18 23:24:05 greeneggs sshd[3545]: Received disconnect from ::1: 11: disconnected by user
Feb 18 23:24:05 greeneggs sshd[3545]: pam_unix(sshd:session): session closed for user root
Feb 18 23:27:15 greeneggs sudo:     saml : TTY=pts/3 ; PWD=/home/saml ; USER=root ; COMMAND=/bin/tail /var/log/secure
</code></pre>

<p>This is controlled through <code>/etc/ssh/sshd_config</code>:</p>

<pre><code># Logging
# obsoletes QuietMode and FascistLogging
#SyslogFacility AUTH
SyslogFacility AUTHPRIV
#LogLevel INFO
</code></pre>

<p>As well as the contents of <code>/etc/rsyslog.conf</code>:</p>

<pre><code># Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;authpriv.none;cron.none                /var/log/messages

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure
</code></pre>

<h3>Your issue</h3>

<p>In one of your comments you mentioned that your <code>rsyslogd</code> config file was named <code>/etc/rsyslog.config</code>. That isn't the correct name for this file, and is likely the reason your logging is screwed up. Change the name of this file to <code>/etc/rsyslog.conf</code> and then restart the logging service.</p>

<pre><code>$ sudo service rsyslog restart
</code></pre>
","115840"
"How long has my Linux system been running?","62290","","<p>Is there a command I can type in a terminal that will tell me the last time a machine was rebooted?</p>
","<p><a href=""http://linux.die.net/man/1/uptime""><code>uptime</code></a></p>

<p>If you want it in numerical form, it's the first number in <code>/proc/uptime</code> (in seconds), so the time of the last reboot is</p>

<pre><code>date -d ""$(&lt;/proc/uptime awk '{print $1}') seconds ago""
</code></pre>

<p>The uptime includes the time spent in a low-power state (standby, suspension or hibernation).</p>
","131776"
"How to check that a user/password is expired in AIX?","62197","","<p>I can check that the user is expired or not with: </p>

<pre><code>lsuser -f USERNAME | fgrep expires
</code></pre>

<p>But how can I check that the user's password is expired or not? Are there any other ""expiring"" things that can cause trouble? [so that user can't login, because he can only reach a server through FTP and his password expired, and he can't change it, because he hasn't got SSH access to give out the ""passwd"" command to update his password.]</p>
","<p>Is there any <code>chage</code> sort of command on AIX? check /etc/shadow file thats where the expiry information is stored.</p>

<p>Update: It seems there is a passwdexpired subroutine that can be loaded and Checks the user's password to determine if it has expired. However, it seems to be used as root.</p>

<p><a href=""http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=%2Fcom.ibm.aix.basetechref%2Fdoc%2Fbasetrf1%2Fpasswdexpired.htm"" rel=""nofollow"">http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=%2Fcom.ibm.aix.basetechref%2Fdoc%2Fbasetrf1%2Fpasswdexpired.htm</a></p>

<p>This link has excellent documentation of what you would require</p>

<p><a href=""http://www.torontoaix.com/scripting/when_pwd_exp"" rel=""nofollow"">http://www.torontoaix.com/scripting/when_pwd_exp</a></p>

<p>As demonstrated earlier in the above article, the expiry of a password is governed by the maxage attribute. </p>

<pre><code>For example:
maxage=0 means never to expire
maxage=2 means will expire in two weeks.
</code></pre>

<p>AIX stores the time in the epoch format in seconds, so first you must determine how many seconds in a week, as this is how maxage measures the time between password expiry, that is in week numbers. There are 86400 seconds in a day, so multiplying that by seven comes in at 604800. So there are 604800 seconds in a week. The next command you need to look at is the pwdadm, which in turn queries the file /etc/security/passwd. This file holds the values in seconds when a user last changed their password. Interrogating the file or using the pwdadm command will return the same result. 
For this demonstration, let us query the user spoll:</p>

<pre><code># grep -p ""spoll:"" /etc/security/passwd
spoll:
        password = EvqNjMMwJzXnc
        lastupdate = 1274003127
        flags =       ADMCHG

# pwdadm -q spoll
spoll:
        lastupdate = 1274003127
        flags = ADMCHG
</code></pre>

<p>You can see the lastupdate value in seconds from the above output. In other words, the last time the password was changed:
1274003127</p>

<p>Next, using the lsuser or interrogating the file with /etc/security/user, you can determine the number of weeks before the user spoll password will expire:</p>

<pre><code># grep -p ""spoll:"" /etc/security/user
spoll:
        admin = false
        maxage = 4

# lsuser -a maxage spoll
spoll maxage=4
</code></pre>

<p>You can see from the above output that the number of weeks before password expiry is 4.
The next task is then to multiply the number of seconds in a week by the number of weeks before the user spoll password is due to expire. In this case, it is 4:
604800 * 4 </p>

<pre><code># expr 604800 \* 4
2419200
</code></pre>

<p>Next, you need to add the maxage value in seconds (604800 * 4) to the last time the password was changed:
2419200 +   1274003127</p>

<pre><code># expr 2419200 + 1274003127
1276422327
</code></pre>

<p>You can now convert that number of seconds from UNIX epoch into a more meaningful current time presentation. You can use different tools, but for this demonstration you'll use gawk with the strftime function:</p>

<pre><code># gawk 'BEGIN {print strftime(""%c"",'1276422327')}'
Sun Jun 13 10:45:27 BST 2010
</code></pre>

<p>The above calculation gives the time of the next password expiry.
So, you now know that user spoll's password was last changed on ( from the pwdadm command):</p>

<pre><code># gawk 'BEGIN {print strftime(""%c"",'1274003127')}'
Sun May 16 10:45:27 BST 2010
</code></pre>

<p>And that it will expire on:</p>

<pre><code>Sun Jun 13 10:45:27 BST 2010
</code></pre>

<p>------------------Perl script-let--------</p>

<pre><code>#!/bin/perl
use POSIX qw(strftime);
$maxage=4; 
$last_update = 1274003127
$max_week_seconds = 86400 * $maxage;
print strftime(""%C "", localtime($max_week_seconds));
</code></pre>

<hr>
","26084"
"Unix & Linux pranks","62176","","<p>Which harmless pranks do you know that would be great to play on your collegues?</p>
","<p>I do not know if this qualifies as a prank, but you can <a href=""http://www.commandlinefu.com/commands/view/1713/watch-star-wars-via-telnet"" rel=""nofollow"">watch StarWars on a shell</a> !</p>

<pre><code>telnet towel.blinkenlights.nl
</code></pre>

<p><a href=""http://blinkenlights.nl/services.html#bofh"" rel=""nofollow"">About it</a>.</p>
","234"
"mount error ""is not a block device""","62120","","<p>I am trying to make <code>olddir</code> accessible from <code>newdir</code> with the mount command:</p>

<pre><code>mount olddir newdir
</code></pre>

<p>Why do I get the following error?</p>

<blockquote>
  <p>mount: olddir is not a block device</p>
</blockquote>
","<p>mount attaches block storage devices that contain a filesystem to a directory, which is not what you're trying to do, hence the error message.  What you want is to create a link from the new directory name to the old existing name.  For that you must use the <code>ln</code> command to create a symbolic link.</p>

<pre><code>ln -s olddir newdir
</code></pre>
","30638"
"I Can't login as root with su command, but I can with SSH","62086","","<p>How is it possible that I cannot log in as root by <code>su root</code> or <code>su</code> (I get incorrect password error), <em>but</em> I can log in by <code>ssh root@localhost</code> or <code>ssh root@my_local_IP</code> with the same password?</p>

<p>I'm using CentOS 6.4.</p>

<hr>

<h3><em>Update1</em> :</h3>

<pre><code>cat /etc/pam.d/su
</code></pre>

<p>gives: </p>

<pre><code>#%PAM-1.0
auth        sufficient  pam_rootok.so
# Uncomment the following line to implicitly trust users in the ""wheel"" group.
#auth       sufficient  pam_wheel.so trust use_uid
# Uncomment the following line to require a user to be in the ""wheel"" group.
#auth       required    pam_wheel.so use_uid
auth        include     system-auth
account     sufficient  pam_succeed_if.so uid = 0 use_uid quiet
account     include     system-auth
password    include     system-auth
session     include     system-auth
session     optional    pam_xauth.so
</code></pre>

<h3><em>Update2</em> :</h3>

<pre><code>$ sudo grep su /var/log/secure | grep -v sudo
</code></pre>

<p>gives :</p>

<pre><code>Feb 23 13:12:17 fallah su: pam_unix(su:auth): authentication failure;
logname=fallah uid=501 euid=501 tty=pts/0 ruser=fallah rhost=  user=root
</code></pre>

<p>repeated about 20 times.</p>
","<p>In your comment, you said that <code>/bin/su</code> has the following mode/owner:</p>

<pre><code>-rwxrwxrwx. 1 root root 30092 Jun 22 2012 /bin/su
</code></pre>

<p>There are two problems here.</p>

<ul>
<li><p>it needs to have the set-uid bit turned on, so that it always runs with root permissions, otherwise when an ordinary (non-root) user runs it, it will not have access to the password info in <code>/etc/shadow</code> nor the ability to set the userid to the desired new user.</p></li>
<li><p>it ought to have the <code>group</code> and <code>other</code> write bits turned off, so that other users cannot alter it.</p></li>
</ul>

<p>To fix this, login as <code>root</code> - you said you can do this with <code>ssh</code>- and type</p>

<pre><code>chmod 4755 /bin/su
</code></pre>

<p>or, alternatively,</p>

<pre><code>chmod u+s,g-w,o-w /bin/su
</code></pre>

<p>(The <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/chmod.html"">standards document for chmod</a> goes into more detail about what kinds of arguments it takes.)
This will restore the mode bits to the way they were when the operating system was first installed. When you list this file, it ought to look like this:</p>

<pre><code>-rwsr-xr-x. 1 root root 30092 Jun 22 2012 /bin/su
</code></pre>

<p>As @G-Man noted, files that are mode 777 could be overwritten by untrusted users, and if that's the case, you may want to reinstall them from the distribution medium or backups.</p>
","117005"
"IPTables rule to allow incoming SSH connections","62048","","<p>The aim of this script is to only allow traffic over the VPN, except for localhost&lt;->localhost and incoming SSH traffic. But when I run the script over SSH I am disconnected and forced to restart the vm. What is wrong with my script?</p>

<pre><code>#!/bin/bash
iptables -F

#Allow over VPN
iptables -A INPUT -i tun+ -j ACCEPT
iptables -A OUTPUT -o tun+ -j ACCEPT

#Localhost
iptables -A INPUT -s 127.0.0.1/8 -j ACCEPT
iptables -A OUTPUT -d 127.0.0.1/8 -j ACCEPT

#VPN
iptables -A INPUT -s 123.123.123.123 -j ACCEPT
iptables -A OUTPUT -d 123.123.123.123 -j ACCEPT

#SSH
iptables -A INPUT -p tcp --dport ssh -j ACCEPT

#Default Deny
iptables -A INPUT -j DROP
iptables -A OUTPUT -j DROP
</code></pre>
","<p>Output chain is responsible for <em>any</em> packet going out.</p>

<p>Your script only allows outbound packets to tunnel interface, localhost and remote host at 123.123.123.123.</p>

<p>If you are connecting to the server in a way that requires SSH daemon to send packets to the destination other than one of the above, the traffic will not be allowed to go out.</p>

<p>To allow outbound packets from your SSH daemon to the SSH client you need to add the following rule:</p>

<pre><code>iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT
</code></pre>

<p>You might also want to add destination IP criteria to the above rule, if you are only connecting from a single location. This rule needs to come before the ultimate 'DROP anything else' rule for the output chain.</p>
","136204"
"Create new-window with current directory in tmux","61982","","<p>Is is possible to open a <code>new-window</code> with its working directory set to the one I am currently in. I am using <em>zsh</em>, if it matters.</p>
","<p>Starting in <em>tmux</em> 1.9 the <code>default-path</code> option was removed, so you need to use the <code>-c</code> option with <code>new-window</code>, and <code>split-window</code> (e.g. by rebinding the <code>c</code>, <code>""</code>, and <code>%</code> bindings to include<br>
<code>-c '#{pane_current_path}'</code>). See some of the other answers to this question for details.</p>

<hr>

<p>A relevant feature landed in the <em>tmux</em> SVN trunk in early February 2012. In <em>tmux</em> builds that include this code, <em>tmux</em> key bindings that invoke <code>new-window</code> will create new a window with the same current working directory as the current pane’s active processes (as long as the <code>default-path</code> session option is empty; it is by default). The same is true for the pane created by the <code>split-window</code> command when it is invoked via a binding.</p>

<p>This uses special platform-specific code, so only certain OSes are supported at this time: Darwin (OS X), FreeBSD, Linux, OpenBSD, and Solaris.</p>

<p>This should be available in the next release of <em>tmux</em> (1.7?).</p>

<hr>

<p>With <em>tmux</em> 1.4, I usually just use</p>

<pre><code>tmux neww
</code></pre>

<p>in a shell that already has the desired current working directory.</p>

<p>If, however, I anticipate needing to create many windows with the same current working directory (or I want to be able to start them with the usual &lt;prefix&gt;<code>c</code> key binding), then I set the <code>default-path</code> session option via</p>

<pre><code>tmux set-option default-path ""$PWD""
</code></pre>

<p>in a shell that already has the desired current working directory (though you could obviously do it from any directory and just specify the value instead).</p>

<p>If <code>default-path</code> is set to a non-empty value, its value will be used instead of “inheriting” the current working directory from command-line invocations of <code>tmux neww</code>.</p>

<p>The <a href=""http://tmux.svn.sourceforge.net/viewvc/tmux/trunk/FAQ""><em>tmux</em> FAQ</a> has an entry titled “How can I open a new window in the same directory as the current window?” that describes another approach; it is a bit convoluted though.</p>
","12091"
"iptables forward all traffic to interface","61948","","<p>I have two interfaces <code>eth1</code> and <code>eth0</code>. I want all traffic on <code>eth0</code>to  be forwarded to <code>eth1</code>. I created an iptable rule like this:</p>

<pre><code>iptables -A FORWARD -s 0/0 -i eth0 -p tcp -o eth1 -j ACCEPT
</code></pre>

<p>But this doesn't work. Is this the correct way of doing this?</p>
","<p>If you haven't already enabled forwarding in the kernel, do so.  </p>

<ul>
<li><p>Open <code>/etc/sysctl.conf</code> and uncomment <code>net.ipv4.ip_forward = 1</code> </p></li>
<li><p>Then execute <code>$ sudo sysctl -p</code></p></li>
</ul>

<p>Add the following rules to <code>iptables</code></p>

<pre><code>sudo iptables -t nat -A POSTROUTING --out-interface eth1 -j MASQUERADE  
sudo iptables -A FORWARD --in-interface eth0 -j ACCEPT
</code></pre>

<p>All of the forwarded traffic will traverse the FORWARD chain.  To filter  packets you'll now have to create rules on that chain specifying which interface is incoming/outgoing instead of using the INPUT/OUTPUT chains.</p>
","126596"
"How to start tmux with attach if a session exists","61943","","<p>If I use</p>

<pre><code>tmux attach
</code></pre>

<p>I can attach to a running session but if there is no session running, I only get the error</p>

<pre><code>no sessions
</code></pre>

<p>How can I automatically start a new session if there is none running? something like</p>

<pre><code>tmux attach-or-create-new-session
</code></pre>
","<p>The answer is much simpler. Just put this in your <code>~/.tmux.conf</code> file:</p>

<pre><code># if run as ""tmux attach"", create a session if one does not already exist
new-session -n $HOST
</code></pre>

<p>If you run <code>tmux attach</code> and there is a session, then it will attach to that session (whether it's already attached or not). If there is <em>not</em> a session already then it will create one for you.</p>
","103956"
"Why use swap when there is more than enough free space in RAM?","61762","","<p>Using <em>swap</em> space instead of RAM can <em>drastically slow down</em> a PC.</p>

<p>So why, when I have more than enough RAM available, does my Linux system (Arch) use the swap?</p>

<p>Checkout my conky output below:</p>

<p><img src=""https://i.stack.imgur.com/Ubmwh.png"" alt=""conky output""></p>

<p>Also, could this be the cause of speed and system-responsiveness issues I'm having?</p>

<p>Output of <code>free -m</code> :</p>

<pre><code>$ free -m
             total       used       free     shared    buffers     cached
Mem:          1257       1004        252          0         51        778
-/+ buffers/cache:        174       1082
Swap:          502        144        357
</code></pre>
","<p>It is normal for Linux systems to use <em>some</em> swap even if there is still RAM free.  The Linux kernel will move to swap memory pages that are very seldom used (e.g., the <code>getty</code> instances when you only use X11, and some other inactive daemon).</p>

<p>Swap space usage becomes an issue <em>only when there is not enough RAM available,</em> and the kernel is forced to continuously move memory pages to swap and back to RAM, just to keep applications running.  In this case, system monitor applications would show a lot of disk I/O activity.</p>

<p>For comparison, my Ubuntu 10.04 system, with two users logged in with X11 sessions both running GNOME desktop, uses ~600MB of swap and ~1GB of RAM (not counting buffers and fs cache), so I'd say that your figures for swap usage look normal.</p>
","2663"
"How can I test the encoding of a text file... Is it valid, and what is it?","61722","","<p>I have several <code>.htm</code> files which open in Gedit without any warning/error, but when I open these same file in <code>Jedit</code>, it warns me of invalid UTF-8 encoding...  </p>

<p>The html meta tag states ""charset=ISO-8859-1""<br>
Jedit allows a <em>List of fallback encodings</em> and a <em>List of encoding auto-detectors</em> (currently ""BOM XML-PI""), so my immediate problem has been resolved. but this got me thinking about: ""What if the meta data wasn't there?""  </p>

<p>When the encoding info is just not available, is there a CLI program which can make a ""best-guess"" of which encodings may apply?  </p>

<p>And, although it is a slightly differnt issue; is there a CLI program which tests the validity of a <em>known</em> encoding? </p>
","<p>The <code>file</code> command makes ""best-guesses"" about the encoding. Use the <code>-i</code> parameter to force <code>file</code> to print information about the encoding.</p>

<p>Demonstration:</p>

<pre><code>$ file -i *
umlaut-iso88591.txt: text/plain; charset=iso-8859-1
umlaut-utf16.txt:    text/plain; charset=utf-16le
umlaut-utf8.txt:     text/plain; charset=utf-8
</code></pre>

<p>Here is how I created the files:</p>

<pre><code>$ echo ä &gt; umlaut-utf8.txt 
</code></pre>

<p>Nowadays everything is utf-8. But convince yourself:</p>

<pre><code>$ hexdump -C umlaut-utf8.txt 
00000000  c3 a4 0a                                          |...|
00000003
</code></pre>

<p>Compare with <a href=""https://en.wikipedia.org/wiki/%C3%84#Computer_encoding"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Ä#Computer_encoding</a></p>

<p>Convert to the other encodings:</p>

<pre><code>$ iconv -f utf8 -t iso88591 umlaut-utf8.txt &gt; umlaut-iso88591.txt 
$ iconv -f utf8 -t utf16 umlaut-utf8.txt &gt; umlaut-utf16.txt 
</code></pre>

<p>Check the hex dump:</p>

<pre><code>$ hexdump -C umlaut-utf16.txt 
00000000  ff fe e4 00 0a 00                                 |......|
00000006
$ hexdump -C umlaut-iso88591.txt 
00000000  e4 0a                                             |..|
00000002
</code></pre>

<p>Mix all three to create something ""invalid"":</p>

<pre><code>$ cat umlaut-iso88591.txt umlaut-utf8.txt umlaut-utf16.txt &gt; umlaut-mixed.txt 
</code></pre>

<p>What <code>file</code> says:</p>

<pre><code>$ file -i *
umlaut-iso88591.txt: text/plain; charset=iso-8859-1
umlaut-mixed.txt:    application/octet-stream; charset=binary
umlaut-utf16.txt:    text/plain; charset=utf-16le
umlaut-utf8.txt:     text/plain; charset=utf-8
</code></pre>

<p>without <code>-i</code>:</p>

<pre><code>$ file *
umlaut-iso88591.txt: ISO-8859 text
umlaut-mixed.txt:    data
umlaut-utf16.txt:    Little-endian UTF-16 Unicode text, with no line terminators
umlaut-utf8.txt:     UTF-8 Unicode text
</code></pre>

<p>The <code>file</code> command has no idea of ""valid"" or ""invalid"". It just sees some bytes and tries to guess what the encoding might be. As humans we can quite easily see that a file is a text file with some umlauts in a ""wrong"" encoding. But as a computer it would need some sort of artificial intelligence.</p>

<p>One might argue that the heuristics of <code>file</code> is some sort of artificial intelligence. Yet, even if it is, it is a very limited one.</p>

<p>Here is more information about the <code>file</code> command: <a href=""http://www.linfo.org/file_command.html"" rel=""noreferrer"">http://www.linfo.org/file_command.html</a></p>
","11607"
"Calling multiple bash scripts and running them in parallel, not in sequence","61657","","<p>Suppose that I have three (or more) bash scripts: <code>script1.sh</code>, <code>script2.sh</code>, and <code>script3.sh</code>.  I would like to call all three of these scripts and run them in <em>parallel</em>.  One way to do this is to just execute the following commands:</p>

<pre><code>nohup bash script1.sh &amp;
nohup bash script2.sh &amp;
nohup bash script3.sh &amp;
</code></pre>

<p>(In general, the scripts may take several hours or days to finish, so I would like to use <code>nohup</code> so that they continue running even if my console closes.)</p>

<p>But, is there any way to execute those three commands in <em>parallel</em> with a <em>single</em> call?</p>

<p>I was thinking something like</p>

<pre><code>nohup bash script{1..3}.sh &amp;
</code></pre>

<p>but this appears to execute <code>script1.sh</code>, <code>script2.sh</code>, and <code>script3.sh</code> in sequence, not in parallel.</p>
","<pre><code>for((i=1;i&lt;100;i++)); do nohup bash script${i}.sh &amp; done
</code></pre>
","169328"
"How to find the IP address of a KVM Virtual Machine, that I can SSH into it?","61632","","<p>I have follow this guide (<a href=""http://www.howtoforge.com/virtualization-with-kvm-on-ubuntu-11.10"" rel=""nofollow noreferrer"">Virtualization With KVM On Ubuntu 11.10</a>) to setup my KVM (Virtual Machines Software) on my Ubuntu 11.10 Server. However, I didn't setup my VM's IP address when creating the VM, instead of using: </p>

<pre><code>vmbuilder kvm ubuntu --suite=oneiric --flavour=virtual --arch=amd64 --mirror=http://de.archive.ubuntu.com/ubuntu -o --libvirt=qemu:///system --ip=192.168.0.101 --gw=192.168.0.1 --part=vmbuilder.partition --templates=mytemplates --user=administrator --name=Administrator --pass=howtoforge --addpkg=vim-nox --addpkg=unattended-upgrades --addpkg=acpid --firstboot=/var/lib/libvirt/images/vm1/boot.sh --mem=256 --hostname=vm1 --bridge=br0
</code></pre>

<p>I used: (I deleted ""--ip=192.168.0.101 --gw=192.168.0.1"" from the command line)</p>

<pre><code>vmbuilder kvm ubuntu --suite=oneiric --flavour=virtual --arch=amd64 --mirror=http://de.archive.ubuntu.com/ubuntu -o --libvirt=qemu:///system --part=vmbuilder.partition --templates=mytemplates --user=administrator --name=Administrator --pass=howtoforge --addpkg=vim-nox --addpkg=unattended-upgrades --addpkg=acpid --firstboot=/var/lib/libvirt/images/vm1/boot.sh --mem=256 --hostname=vm1 --bridge=br0
</code></pre>

<p>I have set up the network bridge as the guide instructed and the new VM's interface is connected to the network bridge. </p>

<p>I assume the KVM will assign my VM via DHCP but I don't have information on my new VM's IP address, where can I find the VM's IP address and SSH to the new VM? Thanks.</p>

<p>[Notes: I have managed to login the VM without knowing the IP address of the VM. Using ""<a href=""http://webmodelling.com/webbits/miscellaneous/ubuntu-virtualization-how-to.aspx"" rel=""nofollow noreferrer"">Xming + SSH with X Graphic Forwarding</a>""  But there is no DHCP ip address assigned to my VM, Besides the above question, I have another question here: How to enable the DCHP on my VM so when I use Xming to login via ""virt viewer"" I can at least see my IP address is there.]</p>
","<p>See the blog below for more details. Simply put, you can run <code>arp -n</code> to see what IP your virtual machine pick up. In that way, you don't have to login guest vm and type <code>ifconfig</code>.</p>

<p><a href=""http://rwmj.wordpress.com/2010/10/26/tip-find-the-ip-address-of-a-virtual-machine/"">Tip: Find the IP address of a virtual machine</a></p>
","69045"
"Disk quota exceeded problem","61628","","<p>I am using Debian Squeeze. Suddenly I have started facing a problem that my user is not able to make directories and other such tasks. Running <code>mkdir abc</code> gives me</p>

<p><code>mkdir: cannot create directory 'abc': Disk quota exceeded</code></p>

<p>My hard disk is not full <code>df -h</code> results are</p>

<pre><code>Filesystem            Size  Used Avail Use% Mounted on
/dev/md1              1.8T   39G  1.8T   3% /
tmpfs                 7.8G     0  7.8G   0% /lib/init/rw
udev                  7.8G  148K  7.8G   1% /dev
tmpfs                 7.8G     0  7.8G   0% /dev/shm
/dev/md0              243M   31M  200M  14% /boot
</code></pre>

<p><code>uname -a</code> output that might be needed is</p>

<pre><code>Linux server 2.6.32-5-686-bigmem #1 SMP Sun Sep 23 10:27:25 UTC 2012 i686 GNU/Linux
</code></pre>

<p><strong>Note:</strong> If I login as root then everything is fine. This problem is only with a particular user</p>

<p><strong>Edit</strong>: output of <code>quota</code></p>

<pre><code>Disk quotas for user user (uid 1000): none
</code></pre>

<p>output of <code>quota -g</code></p>

<pre><code>Disk quotas for group user (gid 1000): 

Filesystem  blocks   quota   limit   grace   files   quota   limit   grace
/dev/disk/by-uuid/26fa7362-fbbf-4a9e-af4d-da6c2744263c
8971324* 1048576 1048576    none   43784       0       0  
</code></pre>
","<p>The disk isn't full, but the disk space allowed for this user is full. You need to check <code>quota(1)</code>, perhaps persuade the suspect to clean up their junk, or in an outburst of kindness increase it with <code>edquota(8)</code>.</p>
","67962"
"How to read from two input files using while loop","61615","","<p>I wanted to know if there is any way of reading from two input files in a nested while loop one line at a time. For example, lets say I have two files <code>FileA</code> and <code>FileB</code>. </p>

<p><strong>FileA:</strong></p>

<pre><code>[jaypal:~/Temp] cat filea
this is File A line1
this is File A line2
this is File A line3
</code></pre>

<p><strong>FileB:</strong></p>

<pre><code>[jaypal:~/Temp] cat fileb
this is File B line1
this is File B line2
this is File B line3
</code></pre>

<p><strong>Current Sample Script:</strong></p>

<pre><code>[jaypal:~/Temp] cat read.sh 
#!/bin/bash
while read lineA
    do echo $lineA 
    while read lineB
        do echo $lineB 
        done &lt; fileb
done &lt; filea
</code></pre>

<p><strong>Execution:</strong></p>

<pre><code>[jaypal:~/Temp] ./read.sh 
this is File A line1
this is File B line1
this is File B line2
this is File B line3
this is File A line2
this is File B line1
this is File B line2
this is File B line3
this is File A line3
this is File B line1
this is File B line2
this is File B line3
</code></pre>

<h3>Problem and desired output:</h3>

<p>This loops over FileB completely for each line in FileA. I tried using continue, break, exit but none of them are meant for achieving the output I am looking for. I would like the script to read just one line from File A and then one line from FileB and exit the loop and continue with second line of File A and second line of File B. Something similar to the following script -</p>

<pre><code>[jaypal:~/Temp] cat read1.sh 
#!/bin/bash
count=1
while read lineA
    do echo $lineA 
        lineB=`sed -n ""$count""p fileb`
        echo $lineB
        count=`expr $count + 1`
done &lt; filea

[jaypal:~/Temp] ./read1.sh 
this is File A line1
this is File B line1
this is File A line2
this is File B line2
this is File A line3
this is File B line3
</code></pre>

<p>Is this possible to achieve with while loop?</p>
","<p>If you know for sure that some character will never occur in the first file then you can use paste.</p>

<p>Example of paste using default delimiter tab:</p>

<pre><code>paste file1 file2 | while IFS=""$(printf '\t')"" read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done
</code></pre>

<p>Example of paste using <code>@</code>:</p>

<pre><code>paste -d@ file1 file2 | while IFS=""@"" read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done
</code></pre>

<p>Note that it is enough if the character is guaranteed to not occur in the first file. This is because <code>read</code> will ignore <code>IFS</code> when filling the last variable. So even if <code>@</code> occurs in the second file it will not be split.</p>

<p>Example of paste using some bash features for arguably cleaner code:</p>

<pre><code>while IFS=$'\t' read -r f1 f2
do
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done &lt; &lt;(paste file1 file2)
</code></pre>

<p>Bash features used: <a href=""http://wiki.bash-hackers.org/syntax/quoting#ansi_c_like_strings"">ansi c string</a> (<code>$'\t'</code>) and <a href=""http://wiki.bash-hackers.org/syntax/expansion/proc_subst"">process substitution</a> (<code>&lt;(...)</code>) to <a href=""http://mywiki.wooledge.org/BashFAQ/024"">avoid the while loop in a subshell problem</a>.</p>

<p>If you cannot be certain that any character will never occur in both files then you can use <a href=""http://en.wikipedia.org/wiki/File_descriptor"">file descriptors</a>.</p>

<pre><code>while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  printf 'f1: %s\n' ""$f1""
  printf 'f2: %s\n' ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Not tested much. Might break on empty lines.</p>

<p>File descriptors number 0, 1, and 2 are already used for stdin, stdout, and stderr, respectively. File descriptors from 3 and up are (usually) free. The bash manual warns from using file descriptors greater than 9, because they are ""used internally"".</p>

<p>Note that open file descriptors are inherited to shell functions and external programs. Functions and programs inheriting an open file descriptor can read from (and write to) the file descriptor. You should take care to close all file descriptors which are not required before calling a function or external program.</p>

<p>Here is the same program as above with the actual work (the printing) separated from the meta-work (reading line by line from two files in parallel).</p>

<pre><code>work() {
  printf 'f1: %s\n' ""$1""
  printf 'f2: %s\n' ""$2""
}

while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  work ""$f1"" ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Now we pretend that we have no control over the work code and that code, for whatever reason, tries to read from file descriptor 3.</p>

<pre><code>unknowncode() {
  printf 'f1: %s\n' ""$1""
  printf 'f2: %s\n' ""$2""
  read -r yoink &lt;&amp;3 &amp;&amp; printf 'yoink: %s\n' ""$yoink""
}

while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  unknowncode ""$f1"" ""$f2""
done 3&lt;file1 4&lt;file2
</code></pre>

<p>Here is an example output. Note that the second line from the first file is ""stolen"" from the loop.</p>

<pre><code>f1: file1 line1
f2: file2 line1
yoink: file1 line2
f1: file1 line3
f2: file2 line2
</code></pre>

<p>Here is how you should close the file descriptors before calling external code (or any code for that matter).</p>

<pre><code>while true
do
  read -r f1 &lt;&amp;3 || break
  read -r f2 &lt;&amp;4 || break
  # this will close fd3 and fd4 before executing anycode
  anycode ""$f1"" ""$f2"" 3&lt;&amp;- 4&lt;&amp;-
  # note that fd3 and fd4 are still open in the loop
done 3&lt;file1 4&lt;file2
</code></pre>
","26604"
"What are software and hardware interrupts, and how are they processed?","61505","","<p>I am not sure if I understand the concept of hardware and software interrupts.</p>

<p>If I understand correctly, the purpose of a hardware interrupt is to get some attention of the CPU, part of implementing CPU multitasking. </p>

<ol>
<li>Then what issues a hardware interrupt? Is it the hardware driver process?</li>
<li>If yes, where is the hardware driver process running? If it is running on the CPU, then it won't have to get attention of the CPU by hardware interrupt, right? So is it running elsewhere?</li>
<li>Does a hardware interrupt interrupt the CPU directly, or does it first contact the kernel process and the kernel process then contacts/interrupts the CPU?</li>
</ol>

<hr>

<p>On the other hand, I think the purpose of a software interrupt is for a process currently running on a CPU to request some resources.</p>

<ol>
<li>What are the resources? Are they all in the form of running processes? For example, do CPU driver process and memory driver processes represent CPU and memory resources? Do the driver process of the I/O devices represent I/O resources? Are other running processes that the process would like to communicate with also resources?</li>
<li>If yes, does a software interrupt contact the processes (which represent the resources) indirectly via the kernel process? Is it right that unlike a hardware interrupt, a software interrupt never directly interrupts the CPU, but instead, it interrupts/contacts the kernel process?</li>
</ol>
","<p>A hardware interrupt is not really part of CPU multitasking, but may drive it. </p>

<ol>
<li><p>Hardware interrupts are issued by hardware devices like disk, network cards, keyboards, clocks, etc.  Each device or set of devices will have its own IRQ (Interrupt ReQuest) line.  Based on the IRQ the CPU will dispatch the request to the appropriate hardware driver.  (Hardware drivers are usually subroutines within the kernel rather than a separate process.)</p></li>
<li><p>The driver which handles the interrupt is run on the CPU.  The CPU is interrupted from what it was doing to handle the interrupt, so nothing additional is required to get the CPU's attention. In multiprocessor systems, an interrupt will usually only interrupt one of the CPUs.  (As a special cases mainframes have hardware channels which can deal with multiple interrupts without support from the main CPU.)</p></li>
<li><p>The hardware interrupt interrupts the CPU directly.  This will cause the relevant code in the kernel process to be triggered.  For processes that take some time to process, the interrupt code may allow itself to be interrupted by other hardware interrupts. </p>

<p>In the case of timer interrupt, the kernel scheduler code may suspend the process that was running and allow another process to run.  It is the presence of the scheduler code which enables multitasking.</p></li>
</ol>

<p>Software interrupts are processed much like hardware interrupts.  However, they can only be generated by processes which are currently running. </p>

<ol>
<li><p>Typically software interrupts are requests for I/O (Input or Output).  These will call kernel routines which will schedule the I/O to occur.  For some devices the I/O will be done immediately, but disk I/O is usually queued and done at a later time.  Depending on the I/O being done, the process may be suspended until the I/O completes, causing the kernel scheduler to select another process to run.  I/O may occur between processes and the processing is usually scheduled in the same manner as disk I/O.</p></li>
<li><p>The software interrupt only talks to the kernel.  It is the responsibility of the kernel to schedule any other processes which need to run.  This could be another process at the end of a pipe.  Some kernels permit some parts of a device driver to exist in user space, and the kernel will schedule this process to run when needed.  </p>

<p>It is correct that a software interrupt doesn't directly interrupt the CPU.  Only code that is currently running code can generate a software interrupt. The interrupt is a request for the kernel to do something (usually I/O) for running process.  A special software interrupt is a Yield call, which requests the kernel scheduler to check to see if some other process can run.</p></li>
</ol>

<p>Response to comment:</p>

<ol>
<li><p>For I/O requests, the kernel delegates the work to the appropriate kernel driver.  The routine may queue the I/O for later processing (common for disk I/O), or execute it immediately if possible.  The queue is handled by the driver, often when responding to hardware interrupts.  When one I/O completes, the next item in the queue is sent to the device.</p></li>
<li><p>Yes, software interrupts avoid the hardware signalling step.  The process generating the software request must be a currently running process, so they don't interrupt the CPU.  However, they do interrupt the flow of the calling code.  </p>

<p>If hardware needs to get the CPU to do something, it causes the CPU to interrupt its attention to the code it is running.  The CPU will push its current state on a stack so that it can later return to what it was doing.  The interrupt could stop: a running program; the kernel code handling another interrupt; or the idle process. </p></li>
</ol>
","18023"
"How can I find my local mail spool?","61399","","<p>I am a graduate student and a relative Linux novice.  This question is a sort of spin-off of <a href=""https://unix.stackexchange.com/questions/82835/why-does-this-at-command-not-print-to-the-standard-output"">my earlier question</a>.  My institution has a Ubuntu Linux cluster.  I am just a user; I do not have sysadmin permissions, and I certainly do not have the expertise to be a sysadmin!</p>

<p>My question is, how do I find my local mail spool?  As far as I know, I am not using a mail transfer agent.  <a href=""https://unix.stackexchange.com/questions/82835/why-does-this-at-command-not-print-to-the-standard-output"">Answers to my previous question</a> suggested that I look for a file <code>/var/spool/mail/$USER</code>, but unfortunately, I do not see a file corresponding to my user name.  In <code>/var/spool/mail/</code>, using <code>ls</code> I only see two files: <code>nobody</code> and <code>www-data</code>, which are both extensionless files.</p>

<p>Do you have any other ideas of where I can look for my mail spool (which is probably local, since I do not have a mail transfer agent configured, as far as I know)?</p>
","<p>Mail spools are typically under here:</p>

<pre><code>/var/spool/mail/$USER
</code></pre>

<p>Where <code>$USER</code> is your username. For example on my Fedora Linux system:</p>

<pre><code>$ ls -l /var/spool/mail/
total 1908
-rw-------. 1 root root 1943163 Jul 13 12:00 root
-rw-rw----. 1 rpc  mail       0 Dec 18  2010 rpc
-rw-rw----. 1 saml mail     689 Jul 12 19:38 saml
</code></pre>

<p>Mail spools however are not necessarily local. If you do not have this file then your mail is being maintained on another server. You can either interact with this server using protocols such as <a href=""https://en.wikipedia.org/wiki/IMAP"">IMAP</a> or <a href=""https://en.wikipedia.org/wiki/Post_Office_Protocol"">POP3</a>. </p>

<p>It is on this system where your mail spool is being maintained. </p>

<p>In some environments the mail spool can be shared out as part of a user's home directory, often times in a directory called <code>mail</code> <code>($HOME/mail)</code>. Other times it can be shared out as its own share under something like <code>/mail/users/$USER</code>. </p>

<p>It really depends on how the sysadmins within a given environment choose to do so.</p>
","82912"
"Grep for range of numbers","61386","","<p>How would you search a log file for a date range?</p>

<p>Log file looks like this:</p>

<pre><code>01/14 00:00:01  INFO:     received connect request from 10.10.10.10 
</code></pre>

<p>I need to condense the log to 10 minutes starting at 8:25 and going until 8:35</p>

<p>When I use the following egrep I find too many results, such as<code>00:08:25</code> <code>01:08:25</code> ... <code>08:25:00</code>..</p>

<p>How can I just get <code>08:25:00</code> through <code>08:35:59</code></p>

<p>What I've tried</p>

<pre><code>cat foo | egrep ""08:2[5-9]|08:3[0-5]""
cat foo | egrep ""08:2[5-9]:??|08:3[0-5]:??""
cat foo | egrep ""08:2[5-9]:[0-9][0-9]|08:3[0-5]:[0-9][0-9]""
</code></pre>
","<p>This should yield the desired result.</p>

<pre><code>egrep ""08:[2][5-9]:[0-5][0-9]|08:[3][0-5]:[0-5][0-9]"" foo
</code></pre>

<p>Using <code>cat</code> in this case is not needed.</p>
","120837"
"How to ssh to a server using another server?","61348","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/25055/ssh-via-multiple-hosts"">ssh via multiple hosts</a>  </p>
</blockquote>



<p>For connecting to <em>server B</em> I have to first ssh to <em>server A</em>.
What's the command line to access <em>server B</em>?</p>
","<p>If server B is reachable via <code>ssh</code> and you only need <code>ssh</code> (not direct <code>scp</code> or <code>sftp</code>), this also works very well:</p>

<pre><code>ssh -t $SERVER_A ssh $SERVER_B
</code></pre>

<p>The <code>-t</code> option forces allocation of a pseudo-tty even when running a single command at the other end. This is helpful, since <code>ssh</code> needs a pseudo-tty.</p>

<p>Since you're using two nested instances of <code>ssh</code>, the escape character in the inner session is <kbd>Enter</kbd> <kbd>~</kbd> <kbd>~</kbd> (two tildes). One tilde will send the escape to the first shell.</p>
","41500"
"chroot ""jail"" - what is it and how do I use it?","61286","","<p>I have heard/read a lot about the chroot jail under linux but have never yet used it (I use Fedora day-to-day), so what is a chroot ""jail""? When and why might I use it/not use it and is there anything else I should know? How would I go about creating one?</p>
","<p>A chroot jail is a way to isolate a process and its children from the rest of the system.  It should only be used for processes that don't run as root, as root users can break out of the jail very easily.</p>

<p>The idea is that you create a directory tree where you copy or link in all the system files needed for a process to run.  You then use the <code>chroot()</code> system call to change the root directory to be at the base of this new tree and start the process running in that chroot'd environment.  Since it can't actually reference paths outside the modified root, it can't perform  operations (read/write etc.) maliciously on those locations.</p>

<p>On Linux, using a bind mounts is a great way to populate the chroot tree.  Using that, you can pull in folders like /lib and /usr/lib while not pulling in /user, for example.  Just bind the directory trees you want to directories you create in the jail directory.</p>
","109"
"Parallelize a Bash FOR Loop","61219","","<p>I have been trying to parallelize the following script, specifically each of the three FOR loop instances, using GNU Parallel but haven't been able to. The 4 commands contained within the FOR loop run in series, each loop taking around 10 minutes. </p>

<pre><code>#!/bin/bash

kar='KAR5'
runList='run2 run3 run4'
mkdir normFunc
for run in $runList
do 
  fsl5.0-flirt -in $kar""deformed.nii.gz"" -ref normtemp.nii.gz -omat $run"".norm1.mat"" -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12 
  fsl5.0-flirt -in $run"".poststats.nii.gz"" -ref $kar""deformed.nii.gz"" -omat $run"".norm2.mat"" -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12 
  fsl5.0-convert_xfm -concat $run"".norm1.mat"" -omat $run"".norm.mat"" $run"".norm2.mat""
  fsl5.0-flirt -in $run"".poststats.nii.gz"" -ref normtemp.nii.gz -out $PWD/normFunc/$run"".norm.nii.gz"" -applyxfm -init $run"".norm.mat"" -interp trilinear

  rm -f *.mat
done
</code></pre>
","<p>Why don't you just fork (aka. background) them?</p>

<pre><code>foo () {
    local run=$1
    fsl5.0-flirt -in $kar""deformed.nii.gz"" -ref normtemp.nii.gz -omat $run"".norm1.mat"" -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12 
    fsl5.0-flirt -in $run"".poststats.nii.gz"" -ref $kar""deformed.nii.gz"" -omat $run"".norm2.mat"" -bins 256 -cost corratio -searchrx -90 90 -searchry -90 90 -searchrz -90 90 -dof 12 
    fsl5.0-convert_xfm -concat $run"".norm1.mat"" -omat $run"".norm.mat"" $run"".norm2.mat""
    fsl5.0-flirt -in $run"".poststats.nii.gz"" -ref normtemp.nii.gz -out $PWD/normFunc/$run"".norm.nii.gz"" -applyxfm -init $run"".norm.mat"" -interp trilinear
}

for run in $runList; do foo ""$run"" &amp; done
</code></pre>

<p>In case that's not clear, the significant part is here:</p>

<pre><code>for run in $runList; do foo ""$run"" &amp; done
                                   ^
</code></pre>

<p>Causing the function to be executed in a forked shell in the background.  That's parallel.</p>
","103922"
"Does LVM impact performance?","61072","","<p>I have to migrate a few servers to Linux, and one important aspect that I need to evaluate is that my new host system must have elastic storage capacity. Naturally, doing some basic research, I came across LVM.</p>

<p>Is there any performance penalty for using lvm? If so, how can I measure it?</p>

<p>What I am considering right now is to have Linux as a host OS with LVM and virtualized Linux boxes running on top of it (should I add LVM on the guest OS as well?).</p>
","<p>LVM is designed in a way that keeps it from really getting in the way very much. From the userspace point of view, it looks like another layer of ""virtual stuff"" on top of the disk, and it seems natural to imagine that all of the I/O has to now pass through this before it gets to or from the real hardware.</p>

<p>But it's not like that. The kernel <em>already</em> needs to have a mapping (or several layers of mapping actually) which connects high level operations like ""write this to a file"" to the device drivers which in turn connect to actual blocks on disk.</p>

<p>When LVM is in use, that lookup is changed, but that's all. (Since it has to happen anyway, doing it a bit differently is a negligible performance hit.) <strong>When it comes to actually writing the file, the bits take as direct a path to the physical media as they would otherwise.</strong></p>

<p>There are cases where LVM can cause performance problems. You want to make sure the LVM blocks are aligned properly with the underlying system, which should happen automatically with modern distributions. And make sure you're not using old kernels subject to bugs like <a href=""https://bugzilla.redhat.com/show_bug.cgi?id=232843"">this one</a>. Oh, and using LVM snapshots degrades performance (and increasingly so with each active snapshot). But mostly, the impact should be very small.</p>

<p>As for the last: how can you test? The standard disk benchmarking tool is <a href=""http://www.coker.com.au/bonnie++/"">bonnie++</a>. Make a partition with LVM, test it, wipe that out and (in the same place, to keep other factors identical) create a plain filesystem and benchmark again. They should be close to identical.</p>
","7141"
"Which shell should I use - tcsh vs bash?","61067","","<p>I have been using <code>tcsh</code> for a long time now. But whenever I am searching for something, I often find that the methods specified are <code>bash</code> specific. Even the syntax for the shell scripts is different for the two.</p>

<p>From what I have experienced searching and learning on the internet, <code>bash</code> seems to be the more common shell used. Even the number of questions on this site tagged <code>bash</code> are way more (five times more currently) than the number of questions tagged <code>tcsh</code>.</p>

<p>So, I am wondering whether I should switch to bash. What do you think? </p>

<p><strong>Why should I stick to <code>tcsh</code> OR why should I move over to <code>bash</code>?</strong></p>
","<p>After learning bash I find that tcsh is a bit of a step backwards. For instance what I could easily do in bash I'm finding it difficult to do in tcsh. <a href=""https://unix.stackexchange.com/questions/5111/operate-on-command-output-in-tcsh"">My question on tcsh</a>.  The Internet support and documentation is also much better for bash and very limited for tcsh.  The number of O'Reilly books on bash are great but I have found nothing similar for tcsh.</p>
","5131"
"Using OpenVPN with systemd","61045","","<p>Ok, so I've been searching the web for solutions to this problem with no answers seeming to work for me.  Hopefully someone can help me.  I'm only trying to configure the OpenVPN Client. </p>

<p>I'm running <code>CrunchBang Linux 3.2.0-4-amd64 Debian 3.2.60-1+deb7u1 x86_64 GNU/Linux</code> and I just switched over to using <code>systemd</code>.  The changeover went smooth enough but now I can't get my OpenVPN client to come up using systemd  I've tried following these configuration tutorials, but nothing works.</p>

<ul>
<li><a href=""http://fedoraproject.org/wiki/Openvpn"" rel=""nofollow noreferrer"">http://fedoraproject.org/wiki/Openvpn</a></li>
<li><a href=""http://d.stavrovski.net/blog/how-to-install-and-set-up-openvpn-in-debian-7-wheezy"" rel=""nofollow noreferrer"">http://d.stavrovski.net/blog/how-to-install-and-set-up-openvpn-in-debian-7-wheezy</a></li>
<li>And looked at a bunch of other different guides.</li>
</ul>

<p>I can bring up the tunnel from the command line with <code>openvpn /etc/openvpn/vpn.conf</code>.  So I know the config file is good, it was working with sysvinit just fine so I'm not surprised.  I then attempt to just do a status with <code>systemctl status openvpn@vpn.service</code> resulting in:</p>

<pre><code>$ sudo systemctl status openvpn@vpn.service
  openvpn@vpn.service
Loaded: error (Reason: No such file or directory)
Active: inactive (dead)
</code></pre>

<p>I realized that I need to do some setup for services.  I want to be prompted for a password so I followed this guide to create an <code>openvpn@.service</code> in <code>/etc/systemd/system/</code>.  But restarting the OpenVPN service still doesn't prompt for a password.  </p>

<pre><code>$ sudo service openvpn restart
[ ok ] Restarting openvpn (via systemctl): openvpn.service.
</code></pre>

<p>The Fedora tutorials go through the steps of creating symbolic links, but don't create any of the .service files in the walk-throughs.  </p>

<p>What piece am I missing?  Do I need to create an openvpn@vpn.service?  If so, where exactly do I place it?  I feel like it shouldn't be this difficult, but I can't seem to find any solution that works for me.  I'm happy to provide any more information that's needed.</p>

<h2>Solution</h2>

<pre><code>-rw-r--r--  1 root root   319 Aug  7 10:42 openvpn@.service

[Unit]
Description=OpenVPN connection to %i
After=network.target

[Service]
Type=forking
ExecStart=/usr/sbin/openvpn --daemon ovpn-%i --status /run/openvpn/%i.status 10 --cd /etc/openvpn --config /etc/openvpn/%i.conf
ExecReload=/bin/kill -HUP $MAINPID
WorkingDirectory=/etc/openvpn

[Install]
WantedBy=multi-user.target
openvpn@.service (END)
</code></pre>

<p>Symlink: </p>

<pre><code>lrwxrwxrwx  1 root root   36 Aug  7 10:47 openvpn@vpn.service -&gt; /lib/systemd/system/openvpn@.service
</code></pre>

<h2><strong>Prompt For Password</strong></h2>

<p>Everything is working now, except for being prompted for a password to connect.  I've attempted <a href=""https://bbs.archlinux.org/viewtopic.php?id=150440"" rel=""nofollow noreferrer"">this solution</a>.  I tweaked the file from above just a bit, and added an <a href=""http://en.wikipedia.org/wiki/Expect"" rel=""nofollow noreferrer"">Expect script</a> like in the example.  Working like a charm!  My files are below.</p>

<p>Modified lines from the above <code>/lib/systemd/system/openvpn@.service</code></p>

<pre><code>ExecStart=/usr/sbin/openvpn --daemon ovpn-%i --status /run/openvpn/%i.status 10 --cd /etc/openvpn --management localhost 5559 --management-query-passwords --management-forget-disconnect --config /etc/openvpn/%i.conf
ExecStartPost=/usr/bin/expect /lib/systemd/system/openvpn_pw.exp
</code></pre>

<p>Expect script <code>/lib/systemd/system/openvpn_pw.exp</code>.  Make sure to do the following:</p>

<ul>
<li><code>chmod +x</code> on the script.</li>
<li>Have <code>telnet</code> installed</li>
</ul>

<p>Code of the expect script:</p>

<pre><code>#!/usr/bin/expect
set pass [exec /bin/systemd-ask-password ""Please insert Private Key password: ""]

spawn telnet 127.0.0.1 5559
expect ""Enter Private Key Password:""
send ""password 'Private Key' $pass\r""
expect ""SUCCESS: 'Private Key' password entered, but not yet verified""
send ""exit\r""
expect eof
</code></pre>

<p><strong>It should be noted that the above solution does log your password entered in plaintext in the following logs in <code>/var/log/syslog</code> and <code>/var/log/daemon.log</code></strong></p>
","<p>I think the Debian OpenVPN setup with systemd is currently a tad bit broken. To get it to work on my machines I had to:</p>

<ol>
<li><p>Create <code>/etc/systemd/system/openvpn@.service.d</code> (the directory), and place in it a new file with this:<pre>[Unit]
Requires=networking.service
After=networking.service</pre>I called my file <code>local-after-ifup.conf</code>. It needs to end with <code>.conf</code>. (This is the bit that's currently a tad bit broken.)</p></li>
<li><p>Create a file in <code>/etc/tmpfiles.d</code> (I called mine <code>local-openvpn.conf</code>) with the contents:<pre># Type Path         Mode UID  GID  Age Argument
d      /run/openvpn 0755 root root  -  -</pre>This is <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=741938"" rel=""noreferrer"">Debian bug 741938</a> (fixed in 2.3.3-1).</p></li>
<li><p>Create a symlink into <code>multi-user.target.wants</code> (easiest way is <code>systemctl enable openvpn@CONF_NAME.service</code>) E.g., if you have <code>/etc/openvpn/foo.conf</code>, you'd use <code>openvpn@foo.service</code>.</p></li>
<li><p>If you also have the SysV init script showing up in systemd, disable it. This is <a href=""https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=700888"" rel=""noreferrer"">Debian bug 700888</a> (fixed in 2.3.3-1).</p></li>
</ol>

<p>NOTE: 2.3.3-1 or later is <a href=""https://tracker.debian.org/pkg/openvpn"" rel=""noreferrer"">not yet in testing</a>, though it is in unstable.</p>
","149003"
"How to kill a process that says ""Operation not permitted"" when attempted?","61042","","<p>I have a process I would like to kill:</p>

<pre><code>computer@ubuntu:~$ ps aux | grep socat
root      2092  0.0  0.0   5564  1528 pts/1    T    14:37   0:00 sudo socat TCP:xxx.17.29.152:54321 PTY,link=/dev/ttyGPS0,raw,echo=0,mode=666
computer@ubuntu:~$ kill 2092
-bash: kill: (2092) - Operation not permitted   &lt;--------------- How to kill ??
</code></pre>
","<p>try kill command with with -9 signal if <code>sudo kill 'pid'</code> does not work: 
<code>sudo kill -9 2092</code></p>
","89365"
"How can I safely remove a SATA disk from a running system?","61019","","<p>I sometimes need to plug a disk into a disk bay. At other times, I have the very weird setup of connecting a SSD using a SATA-eSATA cable on my laptop while pulling power from a desktop.</p>

<p>How can I safely remove the SATA disk from the system? This <a href=""http://phoronix.com/forums/showthread.php?21985-Proper-way-to-plug-unplug-an-eSATA-hard-disk"">Phoronix</a> forum thread has some suggestions:</p>

<blockquote>
  <p>justsumdood wrote:</p>
  
  <blockquote>
    <p>An(noymous)droid wrote:<br>
    What then do you do on the software side before unplugging? Is it a simple ""umount /dev/sd""[drive letter]?
    after unmounting the device, to ""power off"" (or sleep) the unit:</p>
  </blockquote>

<pre><code>hdparm -Y /dev/sdX
</code></pre>
  
  <p>(where X represents the device you wish to power off. for example: /dev/sdb)</p>
  
  <p>this will power the drive down allowing for it's removal w/o risk of voltage surge.</p>
</blockquote>

<p>Does this mean that the disk caches are properly flushed and powered off thereafter?</p>

<p>Another suggestion from the same thread:</p>

<blockquote>
  <p>chithanh wrote:<br>
  All SATA and eSATA hardware is physically able to be hotplugged (ie. not damaged if you insert/pull the plug).</p>
  
  <p>How the chipset and driver handles this is another question. Some driver/chipset combinations do not properly handle hotplugging and need a warmplug command such as the following one:</p>

<pre><code>echo 0 - 0 &gt; /sys/class/scsi_host/hostX/scan
</code></pre>
  
  <p>Replace X with the appropriate number for your SATA/eSATA port. </p>
</blockquote>

<p>I doubt whether is the correct way to do so, but I cannot find some proof against it either.</p>

<p>So, what is the correct way to remove an attached disk from a system? Assume that I have already unmounted every partition on the disk and ran <code>sync</code>. Please point to some official documentation if possible, I could not find anything in the Linux documentation tree, nor the <a href=""https://ata.wiki.kernel.org/"">Linux ATA wiki</a>.</p>
","<p>Naturally, you need to unmount any filesystems on the disk, and it'd be a good idea to deactivate any LVM groups (<code>vgchange -an</code>), and generally make sure nothing is using the disk for anything.</p>

<p>Once you've done that, it should be safe to unplug.  If you want to be extra cautious, do <code>echo 1 &gt; /sys/block/(whatever)/device/delete</code> first.  That'll unregister the device from the kernel, so you <em>know</em> nothing's using it when you unplug it.  When I do that with a drive in an eSATA enclosure, I can hear the drive's heads park themselves, so the kernel apparently tells the drive to prepare for power-down.</p>

<p>If you're using an AHCI controller, it should cope with devices being unplugged.  If you're using some other sort of SATA controller, the driver might be confused by hotplugging.</p>

<p>In my experience, SATA hotplugging (with AHCI) works pretty well in Linux.  I've unplugged an optical drive, plugged in a hard drive, scanned it for errors, made a filesystem and copied data to it, unmounted and unplugged it, plugged in a differerent DVD drive, and burned a disc, all with the machine up and running.</p>
","43450"
"Debian not booting into GUI","60896","","<p>My problem is, that my <code>Debian</code> installation shows a grey screen on start up and boots into a console, instead into <code>gnome</code>.</p>

<p>When I start <code>X</code> manually with <code>startx</code> everything starts fine, so the DE seems to be functioning.</p>
","<p>The program where you type your user name and password in a graphical environment, and that logs you into a graphical session, is called a <a href=""https://en.wikipedia.org/wiki/X_display_manager_(program_type)"" rel=""noreferrer"">display manager</a>. You need to install a display manager. On Debian, if you install any of the display manager packages then one of them will be started at boot time.</p>

<p>Any of the packages that provide the <code>x-display-manager</code> virtual package will do. As of Debian jessie, that's <a href=""http://packages.debian.org/gdm3"" rel=""noreferrer"">gdm3</a> (Gnome), <a href=""http://packages.debian.org/kdm"" rel=""noreferrer"">kdm</a> (KDE), <a href=""http://packages.debian.org/lightdm"" rel=""noreferrer"">lightdm</a> (lightweight but themable), <a href=""http://packages.debian.org/slim"" rel=""noreferrer"">slim</a> (lightweight but themable), <a href=""http://packages.debian.org/wm"" rel=""noreferrer"">wdm</a> (lightweight but themable, oldish), <a href=""http://packages.debian.org/xdm"" rel=""noreferrer"">xdm</a> (old-style, bare-bones). You don't have to use a display manager that matches your desktop environment. If in doubt, pick lightdm.</p>
","204408"
"How does the ""tail"" command's ""-f"" parameter work?","60859","","<pre><code>$ tail -f testfile
</code></pre>

<p>the command is supposed to show the latest entries in the specified file, in real-time right? But that's not happening. Please correct me, if what I intend it to do is wrong...</p>

<p>I created a new file ""aaa"" and added a line of text and closed it. then issued this command (first line):</p>

<pre><code>$ tail -f aaa
xxx
xxa
axx
</code></pre>

<p>the last three lines are the contents of the file aaa. Now that the command is still running (since I used <code>-f</code>), I opened the file aaa via the GUI and started adding a few more lines manually. But the terminal doesn't show the new lines added in the file.</p>

<p>What's wrong here? The <code>tail -f</code> command only shows new entries if they are written by system only? (like log files etc)</p>
","<p>From the <code>tail(1)</code> man page:</p>

<blockquote>
<pre><code>   With  --follow  (-f),  tail  defaults to following the file descriptor,
   which means that even if a tail’ed file is renamed, tail will  continue
   to  track  its  end.   This  default behavior is not desirable when you
   really want to track the actual name of the file, not the file descrip-
   tor (e.g., log rotation).  Use --follow=name in that case.  That causes
   tail to track the named file  in  a  way  that  accommodates  renaming,
   removal and creation.
</code></pre>
</blockquote>

<p>Your text editor is renaming or deleting  the original file and saving the new file under the same filename. Use <code>-F</code> instead.</p>
","18762"
"SSH to a host and enter password with shell script","60833","","<p>I want to create a file like this :
sshTOhost :</p>

<pre><code>#!bin/bash
ssh -D 1500 user@host
#enter password (I don't know how)
</code></pre>

<p>can any one help me?</p>
","<p>You can use <a href=""http://www.cyberciti.biz/faq/noninteractive-shell-script-ssh-password-provider/"" rel=""noreferrer"">sshpass</a> to use <code>ssh</code> in a shell script; the script can then automatically provide the password to <code>ssh</code> by using <code>sshpass</code>. The linked article explains how to do this, and also why you absolutely shouldn't do this.</p>

<p>Instead, you should use <a href=""http://www.cyberciti.biz/faq/ssh-password-less-login-with-dsa-publickey-authentication/"" rel=""noreferrer"">public key authentication</a>. If you need to automate it completely without any user input, you can set up a private key without passphrase; your script can then use this key to connect to the remote host without any user input. </p>
","91759"
"Execute a command before shutdown","60809","","<p>I want to execute a simple command just before the computer shuts down (timing is not essential). </p>

<p>For startup, I can use /etc/rc.local; is there something similar for shutdown?</p>

<p>Note that I would still like to use the integrated shutdown button from menu; i.e. I don't want to use a custom script every time I shutdown via terminal - it needs to be automatic.</p>
","<p>Linux Mint is based on Ubuntu, so I'm guesing the runlevel system is probably the same. On Ubuntu, scripts for the different runlevels are executed according to their presence in the <code>/etc/rc[0-6].d</code> directories. <a href=""http://en.wikipedia.org/wiki/Runlevel"">Runlevel</a> 0 corresponds to shutdown, and 6 to reboot.</p>

<p>Typically the script itself is stored in <code>/etc/init.d</code>, and then symlinks are placed in the directories corresponding to the runlevels you require.</p>

<p>So in your case, write your script, store it in <code>/etc/init.d/</code>, then create a symlink in each of <code>/etc/rc0.d</code> and <code>/etc/rc6.d</code> (if you want both) pointing to your script.</p>

<p>The scripts in each runlevel directory will be executed in <a href=""http://en.wiktionary.org/wiki/ASCIIbetical"">asciibetical order</a>, so if the order within the runlevel matters to you, choose the name of your symlink accordingly.</p>
","48974"
"What's ssh port forwarding and what's the difference between ssh local and remote port forwarding","60688","","<p>I feel confused about ssh port forwarding and the difference between ssh local and remote port forwarding. Could you please explain them in detail and with examples? Thanks!</p>
","<h2>I have drawn some sketches</h2>

<p><img src=""https://i.stack.imgur.com/a28N8.png"" alt=""ssh tunnel starting from local""></p>

<hr>

<p><img src=""https://i.stack.imgur.com/4iK3b.png"" alt=""ssh tunnel starting from remote""></p>

<h2>Introduction</h2>

<ol>
<li><p>local: <code>-L Specifies that the given port on the local (client) host is to be forwarded to the given host and port on the remote side.</code></p>

<p><code>ssh -L sourcePort:forwardToHost:onPort connectToHost</code> means: connect with ssh to <code>connectToHost</code>, and forward all connection attempts to the <strong>local</strong> <code>sourcePort</code> to port <code>onPort</code> on the machine called <code>forwardToHost</code>, which can be reached from the <code>connectToHost</code> machine.</p></li>
<li><p>remote: <code>-R Specifies that the given port on the remote (server) host is to be forwarded to the given host and port on the local side.</code></p>

<p><code>ssh -R sourcePort:forwardToHost:onPort connectToHost</code> means: connect with ssh to <code>connectToHost</code>, and forward all connection attempts to the <strong>remote</strong> <code>sourcePort</code> to port <code>onPort</code> on the machine called <code>forwardToHost</code>, which can be reached from your local machine.</p></li>
</ol>

<h2>Examples</h2>

<h3>Example for 1</h3>

<pre><code>ssh -L 80:localhost:80 SUPERSERVER
</code></pre>

<p>You specify that a connection made to the local port 80 is to be forwarded to port 80 on SUPERSERVER. That means if someone connects to your computer with a webbrowser, he gets the response of the webserver running on SUPERSERVER. You, on your local machine, have no webserver running.</p>

<h3>Example for 2</h3>

<pre><code>ssh -R 80:localhost:80 tinyserver
</code></pre>

<p>You specify, that a connection made to the port 80 of tinyserver is to be forwarded to port 80 on your local machine. That means if someone connects to the small and slow server with a webbrowser, he gets the response of the webserver running on your local machine. The tinyserver, which has not enough diskspace for the big website, has no webserver running. But people connecting to tinyserver think so.</p>

<h3>More examples</h3>

<p>Other things could be: The powerful machine has five webservers running on five different ports. If a user connects to one of the five tinyservers at port 80 with his webbrowser, the request is redirected to the corresponding webserver running on the powerful machine. That would be</p>

<pre><code>ssh -R 80:localhost:30180 tinyserver1
ssh -R 80:localhost:30280 tinyserver2
etc.
</code></pre>

<p>Or maybe your machine is only the connection between the powerful and the small servers. Then it would be (for one of the tinyservers that play to have their own webservers):</p>

<pre><code>ssh -R 80:SUPERSERVER:30180 tinyserver1
ssh -R 80:SUPERSERVER:30280 tinyserver2
etc
</code></pre>
","115906"
"How to encrypt messages/text with RSA & OpenSSL?","60557","","<p>I have Alice's public key. I want to send Alice an RSA encrypted message.
How can I do it using the <code>openssl</code> command?</p>

<p>The message is:</p>

<blockquote>
  <p>Hi Alice! Please bring malacpörkölt for dinner!</p>
</blockquote>
","<p>In the <a href=""https://www.openssl.org/docs/manpages.html"" rel=""nofollow noreferrer"">openssl manual</a> (<code>openssl</code> man page), search for <code>RSA</code>, and you'll see that the command for RSA encryption is <code>rsautl</code>. Then read the <a href=""https://www.openssl.org/docs/manmaster/man1/rsautl.html"" rel=""nofollow noreferrer""><code>rsautl</code> man page</a> to see its syntax.</p>

<pre><code>echo 'Hi Alice! Please bring malacpörkölt for dinner!' |
openssl rsautl -encrypt -pubin -inkey alice.pub &gt;message.encrypted
</code></pre>

<p>The default <a href=""http://en.wikipedia.org/wiki/RSA_(cryptosystem)#Padding_schemes"" rel=""nofollow noreferrer"">padding scheme</a> is the original PKCS#1 v1.5 (still used in many procotols); openssl also supports OAEP (now recommended) and raw encryption (only useful in special circumstances).</p>

<p>Note that using openssl directly is mostly an exercise. In practice, you'd use a tool such as <a href=""http://en.wikipedia.org/wiki/GNU_Privacy_Guard"" rel=""nofollow noreferrer"">gpg</a> (which uses RSA, but not directly to encrypt the message).</p>
","12263"
"How do I recursively delete directories with wildcard?","60554","","<p>I am working through SSH on a WD My Book World Edition. Basically I would like to start at a particular directory level, and recursively remove all sub-directories matching <code>.Apple*</code>. How would I go about that?</p>

<p>I tried </p>

<p><code>rm -rf .Apple*</code> and <code>rm -fR .Apple*</code></p>

<p>neither deleted directories matching that name within sub-directories.</p>
","<p><code>find</code> is very useful for selectively performing actions on a whole tree.</p>

<pre><code>find . -type f -name "".Apple*"" -delete
</code></pre>

<p>Here, the <code>-type f</code> makes sure it's a file,  not a directory, and may not be exactly what you want since it will also skip symlinks, sockets and other things. You can use <code>! -type d</code>, which literally means not directories, but then you might also delete character and block devices. I'd suggest looking at the <code>-type</code> predicate on the man page for <code>find</code>.</p>

<p>To do it strictly with a wildcard, you need advanced shell support. Bash v4 has the <a href=""http://www.linuxjournal.com/content/globstar-new-bash-globbing-option""><code>globstar</code> option</a>, which lets you recursively match subdirectories using <code>**</code>. <code>zsh</code> and <code>ksh</code> also support this pattern. Using that, you can do <code>rm -rf **/.Apple*</code>. This is not POSIX-standard, and not very portable, so I would avoid using it in a script, but for a one-time interactive shell action, it's fine.</p>
","23577"
"can we know the password for the other users if we have root access?","60459","","<p>If a person has root access to a particular RHEL machine, will they be able to retrieve the password of the other users?</p>
","<p>TL;DR: No, password are stored as hashes which can (in general) not be recovered.</p>

<p>Linux doesn't store plain-text passwords anywhere <em>by default</em>.  They are hashed or otherwise encrypted through a variety of algorithms.  So, in general, no, this isn't possible with stored data.</p>

<p>If you have passwords stored somewhere other than the <code>/etc/passwd</code> database, they may be stored in a way that allows this.  <code>htpasswd</code> files can contain wealy encrypted passwords, and other applications may store weaker hashes or plain text passwords for various (typically bad) reasons.</p>

<p>Also, user configuration files may contain unencrypted passwords or weakly protected passwords for various reasons - fetchmail grabbing content from another service, <code>.netrc</code>, or simple automated things may include the password.</p>

<p>If the passwords are hashed or encrypted with an older, weak algorithm (3DES, MD5) it would be possible to work out reasonably efficiently / cheaply what the password was - albeit through attacking the data rather than just reversing the transformation.  (eg: things like <a href=""http://project-rainbowcrack.com/"">http://project-rainbowcrack.com/</a> or <a href=""http://www.openwall.com/john/"">http://www.openwall.com/john/</a>)</p>

<p>Since you are root it is also possible to attack the user password at another level - replace the login binary, or sudo, or part of PAM, etc, with something that will capture the password when it is entered.</p>

<p>So, in specific, no, but in general having root access does make it easier to get at the users details through various side-channels.</p>
","36047"
"What is the need for `fakeroot` command in linux","60311","","<p>Why do we need <code>fakeroot</code> command at all? Can't we simply use the <code>sudo</code> or <code>su</code> commands?</p>

<p>The man page says:</p>

<blockquote>
  <p>fakeroot - run a command in an environment faking root privileges for <strong>file manipulation</strong></p>
</blockquote>

<p>About.com says: </p>

<blockquote>
  <p>Gives a fake root environment. This package is intended to enable something like: <code>dpkg-buildpackage -rfakeroot</code> i.e. to remove the need to become root for a package build. This is done by setting <code>LD_PRELOAD</code> to <code>libfakeroot.so</code>, which provides wrappers around <code>getuid</code>, <code>chown</code>, <code>chmod</code>, <code>mknod</code>, <code>stat</code>, ..., thereby creating a fake root environment. If you don't understand any of this, you do not need <code>fakeroot</code>!</p>
</blockquote>

<p>My question is, what special purpose does it solve that a simple <code>su</code> or <code>sudo</code> don't? For example, for repacking all installed packages in ubuntu we give following command:</p>

<pre><code>$ fakeroot -u dpkg-repack `dpkg --get-selections | grep install | cut -f1`
</code></pre>

<p>Can we do the above command with sudo or su instead of fakeroot like this:</p>

<pre><code>$ sudo dpkg-repack `dpkg --get-selections | grep install | cut -f1`
</code></pre>

<p><strong>EDIT:</strong></p>

<p>Running:</p>

<pre><code>$ sudo dpkg-repack `dpkg --get-selections | grep install | cut -f1`
</code></pre>

<p>gives me this error:</p>

<blockquote>
  <p>control directory has bad permissions 700 (must be >=0755 and &lt;=0775)</p>
</blockquote>

<p>Any reason why?</p>
","<p>Imagine that you are a developer/package maintainer, etc. working on a remote server. You want to update the contents of a package and rebuild it, download and customize a kernel from kernel.org and build it, etc. While trying to do those things, you'll find out that some steps require you to have <code>root</code> rights (<code>UID</code> and <code>GID</code> 0) for different reasons (security, overlooked permissions, etc). But it is not possible to get <code>root</code> rights, since you are working on a remote machine (and many other users have the same problem as you). This is what exactly <code>fakeroot</code> does: it pretends an effective <code>UID</code> and <code>GID</code> of 0 to the environment which requires them. </p>

<p>In practice you never get real <code>root</code> privileges (in opposite to <code>su</code> and <code>sudo</code> that you mention).</p>
","9720"
"How to replace one char with another in all filenames of the current directories?","60311","","<p>How do you rename all files/subdirs in the current folder?</p>

<p>Lets say, I have many files and subdirs that are with spaces and I want to replace all the spaces with an underscore.</p>

<pre><code>File 1
File 2
File 3
Dir 1
Dir 3
</code></pre>

<p>should be renamed to</p>

<pre><code>File_1
File_2
File_3
Dir_1
Dir_3
</code></pre>
","<p>If you need to rename files in subdirectories as well, then you can do</p>

<pre><code>find /search/path -depth -name '* *' \
    -execdir bash -c 'mv ""$1"" ""${1// /_}""' _ {} \;
</code></pre>

<p>Thank to @glenn jackman for suggesting <code>-depth</code> option for <code>find</code> and to make me think.</p>
","19059"
"How to combine 2 -name conditions in find?","60221","","<p>I would like to search for files that would not match 2 <code>-name</code> conditions. I can do it like so : </p>

<pre><code>find /media/d/ -type f -size +50M ! -name ""*deb"" ! -name ""*vmdk""
</code></pre>

<p>and this will yield proper result but can I join these 2 condition with OR somehow ?</p>
","<p>You can do this using a negated <code>-regex</code>, too:-</p>

<pre><code> find ./ ! -regex  '.*\(deb\|vmdk\)$'
</code></pre>
","50633"
"How can I populate a file with random data?","60190","","<p>How can I create a new file and fill it with 1 Gigabyte worth of random data? I need this to test some software.</p>

<p>I would prefer to use <code>/dev/random</code> or <code>/dev/urandom</code>.</p>
","<p>On most unices:</p>

<pre><code>head -c 1G &lt;/dev/urandom &gt;myfile
</code></pre>

<p>If your <code>head</code> doesn't understand the <code>G</code> suffix you can specify the size in bytes:</p>

<pre><code>head -c 1073741824 &lt;/dev/urandom &gt;myfile
</code></pre>

<p>If your <code>head</code> doesn't understand the <code>-c</code> option (it's common but not POSIX; you probably have OpenBSD):</p>

<pre><code>dd bs=1024 count=1048576 &lt;/dev/urandom &gt;myfile
</code></pre>

<p><a href=""https://security.stackexchange.com/questions/3936/is-a-rand-from-dev-urandom-secure-for-a-login-key/3939#3939"">Do not use <code>/dev/random</code> on Linux, use <code>/dev/urandom</code>.</a></p>
","33634"
"How do you kick a benign user off your system?","60041","","<p>I was googling this a bit ago and noticed a couple of ways, but I'm guessing that google doesn't know all. So how do <em>you</em> kick users off your Linux box? also how do you go about seeing they are logged in in the first place? and related... does your method work if the user is logged into an X11 DE (not a requirement I'm just curious)?</p>
","<p>There's probably an easier way, but I do this:</p>

<ol>
<li><p>See who's logged into your machine -- use <code>who</code> or <code>w</code>:</p>

<pre><code>&gt; who  
mmrozek  tty1         Aug 17 10:03  
mmrozek  pts/3        Aug 17 10:09 (:pts/2:S.0)
</code></pre></li>
<li><p>Look up the process ID of the shell their TTY is connected to:</p>

<pre><code>&gt; ps t  
PID   TTY      STAT   TIME COMMAND  
30737 pts/3    Ss     0:00 zsh
</code></pre></li>
<li><p>Laugh at their impending disconnection (this step is optional, but encouraged)</p>

<pre><code>&gt; echo ""HAHAHAHAHAHAHAHA"" | write mmrozek pts/3
</code></pre></li>
<li><p>Kill the corresponding process:</p>

<pre><code>&gt; kill -9 30737
</code></pre></li>
</ol>

<hr>

<p>I just discovered you can combine steps 1 and 2 by giving <code>who</code> the <code>-u</code> flag; the PID is the number off to the right:</p>

<pre><code>&gt; who -u
mmrozek  tty1         Aug 17 10:03 09:01        9250
mmrozek  pts/18       Aug 17 10:09 01:46       19467 (:pts/2:S.0)
</code></pre>
","622"
"Auto indent / format code for Vim?","59938","","<p>I'm trying to use Vim more and more when I can. One of my biggest grip between Vim and an IDE like Aptana is the ability to auto indent.</p>

<p>Is there a means of auto formatting code (HTML, CSS, PHP) so it is properly indented?</p>

<p>If so how do you install this into vim? I don't understand plugins very much. 
I tried reviewing this thread and it confused me more: <a href=""https://unix.stackexchange.com/questions/17870/how-to-change-vim-auto-indent-behavior"">How to change vim auto-indent behavior?</a></p>
","<p>To indent the whole file automatically:</p>

<pre><code>gg
=G
</code></pre>

<p>Explained:</p>

<ul>
<li><code>g</code> - go to  </li>
<li><code>gg</code> - go to beginning of the file  </li>
<li><code>G</code> - go to end of the file  </li>
</ul>
","19963"
"What's the easiest way to resize an ext4 partition from the command line?","59937","","<p>What's the easiest way to resize an ext4 partition (or any type partition depending on the method) from the command line (potentially with the fewest commands, but also the easiest to understand)?</p>

<p>Using a tool like Gparted is obviously easy in a GUI, but what about in the command line? I guess text-based GUIs can count for the answer too since it's technically still in the command line. It just needs to be easy.</p>

<p>By partition I mean a simple partition on a single disk of a personal computer (e.g. on a laptop). For example, I want to resize <code>/dev/sda4</code>. There's no RAIDs, there's not more than one disk drive, there's not anything complicated here. Just a simple partition on a single disk (/dev/sdaX on /dev/sda).</p>
","<p>You can use <code>fdisk</code> to change your partition table while running.Refer this link <a href=""http://codesilence.wordpress.com/2013/03/14/live-resizing-of-an-ext4-filesytem-on-linux/"" rel=""nofollow noreferrer"">http://codesilence.wordpress.com/2013/03/14/live-resizing-of-an-ext4-filesytem-on-linux/</a></p>
","139468"
"""rsync: failed to set permissions on ..."" error with rsync -a or -p option","59904","","<p>When I use the -a option as is asked and answered in <a href=""https://unix.stackexchange.com/questions/12198/preserve-the-permissions-with-rsync"">Preserve the permissions with rsync</a>, I got a lot of ""rsync: failed to set permissions on"" errors.</p>

<pre><code>rsync: failed to set permissions on ""/ata/text/RCS/jvlc,v"": Operation not permitted (1)
rsync: failed to set permissions on ""/ata/text/RCS/jvm,v"": Operation not permitted (1)
rsync: failed to set permissions on ...
</code></pre>

<p>Why is this? The files are normal files with permission of 0664.</p>
","<p>Most likely, rsync on the destination end is not running as a user with permission to <code>chmod</code> those files (which would have to be either the file's owner or root).</p>
","12232"
"Set default kernel in GRUB","59864","","<p>How can I pick which kernel GRUB2 should load by default? I recently installed a the linux realtime kernel and now it loads by default. I'd like to load the regular one by default.</p>

<p>So far I only managed to pick the default OS.. and for some reason the <code>/boot/grub.cfg</code> already assumes that I want to load the rt-kernel and put it into the generic linux menu entry (in my case Arch Linux).</p>
","<p>jkt123's will work for most distributions I guess. However for Arch Linux it didn't work, at least not with the packages I have available.</p>

<p>The indices you can set with <code>grub-set-default</code> only correspond to the main menu entries. The kernel options are however in a submenu. So either you move the kernel entry out of the submenu into the main menu or you put the entry on top of the submenu list and select the submenu.</p>

<p><strong>My Grub Menu</strong></p>

<ul>
<li>Arch Linux</li>
<li>Advanced options for Arch Linux
<ul>
<li>Kernel 1</li>
<li>Kernel 2</li>
</ul></li>
<li>Windows</li>
</ul>

<p>To be able to boot Kernel 2 you have to either swap it with Kernel 1 or you put it outside the submenu on the same level as Arch Linux or Windows. And then set the default number to one of the main menu indices. For example in the menu above ""0"" boots ""Arch Linux"" and ""1"" boots Kernel 1.</p>

<p>To change the hierarchy and swap, open <code>/boot/grub/grub.cfg</code> and move the entry you wish to move. An entry could look like this</p>

<pre><code>menuentry 'ENTRY NAME'
   ... some code ...
}
</code></pre>

<p>Then you need to apply your changes. In my case with <code>grub-mkconfig</code>. But this might vary from system to system.</p>
","198100"
"NFS Server changes in /etc/exports file need Service Restart?","59844","","<p>I have NFSv4 Server (on RHELv6.4) and NFS Clients on (CentOSv6.4). Let's say in <code>/etc/exports</code>:</p>

<pre><code>/shares/website1      &lt;ip-client-1&gt;(rw,sync,no_subtree_check,no_root_squash)
/shares/website2      &lt;ip-client-2&gt;(rw,sync,no_subtree_check,no_root_squash)
</code></pre>

<p>Then whenever i made some changes in that (let's say the changes ONLY for <code>client-2</code>), e.g: </p>

<pre><code>/shares/website1      &lt;ip-client-1&gt;(rw,sync,no_subtree_check,no_root_squash)
/shares/xxxxxxxx      &lt;ip-client-2&gt;(rw,sync,no_subtree_check,no_root_squash)
</code></pre>

<p>Then i always <code>service nfs restart</code>. And then eventually .. the mount-point on <code>client-1</code> got <strong>unresponsive</strong> <em>(Can't open its files, etc)</em>. <em>(Why? Because of RESTART?)</em></p>

<p>But as described, i only modified the line for <code>client-2</code> only. Everything for the <code>client-1</code> are still untouched.</p>

<p>So my questions here are:</p>

<ul>
<li>Whenever i modify the <code>/etc/exports</code>, should i <code>restart</code> the service or what?</li>
<li>If i <code>service nfs restart</code>, why the Mount-Point on other Clients are eventually affected? <em>(For those Client Machines with NO changes made in <code>/etc/exports</code> for them.)</em></li>
</ul>

<p>That means, whenever i make the changes in <code>/etc/exports</code> and <code>restart</code> the service, i will need to go <strong>RE-MOUNT</strong> the directories on <strong>EVERY CLIENTS</strong> in the export list, in order to have the mount-points working again.</p>

<p>Any idea, please?</p>
","<p>You shouldn't need to restart NFS every time you make a change to <code>/etc/exports</code>. All that's required is to issue the appropriate command after editing the <code>/etc/exports</code> file:</p>

<pre><code>$ exportfs -ra
</code></pre>

<p>Excerpt from the official Red Hat documentation titled: <a href=""https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/5/html/Deployment_Guide/s1-nfs-server-config-exports.html"">21.7. The /etc/exports Configuration File</a>.</p>

<p><em>excerpt</em></p>

<blockquote>
  <p>When issued manually, the /usr/sbin/exportfs command allows the root user to selectively export or unexport directories without restarting the NFS service. When given the proper options, the /usr/sbin/exportfs command writes the exported file systems to /var/lib/nfs/xtab. Since rpc.mountd refers to the xtab file when deciding access privileges to a file system, changes to the list of exported file systems take effect immediately.</p>
</blockquote>

<p>Also read the <code>exportfs</code> man page for more details, specifically the ""DESCRIPTION"" section which explains all this and more.</p>

<blockquote>
  <p>DESCRIPTION
        An  NFS server maintains a table of local physical file systems that are 
          accessible to NFS clients.  Each file system in this table is  referred 
          to as an exported file system, or export, for short.</p>

<pre><code>  The exportfs command maintains the current table of exports for the NFS 
    server.  The master export table is kept in  a  file  named
    /var/lib/nfs/etab.  This file is read by rpc.mountd when a client sends 
    an NFS MOUNT request.

  Normally  the  master  export  table  is  initialized  with the contents 
    of /etc/exports and files under /etc/exports.d by invoking exportfs -a.  
    However, a system administrator can choose to add or delete exports 
    without modifying  /etc/exports  or  files  under /etc/exports.d by 
    using the exportfs command.
</code></pre>
</blockquote>

<p>Also take note of the options we're using, <code>-ra</code>:</p>

<pre><code>   -a     Export or unexport all directories.
   -r     Reexport all directories, synchronizing /var/lib/nfs/etab with 
          /etc/exports and files  under  /etc/exports.d.   This  option
          removes  entries  in  /var/lib/nfs/etab which have been deleted 
          from /etc/exports or files under /etc/exports.d, and removes
          any entries from the kernel export table which are no longer
          valid.
</code></pre>
","116975"
"How to make unix service see environment variables?","59838","","<p>I have set my environment variable using <code>/etc/profile</code>:</p>

<pre><code>export VAR=/home/userhome
</code></pre>

<p>Then if I do <code>echo $VAR</code> it shows <code>/home/userhome</code></p>

<p>But when I put reference to this variable into the <code>/etc/init.d/servicename</code> file, it cannot find this variable. When I run <code>service servicename status</code> using <code>/etc/init.d/servicename</code> file with following content: </p>

<pre><code>case ""$1"" in
status)    
    cd $VAR/dir
    ;;
esac
</code></pre>

<p>it says <code>/dir: No such file or directory</code></p>

<p>But it works if I run <code>/etc/init.d/servicename status</code> instead of <code>service servicename status</code></p>

<p>How can I make unix service see environment variables? </p>
","<p>The problem is <code>service</code> strips all environment variables but <code>TERM</code>, <code>PATH</code> and <code>LANG</code> which is a good thing. If you are executing the script directly nothing removes the environment variables so everything works.</p>

<p>You don't want to rely on external environment variables because at startup the environment variable probably isn't present and your init system probably won't set it anyway.</p>

<p>If you still want to rely on such variables, source a file and read the variables from it, e.g. create <code>/etc/default/servicename</code> with the content:</p>

<pre><code>VAR=value
</code></pre>

<p>and source it from your init script, e.g:</p>

<pre><code>[ -f /etc/default/service-name ] &amp;&amp; . /etc/default/service-name

if [ -z ""$VAR"" ] ;  then
  echo ""VAR is not set, please set it in /etc/default/service-name"" &gt;&amp;2
  exit 1
fi

case ""$1"" in
status)    
    cd ""$VAR""/dir
    ;;
esac
</code></pre>
","44378"
"How to turn off color with `ls`?","59822","","<p>It is normally nice to have color output from <code>ls</code>, <code>grep</code>, etc.  But when you don't want it (such as in a script where you're piping the results to another command) is there a switch that can turn it off?  <code>ls -G</code> turns it on (with BSD-derived versions of <code>ls</code>) if it's not the default, but <code>ls +G</code> does not turn it off.  Is there anything else that will?</p>
","<p>Color output for <code>ls</code> is typically enabled through an alias in most distros nowadays.</p>

<pre><code>$ alias ls
alias ls='ls --color=auto'
</code></pre>

<p>You can always disable an alias temporarily by prefixing it with a backslash.</p>

<pre><code>$ \ls
</code></pre>

<p>Doing the above will short circuit the alias just for this one invocation. You can use it any time you want to disable any alias.</p>
","107372"
"How to check that a daemon is listening on what interface?","59715","","<p>Ex.: an sshd is configured to only listen on wlan0. So. Besides checking the sshd_config how can I check that a daemon is listening on what inerface? netstat can do it? how? (OS: openwrt or scientific linux or openbsd)</p>

<p>UPDATE: </p>

<p>I thought sshd could be limited to an interface... but no... (192.168.1.5 is on wlan0...)</p>

<pre><code># grep ^ListenAddress /etc/ssh/sshd_config 
ListenAddress 192.168.1.5:22
# 
# lsof -i -n -P
COMMAND     PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME
sshd      23952 root    3u  IPv4 1718551      0t0  TCP 192.168.1.5:22 (LISTEN)
#
# ss -lp | grep -i ssh
0      128              192.168.1.5:ssh                           *:*        users:((""sshd"",23952,3))
# 
# netstat -lp | grep -i ssh
tcp        0      0 a.lan:ssh                   *:*                         LISTEN      23952/sshd          
#
</code></pre>
","<p>(you might have to install the package <code>ip</code> on openwrt (v12 / attitude adjustment)</p>

<p>ifconfig/netstat etc. are considered <a href=""http://en.wikipedia.org/wiki/Ifconfig#Current_status"">deprecated</a>, so you should use (as root)</p>

<pre><code>ss -nlput | grep sshd
</code></pre>

<p>to show the TCP/UDP sockets on which a running program which contains the string <code>sshd</code> is listening to</p>

<ul>
<li><code>-n</code><br />no port to name resolution</li>
<li><code>-l</code><br />only listening sockets</li>
<li><code>-p</code><br />show processes listening</li>
<li><code>-u</code><br />show udp sockets</li>
<li><code>-t</code><br />show tcp sotckets</li>
</ul>

<p>Then you geht a list like this one:</p>

<pre><code>tcp    LISTEN     0      128                    *:22                    *:*      users:((""sshd"",3907,4))
tcp    LISTEN     0      128                   :::22                   :::*      users:((""sshd"",3907,3))
tcp    LISTEN     0      128            127.0.0.1:6010                  *:*      users:((""sshd"",4818,9))
tcp    LISTEN     0      128                  ::1:6010                 :::*      users:((""sshd"",4818,8))
</code></pre>

<p>the interesting thing is the 5th column which shows a combination of IP address and port:</p>

<ol>
<li><code>*:22</code><br />listen on port 22 on every available IPv4 address</li>
<li><code>:::22</code><br />listen on port 22 on every available IP address (i do not write IPv6, as IP is IPv6 per <a href=""http://tools.ietf.org/html/rfc6540"">RFC 6540</a>)</li>
<li><code>127.0.0.1:6010</code><br />listen on IPv4 address 127.0.0.1 (localhost/loopback) and port 6010</li>
<li><code>::1:6010</code><br />listen on IP address ::1 (0:0:0:0:0:0:0:1 in full notation, also localhost/loopback) and port 6010</li>
</ol>

<p>You then want to know which interfaces has an IPv4 address (to cover 1.)</p>

<pre><code>ip -4 a
# or ""ip -4 address""
# or ""ip -4 address show""
</code></pre>

<p>or an IP address (to cover 2.)</p>

<pre><code>ip -6 a
# or ""ip -6 address
# or ""ip -6 address show
</code></pre>

<p>(if you do not add the option for IP (<code>-6</code>) or IPv4 (<code>-4</code>) both are shown)</p>

<p>You can also have an look that output and search for e.g. <code>127.0.0.1</code> or any other IP/IPv4-address</p>

<pre><code># here a demo where i show all addresses of the device ""lo"" (loopback)
ip a show dev lo
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
</code></pre>

<p>The lines beginning with <code>inet</code> and <code>inet6</code> show that these IPs are bound to this interface, you may have many of these lines per interface:</p>

<pre><code>he-ipv6: &lt;POINTOPOINT,NOARP,UP,LOWER_UP&gt; mtu 1480 qdisc noqueue state UNKNOWN
    link/sit 192.0.2.1 peer 192.0.2.3
    inet6 2001:db8:12::1/64 scope global
       valid_lft forever preferred_lft forever
    inet6 2001:db8::2/64 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::1111:1111/128 scope link
       valid_lft forever preferred_lft forever
</code></pre>

<p>and in a script:</p>

<pre><code>address=""127.0.0.1""
for i in $(grep ':' /proc/net/dev | cut -d ':' -f 1 | tr -d ' ') ; do
        if $(ip address show dev $i | grep -q ""${address}"") ; then
                echo ""${address} found on interface ${i}""
        fi
done
</code></pre>

<p>(replace ""127.0.0.1"")</p>
","61540"
"Unable to resolve hostname","59681","","<p>So I try to do:</p>

<pre><code>ssh $(hostname)
</code></pre>

<p>and it tells me:</p>

<pre><code>ssh: Could not resolve hostname woofy: Name or service not known
</code></pre>

<p>It knows that its own hostname is ""woofy""; why can't it connect to itself?</p>
","<p>So we know <code>hostname</code> returns <code>woofy</code>.   But that name can't be resolved to an IP address.</p>

<p>The short answer is that you need to add an entry for <code>woofy</code> in <code>/etc/hosts</code>.   Make it resolve to 127.0.0.1.     Or if your system is IPv6-capable, ::1.</p>

<p>Keep a backup of the previous version of <code>/etc/hosts</code> in case you make a mistake (I use the handy <code>etckeeper</code> package for this, but if you prefer it ye olde way, you can use a manual backup or even RCS).</p>

<p>The long answer is that how hostnames are resolved to IP addresses is controlled by a set of configuration files which vary slightly between Unix variants.   You can configure your Unix system to resolve host names by hosts file (<code>/etc/hosts</code> will work on almost any Unix system) or by DNS (systems which have direct IP reachability to the Internet will always do this).   There are other alternatives too, mostly less widely used (including LDAP and NIS/NIS+).  See the Wikipedia article about the <a href=""http://en.wikipedia.org/wiki/Name_Service_Switch"" rel=""nofollow"">Name Service Switch</a> for more context on this.</p>

<p><strong>Edit:</strong> if this still causes a DNS lookup, the problem is probably that your Name Service Switch configuration consults DNS before <code>/etc/hosts</code> so the change to <code>/etc/hosts</code> has no effect.  Try looking at <code>/etc/nsswitch.conf</code> (how the NSS is configured varies between operating systems).</p>
","38277"
"What is the basic difference between Arch and Gentoo Linux?","59677","","<p>What is the difference between Arch Linux and Gentoo Linux? Their ideologies seem quite similar to me.</p>
","<p>Yes, the distros are of similar, with both being set to satisfy more experienced users, and both aim to be fast and highly customizable. Th most technical similarity is that both are based upon the Linux Kernel. </p>

<p>While most functions may seem similar, the two are different in many ways.</p>

<ol>
<li><p>Apparently, Gentoo documentation is said to be very intimidating to new users, while Arch documentation is very much up to the KISS (Keep it simple, stupid) motto.</p></li>
<li><p>Package managers are also different. Arch Linux uses the Pacman (or in some spins, such as antergos, Pacman XG) which uses the good <strong>precompiled package system</strong> while Gentoo uses the Portage manager which makes <strong>packages from source code</strong>. </p>

<p>With the difference in package managers one distribution may have fewer packages readied than the other. I would say that Arch would have a larger selection of packages compared to Gentoo, but the selection of individual packages may be different as well.</p>

<p>However, most packages are available in source code. So you can fairly easily build them to suit whatever package manager you may be using.</p>

<p>(If you may be interested, Gentoo's portage manager has many good features not available in the freshly-installed pacman)</p></li>
<li><p><strong>Popularity</strong> is a difference. While you may be interested in being original, the adoption of your OS can make a big difference in your Linux experience. Primarily in how many files you can access out-of-the-disk and how many tutorials you have to look at in times of need.</p>

<p>According to distrowatch, <strong>Arch Linux is the 8th</strong> in overall popularity, while <strong>Gentoo is at 47th</strong>. </p>

<p>While popularity may help, this may not help you to easily choose a distro. I haven't personally tried Gentoo, it could just be an amazingly functional and simple OS, while Arch had risen up much further with its head-start.</p></li>
<li><p>Gentoo has a good variety of officially supported desktops from AfterStep and BlackBox, to Gnome and Xfce. Arch Linux officially supports mostly the major Desktops. (Probably because it is popularly adopted as a command-line system).</p></li>
<li><p>I could list many more differences, but aside from the above (and maybe other) differences, the distributions are quite similar.</p></li>
</ol>

<p>If you would like a good resource to make comparisons with, I recommend <a href=""http://www.distrowatch.com"" rel=""nofollow noreferrer"">distrowatch.com</a>, if you haven't looked at it already.</p>
","137661"
"""Swap file xxx already exists"" when editing apache configuration file in vim?","59619","","<p>Using vim I keep getting a message saying ""Swap file xxx already exists"" when I'm editing an apache config.  However, I don't see it in the working directory on in tmp.  How do I delete this?</p>
","<p>Vim swap files are normally hidden (Unix hidden files begin with a <code>.</code>). In order to view hidden files as well as regular ones, you need to <code>ls -A</code> (mnemonic: A for All). That should show you whether a swap file is there or not.</p>
","33023"
"rsync ignore owner, group, time, and perms","59555","","<p>I want to know how use rsync for sync to folders recursive but
I only need to update the new files or the updated files (only the content not the owner, group or timestamp) and I want to delete the files that not exist in the source.</p>
","<p>I think you can use the <code>-no-</code> options to <code>rsync</code> to <em>NOT</em> copy the ownership or permissions of the files you're sync'ing.</p>

<h3>Excerpt From rsync Man Page</h3>

<pre><code>--no-OPTION
       You may turn off one or more implied options by prefixing the option 
       name with ""no-"".  Not all options may be pre‐fixed  with  a  ""no-"":  
       only options that are implied by other options (e.g. --no-D, 
       --no-perms) or have different defaults in various circumstances (e.g. 
       --no-whole-file, --no-blocking-io, --no-dirs).  You may specify 
       either the short or the long option name after the ""no-"" prefix (e.g. 
       --no-R is the same as --no-relative).

       For example: if you want to use -a (--archive) but don’t want -o 
       (--owner), instead of converting -a into -rlptgD, you could specify 
       -a --no-o (or -a --no-owner).

       The order of the options is important:  if you specify --no-r -a, the 
       -r option would end up being turned on,  the opposite  of  -a  
       --no-r.   Note  also  that the side-effects of the --files-from 
       option are NOT positional, as it affects the default state of several 
       options and slightly changes the meaning of -a (see the  --files-from
       option for more details).
</code></pre>

<h3>Ownership &amp; Permissions</h3>

<p>Looking through the man page I believe you'd want to use something like this:</p>

<pre><code>$ rsync -avz --no-perms --no-owner --no-group ...
</code></pre>

<p>To delete files that don't exist you can use the <code>--delete</code> switch:</p>

<pre><code>$ rsync -avz --no-perms --no-owner --no-group --delete ....
</code></pre>

<h3>Timestamps</h3>

<p>As for the timestamp I don't see a way to keep this without altering how you'd do the comparison of SOURCE vs. DEST files. You might want to tell <code>rsync</code> to ignore timestamps using this switch:</p>

<pre><code>-I, --ignore-times
       Normally  rsync will skip any files that are already the same size 
       and have the same modification timestamp.  This option turns off this 
       ""quick check"" behavior, causing all files to be updated.
</code></pre>

<h3>Update</h3>

<p>For timestamps, <code>--no-times</code> might do what you're looking for.</p>
","102214"
"wkhtmltopdf - QXcbConnection: Could not connect to display","59507","","<p>How to run <code>wkhtmltopdf</code> headless?!</p>

<p>Installation on Debian Whezzy</p>

<pre><code>apt-get install wkhtmltopdf
</code></pre>

<p>Command</p>

<pre><code>wkhtmltopdf --title ""$SUBJECT"" -q $SOURCEFILE $OUTPUTFILE
</code></pre>

<p>Error</p>

<pre><code>QXcbConnection: Could not connect to display
</code></pre>
","<p>This is <a href=""https://github.com/wkhtmltopdf/wkhtmltopdf/issues/2037"">a bug</a>, and the fix hasn't been brought to the Debian repositories. Quoting <em>ashkulz</em> (who closed the bug report) :</p>

<blockquote>
  <p>You're using the version of wkhtmltopdf in the debian repositories, which does not support running headless.</p>
</blockquote>

<p>So you can either...</p>

<ul>
<li><a href=""https://github.com/wkhtmltopdf/wkhtmltopdf"">Download <code>wkhtmltopdf</code> from source</a> and compile it (see the instructions in <a href=""https://github.com/wkhtmltopdf/wkhtmltopdf/blob/master/INSTALL.md"">the INSTALL.md file</a> ; you may remove the <code>--recursive</code> option from their <code>git clone</code> line, if you already have Qt 4.8 installed).</li>
<li>Run it inside <code>xvfb</code>, <a href=""https://github.com/wkhtmltopdf/wkhtmltopdf/issues/2037#issuecomment-62019521"">as suggested by <em>masterkorp</em> in the bug report</a>.</li>
</ul>
","192646"
"Mounting all partitions on hard disk automatically on Linux Mint","59481","","<p><strong>Issue</strong><br>
I have a Linux Mint installation. Every time that I boot, I need to manually mount the two partitions on my computer(<code>New volume D</code> and <code>Drive C</code>). If I  don't do this, these drives don't show up anywhere. I want to know if there is some way to automate this process.</p>

<p><strong>Goal</strong><br>
Automatically mounting all the partitions on the hard disk each time I boot.</p>

<p><strong>Specs</strong><br>
Linux Mint 14 dual boot with Windows XP SP3</p>
","<p>You can do this through the file <code>/etc/fstab</code>. Take a look at this <a href=""http://www.tuxfiles.org/linuxhelp/fstab.html"">link</a>. This <a href=""http://www.cyberciti.biz/faq/linux-finding-using-uuids-to-update-fstab/"">tutorial</a> also has good details.</p>

<h3>Example steps</h3>

<p>First you need to find out the UUID of the hard drives. You can use the command <code>blkid</code> for this. For example:</p>

<pre><code>% sudo blkid
/dev/sda1: TYPE=""ntfs"" UUID=""A0F0582EF0580CC2""
/dev/sda2: UUID=""8c2da865-13f4-47a2-9c92-2f31738469e8"" SEC_TYPE=""ext2"" TYPE=""ext3""
/dev/sda3: TYPE=""swap"" UUID=""5641913f-9bcc-4d8a-8bcb-ddfc3159e70f""
/dev/sda5: UUID=""FAB008D6B0089AF1"" TYPE=""ntfs""
/dev/sdb1: UUID=""32c61b65-f2f8-4041-a5d5-3d5ef4182723"" SEC_TYPE=""ext2"" TYPE=""ext3""
/dev/sdb2: UUID=""41c22818-fbad-4da6-8196-c816df0b7aa8"" SEC_TYPE=""ext2"" TYPE=""ext3"" 
</code></pre>

<p>The output from the <code>blkid</code> command above can be used to identify the hard drive when adding entries to <code>/etc/fstab</code>.</p>

<p>Next you need to edit the <code>/etc/fstab</code> file. The lines in this file are organized as follows:</p>

<pre><code>UUID={YOUR-UID}    {/path/to/mount/point}               {file-system-type}    defaults,errors=remount-ro 0       1
</code></pre>

<p>Now edit the file:</p>

<pre><code>% sudo vi /etc/fstab
</code></pre>

<p>And add a file like this, for example:</p>

<pre><code>UUID=41c22818-fbad-4da6-8196-c816df0b7aa8  /disk2p2      ext3    defaults,errors=remount-ro 0       1
</code></pre>

<p>Save the file and then reprocess the file with the <code>mount -a</code> command.</p>

<h3>Windows partitions</h3>

<p>To mount an ntfs partition you'll need to do something like this in your <code>/etc/fstab</code> file:</p>

<pre><code>/dev/sda2   /mnt/excess ntfs-3g    permissions,locale=en_US.utf8    0   2
</code></pre>
","72394"
"Task manager keyboard shortcut in Linux?","59465","","<p>Is there any keyboard shortcut for the ""task manager"" (like <kbd>Alt</kbd>+<kbd>Ctrl</kbd>+<kbd>Del</kbd> in windows) when my machine goes into a crashed state?</p>
","<p>I am going to assume by ""my machine go into crashed state"" you mean that whatever task is taking up the display you are looking at has stopped responding.  (In general, when something crashes on Linux, only that thing crashes and everything else keeps running.  It's very rare that the entire machine comes to a halt.)</p>

<p>When all else fails, I like to switch back to a standard terminal interface (text mode as opposed to GUI) by hitting <kbd>CTRL</kbd>+<kbd>Alt</kbd>+<kbd>F1</kbd>.  This brings up a login prompt.  I then login, and enter the command <code>top</code> to see what is running.   The process at the top of the list is the one using the most CPU and usually the problem, so I kill it by pressing <kbd>k</kbd>, and entering the process ID (the numbers on the left).  I then go back to the GUI by pressing <kbd>CTRL</kbd>+<kbd>Alt</kbd>+<kbd>F7</kbd>  (or sometimes <kbd>CTRL</kbd>+<kbd>Alt</kbd>+<kbd>F8</kbd>, one of those two will work, but it might change).  If things are now working, I continue on, if not, I'll try again or may just force a reboot.</p>
","6582"
"How can I suppress output from grep, so that it only returns the exit status?","59423","","<p>I have the <code>grep</code> command. I'm searching for a keyword from a file, but I don't want to display the match. I just want to know the exit status of the <code>grep</code>.</p>
","<p>Any <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/grep.html"">POSIX compliant version of <code>grep</code></a> has the switch <code>-q</code> for quiet:</p>

<pre><code>-q
     Quiet. Nothing shall be written to the standard output, regardless
     of matching lines. Exit with zero status if an input line is selected.
</code></pre>

<p>In GNU grep (and possibly others) you can use long-option synonyms as well:</p>

<pre><code>-q, --quiet, --silent     suppress all normal output
</code></pre>

<h3>Example</h3>

<p>String exists:</p>

<pre><code>$ echo ""here"" | grep -q ""here""
$ echo $?
0
</code></pre>

<p>String doesn't exist:</p>

<pre><code>$ echo ""here"" | grep -q ""not here""
$ echo $?
1
</code></pre>
","104644"
"telnet - ""Connection closed by foreign host""","59342","","<p>I want to setup an Apache Spark Cluster but I am not able to communicate from the worker machine to the master machine at port 7077 (where the Spark Master is running). </p>

<p>So I tried to <code>telnet</code> to the master from the worker machine and this is what I am seeing:</p>

<pre><code>root@worker:~# telnet spark 7077
Trying 10.xx.xx.xx...
Connected to spark.
Escape character is '^]'.
Connection closed by foreign host.
</code></pre>

<p>The command terminated with ""Connection closed by foreign host"" immediately. It does not timeout or anything.</p>

<p>I verified that the the host is listening on the port and since <code>telnet</code> output shows ""Connected to spark."" &mdash; this also means that the connection is successful.</p>

<p>What could be the reason for such behavior?
I am wondering if this closing of the connection could be the reason why I am not able to communicate from my worker machine to the master.</p>
","<p>The process that is listening for connections on port 7077 is accepting the connection and then immediately closing the connection. The problem lies somewhere in that application's code or configuration, not in the system itself.</p>
","213374"
"Change Apache httpd ""Server:"" HTTP header","59326","","<p>One of the HTTP headers that the <a href=""http://httpd.apache.org/"">Apache <code>httpd</code></a> sends back with response data is ""Server"". For example, my web server machine is relatively up-to-date Arch Linux.  It sends back headers closely resembling the following:</p>

<pre><code>HTTP/1.1 404 Not Found
Date: Thu, 10 Apr 2014 17:19:27 GMT
Server: Apache/2.4.9 (Unix)
Content-Length: 1149
Connection: close
Content-Type: text/html
</code></pre>

<p>I have <code>ServerSignature off</code> in <code>/etc/httpd/conf/httpd.conf</code>, but the ""Server:"" header still appears. I have experimented with <a href=""http://httpd.apache.org/docs/current/mod/mod_headers.html"">mod_headers</a>.  I have it enabled, and I've tried a few things:</p>

<pre><code>&lt;IfModule headers_module&gt;
Header set ProcessingTime ""%D""
Header set Server BigJohn
&lt;/IfModule&gt;
</code></pre>

<p>After stopping and starting <code>httpd</code> with the above configuration, the HTTP headers include something like <code>ProcessingTime: 1523</code>, but the ""Server:"" header line remains unchanged. So I know that ""mod_headers"" is installed and enabled, and working, but not as I desire.</p>

<p>I see that something called ""mod_security"" claims to do this, but I don't want all the rest of the baggage that mod_security carries with it.</p>

<p><strong>UPDATE:</strong></p>

<p>Once you get <code>mod_security</code> installed, you only need a few directives:</p>

<pre><code>&lt;IfModule security2_module&gt;
SecRuleEngine on
ServerTokens Full
SecServerSignature ""Microsoft-IIS/6.0""
&lt;/IfModule&gt;
</code></pre>

<p>That's for <code>mod_security</code> 2.7.7</p>
","<p>The server ID/token header is controlled by ""<a href=""http://httpd.apache.org/docs/current/mod/core.html#servertokens"">ServerTokens</a>"" directive (provided by <code>mod_core</code>). Aside from modifying the Apache HTTPD source code, or using <code>mod_security</code> module, there is no other way to <em>fully</em> suppress the server ID header.</p>

<p>With the <code>mod_security</code> approach, you can disable all of the module's directives/functions in the <code>modsecurity.conf</code> file, and leverage only the server header ID directive without any additional ""baggage.""</p>
","124145"
"Search text in command output on a PuTTY terminal","59322","","<p>I would like to know how to search particular text on the terminal. If I do <code>cat</code> of log files, I would like to find certain words like job or summary so that I don't have to read through the entire log file.</p>

<p>I know there have been a similar <a href=""https://unix.stackexchange.com/questions/93783/search-text-on-the-terminal-output"">post</a> about this. The answer from that post is <kbd>Ctrl</kbd> + <kbd>A</kbd> + <kbd>[</kbd> <code>&lt;text&gt;</code> which doesn't seem to work for me.  When I press that I get a message <code>No bracket in top line (press Return)</code> or If I press those keys together I get the message <code>ESC</code>.</p>

<p>It there a way to do this with PuTTY? Alternatively, is there a generic way to search for text in the output of commands?</p>
","<p>The <kbd>Ctrl</kbd> + <kbd>a</kbd> + <kbd>[</kbd> is meant for use within the application screen (an app for multiplexing consoles).</p>

<h3>less</h3>

<p>Generally the easiest method to do this is to use tools such as <code>less</code> and to pipe the output from whatever application is generating the messages on the console, and search within the application <code>less</code>. You can do so using the forward slash (<code>/</code>) followed by whatever string you're searching for. Hit return to execute the search.</p>

<h3>Example</h3>

<pre><code>$ less filename.log

...then in less, type a forward slash followed by string to search, foo
</code></pre>

<h3>grep</h3>

<p>In the same vain as above with using <code>less</code>, you can also use tools such as <code>tail</code> to print the lats few lines of a applications log file messages, and also use <code>grep</code> to search for only lines that contain a matching string/pattern.</p>

<pre><code>$ grep ""somestring"" filename.log
</code></pre>
","96242"
"How to determine why my computer crashed?","59307","","<p>Yesterday, I ran a <code>bash</code> script for about 10 hours. When I went to use the computer, it locked up.</p>

<ul>
<li>I have an Eee PC with Debian.</li>
<li>The screen was still visible, but the mouse or keyboard had not effect.</li>
<li>I tried <kbd>Ctrl</kbd><kbd>Alt</kbd><kbd>Delete</kbd>, <kbd>Ctrl</kbd><kbd>Alt</kbd><kbd>Backspace</kbd>, <kbd>Ctrl</kbd><kbd>Alt</kbd><kbd>F1</kbd>, but to no effect.</li>
<li>The hard drive light showed no activity.</li>
</ul>

<p>How can I determine what went wrong? What logs can I check?</p>
","<p>You can find all messages in <code>/var/log/syslog</code> and in other <code>/var/log/</code> files. Old messages are in <code>/var/log/syslog.1</code>, <code>/var/log/syslog.2.gz</code> etc. if logrotate is installed.</p>

<p>However, if the kernel really locks up, the probability is low that you will find any related message.</p>

<p>It could be, that only the X server locks up. In this case, you can usually still access the PC over network via ssh (if you have installed it). There is also the  <a href=""http://en.wikipedia.org/wiki/Magic_SysRq_key#Common_Usage"">Magic SysRq key</a> to unRaw the keyboard such that the shortcuts you tried could work, too.</p>
","38613"
"How to find out which Wi-Fi driver is installed?","59298","","<p>I know about <code>lsmod</code>, but how do I figure out which driver does what?</p>
","<pre><code>$ readlink /sys/class/net/wlan0/device/driver
../../../../bus/pci/drivers/ath5k
</code></pre>

<p>In other words, the <code>/sys</code> hierarchy for the device (<code>/sys/class/net/$interface/device</code>) contains a symbolic link to the <code>/sys</code> hierarchy for the driver. There you'll also find a symbolic link to the <code>/sys</code> hierarchy for the module, if applicable. This applies to most devices, not just wireless interfaces.</p>
","7881"
"How to kill a daemon with its name gracefully?","59298","","<p>I usually kill a process with <code>killall</code>. </p>

<pre><code>killall markdoc
</code></pre>

<p>But I am not sure if this command terminates the process gracefully.
Does this command achieve graceful termination? If it does not, how can I kill a process with its name gracefully?</p>
","<p>Your question is not clear, you talk about a daemon in the title, but in the body only talk about a generic process.</p>

<p>For a daemon there are specific means to stop it, for example in Debian you have</p>

<pre><code>    service daemon-name stop
</code></pre>

<p>or</p>

<pre><code>    /etc/init.d/daemon-name stop
</code></pre>

<p>Similar syntaxes exist for other initscript standards used in other distributions/OS.</p>

<p>To kill a non-daemon process, supposing it is in some way out of control, you can safely use <code>killall</code> or <code>pkill</code>, given that they use by default the <code>SIGTERM</code> (15) signal, and any decently written application should catch and gracefully exit on receiving this signal. Take into account that these utilities could kill more that one process, if there are many with the same name.</p>

<p>If that do not work, you can try <code>SIGINT</code> (2), then <code>SIGHUP</code> (1), and as a last resort <code>SIGKILL</code> (9). This last signal cannot be catched by the application, so that it cannot perform any clean-up. For this reason it should be avoided every time you can.</p>

<p>Both <code>pkill</code> and <code>killall</code> accept a signal parameter in the form <code>-NAME</code>, as in</p>

<pre><code>pkill -INT process-name
</code></pre>
","28265"
"How can I fix/install/reinstall grub?","59239","","<p>So I started out with a 250GB HDD, the stock drive from an EeePC 1015pem that I am trying to turn into a MintBook. The drive is physically operable, but all data has been nuked, including the old OS. Given this, I attached the HDD to my desktop and installed Linux Mint 16 Xfce from a live USB created through Unetbootin-585. Set aside 10GB for <code>swap</code> and 240GB for <code>ext4</code> and <code>/</code>.</p>

<p>The drive now refuses to boot for either the desktop or netbook. Both motherboards are sounding the correct sequence of beeps, so they seem healthy, and I can successfully access the BIOS on both systems. However, the only thing that comes up after starting the computer is a nonresponsive command-line. There is no error message, no grub or grub-rescue, nothing.</p>

<p>Is there anything I can try besides reformatting and starting over? How would I go about installing a boot loader that can boot my OS?</p>
","<p>So, it sounds like you have not installed a boot loader (e.g. grub) on the disk. This means that although you have a valid OS on it, there is no way to boot it and so you can't use it.</p>

<p>You need to attach the HDD to a working computer (you can use a live CD), <a href=""http://howtoubuntu.org/how-to-repair-restore-reinstall-grub-2-with-a-ubuntu-live-cd#.UtbYJXWJAjA"">set up a chroot environment</a> and install grub on it. </p>

<ol>
<li><p>Mount the partition you will be using as <code>/</code> (I will call the drive <code>/dev/sdb</code> and the <code>/</code> partition <code>sdb1</code>, <strong>the names may be different on your system, you will need to use the correct ones</strong>) somewhere :</p>

<pre><code>sudo mount /dev/sdb1 /mnt/foo
</code></pre></li>
<li><p>Bind the directories that <code>grub</code> needs to have access to</p>

<pre><code>sudo mount --bind /dev /mnt/foo/dev &amp;&amp; 
sudo mount --bind /dev/pts /mnt/foo/dev/pts &amp;&amp; 
sudo mount --bind /proc /mnt/foo/proc &amp;&amp; 
sudo mount --bind /sys /mnt/foo/sys
</code></pre></li>
<li><p>Set up the <code>chroot</code> environment</p>

<pre><code>sudo chroot /mnt/foo
</code></pre></li>
<li><p>Create grub's configuration file:</p>

<pre><code>sudo grub-mkconfig -o /boot/grub/grub.cfg
</code></pre>

<p>If you have multiple operating systems installed, make sure that the command above lists all of them. For example:</p>

<pre><code>$ sudo grub-mkconfig -o /boot/grub/grub.cfg
Generating grub.cfg ...
Found background image: /usr/share/images/desktop-base/desktop-grub.png
Found linux image: /boot/vmlinuz-3.10-2-amd64
Found initrd image: /boot/initrd.img-3.10-2-amd64
Found linux image: /boot/vmlinuz-3.2.0-4-amd64
Found initrd image: /boot/initrd.img-3.2.0-4-amd64
Found linux image: /boot/vmlinuz-3.2.0-3-amd64
Found initrd image: /boot/initrd.img-3.2.0-3-amd64
Found linux image: /boot/vmlinuz-3.2.0-2-amd64
Found initrd image: /boot/initrd.img-3.2.0-2-amd64
Found memtest86+ image: /boot/memtest86+.bin
Found memtest86+ multiboot image: /boot/memtest86+_multiboot.bin
Found Windows 7 (loader) on /dev/sda2
done
</code></pre></li>
<li><p>Now install grub to the MBR of your drive (remember to change <code>/dev/sdb</code> to whichever drive you actually want to install it on)</p>

<pre><code>grub-install /dev/sdb
grub-install --recheck /dev/sdb
</code></pre></li>
<li><p>Exit the <code>chroot</code> and unmount everything so your running system is back to normal:</p>

<pre><code>exit
sudo umount /mnt/foo/dev/pts /mnt/foo/dev /mnt/foo/proc /mnt/foo/sys /mnt/foo
</code></pre></li>
<li><p>Try booting from the drive, you should have a grub menu this time.</p></li>
</ol>
","109492"
"ls list files not matching given string in filename","59224","","<p>I have a directory in which lots of files (around 200) with the name <code>temp_log.$$</code> are created with several other important files which I need to check.</p>

<p>How can I easily list out all the files and exclude the <code>temp_log.$$</code> files from getting displayed?</p>

<h3>Expected output</h3>

<pre><code>$ ls -lrt &lt;exclude-filename-part&gt;
-- Lists files not matching the above given string
</code></pre>

<p>I have gone through <code>ls</code> man page but couldn't find anything in this reference. Please let me know if I have missed any vital information here.</p>

<p>Thanks</p>
","<p>With GNU <code>ls</code> (the version on non-embedded Linux and Cygwin, sometimes also found elsewhere), you can exclude some files when listing a directory.</p>

<pre><code>ls -I 'temp_log.*' -lrt
</code></pre>

<p>(note the long form of <code>-I</code> is <code>--ignore='temp_log.*'</code>)</p>

<p>With zsh, you can let the shell do the filtering. Pass <code>-d</code> to <code>ls</code> so as to avoid listing the contents of matched directories.</p>

<pre><code>setopt extended_glob          # put this in your .zshrc
ls -dltr ^temp_log.*
</code></pre>

<p>With ksh, bash or zsh, you can use the ksh filtering syntax. In zsh, run <code>setopt ksh_glob</code> first. In bash, run <code>shopt -s extglob</code> first.</p>

<pre><code>ls -dltr !(temp_log.*)
</code></pre>
","51991"
"Why does Ctrl-D (EOF) exit the shell?","59080","","<p>Are you literally ""ending a file"" by inputting this escape sequence, i.e. is the interactive shell session is seen as a real file stream by the shell, like any other file stream? If so, which file? </p>

<p>Or, is the <kbd>Ctrl</kbd>+<kbd>D</kbd> signal just a placeholder which means ""the user has finished providing input and you may terminate""?</p>
","<p>The <code>^D</code> character (aka <code>\04</code> or 0x4) is the default value for the <code>eof</code> special control character parameter of the terminal or pseudo-terminal driver in the kernel (more precisely of the <a href=""https://unix.stackexchange.com/a/120071/22565""><code>tty</code> line discipline attached to the serial or pseudo-tty device</a>). That's the <code>c_cc[VEOF]</code> of the <code>termios</code> structure passed to the TCSETS/TCGETS <code>ioctl</code> one issues to the terminal device to affect the driver behaviour.</p>

<p>The typical command that sends those <code>ioctls</code> is the <code>stty</code> command.</p>

<p>To retrieve all the parameters:</p>

<pre>
$ stty -a
speed 38400 baud; rows 58; columns 191; line = 0;
intr = ^C; quit = ^\; erase = ^?; kill = ^U; <b><i>eof = ^D</i></b>; eol = &lt;undef&gt;; eol2 = &lt;undef&gt;; swtch = &lt;undef&gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; flush = ^O;
min = 1; time = 0;
-parenb -parodd cs8 -hupcl -cstopb cread -clocal -crtscts
-ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8
opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0
isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke
</pre>

<p>That <code>eof</code> parameter is only relevant when the terminal device is in <code>icanon</code> mode.</p>

<p>In that mode, the terminal driver (not the terminal emulator) implements a very simple <em>line editor</em>, where you can type <kbd>Backspace</kbd> to erase a character, <kbd>Ctrl-U</kbd> to erase the whole line... When an application reads from the terminal device, it sees nothing until you press <kbd>Return</kbd> at which point the <code>read()</code> returns the full line including the last <code>LF</code> character (by default, the terminal driver also translates the <code>CR</code> sent by your terminal upon <kbd>Return</kbd> to <code>LF</code>).</p>

<p>Now, if you want to send what you typed so far without pressing <kbd>Enter</kbd>, that's where you can enter the <code>eof</code> character. Upon receiving that character from the terminal emulator, the terminal driver <em>submits</em> the current content of the line, so that the application doing the <code>read</code> on it will receive it as is (and it won't include a trailing <code>LF</code> character).</p>

<p>Now, if the current line was empty, and provided the application will have fully read the previously entered lines, the <code>read</code> will return 0 character.</p>

<p>That signifies <em>end of file</em> to the application (when you read from a file, you read until there's nothing more to be read). That's why it's called the <code>eof</code> character, because sending it causes the application to see that no more input is available.</p>

<p>Now, modern shells, at their prompt do not set the terminal in <code>icanon</code> mode because they implement their own <em>line editor</em> which is much more advanced than the terminal driver built-in one. However, in their own <em>line editor</em>, to avoid confusing the users, they give the <code>^D</code> character (or whatever the terminal's <code>eof</code> setting is with some) the same meaning (to signify <code>eof</code>).</p>
","110248"
"How to reduce Volume Group size in LVM?","59007","","<pre><code>[root@localhost ~] vgdisplay
  --- Volume group ---
  VG Name               vg_root
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  7
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                3
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               297,59 GiB
  PE Size               4,00 MiB
  Total PE              76182
  Alloc PE / Size       59392 / 232,00 GiB
  Free  PE / Size       16790 / 65,59 GiB
  VG UUID               XXXXXXXXXX
</code></pre>

<p>PV: </p>

<pre><code>[root@localhost ~] pvdisplay

  --- Physical volume ---
  PV Name               /dev/mapper/udisks-luks-uuid-ASDFASDF
  VG Name               vg_root
  PV Size               297,59 GiB / not usable 2,00 MiB
  Allocatable           yes 
  PE Size               4,00 MiB
  Total PE              76182
  Free PE               16790
  Allocated PE          59392
  PV UUID               YYYYYYYYYYY
</code></pre>

<p>So I have a VG with 65 GByte free space. But when I want to shrink this Volume Group about ~50 GByte: </p>

<pre><code>pvresize -tv --setphysicalvolumesize 247G /dev/mapper/udisks-luks-uuid-ASDFASDF
  Test mode: Metadata will NOT be updated and volumes will not be (de)activated.
    Using physical volume(s) on command line
    Test mode: Skipping archiving of volume group.
    /dev/mapper/udisks-luks-uuid-ASDFASDF: Pretending size is 517996544 not 624087040 sectors.
    Resizing volume ""/dev/mapper/udisks-luks-uuid-ASDFASDF"" to 624087040 sectors.
    Resizing physical volume /dev/mapper/udisks-luks-uuid-ASDFASDF from 0 to 63231 extents.
  /dev/mapper/udisks-luks-uuid-ASDFASDF: cannot resize to 63231 extents as later ones are allocated.
  0 physical volume(s) resized / 1 physical volume(s) not resized
    Test mode: Wiping internal cache
    Wiping internal VG cache
</code></pre>

<p>So the error message is: </p>

<pre><code>cannot resize to 63231 extents as later ones are allocated.
</code></pre>

<p><strong>Q:</strong> How can I defrag the vg_root so I can remove the unneeded part of it?</p>

<p>p.s: I already found out that I only need to resize the PV to resize the VG, or are there any better commands to do the VG resize (ex.: what can I do if I would several VG's on a PV? ...)?</p>
","<p>You can use <code>pvmove</code> to move those extents to the beginning of the device or another device:</p>

<pre><code>sudo pvmove --alloc anywhere /dev/device:60000-76182
</code></pre>

<p>Then <code>pvmove</code> chooses where to move the extents to, or you can specify where to move them.</p>

<p>See <code>pvs -v --segments /dev/device</code> to see what extents are currently allocated.</p>
","67707"
"How to alter PATH within a shell script?","58898","","<p>I have several projects that require me to change versions of Java/Grails/Maven. I'm trying to handle this with some scripts that would make the changes. For example:</p>

<pre><code>#!/bin/sh

export JAVA_HOME=/cygdrive/c/dev/Java/jdk1.5.0_22
export PATH=$JAVA_HOME/bin:$PATH
export GRAILS_HOME=/cygdrive/c/dev/grails-1.0.3
export PATH=$GRAILS_HOME/bin:$PATH
export MAVEN_HOME=/cygdrive/c/dev/apache-maven-2.0.11
export PATH=$MAVEN_HOME/bin:$PATH
which java
which grails
which mvn
</code></pre>

<p>When this executes, it successfully changes the PATH within the context of the script, but then the script ends, and no change has been accomplished.</p>

<p>How can I run a script in a way to change the PATH in for the shell in which I am currently working?</p>

<p>I'm using Cygwin.</p>
","<p>You have to use <code>source</code> or <code>eval</code> or to spawn a new shell.</p>

<p>When you run a shell script a new <em>child</em> shell is spawned. This <em>child</em> shell will execute the script commands. The <em>father</em> shell environment will remain untouched by anything happens in the <em>child</em> shell.</p>

<p>There are a lot of different techniques to manage this situation:</p>

<ol>
<li><p>Prepare a file <strong>sourcefile</strong> containg a list of commands to <code>source</code> in the current shell:</p>

<pre><code>export JAVA_HOME=/cygdrive/c/dev/Java/jdk1.5.0_22
export PATH=$JAVA_HOME/bin:$PATH
</code></pre>

<p>and then source it</p>

<pre><code>source sourcefile
</code></pre>

<p>note that there is no need for a sha-bang at the begin of the <strong>sourcefile</strong>, but it will work with it.</p></li>
<li><p>Prepare a script <strong>evalfile.sh</strong> that prints the command to set the environment:</p>

<pre><code>#!/bin/sh
echo ""export JAVA_HOME=/cygdrive/c/dev/Java/jdk1.5.0_22""
echo ""export PATH=$JAVA_HOME/bin:$PATH""
</code></pre>

<p>and then <code>eval</code>uate it:</p>

<pre><code>eval `evalfile.sh`
</code></pre></li>
<li><p>Configure and run a new shell:</p>

<pre><code>#!/bin/sh
export JAVA_HOME=/cygdrive/c/dev/Java/jdk1.5.0_22
export PATH=$JAVA_HOME/bin:$PATH

exec /bin/bash
</code></pre>

<p>note that when you type <code>exit</code> in this shell, you will return to the <em>father</em> one.</p></li>
<li><p>Put an alias in your <code>~/.bashrc</code>:</p>

<pre><code>alias prepare_environ='export JAVA_HOME=/cygdrive/c/dev/Java/jdk1.5.0_22; export PATH=$JAVA_HOME/bin:$PATH;'
</code></pre>

<p>and call it when needed:</p>

<pre><code>prepare_environ
</code></pre></li>
</ol>
","23432"
"How is an Interrupt handled in Linux?","58876","","<p>I just know that <code>Interrupt</code> is a <code>hardware signal assertion</code> caused in a processor pin. But I would like to know how Linux OS handles it.<br>
What all are the things that happen when an interrupt occurs?</p>
","<p>Here's a high-level view of the low-level processing. I'm describing a simple typical architecture, real architectures can be more complex or differ in ways that don't matter at this level of detail.</p>

<p>When an <a href=""http://en.wikipedia.org/wiki/Interrupt"">interrupt</a> occurs, the processor looks if interrupts are masked. If they are, nothing happens until they are unmasked. When interrupts become unmasked, if there are any pending interrupts, the processor picks one.</p>

<p>Then the processor executes the interrupt by branching to a particular address in memory. The code at that address is called the <a href=""http://en.wikipedia.org/wiki/Interrupt_handler"">interrupt handler</a>. When the processor branches there, it masks interrupts (so the interrupt handler has exclusive control) and saves the contents of some registers in some place (typically other registers).</p>

<p>The interrupt handler does what it must do, typically by communicating with the peripheral that triggered the interrupt to send or receive data. If the interrupt was raised by the timer, the handler might trigger the OS scheduler, to switch to a different thread. When the handler finishes executing, it executes a special return-from-interrupt instruction that restores the saved registers and unmasks interrupts.</p>

<p>The interrupt handler must run quickly, because it's preventing any other interrupt from running. In the Linux kernel, interrupt processing is divided in two parts:</p>

<ul>
<li>The “top half” is the interrupt handler. It does the minimum necessary, typically communicate with the hardware and set a flag somewhere in kernel memory.</li>
<li>The “bottom half” does any other necessary processing, for example copying data into process memory, updating kernel data structures, etc. It can take its time and even block waiting for some other part of the system since it runs with interrupts enabled.</li>
</ul>

<p>As usual on this topic, for more information, read <a href=""http://lwn.net/images/pdf/LDD3/"">Linux Device Drivers</a>; <a href=""http://lwn.net/images/pdf/LDD3/ch10.pdf"">chapter 10</a> is about interrupts.</p>
","5816"
"print output to 3 separate columns","58870","","<pre><code>MYPATH=/var/www/html/error_logs/
TOTALFILE=$(ls $MYPATH* | wc -l)
FILETIME=$(stat --format=%y $MYPATH* | head -5 | cut -d'.' -f1)  
FILE=$(ls -1tcr $MYPATH* | head -5 | rev | cut -d/ -f1 | rev)
TOPLINE=$(head -1 $MYPATH* | grep -Po '"".*?""' | head -5)
</code></pre>

<p>how can i elegantly print this out 5 files information into columns with headers?</p>

<pre><code>FILE CREATED TIME   | FILE NAME        | ERROR HEADER
---------------------------------------------
$FILETIME           | $FILE            | $TOPLINE
2012-11-29 11:27:45 | 684939947465     | ""SQLSTATE[HY000] [2002] Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)""
</code></pre>

<p>and so on 5 files</p>

<p><code>total files: $TOTALFILE</code></p>

<p>is there any easy way to get what i want?</p>

<p><strong>note:
this output i got when echo every variable</strong></p>

<pre><code>2012-11-29 11:27:45 2012-11-29 11:27:41 2012-11-28 23:33:01 2012-11-26 10:23:37 2012-11-19 22:49:36
684939947465 1313307654813 1311411049509 1234980770182 354797376843
""SQLSTATE[HY000] [2002] Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)"" ""SQLSTATE[HY000] [2002] Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)"" ""Connection to localhost:6379 failed: Connection refused (111)"" ""An error occurred connecting to Redis."" ""SQLSTATE[HY000] [2002] Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)""
</code></pre>
","<p>You can use the shell command 'column' for that, check: <a href=""http://linux.die.net/man/1/column""><code>column</code> MAN page</a>.</p>

<p>Combine this with a loop and you're in business, e.g.:</p>

<pre><code>#!/bin/sh

MYPATH=/
TOTALFILE=$(ls $MYPATH/* | wc -l)
FILE=$(ls -1tcr $MYPATH/* | head -5 | rev | cut -d/ -f1 | rev)

declare -a FILES
declare -a FILETIME

OUTPUT=""FILENAME CREATED TIME ERROR_HEADER\n\n------------------------------ ----------------------------- ----------------------------------- ------$

for i in $MYPATH/*;
do
    FILES[${#FILES[@]}]=""$i""
    FILETIME[${#FILETIME[@]}]=$(stat --format=%y $i | head -5 | cut -d'.' -f1)
    TOPLINE=$(head -1 $i | grep -Po '"".*?""' | head -5)

    OUTPUT=""$OUTPUT\n${FILES[${#FILES[@]}-1]} ${FILETIME[${#FILETIME[@]}-1]} $TOPLINE\n""
done

echo -ne $OUTPUT | column -t
</code></pre>
","59292"
"How can I delete a word backward at the command line (bash and zsh)?","58855","","<p>How can I delete a word backward at the command line?  I'm truly used to some editors deleting the last 'word' using <kbd>Ctrl</kbd>+<kbd>Backspace</kbd>, and I'd like that functionality at the command line too.</p>

<p>I am using Bash at the moment and although I could jump backward a word and then delete forward a word, I'd rather have this as a quick-key, or event as <kbd>Ctrl</kbd>+<kbd>Backspace</kbd>.</p>

<p>How can accomplish this?</p>
","<p><kbd>ctrl</kbd><kbd>w</kbd> is the standard ""kill word"" (aka <code>werase</code>).
<kbd>ctrl</kbd><kbd>u</kbd> kills the whole line (<code>kill</code>).</p>

<p>You can change them with <code>stty</code>.</p>

<pre><code>-bash-4.2$ stty -a
speed 38400 baud; 24 rows; 80 columns;
lflags: icanon isig iexten echo echoe -echok echoke -echonl echoctl
        -echoprt -altwerase -noflsh -tostop -flusho pendin -nokerninfo
        -extproc -xcase
iflags: -istrip icrnl -inlcr -igncr -iuclc ixon -ixoff ixany imaxbel
        -ignbrk brkint -inpck -ignpar -parmrk
oflags: opost onlcr -ocrnl -onocr -onlret -olcuc oxtabs -onoeot
cflags: cread cs8 -parenb -parodd hupcl -clocal -cstopb -crtscts -mdmbuf
cchars: discard = ^O; dsusp = ^Y; eof = ^D; eol = &lt;undef&gt;;
        eol2 = &lt;undef&gt;; erase = ^?; intr = ^C; kill = ^U; lnext = ^V;
        min = 1; quit = ^\; reprint = ^R; start = ^Q; status = &lt;undef&gt;;
        stop = ^S; susp = ^Z; time = 0; werase = ^W;
-bash-4.2$ stty werase ^p
-bash-4.2$ stty kill ^a
-bash-4.2$
</code></pre>

<p>Note that one does not have to put the actual control character on the line, stty understands putting <code>^</code> and then the character you would hit with control.</p>

<p>After doing this, if I hit <kbd>ctrl</kbd><kbd>p</kbd> it will erase a word from the line.  And if I hit <kbd>ctrl</kbd><kbd>a</kbd>, it will erase the whole line.</p>
","94391"
"How to print certain columns by name?","58701","","<p>I have the following file: </p>

<pre><code>id  name  age
1   ed    50
2   joe   70   
</code></pre>

<p>I want to print just the <code>id</code> and <code>age</code> columns. Right now I just use <code>awk</code>: </p>

<pre><code>cat file.tsv | awk '{ print $1, $3 }'
</code></pre>

<p>However, this requires knowing the column numbers. Is there a way to do it where I can use the name of the column (specified on the first row), instead of the column number?</p>
","<p>Maybe something like this:</p>

<pre><code>$ cat t.awk
NR==1 {
    for (i=1; i&lt;=NF; i++) {
        ix[$i] = i
    }
}
NR&gt;1 {
    print $ix[c1], $ix[c2]
}
$ awk -f t.awk c1=id c2=name input 
1 ed
2 joe
$ awk -f t.awk c1=age c2=name input 
50 ed
70 joe
</code></pre>

<p>If you want to specify the columns to print on the command line, you could do something like this:</p>

<pre><code>$ cat t.awk 
BEGIN {
    split(cols,out,"","")
}
NR==1 {
    for (i=1; i&lt;=NF; i++)
        ix[$i] = i
}
NR&gt;1 {
    for (i in out)
        printf ""%s%s"", $ix[out[i]], OFS
    print """"
}
$ awk -f t.awk -v cols=name,age,id,name,id input 
ed 1 ed 50 1 
joe 2 joe 70 2 
</code></pre>

<p>(Note the <code>-v</code> switch to get the variable defined in the <code>BEGIN</code> block.)</p>
","25144"
"bash variables in for loop range","58654","","<p>I want to print list of numbers from 1 to 100 and I use a for loop like the following:</p>

<pre><code>number=100
for num in {1..$number}
do
  echo $num
done
</code></pre>

<p>When I execute the command it only prints {1..100} and not the list of number from 1 to 100.</p>
","<p>Yes, that's because brace-expansion occurs before parameter expansion. Either use another shell like <code>zsh</code> or <code>ksh93</code> or use an alternative syntax:</p>

<h3>Standard (POSIX) sh syntax</h3>

<pre><code>i=1
while [ ""$i"" -le ""$number"" ]; do
  echo ""$i""
  i=$(($i + 1))
done
</code></pre>

<h3>Ksh-style <code>for ((...))</code></h3>

<pre><code>for ((i=1;i&lt;=number;i++)); do
  echo ""$i""
done
</code></pre>

<h3>use <code>eval</code> (not recommended)</h3>

<pre><code>eval '
  for i in {1..'""$number""'}; do
    echo ""$i""
  done
'
</code></pre>

<h3>use the GNU <code>seq</code> command on systems where it's available</h3>

<pre><code>unset -v IFS # restore IFS to default
for i in $(seq ""$number""); do
  echo ""$i""
done
</code></pre>

<p>(that one being less efficient as it forks and runs a new command and the shell has to reads its output from a pipe).</p>

<h3>Avoid loops in shells.</h3>

<p>Using loops in a shell script are often an indication that you're not doing it right.</p>

<p>Most probably, your code can be written some other way.</p>
","88217"
"How to determine which sd* is usb?","58541","","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""https://unix.stackexchange.com/questions/40143/how-to-know-if-dev-sdx-is-a-connected-usb-or-hdd"">How to know if /dev/sdX is a connected USB or HDD?</a>  </p>
</blockquote>



<p>The output of <code>ls /dev/sd*</code> on my system is - </p>

<pre><code>sda  sda1  sda2  sda3  sda4  sda5  sda6  sda7  sdb  sdc  sdc1  sdc2
</code></pre>

<p>How should I determine which drive is which?</p>
","<p>Assuming you're on Linux.</p>

<p>Try:</p>

<pre><code>sudo /lib/udev/scsi_id --page=0x80 --whitelisted --device=/dev/sdc
</code></pre>

<p>or:</p>

<pre><code>cat /sys/block/sdc/device/{vendor,model}
</code></pre>

<p>You can also get information (including labels) from the filesystems on the different partitions with</p>

<pre><code>sudo blkid /dev/sdc1
</code></pre>

<p>The pathid will help to determine the type of device:</p>

<pre><code>readlink -f /sys/class/block/sdc/device
</code></pre>

<p>See also:</p>

<pre><code>find /dev/disk -ls | grep /sdc
</code></pre>

<p>Which with a properly working <code>udev</code> would give you all the information from the other commands above.</p>

<p>The content of <code>/proc/partitions</code> will give you information on size (though not in as a friendly format as <code>lsblk</code> already mentionned by @Max).</p>

<pre><code>sudo blockdev --getsize64 /dev/sdc
</code></pre>

<p>Will give you the size in bytes of the corresponding block device.</p>

<pre><code>sudo smartctl -i /dev/sdc
</code></pre>

<p>(cross-platform), will also give you a lot of information including make, model, size, serial numbers, firmware revisions...</p>
","60305"
"How can I hook on to one terminal's output from another terminal?","58526","","<p>I need to hook onto output of currently running terminal (tty1) from virtual terminal and capture it (running X server).</p>

<p>I'm pretty sure such a thing is possible by simpler means than debugging (result of my Google searches).</p>

<p>Can anyone help?</p>
","<p>I came across this one tool called <code>ttylog</code>. It's a Perl program available on CPAN <a href=""http://search.cpan.org/~bbb/ttylog-0.83/ttylog"" rel=""noreferrer"">here</a>. It has a couple caveats, one being that I could only figure out how to attach to a terminal that was created as part of someone ssh'ing into my box. The other being that you have to run it with elevated privileges (i.e. root or sudo).</p>

<p>But it works!</p>

<h3>For example</h3>

<p>First ssh into your box in TERM#1:</p>

<pre><code>TERM#1% ssh saml@grinchy
</code></pre>

<p>Note this new terminal's tty:</p>

<pre><code>TERM#1% tty
/dev/pts/3
</code></pre>

<p>Now in another terminal (TERM#2) run this command:</p>

<pre><code>TERM#2% ttylog pts/3
DEBUG: Scanning for psuedo terminal pts/3
DEBUG: Psuedo terminal [pts/3] found.
DEBUG: Found parent sshd pid [13789] for user [saml]
</code></pre>

<p>Now go back to TERM#1 and type stuff, it'll show up in TERM#2.</p>

<p><img src=""https://i.stack.imgur.com/fZCpw.png"" alt=""ss of terminals""></p>

<p>All the commands I tried, (top, ls, etc.) worked without incident using <code>ttylog</code>.</p>
","72334"
"what is >> symbol and >& in unix/Linux?","58521","","<p>I have a CRONTAB entry as below. Can someone tell me what the below statement is exactly doing?</p>

<pre><code>1 0 * * * /vol01/sites/provisioning/MNMS/45627/45627.sh1 &gt;&gt; /vol01/sites/provisioning/MNMS/45627/output/cron.log 2&gt;&amp;1
</code></pre>
","<p><code>&gt;</code> redirects output to a file, overwriting the file. </p>

<p><code>&gt;&gt;</code> redirects output to a file appending the redirected output at the end. </p>

<p>Standard output is represented in bash with number <code>1</code> and standard error is represented with number <code>2</code>. They are separate, so the user can redirect them to different files.</p>

<p><code>2&gt;&amp;1</code> redirects the standard error to the standard output so they appear together and can be jointly redirected to a file. (Writing just <code>2&gt;1</code> would redirect the standard error to a file called ""1"", not to standard output.)</p>

<p>In your case, you have a job whose output (both standard and error) is appended at the end of a log file (<code>cron.log</code>) for later use. </p>

<p>For additional info, check the bash manual (section ""Redirection""), <a href=""https://unix.stackexchange.com/questions/46974/what-is-the-difference-between-and-especially-as-it-relates-to-use-with-th?lq=1"">this question</a>, and <a href=""https://unix.stackexchange.com/questions/42728/what-does-31-12-23-do-in-a-script?lq=1"">this question</a>. </p>
","89416"
"How do I remove multiple files with a common prefix and suffix?","58502","","<p>I have many files named  </p>

<pre><code>sequence_1_0001.jpg  
sequence_1_0002.jpg  
sequence_1_0003.jpg  
...
</code></pre>

<p>and files named  </p>

<pre><code>sequence_1_0001.hmf  
sequence_1_0002.hmf  
sequence_1_0003.hmf  
...
</code></pre>

<p>and files named</p>

<pre><code>sequence_2_0001.jpg  
sequence_2_0002.jpg  
sequence_2_0003.jpg  
...
</code></pre>

<p>and</p>

<pre><code>sequence_2_0001.hmf  
sequence_2_0002.hmf  
sequence_2_0003.hmf  
...
</code></pre>

<p>I just want to remove the files that begin with 'sequence_1' and end in '.hmf', but I don't want to remove them one by one, since there are thousands of files.  How can I specify to the rm command that I want to remove all that begin with the prefilx 'sequence_1' and end in '.hmf'?  </p>

<p>I'm currently working with a RedHat Linux system, but I'd like to know how to do it on other distributions as well.</p>
","<pre><code>rm sequence_1*.hmf
</code></pre>

<p>removes files beginning with <code>sequence_1</code> and ending with <code>.hmf</code>.  </p>

<hr>

<p>Globbing is the process in which your shell takes a pattern and expands it into a list of filenames matching that pattern.  Do not confuse it with regular expressions, which is different. 
If you spend most of your time in <code>bash</code>, the Wooledge Wiki has a good <a href=""http://mywiki.wooledge.org/glob"">page on globbing (pathname expansion)</a>.  If you want maximum portability, you'll want to read the <a href=""http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_13"">POSIX spec on pattern matching</a> as well / instead.</p>

<hr>

<p>In the unlikely case you run into an <em>""Argument list too long""</em> error, you can take a look at <a href=""http://mywiki.wooledge.org/BashFAQ/095"">BashFAQ 95</a>, which addresses this.  The simplest workaround is to break up the glob pattern into multiple smaller chunks, until the error goes away.  In your case, you could probably get away with splitting the  match by prefix digits 0 through 9, as follows:</p>

<pre><code>for c in {0..9}; do rm sequence_1_""$c""*.hmf; done
rm sequence_1*.hmf  # catch-all case
</code></pre>
","37352"
"Can not run configure command: ""No such file or directory""","58484","","<p>I'm trying to install a Debian package from source (via git). I downloaded the
package, changed to the package’s directory and ran <code>./configure</code> command but
it returned <code>bash: ./configure: No such file or directory</code>. What can be the
problem? A <code>configure.ac</code> file is located in the program folder.</p>

<pre><code>./configure
make
sudo make install
</code></pre>
","<p><strong>If the file is called configure.ac,</strong></p>

<p><em>do</em> <code>$&gt; autoconf</code></p>

<p>Depends:
M4, Automake</p>

<p><strong>If you're not sure what to do,</strong></p>

<p><em>try</em> <code>$&gt; cat readme</code></p>

<p>They must mean that you use ""autoconf"" to generate an executable ""configure"" file.</p>

<p>So the order is:</p>

<pre><code>$&gt; autoconf
$&gt; ./configure
$&gt; make
$&gt; make install
</code></pre>
","158961"
"For loops in zsh and bash","58457","","<p>I have noticed there are two alternative ways of building loops in <strong>zsh</strong>:</p>

<ol>
<li><code>for x (1 2 3); do echo $x; done</code></li>
<li><code>for x in 1 2 3; do echo $x; done</code></li>
</ol>

<p>They both print:</p>

<pre><code>1
2
3
</code></pre>

<p>My question is, why the two syntaxes?  Is <code>$x</code> iterating through a different <strong>type of object</strong> in each of them? </p>

<p>Does <strong>bash</strong> make a similar distinction?</p>

<h2>Addendum:</h2>

<p>Why does the following work?:</p>

<pre><code>#!/bin/zsh
a=1
b=2
c=5    

d=(a b c)    
for x in $d; do print $x;done
</code></pre>

<p>but this one doesn't?:</p>

<pre><code>#!/bin/zsh
a=1
b=2
c=5

d=(a b c)    
# It complains with ""parse error near `$d'""
for x $d; do print $x;done 
</code></pre>
","<p>Several forms of complex commands such as loops have <a href=""http://zsh.sourceforge.net/Doc/Release/Shell-Grammar.html#Alternate-Forms-For-Complex-Commands"">alternate forms</a> in zsh. These forms are mostly inspired by the <a href=""http://en.wikipedia.org/wiki/C_shell"">C shell</a>, which was fairly common when zsh was young but has now disappeared. These alternate forms act exactly like the normal forms, they're just a different syntax. They're slightly shorter, but less clear.</p>

<p>The standard form for the <code>for</code> command is <code>for x in 1 2 3; do echo $x; done</code>, and the standard form for the <code>while</code> command is <code>while test …; do somecommand; done</code>. 
Ksh, bash and zsh have an alternate form of <code>for</code>: <code>for ((i = 0; i &lt; 42; i++)); do somecommand; done</code>, which mimics the <code>for</code> loops of languages like Pascal or C, to enumerate integers. Other exotic forms that exist in zsh are specific to zsh (but often inspired by csh).</p>
","23229"
"How to clear terminal completely?","58367","","<p>When we use <code>clear</code> command or <kbd>Ctrl</kbd>+<kbd>L</kbd> in terminal, it clears terminal but we can still scroll back to view the last used commands. Is there a way to completely clear the terminal? </p>
","<p>You can use <code>tput reset</code>.</p>

<p>Besides <code>reset</code> and <code>tput reset</code> you can use following shell script.</p>

<pre><code>#!/bin/sh
echo -e \\033c
</code></pre>

<p>This sends control characters <code>Esc-C</code> to the console which resets the terminal.</p>

<p>Google Keywords: <strong>Linux Console Control Sequences</strong></p>

<p><code>man console_codes</code> says:</p>

<blockquote>
  <p>The  sequence ESC c causes a terminal reset, which is what you want if
  the screen is all garbled.  The oft-advised ""echo ^V^O"" will only make
  G0 current, but there is no guarantee that G0 points at table a).  In
  some distributions there is a program reset(1) that just does ""echo
  ^[c"".  If your terminfo  entry  for the console is correct (and has an
  entry rs1=\Ec), then ""tput reset"" will also work.</p>
</blockquote>
","26979"
"Tab completion errors: bash: cannot create temp file for here-document: No space left on device","58293","","<p>When using the tab bar, I keep getting this error:</p>

<blockquote>
  <p>bash: cannot create temp file for here-document: No space left on device""</p>
</blockquote>

<p>Any ideas?</p>

<p>I have been doing some research, and many people talk about the /tmp file, which might be having some overflow. When I execute <code>df -h</code> I get:</p>

<pre><code>Filesystem      Size  Used Avail Use% Mounted on 
/dev/sda2       9.1G  8.7G     0 100% /
udev             10M     0   10M   0% /dev
tmpfs           618M  8.8M  609M   2% /run
tmpfs           1.6G     0  1.6G   0% /dev/shm
tmpfs           5.0M  4.0K  5.0M   1% /run/lock
tmpfs           1.6G     0  1.6G   0% /sys/fs/cgroup
/dev/sda1       511M  132K  511M   1% /boot/efi
/dev/sda4       1.8T  623G  1.1T  37% /home
tmpfs           309M  4.0K  309M   1% /run/user/116
tmpfs           309M     0  309M   0% /run/user/1000
</code></pre>

<p>It looks like the /dev/data directory is about to explode, however if I tip:</p>

<pre><code>$ du -sh /dev/sda2
0   /dev/sda2
</code></pre>

<p>It seems it's empty.</p>

<p>I am new in Debian and I really don't know how to proceed. I used to typically access this computer via ssh. Besides this problem I have several others with this computer, they might be related, for instance each time I want to enter my user using the GUI (with root it works) I get:</p>

<blockquote>
  <p>Xsession: warning: unable to write to /tmp: Xsession may exit with an error</p>
</blockquote>
","<p>Your root file system is full and hence your temp dir (/tmp, and /var/tmp for that matter) are also full. A lot of scripts and programs require some space for working files, even lock files. When /tmp is unwriteable <em>bad</em> things happen.</p>

<p>You need to work out how you've filled the filesystem up. Typically places this will happen is in /var/log (check that you're cycling the log files). Or /tmp may be full. There's many, many other ways that a disk can fill up, however.</p>

<pre><code>du -hs /tmp /var/log
</code></pre>

<p>You may wish to re-partition to give /tmp it's own partition (that's the old school way of doing it, but if you have plenty of disk it's fine), or map it into memory (which will make it very fast but start to cause swapping issues if you overdo the temporary files).</p>
","277391"
"Change font in echo command","58104","","<p>Is it possible to change the font attributes of the output of echo in either zsh or bash?</p>

<p>What I would like is something akin to:</p>

<pre><code>echo -n ""This is the font: normal ""
echo -n $font=italic ""italic,""
echo -n $font=bold ""bold,""
echo -n ""and""
echo -n $font=small ""small"".
</code></pre>

<p>so that it print: ""This is the font: normal, <em>italic</em>, <strong>bold</strong>, <sub><sup>small</sup></sub>"" within a line  of text.</p>
","<p>On most if not all terminal emulators, you can't set different font sizes or different fonts, only colors and a few attributes (bold, underlined, standout).</p>

<p>In bash (or in zsh or any other shell), you can use the terminal <a href=""http://invisible-island.net/xterm/ctlseqs/ctlseqs.html"">escape sequences</a> directly (apart from a few exotic ones, all terminals follow xterm's lead these days). <code>CSI</code> is <code>ESC [</code>, written <code>$'\e['</code> in bash. The escape sequence to change attributes is <code>CSI Ps m</code>.</p>

<pre><code>echo $'\e[32;1mbold red\e[0mplain\e[4munderlined'
</code></pre>

<p>Zsh has a <a href=""http://zsh.sourceforge.net/Doc/Release/User-Contributions.html#index-colors"">convenient function</a> for that.</p>

<pre><code>autoload -U colors
colors
echo $bold_color$fg[red]bold red${reset_color}plain$'\e'$color[underline]munderlined
</code></pre>
","37404"
"How to display kernel command line parameters?","58092","","<p>In the <code>grub.conf</code> configuration file I can specify command line parameters that the kernel will use, i.e.:</p>

<pre><code>kernel /boot/kernel-3-2-1-gentoo root=/dev/sda1 vga=791
</code></pre>

<p>After booting a given kernel, is there a way to display the command line parameters that were passed to the kernel in the first place? I've found sysctl,</p>

<pre><code>sysctl --all
</code></pre>

<p>but sysctl shows up <em>all</em> possible kernel parameters.</p>
","<pre><code>$ cat /proc/cmdline
root=/dev/xvda xencons=tty console=tty1 console=hvc0 nosep nodevfs ramdisk_size=32768 ip_conntrack.hashsize=8192 nf_conntrack.hashsize=8192 ro  devtmpfs.mount=1 
$
</code></pre>
","48602"
"How to install Debian from USB? (Using full size image not netinstall)","58006","","<p>I learned of a way to install Debian from USB using the netinstall image, which is fine. However it means I have to spend hours and hours downloading packages to do the install. Is there a way I can simply download (for example) the CD1 with most of gnome and then use that? The netinstall method using this does not work because there is not enough space. (I have enough space, it is that the method has a limitation). I rarely have CDs on hand and some machines do not have CD/DVD drives anyway.</p>

<p>I will research on this topic and answer my own question if need be, however any help in the meantime is appreciated.</p>
","<p>How about downloading the CD1 ISO, then <a href=""https://unix.stackexchange.com/q/665/250"">put it on a USB and boot</a>? (My favourite)</p>

<p>How about using an automated tool such as <a href=""http://unetbootin.sourceforge.net/"" rel=""nofollow noreferrer"">UNetbootin</a>?</p>

<p>Here is another tool from <a href=""http://www.pendrivelinux.com/boot-multiple-iso-from-usb-multiboot-usb/"" rel=""nofollow noreferrer"">Pendrivelinux</a>.</p>
","6421"
"Cron running job every 15 seconds","57942","","<p>Could you advise me what to write in <code>crontab</code> so that it runs some job (for testing I will use <code>/usr/bin/chromium-browser</code>) every 15 seconds?</p>
","<p>You can't go below one minute granularity with cron. What you can do is, every minute, run a script that runs your job, waits 15 seconds and repeats. The following crontab line will start <code>some_job</code> every 15 seconds.</p>

<pre><code>* * * * * for i in 0 1 2; do some_job &amp; sleep 15; done; some_job
</code></pre>

<p>This script assumes that the job will never take more than 15 seconds. The following slightly more complex script takes care of not running the next instance if one took too long to run. It relies on <code>date</code> supporting the <code>%s</code> format (e.g. GNU or Busybox, so you'll be ok on Linux). If you put it directly in a crontab, note that <code>%</code> characters must be written as <code>\%</code> in a crontab line.</p>

<pre><code>end=$(($(date +%s) + 45))
while true; do
  some_job &amp;
  [ $(date +%s) -ge $end ] &amp;&amp; break
  sleep 15
  wait
done
[ $(date +%s) -ge $(($end + 15)) ] || some_job
</code></pre>

<p>I will however note that if you need to run a job as often as every 15 seconds, cron is probably the wrong approach. Although unices are good with short-lived processes, the overhead of launching a program every 15 seconds might be non-negligible (depending on how demanding the program is). Can't you run your application all the time and have it execute its task every 15 seconds?</p>
","11105"